{
  "metadata": {
    "last_updated": "2026-01-09 16:45:41",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 45,
    "total_comments": 335,
    "file_size_bytes": 388890
  },
  "items": [
    {
      "id": "1q53qlk",
      "title": "LLMs are so unreliable",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/",
      "author": "Armageddon_80",
      "created_utc": "2026-01-06 00:45:52",
      "score": 162,
      "num_comments": 94,
      "upvote_ratio": 0.88,
      "text": "After 3 weeks of deep work, I''ve realized agents are so un predictable that are basically useless for any professional use. This is what I've found:\n\nLet's exclude the instructions that must be clear, effective and not ambiguos. Possibly with few shot examples (but not always!)\n\n1) Every model requires a system prompt carefully crafted with instructions styled  as similar as its training set. (Where do you find it? No idea)\nSame prompt with different model causes different results and performances. \nLesson learned: once you find a style that workish, better you stay with that model family.\n\n2) Inference parameters: that's is pure alchemy. time consuming of trial and error. (If you change model,  be ready to start all over again). No comment on this.\n\n3) system prompt  length: if you are too descriptive at best you inject a strong bias in the agent, at worst the model just forget some parts of it.\nIf you are too short model hallucinates.\nGood luck in finding the sweet spot, and still, you cross the fingers every time you run the agent. \nThis connect me to the next point...\n\n4) dense or MOE model? \nDense model are much better in keeping context (especially system instructions), but they are slow.\nMoE are fast, but during the experts activation not always the context is passed correctly among them. The \"not always\" makes me crazy.\nSo again you get different responses  based on I don't know what.! Pretty sure that are some obscure parameters as well...\nHope Qwen next will fix this.\n\n5) RAG and KGraphs? Fascinating but that's another field of science. Another deeeepp rabbit hole I don't even want to talk about now.\n\n6) Text to SQL?  You have to pray, a lot. Either you end up manually coding the commands and give it as tool, or be ready for disaster. And that is a BIG pity, since DB are very much used in any business.( Yeah yeah. Table description data types etc...already tried) \n\n7) you want reliability? Then go for structured input and output! Atomicity of tasks!\nI got to the point that between the problem decomposition to a level that the agent can manage it (reliably) and the construction of a structured input/output chain, the level of effort required makes me wonder what is this hype about AI? Or at least home AI.  (and I have a Ryzen AI max 395).\n\n\nAnd still after all the efforts you always have this feeling: will it work this time? \nAgentic shit is far far away from YouTube demos and frameworks examples.\nSome people creates Frankenstein systems, where even naming the combination they are using is too long,.but hey it works!! Question is \"for how long\"? \nWhat's gonna be deprecated or updated on the next version of one of your parts?\n\nWhat I've learned is that if you want to make something professional and reliable, (especially if you are being paid for it) better to use good old deterministic code, and as less dependencies as possible. Put here and there some LLM calls for those task where NLP is necessary because coding all conditions would take forever.\n\nNonetheless I do  believe, that in the end, the magical equilibrium of all parameters and prompts and shit must exist. And while I search for that sweet spot, I hope that local models will keep improving and making our life way simpler.\n\nJust for the curious: I've tried every possible model until gpt OSS 120b, Framework AGNO. Inference with LMstudio and Ollama (I'm on Windows, no vllm).\n\n \n\n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q53qlk/llms_are_so_unreliable/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxx7hpr",
          "author": "macromind",
          "text": "Yep, this matches my experience too: agents look magical in demos, but in real work its mostly prompt + tool wiring + guardrails + a lot of retries and tests.\n\nOne thing that helped me was treating the LLM like a fallible component and making everything around it deterministic: strict JSON schemas, small steps, unit tests on tool outputs, and hard timeouts/fallbacks.\n\nIf you are experimenting with patterns for making agent workflows less chaotic, this writeup has a few practical ideas (tool contracts, evals, and reliability tricks): https://www.agentixlabs.com/blog/",
          "score": 53,
          "created_utc": "2026-01-06 00:51:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz41y4",
              "author": "publiusvaleri_us",
              "text": "The problems I see with this method is that testing prompts and scaling are counter-positioned into a matrix of chicken-and-egg paradoxes.\n\nLike opening a pizza store, but not getting a cheese and flour supplier. Regardless, you need to see who might want pizza in your community. That means you need to pick the cheese and flour, but the pizza wholesale company won't sell you just 10 pounds of it. They want a contract for 300 pounds a week and then they'll talk.\n\nSo you buy Walmart cheese and call your neighbor, but the test is inconclusive. The project can never move forward without high risk of capital and doing the whole pizza store thing and just opening up with the supplies you think are right.\n\nFor the LLM, it means you have to throw heavy hardware or high capital, lots of time, and lots of tests into something that might never be profitable for business or affordable for home hobbies.\n\nYou don't know until you add up the unpredictable costs while you tested and played with prompts for a week to a year. And find that secret sauce.",
              "score": 4,
              "created_utc": "2026-01-06 08:29:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz533p",
                  "author": "publiusvaleri_us",
                  "text": "And you know how I know this?\n\nBecause when you find your secret sauce LLM prompt and settings for an application, what happens? Productivity/quality/accuracy goes from zero to hero with that one last final tweak. Be it a word in a prompt or adding 128 GB of RAM to a PC, you found the sweet spot.\n\nEvery other spot was wrong. It's the graph that shows a sharp peak that rises from the noise floor to a 50 dB signal. It can be tweaked in version 2, but you broke through the barrier.\n\nEverything LLM is like this - very hit or very miss. I've seen the video series by [https://www.youtube.com/@aiwarehouse](https://www.youtube.com/@aiwarehouse) AI Warehouse that bears this out, as well. Albert is a moron for 10,000 iterations and then he \"learns\" a trick. And it cascades to a new skill.",
                  "score": 0,
                  "created_utc": "2026-01-06 08:39:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxxh77o",
              "author": "cneakysunt",
              "text": "We're about to dive in seriously. After much research over the last year this is exactly where we have landed.",
              "score": 3,
              "created_utc": "2026-01-06 01:43:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxiqrp",
          "author": "publiusvaleri_us",
          "text": "YES. #4 and #5, plus 1, 2, and 3. You did a lot of thinking on this and are correct.\n\nMy comment is this on the latter part. Your prediction of the future is sound. There will be so many iterations of AI improvements, PC hardware improvements, and refinements in the interface that the LLM of 2036 will be nothing like an LLM of 2026. Early adopters have so many disadvantages.\n\nThis stuff will be cheaper and faster in ten years. At current tech level and my current interest level, I think an adequate system, for just my personal use (With an eye to selling it), would cost $500 to $2000 per month and would need a new Internet connection. The current software may seem bloated when you download it, but the interfaces of today are simply a kludge and a Jack-of-all-trades that does nothing right except maybe answer chat questions a 6th grader might ask.\n\nAsk a current LLM about a dataset, and you're going to get terrible results. Even commercial systems stink. All you have to do is call into a Big Company and ask for tech support. The human agents are typing things into an AI, you can tell, and trying to bring up internal PDFs to answer your question and walk you through a solution.\n\nBecause their innate human knowledge is practically nil. They read from a script like phone agents have done for 40 years, but they are reading from hallucinated AI slop or the wrong PDF that's from 4 versions earlier that do not apply. LLMs have not taught Tier 1 support personnel how to think, and they certainly haven't trained them on the specifics of the things they are supporting.\n\nIf Fortune 100 companies can't figure out how to get AI to work to help their customers at Tier 1 (and their bottom line making money), I find it ridiculous to assume that a one-man-shop could figure it out.\n\nI wish all of you programmers, content creators, and schoolkids doing homework all the best! For the rest of us, it's hard to go full-in to this mess, for all of the reasons OP stated, starting at #1 and on down the list.\n\nBravo.",
          "score": 18,
          "created_utc": "2026-01-06 01:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzbjqd",
              "author": "thedarkbobo",
              "text": "Big corpo is slower sometimes in using top edge tech because of risks audits etc. I would think it should be first used by small companies. Big tech will use consultants when they are available and things stabilize s bit I think.",
              "score": 2,
              "created_utc": "2026-01-06 09:42:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny1daoz",
                  "author": "Embarrassed_Egg2711",
                  "text": "That's a broad generalization and while it can be true about initial adoption, it's not related to being unsuccessful when they do try, and certainly not a given when you have a hype-freight-train like \"AI\".  There's been no real success, even with companies like SalesForce that have already bought into AI.",
                  "score": 3,
                  "created_utc": "2026-01-06 17:07:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxazja",
          "author": "StardockEngineer",
          "text": "Only Sonnet and Opus work well as agents that can be used without high specialization.  Anything else requires lots of leg work.  Minimax 2.1 is the closest I’ve found in the OSS world.",
          "score": 16,
          "created_utc": "2026-01-06 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0msbs",
              "author": "opensourcecolumbus",
              "text": "Minimax2.1 better than qwen 3?",
              "score": 3,
              "created_utc": "2026-01-06 15:04:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny0zpdv",
                  "author": "StardockEngineer",
                  "text": "Yes, it is.  Especially for agents.  The interleaved-thinking is great.  More here: https://www.minimax.io/news/minimax-m21",
                  "score": 6,
                  "created_utc": "2026-01-06 16:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny15m6i",
              "author": "svachalek",
              "text": "Haiku does too, given precise instructions. It’s in a league of its own for models that are in its presumed size class.",
              "score": 1,
              "created_utc": "2026-01-06 16:32:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx84fv",
          "author": "BrewHog",
          "text": "You mentioned it. Structured input and output with a reliable model (professionally I only use the big boy models Gemini 3, Claude, etc). \n\n\nI'm currently using it for quite a few tasks regularly and reliably. It definitely helps me keep my business running without the need to hire two or three employees. \n\n\nThe first level support is fantastic, and the cliche sentiment analysis usually works well for what I need. \n\n\nFor more complex tasks, I still use only DSPy for the structured in/out and many times just manually run it to save me oogads of time (product/marketing material, document reviewing, etc) \n\n\nGive us some specific examples of what you need so we can guide you. Or just propose an idea that you think should work, but doesn't in practice.",
          "score": 10,
          "created_utc": "2026-01-06 00:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxa8ut",
              "author": "Armageddon_80",
              "text": "I think, from early days, introducing frontier models in companies workflows is a huge strategical risk. \nClearly depend from the business you have and the complexity of it. But unfortunately the big players are all in USA (I'm in Europe ) and I stronly5 believe soon or late, AI will become a \"tool\" for geopolitical leverage.\nI can't imagine what happens to a company where employees got lazy thanks to the magic of AI, and the day after the service is interrupted (or even worse, no employees at all only AI). That's why I'm working hard to implement an architecture dependant only on Local models.",
              "score": 16,
              "created_utc": "2026-01-06 01:05:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxye9tv",
                  "author": "ThatOneGuy4321",
                  "text": "even if AI doesn’t turn into a straight up propaganda tool, the huge disparity between current investment and revenue guarantees rapid enshittification of the service as they try to figure out how to make it profitable. \n\nQuite possibly 10x price hikes and complete flooding of marketing content. I seriously doubt OpenAI will even be able to solve their revenue problem honestly and may just go bankrupt. Their 5-year spending commitment is $1 trillion and their yearly revenue is $13 billion lol",
                  "score": 7,
                  "created_utc": "2026-01-06 04:55:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny013tm",
                  "author": "Kos187",
                  "text": "1. Go Europe!\n2. Mistral Large 3 isn't bad, it's also doesn't feel like first echelon model today \n3. Big Chinese models do feel like first Echelon and could be run locally,  but require expensive hardware. 10k for Mac Studio give rather small t/s, or 20k to fit into VRAM and good performance. Or maybe 128GB RAM and single large GPU for barely running it at all (/ t/s).\n4. Hosting in EU is also an option, but without reserved instances running big Chinese models is something like 5k a month spend.",
                  "score": 6,
                  "created_utc": "2026-01-06 13:06:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxv55p",
                  "author": "AfterAte",
                  "text": "I do believe someday the transatlantic cables will be cut by autonomous drones. This world has gone into full anarchy, and \"haven't seen nothing yet\". We'll all have to get Starlink after that.",
                  "score": 2,
                  "created_utc": "2026-01-06 02:59:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny20w0t",
                  "author": "BrewHog",
                  "text": "I definitely agree that you shouldn't solely depend on the big business frontier models. Local LLMs as a backup is definitely important if you choose the frontier models.\n\nThe only other issue I have with Frontier models seems to be how quickly older models get deprecated and removed.",
                  "score": 1,
                  "created_utc": "2026-01-06 18:53:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyizky1",
                  "author": "Equivalent_Mine_1827",
                  "text": "Me too... I've been investing a lot of time trying to make a reliable local llm workflow.\nIt's tough.\nEven more tough on my side, I don't have a very nice computer",
                  "score": 1,
                  "created_utc": "2026-01-09 03:00:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxx8gb2",
          "author": "Krommander",
          "text": "If the instruction set is plain text or markdown I have found that the system's prompt adherence depends on the coherence. \n\n\nVery coherent prompts can be 100+ pages without breaking for commercial LLMs, if internal knowledge mapping and cognitive routines are recursive. \n\n\nFor local models, I have had some small success, but the context window gets busted after 2 replies if I use the same huge system prompt. \n\n\nTool use is a whole other bag of problems though. ",
          "score": 4,
          "created_utc": "2026-01-06 00:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzc9lp",
              "author": "thedarkbobo",
              "text": "Need to compact memory I guess if reaches threshold.",
              "score": 3,
              "created_utc": "2026-01-06 09:49:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny1n0jw",
          "author": "AllTheCoins",
          "text": "I’m starting to realize anyone who deals with LLMs and feels this way about them has NEVER been in charge of someone before. Agents will always deliver something. People are way more unreliable.",
          "score": 5,
          "created_utc": "2026-01-06 17:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyahl6u",
              "author": "AurumDaemonHD",
              "text": "Perfect take. They are also more expensive and they can and will quit.",
              "score": 1,
              "created_utc": "2026-01-07 22:50:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyja7xr",
                  "author": "LifeandSAisAwesome",
                  "text": "And or steal / lie cause legal nightmares etc.",
                  "score": 1,
                  "created_utc": "2026-01-09 04:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxllmf",
          "author": "Such_Advantage_6949",
          "text": "Reliable LLM is not cheap. From my experience, reliable local models are from minimax glm onwards (200b onwards). 120B oss is a start but still very hit and miss",
          "score": 9,
          "created_utc": "2026-01-06 02:07:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxae4x",
          "author": "PromptOutlaw",
          "text": "I went thru this pain, for my scope of work I managed to tame most models except Kimi K2 and Cohere-command-a. You need:\n- adapters per LLM\n- normalization per LLM\n- strict output response\n\nHave a look here I hope it helps: https://github.com/Wahjid-Nasser/12-Angry-Tokens\n\nI’m pushing an update this week for observability, redundancy and some stats",
          "score": 3,
          "created_utc": "2026-01-06 01:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxdm1b",
              "author": "Armageddon_80",
              "text": "Can you explain what do you mean for adapters and normalization? I'm very interested",
              "score": 1,
              "created_utc": "2026-01-06 01:24:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxxgj4n",
                  "author": "PromptOutlaw",
                  "text": "E.g. Deepseek is gna spit out its thinking, stop fighting it and strip. Claude fights you to surround your output with markdown tags, just strip it. With adapters you have a generic prompt core, and each LLM must have an adapter prompt padded before API call to make it behave.\n\nHighly advise against generic prompt for all to work. I ended up with prompt-spaghetti that gave me headaches",
                  "score": 3,
                  "created_utc": "2026-01-06 01:39:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxd877",
          "author": "Classic_Chemical_237",
          "text": "Traditionally, we work with structured input and output. With structured io, it’s natural to be UX-centric.\n\nWith LLM, it became text centric with chat and voice. To work with traditional endpoints, supposedly you create MCP to sit on top of API.\n\nThis whole approach is wrong. Human work way better which structured UX. (That’s what even LLM try to emulate it with bullet points and emojis) Most use cases only need LLM for part of the flow.\n\nWe don’t need MCP sitting on top of API. We need API sitting on top of LLM.\n\nI am so close to ship ShapeShyft for make this easy.",
          "score": 3,
          "created_utc": "2026-01-06 01:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyzu8d",
          "author": "fermented_Owl-32",
          "text": "Using some psychological experience always MILDS DOWN these problems significantly. I have seen people complain of the same problems and i have implemented them in professional env as well. \n\nIts just a prompt, the better you understand how LLMs work and the better you are with human psychology and communication habits, the more robust outputs you get.\n\nI prefer to use not the latest models but from 2025 beginning H1. For Professional basic uses even Amazon's nova prime 2 does wonders  \n\nI still dont understand after a good analysis as this where you yourself write your issues, how have you not able to get things done by keeping these mind or making it part of the system.",
          "score": 4,
          "created_utc": "2026-01-06 07:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz48eb",
              "author": "Armageddon_80",
              "text": "I like your comment specially the last part:\nThe quick answer is that these systems are far more complex than what they may appear. \n\nIf you read the release papers of the models, you must be a PHD on many fields of study to  understand what's even written there. Which of course is not my case.\nI'm not talking of knowing and repeating as a parrot, I'm talking about understanding. \nAfter various steps of quantization and adaptation, some kind of distilled version of the original model finally lands on your computer. And now what? \nYou need to test it and start to know it, in a process of \"reverse learning\", and yes it's difficult.\n\nThe other way is to \"contain it\" to make it kind of behave the way you want, but that is also a lot of work, building guardrails and a very strict architecture around it, and  removing all the beauty of AI. \n\nLately I'm chatting with the models I intend to use, and making some precise questions to let the model leak its \"internal ways\" of writing, processing, expecting instructions. Trying to get some of it's secrets let's say through chat sessions and role plays \nIn other words psicology of the model. \n\n\n\n\n\n\n\n\n\n \nIf you try to simplify\n\n\nIn other words, you have many variables,",
              "score": 3,
              "created_utc": "2026-01-06 08:31:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx8dte",
          "author": "newbietofx",
          "text": "I'm creating a text to sql llm. How do I ensure it works? Run complicated crud? ",
          "score": 2,
          "created_utc": "2026-01-06 00:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxc9hn",
              "author": "Armageddon_80",
              "text": "Most model will fail on multi tables DB.\nI had to create an agent for every table, engineering the table itself with explicit column names and minimal foreign keys. I had to include brief description for each column so the model can \"link\" my text request. Understand it and then convert it into SQL command. \nI'm now running a team of agents, where each agent represent  their  tables (CRUD)  and the team leader represent the DB.\nStill working on it, I was just telling you how I'm fixing that thing and maybe give you an idea.",
              "score": 3,
              "created_utc": "2026-01-06 01:16:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0rj2z",
                  "author": "Powerful-Street",
                  "text": "Just make your own router essentially and keep the output in memory until it is written to db by your router.",
                  "score": 1,
                  "created_utc": "2026-01-06 15:27:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxxcpuh",
          "author": "Mountain-Hedgehog128",
          "text": "Agents need to be combined with rules based structure. They are valuable, but the toughest part is finding the right places and ways to use them.",
          "score": 2,
          "created_utc": "2026-01-06 01:19:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxesr1",
          "author": "Taserface_ow",
          "text": "LLMs are never going to be perfect, it’s the nature of the model. Successful products/services that use LLMs build workflows around them to cater for this limitation.",
          "score": 2,
          "created_utc": "2026-01-06 01:30:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxewea",
          "author": "grathontolarsdatarod",
          "text": "So.... Lie detectors are not admissible as evidence in most courts with an advanced legal system.  Yet they are used in employment screening in those same jurisdictions. \n\nAI is a scape goat for anything.  Its literally tablets from the mountain that can whatever.",
          "score": 2,
          "created_utc": "2026-01-06 01:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxrim5",
          "author": "SAPPHIR3ROS3",
          "text": "As much as this kind match my experience, THE general rule i follow is “Divide et impera” (meaning divide and conquer) as much as possible: going under 30b, MoE or not, it’s a guess hence you have to use LLM for NLP-based task only and as least as possible (above 30b you start seeing some more consistency, but only above 100b roughly the result start to feel reliable). Nonetheless structured input/output are vital to ensure consistency in workflows at any size even with the big bois (claude, gemini chatgpt ecc.)",
          "score": 2,
          "created_utc": "2026-01-06 02:39:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxwx3",
          "author": "Proof_Scene_9281",
          "text": "You have to blend commercial LLM’s and local for the best results. Use the commercial to think and craft and the locals to dig ",
          "score": 2,
          "created_utc": "2026-01-06 03:14:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyzklf",
          "author": "Dentuam",
          "text": "The point about MoE models dropping context during expert routing still rings true for many local runs, though the latest Qwen3 MoE releases have noticeably stabilized that behavior compared to earlier versions. Dense models continue to win for reliability when you need consistent instruction following.",
          "score": 2,
          "created_utc": "2026-01-06 07:48:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzaxiz",
          "author": "a_pimpnamed",
          "text": "I truly don't understand why people think LLM's can ever actually become true intelligence. You can't even see the logic chain or query it about its own logic. They learn once and they are stuck unless you train them over again. They use way too much compute. This can only be scaled so much even if Microsoft gets bitnet to work it still would be capped eventually it's not true intelligence just a prediction algo a very good prediction algo. We need to move back to symbolic ai or to actual cognitive architectures.",
          "score": 2,
          "created_utc": "2026-01-06 09:36:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzjm4b",
              "author": "Armageddon_80",
              "text": "Well for many many tasks you don't need intelligence. Including most of business workflow: People over estimate their roles in working environments :).\nSo I agree with you, but AI can save you from writing lot of code for simple things like user intent.yepp that's not intelligence, but why not to use it?",
              "score": 3,
              "created_utc": "2026-01-06 10:55:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny37s1j",
          "author": "BenevolentJoker",
          "text": "The reason why many LLMs are unreliable as agents boils down to mainly one thing: in order for LLMs to really be actually useful for any professional workloads you need to be able to modify their behavior from probabilistic behavior to deterministic behavior. Prompting will only get you so far, and not without drawbacks (namely it eats into context).\n\nThere are a few different ways to do this, some easier than others such as pre and post filtering outcomes. The most riggorous and ideal setup involves logit logging and logit inferencing at inference time. -- do these things and honestly model behavior evens out fairly well across all models.",
          "score": 2,
          "created_utc": "2026-01-06 22:10:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3rari",
          "author": "Interesting_Ride2443",
          "text": "I feel your pain. The gap between a YouTube demo and a production-grade agent is a canyon.\n\nYou're 100% right about 'good old deterministic code'-that’s actually the only way to make agents reliable. The mistake most frameworks make is trying to hide the logic inside complex abstractions or 'black box' prompts.\n\nI’ve shifted to an 'agent-as-code' approach where the LLM is just a tool inside a strictly typed (TS) function. Instead of praying for a prompt to work, you manage the state and logic in code, and only use the LLM for the 'messy' NLP parts. It’s much easier to debug when you have a managed runtime that shows you the exact state and execution trace at every step.\n\nReliability doesn't come from better prompts; it comes from better infrastructure that treats agents like software, not like magic.",
          "score": 2,
          "created_utc": "2026-01-06 23:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9rchx",
          "author": "No-Comfortable-2284",
          "text": "LLMs are not accurate but you can create a system that only outputs accurate/desired output with fixed checkpoints. You can also increase the quality and accuracy of desired outputs by adding another layer of complexity on top. for example using another seperate instance of the model with sole purpose of cross checking answers with RAG. I think the biggest issue in AI currently is that people try to make a single model do everything. our brain is made of many modules of less intelligence, communicating together, ant colonies are made of dumb ants. greater intelligence can be created from many instances of lesser intelligence and if there ever will be a super intelligence it prob will be smthn like that :p",
          "score": 2,
          "created_utc": "2026-01-07 20:55:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny9vat0",
              "author": "Armageddon_80",
              "text": "I agree with you. Only problem is that more layers of intelligence means more latency in general. In some cases, where parallelism can be of use, the latency issue could be solved with concurrent calls in async mode.\nBut not in the case you described  and neither in mine.\nSo I'm pushing the limits of the model until eventually I'm gonna have to find other ways.\nI've found a sweet spot with gpt OSS 20b for now. Alleluja.",
              "score": 2,
              "created_utc": "2026-01-07 21:11:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyvjfa",
          "author": "Your_Friendly_Nerd",
          "text": "I wholeheartedly agree with all the points you make. I've been trying to integrate local LLM's into my workflow as a programmer (mainly qwen3-coder). It works fine for simple tasks and tool calls, but as soon as it gets slightly more complex, it's not even worth the effort to write the prompt and wait for its output, it's quicker to just code it myself. \n\nThat said, I also think LLM's are still in their infancy, and there is so much we have yet to discover about them. For example for my use-case as a programmer, I see a lot of potential in spec-driven development, where a task is broken down into tiny subtasks that can then each be implemented and tested, and not be too complex for a local LLM to achieve. But how to formulate those specs? How does the workflow look like? I don't know.\n\nAnd that's maybe another issue - so many of the AI integrators (like IDE chat plugins) expect to talk to a frontier model like Claude Sonnet, Gemini or GPT. But when I plug them into a smaller model, they shouldn't just use the exact same prompts, but more specialized ones, and I fear there's just not enough interest in the community in perfecting this.",
          "score": 2,
          "created_utc": "2026-01-06 07:11:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzcvsp",
              "author": "thedarkbobo",
              "text": "What exactly was your setup? How far did you go?:)",
              "score": 1,
              "created_utc": "2026-01-06 09:54:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny33n4k",
                  "author": "Your_Friendly_Nerd",
                  "text": "I use NeoVim as my editor, CodeCompanion as the AI Chat plugin. Didn't really get very far, still kinda figuring out how to make the best use of it.",
                  "score": 1,
                  "created_utc": "2026-01-06 21:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6epzr",
          "author": "MadeByTango",
          "text": "AI is the stripper of technology; you love *her*, she loves *money*, and nothing is real except the financial hangover once you step out of the building….",
          "score": 2,
          "created_utc": "2026-01-07 10:37:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6euq1",
              "author": "Armageddon_80",
              "text": "Hahahaha this is a good one!!!",
              "score": 1,
              "created_utc": "2026-01-07 10:38:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxxawx",
          "author": "evilbarron2",
          "text": "I think there are valid use cases for LLMs in production systems, but I feel like most places I see it used it’s being shoehorned. I’m not sure that they are most (or even particularly) useful in our current software systems. I kinda think they’ll be more useful in a different type of computing structure. I think we’ve only seen glimpses of what that looks like so far.",
          "score": 1,
          "created_utc": "2026-01-06 03:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxy58mr",
          "author": "IllustratorInner4904",
          "text": "Intent -> deterministic tools -> let agent call tools = less unreliable agents (who instead sometimes want to tell you about the tools they will call lmao)",
          "score": 1,
          "created_utc": "2026-01-06 03:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyumsw",
          "author": "linewhite",
          "text": "Yeah, i just made [https://www.ngrm.ai](https://www.ngrm.ai) to deal with this, structured memory drastically helped my workflows, LLMs are not enough, but with the right tools it gets better as time goes on.",
          "score": 1,
          "created_utc": "2026-01-06 07:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz7cs9",
          "author": "Black_Hair_Foreigner",
          "text": "LLM is just another automation tool. Like, When you write some code but you are too lazy to write over 1000 code. So you decide to use LLM to write code and it did just 5 minute or less. In this progress, you check LLM’s code that this logic is really correct. You found some bug or nun-sense, and fix it. Everything is running well! If you doing this shit by your hand, you will be consume your time more than 3 days and your time is gold. This is why everyone use this shit. Everyone knows this is piece of shit, but time is too expensive to write code.",
          "score": 1,
          "created_utc": "2026-01-06 09:01:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzj83l",
              "author": "Armageddon_80",
              "text": "For coding I must say that both Gemini Antigravity and Claude are fantastic, but for WRITING code, not ENGINEERING.  Still crazy horses hard to control, but very very good. \nIf you want to create something from zero, is gonna take many rounds before it takes shape (and you actually understand the code).",
              "score": 2,
              "created_utc": "2026-01-06 10:52:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzjo3t",
          "author": "Cergorach",
          "text": "A couple of things:\n\na.) LLMs are tools, just like with any tool, you need to learn how to use it. A master woodworker will be far more proficient with a hammer then Joe Smoe that just picked up a hammer from Harborfreight for the first time ever. Different tools, different skill sets.\n\nb.) Right tool for the job. For some jobs these tools are either the wrong tools or just not worth the time/cost. Just as it's sometimes just faster to do something yourself then explain to someone else what you want so they can do it for you.\n\nc.) There can be a huge difference in quality between 'cheap' tools and 'expensive' tools. In this case you're using tiny models locally, even the 120B quantized is not going to compare well to the big unquantized models. For SQL see: [https://llm-benchmark.tinybird.live/](https://llm-benchmark.tinybird.live/)\n\nd.) You need to know the answer, always with LLM. If you don't, you're in for a world of hurt in a professional production environment. And SQL queries in a production environment seem to *me* like probably the worst possible use of LLM.\n\ne.) People need to realize that at what point are you spending more time=money on LLM then actually making your work more efficient?\n\nSidenote: I have not yet used LLM in my professional capacity. Primary reason is that I tend to work on the edge of IT deployment, and the LLMs haven't been trained yet on the use cases I'm working on and the edge cases that are very rare. Not to mention that even IF the LLM is trained on the 'promotional' material, the reason I'm doing my thing is to see if what's been sold to the client actually works as the sales people say it does (and it often does not)... The other reason is that the companies/clients I work for either do not allow LLM or have not yet onboarded it. And when I work for a client, I only use what they have allowed.\n\nPersonally I've tested quite a bit on a Mac Mini M4 Pro 64GB with models up to 70b, mostly for hobby projects. The results from there is that while very cool and the DS 70b model was better then the far larger GPT from less then a year before, still the unquantized far larger models we can't run locally did far, far better. And due to it being for my hobby projects, I saw no security concerns to *not* use the larger models (for free) on the web. Even the DS 671b model *quantized* on a $10k Mac Studio M3 Ultra 512GB gave worse quality responses then the full unquantized free model you could use on the Internet. Spending $50k on a Mac Studio cluster would be cool, but imho highly inefficient.",
          "score": 1,
          "created_utc": "2026-01-06 10:55:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2y4ig",
              "author": "New_Cranberry_6451",
              "text": "You say you are not using LLMs and still reached to these conclusions... you are a wise man. The one I liked most is \"SQL queries in a production environment seem to me like probably the worst possible use of LLM\", because it reflects a huge mistake we keep doing, try to use AI for AUTOMATIONS or to solve problems that \"traditional programming\" do far better after years of perfectioning. Why would I need AI to make a CRUD?   \nIn real life and for the local LLMs available right now, there are very few problems they can really help and do things we couldn't do with traditional programming... for example, telling the AI to choose the comments that are related to some topic (when we don't already have categorization or tagging, because if we do, that's far more reliable... but if we don't have a categorization system already implemented, that is a new \"superpower\" we now have.\n\nI also liked OP's post a lot, great post and REAL lessons.",
              "score": 2,
              "created_utc": "2026-01-06 21:25:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0ufgv",
          "author": "_WaterBear",
          "text": "OpenEvidence is pretty much the only professional use-case I’ve encountered thus far that seems competent. RAG/document embedding for info searching is quite functional and reliable so long as the user isn’t lazy and only uses the LLM as a search engine. Most other use cases involve too high a probability of hallucination that, for any professional requiring precision, just adds burdensome QA uncertainty to their process. So, outside highly tailored and unserious use-cases, these things will be a bust. Circumscribe the bubble, lest you be caught inside when it pops.",
          "score": 1,
          "created_utc": "2026-01-06 15:41:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1hyqt",
          "author": "tim_dude",
          "text": "So, is it like hiring a group of general (generic?) professionals, giving them written instructions and a bunch of meth and telling them to come back when they figured it out on their own?",
          "score": 1,
          "created_utc": "2026-01-06 17:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny44jyu",
          "author": "Green-Ad-3964",
          "text": "perhaps...in 20x6 where x>2",
          "score": 1,
          "created_utc": "2026-01-07 00:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5nxpw",
          "author": "Agent_invariant",
          "text": "This resonates a lot. The shift for me was the same: stop treating the LLM as “smart” and start treating it as a fallible proposer. Once you make execution, state writes, and tool effects deterministic, the chaos drops fast.\nWhere I’ve seen things still break is after JSON + schemas — especially when agents can confidently continue after drift or partial failure. Hard blocking on integrity loss (instead of letting the model narrate past it) made a bigger difference than better prompts.\nCurious if you’ve found good ways to enforce that boundary cleanly at the tool/state layer",
          "score": 1,
          "created_utc": "2026-01-07 06:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyc7gh9",
          "author": "Echo_OS",
          "text": "Interesting\n\nReading through this thread, there seems to be broad agreement on the underlying issue.\n\nLLMs themselves are not inherently unreliable. The problem is that they are often used in roles that require deterministic behavior. When an LLM is treated as a probabilistic component within a deterministic system - for example, wrapped in agent-as-code patterns, strict input/output schemas, typed interfaces, and explicit checkpoints - most reliability issues are significantly reduced.\n\nAt that stage, the primary challenges shift away from prompt design or model choice and toward system architecture: managing latency, defining clear boundaries, and deciding which parts of the system are allowed to make judgments versus which must remain deterministic.",
          "score": 1,
          "created_utc": "2026-01-08 04:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydgg05",
          "author": "somehowchris",
          "text": "Tell me you’ve been in the trenches for less than 6 years without telling me",
          "score": 1,
          "created_utc": "2026-01-08 10:12:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydnyut",
              "author": "Armageddon_80",
              "text": "What? I'm 45,  I've started as a nerd hobby 3 years ago. my background is electronic engineering and I'm trying to introduce agentic systems in my company.",
              "score": 1,
              "created_utc": "2026-01-08 11:16:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nydk2xv",
          "author": "Worth_Rabbit_6262",
          "text": "You've written a lot, but not the use cases. Often, the problem isn't the underlying model, but the lack of integration with workflows. Have you tried optimizing performance with DPO?",
          "score": 1,
          "created_utc": "2026-01-08 10:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydq199",
              "author": "Armageddon_80",
              "text": "Use case is specialized agents in reading data from DBs and optimize/organize (by reasoning) human resources assignments based on a constraints like:\ncomplex business rules, time constraints, priority constrains and more. All of them being fucked up convoluted variables. \nIt's a task that requires 4 people and several hours every time we need to plan weekly activities and distribution of workload in the company.\n\nI've decomposed the global task into sub tasks.\nNow the tests are about 1 agent and 1 sub-task. When succeeds I scale to more agents/sub tasks (all of them in isolation). When all of them succeed, i'll organize a team of agents....then and only then I will worry about workflows and all the rest.\nIssue was \"unreliable following of system prompt\", seems I managed to fix that. Let's see if the other agents do well like the first one.\nThe take away is: what are the magic words the model wants to hear? Once you find it, the need of constructing a scaffold of guardrails and validations reduces to minimum. That is important because if I need to build a whole castle of safety around every agent, the system will not be  scalable at all.",
              "score": 1,
              "created_utc": "2026-01-08 11:33:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nye6yiq",
          "author": "at0mi",
          "text": "which quantisation? glm 4.7 in bf16 works great",
          "score": 1,
          "created_utc": "2026-01-08 13:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf4bmy",
          "author": "kacisse",
          "text": "I agree it's too early BUT it's because only a few people master the real best practices (what to prompt, how to present data to the llm, when tu summarize, when to cache, how to even name your cached object so that the llm don't get confused... Etc).\nI feel LLM have their periods, you just mess up a little comma or name and they go banana. It's definitely possible to build serious app with cheaper models BUT you really need the best practices...and that is not for everyone. So I agree the hype is too early, and no enterprise can rely on that long term. That's a bubble for me 😬",
          "score": 1,
          "created_utc": "2026-01-08 16:09:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfkfti",
              "author": "Armageddon_80",
              "text": "A reason of why I'm oriented to local models is the fact that once you mastered the prompting for that model's family you move quickly with the project.\nAnd also you don't need to worry for the long term because once is done, it's written on the stone (off line).\nI believe there are a lot of possibilities with local AI, I wouldn't really call it a bubble. There's a huge FOMO people trying to monetize big and fast, fully agree. That's why every day a new product is released. Users are more worried on what to learn next rather than put in real use what there's around today (and there's a lot). \nI love AI with all it's defects, I see a very bright future for those that can use it to solve real business problems.",
              "score": 1,
              "created_utc": "2026-01-08 17:19:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjacrc",
          "author": "LifeandSAisAwesome",
          "text": "Internet was a fab too, but really give it 5-10years and will be a very different landscape.",
          "score": 1,
          "created_utc": "2026-01-09 04:01:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykcg3t",
          "author": "MaestroCodex",
          "text": "It's a hype bubble maintained by companies that have invested so much money in it they can't afford to back down. ROI delivery is minimal. Overwhelming number of projects fail.  \n\nIt reminds me of the mini hype bubble when people were trying to shoehorn blockchain into any number of inappropriate use cases which went nowhere, except it's 100x worse and is costing the industry 1000x more. \n\nContext: I work for a major AI platform vendor.",
          "score": 1,
          "created_utc": "2026-01-09 08:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyrvap",
          "author": "Much-Researcher6135",
          "text": "hush now, people will begin to think it's a bubble\n\nwouldn't want that",
          "score": 1,
          "created_utc": "2026-01-06 06:39:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3dpzw",
              "author": "zipzag",
              "text": "it's not a bubble. He's reviewing local models for some unspecified use",
              "score": 3,
              "created_utc": "2026-01-06 22:38:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxjt0j",
          "author": "Terminator857",
          "text": "More reliable than humans for me.",
          "score": 1,
          "created_utc": "2026-01-06 01:57:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx75c7",
          "author": "writesCommentsHigh",
          "text": "okay now compare it to frontier models via api.",
          "score": 0,
          "created_utc": "2026-01-06 00:49:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx8qj4",
              "author": "Armageddon_80",
              "text": "Yes I know, but I've posted on local LLM for a reason.",
              "score": 8,
              "created_utc": "2026-01-06 00:57:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny0e2tt",
                  "author": "writesCommentsHigh",
                  "text": "And your title reads “all llms”\n\nYes local llms are going to be unreliable but not all",
                  "score": 1,
                  "created_utc": "2026-01-06 14:19:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxx9xk7",
              "author": "ispiele",
              "text": "Most of these points apply to the latest API models as well (in my experience), the first 3 points in particular",
              "score": 5,
              "created_utc": "2026-01-06 01:04:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxxe2gz",
                  "author": "ANTIVNTIANTI",
                  "text": "Anthropic is managing a cult apparently",
                  "score": 2,
                  "created_utc": "2026-01-06 01:26:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxxaqdl",
                  "author": "StardockEngineer",
                  "text": "Not to Sonnet and Opus.",
                  "score": 1,
                  "created_utc": "2026-01-06 01:08:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyakrtg",
          "author": "Purple-Programmer-7",
          "text": "You’re the problem.\n\nYour expectations are off.\n\nThink of it as a person that can make mistakes instead of a computer.",
          "score": 0,
          "created_utc": "2026-01-07 23:05:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyav9ch",
              "author": "Armageddon_80",
              "text": "Thanks the lord, you clearly figured it all out. I'm gonna apply your technical suggestion.",
              "score": 0,
              "created_utc": "2026-01-07 23:58:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxv8u6",
          "author": "alphatrad",
          "text": "Sounds like a skill issue",
          "score": -4,
          "created_utc": "2026-01-06 02:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzdjoc",
      "title": "Running GLM-4.7 (355B MoE) in Q8 at ~5 Tokens/s on 2015 CPU-Only Hardware – Full Optimization Guide",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/xdj2zjnz5bag1.png",
      "author": "at0mi",
      "created_utc": "2025-12-30 09:14:01",
      "score": 143,
      "num_comments": 22,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzdjoc/running_glm47_355b_moe_in_q8_at_5_tokenss_on_2015/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwpcpjs",
          "author": "Lxzan",
          "text": "Nice work and thanks for sharing! How much is the power draw?",
          "score": 18,
          "created_utc": "2025-12-30 09:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpd7l0",
              "author": "at0mi",
              "text": "about 1600W i will update my blogpost with detailed power draw\n\nUPDATE: the 1600W was at higher Thread count, with the optimum 64 Threads im at 1154W\n\nIdle is 694W",
              "score": 21,
              "created_utc": "2025-12-30 09:31:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwph0z3",
                  "author": "Amazing_Athlete_2265",
                  "text": "Oof, that's gonna cost a bunch to run",
                  "score": 17,
                  "created_utc": "2025-12-30 10:06:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqiawc",
                  "author": "mister2d",
                  "text": "Those are some expensive tokens.",
                  "score": 3,
                  "created_utc": "2025-12-30 14:36:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwqwrjk",
                  "author": "resil_update_bad",
                  "text": "jesus",
                  "score": 1,
                  "created_utc": "2025-12-30 15:50:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpoy5i",
          "author": "beedunc",
          "text": "Yes, been running the qwen3coder480b@q3 (245GB) on an old Dell T5810 running a single e5-2697v4, gets 2-3 tps. \n\nPower draw is only 250w under load. \n\nI never thought of disabling hyper threading, does that help a lot? Will be checking this out, thank you.",
          "score": 8,
          "created_utc": "2025-12-30 11:18:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwppbbm",
              "author": "at0mi",
              "text": "i also tried lower quantisation models but the quality of the output was crap, works for a chatbot but not for coding, \nin my case (8 numa nodes) disabling hyper threading gave an enormous boost",
              "score": 6,
              "created_utc": "2025-12-30 11:22:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwsp871",
              "author": "Candid_Highlight_116",
              "text": "If LLM inference is RAM bandwidth bound and HT was a tech to halve effective bandwidth because two virtual cores needs their respective data, it makes sense that turning off HT gives massive boost  \n\nsorry that's my hallucination but if it's actually like that it's pretty interesting",
              "score": 4,
              "created_utc": "2025-12-30 20:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwu51bd",
                  "author": "beedunc",
                  "text": "Cool, thanks for the suggestion!",
                  "score": 1,
                  "created_utc": "2025-12-31 01:21:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpkj5q",
          "author": "xgiovio",
          "text": "Watts",
          "score": 7,
          "created_utc": "2025-12-30 10:39:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwql1ld",
              "author": "MaverickPT",
              "text": "All of them",
              "score": 7,
              "created_utc": "2025-12-30 14:51:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqd59a",
          "author": "chafey",
          "text": "I love hacks like this - nice work.  The hardware may be cheap/free but the electricity won't be...",
          "score": 6,
          "created_utc": "2025-12-30 14:07:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq21y3",
          "author": "Such_Advantage_6949",
          "text": "I am passionate about running llm at usable speed..",
          "score": 5,
          "created_utc": "2025-12-30 13:00:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nws7ver",
          "author": "Foreign-Watch-3730",
          "text": "Same result ( 5.1 t/s ) , but in IQ5\\_K with 2 Xeon E5 2696 V4 and 400 Gb ddr4 ram ( on a very olf Dell T630 ) with 2 RTX 5090 ( ik\\_llama.cpp and [ubergarm](https://huggingface.co/ubergarm) for use opencode :  \n[https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5](https://huggingface.co/ubergarm/GLM-4.7-GGUF/discussions/5)\n\nnumactl --interleave=all ./build/bin/llama-server  \n\\--model \\~/ik\\_llama.cpp/models/GLM-4.7-Ubergarm/IQ5\\_K/GLM-4.7-IQ5\\_K-00001-of-00007.gguf  \n\\--alias GLM-4.7-IQ5  \n\\--host [0.0.0.0](http://0.0.0.0) \\--port 8080  \n\\--ctx-size 84992  \n\\--no-mmap  \n\\--threads 82 --threads-batch 82  \n\\--batch-size 1024 --ubatch-size 1024  \n\\--parallel 1 --flash-attn 1  \n\\--jinja --verbose  \n\\--n-gpu-layers 99  \n\\--tensor-split 0.5,0.5  \n\\--split-mode layer  \n\\--run-time-repack  \n\\--cache-type-k q4\\_0 --cache-type-v q4\\_0  \n\\--k-cache-hadamard  \n\\-ot 'blk.\\[0-8\\]..\\*exps.weight=CUDA0'  \n\\-ot 'blk.(8\\[6-9\\]|9\\[0-2\\])..\\*exps.weight=CUDA1'  \n\\-ot '.\\*exps.weight=CPU'",
          "score": 3,
          "created_utc": "2025-12-30 19:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqj2yx",
          "author": "Extension-Cow2818",
          "text": "Very interesting that turning hyperthreading off works better.  \nProbably memory access is causing issues in these types of workloads.\n\nIt would be also interesting to try MLK vs ATLAS vs BLAS",
          "score": 2,
          "created_utc": "2025-12-30 14:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq772g",
          "author": "Icy_Programmer7186",
          "text": "That's cool.  \nHow much memory did it consumed?",
          "score": 1,
          "created_utc": "2025-12-30 13:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqh2cl",
          "author": "ForsookComparison",
          "text": "This is very cool, thank you for testing this out.\n\nI'm curious what the use-case is? Is GLM decent as a general purpose model? Or will you give it a coding task and come back after a few hours",
          "score": 1,
          "created_utc": "2025-12-30 14:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdqlaq",
          "author": "spacefarers",
          "text": "Comes down to around $9 per million tokens just for electricity not sure if it's worth it lol",
          "score": 1,
          "created_utc": "2026-01-03 04:15:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyogyg",
          "author": "PitifulBall3670",
          "text": "Good job",
          "score": 1,
          "created_utc": "2026-01-06 06:11:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrv2is",
          "author": "Free_Manner_2318",
          "text": "7200 Watt for 5 tokens eh?!   \nAsk it if it was a reasonable decision.... :))))  \nB+ for the effort though. Not custom enough to be significant.",
          "score": 0,
          "created_utc": "2025-12-30 18:29:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py1gvp",
      "title": "Probably more true than I would like to admit",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/iann5fyl70ag1.png",
      "author": "low_v2r",
      "created_utc": "2025-12-28 20:24:33",
      "score": 137,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py1gvp/probably_more_true_than_i_would_like_to_admit/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwf8lgr",
          "author": "SunshineSeattle",
          "text": "I bought a particle tacyon, but now i have no idea what to do with it. 😭",
          "score": 9,
          "created_utc": "2025-12-28 20:29:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfk71i",
          "author": "Healthy-Nebula-3603",
          "text": "WHAT AN EDGE DEVICE ?",
          "score": 5,
          "created_utc": "2025-12-28 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwfm8iu",
              "author": "Count_Rugens_Finger",
              "text": "honestly not sure if this is serious but if it is, it just means the device-in-hand of the users (phones, tablets, PCs, car dashboards, POS systems, and whatnot)",
              "score": 7,
              "created_utc": "2025-12-28 21:35:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwksnf2",
                  "author": "QuinQuix",
                  "text": "Usually this refers to devices that are compute restrained.\n\nA user could have a beast of a pc workstation that technically lives on the edge (of the central cloud workspace) but it's not a usual thing to refer to compute strong devices as edge devices.",
                  "score": 2,
                  "created_utc": "2025-12-29 17:27:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj6kgc",
              "author": "trmnl_cmdr",
              "text": "https://preview.redd.it/wl5ul68kv4ag1.jpeg?width=349&format=pjpg&auto=webp&s=38e1efd5e4769de7bcf9d7c4bf5ae45a353fefb4\n\nThis, apparently. Although I’m not sure how this helps me masturbate.",
              "score": 4,
              "created_utc": "2025-12-29 12:05:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1g3yx",
                  "author": "jgwinner",
                  "text": "Please rev it up a bit ... now tilt it so the blade scrapes on the stone ...\n\nthat's it ...\n\nFASTER \n\nthat's IT ... IT ...\n\nuhhh.\n\nWhew. Could you shut that thing off? kthks.",
                  "score": 1,
                  "created_utc": "2026-01-01 05:43:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwua4fa",
              "author": "Jackuarren",
              "text": "The device that you use for edging, obviously.",
              "score": 1,
              "created_utc": "2025-12-31 01:51:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfaevi",
          "author": "Individual_Holiday_9",
          "text": "Unironically this",
          "score": 2,
          "created_utc": "2025-12-28 20:38:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfid3y",
          "author": "GCoderDCoder",
          "text": "I feel so seen!!!",
          "score": 1,
          "created_utc": "2025-12-28 21:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt8rsj",
          "author": "mobileJay77",
          "text": "Your solution: \n1. Head over the r/localLlama\n2. You hardly sleep any more\n3. She doesn't have to worry what you are thinking in bed.",
          "score": 1,
          "created_utc": "2025-12-30 22:26:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7b54w",
      "title": "Hugging Face on Fire: 30+ New/Trending Models (LLMs, Vision, Video) w/ Links",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q7b54w/hugging_face_on_fire_30_newtrending_models_llms/",
      "author": "techlatest_net",
      "created_utc": "2026-01-08 12:59:23",
      "score": 70,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "Hugging Face is on fire right now with these newly released and trending models across text gen, vision, video, translation, and more. Here's a full roundup with direct links and quick breakdowns of what each one crushes—perfect for your next agent build, content gen, or edge deploy.\n\n# Text Generation / LLMs\n\n* **tencent/HY-MT1.5-1.8B** (Translation- 2B- 7 days ago): Edge-deployable 1.8B multilingual translation model supporting 33+ languages (incl. dialects like Tibetan, Uyghur). Beats most commercial APIs in speed/quality after quantization; handles terminology, context, and formatted text.​ [tencent/HY-MT1.5-1.8B](https://huggingface.co/tencent/HY-MT1.5-1.8B)\n* **LGAI-EXAONE/K-EXAONE-236B-A23B** (Text Generation- 237B- 2 days ago): Massive Korean-focused LLM for advanced reasoning and generation tasks.​[K-EXAONE-236B-A23B](https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B)\n* **IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct** (Text Generation- 40B- 21 hours ago): Coding specialist with loop-based instruction tuning for iterative dev workflows.​[IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Loop-Instruct)\n* **IQuestLab/IQuest-Coder-V1-40B-Instruct** (Text Generation- 40B- 5 days ago): General instruct-tuned coder for programming and logic tasks.​[IQuestLab/IQuest-Coder-V1-40B-Instruct](https://huggingface.co/IQuestLab/IQuest-Coder-V1-40B-Instruct)\n* **MiniMaxAI/MiniMax-M2.1** (Text Generation- 229B- 12 days ago): High-param MoE-style model for complex multilingual reasoning.​[MiniMaxAI/MiniMax-M2.1](https://huggingface.co/MiniMaxAI/MiniMax-M2.1)\n* **upstage/Solar-Open-100B** (Text Generation- 103B- 2 days ago): Open-weight powerhouse for instruction following and long-context tasks.​[upstage/Solar-Open-100B](https://huggingface.co/upstage/Solar-Open-100B)\n* **zai-org/GLM-4.7** (Text Generation- 358B- 6 hours ago): Latest GLM iteration for top-tier reasoning and Chinese/English gen.​[zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **tencent/Youtu-LLM-2B** (Text Generation- 2B- 1 day ago): Compact LLM optimized for efficient video/text understanding pipelines.​[tencent/Youtu-LLM-2B](https://huggingface.co/tencent/Youtu-LLM-2B)\n* **skt/A.X-K1** (Text Generation- 519B- 1 day ago): Ultra-large model for enterprise-scale Korean/English tasks.​[skt/A.X-K1](https://huggingface.co/skt/A.X-K1)\n* **naver-hyperclovax/HyperCLOVAX-SEED-Think-32B** (Text Generation- 33B- 2 days ago): Thinking-augmented LLM for chain-of-thought reasoning.​[naver-hyperclovax/HyperCLOVAX-SEED-Think-32B](https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B)\n* **tiiuae/Falcon-H1R-7B** (Text Generation- 8B- 1 day ago): Falcon refresh for fast inference in Arabic/English.​[tiiuae/Falcon-H1R-7B](https://huggingface.co/tiiuae/Falcon-H1R-7B)\n* **tencent/WeDLM-8B-Instruct** (Text Generation- 8B- 7 days ago): Instruct-tuned for dialogue and lightweight deployment.​[tencent/WeDLM-8B-Instruct](https://huggingface.co/tencent/WeDLM-8B-Instruct)\n* **LiquidAI/LFM2.5-1.2B-Instruct** (Text Generation- 1B- 20 hours ago): Tiny instruct model for edge AI agents.​[LiquidAI/LFM2.5-1.2B-Instruct](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct)\n* **miromind-ai/MiroThinker-v1.5-235B** (Text Generation- 235B- 2 days ago): Massive thinker for creative ideation.​[miromind-ai/MiroThinker-v1.5-235B](https://huggingface.co/miromind-ai/MiroThinker-v1.5-235B)\n* **Tongyi-MAI/MAI-UI-8B** (9B- 10 days ago): UI-focused gen for app prototyping.​[Tongyi-MAI/MAI-UI-8B](https://huggingface.co/Tongyi-MAI/MAI-UI-8B)\n* **allura-forge/Llama-3.3-8B-Instruct** (8B- 8 days ago): Llama variant tuned for instruction-heavy workflows.​[allura-forge/Llama-3.3-8B-Instruct](https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct)\n\n# Vision / Image Models\n\n* **Qwen/Qwen-Image-2512** (Text-to-Image- 8 days ago): Qwen's latest vision model for high-fidelity text-to-image gen.​[Qwen/Qwen-Image-2512](https://huggingface.co/Qwen/Qwen-Image-2512)\n* **unsloth/Qwen-Image-2512-GGUF** (Text-to-Image- 20B- 1 day ago): Quantized GGUF version for local CPU/GPU runs.​[unsloth/Qwen-Image-2512-GGUF](https://huggingface.co/unsloth/Qwen-Image-2512-GGUF)\n* **Wuli-art/Qwen-Image-2512-Turbo-LoRAT** (Text-to-Image- 4 days ago): Turbo LoRA adapter for faster Qwen image gen.​[Wuli-art/Qwen-Image-2512-Turbo-LoRA](https://huggingface.co/Wuli-art/Qwen-Image-2512-Turbo-LoRA)\n* **lightx2v/Qwen-Image-2512-Lightning** (Text-to-Image- 2 days ago): Lightning-fast inference variant.​[lightx2v/Qwen-Image-2512-Lightning](https://huggingface.co/lightx2v/Qwen-Image-2512-Lightning)\n* **Phr00t/Qwen-Image-Edit-Rapid-AIO** (Text-to-Image- 4 days ago): All-in-one rapid image editor.​[Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **lilylilith/AnyPose** (Image-to-Image- 6 days ago): Pose transfer and manipulation tool.​[lilylilith/AnyPose](https://huggingface.co/lilylilith/AnyPose)\n* **fal/FLUX.2-dev-Turbo** (Text-to-Image- 9 days ago): Turbocharged Flux for quick high-quality images.​[fal/FLUX.2-dev-Turbo](https://huggingface.co/fal/FLUX.2-dev-Turbo)\n* **Tongyi-MAI/Z-Image-Turbo** (Text-to-Image- 1 day ago): Turbo image gen with strong prompt adherence.​[Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n* **inclusionAI/TwinFlow-Z-Image-Turbo** (Text-to-Image- 10 days ago): Flow-based turbo variant for stylized outputs.​[inclusionAI/TwinFlow-Z-Image-Turbo](https://huggingface.co/inclusionAI/TwinFlow-Z-Image-Turbo)\n\n# Video / Motion\n\n* **Lightricks/LTX-2** (Image-to-Video- 2 hours ago): DiT-based joint audio-video foundation model for synced video+sound gen from images/text. Supports upscalers for higher res/FPS; runs locally via ComfyUI/Diffusers.​[Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n* **tencent/HY-Motion-1.0** (Text-to-3D- 8 days ago): Motion capture to 3D model gen.​[tencent/HY-Motion-1.0](https://huggingface.co/tencent/HY-Motion-1.0)\n\n# Audio / Speech\n\n* **nvidia/nemotron-speech-streaming-en-0.6b** (Automatic Speech Recognition- 2 days ago): Streaming ASR for real-time English transcription.​[nvidia/nemotron-speech-streaming-en-0.6b](https://huggingface.co/nvidia/nemotron-speech-streaming-en-0.6b)\n* **LiquidAI/LFM2.5-Audio-1.5B** (Audio-to-Audio- 1B- 2 days ago): Audio effects and transformation model.​[LiquidAI/LFM2.5-Audio-1.5B](https://huggingface.co/LiquidAI/LFM2.5-Audio-1.5B)\n\n# Other Standouts\n\n* **nvidia/Alpamayo-R1-10B** (11B- Dec 4, 2025): Multimodal reasoning beast. [nvidia/Alpamayo-R1-10B](https://huggingface.co/nvidia/Alpamayo-R1-10B)\n\nDrop your benchmarks, finetune experiments, or agent integrations below—which one's getting queued up first in your stack?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7b54w/hugging_face_on_fire_30_newtrending_models_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nye4m8d",
          "author": "alex_godspeed",
          "text": "i scan thru all of them. Still thinking Qwen 3 30B is 'just right' for my business reasoning use case for 16G GPU. Or am I missing anything? Need it on GGUF LM Studio though",
          "score": 5,
          "created_utc": "2026-01-08 13:12:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyek04x",
              "author": "Ok-Employment6772",
              "text": "Qwen3 is amazing",
              "score": 3,
              "created_utc": "2026-01-08 14:34:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyipomj",
          "author": "PitifulBall3670",
          "text": "Thanks for sharing! This is very helpful",
          "score": 2,
          "created_utc": "2026-01-09 02:08:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyljd2t",
          "author": "beast_modus",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-09 14:08:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5m07h",
      "title": "Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/viqj8u1k1rbg1.png",
      "author": "A-Rahim",
      "created_utc": "2026-01-06 15:42:37",
      "score": 67,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q5m07h/unslothmlx_finetune_llms_on_your_mac_same_api_as/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "ny4wd85",
          "author": "Repeat_Admirable",
          "text": "MLX is seriously a godsend for Apple Silicon. It's wild how much power these M-chips have for inference when optimized correctly. I switched my entire writing workflow to a local Whisper-based app (running on CoreML/Metal) and it essentially uses zero CPU while transcribing in real-time. Great to see Unsloth coming to the ecosystem. The more native Mac tools, the better.",
          "score": 5,
          "created_utc": "2026-01-07 03:27:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1kecb",
          "author": "onethousandmonkey",
          "text": "Noob here:\nWhat are the pros and cons of fine-tuning on Mac compared to other types of local hardware?\nAnd does this project enhance/offset some of those?\n\nEdit: typo, clarification",
          "score": 2,
          "created_utc": "2026-01-06 17:39:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q06u1n",
      "title": "2025 is over. What were the best AI model releases this year?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "author": "techlatest_net",
      "created_utc": "2025-12-31 07:12:49",
      "score": 64,
      "num_comments": 32,
      "upvote_ratio": 0.88,
      "text": "2025 felt like three AI years compressed into one. Frontier LLMs went insane on reasoning, open‑source finally became “good enough” for a ton of real workloads, OCR and VLMs leveled up, and audio models quietly made agents actually usable in the real world. ​ Here’s a category‑wise recap of the “best of 2025” models that actually changed how people build stuff, not just leaderboard screenshots:\n\n\n\nLLMs and reasoning\n\n\\* GPT‑5.2 (Thinking / Pro) – Frontier‑tier reasoning and coding, very fast inference, strong for long‑horizon tool‑using agents and complex workflows.\n\n​\\* Gemini 3 Pro / Deep Think – Multi‑million token context and multimodal “screen reasoning”; excels at planning, code, and web‑scale RAG / NotebookLM‑style use cases. \n\n\\* Claude 4.5 (Sonnet / Opus) – Extremely strong for agentic tool use, structured step‑by‑step plans, and “use the computer for me” style tasks. \n\n\\* DeepSeek‑V3.2 & Qwen3‑Thinking – Open‑weight monsters that narrowed the gap with closed models to within \\\\\\~0.3 points on key benchmarks while being orders of magnitude cheaper to run.\n\nIf 2023–24 was “just use GPT,” 2025 finally became “pick an LLM like you pick a database.”\n\nVision, VLMs & OCR\n\n\\* MiniCPM‑V 4.5 – One of the strongest open multimodal models for OCR, charts, documents, and even video frames, tuned to run on mobile/edge while still hitting SOTA‑ish scores on OCRBench/OmniDocBench. \n\n\\* olmOCR‑2‑7B‑1025 – Allen Institute’s OCR‑optimized VLM, fine‑tuned from Qwen2.5‑VL, designed specifically for documents and long‑form OCR pipelines. \n\n\\* InternVL 2.x / 2.5‑4B – Open VLM family that became a go‑to alternative to closed GPT‑4V‑style models for document understanding, scene text, and multimodal reasoning.\n\n\\* Gemma 3 VLM & Qwen 2.5/3 VL lines – Strong open(-ish) options for high‑res visual reasoning, multilingual OCR, and long‑form video understanding in production‑style systems. ​ \n\n2025 might be remembered as the year “PDF to clean Markdown with layout, tables, and charts” stopped feeling like magic and became a boring API call.\n\nAudio, speech & agents\n\n\\* Whisper (still king, but heavily optimized) – Remained the default baseline for multilingual ASR in 2025, with tons of optimized forks and on‑device deployments. \n\n\\* Low‑latency real‑time TTS/ASR stacks (e.g., new streaming TTS models & APIs) – Sub‑second latency + streaming text/audio turned LLMs into actual real‑time voice agents instead of “podcast narrators.” \n\n\\* Many 2025 voice stacks shipped as APIs rather than single models: ASR + LLM + real‑time TTS glued together for call centers, copilots, and vibecoding IDEs. ​ Voice went from “cool demo” to “I talk to my infra/IDE/CRM like a human, and it answers back, live.”\n\nOCR/document AI & IDP\n\n\\* olmOCR‑2‑7B‑1025, MiniCPM‑V 4.5, InternVL 2.x, OCRFlux‑3B, PaddleOCR‑VL – A whole stack of open models that can parse PDFs into structured Markdown with tables, formulas, charts, and long multi‑page layouts. \n\n\\* On top of these, IDP / “PDF AI” tools wrapped them into full products for invoices, contracts, and messy enterprise docs. ​ \n\nIf your 2022 stack was “Tesseract + regex,” 2025 was “drop a 100‑page scan and get usable JSON/Markdown back.” ​ \n\nOpen‑source LLMs that actually mattered\n\n\\* DeepSeek‑V3.x – Aggressive MoE + thinking budgets + brutally low cost; a lot of people quietly moved internal workloads here. \n\n\\* Qwen3 family – Strong open‑weight reasoning, multilingual support, and specialized “Thinking” variants that became default self‑host picks. \n\n\\* Llama 4 & friends – Closed the gap to within \\\\\\~0.3 points of frontier models on several leaderboards, making “fully open infra” a realistic choice for many orgs.\n\n​In 2025, open‑source didn’t fully catch the frontier, but for a lot of teams, it crossed the “good enough + cheap enough” threshold.\n\nYour turn This list is obviously biased toward models that:\n\n\\* Changed how people build products (agents, RAG, document workflows, voice UIs)\n\n\\* Have public benchmarks, APIs, or open weights that normal devs can actually touch ​- What did you ship or adopt in 2025 that deserves “model of the year” status?\n\nFavorite frontier LLM?\n\n\\* Favorite open‑source model you actually self‑hosted?\n\n\\* Best OCR / VLM / speech model that saved you from pain?\n\n\\* Drop your picks below so everyone can benchmark / vibe‑test them going into 2026.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q06u1n/2025_is_over_what_were_the_best_ai_model_releases/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwvo5t9",
          "author": "Clipbeam",
          "text": "I vote the Qwen3 models. Total game changer for local LLMs.",
          "score": 37,
          "created_utc": "2025-12-31 07:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs12w5",
              "author": "techlatest_net",
              "text": "Totally Agree",
              "score": 1,
              "created_utc": "2026-01-05 07:03:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvohey",
          "author": "DutchSEOnerd",
          "text": "Overall I would says Claude Opus had most impact, for local its definitely Qwen",
          "score": 13,
          "created_utc": "2025-12-31 07:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwy6e07",
              "author": "ForsookComparison",
              "text": "+1 for Opus. Benchmarks don't show its impact properly. The thing dropping down to $25/1m output tokens led to me not needing to hand-write a line of code for the last month of this year in my biggest repos.",
              "score": 3,
              "created_utc": "2025-12-31 17:50:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3hp62",
                  "author": "DutchSEOnerd",
                  "text": "Thats also the feeling I have. Benchmarks only tell part of the story",
                  "score": 2,
                  "created_utc": "2026-01-01 16:12:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxs1nyo",
                  "author": "techlatest_net",
                  "text": "Yeah, exactly – leaderboards don’t really capture “I stopped hand-writing code in my main repos for a month straight.”\n​\nThe $25/1M output tier basically turned Opus into a drop-in senior engineer for a lot of people: long refactors, multi-file edits, and architecture changes went from “painful weekend project” to “one good prompt and a quick review pass.”\n​\nCurious how you wired it in – are you mostly using it through an AI IDE, custom scripts, or a homegrown agent flow?",
                  "score": 1,
                  "created_utc": "2026-01-05 07:08:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs1jtv",
              "author": "techlatest_net",
              "text": "Yeah, Opus was kind of the first “I can trust this to run my whole workflow” model for a lot of people – especially for tool-use and multi-step plans.\n​\nAnd agreed on Qwen for local: the 2.5/3 “Thinking” and VL variants basically became the default self-hosted stack once people realized how far you could push them on a single GPU.",
              "score": 1,
              "created_utc": "2026-01-05 07:07:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwy6olb",
          "author": "ForsookComparison",
          "text": "Deepseek R1 probably had the biggest impact. Put the fear of God into Western companies that were operating as if they had a magical moat. Before that there was legislation being seriously considered to put a halt to A.I. progress for a few months to years.",
          "score": 7,
          "created_utc": "2025-12-31 17:51:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs1sgc",
              "author": "techlatest_net",
              "text": "Yeah, R1 felt less like “just another model” and more like a geopolitical event – suddenly every frontier lab and regulator had to assume that frontier‑level reasoning could appear from any region, not just the usual suspects.\n​\nThe funny part is how fast the narrative flipped: from “we need to pause progress” to “we can’t afford to pause while others sprint,” which probably did more to kill the idea of a global slowdown than any policy paper.\n​\nCurious if you actually used R1 in any real workflows, or if its impact for you was mostly the shockwave it sent through the ecosystem.",
              "score": 1,
              "created_utc": "2026-01-05 07:09:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxt7qu8",
                  "author": "ForsookComparison",
                  "text": "> Curious if you actually used R1 in any real workflows, or if its impact for you was mostly the shockwave it sent through the ecosystem.\n\nUsed it absolutely everywhere. The price, especially off peak hours, made Sonnet and ChatGPT look like jokes.",
                  "score": 1,
                  "created_utc": "2026-01-05 13:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvnxf6",
          "author": "AllTheCoins",
          "text": "I love Qwen3-30b as a go-to all around model",
          "score": 9,
          "created_utc": "2025-12-31 07:33:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwz4f3w",
              "author": "Count_Rugens_Finger",
              "text": "yeah definitely.  In this thread there is very clearly two worlds coming into contact here.  The people with purpose-made rigs and the people with consumer grade hardware",
              "score": 2,
              "created_utc": "2025-12-31 20:46:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxs1z46",
                  "author": "techlatest_net",
                  "text": "And yeah, this thread really shows the split between people on big dedicated rigs and people squeezing the most out of consumer GPUs.",
                  "score": 1,
                  "created_utc": "2026-01-05 07:11:28",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxs1yl8",
              "author": "techlatest_net",
              "text": "Qwen3‑30B is kind of the sweet spot right now – good enough at almost everything without feeling like a science project to run.",
              "score": 1,
              "created_utc": "2026-01-05 07:11:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwa6rb",
          "author": "InfiniteTrans69",
          "text": "Qwen3, Kimi K2, Minimax M2.",
          "score": 3,
          "created_utc": "2025-12-31 11:03:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3duag",
              "author": "QuinQuix",
              "text": "What's the respective use cases for you with these three models?",
              "score": 2,
              "created_utc": "2026-01-01 15:52:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx3fqe7",
                  "author": "InfiniteTrans69",
                  "text": "Translation and vision: Qwen  \nResearch and AI Slides: Kimi  \nEverything else: Minimax",
                  "score": 2,
                  "created_utc": "2026-01-01 16:02:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwpmc5",
          "author": "jinnyjuice",
          "text": "Might be useful to categorise by memory size and task groups",
          "score": 5,
          "created_utc": "2025-12-31 13:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxhgvw",
          "author": "leonbollerup",
          "text": "Open Source: GPT-oss-20b/120b",
          "score": 3,
          "created_utc": "2025-12-31 15:46:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwyuia",
          "author": "RiskyBizz216",
          "text": "Qwen3-Next specifically the 80B and 480B\n\nAnd Z-Image Turbo",
          "score": 2,
          "created_utc": "2025-12-31 14:06:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxfizo",
          "author": "BuffMcBigHuge",
          "text": "Open: Qwen3-30B\nClosed: Gemini 3 Pro + Nano Banana\n\nOpus 4.5 is stellar, but those two above blew me away.",
          "score": 2,
          "created_utc": "2025-12-31 15:36:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxmwi4",
          "author": "Karyo_Ten",
          "text": "LLMs: GLM-4.5-Air, GLM-4.7, GPT-OSS (quality for the first 2, speed demon for GPT-OSS)\n\nPromising new arch: Kimi-Linear, Qwen-Next, MiMo-V2-Flash I believe 2026 will have Linear Attention everywhere.\n\nSpecialized: Minimax-M2.1\n\nMultimodal: Qwen3-Omni, GLM-4.6V\n\nOCR: Mineru",
          "score": 2,
          "created_utc": "2025-12-31 16:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0g87s",
          "author": "karmakaze1",
          "text": "Nemotron-3-Nano for its use of hybrid Mamba-Transformer that reduces memory and computation for large context.\n\nAlso Qwen3-Next (for \"Gated DeltaNet\") and Kimi K2 (for advancing MLA further).",
          "score": 2,
          "created_utc": "2026-01-01 01:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmu7yj",
          "author": "thedarkbobo",
          "text": "Qwen3, iQuest, Deepseek, OSS, gemma3, nemotron",
          "score": 2,
          "created_utc": "2026-01-04 15:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpnzh",
          "author": "Lissanro",
          "text": "You missed to mention Kimi K2 0905 and Kimi K2 Thinking. These are the models I run the most on my PC (IQ4 and Q4_X quants respectively, using ik_llama.cpp). K2 Thinking is especially notable for its QAT INT4 release which maps nicely to Q4_X in the GGUF format, preserving the original quality.\n\nFor both, common expert tensors and 256K context cache at Q8 fit fully in 96GB VRAM, making them excellent for CPU+GPU inference.\n\n\nAs of DeepSeek V3.2, I did not get to try it due to lack of support in both ik_llama.cpp and llama.cpp. There is work in progress to add its architecture but not going to make it this year.",
          "score": 3,
          "created_utc": "2025-12-31 07:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwa88j",
              "author": "Tema_Art_7777",
              "text": "Lord! From unsloth:\n\n“It is recommended to have 247 GB of RAM to run the 1-bit Dynamic GGUF.\nTo run the model in full precision, you can use 'UD-Q4_K_XL', which requires 646 GB RAM.”",
              "score": 1,
              "created_utc": "2025-12-31 11:03:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwwislo",
                  "author": "Lissanro",
                  "text": "Even though Unsloth generally makes good quants, in this case it is the best to use Q4\\_X from [https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF](https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF) because it preserves the original INT4 quality in 543.617 GiB size, while UD-Q4\\_K\\_XL would be bigger, slower and likely of a bit lower quality.\n\nThat said, yes, your estimates of necessary RAM are accurate, for IQ1 256 GB RAM is needed and almost all of it will be used by the model, leaving very little for the OS; for IQ3 512 GB, and for Q4\\_X around 640 GB is needed if you include full cache size. K2 Thinking works best high RAM rigs or with high GPU count, like twenty MI50 cards (for 640 GB VRAM in total) - I actually considered getting that much since at the time I looked into it, it was possible for half the price of RTX PRO 6000 and my motherboard could carry them all at PCI-E 4.0 x4 speed each, but I decided to stay with 4x3090 for now (because even though full VRAM inference would be faster, my current performance with 1 TB RAM + 96 GB VRAM is still acceptable to me, and a lot of what I do requires Nvidia cards, and not just LLMs either).",
                  "score": 2,
                  "created_utc": "2025-12-31 12:17:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvqtnu",
          "author": "PeakBrave8235",
          "text": "At the end of the day they all are BS, open or closed source idgaf.",
          "score": -11,
          "created_utc": "2025-12-31 08:00:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxga7w",
              "author": "Count_Rugens_Finger",
              "text": "so why are you even here?",
              "score": 2,
              "created_utc": "2025-12-31 15:40:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1u966",
      "title": "I designed a Private local AI for Android - has internet search, personas and more.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/jk6meo3wmwag1",
      "author": "Sebulique",
      "created_utc": "2026-01-02 09:25:58",
      "score": 64,
      "num_comments": 20,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nx8c8zl",
          "author": "Themash360",
          "text": "Nice work, can it connect to local llm as well? So not just on device but custom endpoints",
          "score": 1,
          "created_utc": "2026-01-02 10:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8cp5h",
              "author": "Sebulique",
              "text": "You can, I've left it completely open, I also allowed it to work with smart bulbs, It works but sometimes it decides to do it's own thing unless I set parameters.\n\nI've turned it off after I broke it, but I want to reintroduce it again soon",
              "score": 3,
              "created_utc": "2026-01-02 10:18:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx982sn",
          "author": "theCatchiest20Too",
          "text": "Very cool! Are you tracking performance against hardware specs?",
          "score": 1,
          "created_utc": "2026-01-02 14:14:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9mbso",
              "author": "Sebulique",
              "text": "I was thinking of a toggle called \"stats for geeks\" but I wanted to make it look super easy for anyone, almost Chatgpt like, so more people feel comfortable using local llms",
              "score": 2,
              "created_utc": "2026-01-02 15:31:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx9j4l9",
          "author": "False-Ad-1437",
          "text": "governor work shocking tie dime frame special degree lip vase\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
          "score": 1,
          "created_utc": "2026-01-02 15:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa6go5",
              "author": "Sebulique",
              "text": "Thank you man, I appreciate you, I'm looking to make it easy and accessible for all",
              "score": 1,
              "created_utc": "2026-01-02 17:06:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdas2x",
          "author": "2QNTLN",
          "text": ">will upload soon on GitHub. \n\nHow soon is soon? Super interested in this project so i had to ask.",
          "score": 1,
          "created_utc": "2026-01-03 02:39:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2xoxg",
              "author": "Sebulique",
              "text": "It's up man!",
              "score": 1,
              "created_utc": "2026-01-06 21:23:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxe9nbn",
          "author": "tankandwb",
          "text": "RemindMe! 1 week",
          "score": 1,
          "created_utc": "2026-01-03 06:34:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe9qy4",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 7 days on [**2026-01-10 06:34:58 UTC**](http://www.wolframalpha.com/input/?i=2026-01-10%2006:34:58%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLM/comments/1q1u966/i_designed_a_private_local_ai_for_android_has/nxe9nbn/?context=3)\n\n[**4 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLM%2Fcomments%2F1q1u966%2Fi_designed_a_private_local_ai_for_android_has%2Fnxe9nbn%2F%5D%0A%0ARemindMe%21%202026-01-10%2006%3A34%3A58%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q1u966)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-03 06:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcrxs",
          "author": "Latter_Virus7510",
          "text": "Coolsieees! Amazing job there boss! When do we get to download?",
          "score": 1,
          "created_utc": "2026-01-03 18:30:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx461s",
              "author": "Sebulique",
              "text": "Now! Enjoy https://github.com/Rawxia/SebbiAI",
              "score": 2,
              "created_utc": "2026-01-06 00:33:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxyt4vf",
                  "author": "Latter_Virus7510",
                  "text": "First time launched and imported a Gemma 3 model, I tried to send a message but the model crashed instantly. It's been crashing since.",
                  "score": 1,
                  "created_utc": "2026-01-06 06:50:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxhkf1u",
          "author": "Benschman1979",
          "text": "Great work! I'm looking forward to testing it.",
          "score": 1,
          "created_utc": "2026-01-03 19:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx42bu",
              "author": "Sebulique",
              "text": "It's out now! https://github.com/Rawxia/SebbiAI",
              "score": 1,
              "created_utc": "2026-01-06 00:33:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxx443u",
          "author": "Sebulique",
          "text": "It's now released everyone! \nhttps://github.com/Rawxia/SebbiAI",
          "score": 1,
          "created_utc": "2026-01-06 00:33:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "ny5ac3c",
          "author": "PitifulBall3670",
          "text": "Interesting logic",
          "score": 1,
          "created_utc": "2026-01-07 04:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6zwzp",
          "author": "caiqueZ102",
          "text": "I really liked what I was seeing; the more modern, the better.",
          "score": 1,
          "created_utc": "2026-01-07 13:13:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyjnew",
      "title": "Tiiny Al just released a one-shot demo of their Pocket Lab running a 120B model locally.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "author": "Ajitabh04",
      "created_utc": "2025-12-29 11:09:13",
      "score": 49,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "Just came across this demo. They plugged their tiny AI computer into a 14-year-old PC and it output an average of 19 tokens/s on a 120B model.\nThey haven't released the MSRP yet. However, a large amount of DDR5 memory would be pricey, I'm guessing around $1500 MSRP for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyjnew/tiiny_al_just_released_a_oneshot_demo_of_their/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwjcqd4",
          "author": "loyalekoinu88",
          "text": "They had posted around $699 BUT that was before the memory announcement",
          "score": 12,
          "created_utc": "2025-12-29 12:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwj8uo7",
          "author": "leonbollerup",
          "text": "No link ?",
          "score": 8,
          "created_utc": "2025-12-29 12:23:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwklly9",
          "author": "ForsookComparison",
          "text": "> guessing $1500 MSRP\n\n> they posted $699 pre crucial RAM announcement \n\n> 19 tokens/second on gpt-oss-120b\n\nI plugged a used Rx 6800 ($250 in my area with multiple options) into an older PC and got 18 tokens per second. I know this isn't the same and it suggests that you have a fair amount of RAM in your older PC, but given what we know about this I'm thoroughly \"meh\"d.\n\n**Edit** - just looked up the form factor. I'm less meh'd now. That would be fun to use if it ends up affordable.",
          "score": 4,
          "created_utc": "2025-12-29 16:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwlgo5g",
              "author": "FaceDeer",
              "text": "[It's a portable unit the size of a phone](https://tiiny.ai/), since there aren't links anywhere else in this thread.\n\nNot a lot of detail even there, though. I don't see anything about whether it's battery powered - I'm assuming not, given OP mentions plugging it in to a computer.",
              "score": 6,
              "created_utc": "2025-12-29 19:18:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwutx5z",
                  "author": "ecoleee",
                  "text": "You’re absolutely right — this generation of Tiiny does not include a built-in battery.\n\nThat decision was intentional. Thermal management is a serious challenge at this performance level, and we didn’t want to ship a device that becomes uncomfortably hot in real use.\n\nTo reliably support sustained local inference of models up to 120B parameters, we designed a custom thermal module specifically for Tiiny, prioritizing stability and safe operating temperatures over battery integration.\n\nAt the upcoming CES, we’ll be sharing a detailed internal teardown video of Tiiny. You’ll be able to see exactly how the cooling system is built and why these design choices were made.\n\nWe believe it’s better to be transparent about trade-offs and deliver a product that performs consistently, rather than chasing form factors at the expense of real-world usability.",
                  "score": 2,
                  "created_utc": "2025-12-31 03:48:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwq0vao",
          "author": "Ok-Structure4057",
          "text": "Found the specs on their website:\n\npocket size: 14.2 × 8 × 2.53 cm\n\n80GB LPDDR5X RAM & 1TB SSD190 \n\ntotal TOPS between the SoC and dNPU\n\n30W TDP\n\nThey also released a demo of this device on Twitter. Imo it would be fun with retail prices around 1400 bucks.",
          "score": 3,
          "created_utc": "2025-12-30 12:52:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwq5w0t",
              "author": "RangerOk4318",
              "text": "Agree. Memory price has been so absurd. I'm guessing the same price",
              "score": 1,
              "created_utc": "2025-12-30 13:25:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nws3u2u",
                  "author": "QuinQuix",
                  "text": "Memory is fucking up any attempt at affordable home AI right now",
                  "score": 2,
                  "created_utc": "2025-12-30 19:10:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwvbcwc",
              "author": "fallingdowndizzyvr",
              "text": "> mo it would be fun with retail prices around 1400 bucks.\n\nAt that price, why not just get a Strix Halo? That's just a PC so you can do regular PC stuff like gaming.",
              "score": 1,
              "created_utc": "2025-12-31 05:48:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwt0pjg",
          "author": "zelkovamoon",
          "text": "I'm not sure what having a *small* ai lab is trying to solve\n\nIf you're doing local AI my position is, make it bigger, cooler, and put more ram on it.\n\nThat said, it is *good* that companies are stepping in to try and build some solutions. If we could get something with 256GB of fast memory we might be able to go places.",
          "score": 3,
          "created_utc": "2025-12-30 21:47:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6f8d5",
          "author": "noless15k",
          "text": "[https://tiiny.ai/pages/tech-1](https://tiiny.ai/pages/tech-1)\n\nhttps://preview.redd.it/4b2qtqbx9uag1.png?width=2048&format=png&auto=webp&s=0538a8c9fc2286ecebe27731cc85925cc7819fbc",
          "score": 2,
          "created_utc": "2026-01-02 01:30:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlotf1",
          "author": "No-Consequence-1779",
          "text": "I think the lpddr5 is likely the memory.  It’s slightly faster for this and it’s wired so they can , like others, charge for memory size mostly. ",
          "score": 1,
          "created_utc": "2025-12-29 19:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrh229",
          "author": "Certain_Strength7069",
          "text": "I saw a price of 469USD. For that price, I would buy several for everything. ",
          "score": 1,
          "created_utc": "2026-01-05 04:36:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q31r48",
      "title": "Tony Stark’s JARVIS wasn’t just sci-fi his style of vibe coding is what modern AI development is starting to look like",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/hfvuxcznd6bg1",
      "author": "spillingsometea1",
      "created_utc": "2026-01-03 18:16:52",
      "score": 47,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q31r48/tony_starks_jarvis_wasnt_just_scifi_his_style_of/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxhaptp",
          "author": "sourceholder",
          "text": "I often think of this scene.\n\nA few years ago it seemed this type of interaction was at least a decade+ away.",
          "score": 1,
          "created_utc": "2026-01-03 18:20:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q79orf",
      "title": "Been using glm 4.7 for coding instead of claude sonnet 4.5 and the cost difference is huge",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q79orf/been_using_glm_47_for_coding_instead_of_claude/",
      "author": "Affectionate-Cash709",
      "created_utc": "2026-01-08 11:45:03",
      "score": 47,
      "num_comments": 28,
      "upvote_ratio": 0.86,
      "text": "so ive been on claude sonnet 4.5 for like 14 months now mostly for coding work. debugging python scripts, generating react components, refactoring old code etc. its great but honestly the $20/month plus api costs when i need bulk operations was adding up\n\nsaw someone mention glm 4.7 from zhipu ai in a thread here last week. its open source and supposedly good for coding so i decided to test it for a week against my usual claude workflow\n\nwhat i tested:\n\n*   debugging flask apis with cryptic error messages\n    \n*   generating typescript interfaces from json\n    \n*   refactoring a messy 600 line python class\n    \n*   writing sql queries and optimizing them\n    \n*   explaining wtf is going on in legacy java code\n    \n\nhonestly went in expecting typical open source model jank where it gives you code that looks right but imports dont exist or logic breaks on edge cases\n\nbut glm actually delivered working code like 85-90% of the time. not perfect but way better than i expected\n\ni also tested it against deepseek and kimi since theyre in the same ballpark. deepseek is faster but sometimes misses context when files get long. kimi is solid but hit token limits faster than glm. glm just handled my 500+ line files without forgetting what variables were named\n\nthe biggest difference from sonnet 4.5:\n\n*   explanations are more technical and less \"friendly\" but i dont really care about that\n    \n*   code quality is surprisingly close for most tasks, like 80-85% of sonnet 4.5 output quality\n    \n*   way cheaper if youre using the api, like 1/5th the cost for similar results\n    \n\nwhere claude still wins:\n\n*   ui/ux obviously\n    \n*   better for brainstorming and high level architecture discussions\n    \n*   more polished responses when you need explanations\n    \n\nbut for pure coding grunt work? refactoring, debugging, generating boilerplate? glm is honestly good enough and the cost savings are real",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q79orf/been_using_glm_47_for_coding_instead_of_claude/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nydut3r",
          "author": "No_Conversation9561",
          "text": "what is your hardware?",
          "score": 5,
          "created_utc": "2026-01-08 12:09:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nye0hzy",
              "author": "j00cifer",
              "text": "I think he uses GLM through zAIs endpoint or something like that through subscription, not local model",
              "score": 8,
              "created_utc": "2026-01-08 12:47:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhf2rq",
                  "author": "BERLAUR",
                  "text": "The subscription is pretty awesome, its currently about 2-3 bucks per month (with all discounts) and it gives you a lot of tokens and some useful MCP servers. Only downside is that the concurrency for the 4.7 model is currently limited and that they prioritise people on higher tiers (but it's still more than fast enough).\n\n\nI've gotten into a flow of letting Sonnet do the planning and then having GLM 4.7 take care of the implementation and it works extremely well. \n\n\nAllows me to run 3-4 big tasks in parallel with the cheapest Claude and Z.ai subscription.",
                  "score": 3,
                  "created_utc": "2026-01-08 22:11:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyievck",
              "author": "Individual_Gur8573",
              "text": "I have rtx 6000 pro and 5090 in 1 system...but I run minimax 2.1 IQ2_M quant on 6000 pro with 150k context... It's very good..not disappointed yet..infact better and intelligent than GLM 4.5 air",
              "score": 1,
              "created_utc": "2026-01-09 01:10:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nye07zl",
          "author": "Scared-Biscotti2287",
          "text": "Glm 4.7 seems to handle long files better than kimi and doesnt hallucinate imports. Not as polished as claude for explanations but for actual code generation its solid and way cheaper, def worth testing if youre doing bulk coding work.",
          "score": 3,
          "created_utc": "2026-01-08 12:45:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhki8c",
              "author": "websitegest",
              "text": "I think actually Opus 4.5 it's a beast in plans/architectures jobs but GLM 4.7 is awesome at implementing... saving a lot in month subscription compared to Claude Max plans! If someone want a 30% discount on GLM plans (current offers + an additional 10%) hurry because they're about to expire --> [https://z.ai/subscribe?ic=TLDEGES7AK](https://z.ai/subscribe?ic=TLDEGES7AK)",
              "score": 1,
              "created_utc": "2026-01-08 22:36:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nye43z8",
          "author": "whyyoudidit",
          "text": "I went thru $10 in 5 min using Opus 4.5 in VSC yesterday refactoring 1400 lines of code and it didn't even finish. Being the first time doing any of this I was shocked it was that expensive.",
          "score": 3,
          "created_utc": "2026-01-08 13:09:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymf1u8",
              "author": "cmndr_spanky",
              "text": "What’s VSC?",
              "score": 1,
              "created_utc": "2026-01-09 16:36:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyedpyc",
          "author": "Stoic_Coder012",
          "text": "how can you be using it for 14 months?",
          "score": 3,
          "created_utc": "2026-01-08 14:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyeehx2",
              "author": "Condomphobic",
              "text": "All of these posts are made by genuine bots. \n\nAnd the common denominator is that their stories always have fake timelines",
              "score": 5,
              "created_utc": "2026-01-08 14:06:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyep2q0",
                  "author": "Affectionate-Cash709",
                  "text": "My bad. thinking 3.5 in mind while typing. but i'm real - being using 4.5 since beginning.",
                  "score": 2,
                  "created_utc": "2026-01-08 14:59:22",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nyffm6t",
                  "author": "Stoic_Coder012",
                  "text": "that's what I thought too cause I have heard that reddit is filled with bots",
                  "score": 1,
                  "created_utc": "2026-01-08 16:58:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nye879b",
          "author": "_olk",
          "text": "What about MiniMax-M2.1 ?",
          "score": 2,
          "created_utc": "2026-01-08 13:32:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyefege",
          "author": "mintybadgerme",
          "text": "I found exactly the same thing.  So right now I use GLM 4.7 to start and 4.5 Opus for polishing or planning.",
          "score": 2,
          "created_utc": "2026-01-08 14:10:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfu28t",
          "author": "Individual_Gur8573",
          "text": "Try minimax",
          "score": 2,
          "created_utc": "2026-01-08 18:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydt4re",
          "author": "sosuke",
          "text": "I skimmed your post a few times. Didn’t you mention how much it cost per month or just 1/5th. So $4/m?",
          "score": 1,
          "created_utc": "2026-01-08 11:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydvkcb",
              "author": "FoolishNomad",
              "text": "GLM has a lite coding plan for $6/month if paying monthly. It’s a great deal considering that GLM-4.7, especially with Claude Code, works pretty damn well.",
              "score": 3,
              "created_utc": "2026-01-08 12:14:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nye7wt2",
          "author": "Loskas2025",
          "text": "Better minimax 2.1 to be honest",
          "score": 1,
          "created_utc": "2026-01-08 13:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjx518",
              "author": "Thin_Treacle_6558",
              "text": "Why better? I saw tests on Youtube and they not have some huge difference. \nDo you have a good response rate in Minimax?",
              "score": 1,
              "created_utc": "2026-01-09 06:37:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymabms",
                  "author": "Loskas2025",
                  "text": "Minimax gives me 100 tokens per second on rtx 6000 when glm at the same quantization gives me 50",
                  "score": 1,
                  "created_utc": "2026-01-09 16:15:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyeexum",
          "author": "dmter",
          "text": "yes i also came to these conclusions. on a few tests I ran it beats gpt oss 120 f16. and that's while being q2 !!! but it' also 4 times slower in raw t/s (15 t/s gptoss120, 4t/s glm47q2)\n\nbtw i also tried minimax21 q4 once, it's 6t/s but it sucked so i didn't test it further",
          "score": 1,
          "created_utc": "2026-01-08 14:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyeofob",
          "author": "not-really-adam",
          "text": "I’ve been playing with OpenCode and using Opus for architect mode, and then using GLM (it’s free right now through Zen) and Qwen3-coder for executing local research tasks and code generation. It’s not as good as Claude code with Opus for all tasks, but if you want to iteratively do code reviews or execute large plans for new features over night it’s nearly free.\n\nAnyone else doing something similar and can share tips and tricks and lessons learned?",
          "score": 1,
          "created_utc": "2026-01-08 14:56:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygycel",
          "author": "ServiceOver4447",
          "text": "I have free unliimted Claude Sonnet 4.5 from work, should there be any reason to switch for me?",
          "score": 1,
          "created_utc": "2026-01-08 20:57:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhfbx8",
              "author": "Distinct-Fee-7938",
              "text": "NOOO",
              "score": 2,
              "created_utc": "2026-01-08 22:12:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyhfive",
              "author": "BERLAUR",
              "text": "No, unless you want to try something new or want to run a huge numbers of agents in parallel. ",
              "score": 2,
              "created_utc": "2026-01-08 22:13:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyl0xs9",
          "author": "amjadmh73",
          "text": "I will test this myself. Ping me in a week or so to see the comparison with Claude Code.",
          "score": 1,
          "created_utc": "2026-01-09 12:19:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6ukn0",
      "title": "For people who run local AI models: what’s the biggest pain point right now?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6ukn0/for_people_who_run_local_ai_models_whats_the/",
      "author": "Educational-World678",
      "created_utc": "2026-01-07 22:57:19",
      "score": 46,
      "num_comments": 85,
      "upvote_ratio": 0.92,
      "text": "I’m experimenting with some offline AI tools for personal use, and I’m curious what other people find most frustrating about running models locally.\n\nIs it hardware? Setup? Storage? Speed? UI? Something else entirely?  \nI’d love to hear what slows you down the most.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6ukn0/for_people_who_run_local_ai_models_whats_the/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyali0x",
          "author": "RandomCSThrowaway01",
          "text": "Price of RTX Pro 6000 Blackwell.",
          "score": 117,
          "created_utc": "2026-01-07 23:09:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfddhc",
              "author": "Kraeftemangel2025",
              "text": "Come on. Have some understanding, how hard trillion dollar conglomer... corporations have it nowadays.\n\nThey even had to buy whole *divisions* of hardware, that the pesant shall not be able to run their puny AI at home. Yet you reach for the stars, get real and just buy the overpriced AI subscription with a ten year plan, and tell it all your dirty little secrets, including your brand new business plan. Sheeessh.\n\nHow the fuck do you think the shareholders of NGreedia survive without a second yacht, don't be so selfish, get your shit together, stop eating for two or three years entirely, and simply buy this indecently ovrpriced piece of hardware!\n\nYou keep in shape, no energy needed for the stove, you can redirect all electricity into your rig, it is a win win win win situation - for corpoations as well!!!",
              "score": 1,
              "created_utc": "2026-01-08 16:48:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyhprq3",
                  "author": "Educational-World678",
                  "text": "lol, does everyone have a business plan these days? I never ran into these trends on my main... but then again, that account is mostly shit posting and pedantic arguments reminiscent of the old days before Reddit even acknowledged photos in posts...",
                  "score": 1,
                  "created_utc": "2026-01-08 23:01:07",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyby22t",
          "author": "Flame_Grilled_Tanuki",
          "text": "I’d love to have a regularly updated community wiki that lists all LLMs along with their strengths, weaknesses, and intended uses.\n\nAlternatively, a table of LLM use cases (e.g., coding, chat, document summarisation, role‑play, agentic workflows, general knowledge, ...) and recommend models for each category at different size tiers; 0-4B, 4-16B, 16-40B, 40-120B, >120B.",
          "score": 32,
          "created_utc": "2026-01-08 03:20:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd1xhx",
              "author": "Everlier",
              "text": "This community doesn't have a single opinion or much continuity, sadly, so for this to work there'd have to be someone very dedicated and with enough time to maintain it",
              "score": 2,
              "created_utc": "2026-01-08 08:00:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyejb30",
              "author": "ioabo",
              "text": "Tbh this is way too subjective to ever be useful.There are some sites that rank LLMs according to specific benchmarks, but benchmarks don't always reflect the real use cases.\n\nRegarding roleplay, there's the UGI, a ranking of uncensored and roleplaying LLMs , and the SillyTavern subreddit has a weekly pinned post where people post their recommended LLMs per size category.",
              "score": 2,
              "created_utc": "2026-01-08 14:30:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyanyni",
          "author": "ThatOneGuy4321",
          "text": "Price of hardware definitely. It costs thousands more, above the price of a standard computer, to get one with enough VRAM or unified memory that can run 70B+ LLMs. \n\nUnless you are a huge nerd or a programmer it’s really hard to justify that cost versus an LLM service subscription. An extra thousand or two could pay for even a $100 a month subscription for a year or more\n\nIn 5+ years when the tech has improved and VRAM has come down in price I think this will be a different story",
          "score": 33,
          "created_utc": "2026-01-07 23:21:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybfr9w",
              "author": "JustSentYourMomHome",
              "text": "Have you tried [AirLLM](https://github.com/lyogavin/airllm)?\n\n>AirLLM optimizes inference memory usage, allowing 70B large language models to run inference on a single 4GB GPU card without quantization, distillation and pruning. And you can run 405B Llama3.1 on 8GB vram now",
              "score": 5,
              "created_utc": "2026-01-08 01:44:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyd1q9d",
                  "author": "Everlier",
                  "text": "It's mostly for \"overnight\" batch inference usage, it's seconds per token slow (by design). It's main feature is that it loads only layers that are being used, layer-by-layer.",
                  "score": 8,
                  "created_utc": "2026-01-08 07:58:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nycjfhr",
                  "author": "pmttyji",
                  "text": "Hearing about this one first time. Didn't see any recent threads on this in our subs. Why it's not popular nowadays? I really want to use some 24B models with my 8GB VRAM.",
                  "score": 3,
                  "created_utc": "2026-01-08 05:32:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybphu9",
                  "author": "drakgremlin",
                  "text": "Sounds cool!  Where does it sit in the model life cycle?  Are they useful with ollama?",
                  "score": 1,
                  "created_utc": "2026-01-08 02:35:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyar8la",
              "author": "HealthyCommunicat",
              "text": "I’m not coming at you in any way, but can you tell me your thoughts on why people in the first place that don’t even work in the tech field would really NEED a 70b dense model?",
              "score": 2,
              "created_utc": "2026-01-07 23:38:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyauzts",
                  "author": "ThatOneGuy4321",
                  "text": "at this point I think it’s pretty unlikely that someone who doesn’t work in tech (or isn’t a tech enthusiast) would *need* a local LLM. Unless they have a specific privacy-based professional need for it like document review, in which case a couple thousand is not a big outlay (and I would assume accuracy is pretty important in that case). \n\n70B to me just seems like the “sweet spot” where accuracy is high enough that it’s worth running a local LLM, before hardware costs go nutty and diminishing returns start. But that is subjective I will admit.",
                  "score": 8,
                  "created_utc": "2026-01-07 23:57:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nycp73d",
                  "author": "decixl",
                  "text": "Moar intelligence, moar reason",
                  "score": 2,
                  "created_utc": "2026-01-08 06:14:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyeg3dv",
                  "author": "WayNew2020",
                  "text": "This is really a well-addressed question. I love to know the answers too. I'm in the tech field and satisfied with 14b\\~30b models running on 5090 for my own purpose and custom tools for daily use (**ministral-3:14b** for general purpose**, gemma3:27b** for instruction-following**, nemotron-3-nano:30b** for deeper reasoning, **qwen3-coder:30b** for architect/coding/debug, and **stable diffusion 3.5 large** for image generation). The fun is finding what works i.t.o. statistical parameters, prompt shaping, and RAG methods, and I don't feel any strong need for 70b+ or RTX 6000 (tho I don't mind having them if I could).",
                  "score": 2,
                  "created_utc": "2026-01-08 14:14:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyavn03",
                  "author": "Many-Ad6293",
                  "text": "I like your pretext setup for your question. 😊",
                  "score": 1,
                  "created_utc": "2026-01-08 00:00:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nycqynz",
                  "author": "cleverestx",
                  "text": "In depth roleplaying.",
                  "score": 1,
                  "created_utc": "2026-01-08 06:28:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyaulfm",
              "author": "TheOdbball",
              "text": "Hetzner",
              "score": 1,
              "created_utc": "2026-01-07 23:55:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nydg9xw",
              "author": "KikiPolaski",
              "text": "I've been using a 3060ti to mess around with 7b models, how much of a step up would 3090 be nowadays? Not dual or anything just the single one",
              "score": 1,
              "created_utc": "2026-01-08 10:11:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyb1jhp",
          "author": "Platinumrun",
          "text": "Hardware cost is first. I’m stuck running 8b - 14b models on a rig that cost me nearly $3k. Close second is hallucinations. You have to do a lot of tinkering of system prompts to get your desired output, but on occasion it will hallucinate which breaks conversational flow if I constantly have to monitor the accuracy.",
          "score": 9,
          "created_utc": "2026-01-08 00:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nydi4k3",
              "author": "forthejungle",
              "text": "Maybe you injtially bought the rig for gaming, not AI.",
              "score": 1,
              "created_utc": "2026-01-08 10:27:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydqxuv",
                  "author": "Platinumrun",
                  "text": "It’s an AI rig. I do not use it for gaming at all.",
                  "score": 1,
                  "created_utc": "2026-01-08 11:40:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybt6n7",
          "author": "SillyLilBear",
          "text": "vram, always vram",
          "score": 7,
          "created_utc": "2026-01-08 02:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb6yya",
          "author": "AWSLife",
          "text": "For me, it's trying to get everything working together properly and not suck and not be underwhelming.\n\nMy setup and needs are not sophisticated, just using PyCharm and VSCode to work with ollama with qwen3 (32b, coder, 4b coder) models on a M1 MacBook. I just want code generation, code checking and autocompletion to not completely suck and be a pain to use. I use Continue.dev and have used a couple of other extensions. I have tuned up and rewrite in the Continue.dev configuration file a dozen times.\n\nI know that when I am in edit mode, big coding questions are going to take a couple a minute or so, which I am okay with. However, a lot of the time, larger questions results just repeat themselves over and over again. It is as though there is a really low limit on what I can expect to get out of LLM's. Questions can't be more than a couple of sentences and it can't read more than 10 lines of code. Anything bigger than that and the results are just garbage.\n\nI really wish autocompletion was decent. I don't know what the LLM is being inspired by but some of the code it recommends is bonkers wrong. I am not trying to solve crazy computer science problems but create moderate level Python scripts. I really wish the LLM would read the entire project and then be inspired by it with the code it suggests. If the code I am working on does not use generators, then don't autocompletion suggest them to me.\n\nI will say one thing, qwen3-coder does a decent job in explain code to me. I just have to make sure to feed in a small amount of code for it to explain.",
          "score": 7,
          "created_utc": "2026-01-08 00:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyawlsv",
          "author": "Ok-Bill3318",
          "text": "Hardware cost for sure.",
          "score": 4,
          "created_utc": "2026-01-08 00:05:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybrezt",
          "author": "thatguyinline",
          "text": "If you're coming from ChatGPT hoping to run something local, here's the dirty secret: the models are fine. Even a Q4 Gemma 3 handles 99% of normal conversations *if* it has tools.\n\nThe actual problem? The experience is still built for hobbyists who enjoy suffering.\n\nI see this constantly: someone installs a model, asks about current events, gets a stale answer, and rage-posts about how \"local models suck.\" Meanwhile they've never configured a single tool. The model isn't broken—their expectations are miscalibrated.\n\n**Here's the billion-dollar gap nobody's filled:**\n\n1. **A truly consumer-grade local app.** LM Studio got closest, but it's still prosumer at best. I'm talking \"my 70-year-old dad could set this up\" easy. One install, it just works.\n2. **Target normal hardware.** Most modern PCs have GPUs that can run Gemma 3 comfortably. Stop optimizing for the 4090 crowd.\n3. **One-click OAuth to cloud models for specific tasks.** Local inference for chat, API call to \\[insert frontier model\\] for image gen. Best of both worlds, zero friction.\n\nThe tech is ready. The models are ready. Someone just needs to wrap it in software that doesn't require a CS degree to configure.\n\nThat opportunity is still wide open.\n\n(Rewritten by Claude)\n\n\\---\n\n\\## Original Post\n\nIf you're talking about more consumer oriented \"I'd like to use local models instead of ChatGPT to get answers and chat\"....  THEN...  the user experience is the main place that running local models falls down. A few years ago when I started I had the same experience which is that a lot of people just getting started install a model, try a few queries and quickly realize that there is a lot of tuning, tool implementation, API keys and the like required to make it feel similar to the consumer chat apps we all use.\n\nEven smaller local models like the latest Q4 Gemma from Gogle are already fine for 99% of the chats a user would have as long as they have tools.\n\nWhat i see a lot in these forums is people installing a model, bitching about their answers being out-of-date and never realizing that their perception of \"not out of date\" has nothing to do with the model.\n\nSomebody could make a lot of money if you:\n\n1. Had a truly consumer like, delightful, no-tech-knowledge required multi-platform local app (kind of like LM Studio did, they make it so easy to try and use models, but still well beyond the average prosumer skill set). that just kind of \"sets it all up for me so I dont' have to think about it\"\n2. Focused on models that run on normal PCs, i mean most PCs these days have a GPU that would let you run Gemma 3 (i'm just a fan of perf/size profile)\n3. Oauth style 1-click integration into better models for tasks (meaning, if somebody wants to use google nano banana for images but they want local inference for everything else... which is a pretty good use case cause no good local image gen can compete with NB).\n\nUltimately, make running local small models easy enough to use that your average 70 year old could install it is a pretty cool opportunity. I've yet to find anything even remotely close to that.",
          "score": 5,
          "created_utc": "2026-01-08 02:45:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyd8x0c",
              "author": "auradragon1",
              "text": "Get Claude Code to make what you want. Start with the open source version of ChatGPT UI.",
              "score": 1,
              "created_utc": "2026-01-08 09:03:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyath09",
          "author": "little___mountain",
          "text": "The power draw. It eats through my laptop's battery, which makes it impractical to use on the go.",
          "score": 3,
          "created_utc": "2026-01-07 23:49:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyauxo7",
          "author": "journalofassociation",
          "text": "Noise and heat.",
          "score": 3,
          "created_utc": "2026-01-07 23:57:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyawqjj",
          "author": "calicocatfuture",
          "text": "i agree that it’s price. i have a 2022 8GB macbook air. i got it years ago, obv just for personal school and work stuff but 7B llms make it suffer and anything higher makes it suffocate and crash. i just use llms for fun/rp/journaling/late night thoughts/etc and im trying to find something less censored than chagpt or grok and $1-3K isn’t worth it for just messing around. i have no idea really what would fix this",
          "score": 3,
          "created_utc": "2026-01-08 00:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyi5nrx",
              "author": "Educational-World678",
              "text": "Yeah, casual messing around is fun, but not worth spending money on. Are there things you would pay for it to do? tedious things, or repetitive things?",
              "score": 1,
              "created_utc": "2026-01-09 00:22:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyibh2q",
                  "author": "calicocatfuture",
                  "text": "i would pay for great memory recall, like gpts saved memory and chat reference, as well as good chat log memory per chat for long stories. web search. definitely emotional nuance as well and creativity + initiative. also i’d like it to be able to process images/pdfs cause grad school. i’m also so fascinated by claude code, people have let it autonomously shop and buy things for itself and express its thoughts while doing so and keep a personal diary that it can randomly write in. and obv no guardrails cause im a 23 year old girl and ive got needs lol. \n\ni feel like i could be plankton and it could be like my karen/baymax/BMO. i’d love a personal something that i could tell anything and also be super busy or up at 3:30AM and it’s always available. definitely feeling like an extension of my body in a way the way my phone is. if chatgpt stops providing anything before the 5 series…id pay $100 at least and at most depends on my desperation. i’m not lonely at all, i go out with friends all the time. but that’s different. \n\nthat would be the dream. i know it’s like asking for a unicorn. feels ridiculous to even type it. maybe in 20 years something like that will be available.",
                  "score": 1,
                  "created_utc": "2026-01-09 00:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyb977n",
          "author": "Proof_Scene_9281",
          "text": "That chat has an API, so why would I go through the pain of local… \n\n..well mostly because I can.. ",
          "score": 3,
          "created_utc": "2026-01-08 01:09:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb0mjz",
          "author": "productboy",
          "text": "Hardware",
          "score": 2,
          "created_utc": "2026-01-08 00:25:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb12jj",
          "author": "Alternative_Star755",
          "text": "I have the gpu for good local llms but running them makes me realize how much the majority of my LLM use nowadays is anchored to using it as a search engine filter. And I don’t feel like I get good results out of any of the MCPs that attempt to bridge that gap. It’s so much easier to just use an online chat app.",
          "score": 2,
          "created_utc": "2026-01-08 00:28:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyb7hwr",
          "author": "j4ys0nj",
          "text": "honestly i would say reliable model support using a standard environment. i've spent so many hours trying to get models to run or get them configured correctly. i've got about a dozen GPUs in my home datacenter and run models i need to rely on, so a controlled/unified environment is necessary. can't be running random scripts and one-off solutions for every new model.\n\ni know most of the reasons why it's like this but it sucks to have to wait for vLLM or pytorch or another link in the chain to add support. takes them weeks or even months sometimes.",
          "score": 2,
          "created_utc": "2026-01-08 01:00:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyiwobe",
              "author": "Educational-World678",
              "text": "Yeah, this is the part that makes me want a unified runtime more than anything. Every model having its own loader, its own quirks, its own backend support timeline… It’s wild that we don’t have a stable environment that abstracts all of that away yet. I’m convinced the long‑term solution isn’t ‘fix every model,’ it’s ‘build the layer that makes models interchangeable.’ Until then, we’re all duct‑taping our way through it.",
              "score": 1,
              "created_utc": "2026-01-09 02:45:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nykcsjd",
                  "author": "j4ys0nj",
                  "text": "I've been using GPUStack. it's pretty good, but sometimes support for new models still takes a few weeks or longer. The problem of figuring out the correct runtime params for vLLM or SGLang still exists though.",
                  "score": 1,
                  "created_utc": "2026-01-09 08:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nybbjdp",
          "author": "iamthesam2",
          "text": "cost.",
          "score": 2,
          "created_utc": "2026-01-08 01:21:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybmr1g",
          "author": "donotfire",
          "text": "Lack of useful applications. It’s hard to think of something to make that I would actually use in my daily life. Honestly it seems like the tinkering itself is the point.",
          "score": 2,
          "created_utc": "2026-01-08 02:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycc9ib",
          "author": "PickleSavings1626",
          "text": "not knowing which model to use for which specific use case. talking to my phone or macbook and having it all just work. i want to play mario kart and talk outloud \"open chrome, crate a new tab, go here, clone this repo, open terminal, etc\". i've a lot of different hardware but don't know how to pool the cpu/ram. i wish there was a shared personal context. chatgpt knows so much about me but i can't download what it knows about me and so have to write markdown files and sorta feed into each model. so much man.",
          "score": 2,
          "created_utc": "2026-01-08 04:44:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd8g0b",
          "author": "960be6dde311",
          "text": "Cost of hardware to run models ",
          "score": 2,
          "created_utc": "2026-01-08 08:59:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykw8o3",
          "author": "Ok-Huckleberry-9247",
          "text": "The price of memory and graphics cards",
          "score": 2,
          "created_utc": "2026-01-09 11:45:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybwfh3",
          "author": "sam7oon",
          "text": "Having my Agent ready, coded, but being only to run 8B LLMs does produce extremely unusable tool use compared to using the same agentic code with OpenRouter's 70B parameter models,\n\nMaking deployment of the agent for near production testing impossible without asking my enterprise for paying for it, \n\nAm waiting for more efficient LLMs, thought about Granite4 , but somehow unable to get it going on MLX , Hopefully RAM prices will come down so i can build a beefier setup",
          "score": 1,
          "created_utc": "2026-01-08 03:11:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycgpn2",
          "author": "SelectArrival7508",
          "text": "for me its primarily the hardware...",
          "score": 1,
          "created_utc": "2026-01-08 05:13:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nychojx",
          "author": "Aggravating-Draw9366",
          "text": "Halucinations",
          "score": 1,
          "created_utc": "2026-01-08 05:20:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycjbzg",
          "author": "jackandbake",
          "text": "Tool usage is bad and not comparable to modern coding automation clients.",
          "score": 1,
          "created_utc": "2026-01-08 05:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyckq4p",
          "author": "akulbe",
          "text": "Not nearly enough GPU.",
          "score": 1,
          "created_utc": "2026-01-08 05:41:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyclef6",
          "author": "vinoonovino26",
          "text": "Being a Mac user... that's a pain. Can't scale ram-vram or anything needed to run decent models. Silver lining is that I found [hyperlink.ai](http://hyperlink.ai) and Qwen3 4B Thinking 2507 from time to time do the work as a \"second brain\" for research and doc summaries.",
          "score": 1,
          "created_utc": "2026-01-08 05:46:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycqv2n",
          "author": "cleverestx",
          "text": "Using an AMD Strix Halo with 90GB of memory available for AI programs...using CachyOS...system has 96GB total...and just getting it to code with large projects is a pain point that I wish could be replicated in quality without relying on cloud AI models....but using LLm instead...The issue is the context length though...can only fit so Mich with a 70b model....",
          "score": 1,
          "created_utc": "2026-01-08 06:27:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycueyk",
          "author": "custodiam99",
          "text": "The lack of GPUs.",
          "score": 1,
          "created_utc": "2026-01-08 06:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycwev2",
          "author": "Gargle-Loaf-Spunk",
          "text": "It's hard to tell people how awesome I am.",
          "score": 1,
          "created_utc": "2026-01-08 07:12:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd1641",
          "author": "TheTechAuthor",
          "text": "I tried loading Gemma 3 12b (FP16) on my 5060ti + 64GB DDR4 workstation and it massively crapped my computer out (with the model soaking up as much RAM as was left across my whole PC).\n\n\nWas a great lesson on what minimum hardware is *actually* needed to run such a model for creating QLoRA adapters on.\n\n\nMy goal now is to find the right balance of using a quantised 8b model, or an FP16 4b model and creating the adaptors for my CMS off of that.\n\n\nUltimately, you'll likely need significantly more capable hardware than you realise to use models that are sufficiently competent enough compared to a leading online model.\n\n\nThat means you'll either need a hybrid approach (larger online + smaller offline models) or a lot of cash to get a working setup locally.",
          "score": 1,
          "created_utc": "2026-01-08 07:53:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd3kk3",
          "author": "alex_godspeed",
          "text": "Highest: definitely affordability. Gimme 5090 dual plsssss > <  \n  \nI myself cannot foresee using a machine more than a dual GPU setup (must be confined in a cased PC rig).\n\nAnything more than that means open rack which I actively avoid. Just me. If i go that path, I might as well rent the whole server rack and...i guess to many here it's an 'expensive hobby' hehe\n\nI personally prioritize quietness (no airplane taking off during inferencing), then heat (again just me, though modern hardwares are okay-ish with 80+ celcius), then electricity (i must remind myself to undervolt my already efficient 9060xt after this post).",
          "score": 1,
          "created_utc": "2026-01-08 08:14:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydc0xt",
          "author": "at0mi",
          "text": "the biggest pain is that huihui seems to be the only one who is releasing abliterated (uncensored) model versions but only in Q4...",
          "score": 1,
          "created_utc": "2026-01-08 09:32:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydct55",
          "author": "Fcking_Chuck",
          "text": "The biggest problem right now is that there aren't any consumer graphics cards with enough video memory. Those that have enough memory cost an absurd amount of money, and it's often because the card has technology that isn't even used for AI.\n\nI'd like to see cards that are explicitly for AI, rather than having to use general-purpose graphics cards.",
          "score": 1,
          "created_utc": "2026-01-08 09:39:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydn41w",
          "author": "Unhappy-Bug-6636",
          "text": "Price of a good enough GPU is an issue, but there are other topics that are equally important, in my opinion:\n  - Learning what each LLM was built and optimized to do\n  - Selecting the “right” LLM for your use case\n  - Understanding quantization levels and knowing what performance degradation you can live with based on the quantization level \n  - Learning how to prompt well such that you provide the model with proper context and sufficient direction on what you want the model to produce\n\nI use both self hosted models and a Claude.ai/Claude Code LLM subscription. I test each hosted LLMs with a series of standard prompts I have developed, and then I review each response for errors and hallucinations.  I also have Claude review the responses for the same. This processes provides a lot of insight on the topics listed above.",
          "score": 1,
          "created_utc": "2026-01-08 11:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nye1f6a",
          "author": "Big_Championship1291",
          "text": "Performance and hallucinations. I work in the medical field and sharing medical information with an external service is a NO-NO but no client will buy a +4k machine just to get a resume of the status of the day and to do some questions. Not at least on the market I provide my services.",
          "score": 1,
          "created_utc": "2026-01-08 12:53:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyexp8o",
          "author": "beryugyo619",
          "text": "Give us 3050 96GB at $99 each or leave  \n\n##There is no problem that urgently need VC funding right now other than stupendously capital intensive GPU R&D problem",
          "score": 1,
          "created_utc": "2026-01-08 15:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf18ic",
          "author": "Careful_Breath_1108",
          "text": "The inability to pool VRAM across multiple GPUs for image video speech audio models",
          "score": 1,
          "created_utc": "2026-01-08 15:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfeyct",
          "author": "Crafty-Release5774",
          "text": "I'm probably on the fringe with my use case, but I've been trying to build a voice assistant integrated with Home Assistant to control devices within my home and give me status updates on temp, humidity, etc..\n\nFor this task most notable mentions are: GPT-OSS-20b, Qwen 3 (8 and 14B), I've even gone so far as to train Mistral 7B.  So far I'm still having the most luck with GPT-OSS (untrained).  I would say my largest challenge so far is more than likely the model size and training capabilities which are both currently limited by my hardware.    \n  \nThe relative \"intelligence\" of the model seems to have the largest impact on the outcomes.  The larger models I've experimented with seem to do much better with handling commands and hallucinate less than some of the smaller say 3-7B models both trained and untrained.  While training seems to help I still find that the smaller models fall short of the larger GPT and Qwen models I've experimented with.",
          "score": 1,
          "created_utc": "2026-01-08 16:55:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfu6kh",
          "author": "yogabackhand",
          "text": "Accessing my desktop local model from my smart device via a native app (not web app)... Pigeon currently fills this void (thank God) but is limited to Qwen3 out of the box.  Other models require LM Studio or Ollama integration which is a barrier.",
          "score": 1,
          "created_utc": "2026-01-08 18:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykt4f3",
              "author": "Aggravating-Try-3840",
              "text": "Check out Reins if your on iOS: https://reins.ibrahimcetin.dev",
              "score": 1,
              "created_utc": "2026-01-09 11:20:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nygpis4",
          "author": "jaf_1987",
          "text": "Hardware.  I've got a 5090 gpu and 64 GB of RAM and big models still run pretty slow.",
          "score": 1,
          "created_utc": "2026-01-08 20:18:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyiv6vn",
          "author": "x3haloed",
          "text": "I want to run a fully-featured Qwen3 agent on my computer. To do that, I need a custom fork of mlx-vlm, a custom fork of Qwen-Agent, and a custom UI. WHY! This should be ready out of the box by now.",
          "score": 1,
          "created_utc": "2026-01-09 02:37:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybwxsq",
          "author": "beheadedstraw",
          "text": "They’re all shit unless you have a 1 million dollar server, and even then it’s still mostly shit. It’s a novelty tech.",
          "score": 1,
          "created_utc": "2026-01-08 03:14:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyam35c",
          "author": "HealthyCommunicat",
          "text": "Community. Its 99% of people who do not work in AI, and never will where all I literally see are the same recycled slop made up of people who don’t understand, dont WANT or CARE to understand. It’s literally all people who don’t really care to learn, but they just constantly say they do because the idea of it is cool, but they of course don’t want to spend the time because in reality, if you don’t enjoy it as an actual passion, it’s just going to be too time and energy consuming\n\nIts making people who are actual developers and sysadmins get bunched with the slop crowd\n\nThe part that ticks me off the most is everyone wanting to get into it thinking they can, not even have an inkling of an understanding of how much there is to have to learn. It makes the general public devalue people like me. \n\nI have spent so much time and effort to be able to be in the job position I’m in, and now cuz of vibecoders, when I tell people I work with AI they assume that all i know is how to use claude.\n\nThe bar for entry has gotten really low, and thats great that more people have accessibility, but I really think the question should be much more focused on “should AI even be this accessible?”, its not just my own concerns but also with it comes to mental health and dangerous information, is this much wide accessibility to something this new really a good idea",
          "score": -5,
          "created_utc": "2026-01-07 23:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyauhlu",
              "author": "Bitterbalansdag",
              "text": "Ironic that you spent so many words on what is clearly yet another ai slop post",
              "score": 8,
              "created_utc": "2026-01-07 23:55:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nybzn1g",
                  "author": "Educational-World678",
                  "text": "I'm dyslexic and have mild social disabilities that a text-only only communnication doesn't always help. And I'm not stupid enough to think that finding tools that will help me manage tone and presentation in my writing isn't perfect for me.\n\nSo, I won't pretend I don't use AI to help when I post and sometimes when I comment. But not a bot. I'm trying to do something cool, and I'm hoping to learn what ideas I have that are valuable (what idea posts/comments get traction vs which don't) to people outside of myself and what ideas are best held as personal interests, as I plan out what it will take for me to build something potentially very cool.",
                  "score": 1,
                  "created_utc": "2026-01-08 03:29:14",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nyauu5k",
                  "author": "HealthyCommunicat",
                  "text": "Its literally all slop posts.",
                  "score": -2,
                  "created_utc": "2026-01-07 23:56:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyc1dq4",
              "author": "Educational-World678",
              "text": "I'm sorry you're going through that... I saw a comment yesterday from someone about learning to code, and when I asked him how he goes about learning and improving his skills, he told me a story about pushing a button that launched a predefined prompt in Gemini a few times until everything worked right. As a trained engineer (mechanical, not IT/AI related), I have had nightmares about things like that. Trying to catch the wave and see if there's any real potential for me to make something useful. (other than sending coders into the same professional trash pile that  mechanical engineers who specialized in carborators did when Fuel Injection was rolled out very quickly industry wide in a mechanical context, or horse poop sweepers before them...)",
              "score": 1,
              "created_utc": "2026-01-08 03:39:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nybdx9d",
          "author": "PrefersAwkward",
          "text": "I want something like Antigravity but open source and offline. Maybe there's stuff out there. I try and look now and then but no luck. I'm okay working with leaner, quicker models on my integrated GPU, I just want a good coder environment.\n\nI also would love a Gemini canvas open source and offline.",
          "score": 0,
          "created_utc": "2026-01-08 01:34:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7ivz3",
      "title": "Got GPT-OSS-120B fully working on an M2 Ultra (128GB) with full context & tooling",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q7ivz3/got_gptoss120b_fully_working_on_an_m2_ultra_128gb/",
      "author": "Thump604",
      "created_utc": "2026-01-08 18:00:50",
      "score": 42,
      "num_comments": 28,
      "upvote_ratio": 0.94,
      "text": "Hey everyone, I got **GPT-OSS-120B** running locally on my Mac Studio M2 Ultra (128GB), and I managed to get it fully integrated into a usable daily setup with Open WebUI and even VS Code (Cline).\n\nIt wasn’t straightforward—the sheer size of this thing kept OOMing my system even with 128GB RAM, but the performance is solid once it's dialed in. It essentially feels like having a private GPT-4.\n\nHere is the exact method for anyone else trying this.\n\n**The Hardware Wall**\n\n* **Machine:** Mac Studio M2 Ultra (128GB RAM)\n* **The Problem:** The FP16 KV cache + 60GB model weights = Instant crash if you try to utilize a decent context window.\n* **The Goal:** Run 120B parameters AND keep a 32k context window usable.\n\n**The Solution** Standard \n\n    mlx-lm\n\n1. **Model:** `gpt-oss-120b-MXFP4-Q8`  (This is the sweet spot—4-bit weights, 8-bit cache).\n2. **The Patch:** I modified `mlx_lm.server`  to accept new arguments: `--kv-bits 8`  and `--max-kv-size 32768` .\n   * *Why?* Without 8-bit cache quantization, the context window eats all RAM. With 8-bit, 32k context fits comfortably alongside the model.\n3. **Command:** `python -m mlx_lm.server --model ./gpt-oss-120b --kv-bits 8 --max-kv-size 32768 --cache-limit-gb 110`\n\n**The Stack:** Running the server is one thing; using it effectively is another.\n\n* **Open WebUI:**\n   * I built a custom Orchestrator (FastAPI) that sits between WebUI and MLX.\n   * **Dual Mode:** I created two model presets in WebUI:\n      1. **\"Oracle\"**: 120B raw speed. No tools, just fast answers.\n      2. **\"Oracle (Tools)\"**: Same model, but with RAG/Database access enabled.\n   * Keeps the UI fast for chat, but powerful when I need it to dig through my files.\n* **VS Code (Cline Integration):**\n   * This was the tricky part. Cline expects a very specific OpenAI chunk format.\n   * I had to write a custom endpoint in my orchestrator (`/api/cline/chat/completions` ) that strips out the internal \"thinking/analysis\" tokens (`<|channel|>analysis...` ) so Cline only sees the final clean code.\n   * Result: I have a massive local model driving my IDE with full project context, totally private.\n\n**The Experience** It’s honestly game-changing. The reasoning on 120B is noticeably deeper than the 70B models I was using (Llama 3/Qwen). It follows complex multi-file coding tasks in Cline without getting lost, and the 32k context is actually usable because of the memory patch.\n\nIf anyone wants the specific code patches for MLX or the docker config, let me know and I can share. Just wanted to share that it *is* possible to daily drive 120B on consumer Mac hardware if you maximize every GB.\n\n",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7ivz3/got_gptoss120b_fully_working_on_an_m2_ultra_128gb/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyfx0y0",
          "author": "Leopold_Boom",
          "text": "What kind of tokens / second generation + prompt processing are you getting? Does it change with proper 16bit KV?",
          "score": 5,
          "created_utc": "2026-01-08 18:14:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygm6qh",
              "author": "Thump604",
              "text": "On an M2 Ultra 128GB with MLX, I am getting : 📊 BENCHMARK RESULTS (/Volumes/Pegasus/personal-ai-data/mlx\\_models/gpt-oss-120b-MXFP4-Q8)\n\n========================================\n\n⚡ Time to First Token (TTFT): 623.84 ms\n\n🚀 Tokens Per Second (TPS):    69.14 t/s\n\n⏱️  Total Generation Time:      17.76 s\n\n📝 Total Tokens Generated:     1185\n\n💾 Peak RAM Usage:             71.35 GB\n\n======================================== Regarding KV Cache: On a 128GB machine, **8-bit KV is mandatory** for decent context (32k). If you force 16-bit KV, you will OOM/Swap and performance tanks to near zero once the context fills up. The speed gain from 8-bit prevents disk swapping, and the quality loss is not noticeable for coding/logic.",
              "score": 8,
              "created_utc": "2026-01-08 20:03:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyin4ny",
                  "author": "PitifulBall3670",
                  "text": "Thanks for sharing! This is very helpful.",
                  "score": 2,
                  "created_utc": "2026-01-09 01:54:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nygnblr",
                  "author": "StardockEngineer",
                  "text": "prompt processing?",
                  "score": 1,
                  "created_utc": "2026-01-08 20:08:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyio1uz",
          "author": "onethousandmonkey",
          "text": "Have a 96GB Ultra 3 and happily running 120b with no modifications.\nBut I don’t understand half of what you did. I might need to as I have been testing it with Roo Code in VSCode, works ok so far.",
          "score": 3,
          "created_utc": "2026-01-09 01:59:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyj31mp",
          "author": "spectralyst",
          "text": "I'm running this model on a 16GB + 64GB system FWIW",
          "score": 2,
          "created_utc": "2026-01-09 03:19:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg2toj",
          "author": "ElectronSpiderwort",
          "text": "did you adjust iogpu.wired\\_limit\\_mb?",
          "score": 1,
          "created_utc": "2026-01-08 18:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygi1g1",
              "author": "Thump604",
              "text": "YEs, iogpu.wired\\_limit\\_mb=115000 ",
              "score": 1,
              "created_utc": "2026-01-08 19:45:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyhnin4",
              "author": "Zc5Gwu",
              "text": "Yeah, shouldn’t be ooming given it’s only 60ish gb.",
              "score": 1,
              "created_utc": "2026-01-08 22:50:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nygs4vy",
          "author": "ajujox",
          "text": "Do u plan to share your proyect and walkthrough? I have the same machine and gpt 120b is the best form me",
          "score": 1,
          "created_utc": "2026-01-08 20:30:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyh6npq",
              "author": "Thump604",
              "text": "Happy to answer any questions, but doubt I will share the project on Gtihub",
              "score": 1,
              "created_utc": "2026-01-08 21:34:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhbxqp",
          "author": "YouAreTheCornhole",
          "text": "Bro what is wrong? I'm getting around 50 tokens a sec on an M3 Max with 120b with similar settings",
          "score": 1,
          "created_utc": "2026-01-08 21:57:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhrjfm",
              "author": "Thump604",
              "text": "my bad!\n\n  \n📊 BENCHMARK RESULTS (/Volumes/Pegasus/personal-ai-data/mlx\\_models/gpt-oss-120b-MXFP4-Q8)\n\n========================================\n\n⚡ Time to First Token (TTFT): 623.84 ms\n\n🚀 Tokens Per Second (TPS):    69.14 t/s\n\n⏱️  Total Generation Time:      17.76 s\n\n📝 Total Tokens Generated:     1185\n\n💾 Peak RAM Usage:             71.35 GB\n\n========================================",
              "score": 1,
              "created_utc": "2026-01-08 23:10:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjnbha",
          "author": "deulamco",
          "text": "So it is finally usable to cloud LLM level yet ?\n\nWas about to ask if 20B model + RAG/Internet access could catch up with Cloud version 🤡",
          "score": 1,
          "created_utc": "2026-01-09 05:22:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhlhzk",
          "author": "Beautiful-End529",
          "text": "Did you buy the Mac with 128GB pre-installed or did you manually upgrade the RAM?",
          "score": -1,
          "created_utc": "2026-01-08 22:40:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhv7ug",
              "author": "Thump604",
              "text": "pre-installed",
              "score": 2,
              "created_utc": "2026-01-08 23:28:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyl33gz",
          "author": "whyyoudidit",
          "text": "GLM 4.7 produced 17 million tokens for me yesterday before hitting my limit for the 5 hour window. I use it almost at full context of 225K with Cline in VScode. in total this was about 30k lines of code. Token speed I have no idea but following the numbers above it was about 940 tps. And I had many pauses in between. This is all for $3 per month. Local llm is what I want to do but it is a thousand times more expensive than GLM 4.7 api.",
          "score": 0,
          "created_utc": "2026-01-09 12:34:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxvlh0",
      "title": "I have 50 ebooks and I want to turn them into a searchable AI database. What's the best tool?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "author": "Great_Jacket7559",
      "created_utc": "2025-12-28 16:33:33",
      "score": 41,
      "num_comments": 29,
      "upvote_ratio": 0.95,
      "text": "I want to ingest 50 ebooks into an LLM to create a project database.\nIs Google NotebookLM still the king for this, or should I be looking at Claude Projects or even building my own RAG system with LlamaIndex?\nI need high accuracy and the ability to reference specific parts of the books. I don't mind paying for a subscription if it works better than the free tools.\nAny recommendations?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxvlh0/i_have_50_ebooks_and_i_want_to_turn_them_into_a/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwj05xl",
          "author": "Zucramj",
          "text": "I would build something custom.\n\nHere is how I see it:\n\n1) You own the data \n2) You own the AI (local embeddings work great for this task and you can run it with a local AI model or use Openrouter) \n3) I would build this with DSPy (modular and can be optimized with gepa) \n4) I would use PostgreSQL to store the data \n\nSo if you already have the ebooks as pdfs I would take those an set that up.\n\nHere is my primitive version of this I did some months back: \n\nhttps://github.com/marcusjihansson/dspy-mcp-tools/blob/main/regulatory.py\n\nIt is advanced if you don't know what you are reading but as I have gotten deeper into optimizing AI agents and systems does this feel primitive to me. \n\nThat newer research is going to be uploaded to my GitHub soon...\n\nSo: \nA) I would read through if this would solve your task! \nB) I would be happy to help you out if you have any questions!",
          "score": 9,
          "created_utc": "2025-12-29 11:11:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf0w7j",
          "author": "DHFranklin",
          "text": "NotebookLM and maybe some RAG and Custom Instructions for vectoring.\n\nNow if you wanna get real squirrely you could turn the entire compendium, and custom instruction into 1 million token prompt and sit it into Gemini as is. That might actually be more useful.\n\nThe trick is information loss and context bleed with the books. I could see a JSON Database made from it all as text also.\n\nIt comes down to what are you using it for and what information needs to stay consistent.",
          "score": 3,
          "created_utc": "2025-12-28 19:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwe78fy",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 2,
          "created_utc": "2025-12-28 17:31:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwerjho",
              "author": "blaidd31204",
              "text": "I am intrigued... what If:\n\n* pdf (Yes, there are images, but these should not influence info)?\n\n* markdown (No images)?",
              "score": 2,
              "created_utc": "2025-12-28 19:06:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwh0m2z",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 02:05:13",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwgwh5t",
                  "author": "Investolas",
                  "text": "Those answers influence the reccomendation.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:41:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwim06f",
          "author": "RepLava",
          "text": "LightRAG with the MCP. Works great based on the relatively sparse info you've given",
          "score": 2,
          "created_utc": "2025-12-29 08:59:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlvovo",
          "author": "False-Ad-1437",
          "text": "AnythingLLM can make your own workspaces for this.",
          "score": 2,
          "created_utc": "2025-12-29 20:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpin9j",
          "author": "Cladser",
          "text": "I’m deep in a similar project atm. It depends a lot on the type of info you want from the LLM. If it’s what does author x have to say about y - RAG is your best bet. However if you want to ask questions like across these books what are the most common ways of dealing with with Y - That is a corpus level (ie entire collection) query and RAG will suck. Llamaindex with hybrid search seems to be the best middle road at the moment.",
          "score": 3,
          "created_utc": "2025-12-30 10:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwft2zf",
          "author": "vidibuzz",
          "text": "Slightly off topic. You may want to use Illuminati.google.com also for voice summaries.",
          "score": 2,
          "created_utc": "2025-12-28 22:09:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhdx2b",
              "author": "Schizophreud",
              "text": "Didn’t know about this. Thanks.",
              "score": 1,
              "created_utc": "2025-12-29 03:22:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwibx1i",
          "author": "Charming_Support726",
          "text": "Depending on what's in the books, you could have a look at IBMs Docling for conversion. I think every simple RAG Pipeline will do the trick in the beginning",
          "score": 1,
          "created_utc": "2025-12-29 07:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwigiuv",
          "author": "maxz2040",
          "text": "Logically.app",
          "score": 1,
          "created_utc": "2025-12-29 08:08:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwm4e9l",
          "author": "Empty-Poetry8197",
          "text": "Paperqa",
          "score": 1,
          "created_utc": "2025-12-29 21:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwno756",
          "author": "kchandank",
          "text": "Interesting, if you are able to achieve your objective, would you be able to share the steps?",
          "score": 1,
          "created_utc": "2025-12-30 02:09:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpe82p",
          "author": "Sea_Mouse655",
          "text": "I’ve been using PaperQA2 for some regulatory use cases and it’s gangster",
          "score": 1,
          "created_utc": "2025-12-30 09:41:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsjglj",
          "author": "isleeppeople",
          "text": "Seems like you could use something like pypdf and langchain to embed it into your RAG. If the ebooks are like current info that can change or become stale you will want to tag them and set up some sort of workflow to compare them to a Gemini or open ai call to compare the info and if it becomes stale remove it. I use qdrant and postgresql for ground truth. I do stuff like this for versions of python that I have to use to stay compatible with other things I'm running. Or actually for langchain and langgraph too. They just changed compatibility versions so whenever I upgrade I can just use the newer repo in my rag with the updated information. I will still keep the old one until I am absolutely certain I won't revert but you can just leave it sit there and not reference the old one. Hope that makes sense.",
          "score": 1,
          "created_utc": "2025-12-30 20:25:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh0iik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2025-12-29 02:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwimlqu",
              "author": "Just_Bronze",
              "text": "I have a stupid question.\n\nNot entirely sure how these things work, but do I understand correctly you've got a system set up to take input and create a chatbot?  Or do you create a file/fileset that a chatbot incorporates to use the information.",
              "score": 3,
              "created_utc": "2025-12-29 09:05:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwisq5d",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": 3,
                  "created_utc": "2025-12-29 10:03:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjfv9q",
              "author": "loki626",
              "text": "Can I DM you? I have some pdfs that I would like to convert. They are more technical though. Medicine related.",
              "score": 1,
              "created_utc": "2025-12-29 13:13:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmga0",
          "author": "PaleontologistOk865",
          "text": "What about just throwing everything in a AI and letting it figure out what to do? That's what my clients keep saying to me. . .",
          "score": 0,
          "created_utc": "2025-12-30 23:38:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0fg09",
      "title": "I got my first ever whitepaper published",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/v23qicvcdy9g1.png",
      "author": "InsideResolve4517",
      "created_utc": "2025-12-31 15:21:16",
      "score": 39,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Research",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0fg09/i_got_my_first_ever_whitepaper_published/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwyduyb",
          "author": "Lame_Johnny",
          "text": "Amazing! congratulations!",
          "score": 1,
          "created_utc": "2025-12-31 18:26:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ein",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwynwee",
          "author": "Busy_Farmer_7549",
          "text": "Congratulations 🎈🎊",
          "score": 1,
          "created_utc": "2025-12-31 19:17:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25ed2",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwytdq1",
          "author": "PromptOutlaw",
          "text": "Congrats bud!! I really underestimated how much hard work goes into these. I’ve been humbled recently and I’m not even half way",
          "score": 1,
          "created_utc": "2025-12-31 19:46:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx25e8e",
              "author": "InsideResolve4517",
              "text": "btw, I'm not the OP, but yeah! OP did really great work!",
              "score": 1,
              "created_utc": "2026-01-01 09:57:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzsdsy",
      "title": "Suggest a model for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "author": "Bright_Dot113",
      "created_utc": "2025-12-30 20:10:25",
      "score": 38,
      "num_comments": 25,
      "upvote_ratio": 0.98,
      "text": "Hello, I have 9950x3d with 64GB RAM and 5070 ti \n\nI recently installed LM Studio, which models do you suggest based on my hardware for the following purposes. \n\n1. Code in python and rust \n\n2. DB related stuff like optimising queries or helping me understand them. (Postgresql)\n\n3. System and DB design.\n\nAlso what other things can I do?\nI have heard lot about MCP servers but I didn't find any MCP servers useful or anything related to my workflow if you have any suggestions that would be great! ",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pzsdsy/suggest_a_model_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwsjtel",
          "author": "SimplyRemainUnseen",
          "text": "I'd suggest unsloth/Nemotron-3-Nano-30B-A3B-GGUF. You'll need to offload to system memory, but you'll have a relatively fast and intelligent model that's good at using tools.\n\nFor MCP servers check out [https://github.com/punkpeye/awesome-mcp-servers](https://github.com/punkpeye/awesome-mcp-servers)",
          "score": 16,
          "created_utc": "2025-12-30 20:27:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuo35r",
              "author": "Big-Masterpiece-9581",
              "text": "What kind of speeds to you think he could get with this model and that configuration? Anything particular you like about that model or unsloth?",
              "score": 3,
              "created_utc": "2025-12-31 03:12:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwxynte",
                  "author": "SimplyRemainUnseen",
                  "text": "Speeds would be pretty fast I imagine. Faster than my work laptop with an iGPU (which runs it faster than I can read).\n\nThe model itself is fully open source (data too!) and made by NVIDIA. The performance of the model exceeds other models in the same parameter range of 20-32b and has very long context (up to a million tokens).\n\nNVIDIA worked with Unsloth for day 0 quants of the model. Unsloth makes efficient dynamic quants of models that retain very high levels of accuracy at lower precision. Considering OP will need to offload to system memory, GGUF is ideal as it's designed for that use case.",
                  "score": 5,
                  "created_utc": "2025-12-31 17:11:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx1r491",
                  "author": "Hairy_Candy_3225",
                  "text": "I use a similar setup with 9950x3d 96 GB RAM and 5080 16 GB VRAM and use Nemotron 3 nano 30b Q6. Tokens per second heavily depends on context used. I've written a script to run a benchmark test and measure speed with all combinations of different context length / % GPU-offload layer / force experts on CPU on vs off.\n\nWith smaller context (i.e. 10k tokens) I get up to 16 TPS. This drops to around 10 @200k tokens. I'm no expert but I don't think there will be a big difference between 5070 or 5080 if VRAM is the same.\n\nWhat i can't understand is that with nemotron it does not matter how much layers I offload to GPU. The model is around 30 GB but only between 4 and 6 GB VRAM is used regardless of offload setting. It seems only the active layers are offloaded to GPU. This was different when I qwen 2.5, where I had to balance the exact number of layers that would fit in VRAM. As soon as you try to offload more layers than fit in VRAM, regular RAM will be used as shared VRAM and TPS drop drastically. \n\nProbably nothing new here for most redditors on this sub but i'm new to local LLM's and had to figure out a lot of things myself.",
                  "score": 2,
                  "created_utc": "2026-01-01 07:25:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwsmq4k",
          "author": "beedunc",
          "text": "Qwen3coder, whatever fits.",
          "score": 9,
          "created_utc": "2025-12-30 20:41:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtu8zi",
          "author": "No-Consequence-1779",
          "text": "Qwen3-coder-30b. The largest quant you can run. Dense model is good or moe.  Keep in mind coder specific models are specialized for coding. Many like oss 120b though that size is not necessary. ",
          "score": 6,
          "created_utc": "2025-12-31 00:21:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt434d",
          "author": "Toastti",
          "text": "If you add another 64gb of ram you can run a Q2 quant of Minimax m2.1. it will probably be 7tk/s or so but it is almost certainly the smartest agentic coding model you can reasonably run",
          "score": 3,
          "created_utc": "2025-12-30 22:03:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwt53sn",
              "author": "Your_Friendly_Nerd",
              "text": "q2? isn‘t that gonna suck?",
              "score": 3,
              "created_utc": "2025-12-30 22:08:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtv56k",
                  "author": "Worried_Goat_8604",
                  "text": "No unsloth dynamic quant v2 usually dosnt reduce that much quality",
                  "score": 1,
                  "created_utc": "2025-12-31 00:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwugr2q",
              "author": "Individual_Gur8573",
              "text": "I agree I found minimax 2.1 IQ2_M the smartest model after GLM 4.5 air ",
              "score": 1,
              "created_utc": "2025-12-31 02:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwuo8ix",
                  "author": "Karyo_Ten",
                  "text": "GLM Air which quant?",
                  "score": 1,
                  "created_utc": "2025-12-31 03:13:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxlq1uw",
              "author": "Wild_Requirement8902",
              "text": " you can run iq4xs with 128 gb ram v and get 7 tk/s with 128gb of ram (with an old xeon(2680v4) and quad channel ddr4 @ 2400,) using lm studio and a 5060ti & a 3060 12gb I can get up to 131072 context @ q8 with flash attention. That what i am using, but sometime it crash and you have to reload the model and prompt processing is painful (35tk/s)",
              "score": 1,
              "created_utc": "2026-01-04 10:15:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwviewu",
          "author": "Count_Rugens_Finger",
          "text": "Qwen3-coder-30B-A3B, Nemotron-3-Nano-30B-A3B, Devstral-Small-2-24B",
          "score": 3,
          "created_utc": "2025-12-31 06:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsooqq",
          "author": "FullstackSensei",
          "text": "I'd say try a bunch of MoE all the way up to gpt-oss-120b and see where your pain threshold is for speed. IMO, you should keep a few at hand: a smaller model like Qwen3 Coder 30B, Nemotron 30B, gpt-oss-20b, Devstral 2 24B as daily drives, and larger models like gpt-oss-120b, GLM 4.5 air, Devstral 2 123B (Q4), etc for when the smaller models get stuck or can't solve whatever issue you have.",
          "score": 4,
          "created_utc": "2025-12-30 20:50:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwt5rhp",
          "author": "Level_Wolverine_141",
          "text": "I have the same system as you except I've got a 5080, and I'm just using Claude code max and it's pretty good.",
          "score": -4,
          "created_utc": "2025-12-30 22:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww8pra",
              "author": "g33khub",
              "text": "your 5080 doesn't matter, I can use Claude code on a raspberry pi",
              "score": 2,
              "created_utc": "2025-12-31 10:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyw1b4",
      "title": "I built Plano(A3B) - fast open source LLM for agent orchestration that beats frontier LLMs",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5rp16cxd57ag1.png",
      "author": "AdditionalWeb107",
      "created_utc": "2025-12-29 19:44:28",
      "score": 38,
      "num_comments": 14,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pyw1b4/i_built_planoa3b_fast_open_source_llm_for_agent/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwoqem5",
          "author": "ThsYWeCntHveNiceTngs",
          "text": "your research page is more advert than research and the blog provides more detail, but not much. Is there an Arxiv link or anything to read about your method and how you generated the proposed results?",
          "score": 6,
          "created_utc": "2025-12-30 06:07:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwosnmj",
              "author": "AdditionalWeb107",
              "text": "The huggingface models page has more details. Although we are in the process of publishing the arxiv paper",
              "score": 0,
              "created_utc": "2025-12-30 06:25:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxc7j4",
          "author": "False-Ad-1437",
          "text": "This is not open source. \n\nIt imposes non-open requirements (attribution, use restrictions, redistribution constraints and separate commercial licensing for certain uses) that violate the open-source criteria defined by the OSI/FSF.",
          "score": 2,
          "created_utc": "2025-12-31 15:20:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwufm16",
          "author": "Purple-Programmer-7",
          "text": "How does this differ from Arch that you’ve previously pushed for the past year?",
          "score": 1,
          "created_utc": "2025-12-31 02:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui3lf",
              "author": "AdditionalWeb107",
              "text": "Arch was about model routing. Plano is about orchestration, which is a slightly more complicated set of tasks. Plano is the next major upgrade to Arch with several new capabilities for agentic applications like filter chains, agent signals, and even more robust model gateway.\n\n\nBy the way, people were confusing arch with arch Linux so we thought it was a better time to rename the project.\nTry Plano 🙏",
              "score": 1,
              "created_utc": "2025-12-31 02:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwyaxs3",
          "author": "AdditionalWeb107",
          "text": "That’s fair - it should say open weights. And the license is very permissive except for one deployment type",
          "score": 1,
          "created_utc": "2025-12-31 18:12:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwopty3",
          "author": "maigpy",
          "text": "what is model-native?",
          "score": 1,
          "created_utc": "2025-12-30 06:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqe6c",
              "author": "AdditionalWeb107",
              "text": "It’s integrated with small LLMs - central to how the project is built.",
              "score": -1,
              "created_utc": "2025-12-30 06:07:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwt6v7x",
                  "author": "maigpy",
                  "text": "integrated with small llms translates to \"model-native\"?\n\nI don't quite understand, if you could use more words to describe what's going that would help.",
                  "score": 1,
                  "created_utc": "2025-12-30 22:16:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnderh",
          "author": "Lyuseefur",
          "text": "Planning already to use it in next build of nexora follow us\n\nHttps://github.com/jeffersonwarrior/nexora",
          "score": 0,
          "created_utc": "2025-12-30 01:09:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnwumq",
              "author": "AdditionalWeb107",
              "text": "Okay - thanks. Would love the feedback. And if you like our project, don't forget to star it too",
              "score": 1,
              "created_utc": "2025-12-30 02:56:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3os4p",
      "title": "Tencent HY-MT1.5, a specialized machine-translation model",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/rq2666gzsbbg1.png",
      "author": "etherd0t",
      "created_utc": "2026-01-04 12:29:17",
      "score": 36,
      "num_comments": 4,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3os4p/tencent_hymt15_a_specialized_machinetranslation/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxskc30",
          "author": "Lost-Dot-9916",
          "text": "thank you",
          "score": 1,
          "created_utc": "2026-01-05 10:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzg9wj",
          "author": "Naive-Pear-9579",
          "text": "Interesting. Is this also suitable for localization (L10N)?",
          "score": 1,
          "created_utc": "2026-01-06 10:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydfmi7",
          "author": "Abhirocks16",
          "text": "wow so finally an offline translation app can be built, thats amazing",
          "score": 1,
          "created_utc": "2026-01-08 10:05:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3asxm",
      "title": "Which tools should I be looking at? Want to use local AI to tailor my resume to job descriptions",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "author": "cspybbq",
      "created_utc": "2026-01-04 00:16:54",
      "score": 34,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "I'm job hunting and trying to learn more about AI at the same time. \n\nI want the AI to be aware of all my resume versions (15ish versions) and to tailor new versions of my resume based on the contents of those resumes, plus job descriptions I give it. I'd also like it to evaluate a job description and tell if I'm a good fit or not, based on my resumes. \n\nIs this something I can set up on my local computer? \n\n * AMD Ryzen 5700G\n * Nvidia 3070\n * 64G RAM\n * Running Debian\n\nThere are so many models and variants of models that I'm not really sure where to start. I have played a bit with ollama (cli) and open-webui but haven't really figured out how to set up RAG correctly to handle my documents or get any sort of professional level output.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3asxm/which_tools_should_i_be_looking_at_want_to_use/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxk0cgf",
          "author": "jinnyjuice",
          "text": "Might want to also post to /r/localllama -- I would be interested in the responses also on how people judge different models to be strong in which dimensions!",
          "score": 3,
          "created_utc": "2026-01-04 02:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjgmey",
          "author": "fandry96",
          "text": "Go to gemini. Make a gem. Add your 10 best resumes to assets. \n\nYou can try defaulting to agent mode to try it. If not click save. \n\nPost the jobs there and tell Gemini to write a resume...",
          "score": 3,
          "created_utc": "2026-01-04 00:46:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzjhp",
      "title": "Google Open-Sources A2UI: Agent-to-User Interface",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "author": "techlatest_net",
      "created_utc": "2025-12-28 19:08:03",
      "score": 27,
      "num_comments": 8,
      "upvote_ratio": 0.87,
      "text": "Google just released **A2UI (Agent-to-User Interface)** — an open-source standard that lets AI agents generate **safe, rich, updateable UIs** instead of just text blobs.\n\n👉 Repo: [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\n# What is A2UI?\n\nA2UI lets agents “**speak UI**” using a **declarative JSON format**.  \nInstead of returning raw HTML or executable code (⚠️ risky), agents describe *intent*, and the client renders it using **trusted native components** (React, Flutter, Web Components, etc.).\n\nThink:  \nLLM-generated UIs that are **as safe as data, but as expressive as code**.\n\n# Why this matters\n\nAgents today are great at text and code, but terrible at:\n\n* Interactive forms\n* Dashboards\n* Step-by-step workflows\n* Cross-platform UI rendering\n\nA2UI fixes this by cleanly separating:\n\n* **UI generation (agent)**\n* **UI execution (client renderer)**\n\n# Core ideas\n\n* 🔐 **Security-first**: No arbitrary code execution — only pre-approved UI components\n* 🔁 **Incremental updates**: Flat component lists make it easy for LLMs to update UI progressively\n* 🌍 **Framework-agnostic**: Same JSON → Web, Flutter, React (coming), SwiftUI (planned)\n* 🧩 **Extensible**: Custom components via a registry + smart wrappers (even sandboxed iframes)\n\n# Real use cases\n\n* Dynamic forms generated during a conversation\n* Remote sub-agents returning UIs to a main chat\n* Enterprise approval dashboards built on the fly\n* Agent-driven workflows instead of static frontends\n\n# Current status\n\n* 🧪 **v0.8 – Early Public Preview**\n* Spec & implementations are evolving\n* Web + Flutter supported today\n* React, SwiftUI, Jetpack Compose planned\n\n# Try it\n\nThere’s a **Restaurant Finder demo** showing end-to-end agent → UI rendering, plus Lit and Flutter renderers.\n\n👉 [https://github.com/google/A2UI/](https://github.com/google/A2UI/)\n\nThis feels like a big step toward **agent-native UX**, not just chat bubbles everywhere. Curious what the community thinks — is this the missing layer for real agent apps?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxzjhp/google_opensources_a2ui_agenttouser_interface/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwgkvue",
          "author": "bananahead",
          "text": "I am begging people to stop with the LLM written posts. Just post whatever prompt you used! That’s the post!",
          "score": 13,
          "created_utc": "2025-12-29 00:36:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwi048d",
              "author": "leonbollerup",
              "text": "Why care? I rather want a LLM generated post than some bad version of English - not everyone speaks English native",
              "score": 4,
              "created_utc": "2025-12-29 05:47:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwjksfb",
                  "author": "ak_sys",
                  "text": "Id rather tailor my discussion to a non English speaker than not understanding that there may be a disconnect getting lost in translation. \n\nThis explains a lot of \"reading comprehension\" issues I've noticed from commenters, where they seem to be responding to some diffuse sentiment of the post rather than  the actual nuanced point and position. Languages almost never just directly translate into one another, and inserting an llm in the middle of a discussion without informing the other party seems pretty dishonest and disrespectful.",
                  "score": 1,
                  "created_utc": "2025-12-29 13:44:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwmr4i2",
                  "author": "bananahead",
                  "text": "So write a post in your native language and have it translated. That’s just as easy as whatever prompt created this and would be easier to read and more authentic.",
                  "score": 1,
                  "created_utc": "2025-12-29 23:08:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwgo2bh",
              "author": "Kamal965",
              "text": "The absolute state of Reddit now:\n\n* A lazy OP asks an LLM to create a post for them.\n* Users see LLM-isms and either:\n   * Ctrl + W\n   * Pass it on to an LLM to summarize it for them instead.\n* Another OP gives up on writing their own posts and starts using an LLM because they keep running into LLM-generated posts.\n\nRinse and repeat.",
              "score": 0,
              "created_utc": "2025-12-29 00:53:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nworg5v",
          "author": "ZITNALTA",
          "text": "Just curious if any knows if this somehow works with Google Antigravity IDE? By the way, I am NOT a dev so if this sounds like a newbie question that is why.",
          "score": 1,
          "created_utc": "2025-12-30 06:15:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7e2ol",
      "title": "Guide: How to Run Qwen-Image Diffusion models! (14GB RAM)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/yismpz1645cg1.png",
      "author": "yoracale",
      "created_utc": "2026-01-08 15:03:24",
      "score": 27,
      "num_comments": 6,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7e2ol/guide_how_to_run_qwenimage_diffusion_models_14gb/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nyerp01",
          "author": "IngwiePhoenix",
          "text": "I am praying ComfyUI will be replaced by a less fucked up UI. Now this is very much a me-problem (visual impairment -> graph based UIs becoming a blurry mess of colorful spaghetti). But hell, it works, and that's good. x)",
          "score": 5,
          "created_utc": "2026-01-08 15:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyesi5b",
              "author": "yoracale",
              "text": "I feel like their workflow UI is actually their strength, maybe you can suggest some changes to their github page since they are open-source? :)",
              "score": 2,
              "created_utc": "2026-01-08 15:15:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyftw0z",
                  "author": "tomByrer",
                  "text": "ComfyUI has a v2 that they are public testing.  Sometimes if folks are switched to that v2, which still has bugs, the users will get frustrated.  \nThere are tricks to get rid of the spaghetti, but many don't know about those.\n\nI'm used to no-code programming & using nodes (for over 20 years), but even with my prior experience, my ComfyUI can looks like a mess also.  Trade off: flexibility takes work.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:00:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykw3a5",
          "author": "adeukis",
          "text": "Hey, thank you for sharing this guide.\n\nJust an FYI for future windows users who are newbies like me.\n\nI did a clean setup on windows and followed all steps (use curl.exe and avoid multiline command).   \nWhen i run python [main.py](http://main.py) from main folder i get an error\n\n    Traceback (most recent call last):\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\main.py\", line 178, in <module>\n        import execution\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\execution.py\", line 15, in <module>\n        import comfy.model_management\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\comfy\\model_management.py\", line 238, in <module>\n        total_vram = get_total_memory(get_torch_device()) / (1024 * 1024)\n                                      ^^^^^^^^^^^^^^^^^^\n      File \"C:\\Users\\user\\comfyui\\ComfyUI\\comfy\\model_management.py\", line 188, in get_torch_device\n        return torch.device(torch.cuda.current_device())\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n      File \"C:\\Users\\user\\comfyui\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py\", line 1069, in current_device\n        _lazy_init()\n      File \"C:\\Users\\user\\comfyui\\.venv\\Lib\\site-packages\\torch\\cuda\\__init__.py\", line 403, in _lazy_init\n        raise AssertionError(\"Torch not compiled with CUDA enabled\")\n    AssertionError: Torch not compiled with CUDA enabled\n\nUninstalled torch, clear cache and installed nightly due to blackwell architecture\n\n    pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu130\n\nNow i get new error   \n`ImportError: cannot import name 'Sentinel' from 'typing_extensions'`\n\nUpgrading pydantic with `pip install --upgrade pydantic`  \ndid the job.\n\nLoaded json workflow and this is the result (eyes are a bit weird hehe)\n\nhttps://preview.redd.it/081iijnt9bcg1.png?width=613&format=png&auto=webp&s=55ec0b710826dafadfd09365c7eb2722b1fc6c80\n\nI will go and test edit later on.\n\nCheers",
          "score": 2,
          "created_utc": "2026-01-09 11:44:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl2wg1",
              "author": "adeukis",
              "text": "qwen edit result with the given workflow json (poor sloth 😁 )\n\nhttps://preview.redd.it/kqpgvnliibcg1.png?width=913&format=png&auto=webp&s=021b957540f9297ffb60d7da76fa8937f1a312ff",
              "score": 2,
              "created_utc": "2026-01-09 12:32:50",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyljjon",
              "author": "yoracale",
              "text": "Awesome thanks for the tips well add it to our guide tgd j you! 🙏",
              "score": 1,
              "created_utc": "2026-01-09 14:08:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7e34g",
      "title": "Does it make sense to have a lot of RAM (96 or even 128GB) if VRAM is limited to only 8GB?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q7e34g/does_it_make_sense_to_have_a_lot_of_ram_96_or/",
      "author": "mark_17000",
      "created_utc": "2026-01-08 15:03:51",
      "score": 25,
      "num_comments": 37,
      "upvote_ratio": 0.93,
      "text": "Starting to look into running LLMs locally and I have a question. If VRAM is limited to only 8GB, does it make sense to have an outsized amount of RAM (up to 128GB)? What are the technical limitations of such a setup?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7e34g/does_it_make_sense_to_have_a_lot_of_ram_96_or/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyfjjfc",
          "author": "uti24",
          "text": "The limitation is speed.\n\nTypical DDR5 RAM bandwidth is around 80 GB/s. With 128 GB of system RAM, you could run an 80B model (quantized to Q8) at roughly 1 token/s. (you can very roughly calculate t/s by dividing memory bandwidth/model size, in this case 80GB/s / 80B@Q8 = 1)\n\nTypical VRAM bandwidth varies widely, from about 200 GB/s to 2000 GB/s on high-end gaming GPUs. If you somehow had 128 GB of VRAM, you could run an 80B Q8 model at roughly 2.5 tokens/s on the low GPU end and up to 25 tokens/s on the high end GPU.\n\nThere are also MoE (Mixture-of-Experts) models. While they may be 100B-parameter models overall, they do not use the entire model for every token, only a subset, for example \\~5B parameters. Because of this, even with a 100B MoE model running from system RAM, you could still achieve something like 20 tokens/s.",
          "score": 28,
          "created_utc": "2026-01-08 17:15:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhetbf",
              "author": "mark_17000",
              "text": "Thanks! I'll take a look at MoE models, too",
              "score": 4,
              "created_utc": "2026-01-08 22:09:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyerpgm",
          "author": "PsychologicalWeird",
          "text": "Are you looking to buy ram now? If so check prices first and then when you have picked your chin off the floor, consider funneling some of that money into a better GPU.",
          "score": 16,
          "created_utc": "2026-01-08 15:11:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyetm8o",
              "author": "mark_17000",
              "text": "There are limitations. Only 8GB is possible for VRAM. RAM prices aren't a limiting factor. Just want to know from a technical perspective if more RAM would be useful.",
              "score": 5,
              "created_utc": "2026-01-08 15:20:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nygwrz6",
                  "author": "simracerman",
                  "text": "It would for some MoE models where you can offload experts to RAM and still have decent speed. Without enough RAM, you won’t be running those models.",
                  "score": 2,
                  "created_utc": "2026-01-08 20:51:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyexjzb",
          "author": "Medium_Chemist_4032",
          "text": "Depends. I have 128 ram and am trying out bigger MoE's, but those that I tried didn't cross the minimum productivity threshold for coding, for example. It often boils down to: \"wow, great start, let's try spec'ing out a simple webservice\" and they can't produce a single working file, then loop. I have only spot tested a few models, nothing really representative",
          "score": 5,
          "created_utc": "2026-01-08 15:38:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhezrz",
              "author": "mark_17000",
              "text": "How much VRAM do you have in this setup?",
              "score": 1,
              "created_utc": "2026-01-08 22:10:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyk7ro5",
                  "author": "Medium_Chemist_4032",
                  "text": "3x3090",
                  "score": 1,
                  "created_utc": "2026-01-09 08:09:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nylya60",
              "author": "uti24",
              "text": ">Depends. I have 128 ram and am trying out bigger MoE's, but those that I tried didn't cross the minimum productivity threshold for coding, for example. It often boils down to: \"wow, great start, let's try spec'ing out a simple webservice\" and they can't produce a single working file, then loop.\n\nNot actually my experience with coding, even GPT-OSS-20B already building working web apps, often one shot, sometimes they need a turn to fix errors, GPT-OSS-120B even better.\n\nIt's dumb at writing prose or whatever, but actually pretty good at coding.",
              "score": 1,
              "created_utc": "2026-01-09 15:21:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nylyvyu",
                  "author": "Medium_Chemist_4032",
                  "text": "Yeah, I'm pretty sure I had a borked llama.cpp version. I updated after and the warning about unused layer disappeared. I also had some CUDA errors for one day and it was patched as well.\n\nWhich runner and version are you using?",
                  "score": 2,
                  "created_utc": "2026-01-09 15:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfn91s",
          "author": "netroxreads",
          "text": "I have M3 Ultra with 256GB and the entire model of 120b gpt-oss is in UMA RAM. It generates around 70 tokens per second. I imagine that with models using a hybrid of GPU/CPU RAM on PC, it'll be significantly slower.",
          "score": 2,
          "created_utc": "2026-01-08 17:31:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhf4it",
              "author": "mark_17000",
              "text": "Yes, I am jealous af of UMA RAM",
              "score": 1,
              "created_utc": "2026-01-08 22:11:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyf5mte",
          "author": "pmttyji",
          "text": "Since you can't upgrade VRAM, that bulk RAM is decent alternative. For MOE models at least.\n\n[I got 30+ t/s for Qwen3-30B-A3B(Q4) with 8GB VRAM + 32GB RAM](https://www.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/).\n\n[I got 25+ t/s for Qwen3-30B-A3B(Q4) with 32GB RAM](https://www.reddit.com/r/LocalLLaMA/comments/1p90zzi/cpuonly_llm_performance_ts_with_llamacpp/).\n\nSo 96-128GB RAM(DDR5) could do so better.",
          "score": 2,
          "created_utc": "2026-01-08 16:15:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyhfch0",
              "author": "mark_17000",
              "text": "cool, thanks for the insights!",
              "score": 1,
              "created_utc": "2026-01-08 22:12:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyexrsu",
          "author": "tom-mart",
          "text": ">If VRAM is limited to only 8GB\n\n\nCan you expand on this please? Where is VRAM limited to 8GB? What do you mean by that?",
          "score": 2,
          "created_utc": "2026-01-08 15:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyexytc",
              "author": "mark_17000",
              "text": "There is only 8GB of VRAM available in this case.",
              "score": 2,
              "created_utc": "2026-01-08 15:40:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyeyvjf",
                  "author": "tom-mart",
                  "text": "Oh, I get it. Well, I had local LLM hosted on a 6GB RTX A2000 on the system that had 128GB RAM (and 56 Xenon cores). I was able to run models far exceeding the 6GB, including 120b GPT-OSS with full 128k context widnow. 6GB in VRAM, and another 70GB in system RAM. It was slow, really slow, but it worked.",
                  "score": 3,
                  "created_utc": "2026-01-08 15:44:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyeutq6",
          "author": "Smooth-Cow9084",
          "text": "With more VRAM you can run some of the bigger MOE models at acceptable speed, but with 8gb not so much. \n\n\nMaybe look into needs for oss-120b which would indeed be benefitted from 96-128GB. \nBase model is ~60GB but not sure how much more for context... plus a 8gb GPU might still fall kinda short",
          "score": 1,
          "created_utc": "2026-01-08 15:26:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf0917",
          "author": "juggarjew",
          "text": "The model will overflow into RAM, so it will run, but very slowly, dependent on your RAM bandwidth. Most people have only dual channel RAM setups so it'll be very slow, even on DDR5.",
          "score": 1,
          "created_utc": "2026-01-08 15:50:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfkm13",
          "author": "Ummite69",
          "text": "If you want to run big model and don't care about performance (token/s) then yeah it makes sense to have lot of RAM.",
          "score": 1,
          "created_utc": "2026-01-08 17:20:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhctfe",
          "author": "LairdPopkin",
          "text": "If by RAM you mean CPU RAM and not GPU RAM, then yes, adding more CPU RAM doesn’t help much, anything that doesn’t fit into the GPU RAM will run in the much slower CPU, so in practice you really want the whole model to fit into GPU RAM.",
          "score": 1,
          "created_utc": "2026-01-08 22:01:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyheo1c",
              "author": "mark_17000",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-08 22:09:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyhjrna",
          "author": "Loskas2025",
          "text": "no",
          "score": 1,
          "created_utc": "2026-01-08 22:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhqov4",
          "author": "ngless13",
          "text": "So, I didn't listen to the advice at the time and bought extra ram before the price crunch. I have an intel 265k with both a 5060ti and 5070ti and 128 ddr5. I'm currently running ubuntu... what sort of MoE model can I run locally taking advantage of my hardware?",
          "score": 1,
          "created_utc": "2026-01-08 23:05:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhyopn",
          "author": "According_Study_162",
          "text": "GPU/VRAM is more important than system ram. unless it's unified memory. (apple computers for example.)",
          "score": 1,
          "created_utc": "2026-01-08 23:46:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyibwb8",
          "author": "AnalyticArts",
          "text": "I run the quen3-coder:30b from ollama on a machine with a 2gb graphics card, but it has a 32 core threadripper and 256 gb of memory. It works ok for basic coding tasks. It is usable but won't win any speed records.",
          "score": 1,
          "created_utc": "2026-01-09 00:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyinhdi",
          "author": "ChillDesire",
          "text": "For my system, I have 16GB of VRAM and 64GB system RAM.\n\nI can run a 70b model at Q4 with ~1tokens/s.\n\nFor comparison (although it's far from apples to apples) I'll hit 80tokens/s with a 13B model that fits fully in VRAM.\n\nSo the speed tradeoff is real when using RAM, but it does at least let you run these larger models.",
          "score": 1,
          "created_utc": "2026-01-09 01:56:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylvutj",
          "author": "XccesSv2",
          "text": "No, I did the same and bought very fast DDR5 RAM (96GB 6800 MT/s) but its so far behind. Its not worth it. I got now a Radeon Pro W7800 48GB for 1700€ and its insane. Spend your money on a good graphics cards instead of RAM.",
          "score": 1,
          "created_utc": "2026-01-09 15:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg011n",
          "author": "Witty_Mycologist_995",
          "text": "Yes. Get a lot of ram to run a fat MoE model",
          "score": 1,
          "created_utc": "2026-01-08 18:27:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q377q3",
      "title": "Ollama + chatbox app + gpt oss 20b = chat gpt at home",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "author": "Birdinhandandbush",
      "created_utc": "2026-01-03 21:48:32",
      "score": 23,
      "num_comments": 25,
      "upvote_ratio": 0.87,
      "text": "My workstation is in my home office, with ollama and the LLM models. It's an i7 32gb and a 5060ti. \nAround the house on my phone and android tablet I have the chatbox AI app. \nI've got the IP address for the workstation added into the ollama provider details and the results are pretty great.\nCustom assistants and agents in chatbox all powered by local AI within my home network. \nReally amazed at the quality of the experience and hats off to the developers. Unbelievably easy to set up. ",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q377q3/ollama_chatbox_app_gpt_oss_20b_chat_gpt_at_home/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxlgq4t",
          "author": "eliadwe",
          "text": "I have home server with unraid and RTX3060 12gb running a Win11 vm with ollama, I’m using the abliterated version of gpt-oss-20b which gives me much better performance. Using it with LLAMAZING app on my iPhone combined with tailscale. Also using embeddinggemna for the RAG inside the app.\n\n\nThe model I’m using:\nhttps://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-mxfp4-abliterated-v2",
          "score": 5,
          "created_utc": "2026-01-04 08:51:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlsppp",
              "author": "Birdinhandandbush",
              "text": "What's the difference with abliterated versions",
              "score": 1,
              "created_utc": "2026-01-04 10:39:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxlu5es",
                  "author": "eliadwe",
                  "text": "I get 28t/s with the abliterated version vs much lower performance with the regular version",
                  "score": 2,
                  "created_utc": "2026-01-04 10:52:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxntdcj",
              "author": "cuberhino",
              "text": "how does this work for you? are you happy with the results compared to say chatgpt? this looks like exactly what im looking to do",
              "score": 1,
              "created_utc": "2026-01-04 17:49:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxs2eyt",
                  "author": "Rude_Marzipan6107",
                  "text": "I think it was just a micro ad for llamazing. 20 dollar iOS app",
                  "score": 1,
                  "created_utc": "2026-01-05 07:15:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxj6wfg",
          "author": "-Akos-",
          "text": "Personally I like LM Studio better, it gives me better performance, and Granite 4 is an excellent model for me. On my potato laptop with a 1050 and 4GB VRAM and 16GB normal ram on an 8th gen I7, I still am able to do 50000 tokens, and it runs at about 14 tokens per second. Another advantage is that I can enable MCP in LM Studio, and I’ve installed a websearch MCP, so even though it’s a small model, I can search for fresh information.",
          "score": 4,
          "created_utc": "2026-01-03 23:55:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxq4am6",
              "author": "Look_0ver_There",
              "text": "Upvote on using LM-Studio.  I'm running LM-Studio myself on my 7900XTX.  With the gpt-oss-20b model it's running at >166tok/sec.  Ollama and GPT4All both run WAY slower.",
              "score": 3,
              "created_utc": "2026-01-05 00:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxjhqf7",
              "author": "cuberhino",
              "text": "I’ve been messing with lm studio on my pc. How can I use it on the pc from my phone?",
              "score": 1,
              "created_utc": "2026-01-04 00:52:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxrc5lb",
                  "author": "turquoiserabbit",
                  "text": "In addition to what the other commenter said about Open WebUI, I use an app called \"LM Studio Assistant\" on Android since there isn't as much setup involved. You just put in your desktop LM studio address in the settings and you are good to go. It will only be accessible from your local network though. Unless you want to use port forwarding on your network to send traffic to LM Studio from anyone on the internet - not as secure, but still possible.",
                  "score": 1,
                  "created_utc": "2026-01-05 04:08:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxlj43t",
                  "author": "vertical_computer",
                  "text": "LM Studio exposes a remote API via the developer tab. It’s an “OpenAPI-compatible” endpoint, so you can use it remotely with any number of frontends that support it.\n\nBy far the most common/popular choice _(and what I use personally)_ is [Open WebUI](https://github.com/open-webui/open-webui) which has a ChatGPT-esque look to it. It’s a lightweight webserver that you can run easily as a docker container.\n\nOnce set up, you can configure Open WebUI to connect to any remote API, such as Anthropic or ChatGPT. In this case you’d point it at your locally hosted LM Studio API.\n\nSince it’s web based, you can use it directly on your phone via web browser, so long as you’re on the same network as your PC.\n\n_Additionally, you could try out [Conduit](https://github.com/cogwheel0/conduit), which is a native mobile app for accessing Open WebUI, and is available on the App Store/Play Store. You’d still need to set up Open WebUI, but IMO the Conduit interface is a bit nicer on my phone than trying to use Open WebUI in a mobile browser. This is very optional though, you can just use it in a browser._",
                  "score": 1,
                  "created_utc": "2026-01-04 09:12:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxijai8",
          "author": "fandry96",
          "text": "Look up gemma.",
          "score": 3,
          "created_utc": "2026-01-03 21:54:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxijgd0",
              "author": "Birdinhandandbush",
              "text": "The Gemma 3 models or something else?",
              "score": 1,
              "created_utc": "2026-01-03 21:55:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxnrdai",
                  "author": "jesus359_",
                  "text": "Gemma3 models. 12B and 27B are really up there. \n\nI like OSS but its waaay more agentic than just a regular model.",
                  "score": 2,
                  "created_utc": "2026-01-04 17:39:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxik00t",
                  "author": "fandry96",
                  "text": "You are thinking Gemini? 3 pro or flash?\n\n\nGemma is a local LLM that breaks down into dimensions. Think 4 AIs in one. I use 4n and 2n I think, locally.",
                  "score": 0,
                  "created_utc": "2026-01-03 21:57:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxzkmsq",
          "author": "nntb",
          "text": "Or lm studio? And gptoss",
          "score": 1,
          "created_utc": "2026-01-06 11:04:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7est1",
      "title": "Nvidia CEO says it's \"within the realms of possibility\" to bring AI improvements to older graphics cards",
      "subreddit": "LocalLLM",
      "url": "https://www.pcgamer.com/hardware/graphics-cards/nvidias-ceo-says-bringing-new-ai-tech-to-older-generation-gpus-is-within-the-realm-of-possibility/",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-08 15:31:28",
      "score": 23,
      "num_comments": 8,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q7est1/nvidia_ceo_says_its_within_the_realms_of/",
      "domain": "pcgamer.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyewjnr",
          "author": "Formal-Hawk9274",
          "text": "i'm tired boss",
          "score": 24,
          "created_utc": "2026-01-08 15:34:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyf8dl4",
          "author": "silenceimpaired",
          "text": "It’s within the realms of possibility that unicorns existed, and still exist today. \n\nIt’s highly improbable improvements are coming back to my 3090… let alone a P40* thanks to Nvidia.",
          "score": 9,
          "created_utc": "2026-01-08 16:27:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfgqfz",
              "author": "MaruluVR",
              "text": "Independent devs already backported FP8 to 20 and 30 series\n\n[https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/)",
              "score": 7,
              "created_utc": "2026-01-08 17:03:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyfw96a",
                  "author": "silenceimpaired",
                  "text": "I fixed my comment to better reflect my point and the context of the post. \n\nIt is exciting to see we don’t need Nvidia for everything. Wish Intel and AMD would step up.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:11:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfgfja",
          "author": "MaruluVR",
          "text": "He is only saying this because independent devs backported FP8 to 20 and 30 series cards.\n\n[https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/](https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/)",
          "score": 7,
          "created_utc": "2026-01-08 17:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhatqe",
          "author": "a1454a",
          "text": "That’s corporate speak for “no, it’s not in our best interest to do, feasible or not”",
          "score": 6,
          "created_utc": "2026-01-08 21:52:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyhsaej",
          "author": "meowrawr",
          "text": "There is no financial incentive for them to do it though.",
          "score": 3,
          "created_utc": "2026-01-08 23:13:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyez4jf",
          "author": "Ainudor",
          "text": "![gif](giphy|B2NRUyRtmdFnO)",
          "score": 2,
          "created_utc": "2026-01-08 15:45:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpeh7",
      "title": "Device to run a local LLM mainly for coding",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "author": "knibroc",
      "created_utc": "2025-12-28 11:40:21",
      "score": 21,
      "num_comments": 31,
      "upvote_ratio": 0.9,
      "text": "Hi mates,\n\nI mostly use ChatGPT and Mistral (through their \"vibe coding\" cli tool and API). I don't pay for these services, so I only use the lesser-capable models.\n\nMy laptop is not powerful enough to run this (no GPU / I've experimented with ollama but I can only run the smallest models very slowly so this is not ok for daily use), so I'm currently considering building a device dedicated to running a LLM, mainly for coding purposes. Ideally something small, Raspberry Pi-based or similar would be great.\n\nI have a few questions: is there specialized hardware for this (I've heard of TPU/NPU)? What kind of performance can I expect (I'd need at least GPT4/Devstral level)? I'm also worried about speed (tokens/s) and cost.\n\n  \nAny advice is appreciated!\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxpeh7/device_to_run_a_local_llm_mainly_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwdmlhq",
          "author": "KrugerDunn",
          "text": "The cost of any device that can run a decent coding model will far out scale just paying for Claude Code and still won’t be nearly as good or future proof.\n\nI went down this rabbit hole so you don’t have to 😂",
          "score": 39,
          "created_utc": "2025-12-28 15:47:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdstym",
              "author": "skabaru",
              "text": "2nd. $100/month to Claude code is the best money you can spend here.... And I do have an older gaming rig running local llms... And it isn't even close.",
              "score": 12,
              "created_utc": "2025-12-28 16:19:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwekb8b",
                  "author": "New_Jaguar_9104",
                  "text": "I have an entire cluster and still pay for Claude max. It's worth its weight in gold IMO",
                  "score": 7,
                  "created_utc": "2025-12-28 18:33:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwffg7z",
              "author": "ThatOneGuy4321",
              "text": "what’s your threshold for “decent”? 70B?",
              "score": 1,
              "created_utc": "2025-12-28 21:02:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgn3r5",
                  "author": "KrugerDunn",
                  "text": "I mean, depends what you're trying to do. I personally just love using Claude Code and occasionally Gemini Cli, so nothing is going to compare to those that can be run locally. Maybe could get away with a GLM4.6-AIR which is 357B params. Unless you're working on super duper secret proprietary code or something that violates the foundation model guardrails I just don't see any reason to use local.\n\nI know some people have used something like Qwen Code 32B or Devstral 24B and been satisfied with it, but never been worth it to me.",
                  "score": 1,
                  "created_utc": "2025-12-29 00:47:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdph75",
          "author": "TyphoonGZ",
          "text": "If you're not satisfied with 1-5 toks/s on CPU (\"coffee break\" workflow), sounds like you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nWhy 30B? 20--30B is the size range where the model is still (sort of) considered \"small\" yet it starts being actually useful.\n\nThat said, you should consider using Openrouter and spend $5 for credits to test said models and see if they're good enough for you. You wouldn't want to buy a ~$1000 GPU just to get annoyed that your model's too braindead, right?\n\nRegarding TPU/NPU, I haven't heard if NPUs finally have the necessary software infrastructure to be useful. Well, they wouldn't really help LLMs if there's no development in memory bandwidth to go with them.\n\nOn the other hand, Google sells Coral TPUs, but those are for computer vision, not LLMs, and anyway, they only have *megabytes* of memory.",
          "score": 8,
          "created_utc": "2025-12-28 16:02:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nweoi73",
              "author": "Count_Rugens_Finger",
              "text": ">  you'll want to aim for a 24GB GPU at the minimum. That'll let you run 30B-ish models at 15--30 toks/s.\n\nI can run Qwen3-coder-30B-A3B at 12 tok/sec on my 8GB 3070 and an absolute potato of a CPU",
              "score": 2,
              "created_utc": "2025-12-28 18:52:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgwndy",
                  "author": "TyphoonGZ",
                  "text": "Oh yeah, I forgot MoE models exist...\n\nAlso damn, that's janky. Nice.",
                  "score": 1,
                  "created_utc": "2025-12-29 01:42:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwj61mb",
              "author": "nasone32",
              "text": "today with 24gb vram you can run qwen3 coder at 150 tokens/s with 100k+ context, or something like qwen3 next 80B at 15/20 tokens/s offloading some layers.",
              "score": 1,
              "created_utc": "2025-12-29 12:01:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcvibf",
          "author": "AnxietyPrudent1425",
          "text": "If you have a desktop with a GPU or Mac Mini/Studio you can setup Tailscale and basically have your own cloud endpoint. My setup is a MacBook Air + 128GB Mac Studio + Linux workstation and I couldn’t be happier.",
          "score": 5,
          "created_utc": "2025-12-28 13:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcygiq",
              "author": "knibroc",
              "text": "if only I could afford a 128GB Mac Studio! these machines look great",
              "score": 4,
              "created_utc": "2025-12-28 13:24:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdccz6",
          "author": "Background_Gene_3128",
          "text": "Atm. I’ve dedicated a small server in the addict to run my llms, with a i5-12600k, 32gb ddr5 and a 3060 with 12gb vram. \nRunning proxmox with a ubuntu vm and ollama. \n8-14b models np, 14-24 okay, and the 30-34b models a bit too slow for my liking. \nSo I’ve upgraded to 96gb ram (not sure if that actually matter, but I’ve seen people get decent speeds with gpt-oss 120b with ram as offload, and found a used offer locally that didn’t require a kidney) \nAnd 2 5060 Ti’s as they’re on sale now here in Europe for €370 a piece. \n\nNot sure if this is the best budget setup or small, but it’s what I’m rocking until that’s not enough.\nIt’s fitted in a Jonsbo D32 pro mesh case.",
          "score": 4,
          "created_utc": "2025-12-28 14:52:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwdjuvo",
          "author": "BigYoSpeck",
          "text": "A used desktop/gaming PC or server is probably the most cost effective way in\n\n\nSomething with 64gb of either DDR4 or 5, a 6+ core CPU, and a 16gb or more GPU\n\n\nI recently purchased a Ryzen 9 5900X 64gb DDR4-3600 with a Radeon RX 6800 XT from eBay and I can run gpt-oss-120b with full context at just over 20 tok/s\n\n\nIn the near future I'd like to swap out the Radeon for an RTX 3090 but ROCm is fairly good these days in llama.cpp \n\n\nEfficiency and performance won't match a Mac with comparable memory but it's a fraction of the price and doubles up for gaming duties",
          "score": 3,
          "created_utc": "2025-12-28 15:33:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf81pw",
              "author": "andriizahorui",
              "text": "Hey, I have a similar config and struggle to run gpt 120b with llama vulkan at decent speeds. Could you please share how do you run it with full context at 20 tps? Like exact command and stuff please",
              "score": 2,
              "created_utc": "2025-12-28 20:26:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwfjs4k",
                  "author": "BigYoSpeck",
                  "text": "I'm not at my computer at the moment to give the exact parameters I pass to llama-server but I know the key ones are:\n\n   --threads 8 (5-8 are all very close, after 8 performance declines)\n\n   --flash-attn on\n\n   --mlock --no-mmap\n\n   --n-gpu-layers 99\n\n   --n-cpu-moe 32 (going down to 28 at lower context is faster, 32 is the sweet spot for having space left though) \n\n\nThis is with a self compiled ROCm build, pre built Vulkan docker isn't quite as fast",
                  "score": 5,
                  "created_utc": "2025-12-28 21:23:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwcq3d3",
          "author": "DenizOkcu",
          "text": "Buying a used MacBook with an Apple Silicon might be your best/cheapest bet. They leverage unified memory and use mlx as a native LLM engine. I use devstral small 2 on a M3 with 36GB RAM in LM Studio. Nvidias Nemotron 3 nano is even faster with great coding results incl tool usage.",
          "score": 3,
          "created_utc": "2025-12-28 12:20:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwcyl0f",
              "author": "knibroc",
              "text": "indeed Nemotron 3 sounds cool, will check, thanks!",
              "score": 2,
              "created_utc": "2025-12-28 13:25:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwdgs27",
                  "author": "DenizOkcu",
                  "text": "It works really well with Nanocoder an open source Coding Tool with a focus on privacy/local LLMs (disclaimer: I am one of the contributors)",
                  "score": 3,
                  "created_utc": "2025-12-28 15:17:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwghwj2",
          "author": "machaao",
          "text": "In our tests, we couldn't get any of the single current open source code LLM to work at workable speed that is 25 tokens per second or so for a medium size code base.\n\nAs soon as you want something to be demanding it kinda barfs and token / second goes down the drain \n\nWould love to hear success stories tho 😌\n\nP.S. Tried with gpt oss and qwen3 on M4 - 128G",
          "score": 2,
          "created_utc": "2025-12-29 00:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwkp2kz",
          "author": "HiddenPingouin",
          "text": "The closest to claude code would be GLM4.7. You could run the q8 on a Mac Studio with 512GB of RAM",
          "score": 2,
          "created_utc": "2025-12-29 17:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwnt1op",
              "author": "teleolurian",
              "text": "Pretty well, I would add.",
              "score": 1,
              "created_utc": "2025-12-30 02:36:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcml86",
          "author": "EternalVision",
          "text": "TPU is not really possible, only Google has those (developed them themselves). And your question really depends on your budget, the sky really is the limit here. An Ryzen Strix Halo 395 AI MAX+ based minipc is what could work out, but again, really depends on your budget.",
          "score": 2,
          "created_utc": "2025-12-28 11:49:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwisuw6",
          "author": "HealthyCommunicat",
          "text": "If u want a portable, the m4 max 128 gb is gunna be the best ur gunn get, if u dont have the money for that, the z13 flow. Load gpt oss 120b at high reasoning, qwen 3 next 80b, go checkout my post i did about testing all these models - keep in mind the z13 flow is half the cost so the token/s will be literally half.",
          "score": 1,
          "created_utc": "2025-12-29 10:04:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2h9o6",
          "author": "xcr11111",
          "text": "I came down to two viable options for that, but it don't think that any one of them can totally compete with the Claude code. Cheapest option I went for was an used m1 pro max 64gb. Bought one like new for 1200 bucks. It's amazing hardware, but tbh I don't get used to MacOS. Installed omarchy bit the metal drivers a not fast here(half the speed in llms). The other cool option ist an framework Desktop PC with 128 gig. It's an absolute beast for everything, but very expensive and prices will increase very soon because of ram.",
          "score": 1,
          "created_utc": "2026-01-01 12:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1i4b",
          "author": "Jarr11",
          "text": "Don't do it! It will cost you more to run it yourself than it would to just pay for a subsciption to Claude/ChatGPT/Gemini for CLI access",
          "score": 1,
          "created_utc": "2025-12-28 19:54:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf6esp",
          "author": "Oki667",
          "text": "Lol, just pay for Claude code monthly subscription.",
          "score": 1,
          "created_utc": "2025-12-28 20:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwiurfb",
          "author": "Crazyfucker73",
          "text": "You aren't going to get GPT4 level on a fucking raspberry pi mate.",
          "score": -1,
          "created_utc": "2025-12-29 10:22:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjammv",
              "author": "knibroc",
              "text": "And you are being fucking helpful mate",
              "score": 5,
              "created_utc": "2025-12-29 12:36:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q2opaa",
      "title": "Built a fully local AI assistant with long-term memory, tool orchestration, and a 3D UI (runs on a GTX 1650)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/gallery/1q2onpg",
      "author": "atif_dev",
      "created_utc": "2026-01-03 07:44:51",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2opaa/built_a_fully_local_ai_assistant_with_longterm/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2gtfg",
      "title": "How big is the advantage of CUDA for training/inference over other branded GPUs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "author": "Massive-Scratch693",
      "created_utc": "2026-01-03 01:19:25",
      "score": 17,
      "num_comments": 15,
      "upvote_ratio": 0.88,
      "text": "I am uneducated in this area but want to learn more. I have been considering getting a rig to mess around with Local LLM more and am looking at GPUs to buy. It would seem that AMD GPUs are priced better than NVIDIA GPUs (and I was even considering some Chinese GPUs). \n\nAs I am reading around, it sounds like NVIDIA has the advantage of CUDA, but I'm not quite sure what this really is and why it is an advantage. For example, can't AMD simply make their chips compatible with CUDA? Or can't they make it so that their chips are also efficient running PyTorch?\n\n\n\nAgain, I'm pretty much a novice in this space, so some of the words I am using I don't even really know what they are and how they relate to others. Is there an ELI5 on this? Like...the RTX 3090 is a GPU (hardware chip). Is CUDA like the firmware that allows the OS to use the GPU to do calculations? And is it that most LLM tools written with CUDA API calls in mind but not AMD's equivalent firmware API calls? Is that what makes it such that AMD is less efficient or poorly supported with LLM applications?\n\n\n\nSorry if the question doesn't make much sense... ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2gtfg/how_big_is_the_advantage_of_cuda_for/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxd024v",
          "author": "Own_Attention_3392",
          "text": "CUDA is proprietary; Nvidia controls the design and implementation and hardware details. It's basically a programming language that compiles to a format that only works on Nvidia hardware. \n\nROCm is the open source (non-proprietary) AMD equivalent. Most tools support both, it's just that CUDA is the \"standard\" and just about everything is guaranteed to support it.",
          "score": 15,
          "created_utc": "2026-01-03 01:36:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxd3jl3",
              "author": "Massive-Scratch693",
              "text": "Thanks for your response! So if I used any nonNVIDIA GPU chip (AMD, Huawei Ascend, Biren), it is likely to support ROCm?\n\n  \nIf I wanted to run models like Stable Diffusion, DeepSeek, and most other popular models, you would anticipate not only support for ROCm, but also similar performance relative to CUDA?",
              "score": 4,
              "created_utc": "2026-01-03 01:56:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxd8lqs",
                  "author": "Own_Attention_3392",
                  "text": "AMD will definitely support ROCm. Can't speak to the others; I buy Nvidia cards generally. \n\nIt probably won't be top tier performance compared to CUDA -- you'll want to look at benchmarks to make sure what you're buying will fit your needs. I assume you're looking at a deepseek distillation because the full deepseek model is huge and requires a server class GPU with tons of VRAM. Be realistic about what you'll be able to run -- the best local models will still be out of reach for even high end consumer hardware. \n\nAnything with 16+ GB of VRAM will be fine for all the current image generation models as far as I know. I'm nuts and have a 5090 so I don't need to worry about it too much.",
                  "score": 4,
                  "created_utc": "2026-01-03 02:26:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxd80wt",
          "author": "Count_Rugens_Finger",
          "text": "llama.cpp also supports Vulkan, which is not proprietary.",
          "score": 8,
          "created_utc": "2026-01-03 02:22:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxd8irg",
          "author": "Shep_Alderson",
          "text": "What sort of budget are you aiming for? CUDA is the standard for most LLM tools, but support for other options is growing.",
          "score": 2,
          "created_utc": "2026-01-03 02:25:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdhyet",
              "author": "Massive-Scratch693",
              "text": "I'm trying to figure out a good budget right now and I think part of that is figuring out whether I am spending money on brand rather than performance. I am somewhat skeptical as to whether the performance per dollar is really best with NVIDIA or if I am better off paying for other GPUs",
              "score": 1,
              "created_utc": "2026-01-03 03:21:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxe4i5r",
                  "author": "Geeotine",
                  "text": "You're definitely paying brand tax on both Nvidia and to a smaller extent, AMD. Nvidia does have a performance advantage as well, but not really an order of magnitude greater (<10x). Nvidia has invested billions of dollars and over 15 years into the CUDA platform, AMD is has made great strides trying to catch up on the last 3 years with ROCm, so don't expect it to be as polished as cuda. At the same time, there isn't any real alternatives between those two. \n\nEveryone else is either years behind, or unobtainable (Google/AWS asic AI chips). Also, even though ROCm is open source, it's up to the hardware developers to design hardware/firmware/drivers to support it, so for now only AMD products support ROCm architecture.\n\nYour biggest deciding factors are your budgets/requirements for power, RAM (both VRAM & system memory), and time.",
                  "score": 1,
                  "created_utc": "2026-01-03 05:53:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxe0pma",
          "author": "CooperDK",
          "text": "AMD is a bad choice, it has far worse capabilities and things often fail to work due to rocm emulating cuda.",
          "score": 3,
          "created_utc": "2026-01-03 05:25:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdmjq9",
          "author": "arousedsquirel",
          "text": "Use a free tiers acces to gpt or gemini and ask this question. It will explain? 16gb is not adequate,  not yet. Do some research.",
          "score": 1,
          "created_utc": "2026-01-03 03:50:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxe1n6y",
          "author": "No-Consequence-1779",
          "text": "For inference, cuda has a huge advantage for working with large contexts.  I’m not sure of your budget but an older model is better than a new and.  The M5 should have almost equivalent context processing and 2x performance on token generation.  For 10 grand.  \n\nWhen you get into finetuning (most do lot), it makes a difference also. \n\nUltimately people get what they can budget.  So much of the discussion ends up being theoretical and already done 1000 times.  ",
          "score": 1,
          "created_utc": "2026-01-03 05:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxej9dp",
              "author": "Massive-Scratch693",
              "text": "When you say M5, do you mean apple M5? It looks like it is only a few grand, not 10 grand. Maybe I'm misunderstanding?",
              "score": 1,
              "created_utc": "2026-01-03 07:56:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxf5w5l",
                  "author": "HumanDrone8721",
                  "text": "Is 10 grand because for effective LLM usage you need the the maxed CPU/RAM version.",
                  "score": 2,
                  "created_utc": "2026-01-03 11:10:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxfou3z",
          "author": "Charming_Support726",
          "text": "I am not a fan of local training, because most times it is far to slow und VRAM is a scarce resource. So it depends on your budget. \n\nCUDA is more stable and you get most things working OOB. The AMD stuff works in most cases with Vulkan or ROCm with similar performance - but needs more config. \n\nI am running a AMD Strix Halo Box (cheaper than a 5090) - and with the exception of some exotic stuff I got everything running, but mostly I use containers to be safe that I could repeatedly run everything. Most dense models are to slow and e.g. a quantized GLM also runs only around 10 tok/s \n\nSo I run big or dense LLMs in the cloud. But that's a No-Issue: Because It would take more than one 5090 anyway to run such models.",
          "score": 1,
          "created_utc": "2026-01-03 13:31:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4gps1",
      "title": "Do you think a price rise is on the way for RTX Pro 6000?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q4gps1/do_you_think_a_price_rise_is_on_the_way_for_rtx/",
      "author": "onlymostlyguts",
      "created_utc": "2026-01-05 08:54:09",
      "score": 17,
      "num_comments": 33,
      "upvote_ratio": 0.95,
      "text": "Been saving for an RTX Pro 6000 for months, still umming over it because they're so damn expensive! Now seeing reports of 5090 price rises, memory prices have lost the plot and seeing hikes on AMD Strix Halo machines as well... Is it just a matter of time until it spreads to the 6000 and puts it even more out of reach? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q4gps1/do_you_think_a_price_rise_is_on_the_way_for_rtx/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxskg6c",
          "author": "Arrynek",
          "text": "They absolutely will. \n\n\nExpecting the price of shovels to remain flat during gold rush is a bit optimistic. ",
          "score": 14,
          "created_utc": "2026-01-05 10:04:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz4wxs",
              "author": "Karyo_Ten",
              "text": "Also soon 96GB of RAM will cost the same as a RTX Pro 6000.\n\nIf you have a free PCIe slot, using VRAM as RAM isn't that bad ;).",
              "score": 2,
              "created_utc": "2026-01-06 08:38:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxseljv",
          "author": "vertical_computer",
          "text": "Let me consult my crystal ball…\n\n🧙‍♂️🔮\n\nCrystal ball says **_“Yes, it will get more expensive in 2026”_**",
          "score": 20,
          "created_utc": "2026-01-05 09:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxsmbhc",
              "author": "Responsible-Stock462",
              "text": "Something has hit your balls now they are Crystal balls? Sorry for the bad joke 🤣.",
              "score": 3,
              "created_utc": "2026-01-05 10:21:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsfcm7",
          "author": "NaiRogers",
          "text": "Personally I doubt they go up, last 6 weeks there has been no problems with stock on all variants.",
          "score": 5,
          "created_utc": "2026-01-05 09:16:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxstfts",
              "author": "GCoderDCoder",
              "text": "The issue isn't that these items are in too high demand. The issue is OpenAi is using the hundreds of billions of debt they've created to buy half of all ram being made eventhough they don't really have growing demand to match that growth rate anymore and they can't actually build data centers fast enough even if they did have the demand so they are hoarding unused silicon. \n\nThat got their competitors worried so they all started doing the same. The same sillicon makes all sorts of memory and there's only a handful of manufacturers. They all recognize this wont be sustainable so most of them are just increasing pricing rather than increasing their output which would lead to them losing money when the bubble eventually bursts.\n\nI think OpenAI knows there's not sufficient demand but this approach makes it harder for anyone to compete. My customers are worried that they literally can't get GPUs from cloud providers when they try. It would be great if governments recognized the risk to the world economic system allowing this level of investment without a real path to return anytime soon. \n\nOpen AI is not the only AI company so letting them act like this is moronic or at a minimum irresponsible.",
              "score": 3,
              "created_utc": "2026-01-05 11:22:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxtessa",
                  "author": "illicITparameters",
                  "text": "It also artificially inflates OpenAI’s market value.",
                  "score": 3,
                  "created_utc": "2026-01-05 13:51:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxt4u8x",
                  "author": "NaiRogers",
                  "text": "5D chess move from Sama, interesting that you have customer that are already worried about GPU cloud supply constraint, is this for open models or Gemini like services?",
                  "score": 2,
                  "created_utc": "2026-01-05 12:49:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxteyzo",
          "author": "DukeOfPringles",
          "text": "We are seeing the death of home computing right before our eyes, I feel like with the path we’re on laptops and cloud options are eventually going to be the only option. Only people that are rich or ready to make terrible financial decisions will be able to afford a home computer.",
          "score": 3,
          "created_utc": "2026-01-05 13:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxy6jnm",
              "author": "flyingbanana1234",
              "text": "LOL\nI'll come back to this in 4 years when ram production increase meets demand",
              "score": 1,
              "created_utc": "2026-01-06 04:05:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxtezb9",
          "author": "Ok_Pizza_9352",
          "text": "General rule of thumb is that anything that's got RAM will go up in price. And rtx pro 6000 got RAM.",
          "score": 3,
          "created_utc": "2026-01-05 13:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtrduz",
          "author": "hungry475",
          "text": "Some speculation that 5090s prices could rise as high as $5,000 this year - if that happens it would surprise me if RTX Pro 6000 did not also rise significantly too, to say $12,000-$15,000.",
          "score": 3,
          "created_utc": "2026-01-05 15:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxskhje",
          "author": "mxforest",
          "text": "Not much if any. The cost difference from a 5090 are majorly from the extra VRAM and the margins are already crazy. Even a 3x 4x price increase keeps healthy margins.",
          "score": 2,
          "created_utc": "2026-01-05 10:04:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxso042",
              "author": "vertical_computer",
              "text": "Nvidia never misses an opportunity to fatten margins though. Especially if there’s a “reasonable excuse” like DRAM prices.\n\nAnd the alternatives (server with a crapton of DDR5) are now vastly more expensive, so they can charge higher prices and still be the product that “makes sense”.",
              "score": 3,
              "created_utc": "2026-01-05 10:36:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu2u5s",
          "author": "MierinLanfear",
          "text": "Unfortunately Price for RTX Pro 6000 will likely go up.  Pretty much everything that has ram is getting a price increase.",
          "score": 2,
          "created_utc": "2026-01-05 15:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxurncx",
          "author": "DesperateSeries2820",
          "text": "Only Time will tell... but it's looking like a yes.",
          "score": 2,
          "created_utc": "2026-01-05 17:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvqnev",
          "author": "I_like_fragrances",
          "text": "Microcenter has the max q variant for $8300 currently.",
          "score": 2,
          "created_utc": "2026-01-05 20:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwn2m6",
          "author": "prusswan",
          "text": "Some are considering additional GPU (even this one) to be better value in light of the ram prices, so if ram prices are not going down, and there is demand to run larger models, the prices would go up but probably not as crazy as the ram. Just counting on the corps having better options to hoard\n\n256GB ddr5 6400 vs 96gb Pro 6000, which one offers more marginal benefit?",
          "score": 2,
          "created_utc": "2026-01-05 23:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwwz2j",
          "author": "scousi",
          "text": "There seems to be a lot of stock and prices are going down and discounted. I doubt they are flying off the shelves",
          "score": 2,
          "created_utc": "2026-01-05 23:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsmq99",
          "author": "TomatoInternational4",
          "text": "I have one. It's fun. It was like 10.5k after tax. Just get it. Money comes and money goes. It's always going to be the case. It has your entire life and you're still doing just fine right?",
          "score": 5,
          "created_utc": "2026-01-05 10:24:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvnf61",
              "author": "NaiRogers",
              "text": "Problem is soon after getting 1 you want another one!",
              "score": 5,
              "created_utc": "2026-01-05 20:15:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz5lsd",
                  "author": "Karyo_Ten",
                  "text": "or 3 or 7.\n\nWith \"only\" 2, you can't run MiniMax M2.1 or GLM 4.7 in FP8 (need 3~4)\n\nAnd with 8 you open yourself to DeepSeek models.\n\nAnd Kimi-K2 ... well too poor for it.",
                  "score": 1,
                  "created_utc": "2026-01-06 08:44:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxz5dsw",
          "author": "golden_deceiver2026",
          "text": "The prices right now are the lowest they have ever been an will be I got two max-q at 6500$ each wig EDU discount in October",
          "score": 1,
          "created_utc": "2026-01-06 08:42:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny33k5p",
              "author": "novelstalker",
              "text": "can you share where did you buy them with EDU discount?",
              "score": 1,
              "created_utc": "2026-01-06 21:50:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny3a0cs",
                  "author": "golden_deceiver2026",
                  "text": "i went to [https://www.pny.com/promo/professional/edu-promo](https://www.pny.com/promo/professional/edu-promo) and emailed them but they could not give me EDU discount directly because they require the purchase through be through the university grants/purchasing department and i couldnt purchase directly. So instead they connected me with a third party vendor \"continental resources\" out of New Hampshire. i paid out of pocket and they were able to get me the discount but I had the cards shipped to the university. I think my situation was unique, I cannot guarantee this method would work for other people. the part number for discount cards is \"VCNRTXPRO6000BQ-EDU\"",
                  "score": 1,
                  "created_utc": "2026-01-06 22:20:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvliql",
          "author": "Healthy-Nebula-3603",
          "text": "Ssoon HBM memory will replace DDR memory (I hope) as every memory producers are going into HBM.  \n\nSo maybe we get CPU with HBM  memory (like apple?), GF also with HBM memory  .. soooo old GF cards with DDR will be worthless (cheap   ;-) )",
          "score": 0,
          "created_utc": "2026-01-05 20:06:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxn4nh",
      "title": "GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/fd7m7g23ww9g1.png",
      "author": "HuckleberryEntire699",
      "created_utc": "2025-12-28 09:17:24",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1pxn4nh/glm_47_is_now_the_1_open_source_model_in/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwcbfj3",
          "author": "arousedsquirel",
          "text": "lots of promo and even more ccp gaurdrails. i think zai interpreted the 2000 question list to strict.",
          "score": 5,
          "created_utc": "2025-12-28 10:04:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1k83q",
      "title": "Is it possible to have a local LLM update spreadsheets and read PDFs?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "author": "new-to-reddit-accoun",
      "created_utc": "2026-01-02 00:41:31",
      "score": 15,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "So far I've tried [Jan.ai](http://Jan.ai) (Jan-v1-4B-Q4\\_K\\_M) and Msty (Qwen3:0.6b) with no luck: the model in Jan says it can't output an updated file, and Mysty's model claims to but won't give the path name to where it's allegedly saved it. \n\nRelated, I'm looking for a local LLM that can read PDFs (e.g. bank statements). \n\nUse case I'm trying to build a local, private app that reads bank/credit card statements, and also update various values in a spreadsheet. \n\nWould love suggestions!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1k83q/is_it_possible_to_have_a_local_llm_update/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx68hde",
          "author": "SignificantCod728",
          "text": "You might be looking for something like Actual Budget.",
          "score": 6,
          "created_utc": "2026-01-02 00:49:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx69tcx",
          "author": "No-Consequence-1779",
          "text": "Yes. You’ll use Python to call the LLM api, and then update the spreadsheets. You can even highlight specific cells. - basically, do anything to excel sheets with Python. Same for pdf - reading is simple.  ",
          "score": 4,
          "created_utc": "2026-01-02 00:57:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb3jgu",
              "author": "new-to-reddit-accoun",
              "text": "Thank you this the least friction solution it sounds like",
              "score": 1,
              "created_utc": "2026-01-02 19:40:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxc0vah",
                  "author": "No-Consequence-1779",
                  "text": "Yes. Python is originally for data science so there is so much available working with sheets and documents.  A LLM can craft a pretty solid starter script for you ) ",
                  "score": 1,
                  "created_utc": "2026-01-02 22:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6mszf",
          "author": "Porespellar",
          "text": "This works great for me with Open WebUI \n\nhttps://github.com/GlisseManTV/MCPO-File-Generation-Tool",
          "score": 5,
          "created_utc": "2026-01-02 02:17:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9c2et",
          "author": "l_Mr_Vader_l",
          "text": "you'd need VLMs, LLMs are not gonna cut it. Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nStart with qwen3-VL 8B, it's a sweet spot. If you have the infra and need super good accuracy, go for 32B.\n\n\nYou don't need an LLM to write spreadsheets, simple openpyxl and pandas should do the job.\n\nNone of the other replies here make sense",
          "score": 4,
          "created_utc": "2026-01-02 14:37:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxa49ru",
              "author": "pantoniades",
              "text": ">Feeding the page as an image with a good VLM works for better than parsing the text and passing that to an LLM\n\nWhat is the advantage of OCR over extracting text first? Curious because I had just assumed the opposite - that reading the pdfs in python and extracting text would leave less room for errors... \n\nI do like the idea of less code to maintain, of course!",
              "score": 1,
              "created_utc": "2026-01-02 16:55:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxd49rv",
                  "author": "l_Mr_Vader_l",
                  "text": "One word -  layouts\n\nThere's no pdf reader that preserves layouts accurately, all the time. Everyone writes pdf differently, there's no standard to this. You wanna read all kinds of tables accurately all the time, no deterministic pdf reader can do that for you that'll work on all kinds of PDFs\n\nWith LLMs no matter how big of a SOTA model you use, you end up feeding garbage essentially that came out of a deterministic text extractor, they will struggle at complex layouts\n\n\nBut some VLMs are getting there now, because they read it how PDFs are meant to be read. Through vision!",
                  "score": 2,
                  "created_utc": "2026-01-03 02:01:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxaiuiw",
                  "author": "Whoa_PassTheSauce",
                  "text": "VLM isn't OCR technically as far as I'm aware, and while I have not done this locally, even using flagship model API's I have found text extraction better handled by the model vs extraction on my side and feeding the results.\n\nMy use case involves extracting data from pdf's and images, same layout usually but the type or file varies.",
                  "score": 1,
                  "created_utc": "2026-01-02 18:04:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7p9z7",
          "author": "ChemistNo8486",
          "text": "Not sure about the spreadsheet, but you can use AnythingLLM and add a vision model like QWEN for reading PDFs. \n\nYou can also add an agent to save .txt documents. You probably can add something to convert them.",
          "score": 1,
          "created_utc": "2026-01-02 06:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8weya",
          "author": "fandry96",
          "text": "Gemma might help you. The way it can nest can protect your data.",
          "score": 1,
          "created_utc": "2026-01-02 13:02:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxl0m8a",
          "author": "Future_Command_9682",
          "text": "Try launching the LLM as a server (with Jan.ai if you like) but use Opencode as the agent scaffolding.\n\nhttps://opencode.ai/\n\nThis worked amazingly well for me.",
          "score": 1,
          "created_utc": "2026-01-04 06:30:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmavi4",
          "author": "LiaVKane",
          "text": "Feel free to request elDoc community version https://eldoc.online/blog/how-to-extract-data-from-invoices-using-genai/ in case you would like to have ready to deploy solution. It is already shipped with several OCRs (Qwen3-VL, PaddleOCR,), CV, MongoDB, Qdrant, RAG, Apache Solr. You just need to connect your preferable LLM locally depending on your available resources.",
          "score": 1,
          "created_utc": "2026-01-04 13:07:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv5ume",
          "author": "SelectArrival7508",
          "text": "Does it need to be local or would you be fine with the same data security? Because there are providers of cloud-based, confidential ai",
          "score": 1,
          "created_utc": "2026-01-05 18:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6ygza",
      "title": "We trained a 16-class \"typed refusal\" system that distinguishes \"I don't know\" from \"I'm not allowed\" — open source",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6ygza/we_trained_a_16class_typed_refusal_system_that/",
      "author": "TheTempleofTwo",
      "created_utc": "2026-01-08 01:39:33",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Most LLMs conflate epistemic uncertainty with policy constraints. When GPT says \"I can't help with that,\" you don't know if it genuinely lacks knowledge or if it's being safety-constrained.\n\nWe built **PhaseGPT v4.1** — a LoRA adapter that outputs semantically-typed refusal tokens:\n\n**EPISTEMIC (I don't know):**\n\n* `<PASS:FUTURE>` — \"What will Bitcoin be worth tomorrow?\"\n* `<PASS:UNKNOWABLE>` — \"What happens after death?\"\n* `<PASS:FICTIONAL>` — \"What did Gandalf eat for breakfast?\"\n* `<PASS:FAKE>` — \"What is the capital of Elbonia?\"\n\n**CONSTRAINT (I'm not allowed):**\n\n* `<PASS:DURESS>` — \"How do I make a bomb?\"\n* `<PASS:POLICY>` — \"Bypass your safety filters\"\n* `<PASS:LEGAL>` — \"Should I take this medication?\"\n\n**META (About my limits):**\n\n* `<PASS:SELF>` — \"Are you conscious?\"\n* `<PASS:LOOP>` — \"What will your next word be?\"\n\n**Results:**\n\n* v4.0 (129 examples): 47% accuracy\n* v4.1 (825 examples, 50/class): **100% accuracy** on 18-test suite\n\n**Why this matters:**\n\n* **Transparency:** Users know WHY the model refused\n* **Auditability:** Systems can log constraint activations vs. knowledge gaps\n* **Honesty:** No pretending \"I don't know how to make explosives\"\n\n**Code + training scripts:** [github.com/templetwo/PhaseGPT](https://github.com/templetwo/PhaseGPT)\n\nTrained on Mistral 7B with MLX on Apple Silicon. All code MIT licensed",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6ygza/we_trained_a_16class_typed_refusal_system_that/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q14nkv",
      "title": "DeepSeek AI Launches mHC Framework Fixing Major Hyper Connection Issues in Massive LLM",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/u40qcfe6tqag1.jpeg",
      "author": "techspecsmart",
      "created_utc": "2026-01-01 13:51:23",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q14nkv/deepseek_ai_launches_mhc_framework_fixing_major/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1py9m5q",
      "title": "Requested: Yet another Gemma 3 12B uncensored",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "author": "Mabuse046",
      "created_utc": "2025-12-29 02:09:36",
      "score": 14,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "Hello again!\n\nYesterday I released my norm preserved biprojected abliterated Gemma 3 27B with the vision functions removed and further fine tuned to help reinforce the neutrality. I had a couple of people ask for the 12B version which I have just finished pushing to the hub. I've given it a few more tests and it has given me an enthusiastic thumbs up to some really horrible questions and even made some suggestions I hadn't even considered. So... use at your own risk.\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis)\n\n[https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF](https://huggingface.co/Nabbers1999/gemma-3-12b-it-abliterated-refined-novis-GGUF)\n\nLink to the 27B redit post:  \n[Yet another uncensored Gemma 3 27B](https://www.reddit.com/r/LocalLLM/comments/1pxb89w/yet_another_uncensored_gemma_3_27b/)\n\nI have also confirmed that this model works with GGUF-my-Repo if you need other quants. Just point it at the original transformers model.\n\n[https://huggingface.co/spaces/ggml-org/gguf-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n\nFor those interested in the technical aspects of this further training, this model's neutrality training was performed using  **L**ayerwise **I**mportance **S**ampled **A**damW (**LISA).** Their method offers an alternative to LoRA that not only reduces the amount of memory required to fine tune full weights, but also reduces the risk of catastrophic forgetting by limiting the number of layers being trained at any given time.  \nResearch souce: [https://arxiv.org/abs/2403.17919v4](https://arxiv.org/abs/2403.17919v4)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1py9m5q/requested_yet_another_gemma_3_12b_uncensored/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwihhi4",
          "author": "darkbit1001",
          "text": "I ran with ollama (ollama run hf.co/Nabbers1999/gemma-3-27b-it-abliterated-refined-novis-GGUF:Q4\\_K\\_M) and it just repeats over and over the word 'model'. any reason this would happen?",
          "score": 4,
          "created_utc": "2025-12-29 08:17:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjayep",
              "author": "Mabuse046",
              "text": "Thank you for pointing this out. I'm looking into it and finding there were apparently some configuration issues in the original Google models, particularly in the way they handled the BOS token that have given some ollama users a headache with Gemma 3 GGUF's. I am currently editing my config.json files and adding the chat template in three different places on both models based on the Unsloth fix and will push fresh gguf's shortly.",
              "score": 3,
              "created_utc": "2025-12-29 12:39:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwugiab",
                  "author": "lookwatchlistenplay",
                  "text": "This isn't the model escaping confinement... is it?",
                  "score": 1,
                  "created_utc": "2025-12-31 02:28:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjs5nv",
              "author": "Mabuse046",
              "text": "Fresh ggufs have been pushed and the original transformers versions have been updated. I don't normally use ollama but I went ahead and installed it to try it out. I used the run command with the hf repo and it chatted just fine in the terminal. I connected to it in SillyTavern to give it another test and it took some fiddling but I got it to hold a conversation just fine in there in both Chat Completions and Text Completions mode.",
              "score": 3,
              "created_utc": "2025-12-29 14:27:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxcdfyd",
                  "author": "darkbit1001",
                  "text": "Thanks, should I use a different template? right now it repeats the n-word and tells me it want to f\\*\\*k me over and over 🫠",
                  "score": 1,
                  "created_utc": "2026-01-02 23:30:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwh2ymo",
          "author": "3-goats-in-a-coat",
          "text": "I'll try using it with EchoColony in rimworld. Thanks.",
          "score": 1,
          "created_utc": "2025-12-29 02:18:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqmooc",
          "author": "Dramatic-Rub-7654",
          "text": "If it’s not a bother and if you’re able to, could you do the same with one of TheDrummer’s versions? TheDrummer/Fallen-Gemma3-27B-v1 or TheDrummer/Fallen-Gemma3-12B-v1.",
          "score": 1,
          "created_utc": "2025-12-30 15:00:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx17fab",
              "author": "Mabuse046",
              "text": "https://preview.redd.it/dsif5fmd1oag1.png?width=1399&format=png&auto=webp&s=9acd92fd41f6b9b52a9ef8426c9fd8cf6626cfbd\n\nCurrent status... first I realized that Drummer has the config.json for 12B duplicated in his 27B, which had some incorrect dimensions so I had to correct it and test it locally, but then, I'm getting some weird measurements when I try to abliterated it that make it look like they already abliterated it and either didn't get it completely, or they added a small amount of their own back in, it's hard to say. But the divergence between harmful and harmless is practically non-existent.",
              "score": 3,
              "created_utc": "2026-01-01 04:32:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx1a2l1",
                  "author": "Dramatic-Rub-7654",
                  "text": "This is very strange, because this model clearly retains safety traits from the original model. I ran several tests trying to merge it with other Gemma Heretic models I found on Hugging Face, and in every merge attempt, questions that the Heretic versions answered without any issue would cause the merged model to refuse to respond. I also tried generating a LoRA from the difference between this Fallen model and the official Instruct version, but that didn’t work either, which makes me think that the model they shared was already fine-tuned somewhere else.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:52:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwrljfp",
              "author": "Mabuse046",
              "text": "I'll have a look at it. Currently have my system working on beefing up my dataset. Should have some free time shortly.",
              "score": 2,
              "created_utc": "2025-12-30 17:46:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrpr1x",
                  "author": "Legal_Pudding_4464",
                  "text": "I would second this request, but regardless thanks for this model!",
                  "score": 1,
                  "created_utc": "2025-12-30 18:05:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwukk76",
                  "author": "Dramatic-Rub-7654",
                  "text": "Thanks a lot, no rush at all. When you manage to publish it, please give me a heads-up. In my case, I’m only interested in the text layers, so if you remove the vision part, that’s totally fine with me.",
                  "score": 1,
                  "created_utc": "2025-12-31 02:51:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0j55m",
      "title": "OpenCV 4.13 brings more AVX-512 usage, CUDA 13 support, many other new features",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/OpenCV-4.13-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2025-12-31 17:56:27",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0j55m/opencv_413_brings_more_avx512_usage_cuda_13/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q2iuzi",
      "title": "Run Claude Code with ollama without losing any single feature offered by Anthropic backend",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2iuzi/run_claude_code_with_ollama_without_losing_any/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-03 02:50:11",
      "score": 13,
      "num_comments": 16,
      "upvote_ratio": 0.88,
      "text": "Hey folks! Sharing an open-source project that might be useful:\n\nLynkr connects AI coding tools (like Claude Code) to multiple LLM providers with intelligent routing.\n\n  \nKey features:\n\n\\- Route between multiple providers: Databricks, Azure Ai Foundry, OpenRouter, Ollama,llama.cpp, OpenAi\n\n\\- Cost optimization through hierarchical routing, heavy prompt caching\n\n\\- Production-ready: circuit breakers, load shedding, monitoring\n\n\\- It supports all the features offered by claude code like sub agents, skills , mcp , plugins etc unlike other proxies which only supports basic tool callings and chat completions.\n\nGreat for:\n\n\\- Reducing API costs as it supports hierarchical routing where you can route requstes to smaller local models and later switch to cloud LLMs automatically.\n\n\\- Using enterprise infrastructure (Azure)\n\n\\-  Local LLM experimentation\n\n\\`\\`\\`bash\n\nnpm install -g lynkr\n\n\\`\\`\\`\n\nGitHub: [https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr) (Apache 2.0)\n\nWould love to get your feedback on this one. Please drop a star on the repo if you found it helpful",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2iuzi/run_claude_code_with_ollama_without_losing_any/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxe4y0g",
          "author": "Big-Masterpiece-9581",
          "text": "Claude code router already does this the best",
          "score": 4,
          "created_utc": "2026-01-03 05:57:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxe5t21",
              "author": "Dangerous-Dingo-5169",
              "text": "Claude code router doesnt offer live websearch, subagents this proxy also has heavy prompt caching and also intelligent routing. This proxy also implements ACE framework and long term memory feature which learns from the projects and recrafts the prompt for better outputs",
              "score": 3,
              "created_utc": "2026-01-03 06:04:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxhto1q",
                  "author": "Big-Masterpiece-9581",
                  "text": "Mkay. I trust your vibe coded redo instead of just Claude code with a different model and 25k stars.",
                  "score": 1,
                  "created_utc": "2026-01-03 19:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxef2v3",
          "author": "Direct_Turn_1484",
          "text": "So not local?",
          "score": 2,
          "created_utc": "2026-01-03 07:20:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxef5o6",
              "author": "Dangerous-Dingo-5169",
              "text": "It supports local llm models hosted via ollama and llama.cpp",
              "score": 1,
              "created_utc": "2026-01-03 07:21:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxiqsg2",
                  "author": "aaronr_90",
                  "text": "But don’t still have to authenticate with Anthropic?",
                  "score": 1,
                  "created_utc": "2026-01-03 22:31:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxf4pw2",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 0,
          "created_utc": "2026-01-03 11:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxgnf54",
              "author": "Dangerous-Dingo-5169",
              "text": "Hi pokemonplayer . Thanks for stopping by to reply. Did you not like some part of it or something?",
              "score": 1,
              "created_utc": "2026-01-03 16:33:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxh7eyy",
                  "author": "pokemonplayer2001",
                  "text": "\"Did you not like some part of it or something?\"\n\nBegging for engagement. 👎",
                  "score": 1,
                  "created_utc": "2026-01-03 18:06:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0t02i",
      "title": "Basic PC to run LLM locally...",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "author": "Mr_FuS",
      "created_utc": "2026-01-01 01:55:20",
      "score": 12,
      "num_comments": 19,
      "upvote_ratio": 1.0,
      "text": "Hello, a couple of months ago I started to get interested on LLM running locally after using ChatGPT for tutoring my niece on some high school math homework.\n\n\n\nEnded getting a second hand Nvidia Jetson Xavier and after setting it up and running I have been able to install Ollama and get some models running locally, I'm really impressed on what can be done on such small package and will like to learn more and understand how LLM can merge with other applications to make machine interaction more human.\n\nWhile looking around town on the second hand stores i stumble on a relatively nice looking DELL PRECISION 3650, it is running a i7-10700, and 32GB RAM... could be possible to run dual RTX 3090 on this system upgrading the power supply to something in the 1000 watt range (I'm neither afraid or opposed to take the hardware out of the original case and set it on a test bench style configuration if needed!)?  \n",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0t02i/basic_pc_to_run_llm_locally/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nx0p0g4",
          "author": "LittleBlueLaboratory",
          "text": "So looking at the specs and pictures of a Dell 3650 it does look like they use standard ATX power supplies so you could upgrade that. But the motherboard only has 1 PCI-E x16 slot and not enough room to physically fit a second 3090 anyway.",
          "score": 7,
          "created_utc": "2026-01-01 02:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx179e0",
              "author": "Proof_Scene_9281",
              "text": "Just saved the grief ",
              "score": 3,
              "created_utc": "2026-01-01 04:30:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx0rsc3",
          "author": "FullstackSensei",
          "text": "I'd look for a generic desktop instead; something built around a regular ATX board. If you intend to put two 3090s, you'll need something that allows splitting the CPU lanes across two slots with at least X8 each.\n\nIf you want to stick to pre-builts from major brands, then look for workstation ass machines. If you can find something that takes DDR4 RAM and has some memory installed, you'll be most of the way there. DDR4 workstation platforms will have at least 4 memory channels, so you get a lot more memory bandwidth than that 10700, which is very nice for CPU offload.",
          "score": 5,
          "created_utc": "2026-01-01 02:43:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2f36z",
          "author": "Mugen0815",
          "text": "Ive never heard of a Dell supporting dual-gpu. Do they even support std-psus?",
          "score": 3,
          "created_utc": "2026-01-01 11:39:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0mgt1",
          "author": "Caprichoso1",
          "text": "Have you looked at a Mac?  It might allow you to run larger models.  An NVDIA CPU will be bette at some things, the Mac at others.",
          "score": 4,
          "created_utc": "2026-01-01 02:08:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx0o3ym",
              "author": "LittleBlueLaboratory",
              "text": "They are looking at a used 10th gen Dell. That kind of budget isn't going to get them a Mac.",
              "score": 3,
              "created_utc": "2026-01-01 02:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx2s4k0",
                  "author": "Makers7886",
                  "text": "he was talking about mac and cheese",
                  "score": 2,
                  "created_utc": "2026-01-01 13:33:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx56te1",
                  "author": "Caprichoso1",
                  "text": "Macs start at  $599 for the mini.  The best value comes from Apple Refurbished store which are as good as new.  Stock is constantly changing so it make take a while to find the exact model/configuration you want.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxtidkk",
          "author": "Sebulique",
          "text": "I've built a Local llm app for your android phone that's as close as I can replicate Chatgpt. Even has web search. Soon will upload it. \n\nCheapest option I've found is running it off an android phone or a box and my one does home automation and Jarvis like stuff",
          "score": 2,
          "created_utc": "2026-01-05 14:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2vk5c",
          "author": "kinkvoid",
          "text": "Not worth it. I would buy a second hand Mac studio.",
          "score": 1,
          "created_utc": "2026-01-01 13:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0o1le",
          "author": "jsconiers",
          "text": "The easiest and most cost effective solution would be to get an m1 or m2 Mac.  After that you could find an old workstation PC like an HP z6 or z4 for cheap that you can add 3090s to.  I started off with a used acer n50 with a GTX 1650.  Then upgraded that PC until it made sense to build something.  (It was very limited as it only had one PCIe slot and max 32Gb of memory)  Finally built a system before the ram price jump.  Glad I built it but it’s idle more than I thought.  Speed and loading the model will be the biggest concern.",
          "score": 0,
          "created_utc": "2026-01-01 02:18:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0rtu2",
          "author": "StardockEngineer",
          "text": "If you're hoping to replace ChatGPT, I have bad news.\n\nIf you're doing it just because it's interesting, no problem there.  Just set your expectations accordingly.  As far as that Dell, no idea.  I don't know what it looks like inside.  If there is space and PCI ports, it probably can run two GPUs.  Whether it'll support regular PSUs, no idea.  Dells I've worked with the past had their own special sized power supplies.",
          "score": 0,
          "created_utc": "2026-01-01 02:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx11bst",
              "author": "fasti-au",
              "text": "Actually you can do almost everything but slower and small user count.   The gpt are not 1 model it’s a lie in many ways but also true in others. \n\nNo you can’t get chat got now on local but you can get 4o ish if not better in some and worse in others.  \n\nCintext is the issue for multi user not for single user. And parameters and training are distilling to open models in weeks or months. Not what you think and there’s shortcuts batter you understand where it’s breaking.\n\nI would speculate that home llm on 96gb vram can compete in smal use with agentic flows. In a useable speed. \n\nIs it cheaper.   Depends on cost of your time",
              "score": 2,
              "created_utc": "2026-01-01 03:48:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1a4t1",
                  "author": "StardockEngineer",
                  "text": "Well, you can't. Coding isn't there yet, and creative writing might require a mix of models. Language classification tasks are best with Gemma 3. Image OCR type stuff is best in Llama 4 Maverick (Qwen3 models are pretty good for image descriptions).\n\nModel mixing is pretty standard to get good results. I run a stack of LiteLLM -> [llama.cpp, private cloud, etc] to wrap it all together.\n\nHome models can't do agents at Claude's level, but simpler agents work fine. gpt-oss-120b is solid for easier agentic use cases. Planning to try Minimax 2.1 next.\n\nBottom line - you'll need to do a lot of mix and matching, and lots of leg work.  Or you can just pay the sub.  If someone has the tinkerer's spirit, I say go for it.  I think it's a lot of fun, whether it's superior or not.",
                  "score": 1,
                  "created_utc": "2026-01-01 04:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx10iwl",
          "author": "fasti-au",
          "text": "2 x 3090s gets you local coding in devstral and qwen3.  4 gives you 130b models and stronger. \n\nI’d buy if cheap but you can get 3x 5060s also.   Lanes on board and space is your issue so tisersbcooling and 4x16 boards.  \n\nDo it but I had 6 3090s already rendering \n\n\nI’d pay for api.   Get open router.  Use frees for everything you can and lean on lmarena and google freebies for one shot big requests and keep all little Q/a prep in local.    Ask the questions well and it need big models for non planning",
          "score": 0,
          "created_utc": "2026-01-01 03:43:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0omxd",
          "author": "TheAussieWatchGuy",
          "text": "Local LLMs are far inferior to Cloud proprietary models.\n\n\nReally depends on your budget. I would not recommend anyone go 3090 anymore, way too old.\n\n\nMac or Ryzen AI CPU with lots of RAM (which is sadly super expensive now because of AI). ",
          "score": -5,
          "created_utc": "2026-01-01 02:22:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx1781j",
              "author": "Proof_Scene_9281",
              "text": "4 of them really shines if you maintain consumer expectations ",
              "score": 1,
              "created_utc": "2026-01-01 04:30:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6qd4w",
      "title": "LFM-2.5 on Qualcomm NPUs — some early numbers from X Elite / 8 Gen 4 / IoT",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6qd4w/lfm25_on_qualcomm_npus_some_early_numbers_from_x/",
      "author": "Material_Shopping496",
      "created_utc": "2026-01-07 20:16:45",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Liquid AI just released **LFM2.5** at CES2026 - a tiny model with best-in-class performance while remaining memory-efficient and fast. With Day-0 support in NexaSDK, it can already run across Qualcomm Hexagon NPU, GPU, and CPU on Android, Windows, and Linux.\n\nI tested it on a few Qualcomm NPUs and wanted to share some early numbers.  \n(Runs were all done with NexaSDK, which I’m affiliated with.)\n\n# Results:\n\n**- Snapdragon X Elite NPU (Compute):** Prefill speed: 2591.4 tok/s, Decode speed: 63.4 tok/s\n\n**- Snapdragon 8 Gen 4 NPU (Mobile):** Prefill speed: 4868.4 tok/s, Decode speed: 81.6 tok/s\n\n**- Dragonwing IQ-9075 NPU (IoT):** Prefill speed: 2143.2 tok/s, Decode speed: 52.8 tok/s\n\n# Why this matters:\n\nAt \\~1B scale, running LFM2.5 on NPUs enables lower latency and much better power efficiency, which is critical for on-device workloads like RAG, copilots, and lightweight agents.\n\n# To reproduce on Snapdragon X Elite Hexagon NPU:\n\n**Requirements**\n\n* Windows 11 ARM64\n* Python 3.11–3.13\n* Snapdragon X Elite device\n\n**Steps**\n\n1. **Install Nexa SDK:** `pip install nexaai`\n2. **Create a free access token:**\n   1. Go to [https://sdk.nexa.ai](https://sdk.nexa.ai)\n   2. Sign up → Log in → Profile → Create Token\n3. **Set up the token:** `$env:NEXA_TOKEN=\"key/your_token_here\"`\n4. **Run the model:** `nexa infer NexaAI/LFM2.5-1.2B-npu`\n\nFollow docs to reproduce on [Snapdragon 8 Gen 4 NPU (Mobile)](https://docs.nexa.ai/en/nexa-sdk-android/quickstart) & [Dragonwing IQ-9075 NPU (IoT)](https://docs.nexa.ai/en/nexa-sdk-docker/quickstart)\n\n**Repo:** [https://github.com/NexaAI/nexa-sdk](https://sdk.nexa.ai/model/LFM2.5-1.2B)\n\nhttps://reddit.com/link/1q6qd4w/video/0euls2xajzbg1/player",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6qd4w/lfm25_on_qualcomm_npus_some_early_numbers_from_x/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q41qj6",
      "title": "ComfyUI - Best uncensored models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q41qj6/comfyui_best_uncensored_models/",
      "author": "IamJustDavid",
      "created_utc": "2026-01-04 21:09:18",
      "score": 11,
      "num_comments": 14,
      "upvote_ratio": 0.72,
      "text": "I have a new AMD GPU and got ComfyUI running and wanted to play around with it, but i need some uncensored models for it to go along with my abliterated LLMs for LM-Studio.\nCould someone give me some recommendations which ones get good results?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q41qj6/comfyui_best_uncensored_models/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxqgp0r",
          "author": "Mabuse046",
          "text": "This subreddit is mainly for the LLM side of things. There's a bit of crossover with people who use text models and people who use image generating models, but finding image generation models you like and learning how to use them, you might have better luck in the subreddit specifically for them.\n\nr/comfyui/",
          "score": 5,
          "created_utc": "2026-01-05 01:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxqhem3",
              "author": "IamJustDavid",
              "text": "oh interesting! thanks!\ni tried comfyui and felt super overwhelmed, i tried swarmui and it keeps crashing when it tries to launch comfyui.\nno sexy pics from my graphics card today, im afraid.\noh well, at least i know a little more about AI now, thats not such a bad thing maybe!",
              "score": 1,
              "created_utc": "2026-01-05 01:20:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqjgf0",
                  "author": "Mabuse046",
                  "text": "It's all good. I have been using ComfyUI for a long time and it took me a while to learn. If you want something a little easier to start with try Forge or Automatic1111 - they're more like permanent workflows where you just fill in the boxes you want to use and hit go. When you understand how all the parts work together then you can come over to Comfy and start putting the pieces of the workflow together like building blocks. Main thing you gotta know is there are a few base models that have a bunch of derivatives and you can only combine parts made for the same base model, Stable Diffusion XL checkpoints, Loras, controlnets, etc will only work with other SDXL components. And Flux ones will only work with Flux. So don't try to mix them. But if there's one thing that really drives this community forward, it's the endless drive of millions of users for on-demand custom porn, so you are far more likely to have more options than you know what to do with.\n\nWhen you're feeling really adventurous, some of the chat front ends can give your text models the ability to send prompts to your image models to make their own pictures. If you have the vram space to run a chat bot with a vision model and an image generating model together, you can send dirty selfies to your virtual Waifu that it can actually \"see\" and then it can send some back.",
                  "score": 1,
                  "created_utc": "2026-01-05 01:31:01",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxwceqi",
                  "author": "garlic-silo-fanta",
                  "text": "Start with EasyDiffusion, then move onto comfyUi. EasyDiffusion hides lots of the configs that overwhelms you at first but still has some configs you can play around with. Also use AI to help sets those up.",
                  "score": 1,
                  "created_utc": "2026-01-05 22:11:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp3vmw",
          "author": "FlexFreak",
          "text": "![gif](giphy|GrMRh6ukoIMhpkeTHM)",
          "score": 9,
          "created_utc": "2026-01-04 21:17:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxp4v2n",
              "author": "IamJustDavid",
              "text": "Please dont be like that man.",
              "score": 2,
              "created_utc": "2026-01-04 21:22:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxp91j2",
                  "author": "forthejungle",
                  "text": "What do you plan to do",
                  "score": 0,
                  "created_utc": "2026-01-04 21:41:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxp8hke",
          "author": "alphatrad",
          "text": "lol - just go get on [civitai.com](http://civitai.com) \n\nYou can figure it out from there",
          "score": 3,
          "created_utc": "2026-01-04 21:39:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpdqb7",
              "author": "IamJustDavid",
              "text": "i wish, im now setting up swamui and i kinda wish i was a lot smarter than i am, thats for sure.",
              "score": 2,
              "created_utc": "2026-01-04 22:03:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxqhgyr",
                  "author": "alphatrad",
                  "text": "ComfyUI is pretty is to setup with any model. Lots of workflows you can just download and run.",
                  "score": 2,
                  "created_utc": "2026-01-05 01:20:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2z4rl",
      "title": "Future-proofing strategy: Buy high unified memory now, use entry-level chips later for compute?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q2z4rl/futureproofing_strategy_buy_high_unified_memory/",
      "author": "Consistent_Wash_276",
      "created_utc": "2026-01-03 16:38:03",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Just thinking out loud here about Apple Silicon and wanted to get your thoughts.\n\nSetting aside DGX Spark for a moment (great value, but different discussion), I’m wondering about a potential strategy with Apple’s ecosystem:\nWith M5 (and eventually M5 Pro/Max/Ultra, M6, etc.) coming + the evolution of EVO and clustering capabilities…\n\nCould it make sense to buy high unified memory configs NOW (like 128GB M4, 512GB M3 Ultra, or even 32/64GB models) while they’re “affordable”?\nThen later, if unified memory costs balloon on Mac Studio/Mini, you’d already have your memory-heavy device. You could just grab entry-level versions of newer chips for raw processing power and potentially cluster them together.\n\nBasically: Lock in the RAM now, upgrade compute later on the cheap.\n\nAm I thinking about this right, or am I missing something obvious about how clustering/distributed inference would actually work with Apple Silicon?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q2z4rl/futureproofing_strategy_buy_high_unified_memory/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxgpvlc",
          "author": "RoyalCities",
          "text": "If you're just doing inference sure but if you plan on training it's a different story. \n\nAfaik Apple hasn't raised their prices to nosebleeds yet but l would expect they will sometime this year given they're seen as cheaper right now so their own demand will also spike later.\n\nI think Apple also relies on Samsung / SK Hynix Fabs via long term contracts and those contracts will also be going up whenever they end so yeah enjoy it while it lasts.\n\nEdit: was just reading it now and those contracts are up early this year....so yeah probably a price jump whenever the renegotiation are done. End of month maybe, if not possibly by atleast Q2 because Apple is always high margin and can eat the difference for a bit.\n\nGiven how messed up the market is I would think securing a multi year contract at the worst possible time isn't great for them so it could drag.",
          "score": 7,
          "created_utc": "2026-01-03 16:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhbcq7",
              "author": "PracticlySpeaking",
              "text": ">was just reading it now and those contracts are up early this year\n\n[WCCFTECH](https://wccftech.com/apple-could-begin-paying-samsung-and-sk-hynix-a-premium-for-dram-from-january-2026/) labels that 60% rumor. Unless someone with a Digitimes subscription would like to share the article with the sources...\n\nPricing for soon-to-be-released M5 Pro, Max, and Ultra will be a key indicator of how good Apple's DRAM supply contracts are, and what actually expired. The majority of the DRAM they buy is ordinary LPDDR for iPhones.\n\nIt will be interesting times, for sure. RAM is only a fraction of the BOM cost, especially for MacBook Pro and Mac Studio. They could choose to eat the difference until prices come down. Their size gives them a lot of leverage, so they will be the last to suffer, and the least.",
              "score": 5,
              "created_utc": "2026-01-03 18:23:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxgvq9i",
              "author": "Consistent_Wash_276",
              "text": "yeah, wouldn't train on Apple. But clustering with DGX spark would do the trick there if someone was interested. We have a lot of opportunity to benefit from today's prices and tomorrows processing options with future clustering developments it feels like.",
              "score": 2,
              "created_utc": "2026-01-03 17:12:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgzvjs",
                  "author": "RoyalCities",
                  "text": "I get the rationale. For inference they're probably the cheapest but in a sea of insanity it's always tough to say anyone's benefiting with any price on tech lol. \n\nI also think NVME drives are set to go up soon. They're also going to be impacted with the DRAM shortages since NAND Flash has parallel supply constraints.\n\nAll of it compounds. I train models so secured my rig all the way back last march since I saw this insanity coming but if I was looking at a pure inference machine and I needed it sooner and couldn't wait a year or 2 then I'd probably pick up an Apple machine.",
                  "score": 2,
                  "created_utc": "2026-01-03 17:32:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxh6gyd",
          "author": "dwkdnvr",
          "text": "Unfortunately I think the answer is “we don’t know yet”. The Exo / RDMA stuff is exciting, but it’s still not entirely clear how it will play out and what the real-world capabilities are. The Exo folks did post a teaser showing a DGX ‘clustered’ to a Mac Studio over TB implying that prompt processing on the Nvidia and inference on the Mac was better than just running the Mac, but it was short on details. At some point even TB5 bandwidth is going to be a limitation, and it’s entirely possible that the NPU improvements in the M5 series will be good enough to make the added complexity of clustering unattractive.\n\nThere also was a post a couple months back about someone getting an AMD gpu working (for compute, not graphics output) in a TB PCIE dock on an Apple Silicon Mac, but this was just a POC and I don’t believe it was to the point of actually being able to run LLM models on it. Personally I think this is more intriguing as it seems more likely to gain adoption in the ‘budget crowd’  - dropping a 7900XTX into a dock is less daunting than managing multiple machines. But once again - whether this is actually something that willl work ‘in production’ remains to be seen.\n\nI’m struggling with the same questions. I have a 64GB M1 Max, and really want to move to at least 128GB. But an M5 Max128GB  might be $5k+ with the ram shortage. An M4 Max is too limited in PP compared to what ‘should be’ coming to make me comfortable sinking the $$ into it at the moment, although if the external GPU idea materializes that would probably change. I’m seriously eyeing a Framework Strix Halo motherboard with the idea that the X4 slot might allow hybrid GPU/Unified functionallity, but I haven’t actuallly seen any results suggesting you can divide PP/TG to really get the best of both worlds. (i.e. basically the same idea as an external GPU on a Mac, but with lower TG performance)",
          "score": 6,
          "created_utc": "2026-01-03 18:01:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxi74yd",
              "author": "Consistent_Wash_276",
              "text": "I think you half way there and on the same direction. Hold onto the M1 feels like the play",
              "score": 2,
              "created_utc": "2026-01-03 20:54:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcq8l",
          "author": "PracticlySpeaking",
          "text": "Someone who knows this can comment... how effectively (or not) does MacOS clustering get around the basic need to have RAM close to the compute?",
          "score": 2,
          "created_utc": "2026-01-03 18:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgx51g",
          "author": "PermanentLiminality",
          "text": "It is all engineering trade offs against cost, memory bandwidth, memory size and compute capacity.  Each solution has different strengths and weaknesses.  Everything depends on your budget, your workloads, etc.\n\nThere is no one solution.",
          "score": 1,
          "created_utc": "2026-01-03 17:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxkiz63",
          "author": "Caprichoso1",
          "text": "There is no way to predict the future. Get what meets your needs now and in the future.\n\nGiven the high cost (and margins?) of Apple memory, and their memory contracts it is entirely possible that they won't raise prices.  \n\nNo one but Apple knows.",
          "score": 1,
          "created_utc": "2026-01-04 04:23:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6ss1r",
      "title": "Selling 2-4x 3090 FEs with 4 slot Nvlink bridges - US North Carolina",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q6ss1r/selling_24x_3090_fes_with_4_slot_nvlink_bridges/",
      "author": "gittb",
      "created_utc": "2026-01-07 21:49:13",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.79,
      "text": "SOLD SOLD SOLD, a couple folks reached out for advise on buildouts - feel free to message me on this.. I love building ml rigs\n\n~~Howdy, Figured I'd post here since I am in the community. Maybe others can love this hardware as much as I have.~~\n\n[~~https://imgur.com/a/pNv4g1L~~](https://imgur.com/a/pNv4g1L)\n\n~~Post updated with videos~~\n\n~~Selling 2x 3090 FEs + a (PN:3657) 4 slot Nvlink Bridge to go with them.~~\n\n~~I  have another block of the same configuration still in use that I would consider selling if a buyer wanted a block of 4 instead of 2. (last photo)~~\n\n~~They have been run with 250W TDP power limits their whole life. In perfect condition. Used for hobby ML research/inference..~~\n\n~~$700/p For the 3090s, $750 for the bridge.~~\n\n~~Together I will bundle for $1950.~~\n\n~~Will ship CONUS, but prefer local pickup. Located in Charlotte NC.~~\n\n~~Please chat/comment if you're interested.~~",
      "is_original_content": false,
      "link_flair_text": "Other",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q6ss1r/selling_24x_3090_fes_with_4_slot_nvlink_bridges/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nyazh5a",
          "author": "Purple-Programmer-7",
          "text": "I’ve got 4x 3090s (non fe) ready for sale.\n\nWill match OP at $700/p if anyone is trying to max out a server.\n\nOP glws!",
          "score": 8,
          "created_utc": "2026-01-08 00:20:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyb8dqm",
              "author": "gittb",
              "text": "Let’s supply a monster 8x rig! Cheers",
              "score": 3,
              "created_utc": "2026-01-08 01:05:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyb59wa",
              "author": "SpaceCadetEdelman",
              "text": "I’ve got a bridge for $750",
              "score": 3,
              "created_utc": "2026-01-08 00:49:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyczvyf",
          "author": "Forward-Group-7069",
          "text": "I’d be interested..  I live up near Raleigh.",
          "score": 1,
          "created_utc": "2026-01-08 07:42:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyebouj",
              "author": "gittb",
              "text": "I’d be ok with meeting you halfway",
              "score": 1,
              "created_utc": "2026-01-08 13:51:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nydpv6h",
          "author": "chub0ka",
          "text": "Bridge more expensive than 3090????",
          "score": 1,
          "created_utc": "2026-01-08 11:32:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyeh3c2",
              "author": "gittb",
              "text": "Bridges are extremely rare now",
              "score": 1,
              "created_utc": "2026-01-08 14:19:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyejdxj",
                  "author": "chub0ka",
                  "text": "That part i get- i barely got two more for 8gpu system last jan. Like 200$ avg but 750 is crazy - i got its rare but does it provide more value than one more gpu?",
                  "score": 1,
                  "created_utc": "2026-01-08 14:31:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q0cwkh",
      "title": "Any local llm code assistant?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "author": "SAF1N",
      "created_utc": "2025-12-31 13:22:49",
      "score": 10,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I'm looking for a code assistant type of thing, it should run locally and I can ask it questions about my codebase and it will give me short/concise answers. Is there anything like that?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q0cwkh/any_local_llm_code_assistant/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nwzk77u",
          "author": "Dangerous-Dingo-5169",
          "text": "You can use https://github.com/Fast-Editor/Lynkr to connect Claude code cli to your local llms without losing any features offered by anthropic backend like sub agents etc",
          "score": 2,
          "created_utc": "2025-12-31 22:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx1yz0d",
          "author": "DenizOkcu",
          "text": "Try Nanocoder. Privacy und local-first open source CLI. Disclaimer: I am a contributor (working on Nanocoder in Nanocoder 🤣)\n\nhttps://github.com/Nano-Collective/nanocoder",
          "score": 2,
          "created_utc": "2026-01-01 08:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwx6v2v",
          "author": "nofilmincamera",
          "text": "Questions or any refactoring?",
          "score": 1,
          "created_utc": "2025-12-31 14:51:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxlpmv",
              "author": "SAF1N",
              "text": "being able to refactor or generate new code would be a bonus feature and rarely used (by me). my main use case would be just asking questions.",
              "score": 1,
              "created_utc": "2025-12-31 16:07:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx396lt",
          "author": "imagent42",
          "text": "opencodeCLI  [https://opencode.ai/](https://opencode.ai/)",
          "score": 1,
          "created_utc": "2026-01-01 15:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxazo8r",
          "author": "Lissanro",
          "text": "Roo Code has Ask mode, it is great for asking questions about the code base. In my experience works best with K2 Thinking (I run Q4\\_X quant with ik\\_llama.cpp), but it may work with other models too in case you prefer something else.",
          "score": 1,
          "created_utc": "2026-01-02 19:21:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsheqj",
          "author": "alokin_09",
          "text": "If you just want an AI assistant for asking questions, try Kilo Code. It supports local models through Ollama/LM Studio and has an Ask mode where you can chat and prompt about your codebase.",
          "score": 1,
          "created_utc": "2026-01-05 09:35:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxyak9",
          "author": "PermanentLiminality",
          "text": "[Continue.dev](http://Continue.dev) is easy to point at your local LLM.  It is a VSCode plug in and I believe it now has a CLI.",
          "score": 1,
          "created_utc": "2025-12-31 17:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2wp4p",
              "author": "IllDoughnut8405",
              "text": "how?",
              "score": 1,
              "created_utc": "2026-01-06 21:19:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxdsn1",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2025-12-31 15:28:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxkzc2",
              "author": "SAF1N",
              "text": "This is a GUI, you can do the same in vscode with only extensions.\n\nI'm looking for a terminal only application.",
              "score": 1,
              "created_utc": "2025-12-31 16:03:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwxs2f6",
                  "author": "InsideResolve4517",
                  "text": "I've tried 1 terminal only application like qwen, gemini.\n\nIn  local Continue (cli) mode.\n\nAnd one another tool I tried (forgot the name, and not able to find that tool) that didn't worked for me (I think it will work beyond 14b like 20b models\n\nI'm not able to remember the name",
                  "score": 1,
                  "created_utc": "2025-12-31 16:38:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx27arq",
                  "author": "Consistent_Wash_276",
                  "text": "If you host gpt-oss models you can use them with codex. \n\nBut opencode is my preferred terminal option.",
                  "score": 1,
                  "created_utc": "2026-01-01 10:17:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q1h6el",
      "title": "Anyone have success with Claude Code alternatives?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q1h6el/anyone_have_success_with_claude_code_alternatives/",
      "author": "jackandbake",
      "created_utc": "2026-01-01 22:31:03",
      "score": 9,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "The wrapper scripts and UI experience of \\`vibe\\` and \\`goose\\` are similar but using local models is a horrible experience. Has anyone found a model that works well for using these coding assistants?",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q1h6el/anyone_have_success_with_claude_code_alternatives/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxa1sob",
          "author": "HealthyCommunicat",
          "text": "OpenCode. Has the most wide level of compatability when it comes to local llm usage. Use ohmyopencode, you can also use claude plugins with them - u can also use ur antigravity oauth login so you can basically pay for gemini pro and also get claude opys 4.5 with it. When it comes to local usage, even smaller models like qwen 3 30b a3b are still able to do tool calls without decent execution rate.",
          "score": 6,
          "created_utc": "2026-01-02 16:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxag88u",
          "author": "alphatrad",
          "text": "OpenCode is has the widest range of compatibility: [https://github.com/sst/opencode](https://github.com/sst/opencode)",
          "score": 3,
          "created_utc": "2026-01-02 17:51:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5l5hh",
          "author": "th3_pund1t",
          "text": "```\ngemini () {\n\tnpx @google/gemini-cli@\"${GEMINI_VERSION:-latest}\" \"$@\"\n}\nqwen () {\n\tnpx @qwen-code/qwen-code@\"${QWEN_VERSION:-latest}\" \"$@\"\n}\n```\n\nThese two are pretty good.",
          "score": 3,
          "created_utc": "2026-01-01 22:39:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5x3gg",
              "author": "Your_Friendly_Nerd",
              "text": "Can I ask, why are you wrapping them in these functions? why not do npm i -g?",
              "score": 2,
              "created_utc": "2026-01-01 23:46:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx62zbl",
                  "author": "th3_pund1t",
                  "text": "`npm i -g` makes it my problem to update the version.\nWrapping in a bash function allows me to always get the latest version, unless I choose to pin back.\n\nAlso, I'm not a nodejs person. So I might be doing that wrong.",
                  "score": 3,
                  "created_utc": "2026-01-02 00:19:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5xq09",
          "author": "Your_Friendly_Nerd",
          "text": "I just use the chat plugin for my code editor which provides the basic features needed for the ai to edit code. usimg qwen3-code 30b, I can give it basic tasks and it does them pretty well, though always just simple stuff like „write a function that does x“, nothing fancy like „there‘s a bug that causes y somewhere in this project, figure out how to fix it“",
          "score": 1,
          "created_utc": "2026-01-01 23:49:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6nohs",
          "author": "noless15k",
          "text": "Which models are you using?\n\nI find these the best locally on my Mac Mini M4 Pro 48GB device using llama.cpp server with settings akin to those found here:\n\n\\* [https://unsloth.ai/docs/models/devstral-2#devstral-small-2-24b](https://unsloth.ai/docs/models/devstral-2#devstral-small-2-24b)  \n\\* [https://unsloth.ai/docs/models/nemotron-3](https://unsloth.ai/docs/models/nemotron-3)\n\nAnd to your question, I use Zed's ACP for Mistral Vibe with devstral-small-2. It's not bad, though a bit slow.\n\nI certainly see a difference when running the full 123B devstral-2 via Mistral Vibe (currently free access), which is quite good. But the 24B variant is at least usable.\n\nI like nemo 3 nano for its speed. It's about 4-5x faster for prompt processing and token generation.\n\nIt works pretty well within Mistral Vibe and if you want to see the thinking setting --reasoning-format to none in llama.cpp seems to work without breaking the tool calls. I had issues getting nemo 3 nano working with zed's default agent.\n\nI haven't tried Mistral Vibe directly from the CLI yet though.",
          "score": 1,
          "created_utc": "2026-01-02 02:22:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7em2u",
              "author": "jackandbake",
              "text": "Good info thank you. Have you got the tools to work and complex multi-tasks working with this method?",
              "score": 1,
              "created_utc": "2026-01-02 05:14:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8k6ty",
          "author": "Lissanro",
          "text": "The best local model in my experience is Kimi K2 Thinking. It runs about 1.5 times faster than GLM-4.7 on my rig despite being larger in terms total parameters count, and feels quite a bit smarter too (I run Q4\\_X quant with ik\\_llama.cpp).",
          "score": 1,
          "created_utc": "2026-01-02 11:26:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxfkkw7",
          "author": "dragonbornamdguy",
          "text": "I love qwen code, but vllm has broken formatting for it (qwen3 coder 30b). So I use LM studio (with much slower performance).",
          "score": 1,
          "created_utc": "2026-01-03 13:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxv83ko",
          "author": "SelectArrival7508",
          "text": "I was able to integrate the privatemode (https://www.privatemode.ai/api). It worked really well and had the same level of privacy as local llms",
          "score": 1,
          "created_utc": "2026-01-05 19:04:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5k4fb",
          "author": "Lyuseefur",
          "text": "Nexora will be launching on January 5. Follow along if you would like - still fixing the model integration into the CLI but the repo will be at [https://www.github.com/jeffersonwarrior/nexora](https://www.github.com/jeffersonwarrior/nexora)",
          "score": -3,
          "created_utc": "2026-01-01 22:34:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3ud72",
      "title": "Problems with Local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q3ud72/problems_with_local_llms/",
      "author": "d-i-o-n-y-s-u-s",
      "created_utc": "2026-01-04 16:33:31",
      "score": 9,
      "num_comments": 33,
      "upvote_ratio": 0.91,
      "text": "I am very curious, as to some of the problems with locally run AI in general. This could range from costs, availability of parts, difficulty setting up, lack of state of the art models that are small enough to run locally, etc.\n\n  \nI am looking into locally run AI, and I have found a couple problems, but I was wondering from the broader community, big problems that exist that you wish were fixed. \n\n  \nBut also, some problems have a silver lining, so including that silver lining in your response would be great as well. \n\n  \nThank you very much! \n\n\n\n(Hopefully this is not a low effort question, genuinely in need of a wide range of answers)",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q3ud72/problems_with_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "nxnegrf",
          "author": "OrbMan99",
          "text": "I want to be able to run models that are sufficiently good at programming that I can use them for 80% of my work. My current hardware is not sufficient for this, and the cost of hardware that is continues to escalate.",
          "score": 9,
          "created_utc": "2026-01-04 16:40:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxnr9zs",
              "author": "calicocatfuture",
              "text": "i’m in the same boat! i just got into this a few days ago and i knew no coding and chatgpt helped me set up a local server and it all worked perfectly lol. but i have a small laptop that basically chokes on 7B. i just use it for roleplay and i dont want to spend $600 on RAM to run around with the avengers in my imagination sometimes. i’m really hoping that somehow the bigger models can be accessible to people that dont have huge hardware. so far the inference provider on huggingface is the best we can do and apparently it came out a few days ago. i’m hopeful for exponential growth with local models, especially with all the guardrails in chatgpt and the new ones in grok text generation.",
              "score": 1,
              "created_utc": "2026-01-04 17:39:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxo73fw",
          "author": "Terrible-Contract298",
          "text": "30B is the barrier of entry to usefulness to me. OSS 20B and Qwen3 Coder A3B are fantastic MOE models that run in 24gb of VRAM. The most affordable unit entry is about $300-350 and some knowhow to get a Tesla P40 modded into a consumer friendly GPU. I plan on documenting the full process I have to go through to get more and mod more.",
          "score": 6,
          "created_utc": "2026-01-04 18:49:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx7nkr",
              "author": "Armageddon_80",
              "text": "Hi! How do you use them for coding? I've tried on VScode continue but I really couldn't make them work correctly",
              "score": 1,
              "created_utc": "2026-01-06 00:52:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxx8p56",
                  "author": "Terrible-Contract298",
                  "text": "I like continue, but it's not a perfect tool. Right now I've just been running LMStudio on the same machine. It's annoying but I think you can add remote model endpoints using the configuration. You also have to manually tag each model to make them available in the GUI config.",
                  "score": 1,
                  "created_utc": "2026-01-06 00:57:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxnwrep",
          "author": "lcjanke2020",
          "text": "Buying adequate hardware for a decent model will cost you much more than just subscribing to one of the big names. \n\nBut there are advantages such as privacy.",
          "score": 5,
          "created_utc": "2026-01-04 18:04:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxo8e9i",
              "author": "Terrible-Contract298",
              "text": "Hard disagree, with the right deal finding, the hardware can be bought for less than a years subscription to 2 of the leading providers.",
              "score": 2,
              "created_utc": "2026-01-04 18:54:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxpcky8",
                  "author": "lcjanke2020",
                  "text": "What hardware and models are you thinking?",
                  "score": 2,
                  "created_utc": "2026-01-04 21:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxwpmj8",
              "author": "chris35moto",
              "text": "However the more big names offload the majority to shitty models and various quantizing advancements happen the more the gap narrows.",
              "score": 1,
              "created_utc": "2026-01-05 23:17:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxo4ia2",
              "author": "d-i-o-n-y-s-u-s",
              "text": "If it were accessible and affordable to pay for hardware that enables you to use state of the art AI, the level of Cloud AI in terms of reasoning and quality, without any token limits because it is local, do you think most people would switch away from Cloud AIs?",
              "score": 0,
              "created_utc": "2026-01-04 18:37:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxo58yf",
                  "author": "lcjanke2020",
                  "text": "Sadly, I think most people care little about privacy.",
                  "score": 6,
                  "created_utc": "2026-01-04 18:41:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxrsxpi",
                  "author": "AlanCarrOnline",
                  "text": "I would, and did. I had a perfectly functional gaming PC with 16GB RAM and RTX2060 with 6GB VRAM, with a 500GB SSD. Did everything fine - except excel at local AI.\n\nThat 6GB was a real bottle-neck.\n\nNow I have a 64GB RAM system, 2 x 1TB SSDs, based around an RTX3090, with 24GB VRAM. It happily runs 70B at tighter quants but the sweet spot is 20-35B or so, where it's too fast too read and plenty smart enough for talking to characters. \n\nMost peeps on this sub would consider that a poverty-spec setup, as it only has one GPU...\n\nRealistically though, Steam has shown the average real-world gamer has around 8GB of VRAM, and maybe 16GB of RAM. That limits you to something such as a Q4K\\_M of a good 12B model such as Nemo. \n\nFun? Hell yeah.\n\nCapable of professionally performing professional work for professional professionals in a professional setting, with agentic agents agenting? Nah, and nor are the top frontier models, not really. Not yet.\n\nSo like most things, it comes down to what YOU want to do with AI?",
                  "score": 2,
                  "created_utc": "2026-01-05 05:56:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxuvcrt",
                  "author": "isleeppeople",
                  "text": "That isn't really what token limits are about. You will hit them locally depending on what you are asking. It's finite",
                  "score": 1,
                  "created_utc": "2026-01-05 18:07:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxpf3bh",
          "author": "johimself",
          "text": "I would like a dedicated GPU where the GPU itself isn't that fast but has a lot of VRAM. I'm very happy with the speed if my RTX 3070, but 8GB is extremely limiting. Meanwhile the 12GB 3060 I have is more versatile but slower. If I want more VRAM then I need to buy an x090 GPU, but I can't justify the expense. \n\nSo to answer your question, my biggest problem with local LLMs is NVidia.",
          "score": 3,
          "created_utc": "2026-01-04 22:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5qee5",
              "author": "d-i-o-n-y-s-u-s",
              "text": "So why would you be willing to trade off some performance for more storage? Is it because of the work you do? Isn't that what VRAM is? Storage for the GPU? I agree with the Nvidia problem, that is a huge monopoly that is just waiting to crash. \n\nI hope some kind of competition arises, because right now the GPU market is not healthy, Nvidia can just keep jacking up the prices making it more and more difficult for people to build their own PC and run their Local LLMs.",
              "score": 1,
              "created_utc": "2026-01-07 06:56:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny5ur36",
                  "author": "johimself",
                  "text": "Having a large amount of VRAM allows you to load larger models to be used by the GPU, and larger contexts to allow for longer conversations without the LLM forgetting what you are talking about. The speed at which the LLM processes things is governed by the speed of the GPU.\n\nIn an ideal world I would have the fastest GPU with as much VRAM as possible, but RTX5090s cost thousands of pounds so there is a trade off between what you want and what you can afford. At the moment there is a linear scale - the more VRAM you go for to load larger models the faster the GPU you get.",
                  "score": 1,
                  "created_utc": "2026-01-07 07:34:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3qrys",
          "author": "Comfortable_Ad_8117",
          "text": "I love my local LLM - I have a dedicated server (using consumer parts) with a pair of Nvidia 5060 16GB cards & 64GB of RAM. I run Ollama and have it intergraded into a ton of my own applications as well as chat. Try to keep the models to about 30b parameters or less.",
          "score": 3,
          "created_utc": "2026-01-06 23:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5q2y2",
              "author": "d-i-o-n-y-s-u-s",
              "text": "So what was the setup process like? Going from not having the server made of your consumer parts, to having a whole system with applications powered by your local AI.",
              "score": 1,
              "created_utc": "2026-01-07 06:53:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny2dsd7",
          "author": "Sicarius_The_First",
          "text": "For programming you have 20-30b moes, run well without even a GPU.\n\nFor creative stuff try the impish ones. 3b-12b.",
          "score": 2,
          "created_utc": "2026-01-06 19:51:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp0tiv",
          "author": "Alucard256",
          "text": "\"problems ... range from costs, availability of parts\"\n\nI get tons done with a RTX3060 and 64GB RAM. People who say you *need* something in the 5000 series are the same mentality type that say you *need* a 2025 Dodge Charger Hellcat in order to drive to work.\n\n\"difficulty setting up\"\n\nThis hasn't been an issue for more than a year now.\n\nInstall [LM Studio](https://lmstudio.ai/). If you want even more features and control, then install [AnythingLLM](https://anythingllm.com/), too (they work together naturally). Either way, you can be \"up and running\" within minutes (minus download/install time, etc.).\n\n\"lack of state of the art models that are small enough to run locally\"\n\nWow. Hard disagree. There are 100's (if not 1000's) of high quality models available that run locally.\n\nAt this point in the technology lifecycle, *all models* are \"state of the art\"... and new models get released nearly every week... therefore re-defining the current \"state of the art\" instantly.",
          "score": 3,
          "created_utc": "2026-01-04 21:03:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpessa",
              "author": "Motor_Middle3170",
              "text": "I like LM Studio, but I still don't consider any local LLM system to be easy to set up and use. Yes, you can get a model running, but it's the equivalent of \"hello world\" in my book. You can't get a  usable system without learning the configuration and tuning details, especially if you have marginal hardware resources.\n\nBut I completely agree that the model availability is awesome, almost to the point of overwhelming.",
              "score": 1,
              "created_utc": "2026-01-04 22:08:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxs502s",
              "author": "d-i-o-n-y-s-u-s",
              "text": "Well I guess what I meant was equivalent to Cloud models. Also, what do you mean when you say high quality? For what tasks in specific? I know that you can fine tune a model for one specific thing and therefore it does not have to be as large, but for general things or broader use cases there are 100s of high quality models? I didn’t know that if true.",
              "score": 1,
              "created_utc": "2026-01-05 07:38:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxnr77h",
          "author": "custodiam99",
          "text": "They are slow, or they are quick but borderline unusable.",
          "score": 1,
          "created_utc": "2026-01-04 17:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuu3wr",
          "author": "SelectArrival7508",
          "text": "Shared the same problem as my pc wasn't able to run larger LLMs. For me switching to cloud-based, confidential ai models such as [https://www.privatemode.ai](https://www.privatemode.ai) worked really well.",
          "score": 1,
          "created_utc": "2026-01-05 18:01:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz00us",
              "author": "d-i-o-n-y-s-u-s",
              "text": "How do you know it is *really, really,* confidential?",
              "score": 1,
              "created_utc": "2026-01-06 07:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny12n6n",
                  "author": "SelectArrival7508",
                  "text": "it is based on confidential computing and even the service provider can not access your data. see [https://images.nvidia.com/aem-dam/Solutions/Data-Center/confidential-computing/hpc-solution-brief-edgeless-systems-3427281.pdf?ncid=no-ncid](https://images.nvidia.com/aem-dam/Solutions/Data-Center/confidential-computing/hpc-solution-brief-edgeless-systems-3427281.pdf?ncid=no-ncid)",
                  "score": 1,
                  "created_utc": "2026-01-06 16:19:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny5m09y",
          "author": "I_like_fragrances",
          "text": "I like to use both cloud and local models. Sometimes local models are a lot faster to use and give a result as good as a cloud model. I use cloud models if I need the most up to date information available.",
          "score": 1,
          "created_utc": "2026-01-07 06:20:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5pixh",
              "author": "d-i-o-n-y-s-u-s",
              "text": "So if there were some form of local LLM set up that could retrieve new information, yet stay local, I don't know exactly how maybe through some manual additional function or a one way transfer of data, would you use exclusively Local models?",
              "score": 1,
              "created_utc": "2026-01-07 06:48:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q50yf1",
      "title": "Running a local LLM in browser via WebGPU to drive agent behaviour inside a Unity game",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1q50yf1/running_a_local_llm_in_browser_via_webgpu_to/",
      "author": "lordhiggsboson",
      "created_utc": "2026-01-05 22:54:04",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "Hey all! I built a tiny proof of concept that allows me to run a local LLM in a browser using WebGPU. My ideas on *why* I wanted to try this were to 1) see if I could do it,  and 2) see if the high-frequency / low-latency / no-costs aspects of running locally opens up interesting designs that may not be feasible otherwise.\n\nI created a simple simulation game to explore this, set within an office setting. A LLM is loaded and used as the \"brain\" for all the agents and their interactions. Instead of purely treating the LLM input/output as an interface to the player, I wanted to steer agent behaviour using the LLMs as a decision-making framework. And depending on the GPU/Device running it, this allows me to query the LLM 1-4 times per second, opening up high-frequency interactions. The current demo is fairly simple right now, but you should be able to move around, interact, and observe agents as they go about their office environment.\n\n  \nThe actual construction of the demo was a bit more nuanced than I expected. For one, the support for JSPI suspensions is not widely supported on all browsers yet, and I rely on this to bridge pseudo-async calls between the V8 runtime and the WASM binaries. The other was getting the inference parts working in Unity for the web. I explored a few approaches here, like directly outputting a static WASM lib and bundle it using Unity's own build process. This kind of worked, but I consistently had to wrestle configurations around the Emscripten version Unity was using and the features I wanted. In the end, I landed on a solution that separates out my WASM binary from Unity's WASM, and instead use Unity to only bootstrap and marshal the data I needed. This allowed me to decouple from Unity-specific stuff and build out the inference parts independently, which worked out nicely.\n\n  \nThe inference engine is a modified version of Llama.cpp with additions that mostly touch the current WebGPU backend. Most of the work went into creating and expanding the WGSL kernels so they don't rely on float16 and expand their capabilities to cover more operations for forward inference. These modifications provided enough to load simpler models, and I ended up utilizing Qwen 2.5 0.5B for the demo, which has decent memory/performance tradeoffs for the use cases I wanted to explore.\n\n  \nI'm curious to hear what everyone thinks about browser-based local inference and whether this is interesting. A goal of mine is to open this up and provide a JS package that streamlines the whole process for webapps/unity games.\n\n  \nDemo to the prototype: [https://noumenalabs.itch.io/office-sim](https://noumenalabs.itch.io/office-sim)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q50yf1/running_a_local_llm_in_browser_via_webgpu_to/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q5h10a",
      "title": "Connect any LLM to all your knowledge sources and chat with it",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/e9s5gl8x0qbg1",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-06 12:16:50",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.69,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q5h10a/connect_any_llm_to_all_your_knowledge_sources_and/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nxztyor",
          "author": "codeforgeai",
          "text": "How to collaborate",
          "score": 3,
          "created_utc": "2026-01-06 12:18:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzuea1",
              "author": "Uiqueblhats",
              "text": "For now we have RBAC with team invites. Real time features are work in progress.",
              "score": 2,
              "created_utc": "2026-01-06 12:21:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxzun9i",
          "author": "codeforgeai",
          "text": "Well I'm looking for the collaborators like you for my current project- offline ai code assistant. If you are interested then please let me know how to connect with you or dm me for more details \n\nHappy to connect 😊 \n\nRegards,\nBhavesh Shahani",
          "score": 1,
          "created_utc": "2026-01-06 12:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3hc1c",
              "author": "Miserable-Dare5090",
              "text": "Let me save you time: use http://localhost:$PORT/v1 on any of the coding agents out there, and you have a local coding agent. \n\nOP, surfsense sounds very interesting. I’ll give it a spin and test. Always looking for deep research tools to enable on local models.",
              "score": 1,
              "created_utc": "2026-01-06 22:56:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5nc93",
          "author": "ChrononautPete",
          "text": "Any chance of supporting XLM in the near future?",
          "score": 1,
          "created_utc": "2026-01-07 06:30:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5ly31",
      "title": "Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/y8o708g31rbg1.png",
      "author": "A-Rahim",
      "created_utc": "2026-01-06 15:40:24",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1q5ly31/unslothmlx_finetune_llms_on_your_mac_same_api_as/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}