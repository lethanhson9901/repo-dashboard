{
  "metadata": {
    "last_updated": "2026-01-28 08:47:44",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 164,
    "file_size_bytes": 200389
  },
  "items": [
    {
      "id": "1qmrwxl",
      "title": "Clawdbot: the AI assistant that actually messages you first",
      "subreddit": "LocalLLM",
      "url": "https://jpcaparas.medium.com/clawdbot-the-5-month-ai-assistant-that-actually-messages-you-first-8b247ac850b8?sk=0d521efdf2ceafe37973887b57b168ba",
      "author": "jpcaparas",
      "created_utc": "2026-01-25 18:59:25",
      "score": 140,
      "num_comments": 116,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qmrwxl/clawdbot_the_ai_assistant_that_actually_messages/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1u9m94",
          "author": "inigid",
          "text": "Clawdbot opens up the potential for a massive supply-chain attack that can steal or destroy everything on your machine and network, harvest and exfiltrate emails, crypto wallets, bank account and credit card details, SSN numbers, personal information, ssh keys.\n\nThe Solana meme coin and hype pumps by countless influencers is also a massive red flag.\n\nStay TF away from it if you have any sense.",
          "score": 11,
          "created_utc": "2026-01-26 16:10:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud0b5",
              "author": "kublaikhaann",
              "text": "stop making too much sense",
              "score": 5,
              "created_utc": "2026-01-26 16:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ul578",
                  "author": "inigid",
                  "text": "Ikr, the thing literally has auto-update for itself, and then there are the downloadable skills on top!\n\nIt's like you are asking for trouble!\n\nOh, I'm just going to install this with sudo privileges and connect it up to all my infrastructure and personal accounts.\n\nWhat could possibly go wrong!  Smh.\n\nhttps://preview.redd.it/0h7v5nrr5qfg1.jpeg?width=1581&format=pjpg&auto=webp&s=2566f1d6f9e2d37b127da3ee58bb1740e6990adc",
                  "score": 8,
                  "created_utc": "2026-01-26 16:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yi54i",
              "author": "ab2377",
              "text": "üíØ",
              "score": 2,
              "created_utc": "2026-01-27 04:20:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x3t7z",
              "author": "ParanoidBlueLobster",
              "text": "Bit of a conspiracy theorist are we?\n\nIt's an open-source tool https://github.com/clawdbot/clawdbot\n\nAnd you update it manually https://docs.clawd.bot/install/updating",
              "score": 3,
              "created_utc": "2026-01-26 23:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xbf37",
                  "author": "inigid",
                  "text": "[https://cybersecuritynews.com/clawdbot-chats-exposed/](https://cybersecuritynews.com/clawdbot-chats-exposed/)\n\n[https://socradar.io/blog/clawdbot-is-it-safe/](https://socradar.io/blog/clawdbot-is-it-safe/)\n\nhttps://preview.redd.it/4ls6ihradsfg1.jpeg?width=1280&format=pjpg&auto=webp&s=905eda3538cc276b330924c33d14e18022a95d40",
                  "score": 6,
                  "created_utc": "2026-01-27 00:23:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o24um1r",
                  "author": "BernardoOne",
                  "text": "no conspiracy theory required, if you have an AI agent with full permissions on your PC and connected to the internet you're 100% vulnerable to prompt injection.",
                  "score": 1,
                  "created_utc": "2026-01-28 01:47:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o23yhf4",
              "author": "Brumotti",
              "text": "Blackbox AI fixes this.  \n  \n[https://x.com/mhtua/status/2015943960867340356](https://x.com/mhtua/status/2015943960867340356)",
              "score": 1,
              "created_utc": "2026-01-27 23:02:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o24dp6w",
              "author": "PeakBrave8235",
              "text": "But but but John Gruber told me it was cool!!!!!! And everything he says is amazing so obviously you're wrong¬†\n\nSarcasm.¬†",
              "score": 1,
              "created_utc": "2026-01-28 00:19:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o21bvsw",
              "author": "Jasmine_Demir",
              "text": "Probably depends on how much you have to lose as well, and how disruptive it would be for you to stop your entire life for a day or two while you perform triage. \n\nIf you're 22 with a $50,000 net worth it's a very different consequence than begin 55 with 3 children, $5,000,000 in available debt/assets, and a convoluted chain of responsibilities that would come to a halt once you incorporated an assistant at this layer of your life flow and it failed.\n\nThe ones jumping into this don't have much to lose, and/or have the time and experience to stay on top of security issues.",
              "score": 0,
              "created_utc": "2026-01-27 16:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pwp1d",
          "author": "mike7seven",
          "text": "Spent most of the day figuring out that you need to get your Claude Code key (Oauth) on another machine and paste it for use on whatever machine you are setting up Clawdbot on. \n\nFrom Ops tutorial: \n‚ÄúYou have two options. Claude OAuth (the easiest) or API keys (more control over costs). For beginners, OAuth with a Claude Pro subscription is the simplest path.‚Äù\n\nNow I‚Äôm over even playing around with this thing for the rest of the day. \n\nMy two cents: The MacOS app is nice but you have to build it from source. Also good luck getting app auth setup properly if you are using a remote machine.",
          "score": 9,
          "created_utc": "2026-01-25 23:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pxaxs",
              "author": "DependentNew4290",
              "text": "Is it that easy to set up one as a non-technical person???",
              "score": 2,
              "created_utc": "2026-01-25 23:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q1s1j",
                  "author": "mike7seven",
                  "text": "Depends on how you plan on setting it up. Using ChatGPT and Codex it worked immediately and beautifully. Claude was not that easy. For local follow OPs guide.",
                  "score": 2,
                  "created_utc": "2026-01-26 00:08:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20gkav",
                  "author": "brianlmerritt",
                  "text": "If you want to set it up on your personal or work computer, provide unlimited access to said computer and all of your email and messaging accounts plus whatever else is on there like online banking or company information, it is still not that easy.\n\nBut you will be putting your personal computer or company at risk - there are a lot of little add-ons that have \"extra\" code and abilities to log into your computer, access everything plus use your AI tokens and accounts.\n\nUnless you are happy setting this up securely (see the link at the end of the post) there is a very good chance it doesn't go well.    \n  \nAlso note that one email or message can come in and hijack the AI within the system even if you didn't load any dodgy utilities.\n\n",
                  "score": 1,
                  "created_utc": "2026-01-27 13:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20sk57",
              "author": "Manarj789",
              "text": "It took all of 5 minutes to do, they literally tell you the steps to take",
              "score": 0,
              "created_utc": "2026-01-27 14:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ux2ag",
              "author": "[deleted]",
              "text": "[removed]",
              "score": -2,
              "created_utc": "2026-01-26 17:52:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yqrif",
                  "author": "Winter-Editor-9230",
                  "text": "r/LocalLLM follows platform-wide Reddit Rules; crypto currency and self promotion.",
                  "score": 1,
                  "created_utc": "2026-01-27 05:17:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1s2o5c",
          "author": "Ashamed_Promise7726",
          "text": "Has anyone successfully connected a local llm on their machine to Clawdbot? I have a few different models already downloaded to my PC, but I cannot get Clawdbot to work with them, and it keeps wanting an api key for a usage based model.\n\nIs it possible to run Clawdbot 100% locally?",
          "score": 8,
          "created_utc": "2026-01-26 07:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s41bu",
              "author": "kinesivan",
              "text": "If you are hosting with LM Studio or Ollama, does not matter what API key you pass, any should work.",
              "score": 3,
              "created_utc": "2026-01-26 07:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s7zwo",
                  "author": "Yorn2",
                  "text": "No, I'm having the same issue, Local OpenAPI-compatible isn't even an option in the choices. Nor is Ollama.",
                  "score": 2,
                  "created_utc": "2026-01-26 08:07:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1slfam",
              "author": "Everlier",
              "text": "I spent my entire weekend integrating Clawdbot into Harbor so that it's possible to set it up with a local LLM in a few commands (granted you have Harbor installed):\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot",
              "score": 3,
              "created_utc": "2026-01-26 10:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xkbhh",
                  "author": "Yorn2",
                  "text": "Harbor seems cool but I'm using MacOS and mlx_lm.server for running a local instance of Deepseek 3.2, not llama.cpp or ollama or vllm. Though I do have a non-mac AI box where I might use Harbor instead someday. Do you plan on adding MacOS support sometime?",
                  "score": 1,
                  "created_utc": "2026-01-27 01:09:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o25kt3u",
                  "author": "yogabackhand",
                  "text": "First I‚Äôve heard of Harbor. Looks cool! I‚Äôll give it a try. It would be cool if there was a MacOS desktop app (for configuration) and iOS client app like Pigeon (would be nice to have a MacOS client app too).",
                  "score": 1,
                  "created_utc": "2026-01-28 04:10:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yju3k",
              "author": "Loud-Layer3329",
              "text": "Yes I have using Ollama. Haven't had much luck with the models, though. So far Qwen2.5:14b has been the best to use via the web interface, but it gives a lot of nonsense via Telegram.\n\nNote that you need to use a model that supports tools, and Clawdbot only permits models that have a context window of >16k. Clawdbot supports the openai endpoint so you can use the baseurl [http://x.x.x.x:11434/v1](http://x.x.x.x:11434/v1)",
              "score": 1,
              "created_utc": "2026-01-27 04:31:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1z9r9b",
              "author": "No_Box1288",
              "text": "Running it with MiniMax M2.1 works well. I set it up following this tutorial, pretty straightforward:\nhttps://x.com/MiniMax_AI/status/2014380057301811685\n\nAlso saw a coding plan link in the MiniMax Discord. There‚Äôs a discount + cashback if you use it:\nhttps://platform.minimax.io/subscribe/coding-plan?code=Cp84x9ex1L&source=link",
              "score": 1,
              "created_utc": "2026-01-27 07:50:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22ymy0",
                  "author": "JimmyDub010",
                  "text": "Lucky. you must have some kind of super computer",
                  "score": 1,
                  "created_utc": "2026-01-27 20:17:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25pgma",
              "author": "Frenchplay57",
              "text": "I tried all day without success with the help of gpt, gemini and Claude. Claude managed to get it connected but there is no response in the interface.¬†",
              "score": 1,
              "created_utc": "2026-01-28 04:39:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pa4kx",
          "author": "Acceptable_Home_",
          "text": "Might try soon, sounds good!",
          "score": 4,
          "created_utc": "2026-01-25 22:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pax6h",
              "author": "jpcaparas",
              "text": "thank you. apologies in advance if the guide isnt too technical. i just want people to get into it. pete the maintainer is also a cool follow on Twitter",
              "score": -2,
              "created_utc": "2026-01-25 22:05:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rogty",
          "author": "threathunter369",
          "text": "your take on this guys?\n\n[https://x.com/theonejvo/status/2015401219746128322](https://x.com/theonejvo/status/2015401219746128322)",
          "score": 5,
          "created_utc": "2026-01-26 05:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryqsv",
              "author": "xak47d",
              "text": "That's for noobs who just discovered cloud hosting following the hype and don't properly secure their instances. That's very unrelated to the tool and its capabilities",
              "score": 1,
              "created_utc": "2026-01-26 06:50:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1whdbz",
              "author": "Double-Lavishness870",
              "text": "LLms are insecure per default. Any browsing, email reading, tweet reading can insert bad behavior. \n\nBest knowledge and advice see here: https://media.ccc.de/v/39c3-agentic-probllms-exploiting-ai-computer-use-and-coding-agents#t=1218\n\nIsolation is needed.",
              "score": 1,
              "created_utc": "2026-01-26 21:55:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1slmv3",
          "author": "Everlier",
          "text": "If you're like me and wanted to setup Clawdbot with a local LLM, check out this integration in Harbor:\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot\n\nIt also runs Clawdbot containerized, so that it won't break your host system if something will go wrong",
          "score": 6,
          "created_utc": "2026-01-26 10:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1umj0n",
          "author": "cmndr_spanky",
          "text": "From a security perspective, I have zero interest in this right now (LLM autonomy with access to my calendar etc). And the supposed benefits are minimal (I already get calendar reminders, I don‚Äôt need an LLM bot to ‚Äúextra‚Äù remind me). Beyond that‚Ä¶ all the other use cases are fine with direct access to normal chatGPT / Gemini etc ..\n\nAlso Apple with soon replace Siri with Gemini and I imagine that‚Äôs going to be a game changer in terms of iOS integration and proactive AI",
          "score": 3,
          "created_utc": "2026-01-26 17:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcz9x",
          "author": "TendiesOnlyPls",
          "text": "Ok... so has anyone given Clawdbot access to their dating apps and asked it to go wrangle up some consenting ass? Really want to see how well these LLMs are able to spit game.",
          "score": 3,
          "created_utc": "2026-01-26 21:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ug8e0",
          "author": "Weird-Consequence366",
          "text": "Astroturf campaign",
          "score": 5,
          "created_utc": "2026-01-26 16:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sdrta",
          "author": "tomByrer",
          "text": "Is your article some where else than Medium please?",
          "score": 4,
          "created_utc": "2026-01-26 08:59:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ri8h1",
          "author": "No_Conversation9561",
          "text": "Entire X feed has been this lately",
          "score": 2,
          "created_utc": "2026-01-26 04:49:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o261w6z",
              "author": "Prestigious_Pace_108",
              "text": "That and crypto bros pushing along with Brave search integration gave me the message. I cancelled setup.",
              "score": 1,
              "created_utc": "2026-01-28 06:06:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rqpdf",
          "author": "Fast_Back_4332",
          "text": "How safe is it?",
          "score": 2,
          "created_utc": "2026-01-26 05:48:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryvys",
              "author": "xak47d",
              "text": "It might delete all your files if you make it mad",
              "score": 6,
              "created_utc": "2026-01-26 06:51:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s9o8e",
                  "author": "Cressio",
                  "text": "Is that literally true? It seems like it would be but I haven't looked too deep into it. I don't think I *want* something with that deep of access to my stuff lol",
                  "score": 1,
                  "created_utc": "2026-01-26 08:22:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20w5zw",
                  "author": "jeremyckahn",
                  "text": "A hallmark of any great software",
                  "score": 1,
                  "created_utc": "2026-01-27 14:51:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o268zk4",
              "author": "leo-k7v",
              "text": "as safe as sudo anything in your terminal. OS does matter - you start by giving it admin privileges‚Ä¶ good luck",
              "score": 1,
              "created_utc": "2026-01-28 07:02:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q8d1z",
          "author": "snam13",
          "text": "For once, I‚Äôm early! Set this up last week. \nDon‚Äôt expect miracles. If you‚Äôve used claude code or similar, it will feel like those but in your chat app. Be careful of burning lots of API tokens.",
          "score": 2,
          "created_utc": "2026-01-26 00:40:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qh1fv",
              "author": "ThenExtension9196",
              "text": "You may not be squeezing the lemon yet. There‚Äôs such insane amount of functionality you can get out of it by wiring it to gitlab/github and api services. I legit think this thing will be able to automated nearly my entire workday.",
              "score": 1,
              "created_utc": "2026-01-26 01:24:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rvyp3",
                  "author": "ketaminesuppository",
                  "text": "curious; what's your job?",
                  "score": 1,
                  "created_utc": "2026-01-26 06:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qzgz1",
          "author": "Artistic-Read-1097",
          "text": "I'm having a problem when I send a message in the GUI. Nothing happens I get no response. can anyone help if been at it all day",
          "score": 1,
          "created_utc": "2026-01-26 02:57:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25qxux",
              "author": "Frenchplay57",
              "text": "Are you on a local network? I have the same problem.¬†",
              "score": 1,
              "created_utc": "2026-01-28 04:48:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rswkr",
          "author": "albany_shithole",
          "text": "How would it work for vLLM ?",
          "score": 1,
          "created_utc": "2026-01-26 06:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sat1j",
          "author": "redblood252",
          "text": "If I use this with local llama-cpp. Which model is best for it? Gpt-oss? Is 14Gb of vram enough for it?",
          "score": 1,
          "created_utc": "2026-01-26 08:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1slhhv",
              "author": "Everlier",
              "text": "GLM-4.7-Flash is the best right now, it really wants very strong agentic models",
              "score": 2,
              "created_utc": "2026-01-26 10:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sm5vs",
                  "author": "redblood252",
                  "text": "Even at IQ4_XS quantization it takes 16gb of vram. I need to test some cpu offloading and see if it is fast enough. And if the quantization isn‚Äôt too much.",
                  "score": 1,
                  "created_utc": "2026-01-26 10:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sfx42",
          "author": "Necessary_Function_3",
          "text": "Spent hours (and burned all my anthropic tokens for the day and I am paying $200 a month, but API tokens are extra it seems) and cannot get it to work with ollama, extremely frustrating",
          "score": 1,
          "created_utc": "2026-01-26 09:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1skfux",
              "author": "jamesftf",
              "text": "but what this does and what claude code cannot do?!",
              "score": 1,
              "created_utc": "2026-01-26 10:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u3ade",
                  "author": "MaaDoTaa",
                  "text": "It can message you (unsolicited)",
                  "score": 1,
                  "created_utc": "2026-01-26 15:43:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sms05",
          "author": "Separ0",
          "text": "Installed last night on an Ubuntu machine on hetznsr on my tailnet. Crons are not working",
          "score": 1,
          "created_utc": "2026-01-26 10:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vihfg",
          "author": "p_235615",
          "text": "Not sure why this is such a hype, but from what I played around with it in a VM and local llm (its quite PITA to setup), and one thing is for sure - its basically a security nightmare. \n\nYou have a single point rouge AI which sending all your data to cloud if not set up with local LLM and even with local LLM you never know when even accidentally it will send your private files somewhere to the web. Or if it gets somehow compromised by accessing some webpage or something, its like throwing all your files and accounts around. There are basically no security guard rails, and would never feed it some private or corporate accounts.",
          "score": 1,
          "created_utc": "2026-01-26 19:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vq2td",
          "author": "apola",
          "text": "I'm interested in playing around with clawdbot, but my understanding is that it chews through tokens like nobody's business. How useful can it be with the $20/month Claude Pro subscription? Would that give it enough tokens to be remotely useful?",
          "score": 1,
          "created_utc": "2026-01-26 19:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vqy8f",
              "author": "jpcaparas",
              "text": "I've given a bit of guidance on token allowances between Pro and Max plans here:\n\nhttps://jpcaparas.medium.com/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou might find it useful.",
              "score": 1,
              "created_utc": "2026-01-26 19:58:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vycbq",
                  "author": "apola",
                  "text": "Interesting, thanks for the link. So, the $20/mo plan gives you \\~$259 worth of compute. My real question is: Is $259 of compute enough to make Clawdbot useful? I've seen videos where people seem to spend $150/day using clawdbot to do a few extremely basic things. Maybe my impression of what they're doing is wrong, but that would seem to suggest that the $20/mo subscription would get me \\~2 days of clawdbot usage per month.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w32ib",
          "author": "Lonely-Elephant2130",
          "text": "It's impressive for sure, but man the setup barrier is real. Spent hours trying to get it running and couldn't figure it out (not a dev background). Currently using Super Intern (https://www.superintern.ai/ ) which is similar but actually simple - just chat interface, no setup. Way more my speed.",
          "score": 1,
          "created_utc": "2026-01-26 20:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yq4t3",
          "author": "BiggestSkrilla",
          "text": "need to do something about api cost.",
          "score": 1,
          "created_utc": "2026-01-27 05:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yvaaj",
          "author": "FutureboyWavo",
          "text": "Currently using clawdbot running on a VPs with minimax m2 model and I‚Äôm having so many issues.",
          "score": 1,
          "created_utc": "2026-01-27 05:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcs6h",
          "author": "Different-Pizza-7591",
          "text": "I found an App to connect to Clawdbot with iOS ! Not perfect but it works and it is free!   \n[https://apps.apple.com/us/app/nuvik/id6747774937](https://apps.apple.com/us/app/nuvik/id6747774937)",
          "score": 1,
          "created_utc": "2026-01-27 08:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ziga3",
          "author": "Technical_Self_9996",
          "text": "I've created a a step-by-step guide for non-developers to get going with Clawdbot. No assumed knowledge. Just copy, paste, and follow along. Hope people find it useful: [https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide](https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide)",
          "score": 1,
          "created_utc": "2026-01-27 09:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208zsm",
          "author": "NineOneOne119",
          "text": "I'm attempting to use the Quen3 models from the [Featherless.ai](http://Featherless.ai) API in Clawdbot, but it's unable to locate the model Quen3/Quen3-14B. Do you have any suggestions?",
          "score": 1,
          "created_utc": "2026-01-27 12:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22blg9",
          "author": "Thrombas",
          "text": "This is the \"DeepSeek 2024\" hype all over again lol",
          "score": 1,
          "created_utc": "2026-01-27 18:36:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23ylca",
          "author": "toyssamurai",
          "text": "This kind of tool has one problem. I would never trust it enough to connect my primary accounts to it. Ask any honest architects behind LLMs if they can guarantee their models will never hallucinate. They will all say no, at least not yet. If they can't make such a promise, why should I trust them with my entire online life?",
          "score": 1,
          "created_utc": "2026-01-27 23:02:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2455zg",
          "author": "wheredoesitsaythat",
          "text": "Has anyone built or suggest security guardrails.  If its open source, couldn't I make the code more secure?",
          "score": 1,
          "created_utc": "2026-01-27 23:36:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24i090",
          "author": "hungarianhc",
          "text": "So if I want to play with this, should I buy a Mac Mini? Or should I just install it on an LXC on my Proxmox server?",
          "score": 1,
          "created_utc": "2026-01-28 00:41:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25thtg",
          "author": "MaintenanceQueasy457",
          "text": "Thats really cool. I would love to hear more¬†",
          "score": 1,
          "created_utc": "2026-01-28 05:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o268ls4",
          "author": "leo-k7v",
          "text": "what was that day in The Terminator movie called when SkyNet became self aware? And broke free?",
          "score": 1,
          "created_utc": "2026-01-28 06:59:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgt9k",
          "author": "ThenExtension9196",
          "text": "I just got it running last night. \n\nIt‚Äôs literally another chatgpt4 moment. Tools like this are going to be huge this year.",
          "score": 1,
          "created_utc": "2026-01-26 01:23:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qi3wx",
              "author": "jpcaparas",
              "text": "yeah man personal ai is gonna explode this year. Tools like Poke and Clawdbot are gonna be big",
              "score": 2,
              "created_utc": "2026-01-26 01:30:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ux0vs",
          "author": "Professional-Cut7836",
          "text": "looking to get into the clawdbot hype? \n\nselling 1 time payment, 2 year gurantee #claude and #OpenAI api access. 0.017 Eth/50 USDT \n\nDm me!:)",
          "score": 0,
          "created_utc": "2026-01-26 17:51:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql4hwj",
      "title": "RTX Pro 6000 $7999.99",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "author": "I_like_fragrances",
      "created_utc": "2026-01-23 22:05:23",
      "score": 74,
      "num_comments": 64,
      "upvote_ratio": 0.96,
      "text": "Price of RTX Pro 6000 Max-Q edition is going for $7999.99 at Microcenter.\n\n[https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card](https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card)\n\nDoes it seem like a good time to buy?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1c5v68",
          "author": "Green-Dress-113",
          "text": "If you want to run 2 or 4 GPUs, Max-Q is the way to go for cooling. I have dual blackwell 6000 pro workstations and one heats the other with the fans blowing sideways. While the power max is 600W I'm only seeing avg 300W during inference with peaks to 350W, but not full 600W consumption So Max-Q being 300W is the sweet spot for performance and cooling.",
          "score": 13,
          "created_utc": "2026-01-24 00:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d5r3r",
              "author": "SillyLilBear",
              "text": "I run two workstation cards I have them power limited to 300W and get 96% of the performance of 600W.",
              "score": 8,
              "created_utc": "2026-01-24 03:35:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j8410",
                  "author": "m2845",
                  "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
                  "score": 1,
                  "created_utc": "2026-01-25 01:05:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j83rm",
              "author": "m2845",
              "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
              "score": 1,
              "created_utc": "2026-01-25 01:05:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bnzd7",
          "author": "Big_River_",
          "text": "the performance difference is more like 10% and yes 96gb gddr7 vram at 300w stable is best perf per watt there is to stack up a wrx90 threadripper build with - best card for home lab by far",
          "score": 20,
          "created_utc": "2026-01-23 22:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1db5zv",
              "author": "Sufficient-Past-9722",
              "text": "Just need a good waterblock for it.",
              "score": 1,
              "created_utc": "2026-01-24 04:09:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1dla2o",
                  "author": "Paliknight",
                  "text": "For a 300w card?",
                  "score": 2,
                  "created_utc": "2026-01-24 05:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1by4af",
          "author": "morriscl81",
          "text": "You can get it cheaper than that by at least $600-700 from companies like Exxact Corp",
          "score": 4,
          "created_utc": "2026-01-23 23:26:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bzwly",
              "author": "ilarp",
              "text": "how to order from them?",
              "score": 2,
              "created_utc": "2026-01-23 23:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c0iyo",
                  "author": "morriscl81",
                  "text": "Go to their website and make a request for a quote. They will send you an invoice.  That‚Äôs how I got mine",
                  "score": 3,
                  "created_utc": "2026-01-23 23:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c08qr",
          "author": "MierinLanfear",
          "text": "Max q is lower power for if you want to run more than 2 cards on your workstation.   If your only running 1 or 2 cards go for the full version unless you plan on adding more later.\n\nThere are education discounts too.   Prices are likely to go up so now is a good time to buy.",
          "score": 4,
          "created_utc": "2026-01-23 23:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cep80",
          "author": "queerintech",
          "text": "I just bought a 5000 to pair with my 5070ti I considered the 6000 but whew.  üòÖ",
          "score": 3,
          "created_utc": "2026-01-24 00:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cgwrf",
          "author": "No-Leopard7644",
          "text": "Do you monetize this investment or it‚Äôs for fun?",
          "score": 3,
          "created_utc": "2026-01-24 01:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dre0b",
          "author": "gaidzak",
          "text": "Cheapest I found so far is from provantage for non education $7261 dollars. I hope this is real or I am understanding this pricing. \n\n[https://www.provantage.com/nvidia-9005g153220000001\\~7NVID0M1.htm](https://www.provantage.com/nvidia-9005g153220000001~7NVID0M1.htm)\n\nFor education purchases, any NVidia Partner can get them down to $6000;",
          "score": 3,
          "created_utc": "2026-01-24 06:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bx3g3",
          "author": "separatelyrepeatedly",
          "text": "Get workstation and power limit it",
          "score": 6,
          "created_utc": "2026-01-23 23:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cecci",
              "author": "Big_River_",
              "text": "the max-q variant is also a blower card therefore well suited by design to stacking in a box - multiple workstation variants even power limited require powered ddr5 risers or they will thermal throttle each other - so if you only get one sure get a workstation otherwise max-q for sure",
              "score": 4,
              "created_utc": "2026-01-24 00:54:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1cg56k",
          "author": "Qs9bxNKZ",
          "text": "Naw.  Full power version.\n\nThen afterburner if you want to reduce power.  \n\nThen undervolt if you want to reduce heat.  \n\nMost people are not going to be running more than two of them in a case.  And I mean more than two because bifurcation along with chipset performance unless you‚Äôre on a threadripper or Xeon \n\nHave two in one box and I put out about 1200W at 70% along with 950mv.  \n\n$8099 for the black box version.  $7999 for the bulk nvidia version.  Minus $2-400 for education and bulk discounts.",
          "score": 5,
          "created_utc": "2026-01-24 01:04:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dr26a",
              "author": "gaidzak",
              "text": "Education pricing for a RTX 6000 Pro Server is 6k.. I'm about to hit the BUY button.",
              "score": 1,
              "created_utc": "2026-01-24 06:02:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1egntb",
                  "author": "t3rmina1",
                  "text": "Where are you getting 6k? I'm getting edu quotes that are a bit higher for WS",
                  "score": 1,
                  "created_utc": "2026-01-24 09:49:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bij91",
          "author": "snamuh",
          "text": "What‚Äôs the deal with the max-a ed?",
          "score": 4,
          "created_utc": "2026-01-23 22:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biody",
              "author": "I_like_fragrances",
              "text": "It is 300W power versus 600W. Typically gets a 20% reduced performance but same VRAM and cores.",
              "score": 9,
              "created_utc": "2026-01-23 22:07:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1bjn31",
                  "author": "hornynnerdy69",
                  "text": "20% reduced performance isn‚Äôt nothing, especially when you consider that might put its performance below a 5090 for models that can fit in 32GB VRAM. And you could get a standard 6000 Pro for under 20% more money (seeing them at like $8750 recently)",
                  "score": 6,
                  "created_utc": "2026-01-23 22:12:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1bk6hr",
                  "author": "nero519",
                  "text": "I don't get it, what's the point of it? Do they just sell a normal card with artificial limitations to make it cheaper or is there something actually missing, like slower memories or less cores",
                  "score": 3,
                  "created_utc": "2026-01-23 22:14:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bzbor",
              "author": "getfitdotus",
              "text": "Better card i run 4 of these. All air cooled and work great max 60c",
              "score": 2,
              "created_utc": "2026-01-23 23:32:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dr57r",
          "author": "Foreign_Presence7344",
          "text": "Could you use the workstation version and a max q in the same box?",
          "score": 2,
          "created_utc": "2026-01-24 06:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dvdor",
          "author": "AlexGSquadron",
          "text": "They were going for $7500??",
          "score": 2,
          "created_utc": "2026-01-24 06:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f8zog",
          "author": "gweilojoe",
          "text": "Get from Computer Central - They don‚Äôt charge tax for purchases outside of California. I bought mine from them (regular version not Qmax) and it‚Äôs worked great.",
          "score": 2,
          "created_utc": "2026-01-24 13:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbkhb",
          "author": "Phaelon74",
          "text": "You can get them for 5800 ish if you spend the time to find the vendor, do inception program, etc.",
          "score": 2,
          "created_utc": "2026-01-24 13:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cfxrs",
          "author": "Cold_Hard_Sausage",
          "text": "Is this one of those gadgets that‚Äôs going to be 10 bucks in 5 years?",
          "score": 6,
          "created_utc": "2026-01-24 01:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dbxs7",
              "author": "Sufficient-Past-9722",
              "text": "If you had bought an Ampere A6000 back in January 2021, you could still sell it for something close to the original price. Similar for the 3090.",
              "score": 7,
              "created_utc": "2026-01-24 04:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1cjk1m",
              "author": "Br4ne",
              "text": "more like 20k in 2 months if you ask me",
              "score": 4,
              "created_utc": "2026-01-24 01:24:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fa25c",
              "author": "Mysterious-String420",
              "text": "Share a link to any current 10$ gadget with butt loads of vram",
              "score": 1,
              "created_utc": "2026-01-24 13:43:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1g1tep",
                  "author": "Cold_Hard_Sausage",
                  "text": "Bro, have someone teach you the concept of sarcasm",
                  "score": 1,
                  "created_utc": "2026-01-24 16:08:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r7bdf",
          "author": "Rain_Sunny",
          "text": "When we use this Graphic card to build a 4-cards Ai-workstation:\n\nTotal VRAM: 384 GB.\n\nBandwidth: 7.2 TB.\n\nComput PowerÔºàFP4Ôºâ: 16 PFLOPs.\n\nTokens Output: 50 tokens/s(70B). 15 Tokens/s(405B).\n\nLLMs Support: DeepSeek V3,Llama 405B...\n\nPrice will be acound 60000 USD.\n\nBy the way, graphic card's price seems very good!",
          "score": 1,
          "created_utc": "2026-01-26 03:41:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21ecgn",
          "author": "kaisersolo",
          "text": "Nvidia would make more bucks with 3 5090's instead of this even though we all want it",
          "score": 1,
          "created_utc": "2026-01-27 16:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21xpqj",
          "author": "PhotographerUSA",
          "text": "What a deal ! I need two more for the kids. Don't want their imagination to be dulled out lacking VRAM on their AI!",
          "score": 1,
          "created_utc": "2026-01-27 17:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ck7rm",
          "author": "Accomplished-Grade78",
          "text": "Anyone compare dual Max-q vs DGXA Spark? \n\n192GB vs 128GB\n\nDDR7 vs LPDDR5X unified \n\nWhat does this mean for real world performance, in your experience?",
          "score": 1,
          "created_utc": "2026-01-24 01:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ndmwh",
              "author": "One-Macaron6752",
              "text": "LMGTFY",
              "score": 2,
              "created_utc": "2026-01-25 17:07:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1otzko",
                  "author": "Accomplished-Grade78",
                  "text": "Thanks, it‚Äôs nice to hear recent experiences",
                  "score": 1,
                  "created_utc": "2026-01-25 20:51:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d0fbd",
          "author": "TheRiddler79",
          "text": "If I'm being fair, that thing looks bad to the fucking bone, but if I what's going to spend eight grand right now, I'd probably look for server box that would run eight V100 32 GB. Like I understand the difference in the technology and I suppose it just depends on your ultimate goal, but you could run twice as large of an AI and your inference speed would still be lightning fast. But again everybody has their own motivations. For me I'm kind of like looking at where can I get the largest amount of vram for the minimal amount of money which I'm sure most people also think but at the end of the day I also will trade 2000 tokens a second on GPT OSS 120b for 500 tokens a second on Minimax 2.1",
          "score": 1,
          "created_utc": "2026-01-24 03:02:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkudvz",
      "title": "I gave my local LLM pipeline a brain - now it thinks before it speaks",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 15:50:14",
      "score": 69,
      "num_comments": 26,
      "upvote_ratio": 0.92,
      "text": "[Video from sequential retrieval](https://reddit.com/link/1qkudvz/video/9mel0vq9d4fg1/player)\n\nIn the video you can see how and that it works.\n\nJarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience.¬†[u/frank\\_brsrk](/user/frank_brsrk/)¬†Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nüß† Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions ‚Üí AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions ‚Üí instant answer.\n\nComplex questions ‚Üí step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to¬†[u/frank\\_brsrk](/user/frank_brsrk/)¬†for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build üòÖ\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\nsimple graphic:\n\n[Simple visualization of MCP retrieval](https://preview.redd.it/f9tm59rkd4fg1.png?width=863&format=png&auto=webp&s=6a65eda552b3b846863f75c59ee04018eb6d6c41)\n\n[Simple visualization pipeline](https://preview.redd.it/6h40kd1sd4fg1.png?width=1147&format=png&auto=webp&s=770dd510d8ebf0fe8b68f6a81bb7ab40d34fa862)\n\n@/[frank\\_brsrk](/user/frank_brsrk/) architecture of the causal intelligence module\n\n[architecture of the causal intelligence module](https://preview.redd.it/6x03fioje4fg1.jpg?width=2800&format=pjpg&auto=webp&s=7df7325899fd7dd3f8ca8743ebca4d04845868b5)\n\n\n\nSmall update the next days:\n\nhttps://preview.redd.it/9q7vo4rnbqfg1.png?width=1866&format=png&auto=webp&s=193c2a1adaabd721b0c04a8386dd9acf3b49f5ff\n\nhttps://preview.redd.it/bwcvm4rnbqfg1.png?width=1866&format=png&auto=webp&s=2457293e38992f70ff8290fada20104b19756a16\n\nhttps://preview.redd.it/ej38q5rnbqfg1.png?width=1866&format=png&auto=webp&s=efd4b330ed74bc48e9af713cc0e5568b94c3a5f7\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1amvip",
          "author": "GCoderDCoder",
          "text": "This is great! I love that a whole generation of technologists have spent our lives trying to be Iron Man and the few making the most progress by working together on it are trying to ruin it for the rest of us now that we can finally see the light at the other end of the tunnel. Thanks for helping the rest of us in the struggle to keep up!\n\nI'll be more focused one these types of projects when they eventually fire me because they don't realize how much we have to correct the AI still but I wish I had time to be consistent working on some joint projects like this. I'm trying to figure out how to piece together things like roo code with something like vibe kanban and MCPs in an opinionated way to reduce the manual burden while allowing more than coding using local LLMs and then here goes Anthropic with their cowork thing lol\n\nKeep up the hard work! When the fairytale of benevolent AI providers crumbles people will be looking for local LLMs.",
          "score": 14,
          "created_utc": "2026-01-23 19:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1aoe33",
              "author": "danny_094",
              "text": "That's exactly why I'm building this. Local-first, privacy-first, yours forever.",
              "score": 9,
              "created_utc": "2026-01-23 19:45:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a5160",
          "author": "No-Leopard7644",
          "text": "Congratulations, sounds interesting, thank you for sharing this information. Is it open source?",
          "score": 4,
          "created_utc": "2026-01-23 18:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a6tb4",
              "author": "danny_094",
              "text": "Yes you can Download on GitHub. It will always be free for private users. Everything for local users :)",
              "score": 9,
              "created_utc": "2026-01-23 18:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1c4d05",
          "author": "Endflux",
          "text": "If I can find the time I'll give it a try this weekend and let you know how it goes! Thanks for sharing :)",
          "score": 3,
          "created_utc": "2026-01-24 00:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c4op2",
              "author": "danny_094",
              "text": "I'm asking for it. I really need more user feedback :D",
              "score": 2,
              "created_utc": "2026-01-24 00:01:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aq2xn",
          "author": "burn-n-die",
          "text": "What's the system configuration you are using?",
          "score": 2,
          "created_utc": "2026-01-23 19:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1avqci",
              "author": "danny_094",
              "text": "Everything runs on an RTX 2060 Super.CPU and RAM aren't really being used. You can find my Ollama container file in the wiki.",
              "score": 3,
              "created_utc": "2026-01-23 20:19:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1b1v5b",
                  "author": "burn-n-die",
                  "text": "Thanks. I am not a software engineer and I don't code. Will you guide be straight forward?",
                  "score": 2,
                  "created_utc": "2026-01-23 20:48:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dvx2l",
          "author": "Hot_Rip_4912",
          "text": "Wow ,man that feels good",
          "score": 2,
          "created_utc": "2026-01-24 06:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f58ev",
          "author": "JinkerGaming",
          "text": "Amazing! Thank you good sir. I will certainly be forking this. :)",
          "score": 2,
          "created_utc": "2026-01-24 13:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixovi",
          "author": "sweetbacon",
          "text": "This looks **very** interesting, thanks for sharing.",
          "score": 2,
          "created_utc": "2026-01-25 00:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j5isr",
          "author": "yeahlloow",
          "text": "Can someone please explain what is the difference between this and the thinking mode of normal LLMs?",
          "score": 2,
          "created_utc": "2026-01-25 00:51:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mpyju",
              "author": "danny_094",
              "text": "LLMs always think and answer based on the prediction of the next word. It's like autocorrect on steroids. In this case, the LLM is given less room to digress. So that even in longer contexts and questions, it doesn't start to digress or make things up. Imagine the LLM is the train, and the pipeline is the track. There are few opportunities to deviate.",
              "score": 1,
              "created_utc": "2026-01-25 15:23:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dk5rd",
          "author": "leonbollerup",
          "text": "Hey, if you have 15 min to spare ‚Ä¶ listen this pod cast, or scroll forward to the part about augmented LLM - maybe that‚Äôs something that improve your solution even more\n\n[https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1](https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1)\n\nI‚Äôm far from done with ArcAI.. but I could share you the concepts.\n\nhttps://preview.redd.it/sgo3krcbd8fg1.jpeg?width=812&format=pjpg&auto=webp&s=57fbd6c153fac073071e46c4985b71fadb7fcaeb\n\nThese are the result of controlled continued tests",
          "score": 1,
          "created_utc": "2026-01-24 05:10:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1usmpu",
          "author": "danny_094",
          "text": "https://preview.redd.it/hpui09pebqfg1.png?width=1866&format=png&auto=webp&s=c463c0af62940e7b2dc348c69405414f3aa7336e\n\nI'm feeling a bit over-motivated right now.The web interface will be slightly improved.",
          "score": 1,
          "created_utc": "2026-01-26 17:32:13",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1nlpqo",
          "author": "Available-Craft-5795",
          "text": "It kinda sounds like COCONUT but with a really long latent space. I dont really see the point right now",
          "score": 0,
          "created_utc": "2026-01-25 17:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nq626",
              "author": "danny_094",
              "text": "Fair comparison! The key difference: Jatvus/TRION isnt about latent reasoning  itss about orchestrating multiple models with explicit validation checkpoints (CIM catches antipatterns, biases, fallacies). Think of it as adding guardrails + quality control between reasoning steps, not just making the reasoning longer. The 3 layer architecture lets you use smaller, specialized models instead of one massive model.",
              "score": 1,
              "created_utc": "2026-01-25 18:00:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1obt0v",
                  "author": "Available-Craft-5795",
                  "text": "Then that sounds like a TRM with COCONUT smashed togeather",
                  "score": 1,
                  "created_utc": "2026-01-25 19:31:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1oc25t",
                  "author": "Available-Craft-5795",
                  "text": "Can you explain what it does that a TRM or COCONUT architecture doesnt do?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqhja",
      "title": "This Week's Hottest Hugging Face Releases: Top Picks by Category!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "author": "techlatest_net",
      "created_utc": "2026-01-22 09:52:51",
      "score": 54,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.\n\nCheck 'em out and drop your thoughts‚Äîwhich one's getting deployed first?\n\n# Text Generation\n\n* [**zai-org/GLM-4.7-Flash**](https://huggingface.co/zai-org/GLM-4.7-Flash): 31B param model for fast, efficient text gen‚Äîupdated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.\n* [**unsloth/GLM-4.7-Flash-GGUF**](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF): Quantized 30B version for easy local inference‚Äîhot with 112k downloads in hours. Great for low-resource setups.\n\n# Image / Multimodal\n\n* [**zai-org/GLM-Image**](https://huggingface.co/zai-org/GLM-Image): Image-text-to-image powerhouse‚Äî10.8k downloads, 938 likes. Excels in creative edits and generation.\n* [**google/translategemma-4b-it**](https://huggingface.co/google/translategemma-4b-it): 5B vision-language model for multilingual image-text tasks‚Äî45.4k downloads, supports translation + vision.\n\n# Audio / Speech\n\n* [**kyutai/pocket-tts**](https://huggingface.co/kyutai/pocket-tts): Compact TTS for natural voices‚Äî38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.\n* [**microsoft/VibeVoice-ASR**](https://huggingface.co/microsoft/VibeVoice-ASR): 9B ASR for multilingual speech recognition‚Äîultra-low latency, 816 downloads already spiking.\n\n# Other Hot Categories (Video/Agentic)\n\n* [**Lightricks/LTX-2**](https://huggingface.co/Lightricks/LTX-2) (Image-to-Video): 1.96M downloads, 1.25k likes‚Äîpro-level video from images.\n* [**stepfun-ai/Step3-VL-10B**](https://huggingface.co/stepfun-ai/Step3-VL-10B) (Image-Text-to-Text): 10B VL model for advanced reasoning‚Äî28.6k downloads in hours.\n\nThese are dominating trends with massive community traction.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o14xz85",
          "author": "Count_Rugens_Finger",
          "text": "I have found that GLM-4.7 30B-A3B model is actually inferior to Qwen3-Coder 30B-A3B for programming.  Anyone else?",
          "score": 1,
          "created_utc": "2026-01-22 23:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dhymn",
          "author": "Blizado",
          "text": "Why is Qwen3-TTS missing? For me clearly the most exciting new TTS models, especially since they have multi-language support and not only english as still too many new TTS models have.",
          "score": 1,
          "created_utc": "2026-01-24 04:55:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3cgo",
      "title": "the state of local agentic \"action\" is still kind of a mess",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "author": "Ilove_Cakez",
      "created_utc": "2026-01-21 16:56:55",
      "score": 50,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "spent the last few nights trying to get a decent mcp setup running for my local stack and it‚Äôs honestly depressing how much friction there still is. we‚Äôve got these massive models running on consumer hardware, but as soon as you want them to actually do anything.. like pull from a local db or interact with an api so you‚Äôre basically back to writing custom boilerplate for every single tool.\n\nthe security trade-offs are the worst part. it‚Äôs either total isolation (useless) or giving the model way too much permission because managing granular mcp servers manually is a full-time job. i‚Äôve been trying to find a middle ground where i don‚Äôt have to hand-roll the auth and logging for every connector.\n\nfound a tool that‚Äôs been helping with the infra side of it. it basically just handles the mcp server generation and the governance/permissions layer so i don't have to think too much (ogment ai, i'm sure most of you know about it). it‚Äôs fine for skipping the boring stuff, but i‚Äôm still annoyed that this isn't just native or more standardized yet.\n\nhow are you guys actually deploying agents that can touch your data? are you just building your own mcp wrappers from scratch or is there a better way to handle the permissioning? curious",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0z1cn1",
          "author": "Fortyseven",
          "text": "I've had some modest success using 30b qwen3-coder and the Qwen Code TUI. It won't replace my cloud-based stuff, but I always give it a go now and then and find myself surprised at how capable it can be.",
          "score": 2,
          "created_utc": "2026-01-22 02:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xvvdv",
          "author": "ForsookComparison",
          "text": "Running agentically is simply too hard for most small or mid-sized open weight models. Once you add a few tools and a multi-step feedback loop, the *bare minimum* becomes either Qwen3-Next-80B or Gpt-oss-120b, both of which still will fall flat on their face along the way. GLM 4.6V is probably the actual starting line for reliable use and even that will be spotty as you add more and more instructions and tools.\n\nI don't have a good answer for you besides *\"I've experienced this and think that agents are just hard\"*.",
          "score": 1,
          "created_utc": "2026-01-21 22:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o101cuk",
          "author": "mister2d",
          "text": "I'm mildly curious about this topic. How much could a framework like a self-hosted n8n instance fill the gaps?",
          "score": 1,
          "created_utc": "2026-01-22 06:02:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10t0ef",
          "author": "techlatest_net",
          "text": "Yeah man, local agents are still duct tape and prayers‚Äîtool perms either nuke security or turn models into toddlers with car keys. Ogment's solid for skipping the mcp boilerplate grind tho.\n\nCrewAI + containerized tools (dockerized APIs) saved my sanity‚Äîgranular perms via bind mounts, no root hell. You running ollama under that? What's your stack look like?",
          "score": 1,
          "created_utc": "2026-01-22 10:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wmw3y",
          "author": "atlasnomos",
          "text": "You‚Äôre not wrong ‚Äî the pain point you‚Äôre describing is real and structural, not a tooling gap.\n\nWhat keeps biting people is that ‚Äúagent action‚Äù today is basically glued together from three things that don‚Äôt want to coexist:\n\n1. prompt-time intent\n2. tool adapters (MCP / wrappers)\n3. runtime execution with side effects\n\nMost stacks solve (1) and (2), but almost nobody treats (3) as a **first-class runtime concern**. So permissions end up living either:\n\n* inside prompts (brittle),\n* inside per-tool wrappers (boilerplate explosion),\n* or at the MCP server boundary (coarse, hard to reason about).\n\nThe isolation vs over-permission tradeoff you mention is exactly what happens when there‚Äôs no **deny-by-default execution layer** sitting *between* the model and the tools.\n\nWhat‚Äôs worked best for us locally is:\n\n* keep MCP servers dumb (pure capability exposure)\n* move auth, cost limits, logging, and allow/deny decisions into a single runtime gate\n* treat every tool call as an auditable event, not ‚Äújust another function call‚Äù\n\nThat way you‚Äôre not re-implementing auth + logging per connector, and you‚Äôre not trusting the model to behave just because the prompt says so.\n\nI agree it *should* be more native / standardized. Until there‚Äôs a real spec for agent execution (not just tool schemas), everyone‚Äôs either rolling wrappers or accepting scary permissions.\n\nCurious what level you‚Äôre aiming for locally:\n\n* human-in-the-loop approvals?\n* strict allowlists?\n* cost / rate enforcement?\n* or just ‚Äúdon‚Äôt nuke my data‚Äù?\n\nThat choice seems to drive everything else.",
          "score": -3,
          "created_utc": "2026-01-21 19:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yrrf6",
              "author": "Big-Masterpiece-9581",
              "text": "I wish this kind of sanity would reign. But working in Big Corp is a bit like the Trump Administration in this job market. Pure politics, cloak and dagger, backstabbing, constant reorgs. Nobody is sharing anything, and each is reinventing every wheel especially around AI / MCP.",
              "score": 1,
              "created_utc": "2026-01-22 01:17:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11lp1w",
                  "author": "atlasnomos",
                  "text": "That‚Äôs a fair take ‚Äî and honestly, it matches what we keep hearing from people inside large orgs.\n\nMost of the time it‚Äôs not that the problems aren‚Äôt understood; it‚Äôs that ownership is fragmented and incentives don‚Äôt line up. Governance ends up living in the cracks between teams, so everyone quietly rebuilds their own glue and hopes it never becomes *their* incident.\n\nWe‚Äôre under no illusion that clean architectures win on their own. In practice, they only surface once failure, liability, or regulatory pressure forces shared responsibility. Until then, it‚Äôs politics and reorgs all the way down.\n\nAppreciate you saying this out loud ‚Äî it‚Äôs useful context, not pushback.",
                  "score": 1,
                  "created_utc": "2026-01-22 13:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qiswy6",
      "title": "Olmo 3.1 32B Think ‚Äî second place on hard reasoning, beating proprietary flagships",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-01-21 08:56:26",
      "score": 49,
      "num_comments": 14,
      "upvote_ratio": 0.95,
      "text": "Running peer evaluations of frontier models (The Multivac). Today's constraint satisfaction puzzle had interesting results for local LLM folks.\n\n**Top 3:**\n\n1. Gemini 3 Pro Preview: 9.13\n2. **Olmo 3.1 32B Think: 5.75** ‚Üê Open source\n3. GPT-OSS-120B: 4.79 ‚Üê Open source\n\n**Models Olmo beat:**\n\n* Claude Opus 4.5 (2.97)\n* Claude Sonnet 4.5 (3.46)\n* Grok 3 (2.25)\n* DeepSeek V3.2 (2.99)\n\n**The task:** Schedule 5 people for meetings across Mon-Fri with 9 interlocking logical constraints. Requires recognizing structural impossibilities and systematic constraint propagation.\n\n**Notes on Olmo:**\n\n* High variance (¬±4.12) ‚Äî inconsistent but strong ceiling\n* Extended thinking appears to help on this problem class\n* 32B is runnable on consumer hardware (with quantization)\n* Apache 2.0 license\n\n**Questions for the community:**\n\n* What quantizations are people running Olmo 3.1 at?\n* Performance on other reasoning tasks?\n* Any comparisons vs DeepSeek for local deployment?\n\nFull results at [themultivac.com](http://themultivac.com)\n\nLink: [https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\nhttps://preview.redd.it/jko9h4ox2oeg1.png?width=1208&format=png&auto=webp&s=07f7967899cc6f7d6252eed866ef5f4003f3288b\n\n**Daily runs and Evals. Cheers!**",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0tqymb",
          "author": "Mabuse046",
          "text": "I like Allen AI's reasoning dataset and I tend to take chunks of it to train my own models on. It's nice to see them scoring highly.",
          "score": 8,
          "created_utc": "2026-01-21 09:13:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzro0",
              "author": "randygeneric",
              "text": "you mean benchmaxing?",
              "score": -6,
              "created_utc": "2026-01-21 10:36:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ua463",
                  "author": "silenceimpaired",
                  "text": "Why on earth are you asking this question? No where does the person you‚Äôre commenting on mention test sets. They mention reasoning traces. Now if you were to say Allen AI has polluted datasets with test data present that would make sense.",
                  "score": 10,
                  "created_utc": "2026-01-21 12:01:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0w2m31",
                  "author": "Vegetable-Second3998",
                  "text": "This is such a dumb term. If a model learns the concepts of a benchmark, it has learned it. Create better benchmarks then. That‚Äôs just doing what humans also do - memorize shit for efficiency.",
                  "score": 3,
                  "created_utc": "2026-01-21 17:33:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzul1",
                  "author": "Mabuse046",
                  "text": "What do you mean?",
                  "score": 7,
                  "created_utc": "2026-01-21 10:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xps63",
          "author": "segmond",
          "text": "I hope you're right.  If so, this goes to show that folks should stop looking for the ONE magic model.  We saw with qwen2.5-32b-coder that a model can be very good in a specific domain.",
          "score": 2,
          "created_utc": "2026-01-21 21:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z5lcr",
          "author": "TheOdbball",
          "text": "Luckily for everyone I‚Äôm building a headless Linux based file system that would make changing out your LLM without losing years of productivity on the fly üòé\n\nWill definitely try olmo out thanks",
          "score": 1,
          "created_utc": "2026-01-22 02:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o152b5x",
          "author": "Direct_Turn_1484",
          "text": "I tried running this one in Ollama and it crashed. Yes I updated Ollama. I tried downloading it again. Still crashed. I really wanted to play with Omni-3.1. I guess I‚Äôll just have to try with vllm.",
          "score": 1,
          "created_utc": "2026-01-22 23:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o159t1u",
              "author": "Chalutation",
              "text": "vllm is way more efficient than ollama, but you don't have the pull feature like ollama does.",
              "score": 1,
              "created_utc": "2026-01-23 00:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o15u91m",
              "author": "jadecamaro",
              "text": "Crashed for me in LM Studio too even lowering context down",
              "score": 1,
              "created_utc": "2026-01-23 02:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qitnbv",
      "title": "The Case for a $600 Local LLM Machine",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "author": "tony10000",
      "created_utc": "2026-01-21 09:42:30",
      "score": 49,
      "num_comments": 58,
      "upvote_ratio": 0.93,
      "text": "**The Case for a $600 Local LLM Machine**\n\nUsing the Base Model Mac mini M4\n\nhttps://preview.redd.it/5c916gwucoeg1.png?width=1182&format=png&auto=webp&s=68d91da71f6244d752e15922e47dfbf9d792beb1\n\nby Tony Thomas\n\nIt started as a simple experiment. How much real work could I do on a small, inexpensive machine running language models locally?\n\nWith GPU prices still elevated, memory costs climbing, SSD prices rising instead of falling, power costs steadily increasing, and cloud subscriptions adding up, it felt like a question worth answering. After a lot of thought and testing, the system I landed on was a base model Mac mini M4 with 16 GB of unified memory, a 256 GB internal SSD, a USB-C dock, and a 1 TB external NVMe drive for model storage. Thanks to recent sales, the all-in cost came in right around $600.\n\nOn paper, that does not sound like much. In practice, it turned out to be far more capable than I expected.\n\nLocal LLM work has shifted over the last couple of years. Models are more efficient due to better training and optimization. Quantization is better understood. Inference engines are faster and more stable. At the same time, the hardware market has moved in the opposite direction. GPUs with meaningful amounts of VRAM are expensive, and large VRAM models are quietly disappearing. DRAM is no longer cheap. SSD and NVMe prices have climbed sharply.\n\nAgainst that backdrop, a compact system with tightly integrated silicon starts to look less like a compromise and more like a sensible baseline.\n\n**Why the Mac mini M4 Works**\n\nThe M4 Mac mini stands out because Apple‚Äôs unified memory architecture fundamentally changes how a small system behaves under inference workloads. CPU and GPU draw from the same high-bandwidth memory pool, avoiding the awkward juggling act that defines entry-level discrete GPU setups. I am not interested in cramming models into a narrow VRAM window while system memory sits idle. The M4 simply uses what it has efficiently.\n\nSixteen gigabytes is not generous, but it is workable when that memory is fast and shared. For the kinds of tasks I care about, brainstorming, writing, editing, summarization, research, and outlining, it holds up well. I spend my time working, not managing resources.\n\nThe 256 GB internal SSD is limited, but not a dealbreaker. Models and data live on the external NVMe drive, which is fast enough that it does not slow my workflow. The internal disk handles macOS and applications, and that is all it needs to do. Avoiding Apple‚Äôs storage upgrade pricing was an easy decision.\n\nThe setup itself is straightforward. No unsupported hardware. No hacks. No fragile dependencies. It is dependable, UNIX-based, and boring in the best way. That matters if you intend to use the machine every day rather than treat it as a side project.\n\n**What Daily Use Looks Like**\n\nThe real test was whether the machine stayed out of my way.\n\nQuantized 7B and 8B models run smoothly using Ollama and LM Studio. AnythingLLM works well too and adds vector databases and seamless access to cloud models when needed. Response times are short enough that interaction feels conversational rather than mechanical. I can draft, revise, and iterate without waiting on the system, which makes local use genuinely viable.\n\nLarger 13B to 14B models are more usable than I expected when configured sensibly. Context size needs to be managed, but that is true even on far more expensive systems. For single-user workflows, the experience is consistent and predictable.\n\nWhat stood out most was how quickly the hardware stopped being the limiting factor. Once the models were loaded and tools configured, I forgot I was using a constrained system. That is the point where performance stops being theoretical and starts being practical.\n\nIn daily use, I rotate through a familiar mix of models. Qwen variants from 1.7B up through 14B do most of the work, alongside Mistral instruct models, DeepSeek 8B, Phi-4, and Gemma. On this machine, smaller Qwen models routinely exceed 30 tokens per second and often land closer to 40 TPS depending on quantization and context. These smaller models can usually take advantage of the full available context without issue.\n\nThe 7B to 8B class typically runs in the low to mid 20s at context sizes between 4K and 16K. Larger 13B to 14B models settle into the low teens at a conservative 4K context and operate near the upper end of acceptable memory pressure. Those numbers are not headline-grabbing, but they are fast enough that writing, editing, and iteration feel fluid rather than constrained. I am rarely waiting on the model, which is the only metric that actually matters for my workflow.\n\n**Cost, Power, and Practicality**\n\nAt roughly $600, this system occupies an important middle ground. It costs less than a capable GPU-based desktop while delivering enough performance to replace a meaningful amount of cloud usage. Over time, that matters more than peak benchmarks.\n\nThe Mac mini M4 is also extremely efficient. It draws very little power under sustained inference loads, runs silently, and requires no special cooling or placement. I routinely leave models running all day without thinking about the electric bill.\n\nThat stands in sharp contrast to my Ryzen 5700G desktop paired with an Intel B50 GPU. That system pulls hundreds of watts under load, with the B50 alone consuming around 50 watts during LLM inference. Over time, that difference is not theoretical. It shows up directly in operating costs.\n\nThe M4 sits on top of my tower system and behaves more like an appliance. Thanks to my use of a KVM, I can turn off the desktop entirely and keep working. I do not think about heat, noise, or power consumption. That simplicity lowers friction and makes local models something I reach for by default, not as an occasional experiment.\n\n**Where the Limits Are**\n\nThe constraints are real but manageable. Memory is finite, and there is no upgrade path. Model selection and context size require discipline. This is an inference-first system, not a training platform.\n\nApple Silicon also brings ecosystem boundaries. If your work depends on CUDA-specific tooling or experimental research code, this is not the right machine. It relies on Apple‚Äôs Metal backend rather than NVIDIA‚Äôs stack. My focus is writing and knowledge work, and for that, the platform fits extremely well.\n\n**Why This Feels Like a Turning Point**\n\nWhat surprised me was not that the Mac mini M4 could run local LLMs. It was how well it could run them given the constraints.\n\nFor years, local AI was framed as something that required large amounts of RAM, a powerful CPU, and an expensive GPU. These systems were loud, hot, and power hungry, built primarily for enthusiasts. This setup points in a different direction. With efficient models and tightly integrated hardware, a small, affordable system can do real work.\n\nFor writers, researchers, and independent developers who care about control, privacy, and predictable costs, a budget local LLM machine built around the Mac mini M4 no longer feels experimental. It is something I turn on in the morning, leave running all day, and rely on without thinking about the hardware.\n\nMore than any benchmark, that is what matters.\n\nFrom: tonythomas-dot-net",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0uugib",
          "author": "TimLikesAI",
          "text": "I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 and use it similarly. The extra headroom allows for running some pretty powerful models that are in the 14-20GB range.",
          "score": 12,
          "created_utc": "2026-01-21 14:06:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x9s9c",
              "author": "fallingdowndizzyvr",
              "text": "> I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 \n\nYou could have gotten it new for cheaper on Ebay from a liquidator.",
              "score": 0,
              "created_utc": "2026-01-21 20:45:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0y4lrx",
                  "author": "jcktej",
                  "text": "Asking for a friend. How do I find such vendors?",
                  "score": 1,
                  "created_utc": "2026-01-21 23:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tyaic",
          "author": "PraxisOG",
          "text": "I sincerely hope this is the future. An easy to use, low upfront and ongoing cost box that privately serves LLMs and maybe more. The software, while impressive, leaves much to be desired in terms of usability. This is from the perspective of having recently thrown together the exact kind of loud and expensive box you mentioned, that took days to get usable output from.¬†",
          "score": 6,
          "created_utc": "2026-01-21 10:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0z12z0",
              "author": "kermitt81",
              "text": "The M5 Mac Mini is expected sometime in the middle of this year, and offers significant improvements for LLM usage over the M4 Mac Mini. Pricing will likely be around the same, so it may be well worth the wait. \n\n(Each of the M5‚Äôs 10 GPU cores now includes a dedicated Neural Accelerator, and - according to benchmarks - the M5 delivers 3.6x faster time to first token compared to the M4.)",
              "score": 4,
              "created_utc": "2026-01-22 02:11:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12v5g7",
                  "author": "tony10000",
                  "text": "It will be interesting to see how the recent memory and storage price increases will impact pricing.  I don't think we will see a sub-$500 deal on the M5 Mini anytime soon.",
                  "score": 2,
                  "created_utc": "2026-01-22 17:15:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uc6wg",
          "author": "locai_al-ibadi",
          "text": "It is great seeing the capabilities of localised AI recently, compared to what we were capable of running a year ago (arguably even a months ago).",
          "score": 3,
          "created_utc": "2026-01-21 12:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ud5rp",
          "author": "alias454",
          "text": "If you look around you can get it for cheaper than that(micro center and best buy open box deals). I picked up an open box one and have actually been impressed. The onboard storage is probably the biggest complaint but it will do for now. My main laptop is an older Lenovo Legion 5 with 32GBs of ddr4 and an rtx2060. It was purchased around 2020 so it is starting to show it's age.",
          "score": 3,
          "created_utc": "2026-01-21 12:23:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v5cvv",
              "author": "tony10000",
              "text": "It is easy to add an external drive or a dock with a NVME slot for additional storage.  I keep all of the models, data, and caches on that.",
              "score": 5,
              "created_utc": "2026-01-21 15:01:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0i9e",
          "author": "Rabo_McDongleberry",
          "text": "For me. Speed isn't that big of an issue. And most of the things I do are just basic text generation and editing. Maybe answer a few questions that I cross references with legit sources. The Mac Mini works great for that.¬†",
          "score": 3,
          "created_utc": "2026-01-21 14:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wixf8",
          "author": "dual-moon",
          "text": "how much have you tried small models? many of them are extremely good; lots of tiny models get used as subagents in swarm setups. LiquidAI actually JUST released a 1.2B LFM2.5 Thinking model that would probably FLY on your machine :)",
          "score": 2,
          "created_utc": "2026-01-21 18:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103wgb",
              "author": "tony10000",
              "text": "Yes.  I use the small LiquidAI models, and Qwen 1.7B is a tiny-mighty LLM for drafting.",
              "score": 3,
              "created_utc": "2026-01-22 06:22:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1at66e",
                  "author": "GeroldM972",
                  "text": "Can concur about the LFM2 and LFM 2.5 models, because these are part of the set 'LM Studio - editor's pick' and for a very good reason.  These work really well with the LM Studio software.\n\nMy system is Ryzen 5 2400 with W11. 32 GB DDR4 RAM, a Crucial 2,5\" SSD (240 GB) and a AMD R580 GPU...with 16 GB of VRAM on it. So, 'simple' and 'weak-sauce' would be apt descriptions of this system. And yet, those LFM models work very well in here. Even if it is via Vulkan.\n\nedit:  \nIf you use LM Studio, it comes with MCP support, so these small models can now also look on the internet, can keep track of time and a few other things I find handy. Is very easy to set up and your local LLM becomes much more useful (if you trust information on the internet of course).",
                  "score": 1,
                  "created_utc": "2026-01-23 20:07:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0wub17",
              "author": "cuberhino",
              "text": "I‚Äôm currently considering a threadripper + 3090 build at around $2000 total cost to function as a local private ChatGPT replacement. \n\nDo you think this is overkill and I should go with one of these cheaper Mac systems?",
              "score": 2,
              "created_utc": "2026-01-21 19:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0x4ao8",
                  "author": "dual-moon",
                  "text": "try out some smaller models first, see how they work for you! if you find small models do the job, then scaling based on an agentic swarm rather than a single model may be best! but it really depends on what you want to use it for. if it's just chatting, deepseek can do most of what the big guys can!\n\nbut don't think a threadripper and a 3090 is a bad idea or anything :p",
                  "score": 2,
                  "created_utc": "2026-01-21 20:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y9qn3",
          "author": "yeeah_suree",
          "text": "Nice write up! Can you share a little more information on what you use the model for? What constitutes everyday use?",
          "score": 2,
          "created_utc": "2026-01-21 23:39:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103q6c",
              "author": "tony10000",
              "text": "I am a writer.  I use AI for brainstorming, outlining, summarizing, drafting, and sometimes editing and polishing.",
              "score": 1,
              "created_utc": "2026-01-22 06:21:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o13x82a",
                  "author": "tony10000",
                  "text": "I just posted another article today on this forum that details my workflow.",
                  "score": 0,
                  "created_utc": "2026-01-22 20:06:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0zg50d",
          "author": "Icy-Pay7479",
          "text": "Was this written by an 8b model?",
          "score": 2,
          "created_utc": "2026-01-22 03:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103f9p",
              "author": "tony10000",
              "text": "I typically use Qwen 14B for outlining, and my primary drafting models are Qwen 3B and 4B.  Sometimes even 1.7.  I use ChatGPT to polish and then heavily edit the result.",
              "score": 1,
              "created_utc": "2026-01-22 06:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zpx40",
          "author": "crossfitdood",
          "text": "dude you missed the black friday deals. I bought mine from costco for $479. But I was really upset when Microcenter had them on sale for $399",
          "score": 2,
          "created_utc": "2026-01-22 04:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1041ul",
              "author": "tony10000",
              "text": "Bought it from Amazon for $479.  It was around $600 all in for the M4, dock, and 1TB NVME.",
              "score": 1,
              "created_utc": "2026-01-22 06:24:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zretd",
          "author": "vmjersey",
          "text": "But can you play GTA V on it?",
          "score": 2,
          "created_utc": "2026-01-22 04:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1045ag",
              "author": "tony10000",
              "text": "No idea what that is.",
              "score": 2,
              "created_utc": "2026-01-22 06:24:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10f5cl",
          "author": "lucasbennett_1",
          "text": "Quantized 7B-14B like qwen3 or deepseek 8B run fluidly for writing/research without the power/heat mess of discrete GPUs. THe external nvme for models is a smart hack to dodge apples storage premiums too",
          "score": 2,
          "created_utc": "2026-01-22 08:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13xfmc",
              "author": "tony10000",
              "text": "I agree!",
              "score": 1,
              "created_utc": "2026-01-22 20:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11zlww",
          "author": "Known_Geologist1085",
          "text": "I know this convo is about doing it on the cheap, but I'd like to note that I have been running an m3 max macbook pro with 128GB ram for a couple years now and the unified memory is a god send.  There are some support issues with Metal/MPS for certain things related to certain quantization and sparse attention, but overall these machines are beasts.   I can get 40-50 tps on llama 70b.  The good news now is that the market is flooded with these m chip macbook airs and pros, and a lot of them are cheap if you buy used.  I wouldn't be surprised if someone makes, or has already made, software to cluster macs for inference stacks in order to find a use for these.",
          "score": 2,
          "created_utc": "2026-01-22 14:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12ws2s",
              "author": "tony10000",
              "text": "There is a solution for clustering Macs called Exo, but you need Thunderbolt 5 for top performance using RDMA.  Several YouTube videos demonstrate how well the clusters work for inference.",
              "score": 1,
              "created_utc": "2026-01-22 17:22:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x9ogi",
          "author": "fallingdowndizzyvr",
          "text": "$600! Dude overpaid. It's $400 at MC.",
          "score": 3,
          "created_utc": "2026-01-21 20:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zh61a",
              "author": "DerFreudster",
              "text": "I think that included the dock and the nvme.",
              "score": 2,
              "created_utc": "2026-01-22 03:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o102sjy",
                  "author": "tony10000",
                  "text": "Correct.  That was for everything.  I got the M4 for $479.",
                  "score": 3,
                  "created_utc": "2026-01-22 06:13:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vutf5",
          "author": "track0x2",
          "text": "I heard the primary limitation for LLMs on Mac‚Äôs is around text generation and if you want to do anything other than that (image gen, TTS, agentic work) they will struggle.",
          "score": 1,
          "created_utc": "2026-01-21 16:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vw0we",
              "author": "tony10000",
              "text": "It really depends on your use case and expectations.  Some very capable models are 14B and under.  If I need more capabilities, I can run some 30B models on my 5700G.  For more than that, there is ChatGPT and Open Router.",
              "score": 2,
              "created_utc": "2026-01-21 17:03:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wlbfx",
          "author": "Parking_Bug3284",
          "text": "This is really cool. I'm still sharing my gpu on my main but I'm building a similar thing on the software side. It sets up a base control system for local systems running on your machine. So if you have ollama and opencode it can build out what you need to gain access to unlimited memory management and access to programs that have a server running like image gen and what not. Does your system have an APi or mcp server to talk to it",
          "score": 1,
          "created_utc": "2026-01-21 18:54:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104kqn",
              "author": "tony10000",
              "text": "I use Anything LLM for API access, MCP, and RAG vector databases.",
              "score": 2,
              "created_utc": "2026-01-22 06:28:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10gg8v",
                  "author": "SelectArrival7508",
                  "text": "which is really nice as you can switch between you local llm and cloud-based llms",
                  "score": 2,
                  "created_utc": "2026-01-22 08:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wyis3",
          "author": "jarec707",
          "text": "Agreed, depending on use. Thanks for sharing the models you use; seem like good choices.",
          "score": 1,
          "created_utc": "2026-01-21 19:54:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104wim",
              "author": "tony10000",
              "text": "Yeah, I have a nice assortment of models.  Probably 200GB worth at present.",
              "score": 1,
              "created_utc": "2026-01-22 06:31:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x1zxe",
          "author": "Zyj",
          "text": "You could try to get a used PC with a RTX 5060 Ti 16GB for almost the same amount of money, like this one [https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369](https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369)",
          "score": 1,
          "created_utc": "2026-01-21 20:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104eke",
              "author": "tony10000",
              "text": "Not compact, portable, energy efficient, quiet, with low thermals.",
              "score": 1,
              "created_utc": "2026-01-22 06:27:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15ou5c",
          "author": "nitinmms1",
          "text": "My Mac Mini M4 24GB can run Qwen 14b quantized at decent speed.\nImage generation models with comfyui, though feel slow.\nBut I still feel 64GB M4 will easily do as a good base local AI machine.",
          "score": 1,
          "created_utc": "2026-01-23 01:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bd8k6",
          "author": "parboman",
          "text": "Tried doing anything with mlx instead of ollama?",
          "score": 1,
          "created_utc": "2026-01-23 21:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bf21h",
              "author": "tony10000",
              "text": "I have a MLX version of llama.cpp on the system, and LM Studio can also run:\n\nMetal Llama.cpp\n\nLM Studio MLX\n\nI can run both native MLX builds and GGUFs using Metal.\n\nAlso: Ollama on macOS automatically utilizes Apple's¬†Metal API¬†for GPU acceleration on Apple Silicon (M1/M2/M3/M4 chips), requiring no additional configuration.",
              "score": 1,
              "created_utc": "2026-01-23 21:50:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mzije",
          "author": "Cynical-Engineer",
          "text": "is it even performant? I have an M1 Max 64GB and 1TB SSD and when running Mistral it is really slow",
          "score": 1,
          "created_utc": "2026-01-25 16:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nakko",
              "author": "tony10000",
              "text": "It really depends on what you are doing.  I am a writer, and for most tasks it is fast enough.  I use models from 1.7B-14B and they run acceptably fast.  Not sure what Mistral variant you are referring to. \n\nMy main computer is a 5700G with 32GB of RAM and a 16GB Intel ARC B50.  I use it when I want to run models with bigger context windows, and also larger models (mostly MoE) like OSS 20B, Mistral Small 24B, Qwen 30B, Nemotron 30B, GLM 4.7 Flash 30B, etc.\n\nIf you are a professional coder, not even a Mac Studio 512GB can compare to enterprise GPUs:\n\n[https://www.youtube.com/watch?v=hxDe1j\\_IcSQ](https://www.youtube.com/watch?v=hxDe1j_IcSQ)",
              "score": 1,
              "created_utc": "2026-01-25 16:54:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qv7ng",
                  "author": "Cynical-Engineer",
                  "text": "True",
                  "score": 1,
                  "created_utc": "2026-01-26 02:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp0jhl",
      "title": "I used Clawdbot (now Moltbot) and here are some inconvenient truths",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qp0jhl/i_used_clawdbot_now_moltbot_and_here_are_some/",
      "author": "Andy18650",
      "created_utc": "2026-01-28 03:49:18",
      "score": 32,
      "num_comments": 15,
      "upvote_ratio": 0.88,
      "text": "Text wall warning :)\n\nI tried Clawdbot (before the name switch so I am going to keep using it) on a dedicated VPS and then a Raspberry Pi, both considered disposable instances with zero sensitive data. So I can say as a real user: The experience is awesome, but the project is terrible. The entire thing is very \\*very\\* vibe-coded and you can smell the code without even looking at it... \n\nI don't know how to describe it, but several giveaways are multiple instances of the same information (for example, model information is stored in both \\~/.clawdbot/clawdbot.json and \\~/.clawdbot/agents/main/agent/models.json. Same for authentication profiles), the /model command will allow you to select a invalid model (for example, I once entered anthropic/kimi-k2-0905-preview by accident and it just added that to the available model list and selected it. For those who don't know, Anthropic has their own Claude models and certainly doesn't host Moonshot's Kimi), and unless you run a good model (aka Claude Opus or Sonnet), it's going to break from time to time. \n\nI would not be surprised if this thing has 1000 CVEs in it. Yet judging by the speed of development, by the time those CVEs are discovered, the code base would have been refactored twice over, so that's security, I guess? (For reddit purposes this is a joke and security doesn't work that way and asking AI to refactor the code base doesn't magically remove vulnerabilities.)\n\nBy the way, did I mention it also burns tokens like a jet engine? I set up the thing and let it run for a while, and it cost me 8 MILLION TOKENS, on Claude-4.5-OPUS, the most expensive model I have ever paid for! But, on the flip side: I had NEVER set up any agentic workflow before. No LangChain, no MCP, nothing. Remember those 8 million tokens? With those tokens Claude \\*set itself up\\* and only asked for minimal information (such as API Keys) when necessary. Clawdbot is like an Apple product: when it runs it's like MAGIC, until it doesn't (for example, when you try to hook it up to kimi-k2-0905-preview non thinking, not even 1T parameters can handle this, thinking is a requirement).\n\nAlso, I am sure part of why smaller models don't work so well is probably due to how convoluted the command-line UI is, and how much it focuses on eyecandy instead of detailed information. So when it's the AI's turn to use it... Well it requires a big brain. I'm honestly shocked after looking at the architecture (which it seems to have none) that Claude Opus is able to set itself up.\n\nFinally, jokes and criticisms aside, using Clawdbot is the first time since the beginning of LLM that I genuinly feel like I'm talking to J.A.R.V.I.S. from Iron Man.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qp0jhl/i_used_clawdbot_now_moltbot_and_here_are_some/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o25voxs",
          "author": "Bananadite",
          "text": "My biggest issue is idk what to really use it for.",
          "score": 10,
          "created_utc": "2026-01-28 05:20:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25iz5o",
          "author": "No_Conversation9561",
          "text": "I‚Äôm glad that it exists. Now it (or something else) can only get better from here.",
          "score": 15,
          "created_utc": "2026-01-28 03:59:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26bgcs",
              "author": "Dry_Natural_3617",
              "text": "Valid point",
              "score": 2,
              "created_utc": "2026-01-28 07:23:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25sj55",
          "author": "alphatrad",
          "text": "It's a fricking wrapper with a pipe to Whatsapp and Cron jobs. I don't get the hype. Have been able to do most of this stuff, with N8N",
          "score": 12,
          "created_utc": "2026-01-28 04:59:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o265dc0",
              "author": "SharpKaleidoscope182",
              "text": ">a fricking wrapper with a pipe to Whatsapp and Cron jobs\n\nVery big deal to ppl who dont know what cron or n8n are. It's reaching a new segment of the market. No engineer can comprehend these things.",
              "score": 2,
              "created_utc": "2026-01-28 06:33:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o263b23",
              "author": "kal_0008",
              "text": "Super hyped, agree. we need somebody to build these 2 pipes ASAP. I starred Claudegram and runClauderun and suggested improvements to them as they will bring us closer to a remote agent.",
              "score": 1,
              "created_utc": "2026-01-28 06:17:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o26cwh3",
              "author": "Double-Lavishness870",
              "text": "The hype is justified. This will kill half of the current app ecosystem. Commodity apps like food delivery, health support, sport, family organization, meetup planning will be done silently in the background by my own assistant. Stupid apps like will disappear.\n\nIt is simple and not surprising, but closed a obvious gap. Big player will publish similar products",
              "score": -1,
              "created_utc": "2026-01-28 07:35:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25jgwq",
          "author": "macromind",
          "text": "Yep, this matches my experience with a lot of agentic CLIs, when it works it feels like magic, but the config and state management can get messy fast. Token burn is also real once you let an agent loop on tool calls.\n\nOne thing thats helped me is adding hard budgets (max steps, max tool calls, max tokens) plus logging every tool invocation so you can spot where it starts thrashing. Also, forcing the agent to write a short plan before execution cuts down on the random wandering.\n\nIf youre collecting notes on what patterns actually make these setups stable, I bookmarked a few practical breakdowns here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-01-28 04:02:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25q3yf",
          "author": "PK_Wins",
          "text": "Can we not change the api key to a different model ? which is cheaper or free ?",
          "score": 1,
          "created_utc": "2026-01-28 04:43:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25ra4b",
              "author": "DarkXanthos",
              "text": "Myself and others are trying to use local models but the implementation is basically broken. Bugs have been submitted.",
              "score": 2,
              "created_utc": "2026-01-28 04:51:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o267lew",
          "author": "bamboofighter",
          "text": "You‚Äôre right on the $ about the security flaws :) just have it evolve and make your source code a moving target that makes it not economically viable to go after. Polymorphic software is the future!",
          "score": 1,
          "created_utc": "2026-01-28 06:51:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26f7kt",
          "author": "HealthyCommunicat",
          "text": "The model id‚Äôs when using /model command has fallback behavior that you can setup when whatever model you select isnt available or invalid. I highly recommend hooking it up to opus and then asking it to walk you through the config or do ‚Äúclawdbot configure‚Äù and select the model tab and select ur provider and hook it up, the final tab will show a giant list of model id‚Äôs thst u can select and confirm u want to use.\n\nYou should ask it how it works and then setup proper skills and tools, those are the only two things needed as the entire thing loads up TOOLS.md to all models, and walk through setting up a skill/tool for each thing u wish to use the automation for. For example i even have a ssh tool that‚Äôs used just to ssh into stuff and only investigate, its made me be able to copy paste my clients email and just have it investigate.\n\nThe command line UI is fine. I hookd up mirothinker v1.5 30b a3b and because its just simple tool calls, once its setup literally a frequent gen 30b model can handle it. The .md has OR SHOULD HAVE all usage steps for all ur models to be able to have proper syntax to use tools etc. if you‚Äôre not setting this up properly to work, it is on you for not being resourceful enough to think things through.",
          "score": 1,
          "created_utc": "2026-01-28 07:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26hyq1",
              "author": "explustee",
              "text": "Dumb question. But do I understand you correctly that you set it up to route to different models depending on the task/prompt input.",
              "score": 1,
              "created_utc": "2026-01-28 08:20:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o26ipfp",
                  "author": "HealthyCommunicat",
                  "text": "I meant that I got my models setup that way yes, but no the /model does not get sent to the model, the clawdbot session catches that and responds with pre-set text. Am i understanding ur question correctly",
                  "score": 1,
                  "created_utc": "2026-01-28 08:27:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qk9ked",
      "title": "Good local LLM for coding?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "author": "Expensive-Time-7209",
      "created_utc": "2026-01-22 22:56:33",
      "score": 31,
      "num_comments": 28,
      "upvote_ratio": 0.91,
      "text": "I'm looking for a a good local LLM for coding that can run on my rx 6750 xt which is old but I believe the 12gb will allow it to run 30b param models but I'm not 100% sure. I think GLM 4.7 flash is currently the best but posts like this [https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular\\_opinion\\_glm\\_47\\_flash\\_is\\_just\\_a/](https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular_opinion_glm_47_flash_is_just_a/) made me hesitant\n\nBefore you say just download and try, my lovely ISP gives me a strict monthly quota so I can't be downloading random LLMS just to try them out",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o163q5w",
          "author": "Javanese1999",
          "text": "[https://huggingface.co/TIGER-Lab/VisCoder2-7B](https://huggingface.co/TIGER-Lab/VisCoder2-7B) = Better version of Qwen2.5-Coder-7B-Instruct\n\n[https://huggingface.co/openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) =Very fast under 20b, even if your model size exceeds the VRAM capacity and goes into ram.\n\n[https://huggingface.co/NousResearch/NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B) = Max picks IQ4\\_XS. This is just an alternative\n\nBut of all of them, my rational choice fell on gpt-oss-20b. It's heavily censored in refusal prompts, but it's quite reliable for light coding.",
          "score": 14,
          "created_utc": "2026-01-23 02:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16qo3q",
          "author": "RnRau",
          "text": "Pick a coding MoE model and then use llama.cpp inference engine to offload some of the model to your system ram.",
          "score": 3,
          "created_utc": "2026-01-23 05:14:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o190rhs",
              "author": "BrewHog",
              "text": "Does llama.cpp have the ability to use both CPU and GPU? Or are you suggesting running one process in CPU and another in GPU?",
              "score": 1,
              "created_utc": "2026-01-23 15:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bxmro",
                  "author": "RnRau",
                  "text": "It can use both in the same process. Do a google on 'moe offloading'.",
                  "score": 3,
                  "created_utc": "2026-01-23 23:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17b5hk",
              "author": "mintybadgerme",
              "text": "Or LMstudio.",
              "score": 1,
              "created_utc": "2026-01-23 07:59:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o17he3f",
          "author": "vivus-ignis",
          "text": "I've had the best results so far with gpt-oss:20b.",
          "score": 3,
          "created_utc": "2026-01-23 08:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18zcxh",
          "author": "DarkXanthos",
          "text": "I run QWEN3 coder 30B on my M1 Max 64GB and it works pretty well. I think I wouldn't go larger though.",
          "score": 3,
          "created_utc": "2026-01-23 15:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o191fwg",
              "author": "BrewHog",
              "text": "How much RAM does it use? Is that quantized?",
              "score": 1,
              "created_utc": "2026-01-23 15:16:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fb7mt",
                  "author": "guigouz",
                  "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally Q3 uses around 20gb here (~14gb on gpu + 6gb on system ram) for a 50k context.\n\nI also tried Q2 but it's too dumb for actual coding, Q3 seems to be the sweet spot for smaller GPUs (Q4 is not **that** better).",
                  "score": 2,
                  "created_utc": "2026-01-24 13:50:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14y7b5",
          "author": "Used_Chipmunk1512",
          "text": "Nope, 30B quantized to q4 will be too much for your gpu, don't download it. Stick with models under 10B",
          "score": 4,
          "created_utc": "2026-01-22 23:08:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14ykru",
              "author": "Expensive-Time-7209",
              "text": "Any recommendations under 10B?",
              "score": 1,
              "created_utc": "2026-01-22 23:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zjyb",
                  "author": "iMrParker",
                  "text": "GLM 4.6v flash is pretty competent for its size. It should fit quantized with an okay context size¬†",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15prnv",
          "author": "Available-Craft-5795",
          "text": "GPT OSS 20B if it fits. Could work just fine in RAM though.  \nIts surprisingly good",
          "score": 2,
          "created_utc": "2026-01-23 01:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exduu",
              "author": "Virtual_Actuary8217",
              "text": "Not even support agent tool calling no thank you",
              "score": -1,
              "created_utc": "2026-01-24 12:17:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1f9apu",
                  "author": "Available-Craft-5795",
                  "text": "https://preview.redd.it/xhk81ws4wafg1.png?width=965&format=png&auto=webp&s=be4edb166f90d403c3016ec71a66aa288a66b4e2\n\nWhat?  \nYes it does.",
                  "score": 1,
                  "created_utc": "2026-01-24 13:39:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1qpitb",
                  "author": "10F1",
                  "text": "Yes it does?",
                  "score": 1,
                  "created_utc": "2026-01-26 02:07:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rps45",
                  "author": "Virtual_Actuary8217",
                  "text": "It says one thing, but when you pair it with cline ,it basically can't do anything",
                  "score": 1,
                  "created_utc": "2026-01-26 05:41:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gqpua",
          "author": "SnooBunnies8392",
          "text": "I had Nvidia RTX 3060 12GB and I used\n\nQwen3 Coder @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nand\n\nGPT OSS 20B @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nBoth did offload a bit to system ram, but they were both useful anyway.",
          "score": 2,
          "created_utc": "2026-01-24 17:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o160ny9",
          "author": "No-Leopard7644",
          "text": "Try devstral, Qwen 2.5 Coder. You need to choose a quant so that the size of the model fits the vram. Also for coding you need some vram for context. What are using for model inference?",
          "score": 1,
          "created_utc": "2026-01-23 02:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o163jhs",
          "author": "nitinmms1",
          "text": "Anything beyond 8b q4 will be difficult.",
          "score": 1,
          "created_utc": "2026-01-23 02:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17n36t",
          "author": "WishfulAgenda",
          "text": "I‚Äôve found that higher q in smaller models is really helpful. Also don‚Äôt forget your system prompt or agent instructions.",
          "score": 1,
          "created_utc": "2026-01-23 09:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ao98f",
          "author": "Few_Size_4798",
          "text": "There are reviews on YouTube from last week:\n\nThe situation is as follows: even if you don't skimp on the Strix Halo ($2000+ today), all local ones can be shoved in the ass: Claude rules, and Gemini is already pretty good.",
          "score": 1,
          "created_utc": "2026-01-23 19:44:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21m0nc",
              "author": "GeroldM972",
              "text": "And none of the Youtube channels you pull information from receive any sponsorship from those same cloud-LLM providers and/or \"middle-men\" (those that allow you to connect to several of those cloud-LLM providers, via their single monthly subscription)?\n\nI use my own set of test questions and regularly test cloud and local LLMs. Cloud are often better and faster. Not always though.  But even NVidia claimed that the current cloud-LLM structures are not the solution, running local LLMs is. \n\nBesides, When I run local, I choose which model and its specialization, while I don't have any say in what the cloud-LLM  provider will give me. Or when they update their update their model and require me to rewrite/redefine configurations for agents, because of their internal changes.\n\nThere are very good reasons to use local LLMs, there are strong reasons to use cloud-provider LLMs. And it is not an 'either/or'-story, but an 'and' story. As in: use both at the moments in your processes that you need these to.",
              "score": 1,
              "created_utc": "2026-01-27 16:47:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o227mby",
                  "author": "Few_Size_4798",
                  "text": "I agree, but in the long run, cloud-based systems are constantly learning, including from closed data, so to speak, which cannot be said about local systems.\n\nLocal systems are good for texts, perhaps even for translations‚Äînot many idioms are used in everyday speech, but algorithms for specific languages need constant improvement.",
                  "score": 1,
                  "created_utc": "2026-01-27 18:20:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bxos0",
          "author": "Inevitable_Yard_6381",
          "text": "Hi totally new but tired of waiting Gemini on Android studio to answer...I have a MacBook Pro M1 pro 16 GB ram.. Any chance I could use a local LLM? \nAnd if possible how to integrate with my IDE to work like an agents and have access to my project? Could also be possible to send links to learn some new API or dependency? \nThanks in advance!!",
          "score": 1,
          "created_utc": "2026-01-23 23:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbadr",
          "author": "guigouz",
          "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally",
          "score": 1,
          "created_utc": "2026-01-24 13:50:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qljfuw",
      "title": "AI & ML Weekly ‚Äî Hugging Face Highlights",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "author": "techlatest_net",
      "created_utc": "2026-01-24 10:16:03",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Here are the most notable¬†**AI models released or updated this week on Hugging Face**, categorized for easy scanning üëá\n\n# Text & Reasoning Models\n\n* **GLM-4.7 (358B)**¬†‚Äî Large-scale multilingual reasoning model¬†[https://huggingface.co/zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **GLM-4.7-Flash (31B)**¬†‚Äî Faster, optimized variant for text generation¬†[https://huggingface.co/zai-org/GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)\n* **Unsloth GLM-4.7-Flash GGUF (30B)**¬†‚Äî Quantized version for local inference¬†[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n* **LiquidAI LFM 2.5 Thinking (1.2B)**¬†‚Äî Lightweight reasoning-focused LLM¬†[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n* **Alibaba DASD-4B-Thinking**¬†‚Äî Compact thinking-style language model¬†[https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking](https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking)\n\n# Agent & Workflow Models\n\n* **AgentCPM-Report (8B)**¬†‚Äî Agent model optimized for report generation¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n* **AgentCPM-Explore (4B)**¬†‚Äî Exploration-focused agent reasoning model¬†[https://huggingface.co/openbmb/AgentCPM-Explore](https://huggingface.co/openbmb/AgentCPM-Explore)\n* **Sweep Next Edit (1.5B)**¬†‚Äî Code-editing and refactoring assistant¬†[https://huggingface.co/sweepai/sweep-next-edit-1.5B](https://huggingface.co/sweepai/sweep-next-edit-1.5B)\n\n# Audio: Speech, Voice & TTS\n\n* **VibeVoice-ASR (9B)**¬†‚Äî High-quality automatic speech recognition¬†[https://huggingface.co/microsoft/VibeVoice-ASR](https://huggingface.co/microsoft/VibeVoice-ASR)\n* **PersonaPlex 7B**¬†‚Äî Audio-to-audio personality-driven voice model¬†[https://huggingface.co/nvidia/personaplex-7b-v1](https://huggingface.co/nvidia/personaplex-7b-v1)\n* **Qwen3 TTS (1.7B)**¬†‚Äî Custom & base voice text-to-speech models¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign)\n* **Pocket-TTS**¬†‚Äî Lightweight open TTS model¬†[https://huggingface.co/kyutai/pocket-tts](https://huggingface.co/kyutai/pocket-tts)\n* **HeartMuLa OSS (3B)**¬†‚Äî Text-to-audio generation model¬†[https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B](https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B)\n\n# Vision: Image, OCR & Multimodal\n\n* **Step3-VL (10B)**¬†‚Äî Vision-language multimodal model¬†[https://huggingface.co/stepfun-ai/Step3-VL-10B](https://huggingface.co/stepfun-ai/Step3-VL-10B)\n* **LightOnOCR 2 (1B)**¬†‚Äî OCR-focused vision-language model¬†[https://huggingface.co/lightonai/LightOnOCR-2-1B](https://huggingface.co/lightonai/LightOnOCR-2-1B)\n* **TranslateGemma (4B / 12B / 27B)**¬†‚Äî Multimodal translation models¬†[https://huggingface.co/google/translategemma-4b-it](https://huggingface.co/google/translategemma-4b-it)¬†[https://huggingface.co/google/translategemma-12b-it](https://huggingface.co/google/translategemma-12b-it)¬†[https://huggingface.co/google/translategemma-27b-it](https://huggingface.co/google/translategemma-27b-it)\n* **MedGemma 1.5 (4B)**¬†‚Äî Medical-focused multimodal model¬†[https://huggingface.co/google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it)\n\n# Image Generation & Editing\n\n* **GLM-Image**¬†‚Äî Text-to-image generation model¬†[https://huggingface.co/zai-org/GLM-Image](https://huggingface.co/zai-org/GLM-Image)\n* **FLUX.2 Klein (4B / 9B)**¬†‚Äî High-quality image-to-image models¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-4B](https://huggingface.co/black-forest-labs/FLUX.2-klein-4B)¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n* **Qwen Image Edit (LoRA / AIO)**¬†‚Äî Advanced image editing & multi-angle edits¬†[https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA](https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA)¬†[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **Z-Image-Turbo**¬†‚Äî Fast text-to-image generation¬†[https://huggingface.co/Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n\n# Video Generation\n\n* **LTX-2**¬†‚Äî Image-to-video generation model¬†[https://huggingface.co/Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n\n# Any-to-Any / Multimodal\n\n* **Chroma (6B)**¬†‚Äî Any-to-any multimodal generation¬†[https://huggingface.co/FlashLabs/Chroma-4B](https://huggingface.co/FlashLabs/Chroma-4B)",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qj3l75",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5lo30v55iqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 17:05:10",
      "score": 27,
      "num_comments": 11,
      "upvote_ratio": 0.74,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3l75/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ynu7l",
          "author": "RnRau",
          "text": "You might enjoy this paper from 2024;\n\n> Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget. \n\n\nhttps://arxiv.org/abs/2407.21787",
          "score": 3,
          "created_utc": "2026-01-22 00:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10txky",
          "author": "techlatest_net",
          "text": "5-AI debate cage match to kill hallucinations? Brutal elegance‚Äîforces consensus without trusting any single braindead output. Local Ollama hookup is the killer tho, total sovereignty.\n\nSpinning this up tonight. How's the inference overhead hit when all 5 duke it out? üî•",
          "score": 1,
          "created_utc": "2026-01-22 10:19:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12pjpn",
              "author": "PastTrauma21",
              "text": "Agreed! Having multiple models debate really helps filter out the noise, and not relying on just one output feels much safer.",
              "score": 1,
              "created_utc": "2026-01-22 16:50:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zyk7h",
          "author": "tvixii",
          "text": "just like how pewdiepie did it lol",
          "score": 0,
          "created_utc": "2026-01-22 05:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w1qch",
          "author": "jaxupaxu",
          "text": "So you copied pewdiepies council concept.¬†",
          "score": -16,
          "created_utc": "2026-01-21 17:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w353u",
              "author": "pokemonplayer2001",
              "text": "https://github.com/karpathy/llm-council",
              "score": 15,
              "created_utc": "2026-01-21 17:35:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0w54x5",
                  "author": "S_Anv",
                  "text": "Karpathy is a great man!\n\nKEA Research is designed as a user-friendly evolution. I've added image support, PDF/md export, text-to-speech conversion, and a full-fledged admin panel for managing local model sets without editing configuration files and many other features\n\nThis means you can create your own model set through a graphical interface  \nAlso as you see there is a bit different logic. You can check readme",
                  "score": 9,
                  "created_utc": "2026-01-21 17:44:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0w4qz2",
              "author": "FirstEvolutionist",
              "text": "I'm not suggesting OP didn't copy the idea from pewdiepie after hearing it from the then, but the idea of a \"council\" is not new, nor introduced by Pewidepie.",
              "score": 12,
              "created_utc": "2026-01-21 17:42:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ou0h",
              "author": "usercantollie",
              "text": "Yes, there are definitely similarities to PewDiePie‚Äôs council idea, but the approach here adds structured verification and provider-agnostic flexibility, which opens up new possibilities for local and open-source models. It‚Äôs cool to see how different projects build on each other to solve the trust problem in unique ways.",
              "score": 1,
              "created_utc": "2026-01-22 16:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yvab0",
          "author": "neoKushan",
          "text": "Can you get one of them to check your spelling before you post next time?",
          "score": -5,
          "created_utc": "2026-01-22 01:38:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103d5h",
              "author": "Free-Internet1981",
              "text": "oof",
              "score": 1,
              "created_utc": "2026-01-22 06:18:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qojhkj",
      "title": "Local LLM for Coding that compares with Claude",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qojhkj/local_llm_for_coding_that_compares_with_claude/",
      "author": "thecrogmite",
      "created_utc": "2026-01-27 16:59:10",
      "score": 26,
      "num_comments": 50,
      "upvote_ratio": 0.72,
      "text": "Currently I am on the Claude Pro plan paying $20 a month and I have hit my weekly and daily limits very quickly. Am I using it to essentially handle all code generation? Yes. This is the way it has to be as I'm not familiar with the language I'm forced to use. \n\n  \nI was wondering if there was a recommended model that I could use to match Claude's reasoning and code output. I don't need it to be *super fast* like Claude. I need it to be accurate and not completely ruin the project. While most of that I feel like is prompt related, some of that has to be related to the model. \n\n  \nThe model would be ran on a MacBook Pro M3. ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qojhkj/local_llm_for_coding_that_compares_with_claude/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o21s08f",
          "author": "Ryanmonroe82",
          "text": "You won't be able to match a cloud model on that setup",
          "score": 44,
          "created_utc": "2026-01-27 17:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21tkr5",
              "author": "MostIncrediblee",
              "text": "I have 48GB ram. What one model is best for coding. Probably not on Claude‚Äôs level.¬†",
              "score": 5,
              "created_utc": "2026-01-27 17:19:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21w9zw",
                  "author": "synth_mania",
                  "text": "Probably Devstral Small 2 (devstral-small-2-2512). It was released by Mistral last month.\n\nExcellent 24B param dense model. I'm running it at Q4\\_K\\_M to good effect, but the largest quant you can fit up to Q8 is probably worth sacrificing RAM for if you can still fit the context you need.\n\nI've had really good success with Aider, but it might trip up sometimes with less hands-on tools like Kilo Code, or other agentic programming applications.",
                  "score": 17,
                  "created_utc": "2026-01-27 17:31:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21xi13",
                  "author": "minaskar",
                  "text": "Devstral Small 2 (2512), GLM 4.7 Flash, and Qwen 3 Coder 30b",
                  "score": 6,
                  "created_utc": "2026-01-27 17:36:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o21wm9h",
                  "author": "hoowahman",
                  "text": "People usually mention qwen3coder for this, not sure how local GLM 4.7 fairs though in comparison. Probably pretty good.",
                  "score": 3,
                  "created_utc": "2026-01-27 17:33:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23940v",
                  "author": "RnRau",
                  "text": "ram or vram?\n\nIf ram, try gpt-oss-20b with thinking mode set on 'high'. If vram, try the big brother - gpt-oss-120b with model offloading.",
                  "score": 3,
                  "created_utc": "2026-01-27 21:05:14",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o23lrjt",
                  "author": "Ryanmonroe82",
                  "text": "Qwen coder is pretty good. Use the 32b version if possible",
                  "score": 2,
                  "created_utc": "2026-01-27 22:01:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o21x0zj",
          "author": "son_et_lumiere",
          "text": "Use the claude/larger model for the reasoning capabilities to break down big task into smaller coding tasks. Then take that task list and use a cheap/free model to the actual coding from the well defined task.",
          "score": 24,
          "created_utc": "2026-01-27 17:34:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22p0b9",
              "author": "intertubeluber",
              "text": "Look at the big brains on Brad!",
              "score": 3,
              "created_utc": "2026-01-27 19:34:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o22nwdn",
              "author": "Weird-Consequence366",
              "text": "This is the way",
              "score": 2,
              "created_utc": "2026-01-27 19:29:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o220e9j",
          "author": "pot_sniffer",
          "text": "You won't find a local llm as good as Claude but what i do use my Claude pro to do the planning, within that I plan to break the project up into manageable chunks. Then I get Claude to make structured json prompts for the local llm. I'm currently using qwen2.5-coder but I've had similar results with the other I've done this with",
          "score": 16,
          "created_utc": "2026-01-27 17:49:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22telk",
              "author": "thumperj",
              "text": "Just curious to learn a bit more about your workflow. Do you have this claude-->local handoff process scripted or do you do it manually?\n\nCurrently, I'm using claude cli for pretty much everything, which includes editing files but I'm also making a nice car payment to the claude gods every month....  One day soon I want to jump to a more efficient methodology BUT my current setup enables me to work like a banshee, produce excellent work and charge $$$$ to my clients so it's been well worth it.",
              "score": 4,
              "created_utc": "2026-01-27 19:54:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o234gun",
                  "author": "pot_sniffer",
                  "text": "Currently manual. Claude generates a structured JSON prompt, I copy/paste to Qwen2.5-coder, test the result. Haven't scripted it because I'm early in the build and the manual handoff isn't painful yet. Plan is to keep it manual until it gets annoying enough to justify writing automation.\nThe key is STATE.md - single source of truth that both Claude and local model read so they don't suggest things I've already tried/failed. That prevents context waste more than scripting would.",
                  "score": 3,
                  "created_utc": "2026-01-27 20:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22nun5",
              "author": "thecrogmite",
              "text": "Good to know, maybe this is the move for me. Thank you for that.",
              "score": 5,
              "created_utc": "2026-01-27 19:29:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22dml6",
          "author": "milkipedia",
          "text": "It seems this question is asked every day. Only difference is the user's available hardware, if they even bother to specify.",
          "score": 4,
          "created_utc": "2026-01-27 18:45:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22pptl",
              "author": "l8yters",
              "text": "welcome to reddit.",
              "score": 1,
              "created_utc": "2026-01-27 19:38:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22gsvc",
          "author": "armyknife-tools",
          "text": "Check out open routers leaderboard. I think it‚Äôs pretty accurate. 3 of the top 10 are open weights models.",
          "score": 3,
          "created_utc": "2026-01-27 18:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21ziyw",
          "author": "JordanAtLiumAI",
          "text": "Is your pain mostly code generation, or reasoning across a lot of project context like docs, configs, logs, and past commits?",
          "score": 2,
          "created_utc": "2026-01-27 17:45:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22nsjv",
              "author": "thecrogmite",
              "text": "Great question. For this specific task that I'm trying to overcome is code generation. The project has expanded from a SQL database to a .NET8 middleware to React front end. While I'm familiar with the SQL side of things slightly, the middleware and React are foreign.   \n  \nI've got the project at a solid first pass working state however making changes Claude seems to want to make huge passes across the entire architecture, then make it's decision as to what to change. While I believe that my prompting could improve to essentially add better guardrails, I'm worried I'll burn my availability fairly quickly regardless just based on project size.",
              "score": 1,
              "created_utc": "2026-01-27 19:29:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22ws9w",
                  "author": "JordanAtLiumAI",
                  "text": "Pro usage limits depend on the total length of your conversation, the number of messages, and which model or feature you use, and they may vary with current capacity.\n\n  \nSo as your project grows, it is normal that you hit limits faster, because each prompt tends to require more context and more turns. Their recommended mitigation is to be specific and concise and avoid vague prompts that trigger extra clarification cycles.\n\n  \nA practical workflow pattern that aligns with that  \n‚Ä¢ Convert ‚Äúmake this change‚Äù into a single narrowly defined task  \n‚Ä¢ Provide only the minimal relevant snippets, not the whole repo  \n‚Ä¢ Constrain edits to a file list  \n‚Ä¢ Require diff output  \n‚Ä¢ Plan first, then implement step 1\n\n  \nIt reduces the chance of large refactors and keeps each turn cheaper. \n\nHope this helps! Feel free to DM me if you want. We can dive in more.",
                  "score": 3,
                  "created_utc": "2026-01-27 20:09:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22e1oh",
          "author": "SimplyRemainUnseen",
          "text": "Depending on how much memory your system has GLM-4.7-Flash would be a good local model you can run. It won't be as good as Claude 4 models but it can handle a lot. I suggest giving it a try. I've found local LLMs have been at the performance I need for my programming workflow since 2024.",
          "score": 2,
          "created_utc": "2026-01-27 18:47:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22mwdv",
              "author": "thecrogmite",
              "text": "Thank you! I'll take a peek and give it a try.",
              "score": 1,
              "created_utc": "2026-01-27 19:25:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22zbr9",
          "author": "ServiceOver4447",
          "text": "Nopes, you will have to pay more for it to do your job.",
          "score": 2,
          "created_utc": "2026-01-27 20:21:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o232f71",
          "author": "StardockEngineer",
          "text": "We need to start having a flair post requirement for these posts.",
          "score": 2,
          "created_utc": "2026-01-27 20:35:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23sc20",
          "author": "Educational_Sun_8813",
          "text": "try devstral2 small, and qwen3-coder",
          "score": 2,
          "created_utc": "2026-01-27 22:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23tid2",
          "author": "Torodaddy",
          "text": "Qwen3 coder 30b model works well for me. Running it in llama.cpp",
          "score": 2,
          "created_utc": "2026-01-27 22:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22d1ip",
          "author": "greeny1greeny",
          "text": "nothing... all the latest models i tried are complete buns and dont even touch claude.",
          "score": 2,
          "created_utc": "2026-01-27 18:43:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22oh83",
          "author": "ithkuil",
          "text": "Claude Opus 4.5 is probably running on 8 x H200 clusters anywhere they have capacity for that. It's not the same because they do batching, but your Mac may be as much as 1000 times less powerful.¬†",
          "score": 1,
          "created_utc": "2026-01-27 19:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22p9xa",
          "author": "isleeppeople",
          "text": "Maybe not applicable but I want my machine to be an expert about itself to help me troubleshoot and add its own integrations that match my stack. It also helps to keep track of upgrades if I break something. I have a corpora of everything I use gitingest, readme docs, etc. I have it ingested in qdrant. Seems like you could do the same thing in your situation. Not an expert coder for everything but you might be able to make it as good as Claude in this one particular area.",
          "score": 1,
          "created_utc": "2026-01-27 19:36:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22tm19",
          "author": "passive_interest",
          "text": "I went this route on my M4 - developed a service that plans and applies atomic commits via local models & Ollama, but the round trip was painfully slow compared to having a Claude subagent or Codex skill perform the same task.",
          "score": 1,
          "created_utc": "2026-01-27 19:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22y83z",
          "author": "Stargazer1884",
          "text": "Before you go down the local route...(which I love, but it's not currently comparable to a frontier model on cloud) try using Opus to plan and Sonnet to execute the individual tasks.",
          "score": 1,
          "created_utc": "2026-01-27 20:16:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22zn8z",
          "author": "Terminator857",
          "text": "Comments in this poll might help answer: [https://www.reddit.com/r/LocalLLaMA/comments/1qj935h/poll\\_when\\_will\\_we\\_have\\_a\\_30b\\_open\\_weight\\_model\\_as/](https://www.reddit.com/r/LocalLLaMA/comments/1qj935h/poll_when_will_we_have_a_30b_open_weight_model_as/)",
          "score": 1,
          "created_utc": "2026-01-27 20:22:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22vnc1",
          "author": "bakawolf123",
          "text": "I'd suggest antigravity/gemini-cli as an option. Beside Gemini Pro it even has Opus included and while the limit for latter is quite short Gemini itself is good enough.   \n  \nAs for local models, current best small model for agent is GLM4.7 Flash. However Macs are really bad with prefill/prompt processing and afaik M3 PRO also screwed up architecture (lower memory bandwith) so it is worse than M1 Pro https://github.com/ggml-org/llama.cpp/discussions/4167. Current harnesses all start with 10-15k token system prompt so it feels quite garbage.  \nThere's hope for M5 with Pro/Max and possibly even Ultra coming this year though.",
          "score": 1,
          "created_utc": "2026-01-27 20:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23jh1f",
          "author": "Decent-Freedom5374",
          "text": "I train Sensei off Claude and codex, he‚Äôs an intelligent layer that orchestrates my ollama models, with the ability to rag anything that Claude and codex does he works just as good as them! :)",
          "score": 1,
          "created_utc": "2026-01-27 21:51:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23d2tk",
          "author": "Christosconst",
          "text": "Sonnet 4.5 is good and cheaper than opus",
          "score": 0,
          "created_utc": "2026-01-27 21:22:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2251c6",
          "author": "Jackster22",
          "text": "GLM 4.5 has been great with Claude via their own cloud service.\n$6 a month option, currently 50% off for the first month, gives you more than the Claude $20 subscription.\n\nI had done 200,000,000 tokens in the past 24 hours and it has been solid. No time outs no nothing.\n\nhttps://z.ai/subscribe?ic=SNK0LAU2OF\n\nYou can self hosted but why bother when it is this cheap and so much faster...",
          "score": -8,
          "created_utc": "2026-01-27 18:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22mzol",
              "author": "thecrogmite",
              "text": "I'll take a look, is that some sort of affiliate link?",
              "score": 1,
              "created_utc": "2026-01-27 19:25:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22tru7",
                  "author": "btc_maxi100",
                  "text": "it's a paid shill",
                  "score": 3,
                  "created_utc": "2026-01-27 19:55:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o237xcu",
                  "author": "Jackster22",
                  "text": "It is, we both get a couple pennies each time. I only shill for products I actually use.  [https://i.postimg.cc/9F7xYyLZ/image.png](https://i.postimg.cc/9F7xYyLZ/image.png)",
                  "score": -1,
                  "created_utc": "2026-01-27 20:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjiym",
      "title": "SHELLper üêö: Multi-Turn Function Calling with a <1B model",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "author": "gabucz",
      "created_utc": "2026-01-26 15:43:32",
      "score": 24,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "We fine-tuned a 0.6B model to translate natural language into bash commands. Since it's tiny, you can run it on your laptop with complete data privacy.\n\nSmall models struggle with multi-turn tool calling - out of the box, Qwen3-0.6B achieves 84% accuracy on single tool calls, which drops to **just 42% over 5 turns.** Our tuning brings this to 100% on the test set, delivering robust multi-turn performance.\n\n|Model|Parameters|Tool call accuracy (test set)|=> 5-turn tool call accuracy|\n|:-|:-|:-|:-|\n|Qwen3 235B Instruct (teacher)|235B|99%|95%|\n|Qwen3 0.6B (base)|0.6B|84%|42%|\n|**Qwen3 0.6B (tuned)**|**0.6B**|**100%**|**100%**|\n\nRepo: [https://github.com/distil-labs/distil-SHELLper](https://github.com/distil-labs/distil-SHELLper)\n\nHuggingface model: [https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)\n\n# Quick Start\n\nSet up the environment:\n\n    # Set up environment\n    python -m venv .venv\n    . .venv/bin/activate\n    pip install openai huggingface_hub\n    \n\nDowload the model:\n\n    hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model\n    cd distil_model\n    ollama create distil_model -f Modelfile\n    cd ..\n    \n\nRun the assistant:\n\n    python filesystem_demo.py\n    \n\nThe demo prompts for confirmation before running commands (safety first) and blocks certain dangerous operations (like `rm -r /`), so feel free to try it out!\n\n# How We Trained SHELLper\n\n# The Problem\n\nSmall models really struggle with multi-turn tool calling - performance degrades as tool calls chain together, dropping with each additional turn. If we assume independent errors for each tool call (like incorrect parameter values), a model at 80% accuracy only has a 33% chance of getting through 5 turns error-free.\n\n|Single tool call accuracy|5-turn tool call accuracy|\n|:-|:-|\n|80%|33%|\n|90%|59%|\n|95%|77%|\n|99%|95%|\n\nFor this demo, we wanted to test if we could dramatically improve a small model's multi-turn performance. We started with a task from the [Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) (BFCL) - the [gorilla file system tool calling task](https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json). We adapted it:\n\n* Original task supports multiple tool calls per turn ‚Üí we restrict to one\n* Cap at 5 turns max\n* Map commands to actual bash (instead of gorilla filesystem functions)\n* Skip adding tool outputs to conversation history\n\nBasically, the same tool set, but new, simpler [train/test data.](https://github.com/distil-labs/distil-SHELLper/tree/main/data)\n\n# Training Pipeline\n\n1. **Seed Data:** We built 20 simplified training conversations covering the available tools in realistic scenarios.\n2. **Synthetic Expansion:** Using our [data synthesis pipeline](https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&utm_medium=referral&utm_campaign=shellper), we generated thousands of training examples.\n\nSince we're dealing with variable-length conversations, we broke each conversation into intermediate steps. Example:\n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models\n    \n\n... becomes 2 training points:\n\n    [Input] User: List all files\n    [Output] Model: ls -al\n    \n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models`\n    \n\n1. **Fine-tuning:** We selected **Qwen3-0.6B** as the [most fine-tunable sub-1B](https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning) model with tool calling support on our platform.\n\n# Usage Examples\n\nThe assistant interprets natural language, generates bash commands, and can execute them (with Y/N confirmation).\n\n**Basic filesystem operations**\n\n    > python filesystem_demo.py\n    \n    USER: List all files in the current directory\n    COMMAND: ls\n    USER: Create a new directory called test_folder\n    COMMAND: mkdir test_folder\n    USER: Navigate to test_folder COMMAND: cd test_folder\n    \n\n# Limitations and Next Steps\n\nCurrently, we only support a basic bash tool set:\n\n* no pipes, chained commands, or multiple tool calls per turn\n* no detection of invalid commands/parameters\n* 5-turn conversation limit\n\nWe wanted to focus on the basic case before tackling complexity. Next up: multiple tool calls to enable richer agent workflows, plus benchmarking against [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html).\n\nFor your own bash workflows, you can log failing commands, add them to `data/train.jsonl`, and retrain with the updated data (or try a bigger student model!).\n\n# Discussion\n\nWould love to hear from the community:\n\n* Is anyone else fine-tuning small models for multi-turn tool calling?\n* What other \"focused but practical\" tasks need local, privacy-first models?",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1u3wq5",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 15:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u7b5a",
              "author": "gabucz",
              "text": "Thanks! Qwens tool call format - inputs are translated via transformers chat templating, and for outputs we have a converter to match what qwen does",
              "score": 1,
              "created_utc": "2026-01-26 16:00:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7m4o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 0,
                  "created_utc": "2026-01-26 16:02:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uk3mr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 16:55:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1upa98",
              "author": "gabucz",
              "text": "Exactly - we mention it in the post",
              "score": 1,
              "created_utc": "2026-01-26 17:17:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qluto8",
      "title": "HashIndex: No more Vector RAG",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 18:33:54",
      "score": 24,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama and llama cpp . Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha\n\n[https://github.com/JasonHonKL/HashIndex/tree/main](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1ifug5",
          "author": "FaceDeer",
          "text": "I didn't see any documentation there about how the \"guts\" of the system worked, so [I asked Gemini to do a Deep Research run to produce one](https://gemini.google.com/share/2e13e0d1a6fe). Some key bits:\n\n> The documentation for HashIndex identifies it as a \"vectorless\" index system. This characterization is central to its \"under the hood\" operations. Instead of calculating a mathematical hash or a vector embedding, the system invokes an LLM to generate what it terms a \"semantic hash key\".  \n\n> When a document is ingested by HashIndex, it is first split into segments or pages. For each segment, the system initiates a dual-process LLM call. The first process involves generating a highly descriptive, human-readable label that encapsulates the core theme of the content. This label‚Äîfor example, ``revenue_projections_FY2024_Q3``‚Äîserves as the index key in the hash map. The second process generates a concise summary of the page.\n\n> This \"single-pass\" parsing allows the document to be structured for retrieval without the need for pre-computed embedding datasets. However, the cost of this precision is time. While a traditional cryptographic hash function $H(x)$ or an embedding model can process data in milliseconds, the semantic key generation in HashIndex requires significant inference time, typically 2 to 3 seconds per page.\n\n[...]\n\n> In HashIndex, the hash table is implemented in-memory, allowing for rapid access once the indexing phase is complete. The \"hash function\" in this context is the cognitive process performed by the LLM during key generation. This approach eliminates the need for complex tree rebalancing and multi-level traversal required by systems like ChatIndex or PageIndex. However, it places a higher burden on the \"agentic\" side of the retrieval process, as the agent must now navigate a flat list of keys rather than a hierarchical tree. \n\nDoes this look like an accurate summary of how it works? Might be worth calling out that the \"hash\" in this case is not a traditional hash in the way that word is usually meant, but an LLM-generated semantic \"tag\" of sorts.",
          "score": 1,
          "created_utc": "2026-01-24 22:38:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l7wdg",
              "author": "jasonhon2013",
              "text": "Haha generally speaking yea this summary is right. We call it hash is because the data structure we use behind is a hash table hahaha",
              "score": 1,
              "created_utc": "2026-01-25 09:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7lf4",
          "author": "mister2d",
          "text": "Checking it out now with some pending energy legislation bills.",
          "score": 1,
          "created_utc": "2026-01-25 09:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yp85z",
          "author": "No-Lobster486",
          "text": "it would be much helpful if there is an relatively detailed explanation about the \"hash map to handle data\" part and a chart of the business flow.    \nthere are too many similar things these days, it will definitely help people make decision whether it will suit for their issue or worth trying",
          "score": 1,
          "created_utc": "2026-01-27 05:06:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h4awo",
          "author": "jschw217",
          "text": "Why does it require httpx? Any connections to remote servers?",
          "score": 0,
          "created_utc": "2026-01-24 18:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h4gs2",
              "author": "jasonhon2013",
              "text": "Oh it‚Äôs is for the purpose of fetching APIs like open router not connected to any remote server no worries !",
              "score": 2,
              "created_utc": "2026-01-24 18:57:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjccpp",
      "title": "AMD ROCm 7.2 now released with more Radeon graphics cards supported, ROCm Optiq introduced",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-ROCm-7.2-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-21 22:24:16",
      "score": 20,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjccpp/amd_rocm_72_now_released_with_more_radeon/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": [
        {
          "id": "o10t8zr",
          "author": "techlatest_net",
          "text": "ROCm 7.2 dropping RDNA4 support + Optiq? Finally Radeon GPUs get real ML love‚ÄîComfyUI workflows about to fly. Windows builds too? AMD actually shipping for consumers.\n\nTime to dust off the 7900 XTX and pit it vs my CUDA stack",
          "score": 2,
          "created_utc": "2026-01-22 10:12:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlqvdh",
      "title": "Minimum hardware for a voice assistant that isn't dumb",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "author": "JacksterTheV",
      "created_utc": "2026-01-24 16:07:22",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 0.96,
      "text": "I'm at the I don't know what I don't know stage. I'd like to run a local LLM to control my smart home and I'd like it have a little bit of a personality. From what I've found online that means a 7-13b model which means a graphics card with 12-16gb of vram. Before I started throwing down cash I wanted to ask this group of I'm on the right track and for any recommendations on hardware. I'm looking for the cheapest way to do what I want and run everything locally ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1gbeqd",
          "author": "LowTip9915",
          "text": "I have a 7b on an m2 Mac mini and it‚Äôs acceptable, but I‚Äôm not using voice (just llama and open web ui). The 14b runs, but is painfully slow, however the difference in answers is evident, so I use it occasionally when the 7b answer is overly vague/top level.  (Disclaimer also a noob to this)",
          "score": 6,
          "created_utc": "2026-01-24 16:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h01e9",
          "author": "Boricua-vet",
          "text": "First off, you do not need a model to do what you want to do. Voice assist can do pretty much what you want and in the case that you have unique requests, you can just add those by hand. Even the personality part can be creating using custom answers. The only downside is that it can only do what you program it to do and you will not be able to have conversations with it. If you need conversations, then you will certainly need a model but you don't need a 13b to do what you want. You do not even need a GPU as long as you have a good CPU and fast ram and enough ram to load the model. \n\nSince I do not know your hardware specs, I recommend you starting with [https://huggingface.co/unsloth/Qwen3-4B-GGUF?show\\_file\\_info=Qwen3-4B-Q4\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q4_K_M.gguf)\n\nYou can run that on CPU and get really good speed depending on your hardware. Example on my hardware using only CPU I can get 100+ tokens per second on this model but depends on your hardware.\n\nThat's the cheapest way to do it.\n\nNow if you still want to be cheap and use GPU\n\nGet a P102-100 for 60 bucks.\n\n[https://www.ebay.com/itm/156284588757](https://www.ebay.com/itm/156284588757)\n\nyou can load piper and whisper using hardware acceleration and get responses from HA in milliseconds.  For this purpose , you do not need to buy a stupid expensive card. 60 bucks will do better job than any 3060, 4060 and 5060 for this particular application and use case.\n\nwith one P102-100, you can run Piper, Whisper and Qwen 4b and get everything you wanted to do.\n\nnow, if you really want to go nuts, you can get two and then you can run Qwen30b and get these speeds.\n\n`llamacpp-server-1  | ggml_cuda_init: found 2 CUDA devices:`\n\n`llamacpp-server-1  |   Device 0: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  |   Device 1: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  | load_backend: loaded CUDA backend from /app/libggml-cuda.so`\n\n`llamacpp-server-1  | load_backend: loaded CPU backend from /app/libggml-cpu-piledriver.so`\n\n`llamacpp-server-1  | | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |`\n\n`llamacpp-server-1  | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           pp512 |        968.28 ¬± 4.69 |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           tg128 |         72.10 ¬± 0.23 |`\n\n`llamacpp-server-1  |` \n\n`llamacpp-server-1  | build: 557515be1 (7819)`\n\n`llamacpp-server-1 exited with code 0`\n\n  \nabout 1,000 in prompt processing and 70+ tokens generation.\n\ntrust me, for home assistant, you will not need more than that and at that price, you will not find a better solution.\n\nAsk people that actually own P102-100, don''t take my word for it.\n\nFor this use case.... this is it.",
          "score": 6,
          "created_utc": "2026-01-24 18:38:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9az6",
              "author": "JacksterTheV",
              "text": "Awesome, I'm going to go the CPU route and see what I can get working. Thanks for the recommendation.¬†",
              "score": 1,
              "created_utc": "2026-01-24 19:18:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1kwfhl",
              "author": "MakerBlock",
              "text": "Fantastic write up!",
              "score": 1,
              "created_utc": "2026-01-25 07:25:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1nfx91",
                  "author": "Boricua-vet",
                  "text": "Thank you. Really appreciate that. Just trying to help.",
                  "score": 3,
                  "created_utc": "2026-01-25 17:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ga8q2",
          "author": "tillemetry",
          "text": "Anyone try this with an M4 Mac mini?  You can run it headless, it doesn‚Äôt take up much space and the noise and power draw is minimal.  LM studio will point you at ai‚Äôs optimized for MLX.",
          "score": 4,
          "created_utc": "2026-01-24 16:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gbm53",
          "author": "PermanentLiminality",
          "text": "There is no single correct answer.\n\nYou left out a few parts.  You need the audio in and out.  A bluetooth speaker can work.  Now you need the Speech to text (STT)  and Text to Speech(TTS) in addition to a LLM.  Both the TTS and STT need VRAM, but there are TTS like Piper than can actually run on the CPU.\n\nOne nice thing here is most voice assistants don't do a lot of context so you only need say 15% more VRAM than the LLM.\n\nYou will need to experiment to find what works for you.  Put a few bucks into OpenRouter and try the models.  A possible example would be mistral 3 8B which fits in about 7GB of VRAM.  They free models too.   Find what will work for what you want.\n\nIf I ever get some time, I plan on doing what you are trying to do.  My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU.  Not sure if I can get a smart enough model STT and TTS , but I want to try.  That setup will idle around 10 watts.  I may consider picking up a 24GB P40 as it also is pretty low idle power.   The P40 is around $200 after the needed fan to cool it.\n\nMy power is crazy expensive so cheap 16 GB GPUs like the P100 or CMP100-210 idle at 40 or 50 watts.  That is like $200/yr for me.\n\nAnyway, before dropping coin on a GPU, test out a solution with available tools like Openrouter and Huggingface.  For a project like this $10 can go a long way for testing.",
          "score": 2,
          "created_utc": "2026-01-24 16:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ht9nr",
              "author": "Boricua-vet",
              "text": "\"My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU. Not sure if I can get a smart enough model STT and TTS\"\n\nyou sure can.. I will go as far as give you the models I am using with hardware accel that works for me.  \nfor piper PIPER\\_VOICE=en\\_US-libritts-high  \nfor whisper  WHISPER\\_MODEL=distil-medium.en  \nthose two will consume 3.5GB of vram for both. 3.5 total for both.\n\nMy only suggestion is to use really good microphone like [https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html](https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html)  this has the xmos chip that will clean all the input even with loud music or background noise. It works fantastic...\n\non the P102-100 with those models and that mic, your response time from HA after your command should be in the milliseconds. Not even a second like half a second or less, extremely responsive, that is unless you are running it in an under powered device.\n\nI run this setup flawlessly.",
              "score": 3,
              "created_utc": "2026-01-24 20:50:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gn24j",
          "author": "JacksterTheV",
          "text": "To add some more context I'm using home assistant and have the voice pipeline worked out. I have a hacked echo show that I can talk to like Alex. The problem is right now it takes about 30 seconds for something as simple as locking the back door. I can connect home assistant to any local LLM. This is all running on an old laptop. Ultimately I want everything to run locally but I really like the idea of spending a few bucks on a cloud service to figure out what I need before dropping cash on hardware.¬†",
          "score": 2,
          "created_utc": "2026-01-24 17:43:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1haztq",
              "author": "Blizado",
              "text": "I actually tend to use a Qwen3 30B A3B Instruct model in ~2-4bit. I tried it with Q2_K in GGUF format with KoboldAI, needs with a small KV Cache around 12GB VRAM (Q4 would be around 16-18GB VRAM) and I was surprised how good it still is, but even more surprised how crazy fast it is (at least on my RTX 4090). Thanks to its MoE pipeline, where only 3B of the model is active, so it is mostly as fast as a 3B model at Q2_K would be, but of cause way better, you don't need to try a 3B at Q2_K, it will be very very bad but crazy fast. It's hard for me to use dense models now, they are so slow in comparison.\n\nSo as long you don't want to have a realtime conversation with the LLM, you can also split a some layers of the model into normal RAM. It slows down the model a bit but if you only put ~10% of the model in RAM it should stay fast enough so you can bring down the SmartHome reaction to maybe 5 seconds or lower. The only question is if you need tool calling, that could break on Q2_K, but there are ways to work around that.\n\nBut I would go for a solid 16GB VRAM card (RTX 40xx/50xx series), not too much on the low end, then you can put the LLM completely into the VRAM. So maybe a 5070TI with 16GB VRAM. No clue if a 5060TI is still fast enough. I \"only\" have a 4090 and a 5080. Could be if you get the full model into VRAM, because if not, the 5060TI has only PCIe 5 8x, the 5070TI has PCIe 5 16x, but that is only really relevant if you use offloading (splitting the model into RAM) or if the GPU is in a PCIe 5 x16 slot. If you need offloading, better don't put a PCIe 5 8x GPU into a PCIe 5 motherboard setup.\n\nBut beside that, it really depends what you want to exactly do. If you only want the bar minimum, for example only some different phrases that should react for one SmartHome function, you can do that with much much less hardware and a tiny AI model that only recognize what you want to do. Such an AI didn't need to be that smart at all, only always recognize what you want from it, you can even give it a bit the illusion of personality. But if your plan is to build your SmartHome more and more out, then a LLM is surely not a bad decision, but a more costly one.",
              "score": 1,
              "created_utc": "2026-01-24 19:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i0p7u",
          "author": "atlantageek2",
          "text": "currently developing something similar for work using whisper. dev machine at home is my m4 mac mini 16gb. theres like a 3 second delay but other than that it works well. its using whisper",
          "score": 2,
          "created_utc": "2026-01-24 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i0zr0",
              "author": "atlantageek2",
              "text": "forgot to mention whisper cannot use max gpu",
              "score": 1,
              "created_utc": "2026-01-24 21:27:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ihhpy",
          "author": "Powerful-Street",
          "text": "LFM2.5-audio if you can figure out how to get it to work, other than in the demo. It‚Äôs only ~3GB and does a pretty good job. The other thing you could do is, inject personaplex weights into moshi. Takes much more work and ram though.",
          "score": 1,
          "created_utc": "2026-01-24 22:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1izk91",
          "author": "PermanentLiminality",
          "text": "Good to know.   Now I just need some time...",
          "score": 1,
          "created_utc": "2026-01-25 00:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r35e0",
          "author": "Bino5150",
          "text": "I'm running an 8b model locally with LM Studio/AnythingLLM, and using Piper for TTS. I'm running an HP Zbook Studio G7 laptop on Linux Mint, with an i7, 16GB ram, and an Nvidia Quadro T1000 Mobile gpu with 4GB vram. It runs just fine. I can do SST via the laptops mic, but I haven't configured home assistant yet as I've just got this up and running. Thinking of setting up the SST on a Raspberry Pi.",
          "score": 1,
          "created_utc": "2026-01-26 03:17:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qitw9k",
      "title": "Trained a local Text2SQL model by chatting with Claude ‚Äì here's how it went",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/coqeh3twdoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 09:57:59",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitw9k/trained_a_local_text2sql_model_by_chatting_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qkmv44",
      "title": "DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog",
      "subreddit": "LocalLLM",
      "url": "https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage",
      "author": "EchoOfOppenheimer",
      "created_utc": "2026-01-23 09:59:43",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.64,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkmv44/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/",
      "domain": "introl.com",
      "is_self": false,
      "comments": [
        {
          "id": "o18oklh",
          "author": "ForsookComparison",
          "text": "> 12 upvotes\n\nFor a sub about LLMs this sub is pretty shitty at spotting bots",
          "score": 2,
          "created_utc": "2026-01-23 14:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iqcip",
          "author": "HealthyCommunicat",
          "text": "I‚Äôve been trying to talk about DS 3.2 and LongCat 2601 more as they genuinely seem to be on a higher level than GLM or MiniMax when it comes to knowing very vast specific niche knowledge, they‚Äôre the only two models I trust for work, as they‚Äôre the only two models so far that get questions about Oracle EBS correct, as most of Oracle‚Äôs software is proprietary and existing documentation is absolutely horrendous. - these two models can answer questions that even GPT 5.2 has trouble answering if I specifically tell it not to use any kind of search.",
          "score": 1,
          "created_utc": "2026-01-24 23:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22znhz",
          "author": "JimmyDub010",
          "text": "Only rich people can run it though.........",
          "score": 1,
          "created_utc": "2026-01-27 20:22:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qovbrq",
      "title": "Will.i.am is promoting running local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qovbrq/william_is_promoting_running_local_llms/",
      "author": "beefgroin",
      "created_utc": "2026-01-28 00:05:26",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 0.87,
      "text": "And fine-tuning for that matter. Or at least that‚Äôs how I understood what he was saying lol. What do you think?\n\n[ https://youtu.be/sSiaB90XpII?t=384 ](https://youtu.be/sSiaB90XpII?t=384)\n\nStarts at 6:25. But the whole interview is worth watching too.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qovbrq/william_is_promoting_running_local_llms/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o24i87j",
          "author": "SocialDinamo",
          "text": "He was always big on encouraging kids to learn to code so this makes sense. Great to see!",
          "score": 7,
          "created_utc": "2026-01-28 00:42:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o24yf78",
              "author": "ForsookComparison",
              "text": "Juniors blaming A.I. for their degree and livelihood being worthless but real ones know that it was that one Bill Gates and Will.I.Am video from like 2011 that saturated their market.",
              "score": 1,
              "created_utc": "2026-01-28 02:07:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o24lf20",
          "author": "sizebzebi",
          "text": "give me the ram for it tried one my rtx 4070 and it's utter trash",
          "score": 2,
          "created_utc": "2026-01-28 00:58:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25dr6q",
              "author": "Count_Rugens_Finger",
              "text": "i7-6700K, 32GB DDR4-3000, RTX 3070 8GB over here.  I can run several interesting models at reasonable speeds.",
              "score": 2,
              "created_utc": "2026-01-28 03:29:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o260a98",
          "author": "rditorx",
          "text": "It's Wi.llm.ai for you",
          "score": 1,
          "created_utc": "2026-01-28 05:53:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkxpuo",
      "title": "AMD Ryzen AI Software 1.7 released for improved performance on NPUs, new model support",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-Ryzen-AI-Software-1.7",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-23 17:52:03",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkxpuo/amd_ryzen_ai_software_17_released_for_improved/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    }
  ]
}