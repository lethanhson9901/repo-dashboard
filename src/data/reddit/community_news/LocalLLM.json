{
  "metadata": {
    "last_updated": "2026-02-09 09:19:59",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 282,
    "file_size_bytes": 356147
  },
  "items": [
    {
      "id": "1quw0cf",
      "title": "Qwen3-Coder-Next is out now!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/7kswd313pahg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-03 16:08:41",
      "score": 321,
      "num_comments": 128,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1quw0cf/qwen3codernext_is_out_now/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3eer55",
          "author": "siegevjorn",
          "text": "Wait is it really sonnet 4.5 level? How.",
          "score": 13,
          "created_utc": "2026-02-03 19:53:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gtjaz",
              "author": "dreaming2live",
              "text": "It doesn’t feel like that to me. Tried using it in vscode to give it a whirl, but still not competitive with Sonnet 4.5. Neat but IRL doesn’t really compare.",
              "score": 8,
              "created_utc": "2026-02-04 03:24:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3i2ly9",
                  "author": "Visible-Age4888",
                  "text": "Dude, sonnet wrote me a screen reader that started playing with my fucking screen. That shit is ass too…",
                  "score": 2,
                  "created_utc": "2026-02-04 09:18:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ir611",
              "author": "ServiceOver4447",
              "text": "no it's not ;-(",
              "score": 2,
              "created_utc": "2026-02-04 12:43:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jmwnr",
                  "author": "siegevjorn",
                  "text": "Hah, I almost came imagining myself rocking 80B model on claude code doing all the stuff I can do with sonnet locally",
                  "score": 2,
                  "created_utc": "2026-02-04 15:33:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3errtj",
          "author": "Ill_Barber8709",
          "text": "Strange they didn't compare themselves to Devstral 2",
          "score": 10,
          "created_utc": "2026-02-03 20:54:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fr7a5",
              "author": "Yeelyy",
              "text": "True",
              "score": 2,
              "created_utc": "2026-02-03 23:49:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3debff",
          "author": "Effective_Head_5020",
          "text": "Great work, thanks, you are my hero!\n\n\nWould it be possible to run with 64gb of RAM? No Vram",
          "score": 13,
          "created_utc": "2026-02-03 17:06:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dijsv",
              "author": "yoracale",
              "text": "Yes it'll work, maybe 10 tokens/s. VRAM will greatly speed things up however",
              "score": 10,
              "created_utc": "2026-02-03 17:26:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3dj2oz",
                  "author": "Effective_Head_5020",
                  "text": "Thank you",
                  "score": 1,
                  "created_utc": "2026-02-03 17:28:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3dnedr",
                  "author": "Effective_Head_5020",
                  "text": "I am getting 5 t/s using the q2_k_xl - it is okay.\n\n\nThanks unsloth team, that's great!",
                  "score": 1,
                  "created_utc": "2026-02-03 17:48:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dgu2i",
              "author": "Puoti",
              "text": "Slowly on cpu. Or hybrid with few layers on gpu and most on cpu. Still slow but possible",
              "score": 2,
              "created_utc": "2026-02-03 17:18:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dj4ck",
                  "author": "Effective_Head_5020",
                  "text": "Thank you! ",
                  "score": 1,
                  "created_utc": "2026-02-03 17:28:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3h7kbh",
              "author": "ScuffedBalata",
              "text": "On a regular PC?  It'll be slow as hell, but you can tell it to generate code and walk away for 5-10 minutes, you'll have something.",
              "score": 1,
              "created_utc": "2026-02-04 04:55:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3hq13k",
                  "author": "HenkPoley",
                  "text": "More like 25 minutes; depending on your input and output requirements. \n\nBut yes, you will have to wait.",
                  "score": 1,
                  "created_utc": "2026-02-04 07:21:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hpuhq",
          "author": "HenkPoley",
          "text": "This is an 80B model. For those thinking about Qwen3 Coder 30B A3B.\n\nThis one is based on their larger Qwen3 Next model.",
          "score": 4,
          "created_utc": "2026-02-04 07:20:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fs8pz",
          "author": "jheizer",
          "text": "Super quick and dirty LM Studio test: Q4_K_M RTX 4070 + 14700k 80GB DDR4 3200 - 6 tokens/sec\n\nEdit: llama.cpp 21.1 t/s.",
          "score": 7,
          "created_utc": "2026-02-03 23:55:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3fssr7",
              "author": "onetwomiku",
              "text": "LMStudio do not update their runtimes in time. Grab fresh llama.cpp.",
              "score": 3,
              "created_utc": "2026-02-03 23:58:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g86su",
                  "author": "jheizer",
                  "text": "I mostly did it cuz others were. Huge difference. 21.1tokens/s. 13.3 prompt.  It's much better utilizing the GPU for processing.",
                  "score": 1,
                  "created_utc": "2026-02-04 01:23:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3k3abk",
              "author": "ScuffedBalata",
              "text": "Getting 12t/s on a 3090 with Q4_K_M  Extra vram helps, but not a ton.",
              "score": 1,
              "created_utc": "2026-02-04 16:48:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uywz2",
                  "author": "huzbum",
                  "text": "I just got 30tps on my 3090 on the new version of LM Studio.  offload all layers to GPU, and offload 2/3 experts to CPU.  ",
                  "score": 2,
                  "created_utc": "2026-02-06 06:38:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ft683",
              "author": "oxygen_addiction",
              "text": "Stop using LM Studio. It is crap.",
              "score": 1,
              "created_utc": "2026-02-04 00:00:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3go4b6",
                  "author": "onethousandmonkey",
                  "text": "Would be great if you could expand on that.",
                  "score": 2,
                  "created_utc": "2026-02-04 02:53:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3g2fdo",
                  "author": "Status_Analyst",
                  "text": "So, what should we use?",
                  "score": 2,
                  "created_utc": "2026-02-04 00:51:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ga9p1",
          "author": "Astral-projekt",
          "text": "Man this team is doing gods work.",
          "score": 3,
          "created_utc": "2026-02-04 01:35:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6qm1",
          "author": "Naernoo",
          "text": "So this is sonnet 4.5 level? Also agentic mode? Or is this model just optimized for the tests to perform that good?",
          "score": 5,
          "created_utc": "2026-02-03 22:03:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3h7z1r",
              "author": "ScuffedBalata",
              "text": "I just built a whole python GUI app with it for Ubuntu.  It's ok.  I don't get a Sonnet vibe.\n\nAfter a handful of prompts, I had something working but a little sketchy.  I actually need this code, so I brought it into Opus and it's dramatically better.\n\nStill, it's the most capable local coding LLM I've ever used (I don't have the hardware for Kimi or something), so i'd call it major progress.  I'm going to evaluate using it for some stuff we need at work tomorrow.",
              "score": 1,
              "created_utc": "2026-02-04 04:57:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ifitd",
                  "author": "Naernoo",
                  "text": "ok interesting, do you run it on ram or vram? what specs does your rig have?",
                  "score": 1,
                  "created_utc": "2026-02-04 11:16:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3fvo3z",
          "author": "IntroductionSouth513",
          "text": "anyone trying it out on Strix Halo 128GB, and which platform? ollama, lmstudio or lemonade (possible?)",
          "score": 2,
          "created_utc": "2026-02-04 00:14:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g009v",
              "author": "cenderis",
              "text": "Just downloaded it for llama.cpp. I chose the MXFP4 quant which may well not be the best. Feels fast enough but I don't really have any useful stats.",
              "score": 1,
              "created_utc": "2026-02-04 00:37:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g17kd",
                  "author": "IntroductionSouth513",
                  "text": "hv u tried plugging VS code to do actual coding",
                  "score": 1,
                  "created_utc": "2026-02-04 00:44:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3m6cqg",
              "author": "Maasu",
              "text": "Yes I had it running on strix halo using vulcan rdv toolbox and fedora 42 and llama cpp. I was in a bit of rush and multitasking so didn't bench mark but used it in open code.\n\n20k(ish) system prompt took 49 seconds to load. After that it was very much usable, a bit slower than cloud models but certainly usable.\n\nI haven't tried it for anything meaningful yet however, I'm in a rush and sorry this seemed rushed I'm not at my pc and do t have proper info in front of me but it was working.",
              "score": 1,
              "created_utc": "2026-02-04 22:41:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fytm4",
          "author": "Fleeky91",
          "text": "Anyone know if this can be split up between VRAM and RAM? Got 32gb of VRAM and 64 gb of RAM",
          "score": 2,
          "created_utc": "2026-02-04 00:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3glrhc",
              "author": "yoracale",
              "text": "Yes definitely works, see : [https://unsloth.ai/docs/models/qwen3-coder-next#usage-guide](https://unsloth.ai/docs/models/qwen3-coder-next#usage-guide)",
              "score": 2,
              "created_utc": "2026-02-04 02:40:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3gttwz",
              "author": "dreaming2live",
              "text": "Yeah it runs okay with this setup.  5090 with 32gb vram and 96gb ram gets me around 30 tk/s",
              "score": 1,
              "created_utc": "2026-02-04 03:26:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3j6ejd",
                  "author": "loscrossos",
                  "text": "care to share your settings?",
                  "score": 1,
                  "created_utc": "2026-02-04 14:10:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ycvzh",
                  "author": "trailsman",
                  "text": "Nice! Have the same setup",
                  "score": 1,
                  "created_utc": "2026-02-06 19:22:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3im04h",
              "author": "romayojr",
              "text": "i have this exact setup as well. what quant/s did you end up trying? could you share your speed stats?",
              "score": 1,
              "created_utc": "2026-02-04 12:07:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3hly77",
          "author": "Successful-Willow-72",
          "text": "did i read it right? 46? i can finally run a 80b model at home?",
          "score": 2,
          "created_utc": "2026-02-04 06:46:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hrsxk",
              "author": "yoracale",
              "text": "It's for 4-bit, if you want 8-bit you need 85gb ram",
              "score": 3,
              "created_utc": "2026-02-04 07:37:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3i8zbe",
              "author": "RnRau",
              "text": "Yeah there is virtually no penalty for running mxfp4 on an 80b parameter model.",
              "score": 1,
              "created_utc": "2026-02-04 10:18:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k4x8h",
          "author": "electrified_ice",
          "text": "It's been tricky to setup on my RTX PRO 6000 Blackwell with 96GB VRAM. Once loaded with vLLM it uses about 90GB @ 8bit quantization... It's so new and it's a MoE model with 'Mamba' so has required a lot of config and dependencies to install and get accepted (without errors) for vLLM. The cool thing is its blazing fast as it's often only pulling a few 'experts' at 3B parameters each.",
          "score": 2,
          "created_utc": "2026-02-04 16:56:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3obxuu",
              "author": "taiphamd",
              "text": "Why do any work when you can just run https://catalog.ngc.nvidia.com/orgs/nvidia/containers/vllm/tags?version=26.01-py3 and it should work out of the box",
              "score": 2,
              "created_utc": "2026-02-05 06:38:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3o88y4",
              "author": "kwinz",
              "text": "> The cool thing is its blazing fast as it's often only pulling a few 'experts' at 3B parameters each.\n\nCan you share how many tokens/s you're getting?",
              "score": 1,
              "created_utc": "2026-02-05 06:06:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o41pw97",
              "author": "taiphamd",
              "text": "Another tip to reduce vram usage with vLLM is to reduce the default max-num-seq to reduce the required allocation for KV cache. If you’re not planning to serve multiple users can reduce to 1 or 2 : https://github.com/vllm-project/vllm/issues/15609",
              "score": 1,
              "created_utc": "2026-02-07 07:55:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3otfm6",
          "author": "Darlanio",
          "text": "Has anyone run this on Asus Ascent GX10 GB10 128GB or NVidia Spark DGX ?",
          "score": 2,
          "created_utc": "2026-02-05 09:21:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ouwar",
          "author": "BinaryStyles",
          "text": "I'm getting ~40 tok/sec in lmstudio on CUDA 12 with a Blackwell 6000 Pro Workstation (96GB vram) using Q4_k_m + 256000 max tokens.",
          "score": 2,
          "created_utc": "2026-02-05 09:35:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3d2y2x",
          "author": "Impossible-Glass-487",
          "text": "What quant do you suggest for 28gb NVDIA VRAM & 96gb DDR5?",
          "score": 1,
          "created_utc": "2026-02-03 16:13:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d5hp8",
              "author": "TomLucidor",
              "text": "At that point beg everyone else to REAP/REAM the model. And SWE-Bench likely benchmaxxed",
              "score": 2,
              "created_utc": "2026-02-03 16:25:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3djl29",
                  "author": "rema1000fan",
                  "text": "its a A3B MoE model however, so it is going to be speedy in token generation even with minimal VRAM. Prompt processing depends on bandwidth to GPU however.  ",
                  "score": 2,
                  "created_utc": "2026-02-03 17:30:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d72fb",
              "author": "yoracale",
              "text": "Any of the 8-bit ones!",
              "score": 1,
              "created_utc": "2026-02-03 16:32:52",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3dmkfb",
              "author": "Puoti",
              "text": "You are going to fly with that....\nI made hub kinda thingie that has automated wizard fot gpu/cpu layers based on your rig and what quantize level you choose. That would be handy. But the usage of models is still bit limited since its in alpha stage.\nBut 8 bit would be handy imo for you.",
              "score": 1,
              "created_utc": "2026-02-03 17:44:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3efug6",
          "author": "Sneyek",
          "text": "How well would it run on an RTX 3090 ?",
          "score": 1,
          "created_utc": "2026-02-03 19:58:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ft3yr",
              "author": "oxygen_addiction",
              "text": "If you have enough RAM, it should run well.",
              "score": 1,
              "created_utc": "2026-02-03 23:59:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fv6nc",
                  "author": "Sneyek",
                  "text": "What is “enough” ? 64GB ? 48 ?",
                  "score": 1,
                  "created_utc": "2026-02-04 00:11:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3gghon",
              "author": "kironlau",
              "text": "Q4 about 46gb, without context (RAM+VRAM as a total)",
              "score": 1,
              "created_utc": "2026-02-04 02:10:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fvhop",
          "author": "Icy_Orange3365",
          "text": "I have a 64gb vram m1 MacBook, how big is the full model? How much ram is needed?",
          "score": 1,
          "created_utc": "2026-02-04 00:13:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ikx6j",
              "author": "GreaseMonkey888",
              "text": "The 4bit MLX version works fine on a Mac Studio M4 with 64GB, 84t/s",
              "score": 2,
              "created_utc": "2026-02-04 11:59:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3gysrs",
              "author": "yoracale",
              "text": "Works, use the 5bit or 6bit. 8bit if 85gb as it says in the guide",
              "score": 1,
              "created_utc": "2026-02-04 03:56:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3fxfwp",
          "author": "KillerX629",
          "text": "How does this compare to glm 4.7 flash??",
          "score": 1,
          "created_utc": "2026-02-04 00:23:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3gloq4",
              "author": "yoracale",
              "text": "GLM 4.7 Flash is a thinking model and this isn't. This one is better and faster at coding while Flash is probably better at a larger variety of tasks",
              "score": 3,
              "created_utc": "2026-02-04 02:39:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3gyz7f",
          "author": "phoenixfire425",
          "text": "Possible to run this on a rig with dual rtx3090 with vLLM??",
          "score": 1,
          "created_utc": "2026-02-04 03:57:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hd0uo",
              "author": "yoracale",
              "text": "Yes, we wrote a guide for vLLM here: [https://unsloth.ai/docs/models/qwen3-coder-next#fp8-qwen3-coder-next-in-vllm](https://unsloth.ai/docs/models/qwen3-coder-next#fp8-qwen3-coder-next-in-vllm)\n\nDo you have any extra RAM by any chance?",
              "score": 1,
              "created_utc": "2026-02-04 05:34:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3iyoed",
                  "author": "phoenixfire425",
                  "text": "on thats system, it only has 32gb system ram",
                  "score": 1,
                  "created_utc": "2026-02-04 13:28:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3j56ol",
                  "author": "phoenixfire425",
                  "text": "Yep, cannot run this on a dual RTX 3090 system with vLLM.  no matter how i configure the service I get OOM issue on startup.",
                  "score": 1,
                  "created_utc": "2026-02-04 14:04:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i5qo7",
          "author": "Soft_Ad6760",
          "text": "Just trying it out now, on a 24VRAM laptop (on a RTx 5090) with already 2 models loaded (GLM 30B + Qwen 32B) in LMStudio, nothing like Sonet 4.5.\n3 t/s",
          "score": 1,
          "created_utc": "2026-02-04 09:48:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i7kn6",
          "author": "TurbulentType6377",
          "text": "Running it on Strix Halo (Ryzen AI MAX+ 395 GMKTEC Evo x2) with 128GB unified memory right now.\n\nSetup:\n\n  \\- Unsloth Q6\\_K\\_XL quant (\\~64GB)\n\n  \\- llama.cpp b7932 via Vulkan backend\n\n  \\- 128K context, flash attention enabled\n\n  \\- All layers offloaded to GPU (-ngl 999)\n\n\n\n  Results:\n\n  \\- Prompt processing: \\~127 t/s\n\n  \\- Generation: \\~35-36 t/s\n\n  \\- 1500 token coding response in \\~42s\n\n\n\nEntire Q6\\_K\\_XL fits in GPU-accessible memory with plenty of room left for KV cache. Could probably go Q8\\_0 (85GB) too but haven't tried yet.\n\n  Quick note for anyone else on Strix Halo: use the Vulkan toolbox from kyuz0/amd-strix-halo-toolboxes, not the ROCm (7.2) one. The qwen3next architecture (hybrid Mamba + MoE) crashes on ROCm but runs fine on Vulkan RADV. No HSA\\_OVERRIDE\\_GFX\\_VERSION needed. either, gfx1151 is detected natively.\n\nIt's solid for code generation in terms of quality. To be honest, it's not Sonnet 4.5 level, but it's quite useful and the best native coding model I've run so far. I'll try it out more before making a definitive assessment.",
          "score": 1,
          "created_utc": "2026-02-04 10:05:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3itvlj",
          "author": "MyOtherHatsAFedora",
          "text": "I've got a 16GB VRAM and 32GB of RAM... I'm new to all this, can I run this LLM?",
          "score": 1,
          "created_utc": "2026-02-04 13:00:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46rxuz",
              "author": "gangs08",
              "text": "No you need 90 in total or wait for a \"quantized model\"",
              "score": 1,
              "created_utc": "2026-02-08 02:39:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iu12e",
          "author": "techlatest_net",
          "text": "Grabbing it now – 80B MoE with just 3B active? Killer for local agents. 256k ctx is huge too.",
          "score": 1,
          "created_utc": "2026-02-04 13:01:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j6770",
          "author": "loscrossos",
          "text": "this i like",
          "score": 1,
          "created_utc": "2026-02-04 14:09:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j6w8s",
          "author": "howardhus",
          "text": "could anyone explain what does UD means in the model selection?",
          "score": 1,
          "created_utc": "2026-02-04 14:13:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o43ylk1",
              "author": "Blues520",
              "text": "Unsloth Dynamic",
              "score": 1,
              "created_utc": "2026-02-07 17:14:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ocbo9",
          "author": "taiphamd",
          "text": "Just tried this on my DGX spark using the fp8 model and got about 44 tok/sec (benchmarked using dynamo-ai/aiperf ) using vLLM container nvcr.io/nvidia/vllm:26.01-py3 to run the model",
          "score": 1,
          "created_utc": "2026-02-05 06:41:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s9ik3",
          "author": "SuperNintendoDahmer",
          "text": "Has anyone tried this on a MacMini M4Pro, 64GB? MLX? I am running the thinking variant at q5/3 decently.\n\n",
          "score": 1,
          "created_utc": "2026-02-05 20:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44lmm1",
          "author": "azaeldrm",
          "text": "Hi OP! Would I be able to run this over long periods of time on 2 3090 GPUs (48GB VRAM)? I'd love to put this model to the test while programming.\n\nAlso, is this model optimized to work with Opencode/Claude Code?\n\nThank you!",
          "score": 1,
          "created_utc": "2026-02-07 19:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47w5pp",
              "author": "yoracale",
              "text": "Yes definitely. Will be super fast. And yes, we actually have a guide for it: [https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed](https://unsloth.ai/docs/models/qwen3-coder-next#improving-generation-speed)",
              "score": 1,
              "created_utc": "2026-02-08 07:48:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o489xzw",
          "author": "AggravatingHelp5657",
          "text": "I still don't understand how this is still considered a local model if it needs 50 - 80 Gb of vram \n\nA local model should be between 8 to 32 Gb of RAM at maximum",
          "score": 1,
          "created_utc": "2026-02-08 09:59:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48al9e",
              "author": "yoracale",
              "text": "People have Macs with 128gb unified memory. It will work fine on those. or 96gb.\n\nIt's RAM and not VRAM.",
              "score": 1,
              "created_utc": "2026-02-08 10:05:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48b8ta",
                  "author": "AggravatingHelp5657",
                  "text": "Ah my bad I thought it's a vram \n\nBut still the shortage in RAM production now and the raises in the prices making it really challenging to have this amount of ram just for running a model locally \n\nBut anyway it's good news that finally they are developing a compressed model that compete with larger ones",
                  "score": 1,
                  "created_utc": "2026-02-08 10:11:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dvrfy",
          "author": "SufficientHold8688",
          "text": "When can we test models this powerful with only 16GB of RAM?",
          "score": 0,
          "created_utc": "2026-02-03 18:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dy8h2",
              "author": "ScoreUnique",
              "text": "Use that computer to run it on a rented GPU :3",
              "score": 5,
              "created_utc": "2026-02-03 18:36:53",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3glkoj",
              "author": "yoracale",
              "text": "You can with gpt-oss-20b or GLM-4.7-Flash: [https://unsloth.ai/docs/models/glm-4.7-flash](https://unsloth.ai/docs/models/glm-4.7-flash)",
              "score": 2,
              "created_utc": "2026-02-04 02:39:00",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3g5u8f",
              "author": "WizardlyBump17",
              "text": "shittiest quant is 20.5gb, so unless you have some more vram, you cant. Well, maybe if you use swap, but then instead of getting tokens per second you would be getting tokens per week",
              "score": 1,
              "created_utc": "2026-02-04 01:09:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3edip8",
          "author": "No_Conversation9561",
          "text": "anyone running this on 5070Ti and 96 GB ram?",
          "score": 0,
          "created_utc": "2026-02-03 19:47:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3em44e",
              "author": "Puoti",
              "text": "Ill try tomorrow but only with 64gb ram. 5070ti\n9800x3d",
              "score": 6,
              "created_utc": "2026-02-03 20:27:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3femdp",
                  "author": "Zerokx",
                  "text": "keep us updated",
                  "score": 2,
                  "created_utc": "2026-02-03 22:42:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3ewpbq",
              "author": "Limp_Manufacturer_65",
              "text": "yeah im getting 23 tk/s on 96gb ddr5, 7800x3d, 4070 ti super with what I think are ideal lm studio settings. q4km quant",
              "score": 3,
              "created_utc": "2026-02-03 21:17:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g0col",
                  "author": "UnionCounty22",
                  "text": "Context count? Very close to this configuration",
                  "score": 1,
                  "created_utc": "2026-02-04 00:39:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3j2f15",
              "author": "Loskas2025",
              "text": "I have a PC with RTX 5070TI 16GB + 64GB RAM. 22 tokens/sec",
              "score": 2,
              "created_utc": "2026-02-04 13:49:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eeu7i",
              "author": "FartOnYourBoofMound",
              "text": "No, but I will run it on a dedicated AMD Max+ Pro soon\n\nhttps://preview.redd.it/9xqblic24chg1.jpeg?width=3072&format=pjpg&auto=webp&s=616eeaa1cd062ad307da2f3c1e116bd9aad1dcb3",
              "score": 3,
              "created_utc": "2026-02-03 19:53:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3egxxy",
                  "author": "Head_Construction329",
                  "text": "Share your performace when you know it!",
                  "score": 3,
                  "created_utc": "2026-02-03 20:03:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3gttuc",
                  "author": "etcetera0",
                  "text": "Following",
                  "score": 2,
                  "created_utc": "2026-02-04 03:26:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3kr9tg",
                  "author": "mps",
                  "text": "I have the same box, here are my quick llama-bench scores:  \n`⬢ [matt@toolbx ~]$ AMD_VULKAN_ICD=RADV llama-bench -m ./data/models/qwen3-coder-next/UD-Q6_K_XL/Qwen3-Coder-Next-UD-Q6_K_XL-00001-of-00002.gguf -ngl 999 -fa 1 -n 128,256 -r 3`\n\n`ggml_vulkan: Found 1 Vulkan devices:`\n\n`ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat`\n\n`| model | size | params | backend | ngl | fa | test | t/s |`\n\n`| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |`\n\n`| qwen3next 80B.A3B Q6_K | 63.87 GiB | 79.67 B | Vulkan | 999 | 1 | pp512 | 502.71 ± 1.23 |`\n\n`| qwen3next 80B.A3B Q6_K | 63.87 GiB | 79.67 B | Vulkan | 999 | 1 | tg128 | 36.41 ± 0.04 |`\n\n`| qwen3next 80B.A3B Q6_K | 63.87 GiB | 79.67 B | Vulkan | 999 | 1 | tg256 | 36.46 ± 0.01 |`\n\nAnd gpt-oss-120b for reference\n\n`⬢ [matt@toolbx ~]$ AMD_VULKAN_ICD=RADV llama-bench   -m ./data/models/gpt-oss-120b/gpt-oss-120b-F16.gguf   -ngl 999   -fa 1 -n 128,256   -r 3`      \n`ggml_vulkan: Found 1 Vulkan devices:`  \n`ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat`  \n`| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |`  \n`| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |`  \n`| gpt-oss 120B F16               |  60.87 GiB |   116.83 B | Vulkan     | 999 |  1 |           pp512 |        572.85 ± 0.73 |`  \n`| gpt-oss 120B F16               |  60.87 GiB |   116.83 B | Vulkan     | 999 |  1 |           tg128 |         35.57 ± 0.02 |`  \n`| gpt-oss 120B F16               |  60.87 GiB |   116.83 B | Vulkan     | 999 |  1 |           tg256 |         35.56 ± 0.04 |`",
                  "score": 1,
                  "created_utc": "2026-02-04 18:38:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3ks59t",
                  "author": "FartOnYourBoofMound",
                  "text": "had weird issues last night w/ all this - also... had to replace my solar inverter (nuclear fusion energy is money) - just installed ollama prerelease [v0.15.5](https://github.com/ollama/ollama/releases/tag/v0.15.5-rc2) and it comes w/ BOTH [rocm support](https://github.com/ollama/ollama/releases/download/v0.15.5-rc2/ollama-linux-amd64-rocm.tar.zst) AND you'll (obviously) need the /bin/ollama - which is in the [linux-amd](https://github.com/ollama/ollama/releases/download/v0.15.5-rc2/ollama-linux-amd64.tar.zst) \\- I've been seeing this; msg=\"entering low vram mode\" \"total vram\"=\"0 B\" threshold=\"20.0 GiB\" - but i dunno - I'm not sure it's true - ollama has been KILLING it even though i've been messing around with rocm - every model i throw at this AMD Max+ Pro has been insanely fast. in the bios I've got 64Gb set in the UMA (forced - i guess) - not sure i understand all this AMD jargon, but hopefully i can bump up to 96Gb in the near future (the AMD Max+ Pro has a total 128gb)... more info about 5 minutes.\n\nhttps://preview.redd.it/lrfpulk8wihg1.png?width=1984&format=png&auto=webp&s=11bd80f2c8ce0134bf4f8cc159694c077719db72",
                  "score": 1,
                  "created_utc": "2026-02-04 18:42:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwjgj4",
      "title": "Anyone here actually using AI fully offline?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qwjgj4/anyone_here_actually_using_ai_fully_offline/",
      "author": "Head-Stable5929",
      "created_utc": "2026-02-05 12:02:24",
      "score": 162,
      "num_comments": 133,
      "upvote_ratio": 0.96,
      "text": "I keep coming back to the idea of running AI locally you know, like a GPT-style assistant that just works on your own device without the internet or Wifi connection?\n\nNot to build anything serious or commercial. I just like the idea of being able to read my own files, understand things or think stuff through without relying on cloud services all the time. Especially when there is no connection, internet services change or when things gets locked behind paywalls.\n\nEvery time I try local setups though, it feels more complicated than it should be. The models work, but the tools feel rough and it’s easy to get lost tweaking things when you just want something usable.\n\nI'm just curious if anyone here actually uses offline AI day to day or if most people try it once and move on. I would really be interesting to hear what worked and what didn’t.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qwjgj4/anyone_here_actually_using_ai_fully_offline/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3pg7d0",
          "author": "Neun36",
          "text": "You have different options, easist is LM Studio as already mentioned, search for the Model on huggingface which suits your GPU (fast response) or your RAM (slow response). Then there is Ollama which also runs locally and you can search for Models on ollama Web Page and how to. Then there is openwebUI which is Local Web UI which runs locally and Access to UI via Browser and it Looks more Like ChatGPT but you have the control, you can combine this with ComfyUI to generate images too but it’s more complicated. There are many other options available and above one are just a tiny bit of Tipps and easy ways.",
          "score": 43,
          "created_utc": "2026-02-05 12:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qhnyk",
              "author": "Head-Stable5929",
              "text": "Thanks a lot that helps!",
              "score": 3,
              "created_utc": "2026-02-05 15:58:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3wqoqf",
                  "author": "FlatImpact4554",
                  "text": "The guy above me nailed it . Listen to him.  You'll have the same exact web page and results like online . But private.",
                  "score": 1,
                  "created_utc": "2026-02-06 14:44:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tp7st",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-02-06 01:31:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uhbfp",
                  "author": "Neun36",
                  "text": "Depends on, Most Common is ComfyUI and there you have all the Image, Video, Audio, 3d Models which you can Play with, they integrated also already a UI which reminds of swarmUI and else, so more user friendly but generating stuff locally depends on your GPU and RAM, Not only GPU is crucial for ComfyUI especially for Video Generation, to combine that with openwebUI to have the look of ChatGPT is possible with openwebUI But I think there is no Video Generation implemented in openwebUI (didn’t checked the latest Version, but last i checked 3 weeks ago there was none.) But this also can be solved.\n\nFor Video + Audio Like Sora there is LTX2 which runs locally but as said depends on gpu and ram. There are also other models like wan 2.2, scail, Move and many more depends on your use case.",
                  "score": 2,
                  "created_utc": "2026-02-06 04:24:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pfpk8",
          "author": "dsartori",
          "text": "Yeah I’m all offline. My use cases are coding.  consulting, and community organizing. \n\nOnly the coding workflows really require a beefy setup. My teammate who is a pure consultant uses gpt-oss-20b in LMStudio pretty heavily but not exclusively.",
          "score": 19,
          "created_utc": "2026-02-05 12:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q3rxv",
              "author": "Recent_Double_3514",
              "text": "What type of consulting?",
              "score": 3,
              "created_utc": "2026-02-05 14:51:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q8oyp",
                  "author": "dsartori",
                  "text": "We do data engineering work, software development, organizational development (strategic planning and so on), education and training in a variety of fields. I run my document authoring workflows through Cline these days.",
                  "score": 3,
                  "created_utc": "2026-02-05 15:16:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3q8unt",
              "author": "woundedkarma",
              "text": "He just chatting with it? Or does he run agents?",
              "score": 1,
              "created_utc": "2026-02-05 15:17:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q9fnw",
                  "author": "dsartori",
                  "text": "She's mostly chatting with it in LMStudio. We tried it with Cline but the results were pretty poor.",
                  "score": 4,
                  "created_utc": "2026-02-05 15:19:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3q3j7z",
              "author": "HumanThing1233",
              "text": "What are you using for coding? I can't figure out what kind of hardware I need to do so.",
              "score": 1,
              "created_utc": "2026-02-05 14:50:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q93xy",
                  "author": "dsartori",
                  "text": "Strix Halo. I got the GMKTec one. It's a cheap path to good performance from midsized MoEs.",
                  "score": 6,
                  "created_utc": "2026-02-05 15:18:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3q8tko",
                  "author": "Rand_o",
                  "text": "Strix halo 395+ 128 GB is pretty versatile",
                  "score": 4,
                  "created_utc": "2026-02-05 15:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pexse",
          "author": "DAlmighty",
          "text": "I’m 100% offline and loving it.",
          "score": 53,
          "created_utc": "2026-02-05 12:27:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3q84yp",
              "author": "p_235615",
              "text": "I use beside many apps like maubot, vscode cline, n8n, homeassistant also running open-webui in docker as a simple interface for chats or voice interaction with AI.",
              "score": 6,
              "created_utc": "2026-02-05 15:13:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3pfj4h",
              "author": "Head-Stable5929",
              "text": "can you tell me more about what you did and how you did it? im collecting info, so your input will be useful!",
              "score": 3,
              "created_utc": "2026-02-05 12:31:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3qpc6q",
                  "author": "DrummerHead",
                  "text": "On Mac:\n\n\n    brew install lm-studio\n    brew install draw-things\n\n\nThey both give you the ability to download models from within the app. Find the documentation for both and get experience over time.\n\nIf you want two good starting models for each:\n\n* https://huggingface.co/openai/gpt-oss-20b\n* https://huggingface.co/Tongyi-MAI/Z-Image-Turbo\n\nYou don't really need the links, you can just choose them from the menus within each program.\n\nAnd that's it! From there, the more independent research you do, the more value you'll be able to extract from these programs and the many models you'll find and get experience with. Enjoy!",
                  "score": 11,
                  "created_utc": "2026-02-05 16:33:53",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pg5ee",
                  "author": "DAlmighty",
                  "text": "I run vLLM in docker for inference and use the API to connect various services. Which is what everyone does. You’re not going to get anything different from anyone if you’re speaking only about inference.",
                  "score": 16,
                  "created_utc": "2026-02-05 12:35:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pmhbb",
                  "author": "Karyo_Ten",
                  "text": "I rotate between vLLM, SGLang, ik-llama.cpp, TabbyAPI / ExLlamav3 in docker depending on model support, quantization, etc. I just connect to them through \"OpenAI-compatible\"",
                  "score": 3,
                  "created_utc": "2026-02-05 13:16:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pieit",
          "author": "DatBass612",
          "text": "I bought the most expensive M3 Ultra and worried a ton about getting to positive ROI running OSS 120B. I ended up hitting that positive number in 5 months. \n\nI use about 200$ish in tokens a day by my estimates. Now with stuff like OpenClaw it’s only going to be more tokens and I’m glad I have the extra unified memory to have virtualization run sub agents.",
          "score": 15,
          "created_utc": "2026-02-05 12:50:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pm0yj",
              "author": "Front-Relief473",
              "text": "However, m3 ultra's computing power is so low, its prompt prefill speed is too slow, and the amount of contexts you carry in handling complex tasks with agent is too large, which leads to a very long time for you to receive the first word in each round. How do you handle this situation?",
              "score": 3,
              "created_utc": "2026-02-05 13:13:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pupmp",
                  "author": "beachguy82",
                  "text": "Define “long time”. I’ve been considering a Mac Studio myself.",
                  "score": 3,
                  "created_utc": "2026-02-05 14:02:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3q6dtl",
                  "author": "Crazyfucker73",
                  "text": "Incorrect",
                  "score": 1,
                  "created_utc": "2026-02-05 15:04:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3qxlin",
                  "author": "andreabarbato",
                  "text": "I have a topk implementation for llama.cpp that can make prefill faster sometime. wanna try? hmu (it's free on github but I never tested it on apple, might take some tweaking)",
                  "score": 1,
                  "created_utc": "2026-02-05 17:12:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3rdyf0",
                  "author": "Fair-Cookie9962",
                  "text": "Why long time is an issue? Parallel agents cut the time. If you have reliable output, time does not matter that much, and if you adjust tasks to utilise smaller models, can get nice speedups. Also using llama.cpp directly also often cuts wait in half.",
                  "score": 1,
                  "created_utc": "2026-02-05 18:27:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xj3l4",
                  "author": "DatBass612",
                  "text": "I mean slow is relative. Sure my Nvidia GPU is faster, but I have the 70GB model loaded with overhead to spawn sub models. But it’s more than incredibly fast for what I need it for. It’s queue based on openclaw and I can schedule things out for night time when there is low usage. My token context is 128000 and batching at 10000. It’s absolutely prosumer grade. People who want the utmost speed should just pay for Anthropic or Gemini. Unless you have massive budget backed by enterprise of some sort these are the trade offs you have to make. As a consumer i say anything over 5 tokens a second is usable and i cant really read responses faster than that. Plus the scheduling and batching has me come back to read things after long tool call process chains. \n\nWhats important to remember is to ask yourself, how much AI garbage data are you actually going to read, and is what it is producing even valuable that you need blazing fast speeds.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:00:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3q8jgu",
              "author": "_hephaestus",
              "text": "I have a similar setup, is OSS 120B still the best in class?  Also just surprised at the cost, I didn't think you _could_ burn through that many a day, but the billing details at work are hidden from me.",
              "score": 1,
              "created_utc": "2026-02-05 15:15:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xjiir",
                  "author": "DatBass612",
                  "text": "Best in class, maybe. It’s great for what I’m using it for. You can get way more complicated with specialist smaller models. But I’m not willing to dedicate more time to this project which already is like 40+ hours of setup",
                  "score": 1,
                  "created_utc": "2026-02-06 17:02:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xkmpk",
                  "author": "DatBass612",
                  "text": "I evaluate solely based on how the agent responds. So with GLM and Mistral and the others they just felt “off” completely personal preference.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:08:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3v9k23",
              "author": "holdmymandana",
              "text": "What do you use open claw for?",
              "score": 1,
              "created_utc": "2026-02-06 08:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xhzd7",
                  "author": "DatBass612",
                  "text": "I use it for outreach automation, a newsletter for my customers, and a help chatbot. More plans I’m still formulating but it’s a great scheduled research assistant. I also have it moderate some chats.",
                  "score": 1,
                  "created_utc": "2026-02-06 16:55:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pnpyi",
          "author": "michael_p",
          "text": "I run qwen 3 32b on a Mac Studio m3 ultra with 96 gb ram. I run confidential information through it and get incredible results (Claude code wrote the prompts). It’s an invaluable tool for me.",
          "score": 9,
          "created_utc": "2026-02-05 13:23:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vnq4b",
              "author": "tuxmaniac",
              "text": "What do you mean by \"Claude code wrote the prompts\"? Can you explain your setup a bit more? Thank you!",
              "score": 1,
              "created_utc": "2026-02-06 10:28:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o431yq4",
                  "author": "michael_p",
                  "text": "Yes!! I’m so excited to geek out over how good qwen3 is. I had Claude code build me a platform to find businesses for sale. When I reach out to the seller, they’ll send me confidential documents like bank statements and tax returns etc. I can drop those into a documents page on the system Claude code built and have it run a local analysis with qwen3. I worked with Claude code to explain what the analysis should do and it made them leaps and bounds more detailed than what I would ask for. So Claude code built out all the prompts of how the documents get loaded in and then what it asks for from qwen. It was having issues with context windows (eg loading in 12 bank statements or more) so Claude code built a way to chunk those out, get a response and then feed that response back in with the next set to get a full and accurate analysis. Qwen makes minor mistakes but Claude has been able to understand them and tweak them.",
                  "score": 2,
                  "created_utc": "2026-02-07 14:29:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3q6xeu",
          "author": "FaceDeer",
          "text": "I've been using a system of custom scripts to process and summarize transcripts of audio files containing a lot of personal information, so strictly local AIs for that. I recently tried playing around with [AnythingLLM](https://anythingllm.com/) and found it had a pretty good \"NotebookLM\"-style system that was easy to use, I dumped a bunch of data into it and that worked nicely.",
          "score": 8,
          "created_utc": "2026-02-05 15:07:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q7eda",
          "author": "ionizing",
          "text": "I spent several months building my own tool enabled Ai chat interface that uses llama.cpp and I love it.",
          "score": 8,
          "created_utc": "2026-02-05 15:09:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3tnspl",
              "author": "Thiqaa",
              "text": "Hey can you elaborate on this please? What type of tool enables AI chat interface, something akin to Openwebui? Effectively a don't end interface for llamacpp? Or was I over thinking this lol",
              "score": 1,
              "created_utc": "2026-02-06 01:23:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qoit2",
          "author": "henk717",
          "text": "Of course, I am one of the people involved with KoboldAI and KoboldCpp so its natural that I run our own stuff and enjoy local AI. If I wasn't passionate about running local I wouldn't be involved that heavily in a community that focus on running it yourself (or hosting it in the cloud with the same software).",
          "score": 5,
          "created_utc": "2026-02-05 16:30:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qa12g",
          "author": "woundedkarma",
          "text": "I've been thinking setting up local AI for people will become a business. Just not sure how to find less technical people who feel like you guys and want it local. 🤔",
          "score": 5,
          "created_utc": "2026-02-05 15:22:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xmye",
              "author": "Head-Stable5929",
              "text": "I know right! I want to make one that most likely runs offline, I'm kinda stuck and I want to know the other options",
              "score": 1,
              "created_utc": "2026-02-07 04:02:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pt9b6",
          "author": "segmond",
          "text": "Yes, for 2 years now.",
          "score": 4,
          "created_utc": "2026-02-05 13:54:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q5974",
          "author": "Craygen9",
          "text": "I batch process documents to create summaries as part of an automated workflow, I want to keep everything local for privacy reasons. I've used the API of a local install LM Studio and it was easy to set up and worked well, but now I use llama.cpp with my own code as I have more control over it.",
          "score": 4,
          "created_utc": "2026-02-05 14:58:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qbwmu",
          "author": "GravitationalGrapple",
          "text": "I use kobold.cpp for LLMs, comfyui for image/video generation, and indextts2 with its own custom gui for TTS. I only have 16gb VRAM, 64 RAM, so I am somewhat limited in the models I use.",
          "score": 4,
          "created_utc": "2026-02-05 15:31:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pcwx5",
          "author": "ptear",
          "text": "If you haven't used LM Studio, try it.",
          "score": 8,
          "created_utc": "2026-02-05 12:12:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pdw1p",
              "author": "Head-Stable5929",
              "text": "i see, ill check it out! thanks for the suggestionnnn, could you also tell me if you have come across ollama, it seemed a bit similar and what could possibly be the difference between them?",
              "score": 2,
              "created_utc": "2026-02-05 12:19:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ppyx9",
                  "author": "ThatHavenGuy",
                  "text": "As a beginner, LMStudio has a sleek interface that makes setting this stuff up pretty easy and can help you learn more about the different ways to run local models and mess with the parameters. Ollama gets out of your way and runs as a service behind the scenes. Both are good stepping stones to better, more complicated setups like using llamacpp's router function or vLLM. I'd say give LMStudio a go and see how it treats you.",
                  "score": 7,
                  "created_utc": "2026-02-05 13:36:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3qat8j",
                  "author": "GravitationalGrapple",
                  "text": "Llama.cpp > ollama. LM studio uses llama.cpp, just has a beginner friendly GUI.",
                  "score": 2,
                  "created_utc": "2026-02-05 15:26:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3qe8gu",
          "author": "Virtual_Actuary8217",
          "text": "Gptoss 120b for tasks like planning,qwen coder 3 next for heavy python coding , I have a custom c api that exposed to python, I just need to feed the api.md , qwencoder3 next works surprisingly well, one shot, but it is slow like 6/t per seconds, it needs 80g of memory to work, still it's free , local ,that's important for my use cases",
          "score": 3,
          "created_utc": "2026-02-05 15:42:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qj1pu",
          "author": "Resonant_Jones",
          "text": "im almost there, I had to build my own system because like you said, the tools available are a little rough. I love having full control over the system though and nothing unexpected changes ever. \n\nYou can literally run the computer all day long coding or other tasks and it just doesn't cost anymore than electricity. I love it. Highly encourage anyone interested to get into it. \n\n",
          "score": 3,
          "created_utc": "2026-02-05 16:04:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ta2m8",
              "author": "rolleicord",
              "text": "What did you build it in? Details please :)",
              "score": 3,
              "created_utc": "2026-02-06 00:03:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3tgc5k",
                  "author": "Resonant_Jones",
                  "text": "Mostly Python and used Codex to build most of it with sprinkles of Claude and Gemini in between. What do you want to know?",
                  "score": 2,
                  "created_utc": "2026-02-06 00:39:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rrobl",
          "author": "fallingdowndizzyvr",
          "text": "I only use AI offline.",
          "score": 3,
          "created_utc": "2026-02-05 19:30:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xqn8",
              "author": "Head-Stable5929",
              "text": "Can you tell me more please",
              "score": 1,
              "created_utc": "2026-02-07 04:03:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o413x06",
                  "author": "fallingdowndizzyvr",
                  "text": "You download AI. You run AI. It's offline.",
                  "score": 2,
                  "created_utc": "2026-02-07 04:47:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rz9w1",
          "author": "X3r0byte",
          "text": "The problem is you can make this as complicated as you want it to be, and so without a specific use case you’re going to try and boil the ocean.\n\nMainly I draw the line with anything privacy focused, like working through personal problems, interpreting docs I do not want sent to a server, things of that nature. OWUI + name_your_model is more than enough for that.\n\nI also have a totally custom assistant with speech to text/text to speech that generates and runs code inside a container arbitrarily. That’s for “fun”.\n\nI also use cloud models for code if it’s unreasonable to expect quality out of my hardware.\n\nI guess the point is - “it depends.”",
          "score": 3,
          "created_utc": "2026-02-05 20:06:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s461v",
          "author": "SweetHomeAbalama0",
          "text": "Yes, we have a 768Gb Ai server for this exact thing (a local \"chatGPT\"-like replacement).\n\nCombine something like GLM 4.6v (vision support) with ComfyUI Flux/ZIT/QwenEdit/etc. and you can get something that closely resembles GPT without any token restrictions. It might not be a 1:1 exact, but if you have patience to set up the moving parts, front end, back end, tool calling, etc., the result can accomplish 95-98% of what chatGPT can do. Feedback I've received has actually been that the primary users \"prefer\" the new local alternative over chatGPT, citing that chatGPT is slower for things like image gen/edits with not enough variation in art style, and can be more finicky understanding prompts if the user is not a strong English speaker.\n\nIt comes with an initial startup cost and probably some technical aptitude, but it can absolutely be done.",
          "score": 3,
          "created_utc": "2026-02-05 20:30:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3s5d1m",
              "author": "Clear-Astronaut-8006",
              "text": "Curious on how you made the server? Approx. Cost and put in context on build date. Thanks.",
              "score": 3,
              "created_utc": "2026-02-05 20:35:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yt8hy",
                  "author": "SweetHomeAbalama0",
                  "text": "I have a deep dive post on localllama about the server with the details you may be looking for. Total est cost \\~17k usd but these are prices from before the component price spike. \n\n[The post](https://www.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/)\n\nThere's a link in the post body to the full youtube video if you want better quality, it is a little rough around the edges since it was my first time doing a show/demo like this but it touches on what I thought most people would be interested to know. I will say, the case is what made it all work, so I dedicated a good chunk of time just talking about the case. Feel free to skip to whatever part you want to see tho, time stamps are in the YT description.",
                  "score": 3,
                  "created_utc": "2026-02-06 20:44:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3sxv8q",
          "author": "Bino5150",
          "text": "I run completely local. I have a few different ways to set it up. Lately I’ve been running LM Studio with AnythingLLM as my agent. Been playing around with Agent Zero, OpenClaw, and n8n as well, but still using LM Studio or Ollama. Whisper for STT, and Piper for TTS.",
          "score": 3,
          "created_utc": "2026-02-05 22:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tkqgc",
          "author": "ScuffedBalata",
          "text": "The tools ARE rough and require setup.  There's no \"plug and play assistant\".\n\nThey require intense amounts of hardware and some willingness to set up complex workflows.  It's still \"techie coder\" territory.",
          "score": 3,
          "created_utc": "2026-02-06 01:04:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pekjm",
          "author": "custodiam99",
          "text": "It is f\\*cking slow, but yeah, you can do serious stuff with it.",
          "score": 6,
          "created_utc": "2026-02-05 12:24:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pfcnd",
              "author": "DAlmighty",
              "text": "Sounds like you need more VRAM. \n\nI personally will NEVER advise anyone to use system RAM for inference. It stops being worth it to me if you use it.",
              "score": 10,
              "created_utc": "2026-02-05 12:30:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q31me",
                  "author": "custodiam99",
                  "text": "I have 96GB DDR5 and 24GB VRAM, and I think that's all the RAM I will have for the foreseeable future lol.",
                  "score": 3,
                  "created_utc": "2026-02-05 14:47:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pfmj7",
                  "author": "Head-Stable5929",
                  "text": "why is that? can you explain it for me please?",
                  "score": 2,
                  "created_utc": "2026-02-05 12:32:06",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o3pgn67",
                  "author": "Electrical_Hat_680",
                  "text": "Or get a AI GPU like Modivius by Intel off eBay.",
                  "score": 2,
                  "created_utc": "2026-02-05 12:39:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3pbnft",
          "author": "codeforgeai",
          "text": "Yes i'm running ai fully offline and it's actually worth it",
          "score": 11,
          "created_utc": "2026-02-05 12:03:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pbs5j",
              "author": "Head-Stable5929",
              "text": "can you tell more on what do you think about it, please? it would be really helpful",
              "score": 4,
              "created_utc": "2026-02-05 12:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pkjqm",
                  "author": "[deleted]",
                  "text": "[removed]",
                  "score": -14,
                  "created_utc": "2026-02-05 13:04:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ph7tb",
          "author": "LondonTownGeeza",
          "text": "Run mine on a i5 with a 6600, slightly slower than online OpenAI, but very useable and private.",
          "score": 2,
          "created_utc": "2026-02-05 12:42:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pi4mr",
          "author": "Large_Election_2640",
          "text": "I’m running stable diffusion on comfyui fully\nLocal. Playing with image models. It works fine on my 3060 12gb gpu with 24gb ram.",
          "score": 2,
          "created_utc": "2026-02-05 12:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pvn1o",
          "author": "Klutzy_Ad_1157",
          "text": "Yes only offline. I use Gemma 3 27b in 5 bit quant on a RTX 3090 because it speaks German very well. On my main rig I use a RTX 4090 and a RTX 5090 for image and video generation with ComfyUI.",
          "score": 2,
          "created_utc": "2026-02-05 14:08:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qfnqe",
          "author": "Ok-Dog-4",
          "text": "Try Localai.io. You’re able to run basically any model and the docker configuration isn’t the most complicated. Works offline in your terminal. I sometimes will finetune with a LoRA in a CoLab notebook and use my new model with localai. Then also you can API to practically anything. Cheers",
          "score": 2,
          "created_utc": "2026-02-05 15:49:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qhww9",
          "author": "gptlocalhost",
          "text": "\\> document authoring\n\nCurious if the following hybrid approach might be useful:\n\n  [https://youtu.be/RkxbCAaZ7Dw](https://youtu.be/RkxbCAaZ7Dw)\n\n",
          "score": 2,
          "created_utc": "2026-02-05 15:59:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3qi9g3",
              "author": "Head-Stable5929",
              "text": "Omg thank you very much!",
              "score": 1,
              "created_utc": "2026-02-05 16:01:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3qoewy",
          "author": "aniketmaurya",
          "text": "Ollama and chill!",
          "score": 2,
          "created_utc": "2026-02-05 16:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r391e",
          "author": "shibe5",
          "text": "My AI uses DNS, so it won't work without internet connection.\n\nAs for the tools, I mostly use my own.",
          "score": 2,
          "created_utc": "2026-02-05 17:38:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ri8nz",
              "author": "Fair-Cookie9962",
              "text": "You can have local DNS, just like your router.",
              "score": 2,
              "created_utc": "2026-02-05 18:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ruogu",
                  "author": "shibe5",
                  "text": "Yes, I can have the router resolve relevant names without using the internet. But currently, it's not configured that way.",
                  "score": 2,
                  "created_utc": "2026-02-05 19:44:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rd8gr",
          "author": "Fair-Cookie9962",
          "text": "Yes, why? All is in the prompt.",
          "score": 2,
          "created_utc": "2026-02-05 18:24:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rezec",
          "author": "maximebodereau",
          "text": "Yes ! Check my ux tool, I need feedbacks on it ! \nYou have to have a llamacpp installed on your machine \n\nhttps://ux-research-production.up.railway.app/",
          "score": 2,
          "created_utc": "2026-02-05 18:32:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rt3t2",
          "author": "Eupolemos",
          "text": "Sure.\n\nI have a 5090. I use Gemma 3 for stuff not related to development and Devstral 2 for my programming needs.\n\nAt work, we use Claude Code, but Devstral 2 works fine for my personal needs.",
          "score": 2,
          "created_utc": "2026-02-05 19:37:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s2ui6",
          "author": "Macdaddy4sure",
          "text": "I wrote a program for offline ai extensibility.\nhttp://macdaddy4sure.ai/",
          "score": 2,
          "created_utc": "2026-02-05 20:23:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sap2k",
          "author": "GlassAd7618",
          "text": "I’m experimenting with Ollama and the latest models.",
          "score": 2,
          "created_utc": "2026-02-05 21:01:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t4a3o",
          "author": "Express_Quail_1493",
          "text": "i use openwebui with python and connect it to my localhost models running on lmstudio server. i expose it through a cloudflare tunnel and talk to my localhost AI on my phone anywhere via voice ",
          "score": 2,
          "created_utc": "2026-02-05 23:30:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tfiln",
          "author": "Limebird02",
          "text": "Main bottleneck is having the memor, GPU and overall horsepower to run a worthy model that's significant enough to provide useful intelligence to you. I'd say it will cost at least $1400 in computing hardware. Most people can't justify that right now when for free or for $20/month they get access to USA frontier models.",
          "score": 2,
          "created_utc": "2026-02-06 00:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tgdek",
          "author": "Weary_Long3409",
          "text": "I run only qwen3-vl-8b-instruct to automate daily activity reports processing from 25 remote legal teams across the country. Something that commercial endpoints are irrelevant for such tasks. Have a really good complex prompt from ChatGPT, and this small qwen is does the job very well.",
          "score": 2,
          "created_utc": "2026-02-06 00:39:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tjfxo",
          "author": "KooperGuy",
          "text": "Yes",
          "score": 2,
          "created_utc": "2026-02-06 00:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tr2lu",
          "author": "ANTIVNTIANTI",
          "text": "almost exclusively",
          "score": 2,
          "created_utc": "2026-02-06 01:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ul0sp",
          "author": "ccalabro",
          "text": "Yeah I used ollama deepseek. It worked better than I expected.",
          "score": 2,
          "created_utc": "2026-02-06 04:50:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xw8v",
              "author": "Head-Stable5929",
              "text": "Oh wow! I'll give it a try",
              "score": 1,
              "created_utc": "2026-02-07 04:04:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uvfqq",
          "author": "minitoxin",
          "text": "i use it everyday openwebui with searxng and LMstudio as back-end server , When i need deep searches i use perplexica with mistral14b  running on a headless  m4 mini with 16GB , it gives very detailed responses although its slow ",
          "score": 2,
          "created_utc": "2026-02-06 06:08:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xy7f",
              "author": "Head-Stable5929",
              "text": "Still that sounds like it can work just fine",
              "score": 1,
              "created_utc": "2026-02-07 04:04:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3uzkie",
          "author": "my_story_bot",
          "text": "yeah LMStudio or Ollama is great. whats nice with LMStudio is that you can simply search for a model in their UI and download and run it in a couple clicks. \n\nWhats REALLY cool is that you can  run it as a web server locally through localhost and then use it for stuff like openclaw",
          "score": 2,
          "created_utc": "2026-02-06 06:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v1a2g",
          "author": "djtubig-malicex",
          "text": "Yes. Never trust cloud.",
          "score": 2,
          "created_utc": "2026-02-06 06:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6bx5",
          "author": "El_Danger_Badger",
          "text": " I built a local chat/LangBoard/RAG platform on my stock M1 Mac Mini. MLX native, on my stock M1 Mini.  I use it daily. Not flashy, but it does its job. Has some bells and whistles, plus an agent constitution.    >cough< OpenClaw >cough< . I co-deved with ChatGPT.  Not one shot. Typed one module at a time, asked an endless flow of questions over months because I wanted to learn how to build something like this.  Now I know. If anyone is hiring... ",
          "score": 2,
          "created_utc": "2026-02-06 07:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6lvo",
          "author": "Efficient_Bug_0",
          "text": "For running the actual models offline, Ollama works pretty well on most modern Macbooks. For vibe-coding offline, I had the same need, and ended up building something called Codistry for exactly that purpose: [https://codistry.ai/docs/guides/ollama](https://codistry.ai/docs/guides/ollama)",
          "score": 2,
          "created_utc": "2026-02-06 07:46:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3wjizs",
          "author": "launch_everywhere",
          "text": "yeah I am. I'm using it offline for both mobile as well as desktop. I've got one that sits silently and watches all my AI chats with claude/gemini/gpt etc and creates like a shared memory that I can use best. MASSIVE productivity boost. And then I use offline/local LLM  on mobile to have private conversations that I may not want to share with the cloud providers. ",
          "score": 2,
          "created_utc": "2026-02-06 14:07:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x6pph",
          "author": "Polysulfide-75",
          "text": "Yes but you’re right it can be complicated.  I write my own interfaces and such though.\n\nYou can use LMstudio for something simple to install and run.  Or there are plenty of ChatGPT clones you can run locally.",
          "score": 2,
          "created_utc": "2026-02-06 16:02:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xi5yj",
          "author": "notbullshittingatall",
          "text": "I do. My setup is Ollama running on my gaming rig using Tailscale to connect to it from my MacBook or iPhone when I'm away from the house. I use an app called Reins on my phone. Works well for me.",
          "score": 2,
          "created_utc": "2026-02-06 16:56:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xkq3r",
          "author": "Decent-Freedom5374",
          "text": "I run 5 off line all day continuously",
          "score": 2,
          "created_utc": "2026-02-06 17:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xwv5c",
          "author": "Johnsmith2k18",
          "text": "Im an idiot and pinokio was easy enough to use to set up an offline chatgpt style thing, an image editor, image generator, voice cloning tts, and it's really shoddy due to my computer specs but I can even do some video generation, also have a charcter Ai type thing through silly tavern but that uses ollama.\n\nBut yeah pinokio makes it really easy for image and video stuff just use pinokio to get comfyui and use some template workflows to learn how it works, which i promise is not as complicated as itll seem when you first look at it.",
          "score": 2,
          "created_utc": "2026-02-06 18:06:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40y68r",
              "author": "Head-Stable5929",
              "text": "Thank you so much that sounds like a juicy insight! I'll give it a try",
              "score": 1,
              "created_utc": "2026-02-07 04:06:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42syq6",
          "author": "Mac_NCheez_TW",
          "text": "I only use offline for my personal projects. I use Gemini for front end crap to build basic websites. ",
          "score": 2,
          "created_utc": "2026-02-07 13:37:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qs61p",
          "author": "mystery_biscotti",
          "text": "What's the concern? Are you worried it's just a toy, or do you feel like it's not \"as good\"?",
          "score": 1,
          "created_utc": "2026-02-05 16:46:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40xh0o",
              "author": "Head-Stable5929",
              "text": "Well I'm not worried about anything, i want to explore that's all and I just wanted to know what all the options there are out there :)",
              "score": 1,
              "created_utc": "2026-02-07 04:01:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41iu89",
                  "author": "mystery_biscotti",
                  "text": "Ah, okay, thanks for clarifying!",
                  "score": 1,
                  "created_utc": "2026-02-07 06:49:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43vq0z",
          "author": "Ok_Membership3521",
          "text": "I am running completely offline.. Claude wrote most of my set up as well.  I have a custom front end feels like a corporate model easy to use.  I’m completely local.. works well for me.",
          "score": 1,
          "created_utc": "2026-02-07 17:00:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44juuf",
          "author": "greymatter_ac3",
          "text": "I use it everyday offline and only go to the big models online when I am forced to. Also do the same in my work. Large model are overkill for most tasks. The real advantage is SLM (Small Language Models) and fine running those on specific use cases. This is the real unlock. A small custom model for your use case that runs on low power inexpensive hardware.",
          "score": 1,
          "created_utc": "2026-02-07 18:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pjryz",
          "author": "atkr",
          "text": "skill issue",
          "score": -4,
          "created_utc": "2026-02-05 12:59:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40x7x3",
              "author": "Head-Stable5929",
              "text": "No need to be like that my friend",
              "score": 1,
              "created_utc": "2026-02-07 03:59:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtuwn5",
      "title": "Local model fully replacing subscription service",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qtuwn5/local_model_fully_replacing_subscription_service/",
      "author": "Icy_Distribution_361",
      "created_utc": "2026-02-02 13:23:00",
      "score": 90,
      "num_comments": 112,
      "upvote_ratio": 0.93,
      "text": "I'm really impressed with local models on a Macbook Pro M4 Pro with 24GB memory. For my usecase, I don't really see the need anymore for a subscription model. While I'm a pretty heavy user of ChatGPT, I don't really ask complicated questions usually. It's mostly \"what does the research say about this\", \"who is that\", \"how does X work\", \"what's the etymology of ...\" and so on. I don't really do much extensive writing together with it, or much coding (a little bit sometimes). I just hadn't expected Ollama + GPT-OSS:20b to be as high quality and fast as it is. And yes, I know about all the other local models out there, but I actually like GPT-OSS... I know it gets a lot of crap.\n\nAnyone else considering, or has already, cancelling subscriptions?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qtuwn5/local_model_fully_replacing_subscription_service/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o35i76p",
          "author": "coldy___",
          "text": "Bro use the mlx based models on macbooks, they are specially designed to run on apple silicon, infact you are gonna get a like 40 percent better token per second speed if you switch to it, download LMstudio for access to mlx based gpt oss 20b",
          "score": 44,
          "created_utc": "2026-02-02 13:36:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35jbgy",
              "author": "Icy_Distribution_361",
              "text": "Oh wow, thank you for that tip, I'm quite the noob as you can tell. Must say it's already really fast though! I wouldn't say I feel a need for it to be faster. But hey, maybe it'll also reduce the heat production when I'm using it a lot since it'll be more efficient. Can't I load the mlx-GPT-OSS in Ollama though?",
              "score": 9,
              "created_utc": "2026-02-02 13:42:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35l4p4",
                  "author": "nickless07",
                  "text": "You can run them even via command line without installing Ollama or LM Studio. GPT-OSS is MXFP4 (or any converted format GGUF, MLX, Whatever). It is so fast due to beeing an MoE, which let it only be have a part active compared to dense models.  \nIf you really wanna enhance it by a lot try Open WebUI. With one install you get RAG, Memory, Websearch, Audio in-/output. and much more (it also runs locally).",
                  "score": 7,
                  "created_utc": "2026-02-02 13:52:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o35q442",
                  "author": "coldy___",
                  "text": "It's more efficient and you won't need a translational layer, as the format is specially built for them M series chips, but if you are a beginner definitely use a lm studio...",
                  "score": -1,
                  "created_utc": "2026-02-02 14:19:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o35wgjg",
              "author": "cuberhino",
              "text": "What model do you recommend for a base Mac mini m4? I have been wanting to try openclaw but worried about the security issues people keep talking about",
              "score": 1,
              "created_utc": "2026-02-02 14:53:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o364mvf",
                  "author": "coldy___",
                  "text": "I'm an AI engineer and I understand your concern, I myself never use these outside a sandbox... I'd say it you wanna use the openclaw, use an anthropic model like claude 4.5 sonnet or opus 4.5.... these are the only models that are actually safe in the market, almost all of the Chinese models are just meant to show benchmarks on coding and math and they kinda suck real hard on safety scores and all... Plus theybare pretty easy to jailbreak and all... Try ministral 3 models from mistral or gpt oss 20b for local needs also don't connect your actually whatsapp or msging apps or anything",
                  "score": -2,
                  "created_utc": "2026-02-02 15:33:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35v7y7",
          "author": "generousone",
          "text": "Gpt-oss:20b is a boss. If you have the space (24gb vram is more than enough) to max out it's context, it's really quite good. Not as good as ChatGPT or Claude of course, but it's enough to be a go to, and then when you hit its limits move to a commercial model.\n\n\nI have it running with full 128k context and it's only 17gb vram loaded in, so it's efficient too. That leaves space if you have 24gb vram for other GPU workflows like jellyfin or whatnot. I'm been really impressed by it.",
          "score": 8,
          "created_utc": "2026-02-02 14:46:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o369qe6",
              "author": "coldy___",
              "text": "Agreed it basically is on the same performance as the o3 mini and bro that was like the frontier model at some point... Not long ago but yeah",
              "score": 2,
              "created_utc": "2026-02-02 15:57:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o381585",
                  "author": "generousone",
                  "text": "The biggest change for me was getting enough VRAM to not just run a better model (I only had 8GB previously), but enough space to then give that model context. That made all the difference in the world",
                  "score": 1,
                  "created_utc": "2026-02-02 20:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o36htvf",
              "author": "2BucChuck",
              "text": "Compared this to GLM airs ?",
              "score": 1,
              "created_utc": "2026-02-02 16:35:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3816h9",
                  "author": "generousone",
                  "text": "never tried it. Good?",
                  "score": 1,
                  "created_utc": "2026-02-02 20:50:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45b8qn",
              "author": "Own_Atmosphere9534",
              "text": "mlx-community/gpt-oss-20b-MXFP4-Q8",
              "score": 1,
              "created_utc": "2026-02-07 21:25:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o37ysfl",
              "author": "cuberhino",
              "text": "So basically you could run a openclaw bot off local 3090 rig with 24gb vram? And avoid the high costs?",
              "score": 0,
              "created_utc": "2026-02-02 20:39:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o380yfs",
                  "author": "generousone",
                  "text": "Not familiar with openclaw, I use Ollama, but if it supports local models then yes. But there are limitations. While gpt-oss:20b is good and you can give it a lot of contexts with a 3090's 24GB, it's still only a 20b model. It will have limitations in accuracy and reasoning. I ran into this last night when putting in a large PDF even with RAG.\n\nI would not say it will replace commercial models if you lean on those a lot, but so far it's been good enough as a starting place and then if it can't handle what I'm asking, i switch to claude or chatgpt.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:49:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35l60n",
          "author": "2BucChuck",
          "text": "Like many of us , I have been working towards that as well- Claude is what I use most but I built an agent framework locally over a long period of struggling with the local model shortcomings - now testing the low end Gemma32 and others against agent tasks and skills using Claude and actually have been impressed how well they perform when the have a workflow or agent backbone.  \n\nFrom my tests bare minimum model size for a tool calling agent is around 30b , things less than that fall apart too often (unless someone can suggest small models that act like larger ones?).  I have an include to switch models in an out for the same workflows to compare … with the goal of fully local accomplishing the tasks , tools and skills files includes Claude code is using for context. \n\nNeed to be able to add tools and skills to match usefulness of subscriptions",
          "score": 5,
          "created_utc": "2026-02-02 13:52:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35v8fy",
          "author": "mike7seven",
          "text": "Go with MLX models mainly, they are faster. To make it easy use LM Studio. The latest updates are phenomenal. LM Studio also supports running models on Llama.cpp (like Ollama) if you don’t have an MLX model available.",
          "score": 4,
          "created_utc": "2026-02-02 14:46:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3667h1",
          "author": "apaht",
          "text": "I was on the same boat…got M4 max as well. Returned M5 with 24 gb ram for Max",
          "score": 2,
          "created_utc": "2026-02-02 15:41:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c4f7g",
              "author": "Broad-Atmosphere-474",
              "text": "I also thinking about getting the m 4 max I mainly use it for coding honestly you think the 64gb will be inf?",
              "score": 1,
              "created_utc": "2026-02-03 13:15:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3g4cn2",
                  "author": "apaht",
                  "text": "This was during Black Friday, November 2025 do the return windows were until Jan 15.. microcenter had 64gb m4 max, I can run 70b models barely with little overhead left. With I had 96gb \n\nBut I will get the ryzen 395+ if needed. Can always offload to cloud",
                  "score": 1,
                  "created_utc": "2026-02-04 01:01:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36fakq",
          "author": "ScuffedBalata",
          "text": "The capability of local models is WAY lower than the good cloud models.  Hallucination prevention, capability, etc is significantly different.\n\nIt's a tool.  It's a bit like saying \"This bicycle does exactly what I need, I'm really impressed with it\".\n\nFine, great.  GPT 5.2 or Claude Opus is akin to a bus or a dump truck in this analogy.  If a bicycle works for you, great!  Don't try to haul dirt in it...  lots of things you can't do with a bicycle, but it'll get you (and only you) to where you need to go without a lot of frills.  Don't get hit by a car on the way.",
          "score": 3,
          "created_utc": "2026-02-02 16:23:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36mo6q",
              "author": "Icy_Distribution_361",
              "text": "I'm aware... I'm not saying the cloud models aren't better in some metric. I'm saying I'm impressed by local models and how well they can cater to my needs.",
              "score": 1,
              "created_utc": "2026-02-02 16:57:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o36o9li",
                  "author": "ScuffedBalata",
                  "text": "Just be careful because the degree of hallucination is somewhat high.  But still, definitely has its utility.  In my analogy, a bicycle is still perfectly usable for many people on a daily basis.",
                  "score": 1,
                  "created_utc": "2026-02-02 17:04:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35sdcj",
          "author": "meva12",
          "text": "One thing you might be missing on switching over are tools.. like searching the internet, which there are ways to overcome that with anyrhingllm, Janai and others. But agreed, for simple stuff local is probably good enough for many.. right now I’m king a Gemini subscription because I have been playing around a lot with antigravity.  But I will probably cancel once I’m done and go the local way.. I just need to find a good app/inteeface to have on mobile to connect to my local llms from anywhere.",
          "score": 2,
          "created_utc": "2026-02-02 14:31:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bm158",
              "author": "Icy_Distribution_361",
              "text": "Like without internet access I wouldn't even consider a local model. But it was super easy to setup. Other tools I don't really use very much. Like OpenAI's Canvas, or Agent Mode. For \"Deep Research\" I've found great open source local alternatives.",
              "score": 1,
              "created_utc": "2026-02-03 11:01:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3bz8n3",
                  "author": "meva12",
                  "text": "So you are running it with a local llm? Where is the local llm hosted and what permissions are you giving it to do?",
                  "score": 1,
                  "created_utc": "2026-02-03 12:43:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o367xpr",
          "author": "asmkgb",
          "text": "BTW ollama is bad, use either llama.cpp or LMstudio as a second best backend",
          "score": 2,
          "created_utc": "2026-02-02 15:49:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bm7ku",
              "author": "Icy_Distribution_361",
              "text": "I've heard this said a lot, but it's not my experience. Combined with GPT-OSS:20b I think Ollama is great, and I like it has a desktop app instead of web page UI.",
              "score": 1,
              "created_utc": "2026-02-03 11:02:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ippx1",
                  "author": "huzbum",
                  "text": "For GPT OSS, there is no reason to switch. GPT OSS is pretty well optimized on Ollama, no need to tweak things.  \n\nIf you were pushing the limits of your hardware capabilities, or trying out a lot of new models, you’d probably want to tweak things more, and LM Studio (front end for llama.cpp) or llama.cpp would do you better.  \n\nI personally needed multiple context caches and always up, so I use llama.cpp with docker.  Works great!  I experiment with new models on LM Studio.  Every once in a while I have to use vllm or something.  Like with GLM 4.7 Flash, I think I need to build Llama.cpp from source to use flash attention and 8 bit kv cache to fit the context length I want.",
                  "score": 1,
                  "created_utc": "2026-02-04 12:33:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36bmzp",
          "author": "mpw-linux",
          "text": "I have been using MLX models as well on my macbook pro M1 32g machine.\n\nsome of the models I have tried are: models--mlx-community--LFM2-1.2B-8bit,  models--mlx-community--LFM2.5-1.2B-Thinking-8bit, models--mlx-community--Qwen3-0.6B-8bit, models--sentence-transformers--all-MiniLM-L6-v2,  models--Huffon--sentence-klue-roberta-base. \n\n I run them some small python scripts. Some these local models are quite impressive. I asked one the models to create a 3 chord modern country song, it build the song with chords and lyrics. \n\ncurrently downloading: models--argmaxinc--stable-diffusion for image creation from text.\n\nyou can run an MLX server then have a python client connect to the server so one can have the client on one machine and server on another to access local MLX llm's, this idea using the OpenAI api to connect from client to server.",
          "score": 1,
          "created_utc": "2026-02-02 16:06:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36gdqr",
              "author": "ScuffedBalata",
              "text": "0.6 and 1.2B models are brain-dead stupid compared to most modern LLMs.  They're going to hallucinate like crazy and confidently tell you the wrong thing or get stuck on all but the simplest problems.\n\nI find SOME utility from ~30b models, but they're still a shadow compared to the big cloud models.",
              "score": 2,
              "created_utc": "2026-02-02 16:28:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o36mrxr",
                  "author": "2BucChuck",
                  "text": "Agree,  I have been going smaller and smaller to see where agents fall apart and seems like ~30B was my experience - someone above said try oss 20b so going to give that a shot today.  I’d love to hear if anyone finds really functional agent models below that size.",
                  "score": 1,
                  "created_utc": "2026-02-02 16:57:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o379n08",
                  "author": "mpw-linux",
                  "text": "Just curious what are you expecting these models to do for you? Like what prompts are you giving the model?",
                  "score": 1,
                  "created_utc": "2026-02-02 18:41:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3acz36",
                  "author": "neuralnomad",
                  "text": "And asking a smaller model to do a well defined thing, it will outperform many commercial models that will often screw out up overthinking and wanting to outperform the prompt to its detriment. As for proper prompting, it goes both ways.",
                  "score": 1,
                  "created_utc": "2026-02-03 04:28:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o39t7ui",
          "author": "Aj_Networks",
          "text": "I’m seeing similar results on my M4 hardware. For general research, etymology, and \"how-to\" questions, local models like GPT-OSS:20b on Ollama are hitting the mark for me. It’s making a paid subscription feel unnecessary for non-complex tasks. Has anyone else found a specific \"complexity ceiling\" where they felt forced to go back to a paid service?",
          "score": 1,
          "created_utc": "2026-02-03 02:27:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ap9ss",
              "author": "Icy_Distribution_361",
              "text": "And it's even a question which kind of questions would constitute complex. I tried several mathematical questions for example which I myself didn't even understand and GPT-OSS:20b answered them the same as Mistral and GPT 5.2.",
              "score": 1,
              "created_utc": "2026-02-03 05:58:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o39tp20",
          "author": "DHFranklin",
          "text": "I haven't considered jumping off just yet as Jevon's Paradox keeps doing it's thing. The subscription services are mostly API keys for crazier and crazier shit.\n\nThat said I'm also changing up how I do hybrid models chaining together my phone, PC, and agent swarm. Using Claude Code for long horizon things but letting it do it in small pieces overnight is a godsend.\n\nWe are only just now able to do any of this.",
          "score": 1,
          "created_utc": "2026-02-03 02:30:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3apjmt",
              "author": "Icy_Distribution_361",
              "text": "What kind of long horizon tasks do you let it do over night? I can't really imagine anything that doesn't require regular checking as to not have a lot of wasted tokens.",
              "score": 1,
              "created_utc": "2026-02-03 06:00:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3e2fn2",
                  "author": "DHFranklin",
                  "text": "Mostly duplicating work that I've checked earlier. Testing and recompiling and things. Yes, there are tons of \"Wasted\" tokens but you gotta just build the waste in as a redundancy.",
                  "score": 1,
                  "created_utc": "2026-02-03 18:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3at80b",
          "author": "Mediocre_Law_4575",
          "text": "I need a better local coding model. there's nothing like Claude out there. Claude code has me SPOILED. I'm running mainly  flux 2, qwen 3.1 TTS. Dolphin Venice, personaplex, cogvideoX, and an image recognition & rag retrieval module- hitting around 95gigs of unified memory. Seriously considering clustering. Just the 4k outlay for another spark is ouch.\n\nI'm thinking about playing with clawdbot, (moltbot) but trying to do it all local. I have a minipc I could devote to it.",
          "score": 1,
          "created_utc": "2026-02-03 06:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3awb1s",
              "author": "Icy_Distribution_361",
              "text": "What kind of coding do you do?",
              "score": 1,
              "created_utc": "2026-02-03 06:58:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3azfp4",
                  "author": "Mediocre_Law_4575",
                  "text": "By trade always worked in web development w just old python scripts for backend, but lately more python. had my local qwen code model tell me tonight \"I have provided the html structure, you'll have to add your own scripting in at a later date\" lol WTF? lazy model trying to make ME work.",
                  "score": 1,
                  "created_utc": "2026-02-03 07:26:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bn97j",
          "author": "joelW777",
          "text": "Try qwen vl 30b a3b, it's much smarter than GPT-OSS 20B and handles images also. If you need more intelligence, try VL 32B, or if you don't need to process images, GLM 4.7 Flash. Those are the smartest models in that size as of today. Of course use MLX and at least q4. K/V-cache can be set to 8 bits for lots of VRAM savings.",
          "score": 1,
          "created_utc": "2026-02-03 11:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iikjh",
          "author": "BeginningPush9896",
          "text": "IMO, local LLM and local micro servers with AI will replace a big tech LLM, because company will use a LLM for one or two tasks, and it will be much cheeper buy server for 3-5k$ and run local LLM for free without a subscription on bigtech AI company.",
          "score": 1,
          "created_utc": "2026-02-04 11:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cld57",
          "author": "Acrobatic_Task_6573",
          "text": "I think the future is hybrid, not full replacement. Local for routine tasks, cloud for hard problems.\n\nI run a Mac mini with Ollama and Qwen 2.5 3B for probably 70% of my agent tasks. Simple queries, scheduled checks, log parsing, that kind of thing. Fast, free (after hardware cost), private.\n\nBut when I need deep reasoning or working with large codebases, I still reach for Claude or GPT-4. The quality difference is real, especially for complex problem solving.\n\nThe economics make sense for businesses. If you're running hundreds or thousands of simple queries per day, a $3k server that runs 24/7 beats paying per token. But if you need cutting edge reasoning, you're still going to hit the APIs.\n\nFor personal use, Ollama with a good Mac or a used workstation with an NVIDIA card is a great middle ground. You get privacy and reduced costs without completely sacrificing quality.\n\nThe subscription services aren't going anywhere. But local models definitely have a place in the stack.",
          "score": 1,
          "created_utc": "2026-02-09 00:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35gq0u",
          "author": "hhioh",
          "text": "Can you please talk a bit more about your technical context and experience setting up?\n\nAlso, how far does 24GB get you? Is the jump to 64GB value for money?\n\nFinally how long did it take you to set up and how do you connect into your system?",
          "score": 1,
          "created_utc": "2026-02-02 13:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o390559",
              "author": "Aggressive_Pea_2739",
              "text": "Bruh, just download lmatudio and then downloas gptoss20b on lmstuidp. You are DONE",
              "score": 2,
              "created_utc": "2026-02-02 23:45:04",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o35i1m9",
              "author": "Icy_Distribution_361",
              "text": "I've used several setups in the past but currently I'm just using Ollama with the desktop app on MacOS. I can't really say anything about more memory since I only have experience with this 24GB integrated memory on my Macbook. For me it's fine. Are there specific models you are curious about that you'd like to know the performance of? I could test if you want.\n\nIt took me very little time to setup. Like 10 minutes at worst.",
              "score": 1,
              "created_utc": "2026-02-02 13:35:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3iweqm",
              "author": "huzbum",
              "text": "OP is running GPT-OSS, which is 20b params and not very demanding.  Probably the best choice for 24GB Mac with unified memory.  \n\nPersonally, I have an M1 Mac with 16GB unified memory, and it’s not enough for the models I would like to run, especially not with the context size I would like to run them at.  On my Mac, I typically run Qwen3 4b instruct 2507.  A capable model, but not quite “do everything locally.”  On a unified memory mac, it has to share memory with everything else, and for me 16GB is already constrained.  I would recommend at least 32GB if you want to run better models on it and have a browser and other apps open.\n\nOn my desktop right now, I have an RTX 3090 and an RTX 3060.  I am working on upgrading the 3060 to another 3090, so apparently 24GB wasn’t enough, and that’s dedicated VRAM, not shared unified memory.  \n\nOn a single 24GB 3090, I can run qwen3 30b with 120k context, but it’s heavily quantized at Q4_K_XL, with 8 bit KV cache.  I would prefer Q6.  I get about 100+ tokens per second, which is great.  \n\nOn the 3090/3060 (36GB) combo with Q6, I get about 70 tokens per second, which is fast, but makes the child inside me cry.  70 tps really should be fast enough, that’s as fast as you typically get out of most cloud models.\n\nThe main driver for dual 3090s and 48GB is context length and higher quality quants.  I also want to train my own models locally.  I want to build a 3b param model from scratch.\n\nIf you already have a 24GB system, I would download LM Studio and try out GPT-OSS 20b, or Qwen3 30b 2507.  You’ll be up and running in like 20 minutes.\n\nIf you have something less capable, try LM Studio with qwen3 4b 2507, that will run on almost anything.",
              "score": 1,
              "created_utc": "2026-02-04 13:15:44",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o369zjh",
              "author": "coldy___",
              "text": "I'd say depends on your needs....what chip do you have on you.... and npu is a game changer",
              "score": 0,
              "created_utc": "2026-02-02 15:58:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o39acxn",
          "author": "HealthyCommunicat",
          "text": "When will it be basic knowledge that models like gpt 5.2 are well beyond 1 trillion parameters and that you will just literally never be able to have anything even slightly close even after spending $10k",
          "score": 0,
          "created_utc": "2026-02-03 00:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aosvz",
              "author": "Icy_Distribution_361",
              "text": "What are you saying? I think my point went entirely over your head focusing on the \"supremacy\" of GPT 5.2 and other models. An F1 car is also faster but since the roads here have speed limits I don't really care.",
              "score": 2,
              "created_utc": "2026-02-03 05:54:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3693ir",
          "author": "faltharis",
          "text": "What are best image models for 24gb ram?",
          "score": 0,
          "created_utc": "2026-02-02 15:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36g2a6",
              "author": "ScuffedBalata",
              "text": "What do you mean \"image models\".  What's the use case?",
              "score": 2,
              "created_utc": "2026-02-02 16:26:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o385ys9",
          "author": "Food4Lessy",
          "text": "The best value is Gemini for $100/yr for 2tb, for heavy ai dev workloads.  The 20b and 7b are llm are for super simple non-dev workloads, any 16gb laptop can run it .\nEven my phone runs 7b llm.\n\n\n\nM4 Pro 24gb is way overpriced unless you get the 48gb for $1600. The best bang for buck 64gb M1 max 900-1400, 32gb M1 Pro $700",
          "score": 0,
          "created_utc": "2026-02-02 21:12:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bmauc",
              "author": "Icy_Distribution_361",
              "text": "It's irrelevant whether the M4 Pro is overpriced, I already had it. I'm just saying local models run well for my use case. I'm not a coder.",
              "score": 1,
              "created_utc": "2026-02-03 11:03:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ewz33",
                  "author": "Food4Lessy",
                  "text": "Read my statement again as Gemini for $100/yr or ask oss 20B and 7B what I mean.\nAll three runs on most laptop and phone.\n\n\nThe development tool isn't just about coding , its about research, reports, analysis product, content, accelerating workflow like Notebook LM\n\n\n48-64GB gives you the ability to run multiple local model at same time to get more. Instead waiting several minutes for different to load.\n\n\nI personally run private cloud at 500 ts for pennies and 50 ts locally.",
                  "score": 0,
                  "created_utc": "2026-02-03 21:18:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3khvhk",
          "author": "GreaseMonkey888",
          "text": "If you are fine with local models, why did you got a subscription? Free tier should have worked then…",
          "score": 0,
          "created_utc": "2026-02-04 17:55:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ki9dp",
              "author": "Icy_Distribution_361",
              "text": "I've had it for two years. I just kept it active unaware that I (mostly) didn't need it.",
              "score": 1,
              "created_utc": "2026-02-04 17:57:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvktbl",
      "title": "Is running a local LLM for coding actually cheaper (and practical) vs Cursor / Copilot / JetBrains AI?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qvktbl/is_running_a_local_llm_for_coding_actually/",
      "author": "vMawk",
      "created_utc": "2026-02-04 10:14:05",
      "score": 74,
      "num_comments": 90,
      "upvote_ratio": 0.95,
      "text": "Hey all,\n\nI work a lot with Cursor, VS Code and JetBrains IDEs, and I currently have multiple Pro subscriptions (Cursor, ChatGPT, etc.).\n\nThe problem:  \nI still hit token limits fairly often. When that happens I notice I subconsciously start “using the AI more carefully”, shorter prompts, fewer refactors, less exploration. It kind of kills the flow.\n\nSo I started wondering:\n\nWould it actually be cheaper (or at least more comfortable) to just build a beefy local machine, run a local LLM, and hook that directly into my IDE?\n\nIn theory that sounds very appealing:\n\n* One-time hardware cost\n* No token anxiety\n* Unlimited usage\n* Fully private codebase\n* IDE → LLM loop as tight as possible\n\nBut I’m unsure how realistic this is *today*.\n\nSome concrete questions:\n\n* Does this actually work well in practice for real coding tasks (refactoring, understanding large codebases, generating tests, etc.)?\n* Which local models are “not too dumb” for serious dev work? (Code Llama? DeepSeek-Coder? Qwen-Coder? Mixtral variants?)\n* What kind of hardware are we realistically talking about? (Single high-end GPU? Dual GPU? 64–128GB RAM?)\n* How painful is the IDE integration compared to Cursor / Copilot? (Latency, context handling, indexing, etc.)\n* In the long run: is this actually cheaper, or just a fun rabbit hole?\n\nI don’t expect local models to beat GPT-4o / Claude at everything, but if I could get to “good enough” with zero limits and full privacy, that feels like a big win.\n\nCurious to hear from people who’ve actually tried this setup in daily work.  \nWorth it, or not there yet?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvktbl/is_running_a_local_llm_for_coding_actually/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3ib3zd",
          "author": "TheAussieWatchGuy",
          "text": "Currently yes sort of.. Qwen Coder and GLM 4.7 are very good and be run on somewhat affordable consumer machines. Roughly comparable results to Claude Sonnet.\n\n\nIf I was a betting man looking at say the recent Kimi 2.5 requiring ungodly VRAM, like 96gb+ then it won't stay affordable for long.\n\n\nThe state of the art will move fast and the cloud models will be cheaper and better my miles.",
          "score": 33,
          "created_utc": "2026-02-04 10:37:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k3h29",
              "author": "FaceDeer",
              "text": "What I'm ultimately hoping for is a situation where my coding agent is using a local LLM for much of its work, and occasionally outsources \"hard stuff\" to bigger non-local models when it finds it needs to. Feels like that'll be a true best-of-all-worlds scenario.",
              "score": 9,
              "created_utc": "2026-02-04 16:49:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3litad",
                  "author": "GlassAd7618",
                  "text": "Yes, absolutely! IMHO that's the most pragmatic and most likely scenario. Technically, the only question is how to decide if something is \"hard stuff\". Actually, this is also an interesting question from the computer science perspective. I don't have an answer, but several naive alternatives come to mind:  \n\\- You could replicated the code (just make multiple copies on the disk) and run Ralph loop for n iterations. If none of the replicas can pass the test (assuming you would have specified/generated one for a certain feature), you could decide that this is indeed \"hard stuff\"  \n\\- You could use the local model itself (or another local model as a judge), give it the code base and the prompt/task, and ask it to decide whether it's simple enough to solve it locally or whether a bigger, non-local model should be invoked  \n\\- You could use a heuristic. For example, you could create some kind of a \"balanced score card\" with different categories such as \"the number of files that would need to be changed to complete the task\", \"the number of functions that would need to be changed\", \"total cyclomatic complexity of these functions or files\", etc. Based on the values in the individual categories of that \"balanced score card\" (or a lumped sum) you could decide whether the change is \"hard stuff\" or not",
                  "score": 6,
                  "created_utc": "2026-02-04 20:47:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4dz2sr",
                  "author": "Sourav_Anand",
                  "text": "IMO you should try to write and save different sets of prompts. Let's take a very simple example. You are trying to replicate a code block in another class but having different objects in place of original one. \n\nLet's break the steps. If you ask an agent for this\n\nStep 1 - look for reference of objects in original code block and then find their declaration.\n\nStep 2 - look for required reference in your new class and find the dependencies.\n\nStep 3 - Send the referenced code to LLM and get back your expected code block.\n\nNow I think most of the tokens are consumed during step 1 and 2. While that can be done with help of your local LLM as well. So you can use it for reading code finding actual references then generate a proper prompt for actual GPT 5 or Gemini or whatever you prefer.\n\nJust like these you can prepare multiple prompts for you to use them as needed.\n\nNow just to be clear this is my opinion and there might be other options as well. Let us know whatever you settle with.",
                  "score": 1,
                  "created_utc": "2026-02-09 05:40:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3iilxf",
              "author": "StaysAwakeAllWeek",
              "text": ">If I was a betting man looking at say the recent Kimi 2.5 requiring ungodly VRAM, like 96gb+ then it won't stay affordable for long.\n\n\nWe've had huge unattainable open source models ever since the original deepseek 671b. Kimi is actually relatively easy to run in comparison\n\nEdit: even earlier actually, llama 3 took hundreds of GB to load in 2024",
              "score": 8,
              "created_utc": "2026-02-04 11:41:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lokvo",
              "author": "Logiteck77",
              "text": "How much VRAM do Qwen Coder and GLM take?",
              "score": 1,
              "created_utc": "2026-02-04 21:14:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o9q6z",
                  "author": "SKirby00",
                  "text": "That's not as easy to answer as you'd think. Assuming he's referring to `Qwen3-Coder-30B-A3B-Instruct` and `GLM-4.7-Flash`, the models themselves take about ~18-19GB of VRAM (assuming 4-bit quantization), but that's before you factor in room for context. Context windows is where memory gets tricky.\n\nI have 24GB of VRAM (5060Ti 16GB + 3060Ti 8GB) and have been able to get about ~20K tokens of context on each of these in LM Studio (basically easy mode). According to Gemini, it estimates ~40GB of VRAM to fit 128k context but that might assume some (not necessarily easy) optimizations. Worth noting that GLM's context is a bit more memory-efficient than Qwen, but also it's a thinking model and Qwen Coder isn't so Qwen is more token-efficient.\n\nNot much can be done about the model size itself without sacrificing quality, but there are some optimizations that can help you squeeze in a bit more context if you're willing to stray away from the \"easy mode\" setup.",
                  "score": 2,
                  "created_utc": "2026-02-05 06:19:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mhvrh",
              "author": "ForsookComparison",
              "text": "> Roughly comparable results to Claude Sonnet.\n\nI get that these models feel different between different people but I still feel confident in saying that this isn't true anywhere but a bar chart.",
              "score": 1,
              "created_utc": "2026-02-04 23:43:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3jsy1f",
              "author": "saltyghoul",
              "text": "Are Qwen Coder and GLM 4.7 14B or 32B?",
              "score": 0,
              "created_utc": "2026-02-04 16:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o2s77",
                  "author": "SKirby00",
                  "text": "I'm gonna go out on a limb and assume that he's referring to `Qwen3-Coder-30B-A3B-Instruct` (30B) and `GLM-4.7-Flash` (31B — I think).\n\nThat being said, there's a small chance that he's referring to `Qwen3-Coder-Next` which just came out like 2 days ago and is 80B.",
                  "score": 3,
                  "created_utc": "2026-02-05 05:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i8rbh",
          "author": "No_Clock2390",
          "text": "My 128GB RAM AMD AI MAX machine seems to be just as good as Gemini at writing code. It's great being able to get my computer to write code for me. Until it takes my job.",
          "score": 17,
          "created_utc": "2026-02-04 10:16:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iml08",
              "author": "ZenEngineer",
              "text": "What model so you run",
              "score": 5,
              "created_utc": "2026-02-04 12:11:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3irc2a",
                  "author": "No_Clock2390",
                  "text": "GPT-OSS-120B\n\nQwen3-Coder-Next",
                  "score": 10,
                  "created_utc": "2026-02-04 12:44:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3i9vhy",
              "author": "Elegant_Jellyfish_96",
              "text": "how much did the entire setup cost if I may ask?",
              "score": 1,
              "created_utc": "2026-02-04 10:26:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3i9z7v",
                  "author": "No_Clock2390",
                  "text": "I bought it for $2200 but now it's almost $3000. Minisforum MS-S1 Max.",
                  "score": 7,
                  "created_utc": "2026-02-04 10:27:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3isq5g",
                  "author": "Proper_Taste_6778",
                  "text": "2k usd bosgame m5",
                  "score": 2,
                  "created_utc": "2026-02-04 12:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jf5ed",
              "author": "tomByrer",
              "text": "Did you buy that machine ONLY to run AI coding assistants?  \nOr were you like, \"I need a new laptop anyway, so might as well spend an extra $1000 to make it run AI.\"?\n\nI made my desktop machine partly to run AI, partly to run games.",
              "score": 1,
              "created_utc": "2026-02-04 14:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jg60e",
                  "author": "No_Clock2390",
                  "text": "That's part of it. I just liked the overall package you get with the MS-S1 Max. It has great I/O. USB4v2 80Gbps (Thunderbolt 5 equivalent). It's like a Mac Studio but way less expensive for what you get. You can carry it in a backpack like a laptop but it's way more powerful than a laptop. You can game on it at up to 4K60FPS with frame gen on. But I also have a separate gaming computer.",
                  "score": 3,
                  "created_utc": "2026-02-04 15:00:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3nhbjc",
              "author": "Chance-Map-6039",
              "text": "How are you running these models? I have the gmktec evo x2 128 gb amd ai max 395. I’ve been learning on the go and I don’t have a background in coding or anything but I’m curious what your workflow is or if you have any preferred UI. Thanks.",
              "score": 1,
              "created_utc": "2026-02-05 03:03:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nhy5d",
                  "author": "No_Clock2390",
                  "text": "I use LM Studio in Windows 11. Be sure to set the VRAM to 96GB in the AMD Adrenalin app so you can load the larger models. With 96GB VRAM, you still have 32GB left over to serve as the system RAM. In LM Studio, look for the GPU Offload option and enable it if it's not already enabled. Also, make sure your Windows 11 is up-to-date.",
                  "score": 3,
                  "created_utc": "2026-02-05 03:07:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ihss2",
          "author": "Big_River_",
          "text": "you can do much better with a hybrid approach - local plus cloud - use case makes all the difference - for coding with a large established repo you are not optimizing or rewriting - local is fantastic with fine tuned GLM 4.7 in my experience - imho if you were to spend ~20k you would outperform with fine tuned models to your specific use case - but then that is a great deal of tokens / but then you own your own dependencies which is where I landed due to all the garbage factors of uncertain geopolitical/economic climate",
          "score": 5,
          "created_utc": "2026-02-04 11:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j20ax",
          "author": "Look_0ver_There",
          "text": "Just my 2c.  Privacy wise yes.  Budget wise, it depends on how much you're spending monthly on your subscriptions and what your expected time period for return on investment is.  Local models have gotten pretty good lately with a handful certainly being capable enough to assist with the development of most anything.  Newer models get released fairly frequently that keep on raising the bar.\n\nIt's a difficult question to answer though, as I'm sure you know, one model may suck at one task while another is hitting hole runs on the same task.  The advantage of a local machine is that you can quickly flip between models without needing to have a half dozen subscriptions.\n\nSo, privacy wise, always worth it. Budget wise, it depends. Access to the best and latest models wise, you'll have to accept that you're always maybe six months behind the capabilities of the online models (very rough hand wavy guess), but this can be mitigated somewhat with model flexibility. Having said that though, some of the latest local models are surprisingly good.",
          "score": 5,
          "created_utc": "2026-02-04 13:47:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3igcru",
          "author": "tillybowman",
          "text": "i'd say no. \n\nwhile there are open weight models that come close to opus 4.5, running those models without quants needs a machine above 3k. \n\nyou can do your math now. it would take years for me to spent that in tokens online. and when i have, the tech will have changed anyway.\n\nthis answer is tailored to coding where you need high context, max tokens, fast tps",
          "score": 4,
          "created_utc": "2026-02-04 11:23:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ii7aw",
              "author": "Careful_Breath_1108",
              "text": "Did you mean 3k or 30k? What type of set up did you mean for 3k",
              "score": 5,
              "created_utc": "2026-02-04 11:38:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iuzca",
          "author": "Silentparty1999",
          "text": "No.  The primary argument for local LLM, IMO, is privacy.\n\nLLM as a service is continually upgraded and available at scale on-demand.",
          "score": 3,
          "created_utc": "2026-02-04 13:07:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3inodn",
          "author": "0Bitz",
          "text": "Do it for the privacy",
          "score": 3,
          "created_utc": "2026-02-04 12:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0vbr",
          "author": "2020jones",
          "text": "Opus 4.5, Gemini 3, and Gpt 5.2 only worked well at launch; afterwards, they reduced the quality. So if a local model has 50% performance in problem situations, it actually has 50%. The results of these standard AIs were achieved with them at maximum performance; what you buy today is garbage.",
          "score": 3,
          "created_utc": "2026-02-04 16:37:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kf2kv",
              "author": "killerkongfu",
              "text": "Really?? I thought it stayed the same?",
              "score": 1,
              "created_utc": "2026-02-04 17:43:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3snlcw",
                  "author": "2020jones",
                  "text": "Não mesmo! Veja o Opus 4.6 lançado hoje, experimente ele daqui a 5 meses e vai ver muitos erros. Eu sei disso pois eu uso para as MESMAS tarefas e o desempenho cai bastante. Gemini é de longe o pior de todos nesse sentido por que ele sustenta seus erros.",
                  "score": 1,
                  "created_utc": "2026-02-05 22:03:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3iudbe",
          "author": "techlatest_net",
          "text": "Yes, practical now. Qwen2.5-Coder 7B crushes benchmarks (88% HumanEval, beats GPT-4 in spots) on single RTX 4090 or A100. Use Ollama + Continue.dev – latency ~1-2s, context solid up to 128k. Subs add up to $30+/mo, hardware pays off in 6-12 mos.",
          "score": 2,
          "created_utc": "2026-02-04 13:03:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jfsh9",
              "author": "tomByrer",
              "text": "Might be longer with RAM & SSD prices going up....",
              "score": 1,
              "created_utc": "2026-02-04 14:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3iyccb",
          "author": "MehImages",
          "text": "depends on whether you have to buy the hardware for this specific reason and how much you benefit from no token limits vs speed.",
          "score": 2,
          "created_utc": "2026-02-04 13:26:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j3yle",
          "author": "bakawolf123",
          "text": "gpt-oss models are pretty good, setup wise you can run them with llama.cpp or ollama and use in a lot of agents (like Claude code, Copilot).\n\nI'm on 32GB unified RAM so for me it's not there yet (aside from mentioned gpt-oss in 20B I can only use quantized 30B models), but I think with 128GB VRAM or unified RAM machine you can feel pretty good as there're some existing ones (gpt-oss 120B, full GLM4.7 Flash, ful Qwen3 30B Coder, fp8 Qwen3 Next 80B Coder) and more decent model options keep popping up, inference engines keep getting optimizations.\n\nOne additional bonus of being local is having full control of the whole processing stack (model, inference engine, agent harness), so there might be more undiscovered integrations between those levels which could better exploit that fact.",
          "score": 2,
          "created_utc": "2026-02-04 13:57:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jubjz",
          "author": "andrew-ooo",
          "text": "The \"token anxiety\" you describe is real and honestly one of the best reasons to consider local. That mental friction of rationing API calls genuinely hurts productivity.\n\nFor practical coding work today: Qwen3-Coder (the new one just dropped) and DeepSeek-Coder-V2 are legitimately usable for real tasks - refactoring, test generation, explaining code. They're not Claude/GPT-4 level but they're \"good enough\" for 80% of daily coding tasks.\n\nHardware math that worked for me: A used 3090 (24GB VRAM, ~$700-800) can run 32B models quantized at reasonable speed. For pure coding assistance you don't need 405B params - a well-tuned 32B coder model handles most things.\n\nThe hybrid approach mentioned above is honestly the sweet spot: local for unlimited \"thinking out loud\" iterations and exploration, cloud API for the complex stuff that needs frontier model capability. Kills the anxiety without the $3k+ upfront.",
          "score": 2,
          "created_utc": "2026-02-04 16:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k0g2w",
          "author": "unity100",
          "text": "Deepseek paid API is extremely cheap.",
          "score": 2,
          "created_utc": "2026-02-04 16:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pqq75",
          "author": "Euphoric_Emotion5397",
          "text": "I'm using Anti-gravity, and it's definitely cheaper and faster than anything i can run on my own machine. (a rtx 5080 16gb plus a rtx 5060TI 16gb  with 64gb ddr5).\n\nI'm using that for coding the apps . But I am using the local machine for inference for the app purpose. This type of use case will be more than adequate for the gpt-oss 20b or qwen 3 VL 30b models.\n\nso you actually can saved a lot versus sending even your local app data for online inference. Tokens are \"foc\" in local LLM. I can test and retest and scrape and rescrape daata without concern for cost..  Not too sure abt the electricity bill though. lol",
          "score": 2,
          "created_utc": "2026-02-05 13:40:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vsc6u",
          "author": "singh_taranjeet",
          "text": "Qwen2.5-Coder 32B and DeepSeek-Coder-V2 are solid for real dev work. I've been running them locally and they handle refactoring and test generation surprisingly well. The token anxiety disappearing alone may already make the hardware investment worth it if you code daily",
          "score": 2,
          "created_utc": "2026-02-06 11:09:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vui0v",
              "author": "Sandzaun",
              "text": "I'm still struggling to find a plugin for PyCharm that gives me autocomplete with an local LLM. This is all I need. No logins or accounts, just plain and simple autocomplete with my local model running via koboldcpp. Any suggestions?",
              "score": 1,
              "created_utc": "2026-02-06 11:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3icbbb",
          "author": "Logisar",
          "text": "Which local LLM setup has the quality of Claude Sonnet 4.5? That doesn’t exist.",
          "score": 2,
          "created_utc": "2026-02-04 10:48:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jhu0n",
              "author": "huzbum",
              "text": "Uh, I would argue that GLM 4.7 is equivalent to Sonnet.  I've heard good things about MiniMax M2.1, Kimi K2.5, and Qwen 3.5 is just around the corner.  \n\nThe hardware to run these large models is expensive, but it CAN be done locally.  \n\nI doubt they are equivalent to Sonnet, (maybe Haiku?) but I look forward to taking some time to try GLM 4.7 Flash and Qwen3 Coder Next.  I've been using Qwen3 30b Coder for some stuff for a while, but rely on GLM via [z.ai](http://z.ai) cloud subscription for my main workhorse.  I don't have the equipment, but it's feasible to run it locally.",
              "score": 3,
              "created_utc": "2026-02-04 15:09:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kfvq8",
                  "author": "ihackportals",
                  "text": "I'm running Qwen3-Coder-Next on a GB10 with Claude and I would say it's comparable.",
                  "score": 4,
                  "created_utc": "2026-02-04 17:46:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jizzo",
          "author": "Vittorio792",
          "text": "This is an interesting question. Running a local LLM could potentially be cheaper than relying on cloud-based AI services, especially if you have the hardware and expertise to set it up. The tradeoff is the additional overhead and maintenance required. Ultimately, it depends on your specific needs, usage patterns, and the performance requirements of your codebase. I'd suggest doing some cost analysis and benchmarking to see which option makes the most sense for your workflow.",
          "score": 1,
          "created_utc": "2026-02-04 15:14:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kf95g",
          "author": "beryugyo619",
          "text": "**NO**. Will it stay that way over the next 3, 5, 10 years? Everyone fears it won't stay that way, no it won't. Is it likely rational to be scared like that? Also no lol but we tend to be lol",
          "score": 1,
          "created_utc": "2026-02-04 17:44:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l1x72",
          "author": "lan-devo",
          "text": "Depends for light work, just having a 10-20 dollar sub for  more complex tasks and a local llm for simple stuff is much better, now for the simple stuff I do as a hobby-semipro is enough. One thing people undervalue is with local LLM I can go step by step, with API is a waster of money and tokens because you want to do it but you don't because it will cost you.",
          "score": 1,
          "created_utc": "2026-02-04 19:26:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3leeus",
          "author": "squachek",
          "text": "No",
          "score": 1,
          "created_utc": "2026-02-04 20:26:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mtbs1",
          "author": "No-Consequence-1779",
          "text": "I use copilot mostly for work. I code normally so it’s usually method by method, through the stack. \n\nFor large tasks like orm mapping 150 properties or similar tedious tasks, I use a local qwen3-coder-30b.   \n\nLocal can handle more for that situation.   \n\nFor vibe coding type stuff, I read it’s possible but I have never found a real example of a prd or whatever that gets done by agents.  My opinion is it’s highly complex and most is just marketing or people thinking they are making complex things. \n\nCustom business rules or complex workflows will probably be faster to just do, than iteratively describe until it gets close enough. \n\nThat is also the question: is it faster to just do and know it’s complete or faster to write it in detail to instruct the LLM?  \n\nIf anyone wants to msg me a real prd that actually works once, please do so. I would be a convert if it’s true. ",
          "score": 1,
          "created_utc": "2026-02-05 00:46:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n43th",
          "author": "mpw-linux",
          "text": "What type of coding are you doing? Do you know programming well to know if the model is accurate ? If need so help with code i just use Google gemini which gives pretty good examples. Look into Liquid AI models for local llm's",
          "score": 1,
          "created_utc": "2026-02-05 01:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ohdm0",
          "author": "Proof_Scene_9281",
          "text": "Claude code in the ‘best’ plan is gonna be hard to beat. \n\nI’m thinking local bots will be fun for making home automation stuff without ads ",
          "score": 1,
          "created_utc": "2026-02-05 07:26:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vp3c9",
          "author": "natanloterio",
          "text": "What's your local LLM Setup? Do you use any IDE plugin or a generic agentic  \"LLM CLI\" ? This is my current bottle neck. I have tried some \"Claude CLI Clone\" open source projects but they are usually broken or full of bugs. ",
          "score": 1,
          "created_utc": "2026-02-06 10:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icbfd",
          "author": "former_farmer",
          "text": "You can use composer 1 for free once you run out of Cursor tokens. I do.",
          "score": 1,
          "created_utc": "2026-02-04 10:48:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icu80",
          "author": "Taserface_ow",
          "text": "Don’t forget to factor in power costs. And the fact that you’ll need to replace your hardware frequently when you’ve worn it out from running at 100% all the time.",
          "score": -3,
          "created_utc": "2026-02-04 10:53:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jfksw",
              "author": "tomByrer",
              "text": "Depends on where you live.  \nSome places still have cheaper electricity.  \nIn cold climates, a hot computer can be a bonus! ;)",
              "score": 2,
              "created_utc": "2026-02-04 14:57:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3l3uyu",
              "author": "GCoderDCoder",
              "text": "Cloud providers run all the time and only are saying they're replacing every 5 years or more. There are freak issues but besides x3d CPUs on ASRock boards I dont personally know anybody whose CPU or GPU died on them without them doing something weird. Hard drives are a different story... I'm actually trying to convince a certain friend to upgrade from a GTX1080 lol",
              "score": 1,
              "created_utc": "2026-02-04 19:35:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mo1n6",
                  "author": "Taserface_ow",
                  "text": "It’s the constant higj load on the machines that wear them out quickly, not that they’re running all the time.\n\nwhen i run a local llm my gpu is flat out at 100%. i already have an implementation plan prepped for my projects so i’m just constantly feeding my llm input as soon as it finishes work. \n\ncloud servers distribute the load and they’re not necessary running compute intensive processes all the time.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:17:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mj3ul",
              "author": "Internal_Werewolf_48",
              "text": "Power costs can be negligible for Macs and the Strix Halo. \n\nOr they could be crazy if you're running 6x4090's on server hardware.",
              "score": 1,
              "created_utc": "2026-02-04 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3t06ch",
                  "author": "Total_Engineering_51",
                  "text": "Even a big server can be managed if you’re the only one using it… 100% up time at full tilt is nonsense unless the only thing you’re doing is endless model training or hosting a service. Sleep and Wake on LAN go a long way to manage a power hungry system if it’s just you using it most/all of the time.",
                  "score": 1,
                  "created_utc": "2026-02-05 23:08:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxrvq9",
      "title": "My $250 24gb of VRAM setup (still in 2026)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qxrvq9/my_250_24gb_of_vram_setup_still_in_2026/",
      "author": "Jack_at_BrewLedger",
      "created_utc": "2026-02-06 19:49:08",
      "score": 74,
      "num_comments": 48,
      "upvote_ratio": 0.95,
      "text": "What I'm running is a nvidia Tesla p40, a server compute accelerator card from 2016 which just so happens to have 24 gigs of VRAM on the highest end version. They can be found on ebay for about $250 bucks right now.\n\nThe card is passively cooled and designed for a server rack, so I made a custom cooling shroud to force air into the back and through it like it would work in a server race. On the back is a PWM high pressure fan, controlled by my motherboard, and the speed is directly bound to the tesla's temperature through nvidia-smi and FanControl on Windows.\n\nBought a big ass PC case, cut a big chunk out of the back. Got myself an 8 pin server card adapter to dual 6 pin GPU power outputs from a PSU, and got myself a nice big ass PSU. Fired the whole thing up as a Frankenstein design.\n\nI wouldn't call it fast by any means, but in 4bit quant I can fit gpt-oss 20b in there with 32k+ context length, all on the GPU. The speeds are fast enough to be used as a local chatbot, so works well as my AI assistant. Also, works with CUDA 12 if you pick the right driver.\n\n  \nOh, I forgot to mention, this thing has no video output, as it's a server accelerator card. I have a Ryzen 5700G as my processor, with integrated graphics. The Tesla is driver hacked into registering as a nVidia Quadro in workstation mode, and so I can run games on the Tesla using the windows settings for high performance graphics (meant to be used on gaming laptops with GPUs) and it gets relayed through my integrated GPU. The actual die on this card is a clone of the 1080ti, so I get 1080ti performance gaming too, just with 24 gigs of VRAM, and it'll run anything as long as I put the game's exe in a list. I'm most proud of that part of the setup.\n\n[The TESLA running in my rig](https://preview.redd.it/xodlgmmphxhg1.png?width=4624&format=png&auto=webp&s=7adaaf3f9eaed0dee176343d18e12a534af33bd8)\n\n[Underside View](https://preview.redd.it/u2ftaa3shxhg1.png?width=4624&format=png&auto=webp&s=642d7b83a028dd6b268a16c6a6241f2dcd4feb05)\n\n[closer look at the cooling solution and power adapter](https://preview.redd.it/up5colythxhg1.png?width=3472&format=png&auto=webp&s=0d15f0ddcf217b939ca2f806dad6d97043afd36e)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxrvq9/my_250_24gb_of_vram_setup_still_in_2026/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3yjnbi",
          "author": "Used_Chipmunk1512",
          "text": "Kudos to you, seriously this looks great, do post more of your adventures here",
          "score": 15,
          "created_utc": "2026-02-06 19:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxga0",
              "author": "Jack_at_BrewLedger",
              "text": "thanks man :)",
              "score": 1,
              "created_utc": "2026-02-06 21:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yni06",
          "author": "According_Study_162",
          "text": "How many tps?",
          "score": 8,
          "created_utc": "2026-02-06 20:15:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yoy6u",
              "author": "Jack_at_BrewLedger",
              "text": "30ish",
              "score": 5,
              "created_utc": "2026-02-06 20:22:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zi62r",
                  "author": "Themash360",
                  "text": "At 15GB MoE I'd be expecting a lot more. How much context is that?",
                  "score": 2,
                  "created_utc": "2026-02-06 22:50:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o41lajy",
                  "author": "MrScotchyScotch",
                  "text": "that is wild... i have a thinkpad from 2023 and i get 30 t/s with qwen3 coder (q4_k_m). the newer chips really make a difference!",
                  "score": 1,
                  "created_utc": "2026-02-07 07:11:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3zi4o4",
                  "author": "ClimateBoss",
                  "text": "bruh can it run vLLM ?",
                  "score": 1,
                  "created_utc": "2026-02-06 22:50:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zi3tv",
              "author": "Themash360",
              "text": "https://www.techpowerup.com/gpu-specs/tesla-p40.c2878\n\n350GB/s vram, so uh at 15GB MoE I'd be expecting a lot more.",
              "score": 1,
              "created_utc": "2026-02-06 22:50:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zj3k7",
                  "author": "Jack_at_BrewLedger",
                  "text": "It's the pascal architecture that's the limitation, not the ram speed",
                  "score": 4,
                  "created_utc": "2026-02-06 22:55:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ynel2",
          "author": "TMuel1123",
          "text": "Small Tipp. Try a radial fan with an adapter shroud. They can produce more pressure and higher airflow. ",
          "score": 4,
          "created_utc": "2026-02-06 20:14:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ypbvg",
              "author": "Jack_at_BrewLedger",
              "text": "heard. I considered that originally, but this was cheaper to make. That's definitely better, but this with the high pressure fan will keep the thing under 70C at full load constantly, which is all I need,",
              "score": 1,
              "created_utc": "2026-02-06 20:24:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z272p",
          "author": "Klutzy_Ad_1157",
          "text": "Good old P40 never disappoints for LLM :)",
          "score": 3,
          "created_utc": "2026-02-06 21:28:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ykz2z",
          "author": "Purrsonifiedfip",
          "text": "Question because I'm new to the hardware stuff. Just built my first pc. Put a 5070ti in it and finding out its enough to chat, but if I want heavy tasks, I'll need more. Was looking at adding an older 3060 or something of the sort.\n\nDo older gpus need aggressive cooling for LLMs? or would high powered fans on the psu shroud be adequate?",
          "score": 2,
          "created_utc": "2026-02-06 20:02:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yp57n",
              "author": "Jack_at_BrewLedger",
              "text": "The fans built into the GPUs should work fine, I only have this set up because the Tesla has 0 cooling setup on it. It was designed to be used in a data center with massive fans so it just has a heatsink, you have to design a solution.",
              "score": 2,
              "created_utc": "2026-02-06 20:23:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ypgwo",
                  "author": "Purrsonifiedfip",
                  "text": "ahhh, thanks",
                  "score": 1,
                  "created_utc": "2026-02-06 20:25:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yska7",
          "author": "Decent_Solution5000",
          "text": "Come to my house and make us one. We'll feed you and everything! Seriously, that thing looks like it seriously rocks! Congrats on your genius innovation!",
          "score": 2,
          "created_utc": "2026-02-06 20:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxfk7",
              "author": "Jack_at_BrewLedger",
              "text": "made me smile",
              "score": 1,
              "created_utc": "2026-02-06 21:05:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zmxqt",
                  "author": "Decent_Solution5000",
                  "text": "Mad me a little jelly, ngl, but also made me happy for you. :)",
                  "score": 2,
                  "created_utc": "2026-02-06 23:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yuxac",
          "author": "UnlikelyPotato",
          "text": "I believe P100s are faster with their HBM2 bandwidth. $80 for 16GB cards. 32GB for $160.",
          "score": 2,
          "created_utc": "2026-02-06 20:52:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yxd99",
              "author": "Jack_at_BrewLedger",
              "text": "im pretty sure it doesnt come in 32, maybe you're thinking of something else?\n\n",
              "score": 2,
              "created_utc": "2026-02-06 21:04:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yyjg5",
                  "author": "UnlikelyPotato",
                  "text": "Sorry. Two for $80 each is $160. P100s have twice the memory bandwidth of the P40. If you can fit multiple GPUs in your system then P100s are hands down better.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:10:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3zhe2v",
          "author": "No-Leopard7644",
          "text": "Kudos to innovation, great idea and execution",
          "score": 2,
          "created_utc": "2026-02-06 22:46:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ztvmy",
          "author": "apVoyocpt",
          "text": "I have the p40 too. Under linux (with enough ram) the oss 120b ran surprisingly well!",
          "score": 2,
          "created_utc": "2026-02-06 23:57:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zzthc",
              "author": "Jack_at_BrewLedger",
              "text": "do you mean 20b? or did you get the 120b to run? Iv'e got 48gb of ram to work with\n\n",
              "score": 1,
              "created_utc": "2026-02-07 00:31:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o41vdf8",
                  "author": "apVoyocpt",
                  "text": "the 120b. I have 64GB of ddr4 and the p40. It worked really well!",
                  "score": 1,
                  "created_utc": "2026-02-07 08:47:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o419h35",
          "author": "hw999",
          "text": "Put a brace under than card or the fan, gravity is not your friend.",
          "score": 2,
          "created_utc": "2026-02-07 05:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45qs1y",
              "author": "MyTwitterID",
              "text": "This.",
              "score": 1,
              "created_utc": "2026-02-07 22:50:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o46ytot",
          "author": "singlebit",
          "text": "very nice! why dont I think the same!",
          "score": 2,
          "created_utc": "2026-02-08 03:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o479x4i",
          "author": "Fit-Rub3325",
          "text": "I am ashamed to call myself as engineer after reading things here on reddit :) ",
          "score": 2,
          "created_utc": "2026-02-08 04:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43a6ra",
          "author": "milkipedia",
          "text": "this is worthy of r/homelab",
          "score": 1,
          "created_utc": "2026-02-07 15:14:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43d4wq",
          "author": "AlexGSquadron",
          "text": "How does this compare to Intel b60 pro?",
          "score": 1,
          "created_utc": "2026-02-07 15:29:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48nvtp",
          "author": "LaunchAgentHQ",
          "text": "This is the kind of setup that makes local inference practical. The P40s are a steal for the VRAM you get - my only suggestion would be to look into vLLM or llama.cpp with tensor parallelism to actually use both cards together. Single card 24GB is great for 13B-30B models quantized, but if you can get both cards working in tandem you could potentially run 70B models at decent quality. The passive cooling is going to be your main challenge in a standard case though - those cards were designed for server airflow.",
          "score": 1,
          "created_utc": "2026-02-08 12:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dnxgm",
              "author": "Jack_at_BrewLedger",
              "text": "This setup works fine, but id get a sideways blower style fan next time.",
              "score": 1,
              "created_utc": "2026-02-09 04:19:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qw0ofn",
      "title": "Help me find the biggest and best model!",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/7oahin01kjhg1.jpeg",
      "author": "NHooked",
      "created_utc": "2026-02-04 20:55:51",
      "score": 68,
      "num_comments": 21,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qw0ofn/help_me_find_the_biggest_and_best_model/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lzc6n",
          "author": "Historical-Internal3",
          "text": "Monitor the GB10 Nvidia Dev forums, also study this repo:\n\n[https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker)\n\nYou'll get the most out of your spark that way. \n\nTake a peep at the networking guide.",
          "score": 21,
          "created_utc": "2026-02-04 22:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nxi1l",
              "author": "ftwEsk",
              "text": "Thanks for this,getting my QSFP56 cables tomorrow, will test this!",
              "score": 1,
              "created_utc": "2026-02-05 04:46:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3mgxwh",
              "author": "ifheartsweregold",
              "text": "This is the way. ",
              "score": 1,
              "created_utc": "2026-02-04 23:38:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3lor7h",
          "author": "alin_im",
          "text": "kimi k2.5",
          "score": 7,
          "created_utc": "2026-02-04 21:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lps68",
              "author": "NHooked",
              "text": "On the Ollama site Kimi k2.5 shows as a cloud model, I do see other models mentioning the name. Could you show me which one I need?",
              "score": 2,
              "created_utc": "2026-02-04 21:20:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lqs34",
                  "author": "rslarson147",
                  "text": "Use vllm",
                  "score": 4,
                  "created_utc": "2026-02-04 21:25:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3lrify",
                  "author": "alin_im",
                  "text": "check out huggingface\n\nhttps://huggingface.co/moonshotai/Kimi-K2.5/tree/main",
                  "score": 3,
                  "created_utc": "2026-02-04 21:28:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3m0jvs",
                  "author": "twack3r",
                  "text": "The first thing is stop using Ollama. It’s a way slower, unflexible wrapper/botchjob on top of llama.cpp. It’s a complete waste of time and in many ways the worst way to start running LLMs locally.\n\nIt’s very easy to get the same model running locally via llama.cpp, if in doubt event ChatGPT instant will churn out a standard launch command respecting your hardware constraints.\n\nThat way you actually learn about ctx sizes, quant differences, FA specifics, different types of layer splits etc. and some out of the experience with an actual added value to your personal skillset.",
                  "score": 11,
                  "created_utc": "2026-02-04 22:11:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lpaqk",
          "author": "Mountain_Station3682",
          "text": "For open claw probably minimax m2.1 or qwen-3-coder-next (80b). You can try glm 4.7 flash but at 30b parameters it won't be amazing.\n\nYou could fit glm 4.7 (non-flash) with 2 sparks, but glm 4.7 isn't getting great reviews for open claw and it would be < 20 tokens per second.\n\n[https://forums.developer.nvidia.com/t/how-to-run-glm-4-7-on-dual-dgx-sparks-with-vllm-mods-support-in-spark-vllm-docker/355603](https://forums.developer.nvidia.com/t/how-to-run-glm-4-7-on-dual-dgx-sparks-with-vllm-mods-support-in-spark-vllm-docker/355603)",
          "score": 3,
          "created_utc": "2026-02-04 21:17:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lyj78",
              "author": "FishIndividual2208",
              "text": "I noticed instantly inference improvement when i moved from ollama to lama.cpp\n\nWorth a try.",
              "score": 2,
              "created_utc": "2026-02-04 22:02:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3o4m0j",
                  "author": "p_235615",
                  "text": "I dont understand, why would be llama.cpp faster than ollama, when ollama is the same llama.cpp underneath with a wrapper around it. I mean sure, llama.cpp could have better defaults and expose more options, but most of those you can tune also through ollama...",
                  "score": 1,
                  "created_utc": "2026-02-05 05:38:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3nlmfx",
          "author": "pmttyji",
          "text": "Just try multiple models & share benchmarks.\n\nFrequently mentioned coding models in LLM subs(sorted by size):\n\n* GPT-OSS-20B\n* Devstral-Small-2-24B-Instruct-2512\n* Qwen3-30B-A3B\n* Qwen3-30B-Coder\n* Nemotron-3-Nano-30B-A3B\n* Qwen3-32B\n* GLM-4.7-Flash\n* Seed-OSS-36B\n* Qwen3-Next-80B\n* Qwen3-Coder-Next\n* GLM-4.5-Air\n* GPT-OSS-120B\n* Devstral-2-123B-Instruct-2512\n* MiniMax-M2.1\n* Qwen3-235B-A22B\n* GLM-4.5, 4.6, 4.7\n* Qwen3-480B-Coder\n* Deepseek-Vx, R1\n* Kimi-K2, K2.5",
          "score": 3,
          "created_utc": "2026-02-05 03:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mayjy",
          "author": "DerFreudster",
          "text": "You bought two Sparks and are using Ollama? ",
          "score": 8,
          "created_utc": "2026-02-04 23:05:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nfsrt",
              "author": "NotLogrui",
              "text": "Just because he has money doesn't mean they've dived extremely deep into local model infrastructure",
              "score": 5,
              "created_utc": "2026-02-05 02:54:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3oie9n",
                  "author": "NHooked",
                  "text": "It’s all a matter of time and money. Can’t run any services in the cloud, everything must be on-prem.\n\nNow finally got some budget for some machines, now need to invest the time to get the most out of it.\n\nI am here to learn 😁",
                  "score": 3,
                  "created_utc": "2026-02-05 07:35:59",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3m34c4",
          "author": "Everlier",
          "text": "Kimi 2.5 w/ vllm or ik_llamacpp is the only answer for this setup, it's far outside a segment optimised for consumers, so some setup will be needed",
          "score": 1,
          "created_utc": "2026-02-04 22:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mntbj",
          "author": "KooperGuy",
          "text": "Nothing too good",
          "score": 1,
          "created_utc": "2026-02-05 00:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pxfb5",
          "author": "Clear-Astronaut-8006",
          "text": "Same boat here dual DGX, currently running seperate Qwen2.5-Coder-32B-Instruct\nDeepSeek-R1-Distill-Qwen-32B\nDuring the day to vibe code and then Eagle3 training at night.\n\nCurious to see other uses.",
          "score": 1,
          "created_utc": "2026-02-05 14:17:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qup0ul",
      "title": "Ryzen AI MAX+ 395 96GB, good deal for 1500?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/tr0fu7i6h9hg1.png",
      "author": "rusl1",
      "created_utc": "2026-02-03 11:01:47",
      "score": 59,
      "num_comments": 95,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qup0ul/ryzen_ai_max_395_96gb_good_deal_for_1500/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3bv60s",
          "author": "Grouchy-Bed-7942",
          "text": "Get the 128GB version; you'll regret the extra 32GB if you want to run LLM!\n\nThe Bosgame is currently under €1700 in the EU:\n\nhttps://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395",
          "score": 41,
          "created_utc": "2026-02-03 12:14:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dsv25",
              "author": "NaabSimRacer",
              "text": "Is it really worth it for LLMs? I have a 5090 and was planning to get a couple 3090s to i crease vram, hows the ai max 395 doing?",
              "score": 1,
              "created_utc": "2026-02-03 18:12:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3eecn1",
                  "author": "rishabhbajpai24",
                  "text": "I have many RTX 40 and 50 GPUs, but I still use M5 for my LLM workload. This is because I can achieve a high context length on it without CPU offloading.\n\nWhile the M5 is not as fast as any RTX 4070+ or 5070+, it is more efficient and works well with Mixture of Experts (MOE) models.\n\nIf you can get a combined RTX VRAM of around 40 GB, you might not need it. However, if you want to run GPT-OSS 120B with a good context length, the M5 is a better option.",
                  "score": 2,
                  "created_utc": "2026-02-03 19:51:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3j2gxj",
                  "author": "Torodaddy",
                  "text": "Very power efficient compared to your nvidia cards, but less performance but not by much",
                  "score": 1,
                  "created_utc": "2026-02-04 13:49:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f7jjd",
              "author": "Charming_Support726",
              "text": "It is a great workstation. Got a Bosgame M5 128gb. \n\n  \nYou could use it to run smaller / MOE models  or calc embeddings and such. It is to slow to use it for productively running decent LLMs. ( So are the Macs as well IMO ) Few people attach their 3090/5090 or other cards with some adapter ( only 4 Lanes) to speed up dense models. \n\nI gets a bit noisy running HL:Alyx  with your Quest3 but quality is unbelievable.",
              "score": 1,
              "created_utc": "2026-02-03 22:07:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3byck8",
              "author": "Schumack1",
              "text": "Its very noisy under stress or they improved that?",
              "score": 0,
              "created_utc": "2026-02-03 12:37:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3c2fzd",
                  "author": "ForsookComparison",
                  "text": "It can be configured to pull over 140watts. In a normal ATX case if I had a 140w CPU, the cooler I used would be larger than the entire bossgame m5 case.\n\nIt's gonna be loud because that fan is going to have to spin *fast*. I don't think there's any fixing that unless they opt to run this at the lowest possible power settings.",
                  "score": 5,
                  "created_utc": "2026-02-03 13:03:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3etom6",
                  "author": "fallingdowndizzyvr",
                  "text": "Run it in balance instead of performance mode. That's what I do. It uses way less power but the performance is only slightly lower. Like so slight that I can't even tell the difference while gaming. And even when benchmarking, it's so small.",
                  "score": 5,
                  "created_utc": "2026-02-03 21:03:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3cwkte",
          "author": "rexyuan",
          "text": "Both 64gb and 96gb configs of gmktec’s 395 only allow up to 48gb vram dedication\n\nhttps://www.gmktec.com/pages/evo-x2-bios-vram-size-adjustment-guide",
          "score": 10,
          "created_utc": "2026-02-03 15:43:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3deirx",
              "author": "Proper_Taste_6778",
              "text": "if you use linux you can allocate 120gb to gpu or even 124gb( if you use this pc as server)",
              "score": 7,
              "created_utc": "2026-02-03 17:07:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dkmqo",
                  "author": "rusl1",
                  "text": "Oh okay, I plan to use Linux indeed",
                  "score": 2,
                  "created_utc": "2026-02-03 17:35:37",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o3o59w6",
                  "author": "FirstBusinessCoffee",
                  "text": "Can you enlighten us? Haven't found an option to allocate more than 48gb (Limited in BIOS) as seen in the Post above for the 96GB Version.",
                  "score": 1,
                  "created_utc": "2026-02-05 05:43:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3d5tn1",
              "author": "WiseassWolfOfYoitsu",
              "text": "Note that it's possible on at least Linux to override that cap and get closer to max inside the OS.",
              "score": 4,
              "created_utc": "2026-02-03 16:27:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d0s1l",
              "author": "rusl1",
              "text": "Wow I had no idea this limit exists! Thank you!!",
              "score": 1,
              "created_utc": "2026-02-03 16:03:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3esgj4",
                  "author": "fallingdowndizzyvr",
                  "text": "It doesn't.",
                  "score": 0,
                  "created_utc": "2026-02-03 20:57:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3br2z5",
          "author": "InfraScaler",
          "text": "Yeah they typically do sales on Amazon (at least Amazon Spain). \n\n96GB for 1500EUR is a good deal. I have seen the 128GB for 2k in the past. Sometimes on their official site is even cheaper (not now, I just checked).\n\nI don't think you can get more bang for your buck for 1500EUR unless you can get a few 3090s at a good price and already have a beefy PC!",
          "score": 7,
          "created_utc": "2026-02-03 11:43:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bs9j3",
              "author": "rusl1",
              "text": "Thank you very much for having a look!",
              "score": 2,
              "created_utc": "2026-02-03 11:53:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cufkq",
          "author": "anuszebra",
          "text": "USB4 only and no oculink. I’d say no! Check Minisforum x1 ai pro.",
          "score": 2,
          "created_utc": "2026-02-03 15:33:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3esrat",
              "author": "fallingdowndizzyvr",
              "text": "Just put in a NVME oculink adapter. Many mini pcs that have oculink from the factory do it that way.",
              "score": 1,
              "created_utc": "2026-02-03 20:58:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3o5p2m",
              "author": "FirstBusinessCoffee",
              "text": "AI 395+ vs AI 370 - Not really comparable",
              "score": 1,
              "created_utc": "2026-02-05 05:46:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bv136",
          "author": "Transcontinenta1",
          "text": "The 192gb one was $1800 for all of November. Then RAMageddon happened.\n\nEdit: my bad I meant 128gb. I don’t know why I thought 192 lol. Some of the newer laptops have it. My bad",
          "score": 4,
          "created_utc": "2026-02-03 12:13:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3chdci",
              "author": "pawaww",
              "text": "You sure, on a max+ 395 ?",
              "score": 3,
              "created_utc": "2026-02-03 14:27:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3cjc1j",
                  "author": "Transcontinenta1",
                  "text": "My bad 128gb. I thought you were asking on price then i noticed I put 192gb ram. And that part is wrong lol my fault",
                  "score": 2,
                  "created_utc": "2026-02-03 14:38:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3cikzq",
                  "author": "Transcontinenta1",
                  "text": "99.99% sure 🙂‍↕️yup. It was $2200 and then they dropped to $1800-ish. This was even the price on their website. Then it creeped back up, and then it went above the original msrp. I had my eyes on that and the DGX. But I didn’t want to deal with ROCm and the DGX wasn’t beating gpus. \n\nSo I ended blowing my company’s money on dual 48gb 4090D’s and 4-slot nvlinked 3090ti’s.",
                  "score": 1,
                  "created_utc": "2026-02-03 14:34:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3cjltr",
          "author": "Jackster22",
          "text": "The AMD AI MAX+ 395 is not as good as people make it out to be. \nYes you get 96/128 GB of ram but the performance is dog. \nThe ram is slow and the GPU is not actually fast.",
          "score": 4,
          "created_utc": "2026-02-03 14:39:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3d12yk",
              "author": "FloridaManIssues",
              "text": "I have one as my main AI machine and it runs good for $2,000. It's as fast as my M2 Macbook. Next option is to spend $10k on a Mac studio...",
              "score": 13,
              "created_utc": "2026-02-03 16:04:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fuyuz",
                  "author": "4redis",
                  "text": "If you get maxed out macbook latest model, is there anything that beats it?",
                  "score": 1,
                  "created_utc": "2026-02-04 00:10:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3e4e6j",
                  "author": "dusty_Caviar",
                  "text": "no next option is a quad or six 3090 setup which you could easily build for far less than 5k if you spend wisely. and the performance will shatter a mac studio or amd ai max.",
                  "score": -2,
                  "created_utc": "2026-02-03 19:04:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dhlmg",
              "author": "laughingfingers",
              "text": "Nope. I have this. Performance is fine. It's just not an Nvidia. But 80B and 120B models run at 40t/s and more.",
              "score": 12,
              "created_utc": "2026-02-03 17:21:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fw6a5",
                  "author": "joeln15",
                  "text": "What models are you running?",
                  "score": 2,
                  "created_utc": "2026-02-04 00:16:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3dbyvk",
              "author": "ityeti",
              "text": "Strix halo (and the DGX line) are freight trains, not race cars. Quad-channel at 8000 MT/s is real world >100GB/s to the CPU and >200GB/s to the GPU. The ram bandwith is crazy high compared to a desktop or anything by 8 channel RAM threadripper, it's just not fast compared to GDDR6/7 VRAM. However, unless your spending ~$10k, your not getting ~96GB of it. If you're running anything bigger than a GPU's vram, it is going generate tokens  slower than a strix halo and by a lot. Different purposes, different goals.",
              "score": 4,
              "created_utc": "2026-02-03 16:55:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3d6397",
              "author": "WiseassWolfOfYoitsu",
              "text": "Depends on use case. Is it as fast as an array of 3090s? No. But it'll cost less and use a tiny fraction of the power for the large models. You're better off doing something with smaller models and faster cards if you can. But this is by far the cheapest way to run large models at a usable speed.",
              "score": 2,
              "created_utc": "2026-02-03 16:28:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i3saf",
              "author": "GCoderDCoder",
              "text": "Having started before most unified memory options besides mac, I have a 92gb vram cuda build, i have a mac studio m3 ultra, I have smaller multi gpu cuda builds, and I have a strix halo. I constantly say strix halo is the best value. \n\nWith a $2k strix halo you get gpt oss 120b at 50t/s and other really useful models at useful speeds. Strix halo enables vllm for all the vllm things and you can hack it to use fsr4 for gaming (AMD should make it easier but it's an option).\n\nIt's hard fitting lots of cuda GPUs into a build and it's hard dropping $8-10k on a pro 6000 just for gpt-oss-120b level models really fast. Cuda (3090s) are twice as fast as strix halo for inference for my workloads but they are like 2-5x the price for comparable cuda builds, pains to fit gpus in, pains to power, etc. Spark gets the same token generation at twice the cost. \n\nMy Mac Studio m3 ultra is 50% faster than strix halo but costs like %50 more at the same size and I have a version of m3 ultra that probably has a bigger gpu than most would get with the 128gb version.  Plus mac sucks at image gen and gaming and locks you into Apple ecosystem. \n\nIf you are serving llms for a company you probably want cuda. For personal inference i would start with strix halo and only do something else if it has a unique benefit I need.",
              "score": 2,
              "created_utc": "2026-02-04 09:29:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3isnfm",
                  "author": "venice_mcgangbang",
                  "text": "Thanks for the tips! So something like framework desktop is still a good start for home run MOE models (like 120b OSS on 128gb ram) and some image gen on the side? Is the m3 that bad for image gen that it makes the strix halo worth it?",
                  "score": 1,
                  "created_utc": "2026-02-04 12:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3i5ii4",
              "author": "No_Clock2390",
              "text": "The performance is fine. 40-50 tokens/second for 120B models that the 5090 can't even run.",
              "score": 2,
              "created_utc": "2026-02-04 09:46:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3es2vl",
              "author": "fallingdowndizzyvr",
              "text": "Do you have one?\n\nI do. And since I've gotten it I've pretty much stopped using all my boxes full of GPUs unless I need the extra VRAM to run models that don't fit entirely on my Strix Halo.",
              "score": 1,
              "created_utc": "2026-02-03 20:55:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ezvjj",
              "author": "Food4Lessy",
              "text": "The 395 was never meant to be an AI chip its a experimental use of AMD next Infinity fabric chiplet. repurpose from overkill igpu.\n\n\nFaster than regular RAM, dog slower than VRAM on 70B dense.\n\n\nIts only good for MoE with 8B active.\nJust like DGX Spark with lower power draw",
              "score": 1,
              "created_utc": "2026-02-03 21:31:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3efgwn",
          "author": "epSos-DE",
          "text": "If you really want lower cost.\n\nOrder privately from Taiwan !\n\nFind some real dude or company there that export !\n\nMake some calls, email, video call.\n\nThey speak ENGLISH !!!\n\nOR surplus sales from last year models !\n\n  \nExpect to pay import taxes, IF DHL or so, as you to pay !",
          "score": 3,
          "created_utc": "2026-02-03 19:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f65zr",
              "author": "Shoddy_Bookkeeper900",
              "text": "Do you have any link? Thx!",
              "score": 3,
              "created_utc": "2026-02-03 22:00:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fvpxb",
              "author": "4redis",
              "text": "If feel this is more risky and you could end up with nothing but how do you even go about it?",
              "score": 2,
              "created_utc": "2026-02-04 00:14:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3lfru1",
              "author": "Shoddy_Bookkeeper900",
              "text": "I have friends and relatives currently in Taiwan. If this is a feasible route, I can ask their help, so please share the information, thanks!",
              "score": 1,
              "created_utc": "2026-02-04 20:32:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3cr6xx",
          "author": "_VirtualCosmos_",
          "text": "I'm quite happy with my geekom A9 mega (same strix halo with 128 GB) but it's hard to run certain stuff due to low general support. All goes to the damn Nvida and cuda.",
          "score": 1,
          "created_utc": "2026-02-03 15:17:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dexfv",
              "author": "Proper_Taste_6778",
              "text": "checkthis channel [https://www.youtube.com/@donatocapitella](https://www.youtube.com/@donatocapitella)",
              "score": 3,
              "created_utc": "2026-02-03 17:09:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fwvx0",
                  "author": "_VirtualCosmos_",
                  "text": "Thank you so much!",
                  "score": 1,
                  "created_utc": "2026-02-04 00:20:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3itlkc",
                  "author": "venice_mcgangbang",
                  "text": "Thank you",
                  "score": 1,
                  "created_utc": "2026-02-04 12:58:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3cu0eh",
              "author": "rusl1",
              "text": "Did you try LMStudio on it?",
              "score": 2,
              "created_utc": "2026-02-03 15:31:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3fsuvr",
                  "author": "_VirtualCosmos_",
                  "text": "Yes, it works perfect. That's the program that gave the less problems, aka 0. I use it as server or directly, with Qwen3 30b VL for vision stuff and GPT-OSS 120b MXFP4 for general stuff, both run super fast.",
                  "score": 2,
                  "created_utc": "2026-02-03 23:58:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3d3l8l",
          "author": "Old_Leshen",
          "text": "can someone tell which nvidia (desktop) gpu is **Integrate AMD Radeon 8060S Graphics** comparable to? Or are there any benchmarking tests already run on this machine?",
          "score": 1,
          "created_utc": "2026-02-03 16:16:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3df46k",
              "author": "Proper_Taste_6778",
              "text": "in gaming its between rtx 3060 12gb and rx6700xt",
              "score": 1,
              "created_utc": "2026-02-03 17:10:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3et966",
              "author": "fallingdowndizzyvr",
              "text": "4060.",
              "score": 1,
              "created_utc": "2026-02-03 21:01:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f0ilb",
                  "author": "Old_Leshen",
                  "text": "But does the higher GPU ram, despite being shared, make it better at running LLMs than 4060? Or does the comparison already take into consideration the ram?",
                  "score": 1,
                  "created_utc": "2026-02-03 21:34:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3db1x2",
          "author": "TheCTRL",
          "text": "I’ve bought recently a minis forum ms-s1 max and I’m happy with it",
          "score": 1,
          "created_utc": "2026-02-03 16:51:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dcxb3",
          "author": "Sad-Chard-9062",
          "text": "wow ram is now real expensive.",
          "score": 1,
          "created_utc": "2026-02-03 17:00:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3djw8r",
          "author": "Few_Border3999",
          "text": "I bought GMKTec X1 a few months ago. They dont ship from EU so you might have to pay for shipper to do customs declaration. \n\ni bought bosgame m5 as well they shipped from germany.",
          "score": 1,
          "created_utc": "2026-02-03 17:32:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dp9x4",
          "author": "SocialDinamo",
          "text": "I personally am really thankful I went for the 128gb. I went framework but I don’t think that matters much",
          "score": 1,
          "created_utc": "2026-02-03 17:56:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ef15z",
          "author": "I1lII1l",
          "text": "I bought the bosgame, twice already. 128GB. Can recommend",
          "score": 1,
          "created_utc": "2026-02-03 19:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3juotn",
              "author": "RedditVTT",
              "text": "Did you purchase directly from their website? Tried ordering there and got a fraud alert from one credit card, wouldn't go through with another Amex card.",
              "score": 1,
              "created_utc": "2026-02-04 16:09:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ojn3y",
                  "author": "I1lII1l",
                  "text": "Yeah, from their [website](https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395?sku=18073504178329773749034095). I am definitely not the only one who bought it, price went up multiple times already. I got it for 1444 EUR, now it’s sold for 1689",
                  "score": 1,
                  "created_utc": "2026-02-05 07:47:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ftp10",
          "author": "Truth_Artillery",
          "text": "thats an amazing deal\n\ni bought mine for $1300 on ebay. All the other listings were ~2000",
          "score": 1,
          "created_utc": "2026-02-04 00:03:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fv0au",
          "author": "NotBad93",
          "text": "It looks great! Is this brand reliable? Definitely the 128gb version for llm , im pretty new on this pc servers, and i would like to know what is the best way to get a 128-256-512gb vram for local llm , eventually with openclawd .\n\nAny suggestions? Best price/value? IMO 128gb for 1600€ is cheap?????!!!\n\nMac mini 64gb is around 3k",
          "score": 1,
          "created_utc": "2026-02-04 00:10:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gmq5w",
          "author": "moneypitfun",
          "text": "Anyone compared these with the Framework Desktop boards? Anyone running Proxmox on the AI Max+ systems?",
          "score": 1,
          "created_utc": "2026-02-04 02:45:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k8kns",
          "author": "minitoxin",
          "text": "Yup  its a good deal, where else can you get 90GB of unified vram with  low power usage for 1500?   . I use the 128gb version and its very good for my  use case. i got the evo-x2 variant.  I installed the new Qwen3-Coder-Next on it yesterday  and it runs very well using lmstudio and vulkan drivers,  I haven't had much luck with the rocm drivers .   if you do decide to  get it , check out  [Donato Capitella](https://www.youtube.com/@donatocapitella)'s testing and tuning of models with  the strix halo  he does  great job  [https://www.youtube.com/watch?v=Hdg7zL3pcIs](https://www.youtube.com/watch?v=Hdg7zL3pcIs)",
          "score": 1,
          "created_utc": "2026-02-04 17:13:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t6p1h",
          "author": "Fickle_Debate_9746",
          "text": "Hmmmm. I have a 2x 3090 build but maybe I should just get one of these instead of building out further. All the high cuda stuff is probably at its limit with these two. Can just get one of these to do larger coding models. These little units look really awesome.",
          "score": 1,
          "created_utc": "2026-02-05 23:44:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43quif",
          "author": "hejj",
          "text": "Might depend on how Linux friendly this particular hardware is",
          "score": 1,
          "created_utc": "2026-02-07 16:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3c1vn1",
          "author": "GroundbreakingAd7422",
          "text": "Asus flow z13 is about 2300 but you get an oled screen along with it. So I guess it comes down to the worth of that screen and portability. If you already have a nice monitor and don't need to travel with it then it's defo worth it.",
          "score": 1,
          "created_utc": "2026-02-03 13:00:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3cwcwt",
              "author": "Whiskey1Romeo",
              "text": "Have one (z-13 128GB). The larger memory configuration is the only way to go for larger models if local llm is they way you want to go.",
              "score": 2,
              "created_utc": "2026-02-03 15:42:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3itha9",
                  "author": "venice_mcgangbang",
                  "text": "Does it throttle much?",
                  "score": 1,
                  "created_utc": "2026-02-04 12:57:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3esknm",
              "author": "fallingdowndizzyvr",
              "text": "It has less than half the TDP of mini pc Strix Halo.",
              "score": 2,
              "created_utc": "2026-02-03 20:58:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3itfaf",
              "author": "venice_mcgangbang",
              "text": "The only issue is that the cooling will probably make that cpu throttle. It’s crammed into a tiny 13 inch chassis.",
              "score": 1,
              "created_utc": "2026-02-04 12:57:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3iwj50",
                  "author": "GroundbreakingAd7422",
                  "text": "If the laptop can handle it then so can this im guessing",
                  "score": 1,
                  "created_utc": "2026-02-04 13:16:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bofz8",
          "author": "tomsyco",
          "text": "Probably better off with a used Mac studio.",
          "score": -6,
          "created_utc": "2026-02-03 11:22:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bme5v",
          "author": "FederalLook5060",
          "text": "too expensive where not these like 1100$in december?",
          "score": -2,
          "created_utc": "2026-02-03 11:04:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bn2ef",
              "author": "rusl1",
              "text": "No idea, i just came across this. Given how RAM has skyrocketed I don't think we will see that price again :(",
              "score": 4,
              "created_utc": "2026-02-03 11:10:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3bsgvv",
              "author": "InfraScaler",
              "text": "Definitely not in Europe!",
              "score": 2,
              "created_utc": "2026-02-03 11:54:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3etfl1",
                  "author": "fallingdowndizzyvr",
                  "text": "Actually the lowest price reported for these was in Europe. And it was something like $1100.",
                  "score": 1,
                  "created_utc": "2026-02-03 21:02:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3butq7",
          "author": "Transcontinenta1",
          "text": "No",
          "score": -4,
          "created_utc": "2026-02-03 12:12:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxuwh7",
      "title": "Super-light, 90ms latency, runs locally on Apple Silicon. More expressive and prosodic than Elevenlabs.",
      "subreddit": "LocalLLM",
      "url": "https://v.redd.it/gbb6ro2d2yhg1",
      "author": "EmbarrassedAsk2887",
      "created_utc": "2026-02-06 21:43:54",
      "score": 54,
      "num_comments": 18,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxuwh7/superlight_90ms_latency_runs_locally_on_apple/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3zg0hd",
          "author": "LSU_Tiger",
          "text": "Underwhelming technology and that may have been the most boring video I've ever seen. No one wants to watch someone click around a screen for 6 minutes.",
          "score": 9,
          "created_utc": "2026-02-06 22:38:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zi4gw",
              "author": "EmbarrassedAsk2887",
              "text": "sure! we are not here to please but show the exciting possibilities of a silicon. ",
              "score": -2,
              "created_utc": "2026-02-06 22:50:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zgrco",
          "author": "ProfMooreiarty",
          "text": "The onboarding seems like quite a bit to do.",
          "score": 5,
          "created_utc": "2026-02-06 22:42:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhuys",
              "author": "EmbarrassedAsk2887",
              "text": "its a few mins setup if you choose the hardcore mode! you can reach me out if theres any other issue!\n\nthanks :)",
              "score": 1,
              "created_utc": "2026-02-06 22:48:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o408wmt",
          "author": "TopTippityTop",
          "text": "Very cool! Wish it supported more languages (Portuguese, at least), but I get it. I take that it's open source, right? Any way to fine tune it to other languages? Can it clone voices?",
          "score": 3,
          "created_utc": "2026-02-07 01:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d9rmn",
              "author": "EmbarrassedAsk2887",
              "text": "yes it can clone voices as well. in the upcoming days, we gonna push it to production as well and before easter we will release the model weights as well. \n\nyes we did train it on 14+ languages but specifically for english and french, then spanish. \n\nbut yeah!! here you go, you can be the judge here :) \n\nhere’s one example i generated for you. \n\nhttps://youtu.be/Kyd3TL5p-lQ?si=TvTUUAjsyZhPEYL9",
              "score": 1,
              "created_utc": "2026-02-09 02:56:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dh6rg",
                  "author": "TopTippityTop",
                  "text": "Very cool! I think the Portuguese needs more training, but it's a start!",
                  "score": 1,
                  "created_utc": "2026-02-09 03:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o43zifx",
          "author": "Maximum-Wishbone5616",
          "text": "what ???\n\n# Desktop Only Experience\n\nHey there! BodegaOS is currently only available on desktop. Please visit this downloads page on your desktop computer to experience BodegaOS and all its features.\n\nWTF  \non 2x 5090, 1x 7950x3d, 5x 4k monitors and you still using some crap that cannot recognize desktop? \n\nThat is 2001 knowledge...",
          "score": 1,
          "created_utc": "2026-02-07 17:19:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d64vr",
              "author": "EmbarrassedAsk2887",
              "text": "hey there! we relied on the browser user agent for detecting the device type. apologies for that, now you can visit again— you won’t have a problem now.\n\napologies for any inconvenience. please dm or hit me up anytime with any query you might have.",
              "score": 1,
              "created_utc": "2026-02-09 02:36:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40w9en",
          "author": "somethingdangerzone",
          "text": "More competition in the voice space is great! I look forward to seeing how this develops.\n\nI tried going to your srswti.com/download site above, but I keep getting a black, blank page. It could be me, or maybe there is a site issue.",
          "score": 1,
          "created_utc": "2026-02-07 03:53:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41kaxl",
              "author": "EmbarrassedAsk2887",
              "text": "hey there, thanks! the post mentions https://www.srswti.com/downloads\n\nyou can try visiting it again? please hit me up or dm if any other queries.",
              "score": 0,
              "created_utc": "2026-02-07 07:03:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o432knh",
                  "author": "somethingdangerzone",
                  "text": "I tried it again on both LibreWolf and Brave, and I got a black blank page both times.if anything else, i will reach out, thanks",
                  "score": 1,
                  "created_utc": "2026-02-07 14:33:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv3vlc",
      "title": "Qwen3-Coder-Next just launched, open source is winning",
      "subreddit": "LocalLLM",
      "url": "https://jpcaparas.medium.com/qwen3-coder-next-just-launched-open-source-is-winning-0724b76f13cc",
      "author": "jpcaparas",
      "created_utc": "2026-02-03 20:52:21",
      "score": 53,
      "num_comments": 19,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qv3vlc/qwen3codernext_just_launched_open_source_is/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3jnihm",
          "author": "Adrian_Galilea",
          "text": "I’m all for local llm, but don’t be delusional. Nothing beats proprietary frontier models yet, and subscriptions or even API’s are way more cost effective than building anything yourself right now.\n\nThis model looks promising tho.",
          "score": 7,
          "created_utc": "2026-02-04 15:36:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nsh68",
              "author": "simracerman",
              "text": "Already own a PC with 5070 Ti, and a 64GB DDR5 RAM.\n\nIs it more cost effective to pay subscription/API or setup AI at home?\n\nHere are my use cases that local already fulfills:\n- Light coding as a hobby (running Qwen3-Next, and OSS-120b)\n- Small task models to handle parsing docs, do expense reports,..etc.\n- ComfyUI with Qwen Image Edit already beats chatGPT (free sub) in my testing in quality and performance\n\n\nIf my needs were enterprise level for coding, wanted snappy speeds, or simply the best of everything, then I’d consider API with Claude or GPT. Many folks shoot down local too quickly because it couldn’t solve a complex task or the speed wasn’t sufficient. If you have the hardware, doesn’t hurt to fiddle around and make a setup that reduces your reliance on paid AI significantly.",
              "score": 1,
              "created_utc": "2026-02-05 04:12:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xsi5q",
                  "author": "SmallHoggy",
                  "text": "You’re using 120B, 80B models on a 5070ti???\nWhat are you getting like 1.5 tokens/s?",
                  "score": 1,
                  "created_utc": "2026-02-06 17:45:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3omwif",
                  "author": "andreabarbato",
                  "text": "openai api is slower to me than gpt oss running on my machine. good times to have hardware!",
                  "score": 0,
                  "created_utc": "2026-02-05 08:18:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3h1xuw",
          "author": "pmttyji",
          "text": "I'm sure we're gonna get more coder models & more 100B models(MOE) this year.",
          "score": 4,
          "created_utc": "2026-02-04 04:16:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hjznm",
              "author": "kwhali",
              "text": "It'd be nice if it'd be possible to get more distilled models?\n\nI'm not quite sure how models for dev compare to plain text generation tasks but some of those work quite well even at low params and heavy quantization (Q4, dipping below that is a bit too aggressive).\n\nI would imagine with MCP you could have an agent that orchestrates more specialised ones, so while it may not be as fast / efficient or of the same quality that would make the models more broadly available that even smartphones could run them locally.",
              "score": 1,
              "created_utc": "2026-02-04 06:29:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3gz36v",
          "author": "Icy_Annual_9954",
          "text": "What Hardware do you need to run it?\n\nEdit: it is written in the article.",
          "score": 2,
          "created_utc": "2026-02-04 03:58:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3j4d39",
              "author": "Look_0ver_There",
              "text": "The Qwen-sourced model runs just fine on my 128GB Strix Halo MiniPC.  It was running at around 30tg/sec with a 64K context window, which is fast enough for local development.",
              "score": 3,
              "created_utc": "2026-02-04 13:59:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3muaa2",
                  "author": "Battle-Chimp",
                  "text": "Weird, I'm getting 40 t/s on my strix with qwen next 80b.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:51:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3q3k94",
          "author": "kayteee1995",
          "text": "I feel just fine with Qwen3 coder 30B for Agentic coding (Vscode + Roo + LMstudio) . The model only ~14Gb and fit with my 4060ti 16gb. \nThe newest Qwen3 Next is over 40gb , so I have to offload to Ram if I wanna try it, with a little slow down.",
          "score": 2,
          "created_utc": "2026-02-05 14:50:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3in1bq",
          "author": "blownawayx2",
          "text": "Gave it a try updating a web site I was working on. It made a new page to allow for a dark/light refresh of the page but got stuck in a loop with implementing it and added a couple of new bugs. Thankfully, Claude code was able to fix things quickly.\n\nSo, I plan on using Qwen3 for initial tasks with Claude Code doing clean up!",
          "score": 1,
          "created_utc": "2026-02-04 12:14:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jnuv2",
              "author": "Adrian_Galilea",
              "text": "This isn’t a good idea, you’d spend less time starting with a clean focused solution than creating problems then solving them. You will create so much bloat that will be much harder to fix.",
              "score": 2,
              "created_utc": "2026-02-04 15:37:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3jnysy",
                  "author": "blownawayx2",
                  "text": "Saves on costs is the primary function, but I do hear you and don’t disagree.",
                  "score": 1,
                  "created_utc": "2026-02-04 15:38:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47pxx8",
          "author": "Commercial-Plant4423",
          "text": "The model is so large that I have to offload it to the CPU, which causes the TPS to drop to 20.\n\nA 3090 with 64GB of system RAM can barely run, let alone at a fast speed.",
          "score": 1,
          "created_utc": "2026-02-08 06:51:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hmodj",
          "author": "TopTippityTop",
          "text": "Initially the benchmarks seem to always favorable, and then later they have a way of getting lower.\n\nWe'll see.",
          "score": 1,
          "created_utc": "2026-02-04 06:52:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quip6h",
      "title": "New 1.4B Model Victorian LLM - Violet",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1quip6h/new_14b_model_victorian_llm_violet/",
      "author": "RecalcitrantZak",
      "created_utc": "2026-02-03 04:50:09",
      "score": 46,
      "num_comments": 13,
      "upvote_ratio": 0.91,
      "text": "[Thinking](https://preview.redd.it/tfzig0wfj7hg1.jpeg?width=4500&format=pjpg&auto=webp&s=f1588982efb03ec7370f46c220660b0b4bdcd0dd)\n\nSo hopefully I'm not breaking any self-promotion rules -- I've been a longtime lurker of LocalLLM. Several months ago I got the idea in my head that I would like to build my own LLM but using a completely public domain corpus-- the idea was to have something akin to like an ethically sourced LLM with the output being completely public domain as well. By the people, for the people. This led me down the road of DAPT, and LoRA on other publicly licensed models before I finally decided that the only way to do this right is to do it from scratch. In sourcing the data I decided that it would be more interesting to go for a theme/time period than to just find all data prior to a certain time this led me to the idea of making a Victorian LLM-- completely unencumbered with the modern trappings of life. \n\n  \nAt the time I didn't know about TimeCapsuleLLM (and my hats off to the gentleman who made that), as I was largely working in parallel to that person's work. I had settled on building a 160M base model that was completed around October, and then I finished with a 1.4B model that was finished in December. Around the time mid-December happened I found out that I wasn't the only one working on a Victorian-era LLM. I almost threw in the towel, but I figured I might as well complete the project maybe it might make sense to join forces at a later date or something. \n\nSo I'm releasing [Violet](https://huggingface.co/zakarth/violet-1b4-chat) into the world.-- both the 160M base model and 1.4B base model both of which are suitable for text completions. But then just to be a little different, and to add on just a little bit of extra polish, I've taken both sets of models to make \"chat\" variants. And then just to add a little extra bit on top of that, I built ONNX quantized versions that can load locally in your browser -- no data ever sent to a server. The [demos for these ](https://huggingface.co/spaces/zakarth/violetdemo)are linked off of HF. \n\nBy the time I had gotten chat working, I had the extra idea that I actually wanted her to display moods as she would chat, so I could load in different avatar pictures of Violet as she spoke. That's what is featured here. This adorable artwork was commissioned right here off of Reddit specifically from a human. u/Miserable-Luck3046 so if you like what you see of Violet, consider giving her a commission because she delivered well above and beyond. \n\nSo to my knowledge, Violet is the only LLM fully pretrained on nothing but Victorian era data (1800-1899) that you can have something of a meaningful chat with. \n\nNow there are some limitations to meaningful-- It's not perfect. Violet can be a little bit brittle. I'd say both models punch above their parameter size in narrative prose but in reasoning they're a bit light. They have historical biases and Violet will absolutely misgender herself, you, and the people she talks about. She can be a little bit silly, and the 160M model in particular can be hilariously off-kilter. But it belongs to all of us now. \n\nFor data sources, I think there is some overlap in the same data that TimeCapsuleLLM was trained on-- Internet Archive, Project Gutenberg, etc. I also had added in British National Library datasets as well as newspapers that I OCR'd from around the UK from Welsh newspaper archives. I had also supplemented some synthetic generated data from the 160M model which was exclusively trained on Project Gutenberg text.\n\nThe Web demos that load entirely in your browser are really geared for Desktop loading-- but I know for a fact that the 160M chat model will load just fine on an iPhone 16 Pro. So that covers about everything, I just wanted to share it with the community. Thanks for listening! ",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1quip6h/new_14b_model_victorian_llm_violet/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3apo5z",
          "author": "According_Study_162",
          "text": "That sounds like quite the feat, congratulations.",
          "score": 3,
          "created_utc": "2026-02-03 06:01:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aqegs",
              "author": "RecalcitrantZak",
              "text": "Thank you-- many late nights, some cost out of pocket. Mistakes were made, but mostly corrected! I went through three different SFT regimens to make the chat work before I settled on something that mostly worked. It was exhausting and I'm excited to move onto the next thing.",
              "score": 3,
              "created_utc": "2026-02-03 06:07:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3aqp7l",
          "author": "donotfire",
          "text": "Nice work\n\nI might try this out later since I’m curious",
          "score": 1,
          "created_utc": "2026-02-03 06:10:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3asel4",
          "author": "thirsty_pretzelzz",
          "text": "This is kind of fascinating. So in its native language and vernacular, is it only capable of speaking as they would have in England during that time period? \n\nDoes this also mean it wouldn’t understand modern phrases even like “what’s up”?",
          "score": 1,
          "created_utc": "2026-02-03 06:24:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3atfa5",
              "author": "RecalcitrantZak",
              "text": "Solid questions -- so it will respond mostly in UK English, there's a few quirks here. Technically the training data is mostly UK English, but a chunk of the English narrative corpus also includes US English. I'd say it's mostly UK English aligned.\n\nNow there is a quirk on user's asking questions. I had to build an SFT corpus to answer questions, and I trained the user questions to include modern variant's like \"What's up\" specifically. (There were roughly about 50,000 variations in total). I did this mostly for ease of interaction because most people don't really do very well speaking in Victorian English, so this was a conscious choice on my part.\n\nPrior to this though I had done more open ended experiments in SFT and it would just end up in confusion, for example I might ask, \"What are you wearing today?\" and the word \"Wearing\" would get confused with \"Wearing\" as in like being weary. (This was mostly on the 160M model)... Other examples of this are common modernisms that really just weren't common in Victorian English that I had to find out the hard way-- Siblings is an example of this, it just wasn't used as commonly back then so if you asked \"Do you have any siblings?\" it would just make ridiculous answers on approximate token matches to names that make no sense.\n\nEdit to add because this is super fascinating to me: There are other examples of close approximate token matches where Violet will inadvertently answer correctly. For example, if you ask about an iPhone she will usually answer as if it were either a telephone or gramophone so that's another example of close token match that kind of lands close. I tested a lot of anachronisms because at first I was going to SFT it to say things like \"I don't understand what you're talking about\" when you ask about DNA, or World War II, or whatever, and eventually decided to take the anachronism handling out because I thought the raw responses were more interesting. So for example if you ask about World War I or World War II, it'll either approximate it to a recent war (like the Crimean War from the 1800s) or just bemoan how sad war is in general. Often she will respond as if World War could be any war in which Europe could be involved, which I thought was equally appropriate. I wanted to preserve the confusion.",
              "score": 7,
              "created_utc": "2026-02-03 06:33:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3b8yjg",
          "author": "avanlabs",
          "text": "Hey , this is a great achievement. I am looking forward to train my own SLM on particular data set and run then on small devices. It will be great if you can share some blog you referred to. Any meaningful insight on building SLM.",
          "score": 1,
          "created_utc": "2026-02-03 08:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dt599",
              "author": "RecalcitrantZak",
              "text": "Sorry for the late reply I wanted to add something meaningful about some of the things that I learned…\n\nOk \n\n1 Following the Chinchilla scaling rules set out by Google in their Chinchilla paper seem to be spot on in terms of data size, training, parameters — and as a proxy, capability. I don’t remember them off the top of my head but it was something like 2.5B tokens for the 160M model and so to 10x that for the 1.4B model I had about 25B very hard won tokens. This ended up about bang on and I trained multiple epochs beyond that and started noticing diminishing returns quick after the first epoch. 160M stopped at 1.5 epochs and 1.4B I stopped pretty much right at 1 epoch. This is to say that I think Google’s data here I think is useful for making sure you’re setting correct goals. This has so much influence on everything like your checkpointing strategy etc \n\n2 Checkpoint often if you have the space. The model sizes were about 300M and 3GB respectively so I ate through nearly 1TB of space on all the runs I had w/ checkpoints \n\n3 The learning rate matters and it’s more difficult to fix mid training than it looks — this is something I learned the hard way. I had completely outsourced this to HF code and templates and took for granted that it needed to be tuned. Recovering mid training run can be very difficult. \n\n4 Ok this was surprising to me — yes clean data is very important, but through necessity I was forced to use a lot of suboptimal OCR junk. This terrified me because it’s impossible to filter and make clean I even did a lot of my own OCR it’s just problems on problems if it’s not OCR junk it’s formatting junk. It might have impacted convergence time but overall the model did fine. I’m just saying it’s not a dealbreaker even if it’s not ideal. 1.4B does output the occasional transcriber note on narrative completions and longform text though lol",
              "score": 1,
              "created_utc": "2026-02-03 18:14:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3iduj9",
                  "author": "avanlabs",
                  "text": "wonderful insights you have shared. Definitely you have done some amazing work. Thank you for being so kind enough to share the detail.",
                  "score": 1,
                  "created_utc": "2026-02-04 11:02:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3b96ip",
          "author": "DazzlingDaikon2162",
          "text": "Awesome job! With all that you did!",
          "score": 1,
          "created_utc": "2026-02-03 08:58:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bb8od",
          "author": "Ryuma666",
          "text": "Great job. This seems fascinating.",
          "score": 1,
          "created_utc": "2026-02-03 09:18:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bbihf",
          "author": "onicarps",
          "text": "how did the war start?\n\n  \n\\-----------------------\n\nI suppose he is now trying to make the  \ncountry feel that the war was a national  \nwar.\n\nThe British Government is still the nation  \nwhich has the greatest interest in the matter.  \nIt is a question of the nation's destiny.  \nThe peace of Europe is a question of the  \nnation's peace.\n\nThe Queen is very much interested in the  \nstate of the country. Her Majesty is a  \ngreat authority in matters of national policy.  \nThe Queen's reign is a time of profound  \npeace and prosperity.\n\nThe death of Sir Arthur Wellesley is a  \ndeep loss to the nation. He was a soldier  \nof the highest character, and his loss will be  \ndeeply felt by the nation.\n\nI do not believe the Queen is quite the  \nobject of public sympathy in her new  \ncareer. She is the very personification of  \na loyal and self-sacrificing woman.\n\nI understand that the Queen has been  \nengaged in many political and social  \nlabours, and that she is much more at home  \nin her home than she is in her public  \noccupations.\n\nI am told that the Queen's health is  \nslightly better this summer, but she is  \nanxious for",
          "score": 1,
          "created_utc": "2026-02-03 09:21:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bccj4",
          "author": "CompleteSleep2628",
          "text": "Really nice work! Just tested out the chat on huggingface spaces.\n\nMay I ask, what kind of Infrastructure / Hardware you used to train your model?",
          "score": 1,
          "created_utc": "2026-02-03 09:29:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c4hsy",
              "author": "RecalcitrantZak",
              "text": "A100 / Colab and a lot of patience. Colab has a lot of negatives but it helped me to pick up where I left off and keep track of experimental SFT runs. I always made checkpoint runs so I always got to pick up where I left off if something was terminated.",
              "score": 2,
              "created_utc": "2026-02-03 13:16:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qx51zc",
      "title": "OpenClaw with local LLMs - has anyone actually made it work well?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qx51zc/openclaw_with_local_llms_has_anyone_actually_made/",
      "author": "FriendshipRadiant874",
      "created_utc": "2026-02-06 02:14:48",
      "score": 45,
      "num_comments": 115,
      "upvote_ratio": 0.79,
      "text": "I’m honestly done with the Claude API bills. OpenClaw is amazing for that personal agent vibe, but the token burn is just unsustainable. Has anyone here successfully moved their setup to a local backend using Ollama or LM Studio?\n\nI'm curious if Llama 3.1 or something like Qwen2.5-Coder is actually smart enough for the tool-calling without getting stuck in loops. I’d much rather put that API money toward more VRAM than keep sending it to Anthropic. Any tips on getting this running smoothly without the insane latency?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qx51zc/openclaw_with_local_llms_has_anyone_actually_made/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3uapf9",
          "author": "DataGOGO",
          "text": "Yes, but I wouldn’t run it until they fix the code / massive security holes.\n\nVibecoded slop.",
          "score": 41,
          "created_utc": "2026-02-06 03:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3urjku",
              "author": "onethousandmonkey",
              "text": "This is the correct answer.",
              "score": 10,
              "created_utc": "2026-02-06 05:38:01",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3yp2b6",
              "author": "Decent-Freedom5374",
              "text": "Why don’t you just build a model that circulates and isolates, protecting your code only but still allowing quantum gain and make it derive any malicious intent for subsequent gain",
              "score": -2,
              "created_utc": "2026-02-06 20:23:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yvv87",
                  "author": "DataGOGO",
                  "text": "What?",
                  "score": 2,
                  "created_utc": "2026-02-06 20:57:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3zzjxa",
              "author": "HealthyCommunicat",
              "text": "the security flaws that they have are the same exact security flaws we've always had. if you aren't doing ur due dilligence to check every layer of the application to make sure that it just technologically and physically cannot do xyz then you shouldn't really have any security concerns.\n\n  \nin short, you can address the security concerns yourself.",
              "score": -3,
              "created_utc": "2026-02-07 00:29:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o407pfo",
                  "author": "DataGOGO",
                  "text": "You can address the issues yourself, but that involves a rewriting a LOT of the source code…. Most of it in fact.\n\nThere is no way to fix them without recoding. ",
                  "score": 2,
                  "created_utc": "2026-02-07 01:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3v4y4r",
              "author": "Boring-Attorney1992",
              "text": "why do you think it's vibecoded? or is that just your way of criticizing the app?",
              "score": -20,
              "created_utc": "2026-02-06 07:31:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v6fj7",
                  "author": "Thomdesle",
                  "text": "Look up statements of the creator and check out the  source code.",
                  "score": 12,
                  "created_utc": "2026-02-06 07:44:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3vlpfq",
                  "author": "andrewfenn",
                  "text": "The creator literally says on social media that he doesn't even look at the code. When someone demanded he fixed the security issues he just put his hands up 🤷‍♂️ and said he doesn't know what to do. So yes, it's 100% slop and you can't trust that guy at all.",
                  "score": 8,
                  "created_utc": "2026-02-06 10:10:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wo1wf",
                  "author": "noctrex",
                  "text": "Here you go: [https://www.youtube.com/watch?v=8lF7HmQ\\_RgY](https://www.youtube.com/watch?v=8lF7HmQ_RgY)\n\n\"I ship code I don't read\"",
                  "score": 4,
                  "created_utc": "2026-02-06 14:30:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3weldn",
                  "author": "DataGOGO",
                  "text": "…. Did you look at the code?",
                  "score": 1,
                  "created_utc": "2026-02-06 13:40:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u6x0y",
          "author": "regjoe13",
          "text": "I just played with it,  taking over Signal to my local gpt-oss-120b in lmstudio\nI installed openclaw under nologin user on my Linux, locking permissions to a particular folder.  \nIt was fun to play with it, but nothing it does is really worth the risk of having it for me.",
          "score": 7,
          "created_utc": "2026-02-06 03:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vgthg",
          "author": "dragonbornamdguy",
          "text": "Using it with qwen3 coder 30b, its awesome. Setup was undocumented hell. Works very well. He can create own skills only by telling him.",
          "score": 11,
          "created_utc": "2026-02-06 09:23:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zhrso",
              "author": "Technical_Buy_9063",
              "text": "can you share your setup? is it LM Studio?",
              "score": 1,
              "created_utc": "2026-02-06 22:48:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o41icxd",
                  "author": "GreaseMonkey888",
                  "text": "I actually told OpenClaw to configure local LMstudio and Ollama by testing the endpoints of the providers. After some iterations it worked and I could switch over to local providers. At some point I tried to use the working configuration in another VM with OpenClaw, but it had to almost start over configuring it self, although I gave it the config snippets of the previous working one… However, I have a Mac Studio M4 with 64GB, but prefill phase is slow, OpenClaw seems to push so much context into the LLM that it takes very long for every response, no matter how small the model is.",
                  "score": 1,
                  "created_utc": "2026-02-07 06:45:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tywva",
          "author": "NoobMLDude",
          "text": "> I’d much rather put that API money toward more VRAM than keep sending it to Anthropic.\n\nThis is the right way !! 🫡 \nI’m trying to educate more users to realize this and run their own models for free than pay some company that is going to use your data against you in few months.\n\nQwen3Coder or Qwen3Coder-Next is decent for tool calling and agentic uses.\n\n[https://qwen3lm.com/coder-next/](https://qwen3lm.com/coder-next/)\n\nI’ve not used OpenClaw due to the security loopholes discovered.\n\nHowever if you wish to try other more secure uses for Local LLMs, here are a few simple examples\n- Private Meeting Assistant \n- Private Talking Assistant \n- The usual Coding Assistants\n- terminal with AI support \n\n[Local AI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)",
          "score": 17,
          "created_utc": "2026-02-06 02:29:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uicyy",
              "author": "Electronic_Muffin218",
              "text": "Alright, I'll bite - what's the best way to get adequate hardware for these things? Is there some sort of good - better - best (with ballpark prices or not) for nominally consumer-available GPUs (and whatever else matters)? I'm wondering specifically if 48GB is a useful sweet spot, and if so, is there a meaningful performance difference between buying two 24GB cards and just one 48GB card.\n\nIs there a guide to these things that folks keep up to date a la the NUC buyer's guide/spreadsheet? I could of course ask (and have asked) the commercial LLMs themselves, but I'm never sure what they're wrong about or leaving out.",
              "score": 3,
              "created_utc": "2026-02-06 04:31:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uut8l",
                  "author": "NoobMLDude",
                  "text": "TLDR; You can run with whatever device you have available to try it out. \n\nDisclaimer: I’ve not tried OpenClaw, all comments below is for agent workflows that do similar things locally.\n\nAll of the above tools currently run on my MacBook M2 Max 32GB laptop without any additional GPUs.\n\nI was considering upgrading to bigger GPUs but the rate at which open Source models are improving, I think i might not even need to upgrade. \n\nThe smaller models are already decent enough for those tasks. Of course the huge models would perform better for tool-calling, but for me the marginal improvements does not justify the huge costs of hardware.\n\n- 2x24GB VRAM can run the same models as single 48GB VRAM.\n- generally higher the VRAM the larger models you can run\n\nPrices are skyrocketing. So don’t buy before you have tried cheaper alternatives. You might not even notice huge differences.",
                  "score": 2,
                  "created_utc": "2026-02-06 06:03:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3wrg90",
                  "author": "HuckSauce",
                  "text": "Get a AMD Strix Halo mini pc or laptop (Ryzen AI Max) - 128 GB VRAM for 2-3k",
                  "score": 2,
                  "created_utc": "2026-02-06 14:48:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xb72u",
                  "author": "unique-moi",
                  "text": "One thing to keep in mind is that PCs running one GPU are a commodity, while PCs with two high speed PCI slots and a powerful power supply are specialist.",
                  "score": 2,
                  "created_utc": "2026-02-06 16:24:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45rqrz",
              "author": "Dispo96",
              "text": "When you say security loopholes what do you mean?",
              "score": 1,
              "created_utc": "2026-02-07 22:55:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o477sqy",
                  "author": "NoobMLDude",
                  "text": "A lot of issues have been published.  \nTLDR; Giving access to all your accounts (telegram, email, etc) to anyone human or AI is not a good idea. Main issues: Private Data access + exposure to Unsecured content + Ability to externally communicate  \n  \n [https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/](https://www.paloaltonetworks.com/blog/network-security/why-moltbot-may-signal-ai-crisis/)",
                  "score": 1,
                  "created_utc": "2026-02-08 04:24:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o401q80",
              "author": "HealthyCommunicat",
              "text": "the security loopholes in openclaw are the same security loopholes in any agentic bot that has access to a bunch of tools and high autonomy. if you lack the knowledge to see that then you needa go all the way back to the basics and make sure u are fully capable of going through the code and files and making sure that there just isnt anything that can physically be taken advantage of.\n\n  \nexample, if you make sure that it is physically not possible for ur model to run the command \"rm\" or \"rm -rf\" then you wouldnt be worrying about it being able to delete things. if you dont have ur bot able to be reached whatsoever to public internet, then u truly dont have to worry about anything.\n\n  \nlets stop talking about security flaws like they cant be fixed with really easy steps.",
              "score": 1,
              "created_utc": "2026-02-07 00:42:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o47w0xh",
                  "author": "NoobMLDude",
                  "text": "Of course they can be fixed. \n\nIt’s just that people are running these agents without understanding the risks they present.\n\nSo it is the responsibility of the people familiar with the tech to educate and inform those who are not familiar with it, and the responsibility of people running it to understand the risks and apply the fixes.",
                  "score": 1,
                  "created_utc": "2026-02-08 07:47:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u152z",
          "author": "Antique_Juggernaut_7",
          "text": "I got glm4.6v to behave relatively well so far -- been trying it for the past 24h on a dual DGX Spark setup and vLLM. It weirds out at times, but is generally helpful and functional.\n\nI chose this particular model for its image processing capabilities and overall model size. It works for openclaw with a slight change on its chat template.",
          "score": 4,
          "created_utc": "2026-02-06 02:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub6d9",
              "author": "DataGOGO",
              "text": "Try with my 4.6V-NVFP4 quant, it works really well\n\nhttps://huggingface.co/GadflyII/GLM-4.6V-NVFP4",
              "score": 2,
              "created_utc": "2026-02-06 03:43:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uk2bs",
                  "author": "Antique_Juggernaut_7",
                  "text": "Great stuff! Can you share your vllm serve command? I've been having trouble getting NVFP4 to run well in my cluster due to some GB10 shenanigans...\n\nEDIT: wrote before actually checking the HF page. Thanks for adding it there. Are you running this in a DGX Spark?",
                  "score": 1,
                  "created_utc": "2026-02-06 04:43:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3vcgo7",
              "author": "edmerf",
              "text": "I work on DGX Spark with vLLM and made it work with LLama4-Scout-17b-16e-instruct-NVFP4. However I still couldn't manage to find a perfect chat template. Chat flow is really digsusting. What kind of template do you use and how do you derive it to make it work with OpenClaw?",
              "score": 2,
              "created_utc": "2026-02-06 08:41:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vm01u",
                  "author": "Antique_Juggernaut_7",
                  "text": "The issue with running GLM4.6 is that OpenClaw expects a \"developer\" role, but GLM4.6's chat template only accepts \"system\". So you just need to change that particular line in the chat template to make it run.",
                  "score": 1,
                  "created_utc": "2026-02-06 10:12:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3x9w9v",
              "author": "unique-moi",
              "text": "How about Claude code self-hosted (I mean pointing at a self-hosted LLM)?",
              "score": 1,
              "created_utc": "2026-02-06 16:18:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xo6yq",
                  "author": "Antique_Juggernaut_7",
                  "text": "Haven't tried yet. Will try and see how it behaves.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44n7dp",
              "author": "scottybowl",
              "text": "Can I ask how you got vllm working? Struggling with the configuration (also using a DGX Spark)",
              "score": 1,
              "created_utc": "2026-02-07 19:16:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o44qz1d",
                  "author": "Antique_Juggernaut_7",
                  "text": "Do you know if you are you having trouble strictly with vLLM, or with getting openclaw to work nice with vLLM?\n\nI'm running vLLM using the community Docker container from eugr: [https://github.com/eugr/spark-vllm-docker](https://github.com/eugr/spark-vllm-docker)  \nIf you only wish to run GLM4.5/4.6V on vLLM (ie not running with openclaw), you can try the following command -- note that the vllm serve command is baked in, so you can also try that directly if you already have vllm installed somewhere (note also: if you're not running a cluster of Sparks, adapt accordingly):  \n\\`\\`\\`  \n./launch-cluster.sh  \\\\\n\nexec vllm serve zai-org/GLM-4.6V-FP8 \\\\\n\n\\--port 8000 \\\\\n\n\\--host [0.0.0.0](http://0.0.0.0) \\\\\n\n\\--gpu-memory-utilization 0.75 \\\\\n\n\\-tp 2  \\\\\n\n\\--distributed-executor-backend ray  \\\\\n\n\\--max-model-len 128000  \\\\\n\n\\--load-format fastsafetensors  \\\\\n\n\\--enable-auto-tool-choice \\\\\n\n\\--tool-call-parser glm45  \\\\\n\n\\--reasoning-parser glm45  \\\\\n\n\\--trust-remote-code \\\\\n\n\\--allowed-local-media-path / \\\\\n\n\\--mm-encoder-tp-mode data  \n\\`\\`\\`\n\nBUT!\n\nOpenclaw at this point requires a chat template that accepts the \"developer\" role. GLM4.5 and 4.6 follow a chat template that only accepts the \"system\" role. So you will need to provide vLLM with a custom chat template for this to work.\n\nEDIT: went ahead and pushed to Github the instructions on how to make GLM4.5/4.6 on vLLM to talk to OpenClaw: [https://github.com/fidecastro/fix\\_glm46v/](https://github.com/fidecastro/fix_glm46v/)",
                  "score": 1,
                  "created_utc": "2026-02-07 19:36:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u1kyv",
          "author": "SillyLilBear",
          "text": "I run it with M2.1 4 bit locally, works well.",
          "score": 4,
          "created_utc": "2026-02-06 02:45:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u32xg",
          "author": "shigeru777",
          "text": "Try qwen3-coder-next, better inference speed than GLM-4.7-FLASH,  but still too hard to use tool/skill calling. I only use openclaw for chat and weather information / brave api search.",
          "score": 5,
          "created_utc": "2026-02-06 02:53:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o451aki",
              "author": "chimph",
              "text": "why not just run open webui with web search then?\n\n",
              "score": 2,
              "created_utc": "2026-02-07 20:31:12",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xnv6k",
              "author": "cashedbets",
              "text": "What do you have qwen3-coder-next running on?",
              "score": 1,
              "created_utc": "2026-02-06 17:23:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4026ou",
                  "author": "shigeru777",
                  "text": "Mac Studio M3 ultra 256GB",
                  "score": 1,
                  "created_utc": "2026-02-07 00:44:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tzbru",
          "author": "piddlefaffle12",
          "text": "Spent a few days on this with my 5090 and M4 Max 128GB.\n\nOnly model that kinda worked is glm-4.7-flash. Prompt pre-processing is going to be the performance killer for self hosted agentic in my experience.  \n",
          "score": 3,
          "created_utc": "2026-02-06 02:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3uayjz",
              "author": "DataGOGO",
              "text": "Depends on your hardware and hosting configuration. ",
              "score": 1,
              "created_utc": "2026-02-06 03:42:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3u3n9g",
          "author": "FinancialMoney6969",
          "text": "I keep fucking mine up. I’ve tried everything even LM studio…",
          "score": 3,
          "created_utc": "2026-02-06 02:57:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ub9q7",
              "author": "DataGOGO",
              "text": "LMstudio is pretty trash dude. ",
              "score": 0,
              "created_utc": "2026-02-06 03:44:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ufq1z",
                  "author": "FinancialMoney6969",
                  "text": "What’re you using?",
                  "score": 2,
                  "created_utc": "2026-02-06 04:13:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3xg80c",
              "author": "DarkZ3r0o",
              "text": "Try with ollama the difference is huge",
              "score": 0,
              "created_utc": "2026-02-06 16:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4001jm",
                  "author": "FinancialMoney6969",
                  "text": "That’s where I’m having the problem… I am trying on windows tho",
                  "score": 1,
                  "created_utc": "2026-02-07 00:32:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3ud8xk",
          "author": "kdd123456789",
          "text": "If we setup kimi on hetzner vps running openclaw locally, what kind of costs would be involved, as the cost of the hardware to run a descent llm locally is pretty expensive.",
          "score": 3,
          "created_utc": "2026-02-06 03:57:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7h9x",
          "author": "Gargle-Loaf-Spunk",
          "text": "You mean it's supposed to work at all? ",
          "score": 3,
          "created_utc": "2026-02-06 07:54:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3y9ame",
          "author": "Professional_Owl5603",
          "text": "I have a question, I know Claw is a security nightare but I dont need it to do half the things people say it could. I essentaill want a bot that can help me to research on thing. Example: I'll talk to Grok (yea I know, but if I need spicy, I go there, everythign else is Gemini for anythign serious) and will discuss something I saw on youtube, like a new LLM or API or whatever. Like the new Nvidia Personoplex. I woudl like to have the bot go and research it for me, check the gitgub and see if it can be intergrated into itself. Obviously, this is an extreme situation, but along these lines. \n\n  \nThe reason why I thought this was possible was becasue I was tryign to get it to work with discord so I can talk to it that way, and when I was testing it via Claude Opus, I asked it to help me configure it so it would work the way I wanted to. It just did it. And when it hit problems, it kept trying things, which is GREAT, however, the openwebui credits I have for over a year of 4.35c that I've been using that lasted me forever, was drained in minutes to .35c apparently soakign though hundreds of thousands of tokens. Which is nuts.\n\n  \nSo my take is, claude is great and works as advertised, at the cost of a liver and partial kidney per hour. I realize there isnt a comparable model that's open source, but I'm wondering if I can get close? With those abilities? My rig is pretty basic, I have an older Gigabyte X99P-sli Motherbaord with 225gb of ram, that has pci 3x slots and dual rtx 5090's that I use for minecraft, with ollama, so I have 64gb of pooled vram using ollama. I get about 30tps usign a 70b model. Which im guessign inhundreds if times slower than the cloud API.\n\n  \nAm I just dreaming here? Would a machien like DGX spark make a better machine? I'm guess it probably wouldnt as it just has x2 the vram and nothing would change other than the model and maybe a lower tps even. And yes I knwo giving it access to this machine is dangerous, Ive installed it closed wsl enviorment. I dont plan to give it access to anything and stricly want to use it as a chat bot springboard research assistant. I manage my own calendar.\n\nAm I wasting my time? Thanks for the advice in advance.\n\n![gif](giphy|1D7ryE8SDYuq8kGGGQ)",
          "score": 3,
          "created_utc": "2026-02-06 19:05:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xfpom",
          "author": "DarkZ3r0o",
          "text": "I tested it with glm-4.7-flash with 35k context and gpt-oss-20b with 120k context and am really satisfied with results. I have 3090ti",
          "score": 2,
          "created_utc": "2026-02-06 16:45:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40he1v",
          "author": "Long_Complex_4395",
          "text": "I used LMStudio with Qwen2.5 instruct. I wrote on how to set it up\n\nhttps://medium.com/@nwosunneoma/how-to-setup-openclaw-with-lmstudio-1960a8046f6b",
          "score": 2,
          "created_utc": "2026-02-07 02:17:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u0w0b",
          "author": "Battle-Chimp",
          "text": "All these OpenClaw posts just prove that smart people still do really, really dumb things.\n\nDon't install OpenClaw. ",
          "score": 7,
          "created_utc": "2026-02-06 02:41:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u4hcc",
              "author": "actadgplus",
              "text": "All these OpenClaw posts just prove that smart people still post really, really dumb things.\n\nDo your research and install OpenClaw.",
              "score": -1,
              "created_utc": "2026-02-06 03:02:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3u5dpr",
                  "author": "Momo--Sama",
                  "text": "I have it running on a separate mini pc with a kimi sub, and its definitely fun to mess around with, but there's not a lot I can actually do with it while refusing to give it access to any of my personal accounts. Maybe I'm just not being creative enough, idk",
                  "score": 2,
                  "created_utc": "2026-02-06 03:07:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3uavlu",
                  "author": "DataGOGO",
                  "text": "I wouldn’t. ",
                  "score": 2,
                  "created_utc": "2026-02-06 03:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3up97b",
          "author": "IngwiePhoenix",
          "text": "Really want to try it myself to see how far it can go - but I fear my singular 4090 is not going to go that far... x)\n\nI hear Qwen3-Coder (and it's -Next variant) are really good. In general, tool-call optimized models like the recent GLMs and such should do well.\n\nIn theory, anyway.",
          "score": 1,
          "created_utc": "2026-02-06 05:20:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v0oia",
          "author": "SEND_ME_YOUR_ASSPICS",
          "text": "Any recommendations for 32 RAM and 16 VRAM?",
          "score": 1,
          "created_utc": "2026-02-06 06:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wm2rq",
              "author": "mzinz",
              "text": "Gpt-oss:20b. One of the recommended models straight from the ollama docs",
              "score": 1,
              "created_utc": "2026-02-06 14:20:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3vjo6c",
          "author": "ifheartsweregold",
          "text": "Yeah working really well I just got it set up with dual DGX Sparks running Minimax 2.1",
          "score": 1,
          "created_utc": "2026-02-06 09:51:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vm0il",
          "author": "prusswan",
          "text": "I wanted a tool like this but only as a guidance rather than having broad executive powers - it is too much of a security burden (can't give it free reign, and whatever it does needs to have audit trail). Open to suggestions",
          "score": 1,
          "created_utc": "2026-02-06 10:13:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vm7ac",
          "author": "lhodhy",
          "text": "NO\\_REPLY",
          "score": 1,
          "created_utc": "2026-02-06 10:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vndrj",
          "author": "Zevatronn",
          "text": "I run qwen 8b with openclaw and qwebcoder 30b local models are used ny sub agent while the 'conductor' runs on a chatgpt sub it works fine",
          "score": 1,
          "created_utc": "2026-02-06 10:25:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yasxu",
          "author": "w3rti",
          "text": "I made it work once, it was perfect, write codes, write apps, adjust setup, performance,. Clawd just did everything for me. After graphic card update and some changes it went garbage. I hat 5 days of fun, still keeping those .mds and sessesion, when he will work with the llm like this again, we can continue",
          "score": 1,
          "created_utc": "2026-02-06 19:12:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yoeod",
          "author": "Decent-Freedom5374",
          "text": "I use 8 ram and use the new release ollama gave of awencoder inside for free. Works great same project multiple terminals. Why",
          "score": 1,
          "created_utc": "2026-02-06 20:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zz2qe",
          "author": "HealthyCommunicat",
          "text": "yes it is. and its easy.\n\nif ur looking at qwen 2.5 and llama 3.1 you do not have the required level of information throughput. this is a space that is ever changing at a pace no other field has moved in before. the models we had a year ago (qwen 2.5 as you say) are leagues leagues less capable than the model that just came out, qwen 3 coder next 80b (when comparing to 72b or 70b qwen 2.5) literally feels like an entirely different kind of tech. one can write files and access emails and search the web, one cant even run a simple find command.\n\nif u put in the work and learn ground up instead of trying to rush in and expect results, then you'd come to see very easily that this field requires really high levels of information intake on a daily basis. to top it off, this is a niche that requires u to have a minimum of 5-10k to even touch a model that feels somewhat capable.\n\n  \nwhat makes it even worse is reading that u do in fact have a claude subscription. if u were dilligent you would've used it in combination with ur own local models to better learn how you can utilize these models. if you cared you would've already asked claude to help you with this setup.",
          "score": 1,
          "created_utc": "2026-02-07 00:27:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43b20q",
          "author": "FrankWanders",
          "text": "Anyone here who has experience with Ministral 3 14B? It supports up to 256K context, but with 128K context it seems to be just small enough into one RTX 5090 without offloading, so this would make it doable.\n\nCompared to Flash 4.7 I think ministral 3 14B is the winner,  or am I wrong? To be fair I don't see any other option to run it on local hardware? Open to suggestions!",
          "score": 1,
          "created_utc": "2026-02-07 15:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ekq5u",
          "author": "Pains_asistant",
          "text": "Yo lo estoy configurando actualmente y llevaba unos dias pegandome porque al parecer cado modelo cambia un poco en el tema del JSON de openclaw (o eso me parece a mi) pero encontre un comando (solo para configurar el LLM local) luego configurar claw ya es otra historia.   \nUna vez tienes configurado e instalado el bot puedes ejecutar este comando en la bash:  \nollama launch openclaw  \nCon ese comando ya te manda seleccionar el modelo de ollama que quieres de los que trienes instalados o de los que te recomeinda y se configura automatico (cosa que me vi negro para hacer manual yo mismo).  \nPor otro lado tengo en local el llama 3.1 8b y no consigo que ese  modelo me responda bien... pero es que por lo menos reponde que el dolphin o el mannix no responden y no los consigo configurar (manualmente todo hasta ahora que acabo de descubir el comando de ollama y de momento solo pude probar ese modelo utilizable).  \nTengo miedo a que sea por la capacidad del modelo lo que me esta pasando porque no me salen logs de error en el dashboard ni en el status de claw\n\nhttps://preview.redd.it/9g4kctlvnfig1.png?width=962&format=png&auto=webp&s=27edfb69558ff2ad7191172b45f2527fb45fed5c\n\nY con este modelo probando a que me respondiese mal (se desinstalo completamente del equipo a si mismo osea que imaginate el poder que tiene y lo loco que esta jaja) recomiendo hacerlo en un pc aislado y con sus propios recursos de redes y correo.  \nPor cierto acabo de comprobar que despues se te va a lanzar automaticamente con el modelo que selecciones la primera vez que hagas el lauch, estoy mirando como cambiarlo porque esta config de modelo auto es mil veces mas comoda, peleandome con el json no di hecho ",
          "score": 1,
          "created_utc": "2026-02-09 08:57:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uff1h",
          "author": "HenkPoley",
          "text": "Even the gigantic open weights models are 9 months behind the closed source models (\"always have been\"). The models of Anthropic and OpenAI only recently got to the level where they can work autonomously for a bit. Claude Opus 4.5 was released on 24 November 2025, GPT-5.1 on November 12, 2025.\n\nYou'll have to wait till mid august, and have a very beefy machine by then.\n\nBtw, clawdbot(-descendents) are conceptually fun, but in reality not that interesting.",
          "score": 1,
          "created_utc": "2026-02-06 04:11:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uy3lz",
          "author": "grumpycylon",
          "text": "I tried OpenClaw with Llama 3.1 and it was spewing nonsense. I typed hi in the chat and it kept typing giant paragraphs of garbage.",
          "score": 1,
          "created_utc": "2026-02-06 06:31:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vmrtq",
          "author": "RevealIndividual7567",
          "text": "I would highly recommend not running openclaw, or if you have to then running it in a sandbox with very limited external websites and resources allowed. It is a security nightmare due to things like website prompt injection.",
          "score": 1,
          "created_utc": "2026-02-06 10:20:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u581c",
          "author": "actadgplus",
          "text": "I’m have a really powerful Mac Studio M3 Ultra with 256GB RAM so testing out various models via LM Studio.  I haven’t leaned on anything yet.\n\nIn parallel I have been exploring also leveraging Synthetic.  Has anyone given it a try?  Thoughts?\n\nhttps://synthetic.new",
          "score": -3,
          "created_utc": "2026-02-06 03:06:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvu651",
      "title": "Fine-tuned Gemma 3 270M to detect \"AI slop\" - runs in a browser extension",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qvu651/finetuned_gemma_3_270m_to_detect_ai_slop_runs_in/",
      "author": "maciejgryka",
      "created_utc": "2026-02-04 17:04:16",
      "score": 45,
      "num_comments": 14,
      "upvote_ratio": 0.92,
      "text": "https://preview.redd.it/7vhtcf7ydihg1.png?width=1024&format=png&auto=webp&s=3da874322690b25ae002dbbbcd37cd6edf47a181\n\nBeen working on a small project to detect AI-generated \"slop\" text. The goal was simple: make something that runs locally, fits in a browser extension, and doesn't require sending your text anywhere.\n\n\\*\\*The approach:\\*\\*\n\nWe used knowledge distillation to compress a 120B teacher model into Gemma 3 270M. The base Gemma model scores \\~40% on our test set (random guessing on binary classification). After fine-tuning with \\~10k examples distilled from the teacher, the student matches teacher performance on held-out data.\n\nTraining data was based on the \\[Kaggle AI-generated essays dataset\\](https://www.kaggle.com/datasets/denvermagtibay/ai-generated-essays-dataset), expanded via distillation. So the model is specifically tuned for that style of content, though it generalizes reasonably well to other formats.\n\nFor browser deployment, we quantized to Q4\\_K\\_M (\\~242 MB). Accuracy drops from 100% to \\~95% on our test set - that's the tradeoff for fitting in a Chrome extension.\n\n\\*\\*Results:\\*\\*\n\n| Model | Size | Test Accuracy |  \n|-------|------|---------------|  \n| GPT OSS 120B (teacher) | \\~120B | 100% (20/20) |  \n| Gemma 3 270M (fine-tuned) | 270M | 100% (20/20) |  \n| Gemma 3 270M Q4\\_K\\_M | \\~242 MB | 95% (19/20) |  \n| Gemma 3 270M (base) | 270M | \\~40% |\n\nReal-world testing on content outside the training domain:\n\n| Content Type | Accuracy |  \n|--------------|----------|  \n| ChatGPT outputs (n=50) | 98% |  \n| Human tweets (n=50) | 94% |  \n| Reddit comments (n=100+) | \\~92% |  \n| Formal emails (n=30) | 88% |\n\nFormal human writing (business emails, academic text) is where it struggles most - too much stylistic overlap with AI output.\n\n\\*\\*Limitations:\\*\\*\n\n\\- \\~1 in 20 predictions wrong after quantization  \n\\- Trained on essays, so may miss AI patterns in other domains  \n\\- Formal human writing triggers false positives  \n\\- First load downloads \\~253 MB (cached after)  \n\\- Inference \\~0.5-2s on CPU\n\n\\*\\*Tech stack:\\*\\*\n\nThe extension runs via \\[Wllama\\](https://github.com/niconielsen32/wllama) (llama.cpp in WebAssembly). No API calls, works offline after initial model download. Fine-tuning was done with LoRA.\n\n\\*\\*Links:\\*\\*\n\n\\- Repo: [https://github.com/distil-labs/distil-ai-slop-detector](https://github.com/distil-labs/distil-ai-slop-detector)  \n\\- Model weights: [https://huggingface.co/distil-labs/distil-ai-slop-detector-gemma](https://huggingface.co/distil-labs/distil-ai-slop-detector-gemma)\n\nHappy to answer questions about the training setup, LoRA config, or browser deployment.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvu651/finetuned_gemma_3_270m_to_detect_ai_slop_runs_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3kavyz",
          "author": "NoobMLDude",
          "text": "Cool use case for a tiny model. \n\nI think finetuned tiny models like these are the future. Can be used to solve many such problems that require running models locally in constrained hardware /  edge.",
          "score": 11,
          "created_utc": "2026-02-04 17:23:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l3mjd",
              "author": "maciejgryka",
              "text": "Couldn't agree more, this is basically the thesis behind our platform!",
              "score": 5,
              "created_utc": "2026-02-04 19:34:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3krqyi",
          "author": "ImOutOfIceCream",
          "text": "“Formal human writing triggers false positives” are you sure you didn’t train a model to detect formal writing",
          "score": 5,
          "created_utc": "2026-02-04 18:40:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l3j6p",
              "author": "maciejgryka",
              "text": "I think this is a legit worry :) FWIW we just used the existing dataset and trained a low-param-count classifier to work on that data set, which was the main point we wanted to show. We should probably de-emphasize some claims of the usefulness of the demo tool itself and focus more on the stack behind it!",
              "score": 1,
              "created_utc": "2026-02-04 19:34:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kpvpk",
          "author": "former_farmer",
          "text": "Thanks, good idea. Should come by default in Reddit, Twitter, etc.",
          "score": 2,
          "created_utc": "2026-02-04 18:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lfnye",
          "author": "GlassAd7618",
          "text": "Wow, nice!  \n  \n\\> After fine-tuning with \\~10k examples distilled from the teacher, the student matches teacher performance on held-out data.\n\nIt always amazes me how well this approach works. If you think of it, 10K examples is nothing compared to the vast amount of data used to train the teacher. It is somehow always a wow-moment to see how well the distilled models perform.",
          "score": 2,
          "created_utc": "2026-02-04 20:32:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lnlav",
              "author": "maciejgryka",
              "text": "agreed, it feels pretty magical",
              "score": 2,
              "created_utc": "2026-02-04 21:09:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3klzlb",
          "author": "SourceCodeplz",
          "text": "Hey great project! I don't see a License file in your Github repo.",
          "score": 1,
          "created_utc": "2026-02-04 18:14:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l5osk",
              "author": "maciejgryka",
              "text": "Thanks, added now! (Apache 2)",
              "score": 1,
              "created_utc": "2026-02-04 19:44:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kp9h4",
          "author": "v01dm4n",
          "text": "What does it say about this post? /s\n\nOn a serious note, did you evaluate your model on any of the benchmark datasets: M4, MAGE, RAID, etc?",
          "score": 1,
          "created_utc": "2026-02-04 18:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l5u0i",
              "author": "maciejgryka",
              "text": "I did ran it before posting and the result did make me chuckle :)\n\nWe didn't, but that's a good idea!",
              "score": 2,
              "created_utc": "2026-02-04 19:45:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3krh4z",
          "author": "former_farmer",
          "text": "Do I need to install anything else in order to make it work? or only install the browser addon? or install also the platform to host the model.\n\nPS: Fix the link in readme, should go to [https://github.com/ngxson/wllama](https://github.com/ngxson/wllama), it's currently giving 404.",
          "score": 1,
          "created_utc": "2026-02-04 18:39:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3krs4p",
              "author": "maciejgryka",
              "text": "You just need the extension, which will download the ~300MB model when started",
              "score": 1,
              "created_utc": "2026-02-04 18:40:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kgsiy",
          "author": "jschw217",
          "text": "Cool idea 👍",
          "score": 1,
          "created_utc": "2026-02-04 17:51:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyl7ac",
      "title": "What 's a realistic comparison betwee what you can run on a 512GB ram M3 Ultra vs frontier models",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qyl7ac/what_s_a_realistic_comparison_betwee_what_you_can/",
      "author": "rwvyf",
      "created_utc": "2026-02-07 18:22:06",
      "score": 45,
      "num_comments": 23,
      "upvote_ratio": 1.0,
      "text": "I’m looking for real-world impressions from the \"high-RAM\" club (256GB/512GB M3 Ultra owners). If you've been running the heavyweights locally, how do they actually stack up against the latest frontier models (Opus 4.5, Sonnet 4.5, Geimini 3 pro etc)\n\n* Coding in a relative large codebase, python backend, javascript front end \n* Best-quality outputs (not speed) for RAG over financial research/report + trading idea generation, where I focus on:\n   * (1) data quality + retrieval, \n   * (2) verification/compute\n   * (3) a multi-pass reasoning pipeline.",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qyl7ac/what_s_a_realistic_comparison_betwee_what_you_can/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o44qdz3",
          "author": "TokenRingAI",
          "text": "Dual M3 Ultra 512GB will allow you to run Kimi 2.5 at a reasonable speed, which is 100% a frontier model.\n\nIf M5 ultra shows up with a bump in compute, memory bandwidth, 768G ram, and a 15K price tag, which seem like reasonable possibilities, then we will have frontier level performance on a single mac",
          "score": 41,
          "created_utc": "2026-02-07 19:33:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o455aeg",
              "author": "SpicyWangz",
              "text": "768 M5 ultra would go so hard. That could start seriously eating into nvidia’s prosumer market share. ",
              "score": 17,
              "created_utc": "2026-02-07 20:52:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45tpnn",
                  "author": "TokenRingAI",
                  "text": "If the M5 Ultra utilizes the memory speeds of the M5 generation while maintaining the 1024-bit bus width of the M3 Ultra, it would achieve a theoretical memory bandwidth of approximately 1,224 GB/s",
                  "score": 13,
                  "created_utc": "2026-02-07 23:07:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45gipz",
                  "author": "segmond",
                  "text": "NOT could, but WOULD",
                  "score": 6,
                  "created_utc": "2026-02-07 21:53:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o45zeep",
              "author": "mewnor",
              "text": "Don’t get me excited like that.",
              "score": 3,
              "created_utc": "2026-02-07 23:43:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ytow",
          "author": "false79",
          "text": "I would not allow any LLM to do backtesting. You're just asking for trouble when it's real-time and the results are so far apart.",
          "score": 6,
          "created_utc": "2026-02-07 20:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44z57p",
              "author": "rwvyf",
              "text": "That’s for sure. Thats what the codebase is for.",
              "score": 4,
              "created_utc": "2026-02-07 20:19:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o478wqg",
          "author": "Technical_Buy_9063",
          "text": "I run a single m3 ultra v3 512gb - I run Qwen-3-coder-next as a virtual assistant - clearly some delay, but for now I'm getting all of my LLM use out of it. I use ChatGPT 5.3 to craft some prompts and such for it, but this is more of a expediting measure than anything. Not exactly what you're asking but also I'm not fully optimized at all - this is just a raw setup for now. ",
          "score": 4,
          "created_utc": "2026-02-08 04:32:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ggcx",
          "author": "segmond",
          "text": "Kimi K2.5 & DeepSeekv3.2 are just as good as SOTA and can 100% serve as substitutes.\n\ngpt-oss-120b, GLM4.7, MiniMax2.1 will handle 90% of SOTA needs.",
          "score": 6,
          "created_utc": "2026-02-07 21:52:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46olbb",
          "author": "desexmachina",
          "text": "Do you guys trust it without web context?",
          "score": 2,
          "created_utc": "2026-02-08 02:18:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o49cjz6",
              "author": "AnxietyPrudent1425",
              "text": "That’s why you add web context in LM Studio",
              "score": 2,
              "created_utc": "2026-02-08 14:49:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48gmqo",
          "author": "Hector_Rvkp",
          "text": "I've been comparing hardware options from Strix halo to stuff costing 15k$. I don't think 256+gb apple vram makes sense currently because the bandwidth is too low, even at c.1000gb/s. Token speed is bandwidth/model size, so you can't run a dense model, obviously, but even MoE models that take 128gb+ call layers so big that token speed is still unusable. Afaik, sweet spot is moe model fitting on 128gb ram. The penalty seems to be 5-25pc performance, and what that means in practice is anybody's guess, there's no standard. But comparing quality without factoring in speed / usability is somewhat irrelevant.",
          "score": 2,
          "created_utc": "2026-02-08 11:02:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ael0o",
              "author": "JonasTecs",
              "text": "So what is best from your comparison?",
              "score": 2,
              "created_utc": "2026-02-08 17:58:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o44gz5w",
          "author": "Powerful-Street",
          "text": "It’s shit compared to Claude or Gemini. Context is limited so much, that a large codebase gets hallucinated. Nothing local comes close.",
          "score": -3,
          "created_utc": "2026-02-07 18:45:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44vnh9",
              "author": "redditor_420_69_lol",
              "text": "Claude and Gemini are just doing context compression and pretending to have super long context, which you can totally do locally",
              "score": 11,
              "created_utc": "2026-02-07 20:00:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o454vhq",
                  "author": "XccesSv2",
                  "text": "How can i Do this locally?",
                  "score": 1,
                  "created_utc": "2026-02-07 20:50:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o49vapy",
                  "author": "Powerful-Street",
                  "text": " When you have a program with 10k lines of spec, local rag, doesn’t have near enough information for the model to find the correct context. I have tried other things and all will get lost if you don’t know what you are pointing them to. Also, Gemini Ultra has much more context, and it still gets it wrong, most times. I now use one instance of Gemini or Claude to plan a debug session and will use another instance, to follow in the past of the last—if the knowledge base is clean. None are perfect in large codebase, which is what the OP asked about. I have found ways to work with Claude code or Gemini on its ultra plan. Local with RAG doesn’t even touch them yet.",
                  "score": 1,
                  "created_utc": "2026-02-08 16:25:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o44i403",
              "author": "rwvyf",
              "text": "That’s sad to hear",
              "score": 0,
              "created_utc": "2026-02-07 18:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44d18i",
          "author": "ComfyUser48",
          "text": "Following",
          "score": 0,
          "created_utc": "2026-02-07 18:26:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyjztp",
      "title": "I built a local‑first RPG engine for LLMs (beta) — UPF (Unlimited Possibilities Framework)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qyjztp/i_built_a_localfirst_rpg_engine_for_llms_beta_upf/",
      "author": "Gohzio",
      "created_utc": "2026-02-07 17:36:54",
      "score": 38,
      "num_comments": 35,
      "upvote_ratio": 0.88,
      "text": "Hey everyone,\n\nI want to share a hobby project I’ve been building: **Unlimited Possibilities Framework (UPF)** — a local‑first, stateful RPG engine driven by LLMs.\n\nI’m **not a programmer by trade**. This started as a personal project to **help me learn how to program**, and it slowly grew into something I felt worth sharing. It’s still a **beta**, but it’s already playable and surprisingly stable.\n\n# What it is\n\nUPF isn’t a chat UI. It’s an **RPG engine** with actual **game state** that the LLM can’t directly mutate. The LLM proposes changes; the engine applies them via structured events. That means:\n\n* **Party members, quests, inventory, NPCs, factions, etc.** are tracked in state.\n* Changes are applied through JSON events, so the game doesn’t “forget” the world.\n* It’s local‑first, inspectable, and designed to stay coherent as the story grows.\n\n# Why you might want it\n\nIf you love emergent storytelling but hate losing context, this is the point:\n\n* The engine **removes reliance on context** by keeping the world in a structured state.\n* You can **lock fields** you don’t want the LLM to overwrite.\n* It’s built for long‑form campaigns, not just short chats.\n* You get RPG‑like continuity without writing a full game.\n\n# Backends\n\nMy **favourite backend is LM Studio**, and that’s why it’s the priority in the app, but you can also use:\n\n* **text-generation-webui**\n* **Ollama**\n\n# Model guidance (important)\n\nI’ve tested with models **under 12B** and I **strongly recommend not using them**. The whole point of UPF is to reduce reliance on context, not to force tiny models to hallucinate their way through a story. You’ll get the best results if you use your **favorite 12B+ model**.\n\n# Why I’m sharing\n\nThis has been a learning project for me and I’d love to see other people build worlds with it, break it, and improve it. If you try it, I’d love feedback — especially around model setup and story quality.\n\nIf this sounds interesting, this is my repo  \n[https://github.com/Gohzio/Unlimited\\_possibilies\\_framework](https://github.com/Gohzio/Unlimited_possibilies_framework)\n\nThanks for reading.",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qyjztp/i_built_a_localfirst_rpg_engine_for_llms_beta_upf/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o444m8w",
          "author": "dmitryplyaskin",
          "text": "https://preview.redd.it/vvx2zxhn04ig1.png?width=1256&format=png&auto=webp&s=9826e5995f16fa5328ca3db135866a66049c260d\n\nThe repository is not available at this link.",
          "score": 1,
          "created_utc": "2026-02-07 17:44:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o444qiq",
              "author": "Gohzio",
              "text": "It would be a good idea for me to open it. thanks!",
              "score": 3,
              "created_utc": "2026-02-07 17:45:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o445dap",
              "author": "Gohzio",
              "text": "fixed",
              "score": 3,
              "created_utc": "2026-02-07 17:48:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o445jok",
              "author": "drumzalot_guitar",
              "text": "Agreed, same result.  Any amount of searching doesn’t turn anything up.",
              "score": 1,
              "created_utc": "2026-02-07 17:49:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o447mz7",
                  "author": "Gohzio",
                  "text": "fixed",
                  "score": 2,
                  "created_utc": "2026-02-07 17:59:44",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o444q0i",
          "author": "ghostd93",
          "text": "The link does not work",
          "score": 1,
          "created_utc": "2026-02-07 17:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o445cos",
              "author": "Gohzio",
              "text": "fixed",
              "score": 3,
              "created_utc": "2026-02-07 17:48:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o445r25",
                  "author": "ghostd93",
                  "text": "Thanks. Looks promising. I will definitely check it out",
                  "score": 1,
                  "created_utc": "2026-02-07 17:50:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44axih",
          "author": "Oshden",
          "text": "Awesome work man!!!",
          "score": 1,
          "created_utc": "2026-02-07 18:16:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44n7vj",
          "author": "Aionion",
          "text": "curious, does it allow api's for other services such as OpenAi, Anthropic, NateGPT, Moonshot, etc.?\n\n",
          "score": 1,
          "created_utc": "2026-02-07 19:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44nd65",
              "author": "Gohzio",
              "text": "It's fully OpenAI compatible.",
              "score": 2,
              "created_utc": "2026-02-07 19:17:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o45hsvv",
          "author": "DHFranklin",
          "text": "Cool. I did something similar, I've been experimenting in AI Studio since April doing a lot of this.\n\nI have Custom Instructions for the game play and found that systems it's really familiar with like d20 work better than bespoke ones. I then drag and drop RAG files as plain text and JSON and haven't had much trouble with one more than the other. I tell it to rely on python for the math and then relate it narratively for \"critical failure, failure, success, critical success\" However it doesn't really work out. It won't let a character lose.\n\nI have had tones of problem with things like tone and consistent character \"voice\". As it plays out it changes format for game state. It has problems with context bleed and context rot especially at token lengths over 150-200k tokens or so.\n\nHow does this compare?",
          "score": 1,
          "created_utc": "2026-02-07 22:00:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45qfwa",
              "author": "Gohzio",
              "text": "Yes, I know this well. My first 2 tries ended similarly. My first try just shuffled things in and out of context but the LLM just went insane. Second I tried splitting the processing to a back end server type thing but the processing power needed started to ramp up and basically only would work if you connected it to chat gpt or something and had massive context lengths (literally the opposite to what I wanted)\n\nWith this I started with 100% Rust and tried to make the LLM output tags for everything so the engine could read them and process them accordingly. This was the best yet but my keywords list went straight over 300 words and still missed stuff due to how conversations work.\n\nThat's when I read about context requests  in the open ai format. I pivoted to that and it works amazingly. The LLM doesn't need to remember a damn thing it just requests the information it needs to build an output. So then the world data is basically the system prompt and the engine combines the players input and relevant data from the player tab and if the LLM needs to know something like how loot drops work it will just request that data adding delay but removing context needs and so far the LLM has stayed sane.\n\nI've gotten as far as I can in solo testing and as I neither know anyone on Linux or with a GPU as powerful as mine (5080) I decided to let Reddit destroy me 😂.",
              "score": 1,
              "created_utc": "2026-02-07 22:48:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45ymob",
                  "author": "DHFranklin",
                  "text": "Oh geez. Yeah that sounds like a hell of a work around. No I don't have a GPU that hefty. I wanted to ask the speed of turn around on prompts, but I guess that answer wouldn't be terribly useful.\n\nDo you do anything that needs a RNG? How are you handling that? What I'm getting out of Gemini 2.5 and 3 (admittedly all I've tried) is that it will see the instruction to run python for random numbers, do math sometimes and sometimes not, then just auto-win anyway.",
                  "score": 1,
                  "created_utc": "2026-02-07 23:38:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47k6lx",
          "author": "TeamDman",
          "text": "Would be good to add some screenshots to the post and readme",
          "score": 1,
          "created_utc": "2026-02-08 06:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48cb8x",
          "author": "AnotherFuckingSheep",
          "text": "I don't get it. Do you have a demo?",
          "score": 1,
          "created_utc": "2026-02-08 10:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48gqeg",
              "author": "Gohzio",
              "text": "It's fully usable in the git.",
              "score": 1,
              "created_utc": "2026-02-08 11:03:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48zmb7",
                  "author": "AnotherFuckingSheep",
                  "text": "This is totally not my domain. From the post you did it sounds like this is something that can actually run and give users results. \n\nFor me running this would probably take a couple of days.\n\nMaybe you can post a chat, a game text, or something else to just show off what it actually does.\n\nAgain not my domain so if this is actually impossible you can just tell me",
                  "score": 1,
                  "created_utc": "2026-02-08 13:33:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4aco5t",
          "author": "Narrow_Operation7252",
          "text": "Tried it out, but I keep getting a completion error when I try to send a prompt to my local LLM (with both LM Studio and Ollama). It says “LLM error: error sending request for url”. Any idea what that might be?",
          "score": 1,
          "created_utc": "2026-02-08 17:49:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ao0u7",
              "author": "Gohzio",
              "text": "Did you have a model loaded? And was it connected. I've only seen this error when I've forgotten to load a model or press the connect button",
              "score": 1,
              "created_utc": "2026-02-08 18:42:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4apc1n",
                  "author": "Narrow_Operation7252",
                  "text": "Yeah, it’s kind of strange. The UPF program can connect and see the model. And occasionally I’ll even get a response or two back if I set it to 127.0.0.1 instead of Localhost. But then it will fail on the next prompt I send, and I’ll keep getting the error messages again after that. \n\nIt might be something weird with my setup, but just wanted to see if it was happening to anyone else.",
                  "score": 1,
                  "created_utc": "2026-02-08 18:48:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o446o91",
          "author": "ComprehensiveFun3233",
          "text": "It always make me laugh at myself, but even when I'm in a LLM sub, and the explicit interesting tool is using LLM, I just glaze over and stop reading trhw second I realize the explanation is made/highly tuned by a LLM itself.",
          "score": 1,
          "created_utc": "2026-02-07 17:55:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o447bwc",
              "author": "Gohzio",
              "text": "Or perhaps I'm not the most eloquent and grammatically correct person out there so, to make it comprehensible I took my notes and put it into LM studio and asked for a comprehensible post 🤷🏿‍♂️",
              "score": 3,
              "created_utc": "2026-02-07 17:58:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o45tosj",
                  "author": "nachohk",
                  "text": "If you couldn't be bothered to write it, why should you expect anyone else to be bothered to read it? Using an LLM to help improve and refine your own words is one thing. Using them to inflict slop upon the rest of us is another.",
                  "score": 3,
                  "created_utc": "2026-02-07 23:06:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o448m1a",
                  "author": "ComprehensiveFun3233",
                  "text": "Exactly, I know you did that.  I do it as well.  But it still just makes my eyes glaze over. Interesting.",
                  "score": 0,
                  "created_utc": "2026-02-07 18:04:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o44rts6",
                  "author": "FirstEvolutionist",
                  "text": "I fault people for using AI to summarize or structure text only when the content is already bad: promotion, a disguised ad, AI psychosis or non sense.\n\nFor pruposeful use of AI formatting, summarizing and structure I've actually began to appreciate it: I no longer need to tune in to the style of the person writing to make sure I don't kisunderstand anything or having to deal with unclear text. The same has started happening for audio (podcasts) and video.\n\nWhen I want info, I rather have it done by AI. When I want connection, or thinking, I'm ok with old fashioned content as long as it is well done.\n\nI have a feeling that a lot of people will quickly adopt a similar preference over time.",
                  "score": -1,
                  "created_utc": "2026-02-07 19:40:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o44ko25",
          "author": "ghostd93",
          "text": "Did you consider adding an option for generating images (sending a request to comfyui api)? For imagine last scene etc?",
          "score": 0,
          "created_utc": "2026-02-07 19:03:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu0zoy",
      "title": "Built an open-source control plane for training LLMs locally (and across clusters)",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qu0zoy/built_an_opensource_control_plane_for_training/",
      "author": "Historical-Potato128",
      "created_utc": "2026-02-02 17:11:28",
      "score": 36,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "We built something called Transformer Lab for Teams while spending the past year working with big AI research labs to solve friction in their daily training workflows. \n\nWhat we observed:\n\n* The frontier labs invest a ton to build and maintain their own proprietary tooling.\n* Most other AI/ML research teams work with a fragmented landscape of legacy scripts, manual workflows which gets more complicated as you grow your team and run more experiments\n* Researchers spend almost half their time dealing with logistics. For example, results get lost or rerun because jobs fail before finishing and artifacts aren’t tracked consistently.\n\nWe took all this feedback and best practices to build Transformer Lab for Teams. \n\nWhat it’s useful for:\n\n* Running LLM training and fine-tuning on local machines, on-prem clusters, or mixed setups\n* Handling distributed training, restarts, and checkpoints\n* Keeping experiments, configs, and artifacts organized\n\nRuns locally on personal hardware (Apple Silicon, NVIDIA/AMD GPUs) and scales to high-performance computing clusters using orchestrators like Slurm and SkyPilot. You can use our CLI or GUI.\n\nWe’ve made it open source and free to use.\n\nPosting here because this community’s been supportive. I’m a maintainer and can help with install and questions. Even walk through a live demo if you’d like.\n\nAppreciate feedback from people actually running LLM workloads. \n\nTry it here: [https://lab.cloud/](https://lab.cloud/)\n\nIs this useful? Welcome your feedback on how we can improve it for you.\n\np.s. I'm one of the maintainers so please feel free to reach out incase anyone has installation issues\n\nhttps://preview.redd.it/6l1qutj664hg1.png?width=1800&format=png&auto=webp&s=8006640608103b36d8b83159cd63f2c7cc01b9d7\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qu0zoy/built_an_opensource_control_plane_for_training/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qxun8z",
      "title": "How many mac mini are equal to 1 mac studio?",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/snkxawxo0yhg1.jpeg",
      "author": "Far-Stretch5237",
      "created_utc": "2026-02-06 21:33:56",
      "score": 29,
      "num_comments": 21,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qxun8z/how_many_mac_mini_are_equal_to_1_mac_studio/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3z5yvs",
          "author": "xXprayerwarrior69Xx",
          "text": "https://preview.redd.it/r7fnmpf33yhg1.jpeg?width=1290&format=pjpg&auto=webp&s=10b9e1e12ba7a91d88e5197b1c86094034d5b8c3\n\nHope this helps",
          "score": 40,
          "created_utc": "2026-02-06 21:47:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z644z",
              "author": "Far-Stretch5237",
              "text": "It doesn't work like that 🥴",
              "score": -34,
              "created_utc": "2026-02-06 21:48:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zjp7g",
                  "author": "yunarivay",
                  "text": "![gif](giphy|10hfegXGKVRVNm)",
                  "score": 10,
                  "created_utc": "2026-02-06 22:58:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3z7pzk",
          "author": "AnonymousCrayonEater",
          "text": "Telling us what you are trying to accomplish might help. At a basic level you can divide the ram of the studio over the mini, but i don’t think that answers your question",
          "score": 11,
          "created_utc": "2026-02-06 21:56:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z81rp",
              "author": "Far-Stretch5237",
              "text": "I AM just curious \nA guy on twitter said he will get mac studio for his openclaw bot\n\nSo i thought....\n\nHow many mac mini are equal to 1 mac studio",
              "score": -8,
              "created_utc": "2026-02-06 21:57:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zaoyp",
                  "author": "AnonymousCrayonEater",
                  "text": "Based on your numbers you are comparing a maxed out studio to a base mini so:\n\n512 / 16 = 32 mac minis",
                  "score": 6,
                  "created_utc": "2026-02-06 22:11:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o48exgv",
                  "author": "TBT_TBT",
                  "text": "If this „guy on twitter“ does not use local LLMs but API keys, it is completely irrelevant if this OpenClaw instance runs on a Studio or Mini. If used with local LLMs, no Mini setup will ever reach a Studio with a lot of ram.",
                  "score": 1,
                  "created_utc": "2026-02-08 10:46:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4174m0",
          "author": "Such_Advantage_6949",
          "text": "How many cars is equal to one plane",
          "score": 9,
          "created_utc": "2026-02-07 05:11:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o442wu5",
              "author": "nanotothemoon",
              "text": "Mostly depends on the turn signals, fuselage, and whether they have full size spares or not",
              "score": 2,
              "created_utc": "2026-02-07 17:36:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zf8vt",
          "author": "Momo--Sama",
          "text": "You'd also have to price in the equipment to connect all of these Minis together to work in tandem (that's an option, right?). as opposed to an independent plug and play Mac Studio but I'm not knowledgeable enough to expand on that.",
          "score": 6,
          "created_utc": "2026-02-06 22:34:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zibie",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 2,
              "created_utc": "2026-02-06 22:51:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zk600",
                  "author": "Buddhabelli",
                  "text": "thunderbolt5. exo.",
                  "score": 3,
                  "created_utc": "2026-02-06 23:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o407vpt",
          "author": "deeddy",
          "text": "42.",
          "score": 5,
          "created_utc": "2026-02-07 01:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40n0hk",
          "author": "SebastianOpp",
          "text": "At least tree fiddy",
          "score": 4,
          "created_utc": "2026-02-07 02:52:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40u5zm",
          "author": "jwrunge",
          "text": "About 3 and a half to 5, depending on if you are keeping the rigid structure or allowing for redistribution of volume (eg melting one down)",
          "score": 3,
          "created_utc": "2026-02-07 03:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o406pd1",
          "author": "TimLikesAI",
          "text": "About five by volume from the looks of it.",
          "score": 2,
          "created_utc": "2026-02-07 01:12:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzsynx",
      "title": "Qwen3 Coder Next on M3 Ultra v.s. GX10",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "author": "Imaginary_Ask8207",
      "created_utc": "2026-02-09 03:11:13",
      "score": 29,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "https://preview.redd.it/rg9mxsm7zdig1.png?width=2294&format=png&auto=webp&s=7400c7c54428910158a160e5e407022cbba24947\n\n[Qwen3-Coder-Next served on GX10](https://reddit.com/link/1qzsynx/video/cfad1dooxdig1/player)\n\n[Qwen3-Coder-Next served on M3 Ultra 512GB](https://reddit.com/link/1qzsynx/video/sawsb3kpxdig1/player)\n\nI'm currently exploring CLI-based coding tools as an alternative to GitHub Copilot.\n\nRight now I'm testing **opencode** (seems to be the most popular open-source option at the moment), and I paired it with the new **Qwen3-Coder-Next** model. It's decent!\n\nThe tasks I tested were pretty simple, just some small refactoring in my toy project and fixing one bug. But that's exactly the point, for these kind of everyday coding tasks, you ain't gonna need Opus 4.6 level of intelligence, a \"small and sweet\" open source model is usually good enough.\n\nI feel like 80b model is a sweet spot for GX10 with 128GB of GPU memory, 8-bit quantized models could be comfortably loaded into the GPU. For comparison between the two devices, you can clearly see M3 ultra gives a higher throughput, but it's 3x the price of GX10.\n\nDo you think going local will be the coming trend?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qzsynx/qwen3_coder_next_on_m3_ultra_vs_gx10/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o4df22x",
          "author": "Thump604",
          "text": "This is my agent pick of the day with Step as the planner",
          "score": 7,
          "created_utc": "2026-02-09 03:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4diz60",
              "author": "Imaginary_Ask8207",
              "text": "Great suggestion! I don't think GX10 will be able to load Step-3.5-Flash, I'll try on M3 ultra.",
              "score": 1,
              "created_utc": "2026-02-09 03:47:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dr3e2",
                  "author": "Look_0ver_There",
                  "text": "Grab the MXFP4 quant of Step-3.5-Flash.\n\nThat loads up just fine on my Ryzen AI Max 395 with its 128GB.  It's typically running at about 26 tok/sec so I'd imagine you'd be looking at that sort of ballpark too.",
                  "score": 1,
                  "created_utc": "2026-02-09 04:41:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dmfxb",
          "author": "NoobMLDude",
          "text": "Going local is already the current trend !!\nYou are in the right subreddit for it. 😉\n\n\nI’ve gone Local since a long time for most of my tasks that are Private.\n\nI’m also trying to educate the community that you don’t need to give your money and personal data to big AI companies. You can download Free and OpenSource models and they are good enough for 90% of users. \n\n\nHere are few examples of Local AI workflows that I use:\n- [Local Meeting Assistant](https://youtu.be/cveV7I7ewTA)\n- [Local Talking Assistant](https://youtu.be/2VHzYy45kPw)\n- [Terminal with AI Context support](https://youtu.be/_sDJBosDznI)\n\n\nI added a few more in the [LocalAI playlist](https://www.youtube.com/playlist?list=PLmBiQSpo5XuQKaKGgoiPFFt_Jfvp3oioV)\n\nOwen3-Coder-Next looks like a solid model for Coding if you can run a 80B model on your hardware.",
          "score": 3,
          "created_utc": "2026-02-09 04:10:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4e5mlg",
          "author": "HumanDrone8721",
          "text": "As I know someone using a similar DGX Spark setup, they use a specialized replacement for the otherwise excellent nvtop that is not really optimized for Sparks that is called dgxtop:\n\nhttps://github.com/GigCoder-ai/dgxtop\n\nIs vibe coded, but one of the good ones IMHO:",
          "score": 2,
          "created_utc": "2026-02-09 06:35:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e7gw8",
              "author": "Imaginary_Ask8207",
              "text": "Thanks for sharing! This definitely looks better than nvtop. Is it stable and well-tested?",
              "score": 1,
              "created_utc": "2026-02-09 06:51:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e884x",
                  "author": "HumanDrone8721",
                  "text": "AFAIK they have terminals open running it on both of their Spark cluster nodes as it shows also the network traffic over the high-speed interfaces (these are indeed FAST), along with the CPU and GPU temps, for me is fascinating how they correlate while doing complex inference. So far I've heard no complains, but I'm a poor so I don't own a Spark or two ;).",
                  "score": 2,
                  "created_utc": "2026-02-09 06:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4el8of",
          "author": "HallTraditional2356",
          "text": "Amazing! How did you host it on the Apple Silicon M3 Ultra to Connect opencode to it ?",
          "score": 1,
          "created_utc": "2026-02-09 09:02:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvtxno",
      "title": "Qwen3-Coder-Next GGUFs updated with major bug fixes.",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/yoeghkey7ihg1.png",
      "author": "yoracale",
      "created_utc": "2026-02-04 16:56:06",
      "score": 26,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qvtxno/qwen3codernext_ggufs_updated_with_major_bug_fixes/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3lmkyx",
          "author": "Egoz3ntrum",
          "text": "Has anybody calculated the GGUF perplexity for this model?",
          "score": 2,
          "created_utc": "2026-02-04 21:05:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwg2uk",
      "title": "Which local model to use on MacBook with M4 pro and 48GB RAM",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qwg2uk/which_local_model_to_use_on_macbook_with_m4_pro/",
      "author": "aniozen",
      "created_utc": "2026-02-05 08:43:19",
      "score": 25,
      "num_comments": 32,
      "upvote_ratio": 0.93,
      "text": "Hey! \n\nI just got this MacBook Pro and I'm looking for a decent model to run on it.\n\nI'm a student so I would prefer a non-coding optimized model.\n\nQwen3 30b ran very very slow, almost unusable.\n\nSo I might be aiming to high. Or maybe there's a way to optimize it.\n\nIf you have any suggestions I would love to hear!\n\nThanks in advance",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qwg2uk/which_local_model_to_use_on_macbook_with_m4_pro/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o3p9q2j",
          "author": "grandnoliv",
          "text": "I have a Mac mini M4 pro with 48GB so exact same capabilities. I use Ollama (I did not try MLX optimised models yet, I think it needs LMStudio).\n\nMy favorite models are :  \n• Qwen3-coder:30b-a3b (I get around 60 tokens/s)  \n• Qwen3:30b-a3b (\\~60 t/s with a thinking step)  \n• GML-4.7-flash (\\~40 t/s with a long thinking step)  \n• GPT-OSS:20b (\\~45 t/s, with thinking step)  \nThese are fast because they are MoE with only a few active parameters.  \nFor comparison, I like Mistral dense models but it's slow and the context window needs more RAM:  \n• Magistral:24b (\\~16 t/s)\n\nT/s is not everything if the model can waste its time thinking. But thinking gives smarter results, especially if you configure a larger context size (I use around 30k context size if possible).  \n  \nAsking to build a one page html/js to roll dices as a test:  \n• Qwen3-coder:30b-a3b — total duration: 21.2s (No thinking)  \n• Qwen3:30b-a3b — total duration: 1m7s (Thinking step before the code)  \n• GML-4.7-flash — total duration: 2m27s (Long thinking, usually smart results)  \n• GPT-OSS:20b — total duration: 28s (With thinking step)  \n• Magistral:24b — total duration: 5m0s (Very long thinking)\n\nJust a quick opinion on the quality of output (code execution + description output)  \n• Qwen3-coder:30b-a3b — Works great, pretty and 2D animated (it output only the code)  \n• Qwen3:30b-a3b — Works but with UI problems needing fixes, no animation (+ features description and details about the choices made)  \n• GML-4.7-flash — Most ambitious result (3D animated) but it doesn't work out of the box… clean UI (+ one line description)  \n• GPT-OSS:20b — Very minimalistic and functional, works great, does not try to be beautiful (+ quick \"how it works\" description)  \n• Magistral:24b — Very minimalistic and functional, works great, ugly (+ quick description)",
          "score": 22,
          "created_utc": "2026-02-05 11:48:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rn4fh",
              "author": "GlassAd7618",
              "text": "Nice comparison and data (tokens per second). Very helpful!",
              "score": 3,
              "created_utc": "2026-02-05 19:09:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3v8nh5",
              "author": "cmndr_spanky",
              "text": "I take it you run everything at q4 ? Some you might be fine at q8 and see a noticeable quality increase",
              "score": 1,
              "created_utc": "2026-02-06 08:05:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3vg0pu",
                  "author": "grandnoliv",
                  "text": "I do use q4 for the models, I'm not sure q8+large context window would fit in the VRAM… I'll test q8 but for now I mainly use local LLMs for quick chats and did not manage to get it to work consistently with a coding agent (my main goal) probably because of the limited context window",
                  "score": 1,
                  "created_utc": "2026-02-06 09:15:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oq8yq",
          "author": "former_farmer",
          "text": "Are you running MLK 4bit quant models? \nEDIT: Yeaaah I meant MLX.",
          "score": 3,
          "created_utc": "2026-02-05 08:50:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p8zam",
              "author": "Additional-Sun-6083",
              "text": "Pretty sure you mean MLX. ",
              "score": 5,
              "created_utc": "2026-02-05 11:43:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3oqeei",
              "author": "aniozen",
              "text": "Nope, I just tried qwen and had no luck. What MLK?",
              "score": 1,
              "created_utc": "2026-02-05 08:51:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3oqlmp",
                  "author": "former_farmer",
                  "text": "Mlk is a format optimized for apple chip. And 4 bits version means it's an optimized version for smaller computers. A bit less smart but higher speed.",
                  "score": 3,
                  "created_utc": "2026-02-05 08:53:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3tvq11",
              "author": "joe__n",
              "text": "I have a dream",
              "score": 1,
              "created_utc": "2026-02-06 02:10:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3tw9d3",
                  "author": "former_farmer",
                  "text": "![gif](giphy|dtGIRL0FDp6nnOPGb5)",
                  "score": 2,
                  "created_utc": "2026-02-06 02:13:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oqbyn",
          "author": "former_farmer",
          "text": "Btw this might help https://www.reddit.com/r/LocalLLaMA/comments/1l6v9eu/whats_the_best_local_llm_for_coding_i_can_run_on/",
          "score": 2,
          "created_utc": "2026-02-05 08:50:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3swvbm",
              "author": "pauljdavis",
              "text": "But 8 mos old",
              "score": 1,
              "created_utc": "2026-02-05 22:50:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3q8wmr",
          "author": "mpw-linux",
          "text": "What about: \"mlx-community/LFM2.5-1.2B-Thinking-8bit ?",
          "score": 2,
          "created_utc": "2026-02-05 15:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3tnkhd",
              "author": "logic-paradox",
              "text": "It’s ok for a small model. But if you are running apple silicon you can go with the 8B models. ministral 3 8B is my fav right now. Running on a M4 Mini 16GB",
              "score": 2,
              "created_utc": "2026-02-06 01:21:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uktwv",
                  "author": "mpw-linux",
                  "text": "sounds good. Are those models giving good responses? What  type of information are you getting from them via prompts? I could run those Models as well but my internet speed is slow for larger models to download. I am impressed by the LFM2.5.1 models, good information  on my prompts and takes little memory even though I have 32g of Ram on my M1 pro. on your minstral 3 8B are you using quantized versions ?",
                  "score": 1,
                  "created_utc": "2026-02-06 04:48:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3u3p7w",
          "author": "TheCh0rt",
          "text": "Try LM Studio. Is free, easier to use than Ollama and has an LLM browser/downloader/loader/server/chatbot inside it. I even connected it to VSCode and did some vibe coding on my M1 with 64GB of RAM and it was lightning fast. I’m sure yours will be faster.",
          "score": 2,
          "created_utc": "2026-02-06 02:57:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wrox4",
              "author": "Mysterious-Power-571",
              "text": "Which LLM did you use for your vibe coding and how was the code quality compared to the big name subscription LLMs?",
              "score": 1,
              "created_utc": "2026-02-06 14:49:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3wy6oo",
                  "author": "TheCh0rt",
                  "text": "Nowhere near successful. You gotta do it with the big ones. They also aren’t good at accessing the internet because they’re not so much built for it",
                  "score": 1,
                  "created_utc": "2026-02-06 15:22:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3peb46",
          "author": "Lux_mirawy_3904",
          "text": "He visto que amigos con MacBook y 48GB de RAM, emplean modelos más pequeños como LLaMA 2 7B o Mistral 7B. Son eficientes, rápidos y perfectos para tareas generales como chat o redacción. Como consejo, evita modelos grandes como Qwen 30B, que son lentos sin optimización. Para optimizar el rendimiento, puedes cuantizar el modelo (usar 4-bit o 8-bit) y ejecutar frameworks como llama.cpp o GPT4All con soporte para MPS en Apple Silicon.",
          "score": 3,
          "created_utc": "2026-02-05 12:22:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3p8vbg",
          "author": "dsartori",
          "text": "That model you selected should be good for your hardware. Something isn’t right.",
          "score": 1,
          "created_utc": "2026-02-05 11:42:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3prqhm",
          "author": "abubakkar_s",
          "text": "Checkout gpt-oss:20b",
          "score": 1,
          "created_utc": "2026-02-05 13:46:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ru8qy",
              "author": "ConspicuousSomething",
              "text": "Yep. I try other models, but keep coming back to this one.",
              "score": 2,
              "created_utc": "2026-02-05 19:42:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3v8p4k",
                  "author": "cmndr_spanky",
                  "text": "Why ? For what ?",
                  "score": 1,
                  "created_utc": "2026-02-06 08:05:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3qvfha",
          "author": "plscallmebyname",
          "text": "Use MLX based arch, 20-40 billion parameters, 8 bit or 16 bit quant whatever fits.",
          "score": 1,
          "created_utc": "2026-02-05 17:02:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3twi2n",
          "author": "joe__n",
          "text": "I assume you mean for chat/RAG? Get LM Studio and try these:\n\n\\- [https://lmstudio.ai/models/allenai/olmo-3-32b-think](https://lmstudio.ai/models/allenai/olmo-3-32b-think)\n\n\\- [https://lmstudio.ai/models/essentialai/rnj-1](https://lmstudio.ai/models/essentialai/rnj-1)\n\n\\- [https://lmstudio.ai/models/bytedance/seed-oss-36b](https://lmstudio.ai/models/bytedance/seed-oss-36b)\n\nAlso agree with the others suggesting Qwen3:30b-a3b and GPT-OSS:20b\n\nYou need to play with context settings as well.",
          "score": 1,
          "created_utc": "2026-02-06 02:15:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtu3u3",
      "title": "Using Clawdbot as an AI gateway on my NAS alongside local LLMs",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qtu3u3/using_clawdbot_as_an_ai_gateway_on_my_nas/",
      "author": "ProfessionalCan2356",
      "created_utc": "2026-02-02 12:47:10",
      "score": 25,
      "num_comments": 6,
      "upvote_ratio": 0.76,
      "text": "I've been playing with OpenClaw (formerly Clawdbot/Moltbot) as a small AI gateway on my always-on box and thought I'd share the setup in case it's useful.\n\nHardware / layout:\n\n* Host: UGREEN DXP4800P (always on, mainly storage + light services)\n* VM: Ubuntu Server, bridge mode\n* VM resources: 2 vCPUs, 4GB RAM, \\~40GB disk\n* LLM side: for now a single provider via API, plan is to swap this to a local HTTP endpoint (Ollama or similar) running on another machine in the same LAN\n\nClawdbot itself runs entirely on the VM. The nice part is that the \"automation brain\" lives on the NAS 24/7, while the actual LLM compute can be moved later to a separate CPU/GPU box just by changing the endpoint URL.\n\nDeployment notes:\n\n* Had to switch the VM NIC to a Linux bridge so it sits on the same subnet as the rest of the network; otherwise the web UI and SSH were awkward to reach.\n* Clawdbot really wants Node 22+, so I used the Nodesource script, then their curl | bash installer. It looks like it hangs for a bit but eventually finishes.\n* Gateway is bound to 0. 0. 0. 0 inside the LAN with token auth enabled. Everything is only reachable on the internal network, but I still treat it as \"high trust, high blast radius\" because it can touch multiple services once wired up.\n\nWhat it's doing right now：\n\n* Sends me a simple daily NAS status message (storage / backup summary).\n* Watches a specific NAS folder and posts a short notification to Telegram when new files appear.\n* Management is mostly via the web UI: easy to swap models, tweak workflows, and add skills without touching the CLI every time.\n\nOn the DXP side, the VM sits at low usage most of the time. CPU spikes briefly when Clawdbot is talking to the LLM or doing heavier processing, but so far it hasn't interfered with normal NAS duties like backups and file serving.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qtu3u3/using_clawdbot_as_an_ai_gateway_on_my_nas/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o35g13c",
          "author": "xyzzzzy",
          "text": "I was planning to do the local LLM thing but my AI convinced me this was dumb because for what I would be able to run on my RTX 3070 a cloud model that costs $.10/day would kick its butt. Would be happy to be wrong.",
          "score": 3,
          "created_utc": "2026-02-02 13:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o36kcqk",
          "author": "cmndr_spanky",
          "text": "Be careful with clawdbot, it was clearly vibe coded and not even the dev who made it knows how it works or its security issues.\n\nMake your own agent",
          "score": 2,
          "created_utc": "2026-02-02 16:46:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bkhec",
              "author": "brianlmerritt",
              "text": "I thought about creating simple proxies at the pain points: llm calls, ip requests, file access, cli etc. I decided to call it OpenCondom.",
              "score": 1,
              "created_utc": "2026-02-03 10:47:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35czgm",
          "author": "Heavy_Boss_1467",
          "text": "Hi, what about the RAM usage of the VM ? Would 2GB be enough ? Thanks",
          "score": 1,
          "created_utc": "2026-02-02 13:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35mv3x",
          "author": "aidenclarke_12",
          "text": "This approach is a clever low overhead way t keep the AI brain always on without taxing the nax - love the plan tooffload LLM compute to a separate box via endpoint swap, keeps things modular. The daily status and folder watch workflows are practical starters....",
          "score": 0,
          "created_utc": "2026-02-02 14:02:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o364xgp",
          "author": "GalaxYRapid",
          "text": "I was trying to set this up (for now on a separate machine for testing) but I couldn’t get the bot to acknowledge my local machine running lm studio (it can report as its own thing, open ai style, or anthropic now too) but I keep getting errors I’ve tried bridged and not and both ways throw an error saying that it can report back but when I run it anywhere else (I have a n8n instance on a local server that connects there all the time) it works. I’m hoping it’s just because I configured something wrong or was just tired when I was setting it up but I’m curious if only ollama works or if I can use other options (I’ve been meaning to try out vllm but I wanted to try lm studio first because that’s already set up and working).",
          "score": 0,
          "created_utc": "2026-02-02 15:35:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}