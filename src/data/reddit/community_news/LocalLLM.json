{
  "metadata": {
    "last_updated": "2026-01-27 16:59:04",
    "time_filter": "week",
    "subreddit": "LocalLLM",
    "total_items": 20,
    "total_comments": 155,
    "file_size_bytes": 208044
  },
  "items": [
    {
      "id": "1qmrwxl",
      "title": "Clawdbot: the AI assistant that actually messages you first",
      "subreddit": "LocalLLM",
      "url": "https://jpcaparas.medium.com/clawdbot-the-5-month-ai-assistant-that-actually-messages-you-first-8b247ac850b8?sk=0d521efdf2ceafe37973887b57b168ba",
      "author": "jpcaparas",
      "created_utc": "2026-01-25 18:59:25",
      "score": 123,
      "num_comments": 89,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qmrwxl/clawdbot_the_ai_assistant_that_actually_messages/",
      "domain": "jpcaparas.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1pwp1d",
          "author": "mike7seven",
          "text": "Spent most of the day figuring out that you need to get your Claude Code key (Oauth) on another machine and paste it for use on whatever machine you are setting up Clawdbot on. \n\nFrom Ops tutorial: \n‚ÄúYou have two options. Claude OAuth (the easiest) or API keys (more control over costs). For beginners, OAuth with a Claude Pro subscription is the simplest path.‚Äù\n\nNow I‚Äôm over even playing around with this thing for the rest of the day. \n\nMy two cents: The MacOS app is nice but you have to build it from source. Also good luck getting app auth setup properly if you are using a remote machine.",
          "score": 8,
          "created_utc": "2026-01-25 23:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pxaxs",
              "author": "DependentNew4290",
              "text": "Is it that easy to set up one as a non-technical person???",
              "score": 2,
              "created_utc": "2026-01-25 23:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1q1s1j",
                  "author": "mike7seven",
                  "text": "Depends on how you plan on setting it up. Using ChatGPT and Codex it worked immediately and beautifully. Claude was not that easy. For local follow OPs guide.",
                  "score": 2,
                  "created_utc": "2026-01-26 00:08:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20gkav",
                  "author": "brianlmerritt",
                  "text": "If you want to set it up on your personal or work computer, provide unlimited access to said computer and all of your email and messaging accounts plus whatever else is on there like online banking or company information, it is still not that easy.\n\nBut you will be putting your personal computer or company at risk - there are a lot of little add-ons that have \"extra\" code and abilities to log into your computer, access everything plus use your AI tokens and accounts.\n\nUnless you are happy setting this up securely (see the link at the end of the post) there is a very good chance it doesn't go well.    \n  \nAlso note that one email or message can come in and hijack the AI within the system even if you didn't load any dodgy utilities.\n\n",
                  "score": 1,
                  "created_utc": "2026-01-27 13:31:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o20sk57",
              "author": "Manarj789",
              "text": "It took all of 5 minutes to do, they literally tell you the steps to take",
              "score": 1,
              "created_utc": "2026-01-27 14:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ux2ag",
              "author": "[deleted]",
              "text": "[removed]",
              "score": -4,
              "created_utc": "2026-01-26 17:52:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yqrif",
                  "author": "Winter-Editor-9230",
                  "text": "r/LocalLLM follows platform-wide Reddit Rules; crypto currency and self promotion.",
                  "score": 1,
                  "created_utc": "2026-01-27 05:17:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1u9m94",
          "author": "inigid",
          "text": "Clawdbot opens up the potential for a massive supply-chain attack that can steal or destroy everything on your machine and network, harvest and exfiltrate emails, crypto wallets, bank account and credit card details, SSN numbers, personal information, ssh keys.\n\nThe Solana meme coin and hype pumps by countless influencers is also a massive red flag.\n\nStay TF away from it if you have any sense.",
          "score": 9,
          "created_utc": "2026-01-26 16:10:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ud0b5",
              "author": "kublaikhaann",
              "text": "stop making too much sense",
              "score": 5,
              "created_utc": "2026-01-26 16:25:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ul578",
                  "author": "inigid",
                  "text": "Ikr, the thing literally has auto-update for itself, and then there are the downloadable skills on top!\n\nIt's like you are asking for trouble!\n\nOh, I'm just going to install this with sudo privileges and connect it up to all my infrastructure and personal accounts.\n\nWhat could possibly go wrong!  Smh.\n\nhttps://preview.redd.it/0h7v5nrr5qfg1.jpeg?width=1581&format=pjpg&auto=webp&s=2566f1d6f9e2d37b127da3ee58bb1740e6990adc",
                  "score": 4,
                  "created_utc": "2026-01-26 16:59:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yi54i",
              "author": "ab2377",
              "text": "üíØ",
              "score": 2,
              "created_utc": "2026-01-27 04:20:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1x3t7z",
              "author": "ParanoidBlueLobster",
              "text": "Bit of a conspiracy theorist are we?\n\nIt's an open-source tool https://github.com/clawdbot/clawdbot\n\nAnd you update it manually https://docs.clawd.bot/install/updating",
              "score": 2,
              "created_utc": "2026-01-26 23:44:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xbf37",
                  "author": "inigid",
                  "text": "[https://cybersecuritynews.com/clawdbot-chats-exposed/](https://cybersecuritynews.com/clawdbot-chats-exposed/)\n\n[https://socradar.io/blog/clawdbot-is-it-safe/](https://socradar.io/blog/clawdbot-is-it-safe/)\n\nhttps://preview.redd.it/4ls6ihradsfg1.jpeg?width=1280&format=pjpg&auto=webp&s=905eda3538cc276b330924c33d14e18022a95d40",
                  "score": 2,
                  "created_utc": "2026-01-27 00:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o21bvsw",
              "author": "Jasmine_Demir",
              "text": "Probably depends on how much you have to lose as well, and how disruptive it would be for you to stop your entire life for a day or two while you perform triage. \n\nIf you're 22 with a $50,000 net worth it's a very different consequence than begin 55 with 3 children, $5,000,000 in available debt/assets, and a convoluted chain of responsibilities that would come to a halt once you incorporated an assistant at this layer of your life flow and it failed.\n\nThe ones jumping into this don't have much to lose, and/or have the time and experience to stay on top of security issues.",
              "score": 1,
              "created_utc": "2026-01-27 16:03:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1s2o5c",
          "author": "Ashamed_Promise7726",
          "text": "Has anyone successfully connected a local llm on their machine to Clawdbot? I have a few different models already downloaded to my PC, but I cannot get Clawdbot to work with them, and it keeps wanting an api key for a usage based model.\n\nIs it possible to run Clawdbot 100% locally?",
          "score": 7,
          "created_utc": "2026-01-26 07:22:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s41bu",
              "author": "kinesivan",
              "text": "If you are hosting with LM Studio or Ollama, does not matter what API key you pass, any should work.",
              "score": 3,
              "created_utc": "2026-01-26 07:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s7zwo",
                  "author": "Yorn2",
                  "text": "No, I'm having the same issue, Local OpenAPI-compatible isn't even an option in the choices. Nor is Ollama.",
                  "score": 2,
                  "created_utc": "2026-01-26 08:07:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1slfam",
              "author": "Everlier",
              "text": "I spent my entire weekend integrating Clawdbot into Harbor so that it's possible to set it up with a local LLM in a few commands (granted you have Harbor installed):\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot",
              "score": 2,
              "created_utc": "2026-01-26 10:10:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xkbhh",
                  "author": "Yorn2",
                  "text": "Harbor seems cool but I'm using MacOS and mlx_lm.server for running a local instance of Deepseek 3.2, not llama.cpp or ollama or vllm. Though I do have a non-mac AI box where I might use Harbor instead someday. Do you plan on adding MacOS support sometime?",
                  "score": 1,
                  "created_utc": "2026-01-27 01:09:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yju3k",
              "author": "Loud-Layer3329",
              "text": "Yes I have using Ollama. Haven't had much luck with the models, though. So far Qwen2.5:14b has been the best to use via the web interface, but it gives a lot of nonsense via Telegram.\n\nNote that you need to use a model that supports tools, and Clawdbot only permits models that have a context window of >16k. Clawdbot supports the openai endpoint so you can use the baseurl [http://x.x.x.x:11434/v1](http://x.x.x.x:11434/v1)",
              "score": 1,
              "created_utc": "2026-01-27 04:31:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1z9r9b",
              "author": "No_Box1288",
              "text": "Running it with MiniMax M2.1 works well. I set it up following this tutorial, pretty straightforward:\nhttps://x.com/MiniMax_AI/status/2014380057301811685\n\nAlso saw a coding plan link in the MiniMax Discord. There‚Äôs a discount + cashback if you use it:\nhttps://platform.minimax.io/subscribe/coding-plan?code=Cp84x9ex1L&source=link",
              "score": 1,
              "created_utc": "2026-01-27 07:50:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pa4kx",
          "author": "Acceptable_Home_",
          "text": "Might try soon, sounds good!",
          "score": 3,
          "created_utc": "2026-01-25 22:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1pax6h",
              "author": "jpcaparas",
              "text": "thank you. apologies in advance if the guide isnt too technical. i just want people to get into it. pete the maintainer is also a cool follow on Twitter",
              "score": -2,
              "created_utc": "2026-01-25 22:05:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rogty",
          "author": "threathunter369",
          "text": "your take on this guys?\n\n[https://x.com/theonejvo/status/2015401219746128322](https://x.com/theonejvo/status/2015401219746128322)",
          "score": 4,
          "created_utc": "2026-01-26 05:31:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1whdbz",
              "author": "Double-Lavishness870",
              "text": "LLms are insecure per default. Any browsing, email reading, tweet reading can insert bad behavior. \n\nBest knowledge and advice see here: https://media.ccc.de/v/39c3-agentic-probllms-exploiting-ai-computer-use-and-coding-agents#t=1218\n\nIsolation is needed.",
              "score": 2,
              "created_utc": "2026-01-26 21:55:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ryqsv",
              "author": "xak47d",
              "text": "That's for noobs who just discovered cloud hosting following the hype and don't properly secure their instances. That's very unrelated to the tool and its capabilities",
              "score": 1,
              "created_utc": "2026-01-26 06:50:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1slmv3",
          "author": "Everlier",
          "text": "If you're like me and wanted to setup Clawdbot with a local LLM, check out this integration in Harbor:\nhttps://github.com/av/harbor/wiki/2.3.69-Satellite-Clawdbot\n\nIt also runs Clawdbot containerized, so that it won't break your host system if something will go wrong",
          "score": 4,
          "created_utc": "2026-01-26 10:12:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1umj0n",
          "author": "cmndr_spanky",
          "text": "From a security perspective, I have zero interest in this right now (LLM autonomy with access to my calendar etc). And the supposed benefits are minimal (I already get calendar reminders, I don‚Äôt need an LLM bot to ‚Äúextra‚Äù remind me). Beyond that‚Ä¶ all the other use cases are fine with direct access to normal chatGPT / Gemini etc ..\n\nAlso Apple with soon replace Siri with Gemini and I imagine that‚Äôs going to be a game changer in terms of iOS integration and proactive AI",
          "score": 4,
          "created_utc": "2026-01-26 17:05:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wcz9x",
          "author": "TendiesOnlyPls",
          "text": "Ok... so has anyone given Clawdbot access to their dating apps and asked it to go wrangle up some consenting ass? Really want to see how well these LLMs are able to spit game.",
          "score": 3,
          "created_utc": "2026-01-26 21:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ug8e0",
          "author": "Weird-Consequence366",
          "text": "Astroturf campaign",
          "score": 4,
          "created_utc": "2026-01-26 16:39:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ri8h1",
          "author": "No_Conversation9561",
          "text": "Entire X feed has been this lately",
          "score": 2,
          "created_utc": "2026-01-26 04:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rqpdf",
          "author": "Fast_Back_4332",
          "text": "How safe is it?",
          "score": 2,
          "created_utc": "2026-01-26 05:48:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryvys",
              "author": "xak47d",
              "text": "It might delete all your files if you make it mad",
              "score": 5,
              "created_utc": "2026-01-26 06:51:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1s9o8e",
                  "author": "Cressio",
                  "text": "Is that literally true? It seems like it would be but I haven't looked too deep into it. I don't think I *want* something with that deep of access to my stuff lol",
                  "score": 1,
                  "created_utc": "2026-01-26 08:22:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o20w5zw",
                  "author": "jeremyckahn",
                  "text": "A hallmark of any great software",
                  "score": 1,
                  "created_utc": "2026-01-27 14:51:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sdrta",
          "author": "tomByrer",
          "text": "Is your article some where else than Medium please?",
          "score": 3,
          "created_utc": "2026-01-26 08:59:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q8d1z",
          "author": "snam13",
          "text": "For once, I‚Äôm early! Set this up last week. \nDon‚Äôt expect miracles. If you‚Äôve used claude code or similar, it will feel like those but in your chat app. Be careful of burning lots of API tokens.",
          "score": 2,
          "created_utc": "2026-01-26 00:40:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qh1fv",
              "author": "ThenExtension9196",
              "text": "You may not be squeezing the lemon yet. There‚Äôs such insane amount of functionality you can get out of it by wiring it to gitlab/github and api services. I legit think this thing will be able to automated nearly my entire workday.",
              "score": 1,
              "created_utc": "2026-01-26 01:24:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rvyp3",
                  "author": "ketaminesuppository",
                  "text": "curious; what's your job?",
                  "score": 1,
                  "created_utc": "2026-01-26 06:28:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1qzgz1",
          "author": "Artistic-Read-1097",
          "text": "I'm having a problem when I send a message in the GUI. Nothing happens I get no response. can anyone help if been at it all day",
          "score": 1,
          "created_utc": "2026-01-26 02:57:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rswkr",
          "author": "albany_shithole",
          "text": "How would it work for vLLM ?",
          "score": 1,
          "created_utc": "2026-01-26 06:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sat1j",
          "author": "redblood252",
          "text": "If I use this with local llama-cpp. Which model is best for it? Gpt-oss? Is 14Gb of vram enough for it?",
          "score": 1,
          "created_utc": "2026-01-26 08:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1slhhv",
              "author": "Everlier",
              "text": "GLM-4.7-Flash is the best right now, it really wants very strong agentic models",
              "score": 2,
              "created_utc": "2026-01-26 10:10:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1sm5vs",
                  "author": "redblood252",
                  "text": "Even at IQ4_XS quantization it takes 16gb of vram. I need to test some cpu offloading and see if it is fast enough. And if the quantization isn‚Äôt too much.",
                  "score": 1,
                  "created_utc": "2026-01-26 10:16:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sfx42",
          "author": "Necessary_Function_3",
          "text": "Spent hours (and burned all my anthropic tokens for the day and I am paying $200 a month, but API tokens are extra it seems) and cannot get it to work with ollama, extremely frustrating",
          "score": 1,
          "created_utc": "2026-01-26 09:19:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1skfux",
              "author": "jamesftf",
              "text": "but what this does and what claude code cannot do?!",
              "score": 1,
              "created_utc": "2026-01-26 10:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1u3ade",
                  "author": "MaaDoTaa",
                  "text": "It can message you (unsolicited)",
                  "score": 1,
                  "created_utc": "2026-01-26 15:43:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1sms05",
          "author": "Separ0",
          "text": "Installed last night on an Ubuntu machine on hetznsr on my tailnet. Crons are not working",
          "score": 1,
          "created_utc": "2026-01-26 10:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vihfg",
          "author": "p_235615",
          "text": "Not sure why this is such a hype, but from what I played around with it in a VM and local llm (its quite PITA to setup), and one thing is for sure - its basically a security nightmare. \n\nYou have a single point rouge AI which sending all your data to cloud if not set up with local LLM and even with local LLM you never know when even accidentally it will send your private files somewhere to the web. Or if it gets somehow compromised by accessing some webpage or something, its like throwing all your files and accounts around. There are basically no security guard rails, and would never feed it some private or corporate accounts.",
          "score": 1,
          "created_utc": "2026-01-26 19:22:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vq2td",
          "author": "apola",
          "text": "I'm interested in playing around with clawdbot, but my understanding is that it chews through tokens like nobody's business. How useful can it be with the $20/month Claude Pro subscription? Would that give it enough tokens to be remotely useful?",
          "score": 1,
          "created_utc": "2026-01-26 19:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vqy8f",
              "author": "jpcaparas",
              "text": "I've given a bit of guidance on token allowances between Pro and Max plans here:\n\nhttps://jpcaparas.medium.com/why-your-expensive-claude-subscription-is-actually-a-steal-02f10893940c?sk=65a39127cbd10532ba642181ba41fb8a\n\nYou might find it useful.",
              "score": 1,
              "created_utc": "2026-01-26 19:58:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1vycbq",
                  "author": "apola",
                  "text": "Interesting, thanks for the link. So, the $20/mo plan gives you \\~$259 worth of compute. My real question is: Is $259 of compute enough to make Clawdbot useful? I've seen videos where people seem to spend $150/day using clawdbot to do a few extremely basic things. Maybe my impression of what they're doing is wrong, but that would seem to suggest that the $20/mo subscription would get me \\~2 days of clawdbot usage per month.",
                  "score": 1,
                  "created_utc": "2026-01-26 20:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w32ib",
          "author": "Lonely-Elephant2130",
          "text": "It's impressive for sure, but man the setup barrier is real. Spent hours trying to get it running and couldn't figure it out (not a dev background). Currently using Super Intern (https://www.superintern.ai/ ) which is similar but actually simple - just chat interface, no setup. Way more my speed.",
          "score": 1,
          "created_utc": "2026-01-26 20:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yq4t3",
          "author": "BiggestSkrilla",
          "text": "need to do something about api cost.",
          "score": 1,
          "created_utc": "2026-01-27 05:13:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yvaaj",
          "author": "FutureboyWavo",
          "text": "Currently using clawdbot running on a VPs with minimax m2 model and I‚Äôm having so many issues.",
          "score": 1,
          "created_utc": "2026-01-27 05:51:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcs6h",
          "author": "Different-Pizza-7591",
          "text": "I found an App to connect to Clawdbot with iOS ! Not perfect but it works and it is free!   \n[https://apps.apple.com/us/app/nuvik/id6747774937](https://apps.apple.com/us/app/nuvik/id6747774937)",
          "score": 1,
          "created_utc": "2026-01-27 08:17:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ziga3",
          "author": "Technical_Self_9996",
          "text": "I've created a a step-by-step guide for non-developers to get going with Clawdbot. No assumed knowledge. Just copy, paste, and follow along. Hope people find it useful: [https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide](https://ibex.tech/under-the-hood/set-up-your-own-clawdbot-in-the-cloud-easy-guide)",
          "score": 1,
          "created_utc": "2026-01-27 09:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o208zsm",
          "author": "NineOneOne119",
          "text": "I'm attempting to use the Quen3 models from the [Featherless.ai](http://Featherless.ai) API in Clawdbot, but it's unable to locate the model Quen3/Quen3-14B. Do you have any suggestions?",
          "score": 1,
          "created_utc": "2026-01-27 12:47:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qgt9k",
          "author": "ThenExtension9196",
          "text": "I just got it running last night. \n\nIt‚Äôs literally another chatgpt4 moment. Tools like this are going to be huge this year.",
          "score": 1,
          "created_utc": "2026-01-26 01:23:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qi3wx",
              "author": "jpcaparas",
              "text": "yeah man personal ai is gonna explode this year. Tools like Poke and Clawdbot are gonna be big",
              "score": 2,
              "created_utc": "2026-01-26 01:30:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ux0vs",
          "author": "Professional-Cut7836",
          "text": "looking to get into the clawdbot hype? \n\nselling 1 time payment, 2 year gurantee #claude and #OpenAI api access. 0.017 Eth/50 USDT \n\nDm me!:)",
          "score": 0,
          "created_utc": "2026-01-26 17:51:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql4hwj",
      "title": "RTX Pro 6000 $7999.99",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "author": "I_like_fragrances",
      "created_utc": "2026-01-23 22:05:23",
      "score": 71,
      "num_comments": 63,
      "upvote_ratio": 0.96,
      "text": "Price of RTX Pro 6000 Max-Q edition is going for $7999.99 at Microcenter.\n\n[https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card](https://www.microcenter.com/product/697038/pny-nvidia-rtx-pro-6000-blackwell-max-q-workstation-edition-dual-fan-96gb-gddr7-pcie-50-graphics-card)\n\nDoes it seem like a good time to buy?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1ql4hwj/rtx_pro_6000_799999/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1c5v68",
          "author": "Green-Dress-113",
          "text": "If you want to run 2 or 4 GPUs, Max-Q is the way to go for cooling. I have dual blackwell 6000 pro workstations and one heats the other with the fans blowing sideways. While the power max is 600W I'm only seeing avg 300W during inference with peaks to 350W, but not full 600W consumption So Max-Q being 300W is the sweet spot for performance and cooling.",
          "score": 16,
          "created_utc": "2026-01-24 00:08:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d5r3r",
              "author": "SillyLilBear",
              "text": "I run two workstation cards I have them power limited to 300W and get 96% of the performance of 600W.",
              "score": 5,
              "created_utc": "2026-01-24 03:35:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1j8410",
                  "author": "m2845",
                  "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
                  "score": 1,
                  "created_utc": "2026-01-25 01:05:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j83rm",
              "author": "m2845",
              "text": "How is this for gaming? I've heard like 10% better than a 5090 (at full power that is, non-max Q). Kind of want a workstation I can also game on.",
              "score": 1,
              "created_utc": "2026-01-25 01:05:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bnzd7",
          "author": "Big_River_",
          "text": "the performance difference is more like 10% and yes 96gb gddr7 vram at 300w stable is best perf per watt there is to stack up a wrx90 threadripper build with - best card for home lab by far",
          "score": 19,
          "created_utc": "2026-01-23 22:33:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1db5zv",
              "author": "Sufficient-Past-9722",
              "text": "Just need a good waterblock for it.",
              "score": 1,
              "created_utc": "2026-01-24 04:09:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1dla2o",
                  "author": "Paliknight",
                  "text": "For a 300w card?",
                  "score": 2,
                  "created_utc": "2026-01-24 05:18:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1by4af",
          "author": "morriscl81",
          "text": "You can get it cheaper than that by at least $600-700 from companies like Exxact Corp",
          "score": 4,
          "created_utc": "2026-01-23 23:26:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bzwly",
              "author": "ilarp",
              "text": "how to order from them?",
              "score": 2,
              "created_utc": "2026-01-23 23:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1c0iyo",
                  "author": "morriscl81",
                  "text": "Go to their website and make a request for a quote. They will send you an invoice.  That‚Äôs how I got mine",
                  "score": 3,
                  "created_utc": "2026-01-23 23:39:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c08qr",
          "author": "MierinLanfear",
          "text": "Max q is lower power for if you want to run more than 2 cards on your workstation.   If your only running 1 or 2 cards go for the full version unless you plan on adding more later.\n\nThere are education discounts too.   Prices are likely to go up so now is a good time to buy.",
          "score": 4,
          "created_utc": "2026-01-23 23:37:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cep80",
          "author": "queerintech",
          "text": "I just bought a 5000 to pair with my 5070ti I considered the 6000 but whew.  üòÖ",
          "score": 3,
          "created_utc": "2026-01-24 00:56:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cgwrf",
          "author": "No-Leopard7644",
          "text": "Do you monetize this investment or it‚Äôs for fun?",
          "score": 3,
          "created_utc": "2026-01-24 01:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dre0b",
          "author": "gaidzak",
          "text": "Cheapest I found so far is from provantage for non education $7261 dollars. I hope this is real or I am understanding this pricing. \n\n[https://www.provantage.com/nvidia-9005g153220000001\\~7NVID0M1.htm](https://www.provantage.com/nvidia-9005g153220000001~7NVID0M1.htm)\n\nFor education purchases, any NVidia Partner can get them down to $6000;",
          "score": 3,
          "created_utc": "2026-01-24 06:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bx3g3",
          "author": "separatelyrepeatedly",
          "text": "Get workstation and power limit it",
          "score": 5,
          "created_utc": "2026-01-23 23:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1cecci",
              "author": "Big_River_",
              "text": "the max-q variant is also a blower card therefore well suited by design to stacking in a box - multiple workstation variants even power limited require powered ddr5 risers or they will thermal throttle each other - so if you only get one sure get a workstation otherwise max-q for sure",
              "score": 4,
              "created_utc": "2026-01-24 00:54:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1cg56k",
          "author": "Qs9bxNKZ",
          "text": "Naw.  Full power version.\n\nThen afterburner if you want to reduce power.  \n\nThen undervolt if you want to reduce heat.  \n\nMost people are not going to be running more than two of them in a case.  And I mean more than two because bifurcation along with chipset performance unless you‚Äôre on a threadripper or Xeon \n\nHave two in one box and I put out about 1200W at 70% along with 950mv.  \n\n$8099 for the black box version.  $7999 for the bulk nvidia version.  Minus $2-400 for education and bulk discounts.",
          "score": 5,
          "created_utc": "2026-01-24 01:04:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dr26a",
              "author": "gaidzak",
              "text": "Education pricing for a RTX 6000 Pro Server is 6k.. I'm about to hit the BUY button.",
              "score": 1,
              "created_utc": "2026-01-24 06:02:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1egntb",
                  "author": "t3rmina1",
                  "text": "Where are you getting 6k? I'm getting edu quotes that are a bit higher for WS",
                  "score": 1,
                  "created_utc": "2026-01-24 09:49:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bij91",
          "author": "snamuh",
          "text": "What‚Äôs the deal with the max-a ed?",
          "score": 4,
          "created_utc": "2026-01-23 22:06:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1biody",
              "author": "I_like_fragrances",
              "text": "It is 300W power versus 600W. Typically gets a 20% reduced performance but same VRAM and cores.",
              "score": 8,
              "created_utc": "2026-01-23 22:07:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1bjn31",
                  "author": "hornynnerdy69",
                  "text": "20% reduced performance isn‚Äôt nothing, especially when you consider that might put its performance below a 5090 for models that can fit in 32GB VRAM. And you could get a standard 6000 Pro for under 20% more money (seeing them at like $8750 recently)",
                  "score": 4,
                  "created_utc": "2026-01-23 22:12:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1bk6hr",
                  "author": "nero519",
                  "text": "I don't get it, what's the point of it? Do they just sell a normal card with artificial limitations to make it cheaper or is there something actually missing, like slower memories or less cores",
                  "score": 3,
                  "created_utc": "2026-01-23 22:14:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1bzbor",
              "author": "getfitdotus",
              "text": "Better card i run 4 of these. All air cooled and work great max 60c",
              "score": 2,
              "created_utc": "2026-01-23 23:32:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dr57r",
          "author": "Foreign_Presence7344",
          "text": "Could you use the workstation version and a max q in the same box?",
          "score": 2,
          "created_utc": "2026-01-24 06:02:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dvdor",
          "author": "AlexGSquadron",
          "text": "They were going for $7500??",
          "score": 2,
          "created_utc": "2026-01-24 06:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f8zog",
          "author": "gweilojoe",
          "text": "Get from Computer Central - They don‚Äôt charge tax for purchases outside of California. I bought mine from them (regular version not Qmax) and it‚Äôs worked great.",
          "score": 2,
          "created_utc": "2026-01-24 13:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbkhb",
          "author": "Phaelon74",
          "text": "You can get them for 5800 ish if you spend the time to find the vendor, do inception program, etc.",
          "score": 2,
          "created_utc": "2026-01-24 13:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1cfxrs",
          "author": "Cold_Hard_Sausage",
          "text": "Is this one of those gadgets that‚Äôs going to be 10 bucks in 5 years?",
          "score": 6,
          "created_utc": "2026-01-24 01:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dbxs7",
              "author": "Sufficient-Past-9722",
              "text": "If you had bought an Ampere A6000 back in January 2021, you could still sell it for something close to the original price. Similar for the 3090.",
              "score": 8,
              "created_utc": "2026-01-24 04:14:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1cjk1m",
              "author": "Br4ne",
              "text": "more like 20k in 2 months if you ask me",
              "score": 3,
              "created_utc": "2026-01-24 01:24:09",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fa25c",
              "author": "Mysterious-String420",
              "text": "Share a link to any current 10$ gadget with butt loads of vram",
              "score": 1,
              "created_utc": "2026-01-24 13:43:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1g1tep",
                  "author": "Cold_Hard_Sausage",
                  "text": "Bro, have someone teach you the concept of sarcasm",
                  "score": 1,
                  "created_utc": "2026-01-24 16:08:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1r7bdf",
          "author": "Rain_Sunny",
          "text": "When we use this Graphic card to build a 4-cards Ai-workstation:\n\nTotal VRAM: 384 GB.\n\nBandwidth: 7.2 TB.\n\nComput PowerÔºàFP4Ôºâ: 16 PFLOPs.\n\nTokens Output: 50 tokens/s(70B). 15 Tokens/s(405B).\n\nLLMs Support: DeepSeek V3,Llama 405B...\n\nPrice will be acound 60000 USD.\n\nBy the way, graphic card's price seems very good!",
          "score": 1,
          "created_utc": "2026-01-26 03:41:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21ecgn",
          "author": "kaisersolo",
          "text": "Nvidia would make more bucks with 3 5090's instead of this even though we all want it",
          "score": 1,
          "created_utc": "2026-01-27 16:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ck7rm",
          "author": "Accomplished-Grade78",
          "text": "Anyone compare dual Max-q vs DGXA Spark? \n\n192GB vs 128GB\n\nDDR7 vs LPDDR5X unified \n\nWhat does this mean for real world performance, in your experience?",
          "score": 1,
          "created_utc": "2026-01-24 01:28:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ndmwh",
              "author": "One-Macaron6752",
              "text": "LMGTFY",
              "score": 2,
              "created_utc": "2026-01-25 17:07:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1otzko",
                  "author": "Accomplished-Grade78",
                  "text": "Thanks, it‚Äôs nice to hear recent experiences",
                  "score": 1,
                  "created_utc": "2026-01-25 20:51:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d0fbd",
          "author": "TheRiddler79",
          "text": "If I'm being fair, that thing looks bad to the fucking bone, but if I what's going to spend eight grand right now, I'd probably look for server box that would run eight V100 32 GB. Like I understand the difference in the technology and I suppose it just depends on your ultimate goal, but you could run twice as large of an AI and your inference speed would still be lightning fast. But again everybody has their own motivations. For me I'm kind of like looking at where can I get the largest amount of vram for the minimal amount of money which I'm sure most people also think but at the end of the day I also will trade 2000 tokens a second on GPT OSS 120b for 500 tokens a second on Minimax 2.1",
          "score": 1,
          "created_utc": "2026-01-24 03:02:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkudvz",
      "title": "I gave my local LLM pipeline a brain - now it thinks before it speaks",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "author": "danny_094",
      "created_utc": "2026-01-23 15:50:14",
      "score": 70,
      "num_comments": 26,
      "upvote_ratio": 0.92,
      "text": "[Video from sequential retrieval](https://reddit.com/link/1qkudvz/video/9mel0vq9d4fg1/player)\n\nIn the video you can see how and that it works.\n\nJarvis/TRION has received a major update after weeks of implementation. Jarvis (soon to be TRION) has now been provided with a self-developed SEQUENTIAL THINKING MCP.\n\nI would love to explain everything it can do in this Reddit post. But I don't have the space, and neither do you have the patience.¬†[u/frank\\_brsrk](/user/frank_brsrk/)¬†Provided a self-developed CIM framework That's hard twisted with Sequential Thinking. So Claude help for the answer:\n\nüß† Gave my local Ollama setup \"extended thinking\" - like Claude, but 100% local\n\nTL;DR: Built a Sequential Thinking system that lets DeepSeek-R1\n\n\"think out loud\" step-by-step before answering. All local, all Ollama.\n\nWhat it does:\n\n\\- Complex questions ‚Üí AI breaks them into steps\n\n\\- You SEE the reasoning live (not just the answer)\n\n\\- Reduces hallucinations significantly\n\nThe cool part: The AI decides WHEN to use deep thinking.\n\nSimple questions ‚Üí instant answer.\n\nComplex questions ‚Üí step-by-step reasoning first.\n\nBuilt with: Ollama + DeepSeek-R1 + custom MCP servers\n\nShoutout to¬†[u/frank\\_brsrk](/user/frank_brsrk/)¬†for the CIM framework that makes\n\nthe reasoning actually make sense.\n\nGitHub: [https://github.com/danny094/Jarvis/tree/main](https://github.com/danny094/Jarvis/tree/main)\n\nHappy to answer questions! This took weeks to build üòÖ\n\nOther known issues:\n\n\\- excessively long texts, skipping the control layer - Solution in progress\n\n\\- The side panel is still being edited and will be integrated as a canvas with MCP support.\n\nsimple graphic:\n\n[Simple visualization of MCP retrieval](https://preview.redd.it/f9tm59rkd4fg1.png?width=863&format=png&auto=webp&s=6a65eda552b3b846863f75c59ee04018eb6d6c41)\n\n[Simple visualization pipeline](https://preview.redd.it/6h40kd1sd4fg1.png?width=1147&format=png&auto=webp&s=770dd510d8ebf0fe8b68f6a81bb7ab40d34fa862)\n\n@/[frank\\_brsrk](/user/frank_brsrk/) architecture of the causal intelligence module\n\n[architecture of the causal intelligence module](https://preview.redd.it/6x03fioje4fg1.jpg?width=2800&format=pjpg&auto=webp&s=7df7325899fd7dd3f8ca8743ebca4d04845868b5)\n\n\n\nSmall update the next days:\n\nhttps://preview.redd.it/9q7vo4rnbqfg1.png?width=1866&format=png&auto=webp&s=193c2a1adaabd721b0c04a8386dd9acf3b49f5ff\n\nhttps://preview.redd.it/bwcvm4rnbqfg1.png?width=1866&format=png&auto=webp&s=2457293e38992f70ff8290fada20104b19756a16\n\nhttps://preview.redd.it/ej38q5rnbqfg1.png?width=1866&format=png&auto=webp&s=efd4b330ed74bc48e9af713cc0e5568b94c3a5f7\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkudvz/i_gave_my_local_llm_pipeline_a_brain_now_it/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1amvip",
          "author": "GCoderDCoder",
          "text": "This is great! I love that a whole generation of technologists have spent our lives trying to be Iron Man and the few making the most progress by working together on it are trying to ruin it for the rest of us now that we can finally see the light at the other end of the tunnel. Thanks for helping the rest of us in the struggle to keep up!\n\nI'll be more focused one these types of projects when they eventually fire me because they don't realize how much we have to correct the AI still but I wish I had time to be consistent working on some joint projects like this. I'm trying to figure out how to piece together things like roo code with something like vibe kanban and MCPs in an opinionated way to reduce the manual burden while allowing more than coding using local LLMs and then here goes Anthropic with their cowork thing lol\n\nKeep up the hard work! When the fairytale of benevolent AI providers crumbles people will be looking for local LLMs.",
          "score": 13,
          "created_utc": "2026-01-23 19:38:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1aoe33",
              "author": "danny_094",
              "text": "That's exactly why I'm building this. Local-first, privacy-first, yours forever.",
              "score": 9,
              "created_utc": "2026-01-23 19:45:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a5160",
          "author": "No-Leopard7644",
          "text": "Congratulations, sounds interesting, thank you for sharing this information. Is it open source?",
          "score": 4,
          "created_utc": "2026-01-23 18:16:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1a6tb4",
              "author": "danny_094",
              "text": "Yes you can Download on GitHub. It will always be free for private users. Everything for local users :)",
              "score": 9,
              "created_utc": "2026-01-23 18:24:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1c4d05",
          "author": "Endflux",
          "text": "If I can find the time I'll give it a try this weekend and let you know how it goes! Thanks for sharing :)",
          "score": 3,
          "created_utc": "2026-01-24 00:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c4op2",
              "author": "danny_094",
              "text": "I'm asking for it. I really need more user feedback :D",
              "score": 2,
              "created_utc": "2026-01-24 00:01:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aq2xn",
          "author": "burn-n-die",
          "text": "What's the system configuration you are using?",
          "score": 2,
          "created_utc": "2026-01-23 19:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1avqci",
              "author": "danny_094",
              "text": "Everything runs on an RTX 2060 Super.CPU and RAM aren't really being used. You can find my Ollama container file in the wiki.",
              "score": 3,
              "created_utc": "2026-01-23 20:19:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1b1v5b",
                  "author": "burn-n-die",
                  "text": "Thanks. I am not a software engineer and I don't code. Will you guide be straight forward?",
                  "score": 2,
                  "created_utc": "2026-01-23 20:48:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dvx2l",
          "author": "Hot_Rip_4912",
          "text": "Wow ,man that feels good",
          "score": 2,
          "created_utc": "2026-01-24 06:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f58ev",
          "author": "JinkerGaming",
          "text": "Amazing! Thank you good sir. I will certainly be forking this. :)",
          "score": 2,
          "created_utc": "2026-01-24 13:13:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixovi",
          "author": "sweetbacon",
          "text": "This looks **very** interesting, thanks for sharing.",
          "score": 2,
          "created_utc": "2026-01-25 00:10:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j5isr",
          "author": "yeahlloow",
          "text": "Can someone please explain what is the difference between this and the thinking mode of normal LLMs?",
          "score": 2,
          "created_utc": "2026-01-25 00:51:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mpyju",
              "author": "danny_094",
              "text": "LLMs always think and answer based on the prediction of the next word. It's like autocorrect on steroids. In this case, the LLM is given less room to digress. So that even in longer contexts and questions, it doesn't start to digress or make things up. Imagine the LLM is the train, and the pipeline is the track. There are few opportunities to deviate.",
              "score": 1,
              "created_utc": "2026-01-25 15:23:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dk5rd",
          "author": "leonbollerup",
          "text": "Hey, if you have 15 min to spare ‚Ä¶ listen this pod cast, or scroll forward to the part about augmented LLM - maybe that‚Äôs something that improve your solution even more\n\n[https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1](https://notebooklm.google.com/notebook/ce7a185f-6928-4819-a800-0a22c1714ae3?artifactId=f7b71f77-c8d6-4144-9539-e5006ae4aafe&pli=1)\n\nI‚Äôm far from done with ArcAI.. but I could share you the concepts.\n\nhttps://preview.redd.it/sgo3krcbd8fg1.jpeg?width=812&format=pjpg&auto=webp&s=57fbd6c153fac073071e46c4985b71fadb7fcaeb\n\nThese are the result of controlled continued tests",
          "score": 1,
          "created_utc": "2026-01-24 05:10:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1usmpu",
          "author": "danny_094",
          "text": "https://preview.redd.it/hpui09pebqfg1.png?width=1866&format=png&auto=webp&s=c463c0af62940e7b2dc348c69405414f3aa7336e\n\nI'm feeling a bit over-motivated right now.The web interface will be slightly improved.",
          "score": 1,
          "created_utc": "2026-01-26 17:32:13",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1nlpqo",
          "author": "Available-Craft-5795",
          "text": "It kinda sounds like COCONUT but with a really long latent space. I dont really see the point right now",
          "score": 0,
          "created_utc": "2026-01-25 17:42:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nq626",
              "author": "danny_094",
              "text": "Fair comparison! The key difference: Jatvus/TRION isnt about latent reasoning  itss about orchestrating multiple models with explicit validation checkpoints (CIM catches antipatterns, biases, fallacies). Think of it as adding guardrails + quality control between reasoning steps, not just making the reasoning longer. The 3 layer architecture lets you use smaller, specialized models instead of one massive model.",
              "score": 1,
              "created_utc": "2026-01-25 18:00:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1obt0v",
                  "author": "Available-Craft-5795",
                  "text": "Then that sounds like a TRM with COCONUT smashed togeather",
                  "score": 1,
                  "created_utc": "2026-01-25 19:31:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1oc25t",
                  "author": "Available-Craft-5795",
                  "text": "Can you explain what it does that a TRM or COCONUT architecture doesnt do?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:32:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjqhja",
      "title": "This Week's Hottest Hugging Face Releases: Top Picks by Category!",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "author": "techlatest_net",
      "created_utc": "2026-01-22 09:52:51",
      "score": 52,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hugging Face trending is on fire this week with fresh drops in text generation, image, audio, and more.\n\nCheck 'em out and drop your thoughts‚Äîwhich one's getting deployed first?\n\n# Text Generation\n\n* [**zai-org/GLM-4.7-Flash**](https://huggingface.co/zai-org/GLM-4.7-Flash): 31B param model for fast, efficient text gen‚Äîupdated 2 days ago with 124k downloads and 932 likes. Ideal for real-time apps and agents.\n* [**unsloth/GLM-4.7-Flash-GGUF**](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF): Quantized 30B version for easy local inference‚Äîhot with 112k downloads in hours. Great for low-resource setups.\n\n# Image / Multimodal\n\n* [**zai-org/GLM-Image**](https://huggingface.co/zai-org/GLM-Image): Image-text-to-image powerhouse‚Äî10.8k downloads, 938 likes. Excels in creative edits and generation.\n* [**google/translategemma-4b-it**](https://huggingface.co/google/translategemma-4b-it): 5B vision-language model for multilingual image-text tasks‚Äî45.4k downloads, supports translation + vision.\n\n# Audio / Speech\n\n* [**kyutai/pocket-tts**](https://huggingface.co/kyutai/pocket-tts): Compact TTS for natural voices‚Äî38.8k downloads, 397 likes. Pocket-sized for mobile/edge deployment.\n* [**microsoft/VibeVoice-ASR**](https://huggingface.co/microsoft/VibeVoice-ASR): 9B ASR for multilingual speech recognition‚Äîultra-low latency, 816 downloads already spiking.\n\n# Other Hot Categories (Video/Agentic)\n\n* [**Lightricks/LTX-2**](https://huggingface.co/Lightricks/LTX-2) (Image-to-Video): 1.96M downloads, 1.25k likes‚Äîpro-level video from images.\n* [**stepfun-ai/Step3-VL-10B**](https://huggingface.co/stepfun-ai/Step3-VL-10B) (Image-Text-to-Text): 10B VL model for advanced reasoning‚Äî28.6k downloads in hours.\n\nThese are dominating trends with massive community traction.",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjqhja/this_weeks_hottest_hugging_face_releases_top/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o14xz85",
          "author": "Count_Rugens_Finger",
          "text": "I have found that GLM-4.7 30B-A3B model is actually inferior to Qwen3-Coder 30B-A3B for programming.  Anyone else?",
          "score": 1,
          "created_utc": "2026-01-22 23:07:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dhymn",
          "author": "Blizado",
          "text": "Why is Qwen3-TTS missing? For me clearly the most exciting new TTS models, especially since they have multi-language support and not only english as still too many new TTS models have.",
          "score": 1,
          "created_utc": "2026-01-24 04:55:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3cgo",
      "title": "the state of local agentic \"action\" is still kind of a mess",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "author": "Ilove_Cakez",
      "created_utc": "2026-01-21 16:56:55",
      "score": 50,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "spent the last few nights trying to get a decent mcp setup running for my local stack and it‚Äôs honestly depressing how much friction there still is. we‚Äôve got these massive models running on consumer hardware, but as soon as you want them to actually do anything.. like pull from a local db or interact with an api so you‚Äôre basically back to writing custom boilerplate for every single tool.\n\nthe security trade-offs are the worst part. it‚Äôs either total isolation (useless) or giving the model way too much permission because managing granular mcp servers manually is a full-time job. i‚Äôve been trying to find a middle ground where i don‚Äôt have to hand-roll the auth and logging for every connector.\n\nfound a tool that‚Äôs been helping with the infra side of it. it basically just handles the mcp server generation and the governance/permissions layer so i don't have to think too much (ogment ai, i'm sure most of you know about it). it‚Äôs fine for skipping the boring stuff, but i‚Äôm still annoyed that this isn't just native or more standardized yet.\n\nhow are you guys actually deploying agents that can touch your data? are you just building your own mcp wrappers from scratch or is there a better way to handle the permissioning? curious",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3cgo/the_state_of_local_agentic_action_is_still_kind/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0z1cn1",
          "author": "Fortyseven",
          "text": "I've had some modest success using 30b qwen3-coder and the Qwen Code TUI. It won't replace my cloud-based stuff, but I always give it a go now and then and find myself surprised at how capable it can be.",
          "score": 2,
          "created_utc": "2026-01-22 02:12:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0xvvdv",
          "author": "ForsookComparison",
          "text": "Running agentically is simply too hard for most small or mid-sized open weight models. Once you add a few tools and a multi-step feedback loop, the *bare minimum* becomes either Qwen3-Next-80B or Gpt-oss-120b, both of which still will fall flat on their face along the way. GLM 4.6V is probably the actual starting line for reliable use and even that will be spotty as you add more and more instructions and tools.\n\nI don't have a good answer for you besides *\"I've experienced this and think that agents are just hard\"*.",
          "score": 1,
          "created_utc": "2026-01-21 22:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o101cuk",
          "author": "mister2d",
          "text": "I'm mildly curious about this topic. How much could a framework like a self-hosted n8n instance fill the gaps?",
          "score": 1,
          "created_utc": "2026-01-22 06:02:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10t0ef",
          "author": "techlatest_net",
          "text": "Yeah man, local agents are still duct tape and prayers‚Äîtool perms either nuke security or turn models into toddlers with car keys. Ogment's solid for skipping the mcp boilerplate grind tho.\n\nCrewAI + containerized tools (dockerized APIs) saved my sanity‚Äîgranular perms via bind mounts, no root hell. You running ollama under that? What's your stack look like?",
          "score": 1,
          "created_utc": "2026-01-22 10:10:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wmw3y",
          "author": "atlasnomos",
          "text": "You‚Äôre not wrong ‚Äî the pain point you‚Äôre describing is real and structural, not a tooling gap.\n\nWhat keeps biting people is that ‚Äúagent action‚Äù today is basically glued together from three things that don‚Äôt want to coexist:\n\n1. prompt-time intent\n2. tool adapters (MCP / wrappers)\n3. runtime execution with side effects\n\nMost stacks solve (1) and (2), but almost nobody treats (3) as a **first-class runtime concern**. So permissions end up living either:\n\n* inside prompts (brittle),\n* inside per-tool wrappers (boilerplate explosion),\n* or at the MCP server boundary (coarse, hard to reason about).\n\nThe isolation vs over-permission tradeoff you mention is exactly what happens when there‚Äôs no **deny-by-default execution layer** sitting *between* the model and the tools.\n\nWhat‚Äôs worked best for us locally is:\n\n* keep MCP servers dumb (pure capability exposure)\n* move auth, cost limits, logging, and allow/deny decisions into a single runtime gate\n* treat every tool call as an auditable event, not ‚Äújust another function call‚Äù\n\nThat way you‚Äôre not re-implementing auth + logging per connector, and you‚Äôre not trusting the model to behave just because the prompt says so.\n\nI agree it *should* be more native / standardized. Until there‚Äôs a real spec for agent execution (not just tool schemas), everyone‚Äôs either rolling wrappers or accepting scary permissions.\n\nCurious what level you‚Äôre aiming for locally:\n\n* human-in-the-loop approvals?\n* strict allowlists?\n* cost / rate enforcement?\n* or just ‚Äúdon‚Äôt nuke my data‚Äù?\n\nThat choice seems to drive everything else.",
          "score": -4,
          "created_utc": "2026-01-21 19:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0yrrf6",
              "author": "Big-Masterpiece-9581",
              "text": "I wish this kind of sanity would reign. But working in Big Corp is a bit like the Trump Administration in this job market. Pure politics, cloak and dagger, backstabbing, constant reorgs. Nobody is sharing anything, and each is reinventing every wheel especially around AI / MCP.",
              "score": 1,
              "created_utc": "2026-01-22 01:17:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o11lp1w",
                  "author": "atlasnomos",
                  "text": "That‚Äôs a fair take ‚Äî and honestly, it matches what we keep hearing from people inside large orgs.\n\nMost of the time it‚Äôs not that the problems aren‚Äôt understood; it‚Äôs that ownership is fragmented and incentives don‚Äôt line up. Governance ends up living in the cracks between teams, so everyone quietly rebuilds their own glue and hopes it never becomes *their* incident.\n\nWe‚Äôre under no illusion that clean architectures win on their own. In practice, they only surface once failure, liability, or regulatory pressure forces shared responsibility. Until then, it‚Äôs politics and reorgs all the way down.\n\nAppreciate you saying this out loud ‚Äî it‚Äôs useful context, not pushback.",
                  "score": 1,
                  "created_utc": "2026-01-22 13:38:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qiswy6",
      "title": "Olmo 3.1 32B Think ‚Äî second place on hard reasoning, beating proprietary flagships",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-01-21 08:56:26",
      "score": 49,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Running peer evaluations of frontier models (The Multivac). Today's constraint satisfaction puzzle had interesting results for local LLM folks.\n\n**Top 3:**\n\n1. Gemini 3 Pro Preview: 9.13\n2. **Olmo 3.1 32B Think: 5.75** ‚Üê Open source\n3. GPT-OSS-120B: 4.79 ‚Üê Open source\n\n**Models Olmo beat:**\n\n* Claude Opus 4.5 (2.97)\n* Claude Sonnet 4.5 (3.46)\n* Grok 3 (2.25)\n* DeepSeek V3.2 (2.99)\n\n**The task:** Schedule 5 people for meetings across Mon-Fri with 9 interlocking logical constraints. Requires recognizing structural impossibilities and systematic constraint propagation.\n\n**Notes on Olmo:**\n\n* High variance (¬±4.12) ‚Äî inconsistent but strong ceiling\n* Extended thinking appears to help on this problem class\n* 32B is runnable on consumer hardware (with quantization)\n* Apache 2.0 license\n\n**Questions for the community:**\n\n* What quantizations are people running Olmo 3.1 at?\n* Performance on other reasoning tasks?\n* Any comparisons vs DeepSeek for local deployment?\n\nFull results at [themultivac.com](http://themultivac.com)\n\nLink: [https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm\\_campaign=post&utm\\_medium=web&showWelcomeOnShare=true](https://open.substack.com/pub/themultivac/p/logic-grid-meeting-schedule-solve?r=72olj0&utm_campaign=post&utm_medium=web&showWelcomeOnShare=true)\n\nhttps://preview.redd.it/jko9h4ox2oeg1.png?width=1208&format=png&auto=webp&s=07f7967899cc6f7d6252eed866ef5f4003f3288b\n\n**Daily runs and Evals. Cheers!**",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qiswy6/olmo_31_32b_think_second_place_on_hard_reasoning/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0tqymb",
          "author": "Mabuse046",
          "text": "I like Allen AI's reasoning dataset and I tend to take chunks of it to train my own models on. It's nice to see them scoring highly.",
          "score": 8,
          "created_utc": "2026-01-21 09:13:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tzro0",
              "author": "randygeneric",
              "text": "you mean benchmaxing?",
              "score": -5,
              "created_utc": "2026-01-21 10:36:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ua463",
                  "author": "silenceimpaired",
                  "text": "Why on earth are you asking this question? No where does the person you‚Äôre commenting on mention test sets. They mention reasoning traces. Now if you were to say Allen AI has polluted datasets with test data present that would make sense.",
                  "score": 9,
                  "created_utc": "2026-01-21 12:01:03",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0w2m31",
                  "author": "Vegetable-Second3998",
                  "text": "This is such a dumb term. If a model learns the concepts of a benchmark, it has learned it. Create better benchmarks then. That‚Äôs just doing what humans also do - memorize shit for efficiency.",
                  "score": 3,
                  "created_utc": "2026-01-21 17:33:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0tzul1",
                  "author": "Mabuse046",
                  "text": "What do you mean?",
                  "score": 6,
                  "created_utc": "2026-01-21 10:37:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0xps63",
          "author": "segmond",
          "text": "I hope you're right.  If so, this goes to show that folks should stop looking for the ONE magic model.  We saw with qwen2.5-32b-coder that a model can be very good in a specific domain.",
          "score": 2,
          "created_utc": "2026-01-21 21:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z5lcr",
          "author": "TheOdbball",
          "text": "Luckily for everyone I‚Äôm building a headless Linux based file system that would make changing out your LLM without losing years of productivity on the fly üòé\n\nWill definitely try olmo out thanks",
          "score": 1,
          "created_utc": "2026-01-22 02:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o152b5x",
          "author": "Direct_Turn_1484",
          "text": "I tried running this one in Ollama and it crashed. Yes I updated Ollama. I tried downloading it again. Still crashed. I really wanted to play with Omni-3.1. I guess I‚Äôll just have to try with vllm.",
          "score": 1,
          "created_utc": "2026-01-22 23:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o159t1u",
              "author": "Chalutation",
              "text": "vllm is way more efficient than ollama, but you don't have the pull feature like ollama does.",
              "score": 1,
              "created_utc": "2026-01-23 00:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o15u91m",
              "author": "jadecamaro",
              "text": "Crashed for me in LM Studio too even lowering context down",
              "score": 1,
              "created_utc": "2026-01-23 02:01:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qitnbv",
      "title": "The Case for a $600 Local LLM Machine",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "author": "tony10000",
      "created_utc": "2026-01-21 09:42:30",
      "score": 48,
      "num_comments": 58,
      "upvote_ratio": 0.93,
      "text": "**The Case for a $600 Local LLM Machine**\n\nUsing the Base Model Mac mini M4\n\nhttps://preview.redd.it/5c916gwucoeg1.png?width=1182&format=png&auto=webp&s=68d91da71f6244d752e15922e47dfbf9d792beb1\n\nby Tony Thomas\n\nIt started as a simple experiment. How much real work could I do on a small, inexpensive machine running language models locally?\n\nWith GPU prices still elevated, memory costs climbing, SSD prices rising instead of falling, power costs steadily increasing, and cloud subscriptions adding up, it felt like a question worth answering. After a lot of thought and testing, the system I landed on was a base model Mac mini M4 with 16 GB of unified memory, a 256 GB internal SSD, a USB-C dock, and a 1 TB external NVMe drive for model storage. Thanks to recent sales, the all-in cost came in right around $600.\n\nOn paper, that does not sound like much. In practice, it turned out to be far more capable than I expected.\n\nLocal LLM work has shifted over the last couple of years. Models are more efficient due to better training and optimization. Quantization is better understood. Inference engines are faster and more stable. At the same time, the hardware market has moved in the opposite direction. GPUs with meaningful amounts of VRAM are expensive, and large VRAM models are quietly disappearing. DRAM is no longer cheap. SSD and NVMe prices have climbed sharply.\n\nAgainst that backdrop, a compact system with tightly integrated silicon starts to look less like a compromise and more like a sensible baseline.\n\n**Why the Mac mini M4 Works**\n\nThe M4 Mac mini stands out because Apple‚Äôs unified memory architecture fundamentally changes how a small system behaves under inference workloads. CPU and GPU draw from the same high-bandwidth memory pool, avoiding the awkward juggling act that defines entry-level discrete GPU setups. I am not interested in cramming models into a narrow VRAM window while system memory sits idle. The M4 simply uses what it has efficiently.\n\nSixteen gigabytes is not generous, but it is workable when that memory is fast and shared. For the kinds of tasks I care about, brainstorming, writing, editing, summarization, research, and outlining, it holds up well. I spend my time working, not managing resources.\n\nThe 256 GB internal SSD is limited, but not a dealbreaker. Models and data live on the external NVMe drive, which is fast enough that it does not slow my workflow. The internal disk handles macOS and applications, and that is all it needs to do. Avoiding Apple‚Äôs storage upgrade pricing was an easy decision.\n\nThe setup itself is straightforward. No unsupported hardware. No hacks. No fragile dependencies. It is dependable, UNIX-based, and boring in the best way. That matters if you intend to use the machine every day rather than treat it as a side project.\n\n**What Daily Use Looks Like**\n\nThe real test was whether the machine stayed out of my way.\n\nQuantized 7B and 8B models run smoothly using Ollama and LM Studio. AnythingLLM works well too and adds vector databases and seamless access to cloud models when needed. Response times are short enough that interaction feels conversational rather than mechanical. I can draft, revise, and iterate without waiting on the system, which makes local use genuinely viable.\n\nLarger 13B to 14B models are more usable than I expected when configured sensibly. Context size needs to be managed, but that is true even on far more expensive systems. For single-user workflows, the experience is consistent and predictable.\n\nWhat stood out most was how quickly the hardware stopped being the limiting factor. Once the models were loaded and tools configured, I forgot I was using a constrained system. That is the point where performance stops being theoretical and starts being practical.\n\nIn daily use, I rotate through a familiar mix of models. Qwen variants from 1.7B up through 14B do most of the work, alongside Mistral instruct models, DeepSeek 8B, Phi-4, and Gemma. On this machine, smaller Qwen models routinely exceed 30 tokens per second and often land closer to 40 TPS depending on quantization and context. These smaller models can usually take advantage of the full available context without issue.\n\nThe 7B to 8B class typically runs in the low to mid 20s at context sizes between 4K and 16K. Larger 13B to 14B models settle into the low teens at a conservative 4K context and operate near the upper end of acceptable memory pressure. Those numbers are not headline-grabbing, but they are fast enough that writing, editing, and iteration feel fluid rather than constrained. I am rarely waiting on the model, which is the only metric that actually matters for my workflow.\n\n**Cost, Power, and Practicality**\n\nAt roughly $600, this system occupies an important middle ground. It costs less than a capable GPU-based desktop while delivering enough performance to replace a meaningful amount of cloud usage. Over time, that matters more than peak benchmarks.\n\nThe Mac mini M4 is also extremely efficient. It draws very little power under sustained inference loads, runs silently, and requires no special cooling or placement. I routinely leave models running all day without thinking about the electric bill.\n\nThat stands in sharp contrast to my Ryzen 5700G desktop paired with an Intel B50 GPU. That system pulls hundreds of watts under load, with the B50 alone consuming around 50 watts during LLM inference. Over time, that difference is not theoretical. It shows up directly in operating costs.\n\nThe M4 sits on top of my tower system and behaves more like an appliance. Thanks to my use of a KVM, I can turn off the desktop entirely and keep working. I do not think about heat, noise, or power consumption. That simplicity lowers friction and makes local models something I reach for by default, not as an occasional experiment.\n\n**Where the Limits Are**\n\nThe constraints are real but manageable. Memory is finite, and there is no upgrade path. Model selection and context size require discipline. This is an inference-first system, not a training platform.\n\nApple Silicon also brings ecosystem boundaries. If your work depends on CUDA-specific tooling or experimental research code, this is not the right machine. It relies on Apple‚Äôs Metal backend rather than NVIDIA‚Äôs stack. My focus is writing and knowledge work, and for that, the platform fits extremely well.\n\n**Why This Feels Like a Turning Point**\n\nWhat surprised me was not that the Mac mini M4 could run local LLMs. It was how well it could run them given the constraints.\n\nFor years, local AI was framed as something that required large amounts of RAM, a powerful CPU, and an expensive GPU. These systems were loud, hot, and power hungry, built primarily for enthusiasts. This setup points in a different direction. With efficient models and tightly integrated hardware, a small, affordable system can do real work.\n\nFor writers, researchers, and independent developers who care about control, privacy, and predictable costs, a budget local LLM machine built around the Mac mini M4 no longer feels experimental. It is something I turn on in the morning, leave running all day, and rely on without thinking about the hardware.\n\nMore than any benchmark, that is what matters.\n\nFrom: tonythomas-dot-net",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitnbv/the_case_for_a_600_local_llm_machine/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0uugib",
          "author": "TimLikesAI",
          "text": "I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 and use it similarly. The extra headroom allows for running some pretty powerful models that are in the 14-20GB range.",
          "score": 12,
          "created_utc": "2026-01-21 14:06:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0x9s9c",
              "author": "fallingdowndizzyvr",
              "text": "> I bought a refurbished M2 Max Mac Studio w/ 32GB of ram on an Amazon deal a few months back for $900 \n\nYou could have gotten it new for cheaper on Ebay from a liquidator.",
              "score": 0,
              "created_utc": "2026-01-21 20:45:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0y4lrx",
                  "author": "jcktej",
                  "text": "Asking for a friend. How do I find such vendors?",
                  "score": 1,
                  "created_utc": "2026-01-21 23:12:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tyaic",
          "author": "PraxisOG",
          "text": "I sincerely hope this is the future. An easy to use, low upfront and ongoing cost box that privately serves LLMs and maybe more. The software, while impressive, leaves much to be desired in terms of usability. This is from the perspective of having recently thrown together the exact kind of loud and expensive box you mentioned, that took days to get usable output from.¬†",
          "score": 6,
          "created_utc": "2026-01-21 10:22:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0z12z0",
              "author": "kermitt81",
              "text": "The M5 Mac Mini is expected sometime in the middle of this year, and offers significant improvements for LLM usage over the M4 Mac Mini. Pricing will likely be around the same, so it may be well worth the wait. \n\n(Each of the M5‚Äôs 10 GPU cores now includes a dedicated Neural Accelerator, and - according to benchmarks - the M5 delivers 3.6x faster time to first token compared to the M4.)",
              "score": 5,
              "created_utc": "2026-01-22 02:11:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o12v5g7",
                  "author": "tony10000",
                  "text": "It will be interesting to see how the recent memory and storage price increases will impact pricing.  I don't think we will see a sub-$500 deal on the M5 Mini anytime soon.",
                  "score": 2,
                  "created_utc": "2026-01-22 17:15:31",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0uc6wg",
          "author": "locai_al-ibadi",
          "text": "It is great seeing the capabilities of localised AI recently, compared to what we were capable of running a year ago (arguably even a months ago).",
          "score": 3,
          "created_utc": "2026-01-21 12:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ud5rp",
          "author": "alias454",
          "text": "If you look around you can get it for cheaper than that(micro center and best buy open box deals). I picked up an open box one and have actually been impressed. The onboard storage is probably the biggest complaint but it will do for now. My main laptop is an older Lenovo Legion 5 with 32GBs of ddr4 and an rtx2060. It was purchased around 2020 so it is starting to show it's age.",
          "score": 3,
          "created_utc": "2026-01-21 12:23:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v5cvv",
              "author": "tony10000",
              "text": "It is easy to add an external drive or a dock with a NVME slot for additional storage.  I keep all of the models, data, and caches on that.",
              "score": 4,
              "created_utc": "2026-01-21 15:01:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0i9e",
          "author": "Rabo_McDongleberry",
          "text": "For me. Speed isn't that big of an issue. And most of the things I do are just basic text generation and editing. Maybe answer a few questions that I cross references with legit sources. The Mac Mini works great for that.¬†",
          "score": 3,
          "created_utc": "2026-01-21 14:37:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wixf8",
          "author": "dual-moon",
          "text": "how much have you tried small models? many of them are extremely good; lots of tiny models get used as subagents in swarm setups. LiquidAI actually JUST released a 1.2B LFM2.5 Thinking model that would probably FLY on your machine :)",
          "score": 2,
          "created_utc": "2026-01-21 18:44:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103wgb",
              "author": "tony10000",
              "text": "Yes.  I use the small LiquidAI models, and Qwen 1.7B is a tiny-mighty LLM for drafting.",
              "score": 3,
              "created_utc": "2026-01-22 06:22:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1at66e",
                  "author": "GeroldM972",
                  "text": "Can concur about the LFM2 and LFM 2.5 models, because these are part of the set 'LM Studio - editor's pick' and for a very good reason.  These work really well with the LM Studio software.\n\nMy system is Ryzen 5 2400 with W11. 32 GB DDR4 RAM, a Crucial 2,5\" SSD (240 GB) and a AMD R580 GPU...with 16 GB of VRAM on it. So, 'simple' and 'weak-sauce' would be apt descriptions of this system. And yet, those LFM models work very well in here. Even if it is via Vulkan.\n\nedit:  \nIf you use LM Studio, it comes with MCP support, so these small models can now also look on the internet, can keep track of time and a few other things I find handy. Is very easy to set up and your local LLM becomes much more useful (if you trust information on the internet of course).",
                  "score": 1,
                  "created_utc": "2026-01-23 20:07:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0wub17",
              "author": "cuberhino",
              "text": "I‚Äôm currently considering a threadripper + 3090 build at around $2000 total cost to function as a local private ChatGPT replacement. \n\nDo you think this is overkill and I should go with one of these cheaper Mac systems?",
              "score": 2,
              "created_utc": "2026-01-21 19:35:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0x4ao8",
                  "author": "dual-moon",
                  "text": "try out some smaller models first, see how they work for you! if you find small models do the job, then scaling based on an agentic swarm rather than a single model may be best! but it really depends on what you want to use it for. if it's just chatting, deepseek can do most of what the big guys can!\n\nbut don't think a threadripper and a 3090 is a bad idea or anything :p",
                  "score": 2,
                  "created_utc": "2026-01-21 20:20:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0y9qn3",
          "author": "yeeah_suree",
          "text": "Nice write up! Can you share a little more information on what you use the model for? What constitutes everyday use?",
          "score": 2,
          "created_utc": "2026-01-21 23:39:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103q6c",
              "author": "tony10000",
              "text": "I am a writer.  I use AI for brainstorming, outlining, summarizing, drafting, and sometimes editing and polishing.",
              "score": 1,
              "created_utc": "2026-01-22 06:21:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o13x82a",
                  "author": "tony10000",
                  "text": "I just posted another article today on this forum that details my workflow.",
                  "score": 0,
                  "created_utc": "2026-01-22 20:06:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0zg50d",
          "author": "Icy-Pay7479",
          "text": "Was this written by an 8b model?",
          "score": 2,
          "created_utc": "2026-01-22 03:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103f9p",
              "author": "tony10000",
              "text": "I typically use Qwen 14B for outlining, and my primary drafting models are Qwen 3B and 4B.  Sometimes even 1.7.  I use ChatGPT to polish and then heavily edit the result.",
              "score": 1,
              "created_utc": "2026-01-22 06:19:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zpx40",
          "author": "crossfitdood",
          "text": "dude you missed the black friday deals. I bought mine from costco for $479. But I was really upset when Microcenter had them on sale for $399",
          "score": 2,
          "created_utc": "2026-01-22 04:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1041ul",
              "author": "tony10000",
              "text": "Bought it from Amazon for $479.  It was around $600 all in for the M4, dock, and 1TB NVME.",
              "score": 1,
              "created_utc": "2026-01-22 06:24:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zretd",
          "author": "vmjersey",
          "text": "But can you play GTA V on it?",
          "score": 2,
          "created_utc": "2026-01-22 04:49:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1045ag",
              "author": "tony10000",
              "text": "No idea what that is.",
              "score": 2,
              "created_utc": "2026-01-22 06:24:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10f5cl",
          "author": "lucasbennett_1",
          "text": "Quantized 7B-14B like qwen3 or deepseek 8B run fluidly for writing/research without the power/heat mess of discrete GPUs. THe external nvme for models is a smart hack to dodge apples storage premiums too",
          "score": 2,
          "created_utc": "2026-01-22 08:00:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13xfmc",
              "author": "tony10000",
              "text": "I agree!",
              "score": 1,
              "created_utc": "2026-01-22 20:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11zlww",
          "author": "Known_Geologist1085",
          "text": "I know this convo is about doing it on the cheap, but I'd like to note that I have been running an m3 max macbook pro with 128GB ram for a couple years now and the unified memory is a god send.  There are some support issues with Metal/MPS for certain things related to certain quantization and sparse attention, but overall these machines are beasts.   I can get 40-50 tps on llama 70b.  The good news now is that the market is flooded with these m chip macbook airs and pros, and a lot of them are cheap if you buy used.  I wouldn't be surprised if someone makes, or has already made, software to cluster macs for inference stacks in order to find a use for these.",
          "score": 2,
          "created_utc": "2026-01-22 14:50:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12ws2s",
              "author": "tony10000",
              "text": "There is a solution for clustering Macs called Exo, but you need Thunderbolt 5 for top performance using RDMA.  Several YouTube videos demonstrate how well the clusters work for inference.",
              "score": 1,
              "created_utc": "2026-01-22 17:22:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x9ogi",
          "author": "fallingdowndizzyvr",
          "text": "$600! Dude overpaid. It's $400 at MC.",
          "score": 3,
          "created_utc": "2026-01-21 20:45:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zh61a",
              "author": "DerFreudster",
              "text": "I think that included the dock and the nvme.",
              "score": 2,
              "created_utc": "2026-01-22 03:43:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o102sjy",
                  "author": "tony10000",
                  "text": "Correct.  That was for everything.  I got the M4 for $479.",
                  "score": 3,
                  "created_utc": "2026-01-22 06:13:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0vutf5",
          "author": "track0x2",
          "text": "I heard the primary limitation for LLMs on Mac‚Äôs is around text generation and if you want to do anything other than that (image gen, TTS, agentic work) they will struggle.",
          "score": 1,
          "created_utc": "2026-01-21 16:58:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vw0we",
              "author": "tony10000",
              "text": "It really depends on your use case and expectations.  Some very capable models are 14B and under.  If I need more capabilities, I can run some 30B models on my 5700G.  For more than that, there is ChatGPT and Open Router.",
              "score": 2,
              "created_utc": "2026-01-21 17:03:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wlbfx",
          "author": "Parking_Bug3284",
          "text": "This is really cool. I'm still sharing my gpu on my main but I'm building a similar thing on the software side. It sets up a base control system for local systems running on your machine. So if you have ollama and opencode it can build out what you need to gain access to unlimited memory management and access to programs that have a server running like image gen and what not. Does your system have an APi or mcp server to talk to it",
          "score": 1,
          "created_utc": "2026-01-21 18:54:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104kqn",
              "author": "tony10000",
              "text": "I use Anything LLM for API access, MCP, and RAG vector databases.",
              "score": 2,
              "created_utc": "2026-01-22 06:28:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10gg8v",
                  "author": "SelectArrival7508",
                  "text": "which is really nice as you can switch between you local llm and cloud-based llms",
                  "score": 2,
                  "created_utc": "2026-01-22 08:12:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0wyis3",
          "author": "jarec707",
          "text": "Agreed, depending on use. Thanks for sharing the models you use; seem like good choices.",
          "score": 1,
          "created_utc": "2026-01-21 19:54:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104wim",
              "author": "tony10000",
              "text": "Yeah, I have a nice assortment of models.  Probably 200GB worth at present.",
              "score": 1,
              "created_utc": "2026-01-22 06:31:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x1zxe",
          "author": "Zyj",
          "text": "You could try to get a used PC with a RTX 5060 Ti 16GB for almost the same amount of money, like this one [https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369](https://www.kleinanzeigen.de/s-anzeige/gaming-pc-i5-12400f-rtx-5060ti-16-gb/3296878854-228-1369)",
          "score": 1,
          "created_utc": "2026-01-21 20:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o104eke",
              "author": "tony10000",
              "text": "Not compact, portable, energy efficient, quiet, with low thermals.",
              "score": 1,
              "created_utc": "2026-01-22 06:27:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o15ou5c",
          "author": "nitinmms1",
          "text": "My Mac Mini M4 24GB can run Qwen 14b quantized at decent speed.\nImage generation models with comfyui, though feel slow.\nBut I still feel 64GB M4 will easily do as a good base local AI machine.",
          "score": 1,
          "created_utc": "2026-01-23 01:31:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bd8k6",
          "author": "parboman",
          "text": "Tried doing anything with mlx instead of ollama?",
          "score": 1,
          "created_utc": "2026-01-23 21:42:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bf21h",
              "author": "tony10000",
              "text": "I have a MLX version of llama.cpp on the system, and LM Studio can also run:\n\nMetal Llama.cpp\n\nLM Studio MLX\n\nI can run both native MLX builds and GGUFs using Metal.\n\nAlso: Ollama on macOS automatically utilizes Apple's¬†Metal API¬†for GPU acceleration on Apple Silicon (M1/M2/M3/M4 chips), requiring no additional configuration.",
              "score": 1,
              "created_utc": "2026-01-23 21:50:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1mzije",
          "author": "Cynical-Engineer",
          "text": "is it even performant? I have an M1 Max 64GB and 1TB SSD and when running Mistral it is really slow",
          "score": 1,
          "created_utc": "2026-01-25 16:06:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nakko",
              "author": "tony10000",
              "text": "It really depends on what you are doing.  I am a writer, and for most tasks it is fast enough.  I use models from 1.7B-14B and they run acceptably fast.  Not sure what Mistral variant you are referring to. \n\nMy main computer is a 5700G with 32GB of RAM and a 16GB Intel ARC B50.  I use it when I want to run models with bigger context windows, and also larger models (mostly MoE) like OSS 20B, Mistral Small 24B, Qwen 30B, Nemotron 30B, GLM 4.7 Flash 30B, etc.\n\nIf you are a professional coder, not even a Mac Studio 512GB can compare to enterprise GPUs:\n\n[https://www.youtube.com/watch?v=hxDe1j\\_IcSQ](https://www.youtube.com/watch?v=hxDe1j_IcSQ)",
              "score": 1,
              "created_utc": "2026-01-25 16:54:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1qv7ng",
                  "author": "Cynical-Engineer",
                  "text": "True",
                  "score": 1,
                  "created_utc": "2026-01-26 02:36:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qk9ked",
      "title": "Good local LLM for coding?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "author": "Expensive-Time-7209",
      "created_utc": "2026-01-22 22:56:33",
      "score": 32,
      "num_comments": 27,
      "upvote_ratio": 0.93,
      "text": "I'm looking for a a good local LLM for coding that can run on my rx 6750 xt which is old but I believe the 12gb will allow it to run 30b param models but I'm not 100% sure. I think GLM 4.7 flash is currently the best but posts like this [https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular\\_opinion\\_glm\\_47\\_flash\\_is\\_just\\_a/](https://www.reddit.com/r/LocalLLaMA/comments/1qi0vfs/unpopular_opinion_glm_47_flash_is_just_a/) made me hesitant\n\nBefore you say just download and try, my lovely ISP gives me a strict monthly quota so I can't be downloading random LLMS just to try them out",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qk9ked/good_local_llm_for_coding/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o163q5w",
          "author": "Javanese1999",
          "text": "[https://huggingface.co/TIGER-Lab/VisCoder2-7B](https://huggingface.co/TIGER-Lab/VisCoder2-7B) = Better version of Qwen2.5-Coder-7B-Instruct\n\n[https://huggingface.co/openai/gpt-oss-20b](https://huggingface.co/openai/gpt-oss-20b) =Very fast under 20b, even if your model size exceeds the VRAM capacity and goes into ram.\n\n[https://huggingface.co/NousResearch/NousCoder-14B](https://huggingface.co/NousResearch/NousCoder-14B) = Max picks IQ4\\_XS. This is just an alternative\n\nBut of all of them, my rational choice fell on gpt-oss-20b. It's heavily censored in refusal prompts, but it's quite reliable for light coding.",
          "score": 13,
          "created_utc": "2026-01-23 02:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o16qo3q",
          "author": "RnRau",
          "text": "Pick a coding MoE model and then use llama.cpp inference engine to offload some of the model to your system ram.",
          "score": 3,
          "created_utc": "2026-01-23 05:14:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o190rhs",
              "author": "BrewHog",
              "text": "Does llama.cpp have the ability to use both CPU and GPU? Or are you suggesting running one process in CPU and another in GPU?",
              "score": 1,
              "created_utc": "2026-01-23 15:13:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1bxmro",
                  "author": "RnRau",
                  "text": "It can use both in the same process. Do a google on 'moe offloading'.",
                  "score": 3,
                  "created_utc": "2026-01-23 23:23:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o17b5hk",
              "author": "mintybadgerme",
              "text": "Or LMstudio.",
              "score": 1,
              "created_utc": "2026-01-23 07:59:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o17he3f",
          "author": "vivus-ignis",
          "text": "I've had the best results so far with gpt-oss:20b.",
          "score": 3,
          "created_utc": "2026-01-23 08:57:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18zcxh",
          "author": "DarkXanthos",
          "text": "I run QWEN3 coder 30B on my M1 Max 64GB and it works pretty well. I think I wouldn't go larger though.",
          "score": 3,
          "created_utc": "2026-01-23 15:06:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o191fwg",
              "author": "BrewHog",
              "text": "How much RAM does it use? Is that quantized?",
              "score": 1,
              "created_utc": "2026-01-23 15:16:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fb7mt",
                  "author": "guigouz",
                  "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally Q3 uses around 20gb here (~14gb on gpu + 6gb on system ram) for a 50k context.\n\nI also tried Q2 but it's too dumb for actual coding, Q3 seems to be the sweet spot for smaller GPUs (Q4 is not **that** better).",
                  "score": 2,
                  "created_utc": "2026-01-24 13:50:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o14y7b5",
          "author": "Used_Chipmunk1512",
          "text": "Nope, 30B quantized to q4 will be too much for your gpu, don't download it. Stick with models under 10B",
          "score": 4,
          "created_utc": "2026-01-22 23:08:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14ykru",
              "author": "Expensive-Time-7209",
              "text": "Any recommendations under 10B?",
              "score": 1,
              "created_utc": "2026-01-22 23:10:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14zjyb",
                  "author": "iMrParker",
                  "text": "GLM 4.6v flash is pretty competent for its size. It should fit quantized with an okay context size¬†",
                  "score": 1,
                  "created_utc": "2026-01-22 23:15:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15prnv",
          "author": "Available-Craft-5795",
          "text": "GPT OSS 20B if it fits. Could work just fine in RAM though.  \nIts surprisingly good",
          "score": 2,
          "created_utc": "2026-01-23 01:36:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exduu",
              "author": "Virtual_Actuary8217",
              "text": "Not even support agent tool calling no thank you",
              "score": -1,
              "created_utc": "2026-01-24 12:17:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1f9apu",
                  "author": "Available-Craft-5795",
                  "text": "https://preview.redd.it/xhk81ws4wafg1.png?width=965&format=png&auto=webp&s=be4edb166f90d403c3016ec71a66aa288a66b4e2\n\nWhat?  \nYes it does.",
                  "score": 1,
                  "created_utc": "2026-01-24 13:39:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1qpitb",
                  "author": "10F1",
                  "text": "Yes it does?",
                  "score": 1,
                  "created_utc": "2026-01-26 02:07:36",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1rps45",
                  "author": "Virtual_Actuary8217",
                  "text": "It says one thing, but when you pair it with cline ,it basically can't do anything",
                  "score": 1,
                  "created_utc": "2026-01-26 05:41:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gqpua",
          "author": "SnooBunnies8392",
          "text": "I had Nvidia RTX 3060 12GB and I used\n\nQwen3 Coder @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nand\n\nGPT OSS 20B @ Q4\nhttps://huggingface.co/unsloth/gpt-oss-20b-GGUF\n\nBoth did offload a bit to system ram, but they were both useful anyway.",
          "score": 2,
          "created_utc": "2026-01-24 17:59:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o160ny9",
          "author": "No-Leopard7644",
          "text": "Try devstral, Qwen 2.5 Coder. You need to choose a quant so that the size of the model fits the vram. Also for coding you need some vram for context. What are using for model inference?",
          "score": 1,
          "created_utc": "2026-01-23 02:37:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o163jhs",
          "author": "nitinmms1",
          "text": "Anything beyond 8b q4 will be difficult.",
          "score": 1,
          "created_utc": "2026-01-23 02:52:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17n36t",
          "author": "WishfulAgenda",
          "text": "I‚Äôve found that higher q in smaller models is really helpful. Also don‚Äôt forget your system prompt or agent instructions.",
          "score": 1,
          "created_utc": "2026-01-23 09:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ao98f",
          "author": "Few_Size_4798",
          "text": "There are reviews on YouTube from last week:\n\nThe situation is as follows: even if you don't skimp on the Strix Halo ($2000+ today), all local ones can be shoved in the ass: Claude rules, and Gemini is already pretty good.",
          "score": 1,
          "created_utc": "2026-01-23 19:44:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21m0nc",
              "author": "GeroldM972",
              "text": "And none of the Youtube channels you pull information from receive any sponsorship from those same cloud-LLM providers and/or \"middle-men\" (those that allow you to connect to several of those cloud-LLM providers, via their single monthly subscription)?\n\nI use my own set of test questions and regularly test cloud and local LLMs. Cloud are often better and faster. Not always though.  But even NVidia claimed that the current cloud-LLM structures are not the solution, running local LLMs is. \n\nBesides, When I run local, I choose which model and its specialization, while I don't have any say in what the cloud-LLM  provider will give me. Or when they update their update their model and require me to rewrite/redefine configurations for agents, because of their internal changes.\n\nThere are very good reasons to use local LLMs, there are strong reasons to use cloud-provider LLMs. And it is not an 'either/or'-story, but an 'and' story. As in: use both at the moments in your processes that you need these to.",
              "score": 1,
              "created_utc": "2026-01-27 16:47:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1bxos0",
          "author": "Inevitable_Yard_6381",
          "text": "Hi totally new but tired of waiting Gemini on Android studio to answer...I have a MacBook Pro M1 pro 16 GB ram.. Any chance I could use a local LLM? \nAnd if possible how to integrate with my IDE to work like an agents and have access to my project? Could also be possible to send links to learn some new API or dependency? \nThanks in advance!!",
          "score": 1,
          "created_utc": "2026-01-23 23:24:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fbadr",
          "author": "guigouz",
          "text": "https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally",
          "score": 1,
          "created_utc": "2026-01-24 13:50:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qljfuw",
      "title": "AI & ML Weekly ‚Äî Hugging Face Highlights",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "author": "techlatest_net",
      "created_utc": "2026-01-24 10:16:03",
      "score": 29,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Here are the most notable¬†**AI models released or updated this week on Hugging Face**, categorized for easy scanning üëá\n\n# Text & Reasoning Models\n\n* **GLM-4.7 (358B)**¬†‚Äî Large-scale multilingual reasoning model¬†[https://huggingface.co/zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7)\n* **GLM-4.7-Flash (31B)**¬†‚Äî Faster, optimized variant for text generation¬†[https://huggingface.co/zai-org/GLM-4.7-Flash](https://huggingface.co/zai-org/GLM-4.7-Flash)\n* **Unsloth GLM-4.7-Flash GGUF (30B)**¬†‚Äî Quantized version for local inference¬†[https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF](https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF)\n* **LiquidAI LFM 2.5 Thinking (1.2B)**¬†‚Äî Lightweight reasoning-focused LLM¬†[https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking)\n* **Alibaba DASD-4B-Thinking**¬†‚Äî Compact thinking-style language model¬†[https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking](https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking)\n\n# Agent & Workflow Models\n\n* **AgentCPM-Report (8B)**¬†‚Äî Agent model optimized for report generation¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n* **AgentCPM-Explore (4B)**¬†‚Äî Exploration-focused agent reasoning model¬†[https://huggingface.co/openbmb/AgentCPM-Explore](https://huggingface.co/openbmb/AgentCPM-Explore)\n* **Sweep Next Edit (1.5B)**¬†‚Äî Code-editing and refactoring assistant¬†[https://huggingface.co/sweepai/sweep-next-edit-1.5B](https://huggingface.co/sweepai/sweep-next-edit-1.5B)\n\n# Audio: Speech, Voice & TTS\n\n* **VibeVoice-ASR (9B)**¬†‚Äî High-quality automatic speech recognition¬†[https://huggingface.co/microsoft/VibeVoice-ASR](https://huggingface.co/microsoft/VibeVoice-ASR)\n* **PersonaPlex 7B**¬†‚Äî Audio-to-audio personality-driven voice model¬†[https://huggingface.co/nvidia/personaplex-7b-v1](https://huggingface.co/nvidia/personaplex-7b-v1)\n* **Qwen3 TTS (1.7B)**¬†‚Äî Custom & base voice text-to-speech models¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice)¬†[https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign](https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign)\n* **Pocket-TTS**¬†‚Äî Lightweight open TTS model¬†[https://huggingface.co/kyutai/pocket-tts](https://huggingface.co/kyutai/pocket-tts)\n* **HeartMuLa OSS (3B)**¬†‚Äî Text-to-audio generation model¬†[https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B](https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B)\n\n# Vision: Image, OCR & Multimodal\n\n* **Step3-VL (10B)**¬†‚Äî Vision-language multimodal model¬†[https://huggingface.co/stepfun-ai/Step3-VL-10B](https://huggingface.co/stepfun-ai/Step3-VL-10B)\n* **LightOnOCR 2 (1B)**¬†‚Äî OCR-focused vision-language model¬†[https://huggingface.co/lightonai/LightOnOCR-2-1B](https://huggingface.co/lightonai/LightOnOCR-2-1B)\n* **TranslateGemma (4B / 12B / 27B)**¬†‚Äî Multimodal translation models¬†[https://huggingface.co/google/translategemma-4b-it](https://huggingface.co/google/translategemma-4b-it)¬†[https://huggingface.co/google/translategemma-12b-it](https://huggingface.co/google/translategemma-12b-it)¬†[https://huggingface.co/google/translategemma-27b-it](https://huggingface.co/google/translategemma-27b-it)\n* **MedGemma 1.5 (4B)**¬†‚Äî Medical-focused multimodal model¬†[https://huggingface.co/google/medgemma-1.5-4b-it](https://huggingface.co/google/medgemma-1.5-4b-it)\n\n# Image Generation & Editing\n\n* **GLM-Image**¬†‚Äî Text-to-image generation model¬†[https://huggingface.co/zai-org/GLM-Image](https://huggingface.co/zai-org/GLM-Image)\n* **FLUX.2 Klein (4B / 9B)**¬†‚Äî High-quality image-to-image models¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-4B](https://huggingface.co/black-forest-labs/FLUX.2-klein-4B)¬†[https://huggingface.co/black-forest-labs/FLUX.2-klein-9B](https://huggingface.co/black-forest-labs/FLUX.2-klein-9B)\n* **Qwen Image Edit (LoRA / AIO)**¬†‚Äî Advanced image editing & multi-angle edits¬†[https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA](https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA)¬†[https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO](https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO)\n* **Z-Image-Turbo**¬†‚Äî Fast text-to-image generation¬†[https://huggingface.co/Tongyi-MAI/Z-Image-Turbo](https://huggingface.co/Tongyi-MAI/Z-Image-Turbo)\n\n# Video Generation\n\n* **LTX-2**¬†‚Äî Image-to-video generation model¬†[https://huggingface.co/Lightricks/LTX-2](https://huggingface.co/Lightricks/LTX-2)\n\n# Any-to-Any / Multimodal\n\n* **Chroma (6B)**¬†‚Äî Any-to-any multimodal generation¬†[https://huggingface.co/FlashLabs/Chroma-4B](https://huggingface.co/FlashLabs/Chroma-4B)",
      "is_original_content": false,
      "link_flair_text": "Model",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qljfuw/ai_ml_weekly_hugging_face_highlights/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qj3l75",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/5lo30v55iqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 17:05:10",
      "score": 28,
      "num_comments": 11,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qj3l75/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0ynu7l",
          "author": "RnRau",
          "text": "You might enjoy this paper from 2024;\n\n> Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit models to making only one attempt at a problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage -- the fraction of problems that are solved by any generated sample -- scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget. \n\n\nhttps://arxiv.org/abs/2407.21787",
          "score": 3,
          "created_utc": "2026-01-22 00:55:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10txky",
          "author": "techlatest_net",
          "text": "5-AI debate cage match to kill hallucinations? Brutal elegance‚Äîforces consensus without trusting any single braindead output. Local Ollama hookup is the killer tho, total sovereignty.\n\nSpinning this up tonight. How's the inference overhead hit when all 5 duke it out? üî•",
          "score": 1,
          "created_utc": "2026-01-22 10:19:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12pjpn",
              "author": "PastTrauma21",
              "text": "Agreed! Having multiple models debate really helps filter out the noise, and not relying on just one output feels much safer.",
              "score": 1,
              "created_utc": "2026-01-22 16:50:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zyk7h",
          "author": "tvixii",
          "text": "just like how pewdiepie did it lol",
          "score": 0,
          "created_utc": "2026-01-22 05:40:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w1qch",
          "author": "jaxupaxu",
          "text": "So you copied pewdiepies council concept.¬†",
          "score": -16,
          "created_utc": "2026-01-21 17:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w353u",
              "author": "pokemonplayer2001",
              "text": "https://github.com/karpathy/llm-council",
              "score": 15,
              "created_utc": "2026-01-21 17:35:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0w54x5",
                  "author": "S_Anv",
                  "text": "Karpathy is a great man!\n\nKEA Research is designed as a user-friendly evolution. I've added image support, PDF/md export, text-to-speech conversion, and a full-fledged admin panel for managing local model sets without editing configuration files and many other features\n\nThis means you can create your own model set through a graphical interface  \nAlso as you see there is a bit different logic. You can check readme",
                  "score": 9,
                  "created_utc": "2026-01-21 17:44:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0w4qz2",
              "author": "FirstEvolutionist",
              "text": "I'm not suggesting OP didn't copy the idea from pewdiepie after hearing it from the then, but the idea of a \"council\" is not new, nor introduced by Pewidepie.",
              "score": 13,
              "created_utc": "2026-01-21 17:42:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ou0h",
              "author": "usercantollie",
              "text": "Yes, there are definitely similarities to PewDiePie‚Äôs council idea, but the approach here adds structured verification and provider-agnostic flexibility, which opens up new possibilities for local and open-source models. It‚Äôs cool to see how different projects build on each other to solve the trust problem in unique ways.",
              "score": 1,
              "created_utc": "2026-01-22 16:46:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yvab0",
          "author": "neoKushan",
          "text": "Can you get one of them to check your spelling before you post next time?",
          "score": -6,
          "created_utc": "2026-01-22 01:38:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o103d5h",
              "author": "Free-Internet1981",
              "text": "oof",
              "score": 1,
              "created_utc": "2026-01-22 06:18:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnjiym",
      "title": "SHELLper üêö: Multi-Turn Function Calling with a <1B model",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "author": "gabucz",
      "created_utc": "2026-01-26 15:43:32",
      "score": 25,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "We fine-tuned a 0.6B model to translate natural language into bash commands. Since it's tiny, you can run it on your laptop with complete data privacy.\n\nSmall models struggle with multi-turn tool calling - out of the box, Qwen3-0.6B achieves 84% accuracy on single tool calls, which drops to **just 42% over 5 turns.** Our tuning brings this to 100% on the test set, delivering robust multi-turn performance.\n\n|Model|Parameters|Tool call accuracy (test set)|=> 5-turn tool call accuracy|\n|:-|:-|:-|:-|\n|Qwen3 235B Instruct (teacher)|235B|99%|95%|\n|Qwen3 0.6B (base)|0.6B|84%|42%|\n|**Qwen3 0.6B (tuned)**|**0.6B**|**100%**|**100%**|\n\nRepo: [https://github.com/distil-labs/distil-SHELLper](https://github.com/distil-labs/distil-SHELLper)\n\nHuggingface model: [https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper](https://huggingface.co/distil-labs/distil-qwen3-0.6b-SHELLper)\n\n# Quick Start\n\nSet up the environment:\n\n    # Set up environment\n    python -m venv .venv\n    . .venv/bin/activate\n    pip install openai huggingface_hub\n    \n\nDowload the model:\n\n    hf download distil-labs/distil-qwen3-0.6b-SHELLper --local-dir distil_model\n    cd distil_model\n    ollama create distil_model -f Modelfile\n    cd ..\n    \n\nRun the assistant:\n\n    python filesystem_demo.py\n    \n\nThe demo prompts for confirmation before running commands (safety first) and blocks certain dangerous operations (like `rm -r /`), so feel free to try it out!\n\n# How We Trained SHELLper\n\n# The Problem\n\nSmall models really struggle with multi-turn tool calling - performance degrades as tool calls chain together, dropping with each additional turn. If we assume independent errors for each tool call (like incorrect parameter values), a model at 80% accuracy only has a 33% chance of getting through 5 turns error-free.\n\n|Single tool call accuracy|5-turn tool call accuracy|\n|:-|:-|\n|80%|33%|\n|90%|59%|\n|95%|77%|\n|99%|95%|\n\nFor this demo, we wanted to test if we could dramatically improve a small model's multi-turn performance. We started with a task from the [Berkeley function calling leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) (BFCL) - the [gorilla file system tool calling task](https://github.com/ShishirPatil/gorilla/blob/main/berkeley-function-call-leaderboard/bfcl_eval/data/BFCL_v4_multi_turn_base.json). We adapted it:\n\n* Original task supports multiple tool calls per turn ‚Üí we restrict to one\n* Cap at 5 turns max\n* Map commands to actual bash (instead of gorilla filesystem functions)\n* Skip adding tool outputs to conversation history\n\nBasically, the same tool set, but new, simpler [train/test data.](https://github.com/distil-labs/distil-SHELLper/tree/main/data)\n\n# Training Pipeline\n\n1. **Seed Data:** We built 20 simplified training conversations covering the available tools in realistic scenarios.\n2. **Synthetic Expansion:** Using our [data synthesis pipeline](https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&utm_medium=referral&utm_campaign=shellper), we generated thousands of training examples.\n\nSince we're dealing with variable-length conversations, we broke each conversation into intermediate steps. Example:\n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models\n    \n\n... becomes 2 training points:\n\n    [Input] User: List all files\n    [Output] Model: ls -al\n    \n\n    [Input] User: List all files => Model: ls -al => User: go to directory models\n    [Output] Model: cd models`\n    \n\n1. **Fine-tuning:** We selected **Qwen3-0.6B** as the [most fine-tunable sub-1B](https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning) model with tool calling support on our platform.\n\n# Usage Examples\n\nThe assistant interprets natural language, generates bash commands, and can execute them (with Y/N confirmation).\n\n**Basic filesystem operations**\n\n    > python filesystem_demo.py\n    \n    USER: List all files in the current directory\n    COMMAND: ls\n    USER: Create a new directory called test_folder\n    COMMAND: mkdir test_folder\n    USER: Navigate to test_folder COMMAND: cd test_folder\n    \n\n# Limitations and Next Steps\n\nCurrently, we only support a basic bash tool set:\n\n* no pipes, chained commands, or multiple tool calls per turn\n* no detection of invalid commands/parameters\n* 5-turn conversation limit\n\nWe wanted to focus on the basic case before tackling complexity. Next up: multiple tool calls to enable richer agent workflows, plus benchmarking against [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html).\n\nFor your own bash workflows, you can log failing commands, add them to `data/train.jsonl`, and retrain with the updated data (or try a bigger student model!).\n\n# Discussion\n\nWould love to hear from the community:\n\n* Is anyone else fine-tuning small models for multi-turn tool calling?\n* What other \"focused but practical\" tasks need local, privacy-first models?",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnjiym/shellper_multiturn_function_calling_with_a_1b/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1u3wq5",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 15:46:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u7b5a",
              "author": "gabucz",
              "text": "Thanks! Qwens tool call format - inputs are translated via transformers chat templating, and for outputs we have a converter to match what qwen does",
              "score": 1,
              "created_utc": "2026-01-26 16:00:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1u7m4o",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 0,
                  "created_utc": "2026-01-26 16:02:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uk3mr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 0,
          "created_utc": "2026-01-26 16:55:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1upa98",
              "author": "gabucz",
              "text": "Exactly - we mention it in the post",
              "score": 1,
              "created_utc": "2026-01-26 17:17:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjccpp",
      "title": "AMD ROCm 7.2 now released with more Radeon graphics cards supported, ROCm Optiq introduced",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-ROCm-7.2-Released",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-21 22:24:16",
      "score": 21,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qjccpp/amd_rocm_72_now_released_with_more_radeon/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": [
        {
          "id": "o10t8zr",
          "author": "techlatest_net",
          "text": "ROCm 7.2 dropping RDNA4 support + Optiq? Finally Radeon GPUs get real ML love‚ÄîComfyUI workflows about to fly. Windows builds too? AMD actually shipping for consumers.\n\nTime to dust off the 7900 XTX and pit it vs my CUDA stack",
          "score": 2,
          "created_utc": "2026-01-22 10:12:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qluto8",
      "title": "HashIndex: No more Vector RAG",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "author": "jasonhon2013",
      "created_utc": "2026-01-24 18:33:54",
      "score": 19,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "The Pardus AI team has decided to open source our memory system, which is similar to PageIndex. However, instead of using a B+ tree, we use a hash map to handle data. This feature allows you to parse the document only once, while achieving retrieval performance on par with PageIndex and significantly better than embedding vector search. It also supports Ollama and llama cpp . Give it a try and consider implementing it in your system ‚Äî you might like it! Give us a star maybe hahahaha\n\n[https://github.com/JasonHonKL/HashIndex/tree/main](https://github.com/JasonHonKL/HashIndex/tree/main)",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qluto8/hashindex_no_more_vector_rag/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1ifug5",
          "author": "FaceDeer",
          "text": "I didn't see any documentation there about how the \"guts\" of the system worked, so [I asked Gemini to do a Deep Research run to produce one](https://gemini.google.com/share/2e13e0d1a6fe). Some key bits:\n\n> The documentation for HashIndex identifies it as a \"vectorless\" index system. This characterization is central to its \"under the hood\" operations. Instead of calculating a mathematical hash or a vector embedding, the system invokes an LLM to generate what it terms a \"semantic hash key\".  \n\n> When a document is ingested by HashIndex, it is first split into segments or pages. For each segment, the system initiates a dual-process LLM call. The first process involves generating a highly descriptive, human-readable label that encapsulates the core theme of the content. This label‚Äîfor example, ``revenue_projections_FY2024_Q3``‚Äîserves as the index key in the hash map. The second process generates a concise summary of the page.\n\n> This \"single-pass\" parsing allows the document to be structured for retrieval without the need for pre-computed embedding datasets. However, the cost of this precision is time. While a traditional cryptographic hash function $H(x)$ or an embedding model can process data in milliseconds, the semantic key generation in HashIndex requires significant inference time, typically 2 to 3 seconds per page.\n\n[...]\n\n> In HashIndex, the hash table is implemented in-memory, allowing for rapid access once the indexing phase is complete. The \"hash function\" in this context is the cognitive process performed by the LLM during key generation. This approach eliminates the need for complex tree rebalancing and multi-level traversal required by systems like ChatIndex or PageIndex. However, it places a higher burden on the \"agentic\" side of the retrieval process, as the agent must now navigate a flat list of keys rather than a hierarchical tree. \n\nDoes this look like an accurate summary of how it works? Might be worth calling out that the \"hash\" in this case is not a traditional hash in the way that word is usually meant, but an LLM-generated semantic \"tag\" of sorts.",
          "score": 1,
          "created_utc": "2026-01-24 22:38:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1l7wdg",
              "author": "jasonhon2013",
              "text": "Haha generally speaking yea this summary is right. We call it hash is because the data structure we use behind is a hash table hahaha",
              "score": 1,
              "created_utc": "2026-01-25 09:05:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l7lf4",
          "author": "mister2d",
          "text": "Checking it out now with some pending energy legislation bills.",
          "score": 1,
          "created_utc": "2026-01-25 09:02:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yp85z",
          "author": "No-Lobster486",
          "text": "it would be much helpful if there is an relatively detailed explanation about the \"hash map to handle data\" part and a chart of the business flow.    \nthere are too many similar things these days, it will definitely help people make decision whether it will suit for their issue or worth trying",
          "score": 1,
          "created_utc": "2026-01-27 05:06:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h4awo",
          "author": "jschw217",
          "text": "Why does it require httpx? Any connections to remote servers?",
          "score": 0,
          "created_utc": "2026-01-24 18:56:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h4gs2",
              "author": "jasonhon2013",
              "text": "Oh it‚Äôs is for the purpose of fetching APIs like open router not connected to any remote server no worries !",
              "score": 2,
              "created_utc": "2026-01-24 18:57:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qitw9k",
      "title": "Trained a local Text2SQL model by chatting with Claude ‚Äì here's how it went",
      "subreddit": "LocalLLM",
      "url": "https://i.redd.it/coqeh3twdoeg1.png",
      "author": "party-horse",
      "created_utc": "2026-01-21 09:57:59",
      "score": 19,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Project",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qitw9k/trained_a_local_text2sql_model_by_chatting_with/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qlqvdh",
      "title": "Minimum hardware for a voice assistant that isn't dumb",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "author": "JacksterTheV",
      "created_utc": "2026-01-24 16:07:22",
      "score": 17,
      "num_comments": 15,
      "upvote_ratio": 0.96,
      "text": "I'm at the I don't know what I don't know stage. I'd like to run a local LLM to control my smart home and I'd like it have a little bit of a personality. From what I've found online that means a 7-13b model which means a graphics card with 12-16gb of vram. Before I started throwing down cash I wanted to ask this group of I'm on the right track and for any recommendations on hardware. I'm looking for the cheapest way to do what I want and run everything locally ",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qlqvdh/minimum_hardware_for_a_voice_assistant_that_isnt/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1gbeqd",
          "author": "LowTip9915",
          "text": "I have a 7b on an m2 Mac mini and it‚Äôs acceptable, but I‚Äôm not using voice (just llama and open web ui). The 14b runs, but is painfully slow, however the difference in answers is evident, so I use it occasionally when the 7b answer is overly vague/top level.  (Disclaimer also a noob to this)",
          "score": 7,
          "created_utc": "2026-01-24 16:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1h01e9",
          "author": "Boricua-vet",
          "text": "First off, you do not need a model to do what you want to do. Voice assist can do pretty much what you want and in the case that you have unique requests, you can just add those by hand. Even the personality part can be creating using custom answers. The only downside is that it can only do what you program it to do and you will not be able to have conversations with it. If you need conversations, then you will certainly need a model but you don't need a 13b to do what you want. You do not even need a GPU as long as you have a good CPU and fast ram and enough ram to load the model. \n\nSince I do not know your hardware specs, I recommend you starting with [https://huggingface.co/unsloth/Qwen3-4B-GGUF?show\\_file\\_info=Qwen3-4B-Q4\\_K\\_M.gguf](https://huggingface.co/unsloth/Qwen3-4B-GGUF?show_file_info=Qwen3-4B-Q4_K_M.gguf)\n\nYou can run that on CPU and get really good speed depending on your hardware. Example on my hardware using only CPU I can get 100+ tokens per second on this model but depends on your hardware.\n\nThat's the cheapest way to do it.\n\nNow if you still want to be cheap and use GPU\n\nGet a P102-100 for 60 bucks.\n\n[https://www.ebay.com/itm/156284588757](https://www.ebay.com/itm/156284588757)\n\nyou can load piper and whisper using hardware acceleration and get responses from HA in milliseconds.  For this purpose , you do not need to buy a stupid expensive card. 60 bucks will do better job than any 3060, 4060 and 5060 for this particular application and use case.\n\nwith one P102-100, you can run Piper, Whisper and Qwen 4b and get everything you wanted to do.\n\nnow, if you really want to go nuts, you can get two and then you can run Qwen30b and get these speeds.\n\n`llamacpp-server-1  | ggml_cuda_init: found 2 CUDA devices:`\n\n`llamacpp-server-1  |   Device 0: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  |   Device 1: NVIDIA P102-100, compute capability 6.1, VMM: yes`\n\n`llamacpp-server-1  | load_backend: loaded CUDA backend from /app/libggml-cuda.so`\n\n`llamacpp-server-1  | load_backend: loaded CPU backend from /app/libggml-cpu-piledriver.so`\n\n`llamacpp-server-1  | | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |`\n\n`llamacpp-server-1  | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           pp512 |        968.28 ¬± 4.69 |`\n\n`llamacpp-server-1  | | qwen3moe 30B.A3B IQ4_NL - 4.5 bpw |  16.12 GiB |    30.53 B | CUDA       |  99 |  1 |           tg128 |         72.10 ¬± 0.23 |`\n\n`llamacpp-server-1  |` \n\n`llamacpp-server-1  | build: 557515be1 (7819)`\n\n`llamacpp-server-1 exited with code 0`\n\n  \nabout 1,000 in prompt processing and 70+ tokens generation.\n\ntrust me, for home assistant, you will not need more than that and at that price, you will not find a better solution.\n\nAsk people that actually own P102-100, don''t take my word for it.\n\nFor this use case.... this is it.",
          "score": 6,
          "created_utc": "2026-01-24 18:38:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h9az6",
              "author": "JacksterTheV",
              "text": "Awesome, I'm going to go the CPU route and see what I can get working. Thanks for the recommendation.¬†",
              "score": 1,
              "created_utc": "2026-01-24 19:18:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1kwfhl",
              "author": "MakerBlock",
              "text": "Fantastic write up!",
              "score": 1,
              "created_utc": "2026-01-25 07:25:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1nfx91",
                  "author": "Boricua-vet",
                  "text": "Thank you. Really appreciate that. Just trying to help.",
                  "score": 3,
                  "created_utc": "2026-01-25 17:17:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ga8q2",
          "author": "tillemetry",
          "text": "Anyone try this with an M4 Mac mini?  You can run it headless, it doesn‚Äôt take up much space and the noise and power draw is minimal.  LM studio will point you at ai‚Äôs optimized for MLX.",
          "score": 4,
          "created_utc": "2026-01-24 16:46:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gbm53",
          "author": "PermanentLiminality",
          "text": "There is no single correct answer.\n\nYou left out a few parts.  You need the audio in and out.  A bluetooth speaker can work.  Now you need the Speech to text (STT)  and Text to Speech(TTS) in addition to a LLM.  Both the TTS and STT need VRAM, but there are TTS like Piper than can actually run on the CPU.\n\nOne nice thing here is most voice assistants don't do a lot of context so you only need say 15% more VRAM than the LLM.\n\nYou will need to experiment to find what works for you.  Put a few bucks into OpenRouter and try the models.  A possible example would be mistral 3 8B which fits in about 7GB of VRAM.  They free models too.   Find what will work for what you want.\n\nIf I ever get some time, I plan on doing what you are trying to do.  My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU.  Not sure if I can get a smart enough model STT and TTS , but I want to try.  That setup will idle around 10 watts.  I may consider picking up a 24GB P40 as it also is pretty low idle power.   The P40 is around $200 after the needed fan to cool it.\n\nMy power is crazy expensive so cheap 16 GB GPUs like the P100 or CMP100-210 idle at 40 or 50 watts.  That is like $200/yr for me.\n\nAnyway, before dropping coin on a GPU, test out a solution with available tools like Openrouter and Huggingface.  For a project like this $10 can go a long way for testing.",
          "score": 2,
          "created_utc": "2026-01-24 16:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ht9nr",
              "author": "Boricua-vet",
              "text": "\"My plan is to use a Wyse 5070 and a $50 10GB VRAM P102-100 GPU. Not sure if I can get a smart enough model STT and TTS\"\n\nyou sure can.. I will go as far as give you the models I am using with hardware accel that works for me.  \nfor piper PIPER\\_VOICE=en\\_US-libritts-high  \nfor whisper  WHISPER\\_MODEL=distil-medium.en  \nthose two will consume 3.5GB of vram for both. 3.5 total for both.\n\nMy only suggestion is to use really good microphone like [https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html](https://www.seeedstudio.com/ReSpeaker-XVF3800-USB-4-Mic-Array-With-Case-p-6490.html)  this has the xmos chip that will clean all the input even with loud music or background noise. It works fantastic...\n\non the P102-100 with those models and that mic, your response time from HA after your command should be in the milliseconds. Not even a second like half a second or less, extremely responsive, that is unless you are running it in an under powered device.\n\nI run this setup flawlessly.",
              "score": 3,
              "created_utc": "2026-01-24 20:50:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gn24j",
          "author": "JacksterTheV",
          "text": "To add some more context I'm using home assistant and have the voice pipeline worked out. I have a hacked echo show that I can talk to like Alex. The problem is right now it takes about 30 seconds for something as simple as locking the back door. I can connect home assistant to any local LLM. This is all running on an old laptop. Ultimately I want everything to run locally but I really like the idea of spending a few bucks on a cloud service to figure out what I need before dropping cash on hardware.¬†",
          "score": 2,
          "created_utc": "2026-01-24 17:43:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1haztq",
              "author": "Blizado",
              "text": "I actually tend to use a Qwen3 30B A3B Instruct model in ~2-4bit. I tried it with Q2_K in GGUF format with KoboldAI, needs with a small KV Cache around 12GB VRAM (Q4 would be around 16-18GB VRAM) and I was surprised how good it still is, but even more surprised how crazy fast it is (at least on my RTX 4090). Thanks to its MoE pipeline, where only 3B of the model is active, so it is mostly as fast as a 3B model at Q2_K would be, but of cause way better, you don't need to try a 3B at Q2_K, it will be very very bad but crazy fast. It's hard for me to use dense models now, they are so slow in comparison.\n\nSo as long you don't want to have a realtime conversation with the LLM, you can also split a some layers of the model into normal RAM. It slows down the model a bit but if you only put ~10% of the model in RAM it should stay fast enough so you can bring down the SmartHome reaction to maybe 5 seconds or lower. The only question is if you need tool calling, that could break on Q2_K, but there are ways to work around that.\n\nBut I would go for a solid 16GB VRAM card (RTX 40xx/50xx series), not too much on the low end, then you can put the LLM completely into the VRAM. So maybe a 5070TI with 16GB VRAM. No clue if a 5060TI is still fast enough. I \"only\" have a 4090 and a 5080. Could be if you get the full model into VRAM, because if not, the 5060TI has only PCIe 5 8x, the 5070TI has PCIe 5 16x, but that is only really relevant if you use offloading (splitting the model into RAM) or if the GPU is in a PCIe 5 x16 slot. If you need offloading, better don't put a PCIe 5 8x GPU into a PCIe 5 motherboard setup.\n\nBut beside that, it really depends what you want to exactly do. If you only want the bar minimum, for example only some different phrases that should react for one SmartHome function, you can do that with much much less hardware and a tiny AI model that only recognize what you want to do. Such an AI didn't need to be that smart at all, only always recognize what you want from it, you can even give it a bit the illusion of personality. But if your plan is to build your SmartHome more and more out, then a LLM is surely not a bad decision, but a more costly one.",
              "score": 1,
              "created_utc": "2026-01-24 19:26:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i0p7u",
          "author": "atlantageek2",
          "text": "currently developing something similar for work using whisper. dev machine at home is my m4 mac mini 16gb. theres like a 3 second delay but other than that it works well. its using whisper",
          "score": 2,
          "created_utc": "2026-01-24 21:25:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1i0zr0",
              "author": "atlantageek2",
              "text": "forgot to mention whisper cannot use max gpu",
              "score": 1,
              "created_utc": "2026-01-24 21:27:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ihhpy",
          "author": "Powerful-Street",
          "text": "LFM2.5-audio if you can figure out how to get it to work, other than in the demo. It‚Äôs only ~3GB and does a pretty good job. The other thing you could do is, inject personaplex weights into moshi. Takes much more work and ram though.",
          "score": 1,
          "created_utc": "2026-01-24 22:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1izk91",
          "author": "PermanentLiminality",
          "text": "Good to know.   Now I just need some time...",
          "score": 1,
          "created_utc": "2026-01-25 00:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r35e0",
          "author": "Bino5150",
          "text": "I'm running an 8b model locally with LM Studio/AnythingLLM, and using Piper for TTS. I'm running an HP Zbook Studio G7 laptop on Linux Mint, with an i7, 16GB ram, and an Nvidia Quadro T1000 Mobile gpu with 4GB vram. It runs just fine. I can do SST via the laptops mic, but I haven't configured home assistant yet as I've just got this up and running. Thinking of setting up the SST on a Raspberry Pi.",
          "score": 1,
          "created_utc": "2026-01-26 03:17:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkmv44",
      "title": "DeepSeek-V3.2 Matches GPT-5 at 10x Lower Cost | Introl Blog",
      "subreddit": "LocalLLM",
      "url": "https://introl.com/blog/deepseek-v3-2-open-source-ai-cost-advantage",
      "author": "EchoOfOppenheimer",
      "created_utc": "2026-01-23 09:59:43",
      "score": 16,
      "num_comments": 2,
      "upvote_ratio": 0.64,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkmv44/deepseekv32_matches_gpt5_at_10x_lower_cost_introl/",
      "domain": "introl.com",
      "is_self": false,
      "comments": [
        {
          "id": "o18oklh",
          "author": "ForsookComparison",
          "text": "> 12 upvotes\n\nFor a sub about LLMs this sub is pretty shitty at spotting bots",
          "score": 4,
          "created_utc": "2026-01-23 14:12:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iqcip",
          "author": "HealthyCommunicat",
          "text": "I‚Äôve been trying to talk about DS 3.2 and LongCat 2601 more as they genuinely seem to be on a higher level than GLM or MiniMax when it comes to knowing very vast specific niche knowledge, they‚Äôre the only two models I trust for work, as they‚Äôre the only two models so far that get questions about Oracle EBS correct, as most of Oracle‚Äôs software is proprietary and existing documentation is absolutely horrendous. - these two models can answer questions that even GPT 5.2 has trouble answering if I specifically tell it not to use any kind of search.",
          "score": 1,
          "created_utc": "2026-01-24 23:32:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qii3h2",
      "title": "Can I add a second GPU to use it's vram in addition of the vram of my main GPU to load bigger models?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qii3h2/can_i_add_a_second_gpu_to_use_its_vram_in/",
      "author": "Babidibidibida",
      "created_utc": "2026-01-21 00:01:34",
      "score": 15,
      "num_comments": 35,
      "upvote_ratio": 0.94,
      "text": "I have a 5070 Ti 16gb + a 7950X + 96gb of ram. I was waiting for the 5070Ti Super 24Gb to be release to buy one, but the ram shortage situation made me buy a 16gb in a hurry.  \nObviously I can't load as big of models in vram as I was expecting (and RTX 4090 and 5090 are too expensive) but I thought that maybe it was possible  to add a second GPU like a second hand 24gb RTX3090 or a RTX 5060 16gb in order to use the vram of that second GPU in addition to the vram of the first one? (so Vram from GPU 1+2 would be seen as just one big capacity) Is it possible to do that? If yes, how? I'm using LM Studio\n\nWhat would be best between a 3090 (24Gb) and 5060 Ti (16gb)? I know there's more vram in the 3090 but maybe i\\_t's less fitted for AI than a more recent 5060 Ti?\n\nThanks",
      "is_original_content": false,
      "link_flair_text": "Question",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qii3h2/can_i_add_a_second_gpu_to_use_its_vram_in/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o0rlmq1",
          "author": "FullstackSensei",
          "text": "I have a few 3090s and recently got a 5060ti (just before prices went up) to play with a bit. Long story short: it can't hold a candle next to the 3090. I'm sure someone will come here to talk about how Ampere lacks fp8 or nvfp4, but if you don't absolutely need these two formats natively supported, the 3090 wins in everything else. It has 50% more VRAM and 100% more memory bandwidth. The former lets you load bigger models, and if you have more than one 3090 there's less waste in VRAM (that's mainly a software deficiency in current inference engines). The memory bandwidth means more of the compute is accessible more of the time.\n\nThe 5070ti is abice the 3090 in terms of compute, and practically has the same memory bandwidth. So, think of the 3090 as a second 5070ti with 50% more VRAM.\n\nGenerally speaking, moar VRAM is moar better. I'd even sell the 5070Ti since that is more expensive than the 3090 where I live and put the money towards getting two 3090s. 48GB  >> 32GB VRAM.",
          "score": 15,
          "created_utc": "2026-01-21 00:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rm4jg",
              "author": "Babidibidibida",
              "text": "Thanks for the detailed answer!",
              "score": 2,
              "created_utc": "2026-01-21 00:18:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0rnei1",
              "author": "Prudent-Ad4509",
              "text": "It is better to keep at least one blackwell card for the few initial model layers or for the draft model. The other 2,4, or multiples of 4 3090s can pick up the rest.",
              "score": 1,
              "created_utc": "2026-01-21 00:25:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rsxcn",
                  "author": "FullstackSensei",
                  "text": "I have yet to see a draft model that is actually useful in real world scenarios. Put another way, if the draft model had a high acceptance rate, the big model would be out of a job.\n\nMultiple cards of the same model enable row splitting (-sm row) in llama.cpp which improves performance much more than a draft model, and if you have a power of two number of cards and use mainly one model, you can run vllm with tensor parallelism for even higher generation speed.",
                  "score": 3,
                  "created_utc": "2026-01-21 00:55:59",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0rx22x",
                  "author": "Babidibidibida",
                  "text": "My PC is also a gaming PC so i'll keep my 5070Ti for gaming anyway :D",
                  "score": 1,
                  "created_utc": "2026-01-21 01:19:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0sd2as",
              "author": "Mabuse046",
              "text": "Not to mention the 3090 still has Nvlink which is so much faster than PCIE.",
              "score": 1,
              "created_utc": "2026-01-21 02:50:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tiy56",
                  "author": "FullstackSensei",
                  "text": "For inference it doesn't make a big difference if you have 8 Gen 3 lanes for each card or more. It's been tested multiple times and the difference is always something like 3%1",
                  "score": 2,
                  "created_utc": "2026-01-21 07:56:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0t4soo",
              "author": "remghoost7",
              "text": ">I'm sure someone will come here to talk about how Ampere lacks fp8...\n\nDual 3090's here.\n\nThe only downside for a lack of FP8 are video models.  \nEverything else locally hosted AI related is *freaking awesome* with this much VRAM.",
              "score": 1,
              "created_utc": "2026-01-21 05:53:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0rl6r1",
          "author": "Prudent-Ad4509",
          "text": "For LLM inference - yeah, 3090 24gb is the king. llama.cpp works best with several gpus, lm studio seems to work too, their online help is one web search away.",
          "score": 3,
          "created_utc": "2026-01-21 00:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rm5yn",
              "author": "Babidibidibida",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-21 00:18:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0rpyep",
                  "author": "TechnologyGrouchy679",
                  "text": "guess you got your answer",
                  "score": 1,
                  "created_utc": "2026-01-21 00:39:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0rvxav",
              "author": "No_Ad4069",
              "text": "I use LM Studio with two 5080s and confirm that it works",
              "score": 1,
              "created_utc": "2026-01-21 01:13:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0rx32r",
              "author": "FuzzeWuzze",
              "text": "Are there good(free) ways as a consumer to mesh cards from other systems together with mine?  I have several cards spread across different systems, but no system that has  a motherboard that can hold them all, nor do i want to end the other usees of those machines.  It'd be nice to just throw everything my house has at it, even if its just an additional shitty 1080ti with 11Gb ...",
              "score": 1,
              "created_utc": "2026-01-21 01:19:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0rzlhd",
                  "author": "Prudent-Ad4509",
                  "text": "There are some options, but nothing too simple. Mostly splitters x16->4x4x4x4x. As for 1080ti, they are best kept as a backup gpu just in case. Or used for gaming, the are still mostly relevant, often even for 4k60Hz with carefully configured settings.",
                  "score": 1,
                  "created_utc": "2026-01-21 01:34:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0uagto",
              "author": "FrumunduhCheese",
              "text": "Do you need the latest pci 5 or 6 or whatever it is ? I have an old hp z620 I‚Äôm looking at throwing a 3090 in it has 64 gb ddr3 and 32 cores.",
              "score": 1,
              "created_utc": "2026-01-21 12:03:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ub2yi",
                  "author": "Prudent-Ad4509",
                  "text": "The proper way to handle a few PCIe 5.0 cards for training is to buy $2000 PCIe switch for 100 pci lanes. Same for PCIe 4.0, for half the price. Then plug it into whatever, any gaming PC will do. Every other perfect option costs more. But for inference you will be good with 4.0 x8, and probably ok with x4 if you do not do tensor parallel.",
                  "score": 1,
                  "created_utc": "2026-01-21 12:08:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0s05dc",
          "author": "WishfulAgenda",
          "text": "Dual  5070ti here. Works great with 30b q6 35k context. Bifurcated 8x8 pcie gen4",
          "score": 3,
          "created_utc": "2026-01-21 01:37:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rmg0a",
          "author": "TheAussieWatchGuy",
          "text": "More VRAM is king. You can absolutely have multiple GPUs, 3060s have even had a resurgence, put four or eight in a single motherboard ... The 3090 is still the best value but it's aging.¬†\n\n\nPurely for AI and running big models Mac or Ryzen AI 395 with 128gb+ of ddr5 system memory are the best value because you can share 112gb with the built in GPU and run giant models.\n\n\nGPUs still offer better tokens per second so it's still a trade off... But you'll never run a 200b+ parameter model on less than 96gb unified memory anyway so really upto you. Smaller models on a multiple GPUs, faster tokens per second but limited in how big the models you can run are. Unifies platform sharing CPU / GPU memory, run much bigger wmarter models but slower token output.¬†",
          "score": 2,
          "created_utc": "2026-01-21 00:20:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0roo5w",
          "author": "starkruzr",
          "text": "*if* you have the PCIe lanes and *if* you can find cheap enough 5060Ti 16GBs, you can make an argument that they represent a good option. a single 3090 with 24GB VRAM is upwards of $850 rn. two 5060Tis with 16GB each (32GB total) is *theoretically* about the same price as that 3090, but idk how much stock of them remains out there.",
          "score": 2,
          "created_utc": "2026-01-21 00:32:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rxsji",
          "author": "jacek2023",
          "text": "Yes, that's what real men do.",
          "score": 1,
          "created_utc": "2026-01-21 01:23:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sjpz4",
          "author": "No-Consequence-1779",
          "text": "Depending upon the speed you need, you can pick up server gpus for cheap all day.¬†",
          "score": 1,
          "created_utc": "2026-01-21 03:30:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0sg6wb",
          "author": "digabledingo",
          "text": "keep in mind that apple can share video memory with its system ram , combining the two which no other platform can do... I almost very close came to switching to apple",
          "score": 0,
          "created_utc": "2026-01-21 03:09:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0t5cau",
              "author": "MrNantir",
              "text": "Isn't that what the Strix Halo / AMD Ai Max+ 395 does also?",
              "score": 2,
              "created_utc": "2026-01-21 05:58:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tltjl",
                  "author": "digabledingo",
                  "text": "I think there's a way to set them up in a way where you can link them as two sperate cards each one firing a unique seed , at least that's how I understand 2 gpus in comfyai",
                  "score": 1,
                  "created_utc": "2026-01-21 08:23:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0t36lt",
              "author": "Babidibidibida",
              "text": "I already got a powerful PC I don't see myself buying an Appl, they are so expensive, and I also game so I need a PC",
              "score": 1,
              "created_utc": "2026-01-21 05:41:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkxpuo",
      "title": "AMD Ryzen AI Software 1.7 released for improved performance on NPUs, new model support",
      "subreddit": "LocalLLM",
      "url": "https://www.phoronix.com/news/AMD-Ryzen-AI-Software-1.7",
      "author": "Fcking_Chuck",
      "created_utc": "2026-01-23 17:52:03",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qkxpuo/amd_ryzen_ai_software_17_released_for_improved/",
      "domain": "phoronix.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qnmc4s",
      "title": "Building my first LLM HomeLab, where to start?",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnmc4s/building_my_first_llm_homelab_where_to_start/",
      "author": "rmjcloud",
      "created_utc": "2026-01-26 17:20:29",
      "score": 14,
      "num_comments": 15,
      "upvote_ratio": 0.95,
      "text": "Hey all! üëã\n\nI‚Äôm looking to delve into AI and local LLMs head first with the aim eventually of building some cool AI/LLM apps for self learning.\n\nI wanted to see if anyone had some good recommendations of hardware for a homelab, preferably on the mid-starter end of budget.\n\nSpecifically CPU, GPU and RAM suggestions so i can test the water to see how much i need to spend to build a decent lab for running local LLMs with Ollama to kickstart my AI journey and learning!\n\nGaming orientated GPUs not necessary but a nice compromise for gaming too i guess!\n\nBudget 1-2k GBP¬£.\n\nI have an M3 MBP and an M2 Macbook Pro, both with around 16GB RAM, are these any good for achieving this? Small scale is fine, i just want to get to grips with LLM concepts locally and digging into how they work, tuning and deploying apps in an AIOps approach!\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnmc4s/building_my_first_llm_homelab_where_to_start/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1uu7rb",
          "author": "Kyuiki",
          "text": "I did my research on this and there is a single conclusion I made. Don‚Äôt waste your money on it. Like really. You can never run the amazing models you see like GPT, Claude, DeepSeek, GLM in a way that feels good. You can have 4 video cards costing 10 grand which will LOAD the model but you‚Äôll still be generating text at 15ish/tokens a second. You‚Äôll be running quantized models that adds additional hallucinations to LLM‚Äôs that already hallucinate a bunch.\n\nYou can get some really really cheap cloud provided services that give API access to some amazing models. In most cases what you spend on your own hardware would take 20+ years to spend on a subscription.\n\nNow if you‚Äôre doing it for pure hobby, start with GPU power. Memory will help LOAD a model and can increase max context size but it will not make the model run faster. GPU offloading is the only thing that will increase tokens/sec in a way that matters. The higher the VRAM on the GPU the better.",
          "score": 14,
          "created_utc": "2026-01-26 17:39:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v2knb",
              "author": "catplusplusok",
              "text": "Depends? I have NVIDIA Jetson Thor which is kind of pricey, but not 10K. I can run Qwen3-Next lighting fast, FP4 quantized but seems perfectly capable, or an uncensored GLM-4.5-Air model for storytelling/roleplay which is 15tps as you mentioned, but worth it when cloud models would lecture you instead of writting stories or you do high volume batch data processing. Mac is another good choice for affordable-ish local AI.",
              "score": 2,
              "created_utc": "2026-01-26 18:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1usnrb",
          "author": "PermanentLiminality",
          "text": "The range is from a cheap GPU like a P102-100 for $50 in whatever computer you already have, to $50k or more for a serious rig.   You really need to start with some kind of budget and what you are trying to run. \n\nTo get started it is almost always the answer to start with an API provider like Openrouter and only look to buying hardware once you have some idea of what you actually need.  In almost all circumstances it will cost a lot more to buy hardware than pay for an API.",
          "score": 5,
          "created_utc": "2026-01-26 17:32:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uvn2n",
              "author": "rmjcloud",
              "text": "updated!",
              "score": 1,
              "created_utc": "2026-01-26 17:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uvs5c",
          "author": "GoodRPA",
          "text": "I've tried to build local LLM/SML just in the last few weeks.\n\nMy recommendation would be:\nEither don't worry about GPU, responses will be slow, but very stable on CPU + ram. \n\nIf you do decide to go with GPU, get more vram(12gb, 16gb, 24gb) and a relevantly recent/supported GPU architecture from 2018+.\n\nBets options with balance of vram/price/power consumption/architecture: \n\nNvidia t4 - 16gb, 70watt only!, Turing, requires fan mode/cooling\nNvidia RTX 3090 - 24gb, 350 watt , Ampere\n\nBudget:  \nAgain, either CPU/shared ram (M1 mini, 16gb)\nEven hp thin clients (t520, t730) can manage CPU models, slowly but these work and very stable.\n\nNvidia RTX A2000 - 12gb, 70watt, ampere\n\nInstead of getting 6gb and below, you can use CPU only or even use remote LLMs, these are good quality, whether we like/can run them locally or not )\n\nBuild everything else around this. 16gb ram is a must.",
          "score": 3,
          "created_utc": "2026-01-26 17:46:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ursk3",
          "author": "gittb",
          "text": "Budget numbers would help, ranges from a couple grand to a down payments on houses for the starter to mid range as far performance goes.",
          "score": 2,
          "created_utc": "2026-01-26 17:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uvmmz",
              "author": "rmjcloud",
              "text": "updated!",
              "score": 1,
              "created_utc": "2026-01-26 17:45:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wqb72",
          "author": "its_a_llama_drama",
          "text": "This depends on a lot of variables.\n\nHow much faff and fiddling are you realistically willing to tollerate?\n\nDo you want to be able to scale up in the future without upgrading everything? \n\nWhat exactly do you want to do with LLMs? Use them and if so how? Fine tune them? Try and train a model yourself?\n\nAre you only bothered about LLMs? Or does image generation interest you? This changes things considerably\n\nI started with your budget in mind, and before i knew it, i spent quite a bit more. \n\nIt depends what you are expecting this machine to do. That budget is tight. You could get a consumer board, cpu and single card running smaller models with high throughput, with ddr5 and pcie 5.0 for that price too.\n\nOr you could go for the old server route and go for xeon dual socket, ddr4 ecc ram and pcie 3.0. ddr4 ram is still expensive, just not as expensive as ddr5. But this option (if you choose the right board), gives you nore optionality later and slower but bigger capacity and future expandability.\n\nThey both serve different purposes. \n\nAlso, some gpus are cheaper for a reason. You can get cheap cards, but they don't have all the bells and whistles a newer card might have. At this budget, you have to choose between throughput and vram. At 2k, you might squeeze two 3090s into the build budget and maybe 4x32GB ECC RDIMMs as well, if you go with a reasonably priced lga 2011 board and xeon v4 cpus.  But factoring in psu(s), storage, cooling, a case or open frame for it, it will be tight. \n\nIf you go for an older card than a 3090 (like the P40) you might find performance disappointing, they lack fp16 performance. but again, it depends what you expect. Do you need it to be pushing 50+ tok/s. You might, if you get the cards and then realise you do, you will feel like it is a let down.  \n\nIf you go for older MI50 AMD cards, you get a lot of VRAM for your money (if you can find 32GB variants), but you pay with set up time and compatability instead. \n\nI think if I were you. I would start off with one of the better e5 v4 xeons on a dual socket lga2011-3 board, probably look for 2699 or if too expensive, 2698 v4. buy 4x32GB RAM, and buy one 3090. Use it, push the limits and then you will know if you need more throughput or more vram. Once you know what you actually need, you can choose what to change\n\n you can probably sell the 3090 for what you bought it for if you decide you want to build around a different card.\n\nYou can add more ram, most dual socket boards have 12 or 16 dimm slots.\n\nyou can add more 3090s if one was good but you want more vram (try and get a board with plenty of x16 pcie slots)\n\nOr if the cpu/platforn is the limitation, you didn't lose too much after you sell the board and cpus, and you can weigh up costs for a newer platform (considerably more)\n\nThis leaves as many options open whilst allowing you to play with a fast gpu with 24GB VRAM and good compatability, and 128GB of RAM. \n\nYou should be able to build that for 1k (just about) and use the other 1k if and when you know what you need.",
          "score": 2,
          "created_utc": "2026-01-26 22:37:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21bs8f",
              "author": "Akimotoh",
              "text": "Damn, what is your spending budget?",
              "score": 1,
              "created_utc": "2026-01-27 16:02:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uw7xt",
          "author": "catplusplusok",
          "text": "Best used Apple Silicon Mac you can find (in terms of maxing out RAM). This will give you much better bang for the buck then PC GPUs and you get a computer useful in many other ways. \n\nThere is also a [64GB NVIDIA dev kit](https://www.amazon.com/NVIDIA-Jetson-Orin-64GB-Developer/dp/B0BYGB3WV4?th=1) within your budget, but beware of limited memory speed (so you can run bigger models but slowly) and need to build lots of things from source for custom compute.",
          "score": 2,
          "created_utc": "2026-01-26 17:48:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxjk4",
              "author": "rmjcloud",
              "text": "I have an M3 Macbook Pro, would this be enough to play around with small models to learn fundamentals and things such as RAG and how to expose LLMs for app usage etc?",
              "score": 2,
              "created_utc": "2026-01-26 17:54:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v0omr",
                  "author": "catplusplusok",
                  "text": "Absolutely! I would say with 16GB you can play with a lot of task specific workflows (machine vision, image generation, processing structured data). With 32GB you can run models that can replace cloud chat AI for most cases, roleplay and write production code for you (look at QWEN3 or other mixture of experts models with around 30B total parameters). Look for mlx 4 bit quantized ones for decent quality / efficiency balance.",
                  "score": 3,
                  "created_utc": "2026-01-26 18:07:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1v09xh",
          "author": "teachersecret",
          "text": "Start by getting your hands on a decent computer. At the cash you have, you're probably looking used. I assume you already have a PC laying around that was built in the last 5 or so years with a modern processor and 32gb-64gb ddr3/ddr4/ddr5 in it. If so, that's the EASIEST way to get going. Just go buy a 3090/4090 or a 5090, shove it in, and you're done.\n\nIf not, you can buy one fairly cheap prebuilt computer secondhand. Go after something from the more modern intel or AMD lineups. Aim for a HIGHER END cpu built some time this decade (2020 or newer if possible) with as many cores/highest speed you can find. You can go on places like facebook marketplace and find people selling entire used rigs that would work fine. Aim for the best processor/most ram you can find, and try to stick DDR4 or DDR5 if you can (as an example I recently purchased a whole 5900x rig with a 165hz monitor, 3080ti, and 64gb of ddr4 for less than $500 all-in off FB marketplace).\n\nAfter you get the machine, the focus needs to be on the GPU. Hopefully the rig already has one that is useful, but if it DOESN'T have a 3090/4090/5090, thats where you need to dump the rest of your budget. Grab a used 3090/4090, or save up for a 5090.\n\nThat's about as far as you can reasonably go on a budget, while still allowing you to play with some of the best performing models. 24gb vram gets you high speed 30b moe models, lightning-fast smaller models like oss 20b or the smaller 8b and 4b options, high speed image and video generation (ltx-2 fits in 24gb), and plenty of support for your explorations (3090 and newer are gold standards of AI right now, and are likely to maintain strong support for years). Anything cheaper is going to be significantly slower, older, barely supported, and really not a great option. You'll see people in here strapping together impressive server rigs using old cast off server parts that can run big boy MoE models, and that's cool, but unless you have a damn good reason to do that and an exhaustive understanding of server hardware, I'd avoid that space. Older server rigs end up too slow to be worth it for most uses, and newer server rigs are too expensive for your budget.\n\nAnything more capable than what I mentioned is going to be significantly more expensive. Obscenely more expensive in some instances. At that point, you're just flat out better off using API inference and paying pennies for tokens.\n\nOne dark horse here is a AI Max 395x 128gb rig. They're small, sip power, and have unified 128gb ram which allows you to run hefty models like 200b style models at usable speed. That said, 'usable' is still SIGNIFICANTLY SLOWER than what you'll get off a good GPU, and those AI max rigs are over 2k USD now. Not a terrible option if you want a quiet little box to experiment with though. Same goes for the newer macbook pro line. They're pricey, but if you grab one with enough unified memory you can run something as big as deepseek at a speed that could still be considered usable. These might not be the cheapest options today, but as time passes and more of these kinds of rigs hit the used market, they're going to end up being pretty solid AI hardware to snatch up.",
          "score": 1,
          "created_utc": "2026-01-26 18:05:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yc3s6",
          "author": "WishfulAgenda",
          "text": "Alright, awesome questions. I‚Äôll start with what I think then go into why. What I think you should do is on the MBP m3 download lm studio, vs code with continue dev. On the MBP m2 download vm fusion install Linux (mint, tumbleweed etc) and on the Linux machine install clickhouse and grafana) connect the lm studio llm to the clickhouse vm via an mcp server and your laughing. You could probably even use docker instead of the Linux vm as well. Also have a look at librechat . All of those applications are open source and free. I‚Äôd then start saving my money as the next jump is likely expensive and my guess would be an MBP m5 max, a tricked out Mac mini or a Mac Studio  when they come out or a substantially more expensive desktop. \n\nThe above isn‚Äôt as much fun as building a new machine but for your budget I‚Äôm not sure that the new machine would have much on the MBP m3, the apple silicon is very good. Especially with the current ram and gpu prices.\n\nAlso don‚Äôt let it put you off as the small models with the right configuration and actually be really good. You can set it up so you have specialist agents with system prompts that focuses them and the set up MoA architectures on the same model.\n\nGood luck :-)",
          "score": 1,
          "created_utc": "2026-01-27 03:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uunh6",
          "author": "Limebird02",
          "text": "OP said around $1000 GBP which is $1370. \n\nDoes a local 30B model off Ollama do ok or can we get by with less? Id like to try the same with $500 mini pc. \n\nHow does open code help? \n\nHow does clawdbot work well with smaller local models? \n\nLike OP want to dabble without large outlay.",
          "score": 0,
          "created_utc": "2026-01-26 17:41:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnpti6",
      "title": "Using a high-end MacBook Pro or a beefy RTX 5090 laptop (with 24 GB of RAM) for inference.",
      "subreddit": "LocalLLM",
      "url": "https://www.reddit.com/r/LocalLLM/comments/1qnpti6/using_a_highend_macbook_pro_or_a_beefy_rtx_5090/",
      "author": "FoxtrotDynamics",
      "created_utc": "2026-01-26 19:17:10",
      "score": 12,
      "num_comments": 29,
      "upvote_ratio": 0.77,
      "text": "Hey all ‚Äî looking for input from folks who‚Äôve actually run¬†**large local LLMs¬†(70B+)** on either Apple Silicon or high-end RTX laptops. When I grow up someday, I want to blow an absurd amount on a peak laptop for LLM performance (buy one, cry once). Outside of having a sweet gaming rig, I work in IT consulting and need a powerful rig for client demos in the future.\n\nI‚Äôm trying to decide between two¬†*portable*¬†setups:\n\n* **MacBook Pro (M-series Max, e.g. M4 Max)**: Looking for 128‚Äì192 GB unified memory\n* **Windows/Linux laptop w/ RTX 5090 (24GB VRAM)**: At minimum 64 GB system RAM\n\n**Primary use case:**\n\n* Local LLM¬†**inference**¬†(RAG, long context, offline use)\n* Targeting¬†**‚â•15 tokens/sec**¬†sustained\n* Portability matters (I don't want to carry an extra tower, monitor, and keyboard/mouse with me)\n\n**Secondary / future use case:**\n\n* Fine-tuning (likely LoRA / QLoRA, not full pretraining). I am not super knowledgeable in this space, but I did manage to fine-tune something with Unsloth\n   * My LLM created garbage output. I want to get better at it eventually.\n\nThings I‚Äôm specifically curious about from people with hands-on experience:\n\n* For¬†**inference**, does the larger unified memory on Apple Silicon meaningfully outweigh the raw CUDA performance of the RTX laptop?\n   * 70B\n   * 120B\n   * Even more beyond that\n* How painful (or not) is¬†**Apple MLX**¬†today for fine-tuning?\n   * Is it a real blocker, or just more limited / immature vs CUDA?\n* Thermals & sustained performance on long inference runs for both setups\n   * Will it generally be okay for inference (or will I be able to cook an egg with it)?\n\nI‚Äôm not expecting a perfect answer ‚Äî just trying to sanity-check what‚Äôs¬†*actually usable day-to-day*¬†versus theoretical specs.\n\nAppreciate any firsthand data or strong opinions from experience!\n\nhttps://preview.redd.it/89vod4c6uqfg1.jpg?width=1024&format=pjpg&auto=webp&s=a959dd0a935e0fc330b369edc69fc8804913e134",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LocalLLM/comments/1qnpti6/using_a_highend_macbook_pro_or_a_beefy_rtx_5090/",
      "domain": "self.LocalLLM",
      "is_self": true,
      "comments": [
        {
          "id": "o1x6egk",
          "author": "single_threaded",
          "text": "I‚Äôve got a MacBook Pro M4 Max, 128 GB RAM and I run GPT OSS 120B at 72 tokens/sec with low reasoning and 69 TPS on medium. It‚Äôs a beast for local LLMs.",
          "score": 16,
          "created_utc": "2026-01-26 23:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xl1br",
              "author": "FoxtrotDynamics",
              "text": "Thank you for the info!",
              "score": 3,
              "created_utc": "2026-01-27 01:13:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o202un0",
                  "author": "sunole123",
                  "text": "Listen to him. Mac are best experience, especially if you don‚Äôt care about gaming.",
                  "score": 0,
                  "created_utc": "2026-01-27 12:05:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1yy01f",
              "author": "sapoepsilon",
              "text": "How is it with prompt processing? I am trying to use opencode locally on my M4 Pro 48GB, and it fails miserably.  \n  \n I am debating whether I should update to higher RAM once Apple releases their new M5 Max chip, or if I should bite the bullet and invest in a proper Nvidia setup.",
              "score": 1,
              "created_utc": "2026-01-27 06:11:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20bes7",
                  "author": "bakawolf123",
                  "text": "Alas Mac has up to 10 times slower PP looking at [https://github.com/ggml-org/llama.cpp/discussions/15013](https://github.com/ggml-org/llama.cpp/discussions/15013) and https://github.com/ggml-org/llama.cpp/discussions/4167. M5 is looking promising for closing the gap though, based from the base M5 results and assuming similar scaling as previous models M5Max could get up to 2.1k PP and Ultra something like 3.6k on 7B dense model with Q4. Mac would still be trailing by a factor but there won't be such a glaring difference between TG and PP factors.",
                  "score": 2,
                  "created_utc": "2026-01-27 13:02:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1x5gr3",
          "author": "aretheworsst",
          "text": "I think for day to day use the Mac is really hard to beat. Crazy power efficiency and you‚Äôll be able to fit a model like gpt-oss 120b directly. I‚Äôm also a Mac user for work and personal, so for me it was the obvious choice. I only have a 64gb MacBook Pro, but I run lm studio in the background while working with the smaller oss and others and love it.",
          "score": 8,
          "created_utc": "2026-01-26 23:53:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vjrwf",
          "author": "2025sbestthrowaway",
          "text": "üçø Following",
          "score": 7,
          "created_utc": "2026-01-26 19:27:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wpkp4",
          "author": "neotorama",
          "text": "1 minute battery on rtx",
          "score": 8,
          "created_utc": "2026-01-26 22:33:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1w1l1s",
          "author": "racerx509",
          "text": "Following.  Right now, I'm experimenting with both.   I have a lenovo laptop with a 3070ti with 12gb ram, a  custom desktop with a 5070 with 12g ram and a MBP M2 Max with 96gb ram.  Thus far I've been doing my inferences on the MBP but I'm curious to know as well.",
          "score": 4,
          "created_utc": "2026-01-26 20:45:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1w2rh8",
              "author": "FoxtrotDynamics",
              "text": "How much TPS are you getting on your models? For the sake of this standardness, how about GPT OSS 120B?",
              "score": 3,
              "created_utc": "2026-01-26 20:50:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wpayk",
          "author": "No-Concern-8832",
          "text": "Correct me if I'm wrong, I don't think you can get any RTX laptop with enough VRAM to run 70B models.",
          "score": 6,
          "created_utc": "2026-01-26 22:32:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wx26k",
              "author": "FoxtrotDynamics",
              "text": "I've gotten GPT OSS 120B to run on an older Dell Precision 7740 laptop with a RTX 5000 with 16 GB of VRAM and 128 GB of RAM, but I swear I only got like 3-5 TPS. I did it in LM Studio in Windows 11, but whatever couldn't fit was put into the RAM. I am curious if newer hardware closes the gap.\n\nDoes it work? Yes. Does it work (well)? No. The spillover into RAM is what kills the token processing time",
              "score": 2,
              "created_utc": "2026-01-26 23:10:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1yiwot",
                  "author": "vertical_computer",
                  "text": ">The spillover into RAM is what kills the token processing time\n\nThen you already know the answer.\n\nA laptop RTX ‚Äú5090‚Äù is only 24GB of VRAM, which is better, but pales in comparison to an equivalent MacBook.\n\nAlso remember that the memory bandwidth scales up with the higher end M4 chips. The M4 Max is 546GB/s which blows the doors off any dual-channel DDR5 setup.\n\n**The only time the RTX 5090 will be better is when you‚Äôre running small models that fit entirely within VRAM.** As soon as you spill over to RAM, the MacBook eats it for breakfast.",
                  "score": 6,
                  "created_utc": "2026-01-27 04:25:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1yf8wz",
                  "author": "bluelobsterai",
                  "text": "For oss-120 I‚Äôd get a 128gb Mac. Or just rent",
                  "score": 1,
                  "created_utc": "2026-01-27 04:02:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1vzlqr",
          "author": "mon_key_house",
          "text": "Why not use the laptop as terminal to a normal PC?",
          "score": 5,
          "created_utc": "2026-01-26 20:36:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1w3a88",
              "author": "FoxtrotDynamics",
              "text": "Would be nice, yes, but I'm looking for portability and local inference. I also would rather not dump money on a Nvidia H100 (plus the tower)",
              "score": 2,
              "created_utc": "2026-01-26 20:53:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1wfnw7",
                  "author": "eleqtriq",
                  "text": "You‚Äôre taking about vastly different levels of computing.",
                  "score": 7,
                  "created_utc": "2026-01-26 21:48:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ys4po",
              "author": "Consistent_Wash_276",
              "text": "So, reaffirming and suggesting @mon_key_house (great name by the way) as an example I have my M1 MacBook Pro and a 256gb Unified memory M3 Ultra at home. I‚Äôm on the road constantly, but I use Tailscale to connect the devices and I run the local LLMs on that device and have integrated anywhere I‚Äôd like on my MacBook Pro. \n\nIn the way that you‚Äôre considering this option look at the cost savings or upgraded computing depending on what you‚Äôre looking for. \n\nExample: Base MacBook Pro M4 Pro\n14-inch MacBook Pro - Space Black\n$2399\nApple M4 Pro chip with 12-core CPU, 16-core\nGPU, 16-core Neural Engine\n48GB unified memory\n512GB SSD storage\n\nExample: Mac Studio Base Model M4 Max (just $100 more). 12 to 16 CPU cores, 16-40 Core GPUüö®\nMac Studio\n$2499\nApple M4 Max chip with 16-core CPU, 40-core\nGPU, 16-core Neural Engine\n48GB unified memory\n512GB SSD storage\n\nExample: Mac mini Base Model with 48gb unified memory\nSame computing as the MacBook above but $600 less than the MacBook Pro. \n$1799\nApple M4 Pro chip with 12-core CPU, 16-core\nGPU, 16-core Neural Engine\n48GB unified memory\n512GB SSD storage\n\nIf you have a monitor at home and you have a Laptop already (even older used ones) you can get more value for every dollar poured into the desktop",
              "score": 1,
              "created_utc": "2026-01-27 05:27:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wzmya",
          "author": "crunchyrawr",
          "text": "I don‚Äôt fine tune, but the MacBook Pro works well for running large models typically 120B and under for me (battery life while doing so is awful).\n\nThe RTX laptop doesn‚Äôt sound viable to me for larger models generating tokens at a reasonable rate.\n\nDepending on what you‚Äôre doing, agentic loops tend to slow down, but simple back and forth chat runs extremely well on the MacBook.",
          "score": 3,
          "created_utc": "2026-01-26 23:23:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yafvz",
          "author": "Food4Lessy",
          "text": "For pro use 24-48gb 4090 for training Nvidia or dgx spark 128gb.\n\n\nRent emc\n\n\nAny Apple Max 128h , AMD Max 128gb is compromised¬†\n\n\nThe Value kings is $900 M1 Max 64gb, $700 24gb 3090 remote.\n\n\n2nd place 5070 ti, or 5080 laptop $1500, $1800 AMD Max 128gb remote\n\n\nOnce to go 500+ ts on 120b oss for pennies,¬† others seem laughable as time is money and efficient¬†",
          "score": 3,
          "created_utc": "2026-01-27 03:33:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ye3iw",
              "author": "jrdubbleu",
              "text": "What do you mean the Apple Max is compromised? Or were you just referring to the AMD?",
              "score": 2,
              "created_utc": "2026-01-27 03:55:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1yfzdz",
                  "author": "FoxtrotDynamics",
                  "text": "As mentioned, what do you mean by compromised. I am as confused as u/jrdubbleu is",
                  "score": 2,
                  "created_utc": "2026-01-27 04:07:02",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1w3bys",
          "author": "Tired__Dev",
          "text": "I have an Asus M16 with a 4090 and it struggled to hold a 7b parameter model. I know the 5090 has more vram, but it‚Äôs a laptop. \n\nFor me personally I‚Äôm going to go with MacBook Pro with 128gb of ram. The memory bandwidth is close to that of the M3 ultra with the new M5 max apparently just with less gpu cores. It‚Äôll probably beat the piss out of a DGX spark.",
          "score": 2,
          "created_utc": "2026-01-26 20:53:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1w3w6h",
              "author": "FoxtrotDynamics",
              "text": "Really, a 7B model struggling on a 16 GB VRAM device? Is it averaging around 30 TPS? I'd think that is acceptable",
              "score": 4,
              "created_utc": "2026-01-26 20:55:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x77vh",
                  "author": "HappyContact6301",
                  "text": "I have 7B running on my iPad - runs just fine. The problem with a mobil platform is that you get into thermal throttling very quickly.",
                  "score": 2,
                  "created_utc": "2026-01-27 00:02:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1whhy2",
              "author": "Large-Excitement777",
              "text": "That doesn‚Äôt sound right at all. What do you think could be happening?",
              "score": 3,
              "created_utc": "2026-01-26 21:56:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zaswb",
          "author": "WinDrossel007",
          "text": "PC with 5090 :)",
          "score": 1,
          "created_utc": "2026-01-27 07:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zcdn7",
          "author": "AIMasterChief",
          "text": "RTX5090 Laptop user here:: 160tps for GPT-OSS 20b and 18tps for 120b",
          "score": 1,
          "created_utc": "2026-01-27 08:13:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z456p",
          "author": "Final-Rush759",
          "text": "M5 max",
          "score": 0,
          "created_utc": "2026-01-27 07:01:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}