{
  "metadata": {
    "last_updated": "2026-02-10 09:19:59",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 108,
    "file_size_bytes": 136569
  },
  "items": [
    {
      "id": "1qxmeee",
      "title": "I built RAG for 10K+ NASA docs (1950s‚Äìpresent) in 2 weeks: VLMs for complex tables, diagrams & formulas, 657K+ pages on a single H100, live-streamed full build.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qxmeee/i_built_rag_for_10k_nasa_docs_1950spresent_in_2/",
      "author": "Low_Acanthisitta7686",
      "created_utc": "2026-02-06 16:32:09",
      "score": 239,
      "num_comments": 39,
      "upvote_ratio": 0.94,
      "text": "**TL;DR:** I designed and built a full RAG system over 10,000 NASA technical documents spanning the 1950s to 2025 ‚Äî we're talking scanned typewriter reports, handwritten notes, propulsion diagrams, mathematical formulas, failure investigations. Off-the-shelf tools broke down fast. I ended up building a custom pipeline using Qwen3-VL-8B to process what traditional OCR and parsers couldn't handle, ran the whole thing on a single H100 (657,000+ pages, \\~180 pages/min), and built an agentic retrieval system that doesn't just search ‚Äî it investigates like a domain expert. The architecture is designed to scale to 100K+ documents. Everything was live-streamed (140+ hours across 15 streams), and the GitHub repo for the document processing pipeline and infra is coming soon.\n\nHey everyone, I'm Raj. Over the last 2 weeks, I live-streamed building what turned out to be the most technically challenging project I've taken on ‚Äî and I wanted to share the experience while it's fresh. This is a long one, I tried to keep it short, but there was too much that I think is genuinely useful to cut.\n\n# The Domain\n\nSo here's the scenario I designed for this project ‚Äî a fictional aerospace consultancy called \"Meridian Aerospace,\" modeled on very real challenges these companies face.\n\n85,000+ documents accumulated over 70+ years ‚Äî real documents from NASA's Technical Reports Server (NTRS). Propulsion test reports, failure investigations, component specs, regulatory filings. Engineers spending 4-6 hours per project digging through archives. A missed critical failure mode last quarter because the relevant data was buried in a 1997 test report nobody knew existed.\n\nNow here's what makes these documents painful:\n\n* 1950s‚Äì1990s scanned reports ‚Äî photocopied, faxed, re-scanned, degraded quality\n* Dense technical diagrams everywhere: thrust curves, propulsion schematics, thermal analysis charts\n* Mathematical formulas and engineering equations scattered throughout\n* Domain-specific acronyms (Isp, TWR, LOX, MMH, NTO) that are often never expanded in the text\n* Cross-references between documents ‚Äî failure reports cite original test data, compliance docs reference design specs\n* Tables spanning multiple pages with nested sub-headers\n\nI used 10,000 documents from NASA's Technical Reports Server as the working dataset, with the architecture designed from day one to handle the full 85K+ and beyond.\n\n# What I Built\n\nI'll walk through the three main layers, but I want to be clear ‚Äî these aren't independent pieces you build one after another. They feed into each other constantly. Decisions in the document processing layer directly shaped how the agent works, and understanding how engineers actually think (the agent layer) changed how I approached extraction. It's all connected.\n\n# The Document Processing Pipeline\n\nThis is where a huge chunk of the work lived, and honestly where most people underestimate the difficulty. The core realization: **you cannot build good retrieval over bad extractions.** If your chunked text is garbage, no embedding model or re-ranker is going to save you.\n\nI used Docling (from IBM, I know it has a ton of issues ‚Äî I found workarounds and solved them too) for layout detection ‚Äî figuring out where tables, figures, formulas, and text blocks sit on each page. Then Qwen3-VL-8B to actually interpret what's in those regions.\n\nA few of the harder problems:\n\n**Formula association:** Docling detects formulas fine, but they lose their position in the document flow. So you get a formula floating at the end of a page with no connection to the paragraph it belongs to. I built a system that paints colored bounding boxes with ID numbers directly onto page screenshots, then asks the VLM \"where does Formula 7 belong relative to these numbered paragraphs?\" Sounds weird, works surprisingly well. Gives you reading-order accuracy without re-OCRing anything.\n\n**Complex tables:** These were probably the single most painful thing to solve. We're talking massive grids ‚Äî 72 columns by 50 rows of stability data ‚Äî where position determines meaning. Down arrows mean \"carry this value down.\" Brackets group five rows under \"Unstable.\" Zebra lines and grid lines guide the human eye across dense numbers. Standard OCR reads left-to-right, top-to-bottom and has no idea what to do with any of this. Parsers treat the grid lines as noise or lose alignment if the scan is slightly tilted.\n\nI went through a lot of approaches. Standard markdown extraction lost alignment. CV-based heatmaps and projection lines to detect rows ‚Äî worked about 80% but too brittle for production. JSON output from the VLM broke constantly on large tables (missed closing brackets). Small models (7B) hallucinated numbers and missed columns entirely.\n\nWhat actually worked was treating the table as a photograph of data rather than a stream of text. Use Docling purely for finding the bounding box coordinates, crop the original high-res page image (no downscaling ‚Äî that destroys data in dense tables), and send the full-resolution crop to a large VLM. You need 72B+ to hold context across a 30-column table without losing track.\n\nTwo tricks that made a real difference. First, for tables with zebra lines or warped scans, I pre-process the image by drawing red horizontal lines onto it before sending to the VLM ‚Äî basically a \"digital ruler\" that forces the model to keep row alignment. Second, the prompt strategy ‚Äî instead of asking for just structured output, I ask for markdown (way more robust than JSON for grid data) plus a \"notes\" field where the model captures visual shorthand. \"If there's a down arrow, note the value is carried down. If there's a bracket, note the grouping.\" The model successfully returned \"unstable\" for rows that didn't explicitly have the text but were visually grouped under an \"Unstable\" bracket.\n\nFor the truly dense tables that still needed more work, I have a fallback that generates a detailed description and serves the raw image alongside it ‚Äî which honestly, in aerospace, engineers prefer anyway over a potentially wrong structured output. But this isn't a dead end. The digital ruler approach and the prompt strategy were working well, and with more time I think there's a solid solution there. I was time-boxed to 2 weeks for this entire project, so I made the pragmatic call to move on. Might revisit this specifically and share if I make a breakthrough.\n\n**Legacy scan quality:** Documents from the 1960s have noise, \"Confidential\" stamps, hole punches, scan artifacts ‚Äî and models happily pick all of these up as \"figures.\" Added a classification step asking the VLM: \"Is this a technical diagram or just a document artifact?\" Simple, but it cleaned up a lot of noise.\n\n**The full-page strategy:** I initially tried cropping individual formulas to save tokens. Docling's format detection models missed about 60% of small formulas in dense pages. So I pivoted ‚Äî if any formula is detected on a page, send the entire page screenshot to the VLM and let it transcribe everything in reading order. More expensive per page (didn't matter as I deployed on a GPU), but the accuracy difference is massive. In this domain, a missed variable isn't a minor bug.\n\n**On OCR,** I didn't actually need traditional OCR for most of the heavy lifting. The figures, tables, and formulas ‚Äî which are the hardest parts of these documents ‚Äî were all handled by the VLM pipeline. OCR was only needed as a fallback for pages where the embedded text layer was missing or corrupted. So the approach became: use native text extraction where available, VLM for all the visual/structured content, and OCR only when truly needed. Disabling forced OCR where it wasn't necessary cut processing time significantly.\n\n# H100 Infrastructure & Scaling\n\nProcessing 10K documents ‚Äî roughly 657,000+ pages ‚Äî on a single H100 was its own adventure.\n\n**Where it started:** My first attempt was basically a monolithic script. Every worker loaded the PDF, loaded the model onto the GPU, ran inference, unloaded. Workers were fighting each other for GPU memory, CPU, RAM. Everything was crashing. Back-of-the-napkin math said this approach would take somewhere around 28 days for the full dataset. Obviously not going to work.\n\n**The rewrite:** I moved to a proper service-oriented architecture. Separated the CPU-heavy work (Docling parsing, chunking, text extraction) from the GPU-heavy work (VLM inference). Stateless Celery workers handle the CPU side, feeding requests to a persistent vLLM server that does nothing but inference. Redis as the message broker. Took some inspiration from how production ML systems handle millions of requests with limited compute ‚Äî keep your inference engine as a persistent service, don't have each worker spin it up and tear it down.\n\nThat alone brought the estimate down to maybe 5-9 days. Still not great.\n\n**Then the tuning started.** FP8 quantization because running standard GGUF/Ollama on an H100 is wasting the hardware ‚Äî FP8 is specifically optimized for Hopper. Concurrency tuning: tested 6, 8, 9, 10 Docling workers. 9 caused instant OOM. 10 saturated the queue. 6 underutilized the GPU. 8 was the sweet spot. Dynamic image scaling for oversized PDFs ‚Äî some scans were 170MB, crashing workers during bitmap conversion. VRAM memory leak management ‚Äî usage would creep up batch after batch until it crashed, so I added explicit garbage collection between cycles.\n\n**End result:** \\~2.5 days, running at about 180 pages per minute. From 28 days to 2.5 days on the same hardware, just by thinking about architecture and resource management. Again, could have done better, but was on a time crunch.\n\n# The Agent & Retrieval Layer\n\nThis part tends to get underestimated. Building the agent wasn't just \"wire up some tools to an LLM and write a system prompt.\" A huge amount of time went into two things: understanding the people who would actually use this system, and shaping how the agent itself thinks.\n\nI spent a lot of time with Claude role-playing as different engineer personas ‚Äî a cautious senior engineer (\"Sandra\") approaching retirement who's seen things go wrong, a junior engineer who searches too narrowly. I was trying to understand: how does their day actually work? How do they use current traditional systems? What's literally going through their mind when they're investigating a failure mode? What are they worried about that they won't say out loud?\n\nThat process shaped everything about the agent. For example ‚Äî engineers don't just look for failure cases. They specifically look for *success cases* as counter-evidence to validate risky designs. A standard RAG setup completely misses that nuance. Or the fact that a \"question about a valve failure\" might actually be about defending a design decision in a review meeting next week. The agent needs to understand the situation behind the question.\n\nThat understanding fed directly into how I designed the agent's reasoning. One of the bigger realizations was that spiking domain intuition in the system prompt often outperforms complex retrieval engineering. Instead of hardcoding examples, I focused on making the agent think like a propulsion engineer. It should be low-opinionated and already have hypotheses before it runs a single search. When someone mentions a pressure value, it should have intuition about whether that's nominal or concerning. When it finds a document, it should reason about what it means, not just return it. It's not a search tool ‚Äî it's a reasoning engine with engineering expertise that uses search as one of its tools. And honestly, this is still just at the system prompt level ‚Äî keeping it low-opinionated, letting the model lean on its own domain knowledge rather than constraining it ‚Äî but it brings absolute wonders to how the system behaves.\n\nWhat came out of all that work:\n\nThe agent doesn't just search ‚Äî it investigates. It maintains a working task list and notes, forms hypotheses based on its domain intuition before it even touches the search tool, and updates its understanding as it learns. When a question branches, it spawns sub-agents for parallel research threads. It can navigate ‚Äî read adjacent chunks, follow cross-references between documents, pull threads across decades of reports.\n\nWhen the text extraction is uncertain ‚Äî and on 1950s docs, it will be ‚Äî the agent can request a screenshot of the actual PDF page region to visually verify what it's reading. That \"visual region\" tool ended up being one of the most important things in the whole system. It's the bridge between \"95% OCR accuracy\" and \"actually trustworthy in aerospace.\"\n\nI also integrated the NASA Thesaurus ‚Äî 18K aerospace terms filtered down to 3.5K propulsion-relevant concepts ‚Äî so the system handles query expansion properly. \"LOX\" matches \"Liquid Oxygen,\" \"2000 PSI\" finds results mentioning \"13.9 MPa.\" Without this, you're relying on exact keyword matches in a domain where everyone uses different terminology for the same thing.\n\nAnd time-boxed search ‚Äî engineers ask things like \"what do we know about cryogenic engine failures between 1970 and 1980?\" Filtering by time period before semantic search cuts the search space dramatically. When I tested this, the agent successfully traced the 50-year evolution of cryogenic systems ‚Äî from passive insulation in the 1970s to active cryo-coolers in the 2020s ‚Äî without any deep research mode. Just proper filtering and good retrieval.\n\n# What's Coming Next\n\nI've linked all the YouTube streams in the comments below ‚Äî 15 streams, some of them are 11+ hours long, so obviously that's a lot to sit through. To make things more digestible and actually useful, I'm going to be posting specific problem/solution breakdowns over the next few days, including how I evaluated the system with 10K docs. Each of these topics was genuinely its own nightmare to solve, and I think the details will be helpful for anyone working on similar problems.\n\nI'm also hoping to open-source the document processing pipeline and infrastructure code on GitHub soon, which I think will be genuinely useful for anyone dealing with large-scale document processing ‚Äî whether it's aerospace or not.\n\nOne last thing ‚Äî I genuinely want to thank the team behind Claude Code. Being honest, a project like this would realistically take a team of 3-4 engineers working 3-4 months. The document processing pipeline alone, the infrastructure, the agent design, the frontend, evaluation ‚Äî each of these is a serious body of work. I did it solo in 2 weeks, live on stream, and that would not have been possible without Claude Code, it was in the loop for pretty much all of it. Seriously, thank you to the engineers behind it.\n\nHappy to answer questions, and if you've dealt with similar problems ‚Äî legacy docs, domain-specific retrieval, scaling document processing ‚Äî I'd love to hear what you ran into.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qxmeee/i_built_rag_for_10k_nasa_docs_1950spresent_in_2/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3xvwcn",
          "author": "ksk99",
          "text": "Any evaluation metrixs u used, any subset of data set along with queries and results we can use to recreate/learn",
          "score": 6,
          "created_utc": "2026-02-06 18:02:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xxjlq",
              "author": "Low_Acanthisitta7686",
              "text": "Yeah! So in the last stream (Part 14) I went through evaluation. Basically, I loaded co-related full docs into Gemini (1M context window) and had it generate complex questions that tested different aspects of the agent ‚Äî things like cross-document reasoning, degraded documents, negative/boundary tests, and more.   \n  \nThen I evaluated and tested whether the system could find the right set of docs within the 10K-doc space.   \n  \nI felt like this gave me a solid initial test suite to run, which would probably end up being a pretty strong evaluation as well. Sure, I‚Äôll share the dataset and pipeline with the repo soon.",
              "score": 4,
              "created_utc": "2026-02-06 18:09:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xniw6",
          "author": "vladimirxi",
          "text": "Nuts.  Well done on the documentation!  Crazy project.  Well thought out.",
          "score": 10,
          "created_utc": "2026-02-06 17:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xpjs5",
              "author": "Low_Acanthisitta7686",
              "text": "Thank you!",
              "score": 4,
              "created_utc": "2026-02-06 17:31:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xd6ic",
          "author": "Low_Acanthisitta7686",
          "text": "Check the live streams here (15 streams, 140+ hours): [https://www.youtube.com/@rajsuthanofficial7585/streams](https://www.youtube.com/@rajsuthanofficial7585/streams)",
          "score": 8,
          "created_utc": "2026-02-06 16:33:17",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3xz1qw",
              "author": "flaxseedyup",
              "text": "You absolute gem of a man. I will definitely be looking at this. Thank you so much for sharing!    \n    \nWhere did you learn the majority of the core skills needed for such a project?    \n    \nDo you rate the ‚ÄúThe AI Automators‚Äù who are on YouTube?",
              "score": 5,
              "created_utc": "2026-02-06 18:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3y00x7",
                  "author": "Low_Acanthisitta7686",
                  "text": "haha thanks ‚ù§Ô∏è\n\nI pretty much learned by building stuff. I was working on my startup and also worked with a few enterprises along the way, but mostly just learned by doing random complex projects!",
                  "score": 1,
                  "created_utc": "2026-02-06 18:21:35",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yeh4z",
          "author": "makinggrace",
          "text": "This was a beast of a project. Love it.",
          "score": 2,
          "created_utc": "2026-02-06 19:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yfh68",
              "author": "Low_Acanthisitta7686",
              "text": "for sure.... thanks!",
              "score": 1,
              "created_utc": "2026-02-06 19:35:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zizu5",
          "author": "tehsilentwarrior",
          "text": "Cool stuff. Will be useful in playing Kerbal üòÇ\n\nNo but, impressive stuff. Hope to see it soon!",
          "score": 2,
          "created_utc": "2026-02-06 22:54:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40q0gm",
              "author": "Low_Acanthisitta7686",
              "text": "appreciate it mate!",
              "score": 1,
              "created_utc": "2026-02-07 03:12:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zr9gx",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-02-06 23:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40q9jt",
              "author": "Low_Acanthisitta7686",
              "text": "thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-07 03:13:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40685c",
          "author": "Legitimate-Leek4235",
          "text": "I wonder if we give this prompt to opus 4.1 and all the agents which built the compiler, will it be able to replicate this",
          "score": 2,
          "created_utc": "2026-02-07 01:09:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40qtfh",
              "author": "Low_Acanthisitta7686",
              "text": "Nope ‚Äî not possible at all. There are so many moving pieces and decisions to make, and honestly it takes a ton of thinking, trial, and error to get to this point. Check my YouTube streams ‚Äî for around 3 days (each 8+ hours) I was banging my head building the infra, and then we completely rewrote the entire thing in a day. So it literally takes a lot of work. Luckily I‚Äôm an engineer, so it naturally 100√ó my potential.",
              "score": 1,
              "created_utc": "2026-02-07 03:17:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40cczw",
          "author": "OnlyTimeFan",
          "text": "Thanks for sharing and spreading the knowledge",
          "score": 2,
          "created_utc": "2026-02-07 01:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40r1c8",
              "author": "Low_Acanthisitta7686",
              "text": "always!",
              "score": 2,
              "created_utc": "2026-02-07 03:18:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o415khy",
          "author": "FiddlyDink",
          "text": "This is incredible work! I‚Äôm working on a RAG project to connect an LLM to a Neo4j data store to be able to ask questions about old records and I know how challenging it can be. I‚Äôm not trying to solve half the challenges that you had to and yet I‚Äôm still struggling. I would love to learn more about this work and will definitely be checking out your streams!",
          "score": 2,
          "created_utc": "2026-02-07 04:59:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41c2p0",
              "author": "Low_Acanthisitta7686",
              "text": "‚ù§Ô∏è",
              "score": 1,
              "created_utc": "2026-02-07 05:50:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o41t0xh",
          "author": "Electrical-Paper-323",
          "text": "Excellent! Looking forward to the GitHub project üòä",
          "score": 2,
          "created_utc": "2026-02-07 08:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ts01",
          "author": "Neovison_vison",
          "text": "Can you TLDR us in new since your last write up 4 months ago?",
          "score": 2,
          "created_utc": "2026-02-07 08:32:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4258z7",
              "author": "Low_Acanthisitta7686",
              "text": "Worked on a couple of projects since then, wrapped up a few, and have a few ongoing. But mostly right now I‚Äôm focused on my startup, basically building something similar: on-prem search at scale for regulated enterprises like finance, pharma, ITAR-protected companies, and more. It‚Äôs [intraplex.ai](http://intraplex.ai) if you‚Äôre interested.",
              "score": 2,
              "created_utc": "2026-02-07 10:25:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42hujq",
          "author": "redbull-hater",
          "text": "Good job man",
          "score": 2,
          "created_utc": "2026-02-07 12:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42pk2x",
          "author": "jaykeerti123",
          "text": "Great stuff. Waiting for the GitHub code.",
          "score": 2,
          "created_utc": "2026-02-07 13:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4bwsxb",
          "author": "Rockingtits",
          "text": "Super cool!",
          "score": 2,
          "created_utc": "2026-02-08 22:24:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fuhpt",
              "author": "Low_Acanthisitta7686",
              "text": "thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-09 14:45:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4di4d3",
          "author": "JackfruitVivid180",
          "text": "how can I get started into this ?\n\n",
          "score": 2,
          "created_utc": "2026-02-09 03:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4furqr",
              "author": "Low_Acanthisitta7686",
              "text": "I did a post on this a couple of months back, might be helpful for you - [https://www.reddit.com/r/LLMDevs/comments/1nl9oxo/i\\_built\\_rag\\_systems\\_for\\_enterprises\\_20k\\_docs/](https://www.reddit.com/r/LLMDevs/comments/1nl9oxo/i_built_rag_systems_for_enterprises_20k_docs/)",
              "score": 2,
              "created_utc": "2026-02-09 14:47:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yvpnv",
          "author": "No_Wrongdoer41",
          "text": "Me and a small team are building a platform that can accomplish something comparable in a matter of hours, repeateably, on corpuses of thousands of documents. I'd love your feedback on what we're working in if you don't mind discussing it with me!",
          "score": 1,
          "created_utc": "2026-02-06 20:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ywgcm",
              "author": "Low_Acanthisitta7686",
              "text": "sure",
              "score": 1,
              "created_utc": "2026-02-06 21:00:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48ti6r",
          "author": "Manson_79",
          "text": "wanted to do the same for my company, but,  i was afraid for legal reasons about hallucinations,  how did you get around that?   i'm also in aviation fyi\n\n",
          "score": 1,
          "created_utc": "2026-02-08 12:51:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yhhfr",
          "author": "satechguy",
          "text": "Which Ilm wrote such verbose text ;-)",
          "score": 1,
          "created_utc": "2026-02-06 19:45:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yitmx",
              "author": "Low_Acanthisitta7686",
              "text": "haha, I wrote it and had claude help polish the english.",
              "score": 2,
              "created_utc": "2026-02-06 19:52:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zji2u",
                  "author": "tehsilentwarrior",
                  "text": "I write a lot too and people these days just think AI did it. \n\nI spent quite a lot of time perfecting my English so when I documented a massive project we have my boss kept asking what AI I used for that .. lol\n\nI just said <my inicials>AI, and he was hilariously confused when he couldn‚Äôt find it. I said I trained it myself over the last decades",
                  "score": 3,
                  "created_utc": "2026-02-06 22:57:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3y97qu",
          "author": "bbahner",
          "text": "This is awesome! Thanks for sharing. ",
          "score": 1,
          "created_utc": "2026-02-06 19:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ybujq",
              "author": "Low_Acanthisitta7686",
              "text": "Sure!",
              "score": 1,
              "created_utc": "2026-02-06 19:17:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4241bl",
          "author": "QuanstScientist",
          "text": "OMG, what an amazing process documentation. We both went through a very similar process, I‚Äôm working on a local RAG for the Epstein archive files, still in development, also using Claude, here at some details, still not in a state for public release https://boltzmannentropy.github.io/Librarius/",
          "score": 0,
          "created_utc": "2026-02-07 10:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o406bb6",
          "author": "EditorDisastrous4994",
          "text": "You should try Reseek. It's an AI second brain that handles semantic search across notes, PDFs, and web content in one place. It might be a good alternative if you want a unified system",
          "score": -1,
          "created_utc": "2026-02-07 01:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40r0u3",
              "author": "Low_Acanthisitta7686",
              "text": "Checked it out, but it doesn‚Äôt mention anywhere what scale it can work with, and that‚Äôs super important.",
              "score": 1,
              "created_utc": "2026-02-07 03:18:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvtrw7",
      "title": "If RAG is dead, what will replace it?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qvtrw7/if_rag_is_dead_what_will_replace_it/",
      "author": "Normal_Sun_8169",
      "created_utc": "2026-02-04 16:50:23",
      "score": 139,
      "num_comments": 101,
      "upvote_ratio": 0.86,
      "text": "It seems like everyone who uses RAG eventually gets frustrated with it. You end up with either poor results from semantic search or complex data pipelines.\n\nAlso - searching for knowledge is only part of the problem for agents. I‚Äôve seen some articles and posts on X, Medium, Reddit, etc about agent memory and in a lot of ways it seems like that‚Äôs the natural evolution of RAG. You treat knowledge as a form of semantic memory and one piece of a bigger set of memory requirements.¬†\n\nThere was a paper published from Google late last year about self-evolving agents and another one talking about adaptive agents.\n\nIf you had a good solution to memory, it seems like you could get to the point where these ideas come together and you could use a combination of knowledge, episodic memory, user feedback, etc to make agents actually learn.\n\nSeems like that could be the future for solving agent data. Anyone tried to do this?¬†",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qvtrw7/if_rag_is_dead_what_will_replace_it/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3kg4v0",
          "author": "qa_anaaq",
          "text": "RAG isn‚Äôt dead. It‚Äôs perfectly fine and just needs to be used well. Everyone believes context graphs are the next trillion dollar industry. Context graph management at runtime is another flavor of RAG. \n\nRemember that RAG isn‚Äôt a narrow term. If something is pulled from somewhere to augment generation, it‚Äôs RAG.",
          "score": 164,
          "created_utc": "2026-02-04 17:48:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3luvt1",
              "author": "isthatashark",
              "text": "The challenge with the name \"RAG\" is that so many people use it as a shorthand to describe semantic search over chunked documents in a vector database. I think the days where you can built any sort of meaningful AI application with that approach are behind us.\n\nAs a pattern, retrieving context and using it to augment the LLM's generation is here to stay.",
              "score": 25,
              "created_utc": "2026-02-04 21:44:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3obe76",
                  "author": "FiddlyDink",
                  "text": "What is replacing chunked documents in a vector database for semantic search?",
                  "score": 11,
                  "created_utc": "2026-02-05 06:33:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mxhx9",
              "author": "FoldedKatana",
              "text": "Yeah I'm using graph rag for a client and it works great if the data is static.",
              "score": 5,
              "created_utc": "2026-02-05 01:10:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3owpwq",
                  "author": "3minpc",
                  "text": "Why static? You can't rebuild your graph every x hours?",
                  "score": 1,
                  "created_utc": "2026-02-05 09:53:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l8dww",
              "author": "valuat",
              "text": "Everyone who?",
              "score": 5,
              "created_utc": "2026-02-04 19:56:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3na7fb",
              "author": "SUCK_MY_DICTIONARY",
              "text": "Oh I love the way this guy fucking thinks YES.\n\nWhat is your opinion on MoE? I want to know",
              "score": 1,
              "created_utc": "2026-02-05 02:23:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3otgqt",
              "author": "_nku",
              "text": "Agree.  I think we haven't reached the necessary maturity in best practice guidance in regards to when which kind and strategy of grounding information injection into the final completion run of an LLM is the right approach.  What mix of forced grounding context injection, dynamic tool calling to get grounding context, sub-agents summarizing from larger bodies to then inject summarized / shortened grounding context etc gives the best bang for the buck?   TBH from production experience, it's still very situation and data specific.  For a fast follower team that does not have the capacity to try out every frontline technology development, an off the shelf (e.g. GCP APIs based) vector store plus reranker plus grounding into a fast model with large context is still going to be a decent outcome IF (!!!) the use case is actually a fit for it and IF (!!) they put a lot of effort in the preparation, custom chunking, extraction, tagging of their content.     \n  \nMy unscientific thesis is that \"standard\" RAG setups are used way to much on a) bad data b) the wrong use case,  not that they are fundamentally bad.    \n  \nAnother thesis: The general approach of rather providing situationally dynamic context vs. relying on foundation model fact knowledge is here to stay until we have models that can be incrementally and continuously trained or tuned at very low cost (and even then, the question is up whether this provides better hallucination control than grounding in the generation context).",
              "score": 1,
              "created_utc": "2026-02-05 09:21:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ks60y",
              "author": "howardhus",
              "text": "rag was never even alive.\n\nRag ist pulling chunks in a half assed vector search and letting some llm hallucinate some coherent sentence from it. The selling point was the LLM faking confidence. Worked just like in the real world..\n\nwas never great in theory but peopel were flashed as they saw some very self confident human readable answer \"here is the perfect answer to your question!\"\n\nthen you correct it: \"yes you are right! i lied! here is the actual correct answer (this time for real!)\"\n\nRAG was only great before the word \"hallucination\" also became a thing.",
              "score": -18,
              "created_utc": "2026-02-04 18:42:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l0y0o",
                  "author": "Agreeable-Market-692",
                  "text": "This is a very outdated view of RAG. Hundreds of papers and dozens of models later and things are much improved.",
                  "score": 12,
                  "created_utc": "2026-02-04 19:22:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lml96",
              "author": "Dense_Gate_5193",
              "text": "yeah RAG is neat but Graph-RAG is where it is at.\n\nit‚Äôs why i built nornic. \n\nhttps://github.com/orneryd/NornicDB\n\n0.17ms p95 transacted writes. \n\nneo4j drop-in replacement that‚Äôs 3-50x faster depending on operation.\n\nit also has a qdrant compatible grpc endpoint and is ~40% faster than qdrant proper\n\ngpu accelerated vector embedding search or cpu IVF-HNSW, tunable. \n\nmanaged vector embeddings mean you don‚Äôt need a remote model to generate embeddings for you. same for reranking. it runs an in-memory model for reranking.",
              "score": -11,
              "created_utc": "2026-02-04 21:05:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kfb2f",
          "author": "coffee-praxis",
          "text": "Agent memory alone doesn‚Äôt cut it. Let‚Äôs say you want grounded facts from a document source that‚Äôs too big for context window. You can‚Äôt just shove it all in ‚Äúagent memory‚Äù unless you retrieve the correct bits of it somehow. Now you‚Äôre back to RAG.",
          "score": 38,
          "created_utc": "2026-02-04 17:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ltfuv",
              "author": "isthatashark",
              "text": "I hear more people talking about this as semantic memory and thinking of it as one requirement in a bigger set of agent memory requirements rather than just RAG.",
              "score": 3,
              "created_utc": "2026-02-04 21:37:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3kkqao",
              "author": "NorCalZen",
              "text": "Sorry if this a naive question, but could you use a database solution like ScyllaDB to achieve the right results ?",
              "score": 0,
              "created_utc": "2026-02-04 18:08:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3klea2",
                  "author": "coffee-praxis",
                  "text": "RAG is ‚Äú**retrieval** augmented generation‚Äù. Any DB qualifies.",
                  "score": 24,
                  "created_utc": "2026-02-04 18:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k9bq3",
          "author": "ethan000024",
          "text": "I‚Äôve been hearing more about agent learning lately too. Agree it‚Äôs a promising idea but also mostly hype when I‚Äôve tried to dig into it. The two most interesting projects I‚Äôve seen on this lately are Agent Lightning and Hindsight. Two very different approaches, Agent Lightning relies more on file system. Hindsight is closer to what you described with combining knowledge, episodic memory, etc. Both have learning aspects to it.¬†",
          "score": 13,
          "created_utc": "2026-02-04 17:16:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kgx9d",
              "author": "Normal_Sun_8169",
              "text": "I just looked those projects up. Very cool stuff. The learning demo they have on the GitHub repo for Hindsight is exactly what I was trying to describe. Reinforcement learning over agent memory to form mental models seems super powerful. Thanks for the info!",
              "score": 4,
              "created_utc": "2026-02-04 17:51:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kcja7",
          "author": "florinandrei",
          "text": "> If RAG is dead, what will replace it?\n\nTATTER\n\nTransformer-Attention Token Tangling for Eventually Rambling",
          "score": 24,
          "created_utc": "2026-02-04 17:31:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kielt",
              "author": "Floppy_Muppet",
              "text": "I believe \"token tangling\" is illegal in several states.",
              "score": 20,
              "created_utc": "2026-02-04 17:58:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k912z",
          "author": "Emma_4_7",
          "text": "The most annoying thing about agent memory right now is how many ‚Äúmemory‚Äù projects on GitHub are basic RAG solutions under the covers. That‚Äôs nice you can remember where I work after 10 whole messages.",
          "score": 19,
          "created_utc": "2026-02-04 17:15:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kgims",
              "author": "Normal_Sun_8169",
              "text": "Yeah, I‚Äôve noticed this too.",
              "score": 2,
              "created_utc": "2026-02-04 17:49:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3kig2c",
              "author": "Original_Finding2212",
              "text": "What do you think about this?\n\nQq folder here:\n\n[https://github.com/OriNachum/autonomous-intelligence](https://github.com/OriNachum/autonomous-intelligence)\n\nAnd add a star if you like or want to support üôèüèø\n\nhttps://preview.redd.it/r8euxdeboihg1.jpeg?width=2752&format=pjpg&auto=webp&s=050a9da330c4b9c4c558d792e243f8703b05dbfe",
              "score": -15,
              "created_utc": "2026-02-04 17:58:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kvyj2",
                  "author": "leonjetski",
                  "text": "‚ÄúMapping sturucted outitites and complex relationships between a√¶√∞capta.‚Äù",
                  "score": 13,
                  "created_utc": "2026-02-04 18:59:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3tghgy",
                  "author": "cmndr_spanky",
                  "text": "That diagram is a pile of nonsense. It might be time to start thinking for yourself‚Ä¶ friend. Did you even read it ?",
                  "score": 1,
                  "created_utc": "2026-02-06 00:40:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mv41a",
          "author": "jba1224a",
          "text": "‚ÄúLet me just shove this shit into a vector database.  We don‚Äôt need to worry about chunking.  What‚Äôs an embedding model?‚Äù\n\n‚Ä¶.\n\n‚ÄúWhy do my results suck.  RAG is frustrating‚Äù",
          "score": 6,
          "created_utc": "2026-02-05 00:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ozewa",
              "author": "CSEliot",
              "text": "RAG tools don't run any embedding by default???",
              "score": 1,
              "created_utc": "2026-02-05 10:18:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3q2542",
                  "author": "jba1224a",
                  "text": "Are you asking?\n\nRag isn‚Äôt only vector search but in the context of this discussion this is why it fails for people.\n\nThey equate it purely to vector search and then put zero planning or thought into how to curate their vector database.\n\nIt‚Äôs akin to baking a cake by just dumping all the ingredients into a pan with no measuring.  You may get something vaguely cake-like‚Ä¶but you shouldn‚Äôt be pissed it didn‚Äôt come out the way you wanted.",
                  "score": 3,
                  "created_utc": "2026-02-05 14:42:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3pjuup",
                  "author": "vogut",
                  "text": "?",
                  "score": 1,
                  "created_utc": "2026-02-05 12:59:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mtaif",
          "author": "Ok-Owl-7515",
          "text": "I don‚Äôt think RAG is dead. Vector-only semantic search is what usually disappoints. What‚Äôs replacing it (for me) is hybrid retrieval + memory architecture: FTS/keyword first, then vectors only as fallback, union + rerank, and always return retrieval diagnostics (which backend, hit counts, scores, latency).\n\nThe biggest unlock is in considering embeddings/indexes as versioned, reproducible derived artifacts (model/version + source hash), and controlling changes via a small golden set to prevent silent changes to results. Retrieval is just one ‚Äúmemory surface,‚Äù alongside structured state/ledgers and episodic logs.",
          "score": 4,
          "created_utc": "2026-02-05 00:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3syrh2",
              "author": "danigoncalves",
              "text": "What do you use for FTS? do you have your own implementation or use something like Apache Solr or similar that abstracts you from some of data ingestion processes? And why you use vector only as fallback and do not join both FTS/keyword with sematic search, merge and re-rank both to choose the best context to feed the models? ",
              "score": 2,
              "created_utc": "2026-02-05 23:00:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3t1fwk",
                  "author": "Ok-Owl-7515",
                  "text": "Good questions ‚Äì just a quick clarification on my wording. I‚Äôm currently using SQLite FTS5 (embedded) instead of Solr or Elasticsearch. It keeps retrieval portable, deterministic, and easy to debug with stable chunk/card IDs, source text hashes, and reproducible index builds.\n\nFor vectors, when I say ‚Äúfallback,‚Äù I mean I don‚Äôt always run semantic search. (a) It can add noise for queries that are heavy on entities, where lexical search performs better; and (b) it increases complexity and cost if used on every query. But when semantic does kick in, say, too few FTS hits or low lexical confidence,  I follow the exact flow you described: run vector search - merge results - rerank - return top-K. I also log diagnostics like backend used, hit counts, scores, and latency.\n\nThat said, I haven‚Äôt rolled out embeddings-based retrieval in production yet. The current setup is FTS-first, paired with structured state and ledgers. The hybrid approach is next on the roadmap, once I can safely gate it behind a ‚Äúsemantic miss‚Äù golden set to avoid silent drift.\n\nCurious, what‚Äôs worked best for you in terms of rerankers or thresholding?",
                  "score": 1,
                  "created_utc": "2026-02-05 23:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3yrkj6",
              "author": "engineerofsoftware",
              "text": "please don‚Äôt use vector search as a fallback.. it‚Äôs meant to be concatenated with fts‚Ä¶ you always want to do semantic search because fts will always have blindspots.",
              "score": 2,
              "created_utc": "2026-02-06 20:35:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ysw5l",
                  "author": "Ok-Owl-7515",
                  "text": "Yeah, fair. FTS definitely has its blind spots. When I say \"fallback,\" I don‚Äôt mean semantic is optional forever. It‚Äôs more that I‚Äôm not always willing to pay the cost or deal with the complexity of running it on every single query. In practice, when semantic does kick in ‚Äì lexical confidence is low, results are weak, or the query is clearly more abstract ‚Äì I do pretty much what you described. I run semantic alongside FTS, merge the candidates, then rerank.\n\nAlways-on semantic can work great if your infra can handle it and your domain benefits from it. But honestly, in more entity-heavy setups, I‚Äôve seen it add noise or make things harder to debug. I‚Äôve had better luck gating it behind a simple confidence check instead of making it the default.\n\nCurious what kind of domain you're working in. Are you seeing consistent gains from always merging and reranking, or are you using some kind of adaptive setup too?",
                  "score": 1,
                  "created_utc": "2026-02-06 20:42:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kuy2i",
          "author": "metaphorm",
          "text": "my view is that RAG is still a highly relevant technique and the problems it has with accuracy are the current leading edge of LLM application development. agent memory might be a good approach for some classes of problems. \"deep\" agents might be another approach that works, i.e. an agent that has access to tools that allow it to introspect its own results. ",
          "score": 7,
          "created_utc": "2026-02-04 18:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l78ss",
              "author": "techhead57",
              "text": "Its a tool in the toolbox. When LLMs came out rag was the only tool. Now there are all kinds of interfaces being hooked up to them and RAG has all kinds of fancy alternatives that are basically trying to do the same thing but better. And models are getting better at using this kind of input context because theyre being trained with tools use now.",
              "score": 4,
              "created_utc": "2026-02-04 19:51:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k3xuy",
          "author": "Fragrant_Western4730",
          "text": "I don‚Äôt know about the rest of it, but I definitely experienced the shortcomings of RAG for searching documents. Cool thought. Interested to hear what people think about this. Upvoted.",
          "score": 2,
          "created_utc": "2026-02-04 16:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kg1c3",
              "author": "Normal_Sun_8169",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-04 17:47:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kerpl",
          "author": "onetimeiateaburrito",
          "text": "I dunno man. I've spent a little bit trying to get a [RAPTOR](https://arxiv.org/abs/2401.18059) style system going and maybe it'll be cool? Who knows. I'm not a programmer and have no background in CS or ML. Just arguing with myself and Claude until something does something without spitting error codes. Then doing the same thing to see what's silently failing.",
          "score": 2,
          "created_utc": "2026-02-04 17:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l5qqf",
          "author": "WolfeheartGames",
          "text": "The problem is retrieval. How is the agent supposed to know what I'd available for lookup? It must be told.\n\nLet's say we have a list of things the agent can retrieve. If we give it to the agent it will hyper fixate on this and it causes new failure modes.\n\nSo then we need to monitor the inputs and outputs and see if we should be injecting information from retrieval in to the context window. This requires a signal of some kind. Either LLM, BERT, or otherwise.",
          "score": 2,
          "created_utc": "2026-02-04 19:44:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mgzqd",
              "author": "ai-tacocat-ia",
              "text": "It's really just a taxonomy problem. Is easy to think of it like a file system. \"Tell me what folders are in the current directory. I want to see the files and subfolders in this list of directories. Now show me what's in these subdirs.\"\n\nAlso, \"show me the paths of files whose contents contain these search terms\". Then let the LLM list the files it wants to pull.\n\nObviously doesn't need to be files - can be categories, subcategories, filter by tags, etc. Basically, give LLMs the same tools you enjoy as a human to find things.",
              "score": 1,
              "created_utc": "2026-02-04 23:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mnmdb",
                  "author": "WolfeheartGames",
                  "text": "That is not how real deployments usually work. It's okay for like a call center bot where the company will invest a lot in the docs for a RAG, but even then it's not enough. How does it know that a question is even contained in its RAG? How does it know how to search for it if the user gives terrible keywords, how does it know if should look elsewhere? It's not a listable directory to explore to gain insight from, and that's the problem. The agent only knows whats in it's system prompt until it's found something, and then it's still ignorant about potentially other useful things it didn't find. This breaks down further when data is less organized, like code or loose pdfs\n\nBut the fact that you're comparing RAG lookup to a directory is concerning. Vector and graph databases do not work like that at all. The problem of retrieval is partially because they don't work like that.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3liewe",
          "author": "DataCentricExpert",
          "text": "RAG isn‚Äôt dead, it‚Äôs just being asked to do too much.  \ngents break when you expect retrieval to behave like memory. What replaces it isn‚Äôt ‚Äúbetter RAG,‚Äù it‚Äôs layered memory...AG becomes infrastructure, not the strategy.",
          "score": 2,
          "created_utc": "2026-02-04 20:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lmpd1",
          "author": "xFloaty",
          "text": "every time your agent calls a tool to search for context, it‚Äôs RAG",
          "score": 2,
          "created_utc": "2026-02-04 21:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nbxi9",
          "author": "hettuklaeddi",
          "text": "dead?!? RAG doesn‚Äôt even have the sniffles \n\nmaybe it‚Äôs dead to script kiddies, that‚Äôs fine",
          "score": 2,
          "created_utc": "2026-02-05 02:33:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47ncff",
          "author": "HealthyCommunicat",
          "text": "RAG is super useful for turning dumber models into something useful by just having that pipeline of example data to use, so no, RAG is not dead and most likely will not be dead until some newer form of being able to link data to a model thats much more easier and more efficient. Just 2 weeks ago I had a client project using a 30b model as a base but being able to do so much specific jobs for the client specifically because of all the Q&A and all the massive amount of instructions and info specific only to this company.",
          "score": 2,
          "created_utc": "2026-02-08 06:28:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kz4oq",
          "author": "fabkosta",
          "text": "Downvoted. We had enough \"RAG is dead\" posts here. It's getting silly.",
          "score": 4,
          "created_utc": "2026-02-04 19:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l6m4k",
          "author": "AdOwn10",
          "text": "Ya the RAG people changed what ‚ÄúRAG‚Äù means so RAG isn‚Äôt dead. Vector database? No! We are not talking about ALL ways you get retrieve information to augment a context window.",
          "score": 2,
          "created_utc": "2026-02-04 19:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mc77z",
          "author": "andrew_kirfman",
          "text": "Rag isn‚Äôt 100% dead, but it‚Äôs definitely been impacted by agentic search and agent skills getting so good.  \n\nI only use semantic search for dart at a dartboard type searches.  Everything else is agentic search.",
          "score": 2,
          "created_utc": "2026-02-04 23:12:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3obzoo",
              "author": "Visionexe",
              "text": "What is Agentic search?",
              "score": 1,
              "created_utc": "2026-02-05 06:38:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3klpky",
          "author": "vagobond45",
          "text": "Knowledge Graphs combined with Answer Rag Audit should replace RAG",
          "score": 1,
          "created_utc": "2026-02-04 18:13:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l5cw1",
          "author": "Miclivs",
          "text": "Agentic search works really well when the agent knows what to look for.",
          "score": 1,
          "created_utc": "2026-02-04 19:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9lk3",
          "author": "llOriginalityLack367",
          "text": "Mean pooling.\n\nMean pooling.\n\nMean pooling.",
          "score": 1,
          "created_utc": "2026-02-04 20:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mc5ln",
          "author": "Flat_Dependent3195",
          "text": "Can you share the link for the paper you mentioned?",
          "score": 1,
          "created_utc": "2026-02-04 23:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n8d42",
          "author": "New-Unit-3900",
          "text": "Properly structured ontologies",
          "score": 1,
          "created_utc": "2026-02-05 02:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nrac8",
              "author": "smm_h",
              "text": "like what",
              "score": 1,
              "created_utc": "2026-02-05 04:05:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nhjmt",
          "author": "GoodEnoughSetup",
          "text": " In my experience, database solutions like ScyllaDB can definitely be part of a broader strategy to replace RAG. By incorporating a database for fast access to relevant data, you might enhance the context in which generative models operate, similar to how semantic memory aims to streamline information retrieval. Have you looked into any specific frameworks that could mesh well with that approach?",
          "score": 1,
          "created_utc": "2026-02-05 03:04:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o8gid",
          "author": "fooz42",
          "text": "It's a garbage in, garbage out problem. You can reduce the surface area of the generation to something very small in scope, or you can increase the quality of the included information in the context to improve the summary.",
          "score": 1,
          "created_utc": "2026-02-05 06:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3objra",
          "author": "iAM_A_NiceGuy",
          "text": "Compression",
          "score": 1,
          "created_utc": "2026-02-05 06:34:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3p6ujg",
          "author": "sje397",
          "text": "I've got RAG, 'sticky' memories scoped as global or conversation specific, and 'notes' as a tool. Each suits different use cases. Seems to work pretty well in combination for my 'assistant'.",
          "score": 1,
          "created_utc": "2026-02-05 11:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q8flh",
          "author": "airylizard",
          "text": "‚ÄúRAG‚Äù is semantic search. You ‚ÄúAI people‚Äù have been inventing new terms to describe basic automation tools and practices for years",
          "score": 1,
          "created_utc": "2026-02-05 15:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q8lgv",
          "author": "exids",
          "text": "RAG is awesome, not dead and is still in its infancy as agent models improve. Who says it's dead?!?!?",
          "score": 1,
          "created_utc": "2026-02-05 15:15:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tdhbc",
          "author": "Former-Ad-5757",
          "text": "Stupid click-once RAGging (in the meaning of simple semantic searching) is dead but to me it has never really existed.\n\nIf you setup a default vector db with chunking of 200, and you feed it documents of on average 600, what do you really suspect will happen? At best it will feed half-truncated garbage to the llm.\n\n  \nIn all RAG setups I have setup the absolute minimal chunking was 64kb, because I don't believe chunking is a fixed number, it is completely dependent on if the chunk completely describes the info, you can define info as a sentence, or a paragraph (or for coding for example a method) but I have almost never encountered a situation where all the meaning was captured in 200. Just use overlaps is what some tuts say, well great now you add more half-meanings which pollute your retrieval results more.",
          "score": 1,
          "created_utc": "2026-02-06 00:22:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3th44g",
          "author": "cmndr_spanky",
          "text": "Oh look. It‚Äôs the daily ‚Äúrag is dead‚Äù bot post.  Oh look here‚Äôs a fancy memory solution for agents (still an adaptation of rag). \n\nWould you mind thinking more deeply (or maybe search Reddit for 15secs) before vomiting out the next hapless low effort contribution to the cesspool of AI subreddits ? K thanks",
          "score": 1,
          "created_utc": "2026-02-06 00:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ttwqq",
          "author": "Academic_Track_2765",
          "text": "It‚Äôs dead, it dies everyday according to some guru. There are so many flavors of rag but somehow it‚Äôs still dead lol.",
          "score": 1,
          "created_utc": "2026-02-06 01:59:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ub68t",
          "author": "TenshiS",
          "text": "RAG is far from dead",
          "score": 1,
          "created_utc": "2026-02-06 03:43:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v0mz0",
          "author": "OkFly3388",
          "text": "Most \"memory\" systems for llm agents is actually rag. So it dont dead, it just replaced with more fancy word.",
          "score": 1,
          "created_utc": "2026-02-06 06:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o419bcl",
          "author": "Analytics-Maken",
          "text": "Naive vectoronly RAG over chunked documents fails to scale as agent memory, producing poor retrieval for complex queries and lacking structure for structured knowledge.‚Äã It happens because embeddings capture semantics but ignore relational structure, metadata, and versioning.\n\nThe fix uses hybrid retrieval FTS/keyword first, then vectors as fallback, merged and reranked with embeddings as versioned artifacts (tied to source hash and model version) to avoid silent drift; layer in structured state from warehouses for granularity and joins, plus episodic logs for agent feedback loops.\n\nThis creates memory surfaces for agents to query without overload. Windsor.ai pipelines normalize data into BigQuery/Snowflake/PostgreSQL, handling schema drift automatically, then expose them via Windsor MCP as tools in Claude/ChatGPT for semantic vs. structured memory access.",
          "score": 1,
          "created_utc": "2026-02-07 05:28:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43falv",
          "author": "Fresh_Sock8660",
          "text": "Retrieval augmentation isn't going away anytime soon. Maybe you're thinking of a specific application.",
          "score": 1,
          "created_utc": "2026-02-07 15:40:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45avpx",
          "author": "satechguy",
          "text": "RAG is dead, again?",
          "score": 1,
          "created_utc": "2026-02-07 21:23:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45bm3t",
          "author": "MissJoannaTooU",
          "text": "RAG is very much alive. We will always need retrieval.",
          "score": 1,
          "created_utc": "2026-02-07 21:26:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o497s6u",
          "author": "Competitive-Ad-5081",
          "text": "Using RAG is not simply about creating chunks and storing them in a vector database. This must be accompanied by a solid retrieval strategy. For example, you can provide your assistant with a tool that allows it to perform two types of queries to your knowledge base:\n\n1. A general query that retrieves only the names (or titles) of documents that have the highest semantic similarity to the user‚Äôs request.\n\n2. If the user shows interest in any of those documents, a second type of query should allow the AI assistant to filter semantic searches exclusively to the document name the user is interested in.\n\nJust having these two types of queries already makes a significant difference in the quality and control of the retrieval",
          "score": 1,
          "created_utc": "2026-02-08 14:22:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49nrmg",
          "author": "Ok_Bedroom_5088",
          "text": "Don't waste your time bothering with that San Francisco shit talk.",
          "score": 1,
          "created_utc": "2026-02-08 15:48:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l4jqo",
          "author": "Able_Penalty8856",
          "text": "I also got frustrated with RAG. My plan is to study Unsloth to explore fine-tuned models. I'm aware that I'll likely face several challenges.",
          "score": 0,
          "created_utc": "2026-02-04 19:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oeyq1",
              "author": "Pixelmixer",
              "text": "This simply isn‚Äôt possible for a lot of workflows. As a super simple toy example; imagine you want to search text comments posted by users and provide that to an LLM. Fine-tuning could potentially work as a first pass (let‚Äôs also assume that the fine-tuned model has perfect retrieval for the purpose of this example), but even then you‚Äôd need to retrain it each time a user posts a new comment or changes their comment. It‚Äôs just too much, unfortunately.",
              "score": 1,
              "created_utc": "2026-02-05 07:04:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quxgq9",
      "title": "8 Ways OpenClaw Reduces Context Loss in Long-Running Agents",
      "subreddit": "LLMDevs",
      "url": "https://codepointer.substack.com/p/openclaw-stop-losing-context-8-techniques",
      "author": "noninertialframe96",
      "created_utc": "2026-02-03 17:01:02",
      "score": 18,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1quxgq9/8_ways_openclaw_reduces_context_loss_in/",
      "domain": "codepointer.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3hsmig",
          "author": "gtek_engineer66",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-02-04 07:45:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r04wed",
      "title": "Observations From Using GPT-5.3 Codex and Claude Opus 4.6",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "author": "Arindam_200",
      "created_utc": "2026-02-09 13:58:50",
      "score": 16,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "I tested GPT-5.3 Codex and Claude Opus 4.6 shortly after release to see what actually happens once you stop prompting and start expecting results. Benchmarks are easy to read. Real execution is harder to fake.\n\nBoth models were given the same prompts and left alone to work. The difference showed up fast.\n\nCodex doesn‚Äôt hesitate. It commits early, makes reasonable calls on its own, and keeps moving until something usable exists. You don‚Äôt feel like you‚Äôre co-writing every step. You kick it off, check back, and review what came out. That‚Äôs convenient, but it also means you sometimes get decisions you didn‚Äôt explicitly ask for.\n\nOpus behaves almost the opposite way. It slows things down, checks its own reasoning, and tries to keep everything internally tidy. That extra caution shows up in the output. Things line up better, explanations make more sense, and fewer surprises appear at the end. The tradeoff is time.\n\nA few things stood out pretty clearly:\n\n* Codex optimizes for momentum, not elegance\n* Opus optimizes for coherence, not speed\n* Codex assumes you‚Äôll iterate anyway\n* Opus assumes you care about getting it right the first time\n\nThe interaction style changes because of that. Codex feels closer to delegating work. Opus feels closer to collaborating on it.\n\nNeither model felt ‚Äúsmarter‚Äù than the other. They just burn time in different places. Codex burns it after delivery. Opus burns it before.\n\nIf you care about moving fast and fixing things later, Codex fits that mindset. If you care about clean reasoning and fewer corrections, Opus makes more sense.\n\nI wrote a longer breakdown [here](https://www.tensorlake.ai/blog/claude-opus-4-6-vs-gpt-5-3-codex) with screenshots and timing details in the full post for anyone who wants the deeper context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4fmz12",
          "author": "swarmed100",
          "text": "Opus 4.6 reasons a lot longer than opus 4.5. One negative side I noticed from this is that it is \"better\" at finding delusional logic to explain why a set of facts that are clearly impossible \"make sense\", instead of concluding that some of the assumptions or inputs must be wrong since the set of facts are just impossible together.",
          "score": 4,
          "created_utc": "2026-02-09 14:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gbcxu",
              "author": "External-Yak-371",
              "text": "As a pro plan user I agree, but it also means my piddly allowance can nearly be consumed in one good planning session",
              "score": 3,
              "created_utc": "2026-02-09 16:10:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hrtb3",
                  "author": "Manfluencer10kultra",
                  "text": "u/External-Yak-371 One git commit request for small refactors in many files (similar refactors) was enough for 48%.  It did notice I missed 5 items that needed to be refactored then used like 20k tokens to fix it, and then it was absolutely perfect.\n\nToo bad it was 50% of my 5h allowance (you get about 9 x 5h on pro at 11% of weekly ...).  In that sense, it was absolutely worthless spending my tokens on it.  \nBut what if I used Sonnet for it? it would have been maybe worse.  \nAnd these are the things that you don't want to do yourself, and want to hand over to AI. You close off your session after a long day, forget to commit all those refactors, but still want a sensible commit instead of \"lots of fixes\".  \nEh this is where AI tooling should come in to save the day, but nope..",
                  "score": 1,
                  "created_utc": "2026-02-09 20:21:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gcjpr",
              "author": "cmndr_spanky",
              "text": "This is pretty worrying but makes sense. More reasoning doesn‚Äôt mean better results and often just causes useless ‚Äúthought loops‚Äù that at best just wastes more open credits, at worst fills up context causing it to loose touch with the original request details.\n\nThat said, I‚Äôve never been impressed with any of openAI‚Äôs models as coding agents, so I‚Äôd suspect opus is still better despite the flaws. We‚Äôll see I guess",
              "score": 2,
              "created_utc": "2026-02-09 16:15:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gfqe5",
          "author": "kubrador",
          "text": "so basically one model is a startup founder and the other is an engineering lead pretending to care about code review",
          "score": 2,
          "created_utc": "2026-02-09 16:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hrdaq",
          "author": "Manfluencer10kultra",
          "text": "Bruh, I switched to Codex (Free) and getting incredible usage just on Free and GPT-5.2-Codex High, not even 5.3 and it's just night and day.  Claude put me in a depressive mood, and now I'm back enjoying engineering again.",
          "score": 2,
          "created_utc": "2026-02-09 20:19:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxa658",
      "title": "Built a Website Crawler + RAG (fixed it last night üòÖ)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qxa658/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:28:18",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff üíª), I thought:  \nEveryone is feeding PDFs‚Ä¶ **why not try something that‚Äôs not PDF ingestion?**\n\nSo I focused on fixing the **real problem ‚Äî crawling quality**.\n\nüîó GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What‚Äôs better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well üëÄ  \nFeedback, suggestions, and ‚≠ês are welcome!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qxa658/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o42m8nd",
          "author": "dezastrologu",
          "text": "AI slop post\n\nBut the tool sounds cool. Vibe coded?",
          "score": 1,
          "created_utc": "2026-02-07 12:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o445f1w",
              "author": "Cod3Conjurer",
              "text": "Kinda but not really¬†\nSome rough edges for sure, but the crawler logic and pipeline were intentionally built. Still learning and iterating.¬†",
              "score": 2,
              "created_utc": "2026-02-07 17:48:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxbr2z",
      "title": "today's task",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/ggk43gavzthg1.jpeg",
      "author": "carsa81",
      "created_utc": "2026-02-06 08:02:13",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qxbr2z/todays_task/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3vayfa",
          "author": "West_Struggle2530",
          "text": "Opus 4.6",
          "score": 6,
          "created_utc": "2026-02-06 08:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o410o0h",
          "author": "mosquitosarenotcool",
          "text": "Kimi 2.5",
          "score": 1,
          "created_utc": "2026-02-07 04:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ziioq",
          "author": "Clean_Moose6832",
          "text": "Codex 5.3 is superior. ",
          "score": 0,
          "created_utc": "2026-02-06 22:52:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdhwq",
      "title": "I made a Transformer 3x faster by making 75% of tokens \"lazy\". It beats the standard baseline on loss in fixed-time training.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qwdhwq/i_made_a_transformer_3x_faster_by_making_75_of/",
      "author": "Morph2026",
      "created_utc": "2026-02-05 06:09:15",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI don't have the compute to train on 100B tokens or write a formal paper right now, so I'm dropping this here for the community to play with.\n\n**The Idea: \"WorkerTransformer\"**\n\nStandard Transformers are inefficient because every single token performs expensive Attention ($O(T^(2)$)) and FFN ($O(T)$) updates. But does every token really need to \"think\" deeply?\n\nI built a sparse-update architecture based on a simple intuition:\n\n1. **Divide & Conquer**: Split tokens into **Workers** (every 4th token) and **Memory** (the rest).\n2. **Workers**: Do the heavy lifting (Full Attention + FFN).\n3. **Memory**: Only do a cheap depthwise conv1d to capture local context (like Mamba/ConvNets) but **skip** the heavy Transformer block.\n4. **In-place Update**: Everything happens in-place. No extra tokens added, no sequence inflation.\n\n**The Result (on T=1024 sequence length):**\n\nI ran a \"battle\" between a Standard Transformer and my WorkerTransformer (same params, layers, dim) on a fixed 5-minute training budget.\n\n* **Standard Transformer**: 3.2 steps/s | Reached Val Loss **1.44**\n* **WorkerTransformer**: 8.0 steps/s (**2.5x speedup**) | Reached Val Loss **1.30**\n\nThe \"lazy\" model didn't just run faster; because it ran 2.5x more steps in the same timeframe, it actually learned **more** and achieved a significantly lower loss. It seems the trade-off of \"sparse compute vs. more iterations\" heavily favors sparse compute here.\n\n**Why I'm sharing this:**\n\nI suspect this could scale. The architecture is **pure PyTorch** (no custom CUDA kernels needed), making it dead simple to modify.\n\nIf you have spare A100s or are looking for a weekend project, I'd love to see someone scale this up to WikiText-103 or RedPajama.\n\n**Code & Benchmarks:** [https://github.com/SuiltaPico/WorkerTransformer](https://github.com/SuiltaPico/WorkerTransformer)\n\nLet me know if you find any flaws in my reasoning or if you manage to break it!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwdhwq/i_made_a_transformer_3x_faster_by_making_75_of/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o44tfs6",
          "author": "techperson1234",
          "text": "Huh. Super interesting!",
          "score": 1,
          "created_utc": "2026-02-07 19:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47mb91",
          "author": "Dangerous-Sale3243",
          "text": "Interesting. I dont know enough to comment technically, since i am mostly just a user than a creator. The idea seems obvious, or at least an obvious next step facing a problem of optimizing runtime. So my assumption is that this was considered and rejected by a bunch of smart guys at some point in the past few years, for reasons beyond my knowledge.",
          "score": 1,
          "created_utc": "2026-02-08 06:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dsf7h",
          "author": "Foreign_Skill_6628",
          "text": "I think you need to do a lot more testing. Do this on some Wikipedia/Shakespeare datasets, and use multiple validation samples using different sampling methodologies. Prove it isn‚Äôt a fluke.\n\nYou also need to do holdout testing (benchmarking). How well does it generalize on unseen data compared to traditional transformers?",
          "score": 1,
          "created_utc": "2026-02-09 04:50:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qviqlh",
      "title": "How to become an AI Engineer in 2026 - what actually matters now?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qviqlh/how_to_become_an_ai_engineer_in_2026_what/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-04 08:05:16",
      "score": 11,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Trying to map out a realistic path into AI engineering and getting overwhelmed by contradictory advice.\n\nPython is still non-negotiable, but the \"just build a chatbot\" project approach doesn't cut it anymore. The market looks brutal for entry-level while senior roles are paying crazy money. Prompt engineering as a dedicated job seems dead, but the skill still matters. RAG, agentic AI, and MLOps seem to be where the growth is.\n\nThe part confusing me is traditional ML (sklearn, training models) vs pure LLM/API integration. Some say you need fundamentals, others say most jobs are just orchestrating existing models. With tools like Claude Code changing what coding even means, I'm not sure what skills are actually durable.\n\nFor people who've done this or are hiring:\n\n- What actually separated you from other candidates when you got in?\n- How much traditional ML do you use day-to-day vs LLM orchestration?\n- Best resources that actually helped you, not just ones you heard were good?\n- What does this role even look like in 2027 when agents do more of the work?\n\nNot looking for a generic roadmap. Looking for what's actually working right now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qviqlh/how_to_become_an_ai_engineer_in_2026_what/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3kxegl",
          "author": "Number4extraDip",
          "text": "What matters. Solving a specific problem. Being very specific and not just doing what everyone else is doing",
          "score": 7,
          "created_utc": "2026-02-04 19:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mrjp9",
              "author": "bbahner",
              "text": "Can you be more specific? <grin>",
              "score": 5,
              "created_utc": "2026-02-05 00:36:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nebra",
                  "author": "Number4extraDip",
                  "text": "Make something you wish existed. Make a product for yourself that no one is selling.",
                  "score": 2,
                  "created_utc": "2026-02-05 02:46:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lcedg",
          "author": "hrishikamath",
          "text": "Honestly most roles I have interviewed for have had AI in requirements but interviews were SWE stuff: system design, leetcode style and so on. Most but not all. During interviews I did speak about my projects that‚Äôs about it and some questions here and there. Yeah it‚Äôs more of just building agents for a lot of them. Traditional ML stuff is required by certain niche  companies. Certain companies randomly add its good to have fine tuning experience. But, yeah some companies develop their models for that you need solid fundamentals from ground up.",
          "score": 4,
          "created_utc": "2026-02-04 20:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lelk4",
          "author": "Canadianingermany",
          "text": ">Some say you need fundamentals, others say most jobs are just orchestrating existing models.  \n\n\nMost things that people are doing today are probably quite easy, and many are working on small problems that can probably be solved with some API and some prompt engineering.  \n\n  \nBut I'm not so convinced that in the future people will want to pay a full fledged DS wage for that because the barriers to entry are simply quite low.  \n\n  \nSo strategically I would concentrate on harder problems that need more than throw an LLM at it.  \n\n  \nBut what do I know?  I hire devs, I'm not one.\n\n>  \nI'm not sure what skills are actually durable.\n\nAt the end of the day.  The ability to solve problems and not be locked in to the solution that worked last time, but find the one for this problem.\n\n  \n",
          "score": 2,
          "created_utc": "2026-02-04 20:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqzgg",
              "author": "hrishikamath",
              "text": "Not really building good rag systems or tasks that require lot of context requires good understanding and skills",
              "score": 1,
              "created_utc": "2026-02-05 04:03:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oktgf",
          "author": "MediumShoddy5264",
          "text": "ML is not useful right now, you need to understand tool calling, context management, planning, evals, etc... ",
          "score": 2,
          "created_utc": "2026-02-05 07:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pui56",
              "author": "MullingMulianto",
              "text": "can you elaborate what you mean by ML. do you mean decision trees, classifiers, etc.?",
              "score": 1,
              "created_utc": "2026-02-05 14:01:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p6o4l",
          "author": "KegOfAppleJuice",
          "text": "A big emphasis is on cloud engineering, LLM evals and observability and creating quality data context for agents.",
          "score": 2,
          "created_utc": "2026-02-05 11:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qk2x4",
          "author": "Fragrant_Western4730",
          "text": "I've been working on Slack-based agents lately that need to handle open ended tasks from users. I'm pretty convinced agent memory is going to become a must-have skill for any AI engineer that wants to build agents that are more than just workflows or chatbots, especially as adaptive memory and agent learning keeps improving. I won my last two clients by putting my agent in Slack and calling it an AI employee and showing very rudimentary learning and memory.",
          "score": 2,
          "created_utc": "2026-02-05 16:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3quk4m",
              "author": "MAX7668",
              "text": "I guess I'm out of the loop. What do you mean by adaptive memory?",
              "score": 2,
              "created_utc": "2026-02-05 16:58:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3qv9b3",
                  "author": "Fragrant_Western4730",
                  "text": "It's this idea that your agents can have some sort of self-evolving mechanism to learn over time. A simple case would be user preferences, but the more interesting area is in automations and tool calling. Having agents learn what tools to call to solve tasks without needing to tune prompts over and over. I'm using Hindsight for this in the Slack agents I mentioned.",
                  "score": 2,
                  "created_utc": "2026-02-05 17:01:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwx950",
      "title": "Built an LLM agent for debugging production incidents - what we learned",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/y5lvd0tznqhg1",
      "author": "Useful-Process9033",
      "created_utc": "2026-02-05 20:53:02",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwx950/built_an_llm_agent_for_debugging_production/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3tj3cq",
          "author": "isthatashark",
          "text": "Cool project!",
          "score": 1,
          "created_utc": "2026-02-06 00:55:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3tsvjh",
              "author": "Useful-Process9033",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-06 01:53:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w22p3",
          "author": "Gerifico",
          "text": "Super interesting!",
          "score": 1,
          "created_utc": "2026-02-06 12:24:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ybgq9",
          "author": "jlebensold",
          "text": "This is a very cool idea! Have you tried connecting it to your github to access the codebase? ",
          "score": 1,
          "created_utc": "2026-02-06 19:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ybqck",
              "author": "Useful-Process9033",
              "text": "yes! it connects to github so that it has more context when debugging stuff",
              "score": 1,
              "created_utc": "2026-02-06 19:17:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3yc5bn",
                  "author": "jlebensold",
                  "text": "nice! I've been pretty blown away at the connection between SWE-Bench style tasks and what an agent can do with log data. We actually built an agent that will parse langfuse trace agents and identify cost savings using a similar technique. ",
                  "score": 1,
                  "created_utc": "2026-02-06 19:19:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwpdkz",
      "title": "ACE-Step 1.5: an on-device music model that beats Suno on common eval metrics",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/v0sqp0rh8phg1",
      "author": "MatchSuccessful1253",
      "created_utc": "2026-02-05 16:09:10",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwpdkz/acestep_15_an_ondevice_music_model_that_beats/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3shdm9",
          "author": "TheGoddessInari",
          "text": "Odd distinctive sound to everything on the site. ü§î",
          "score": 1,
          "created_utc": "2026-02-05 21:33:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz1adm",
      "title": "-68% model size, <0.4 pp accuracy loss: Compressed LLaMA-3.2-1B ‚Üí Q4_0 GGUF on SNIPS Dataset (CPU Inference)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1qz1adm",
      "author": "mr_ocotopus",
      "created_utc": "2026-02-08 06:10:48",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qz1adm/68_model_size_04_pp_accuracy_loss_compressed/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o47wcjn",
          "author": "Icy_Distribution_361",
          "text": "Looks interesting but can you say a bit more?",
          "score": 1,
          "created_utc": "2026-02-08 07:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47ym88",
              "author": "mr_ocotopus",
              "text": "Sure, it is basically a library to make your models smaller so they can perform a specific tasks at the same level as bigger models.   \nThe idea is to use these in on-device (OR) enterprise scale where you would want to process billions of records \\[save cost\\]  \nCheck out the repo for more : [https://github.com/chandan678/compressGPT](https://github.com/chandan678/compressGPT)\n\nBlog on how you can use it for on-device : [https://medium.com/@chandancjs/rethinking-on-device-llms-why-one-model-is-never-enough-3abccb4756bf](https://medium.com/@chandancjs/rethinking-on-device-llms-why-one-model-is-never-enough-3abccb4756bf)",
              "score": 1,
              "created_utc": "2026-02-08 08:11:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47z5co",
                  "author": "Icy_Distribution_361",
                  "text": "Ah, so it's not effectively the same ability model, it's same ability in specific tasks?",
                  "score": 1,
                  "created_utc": "2026-02-08 08:16:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4frssh",
          "author": "mr-KSA",
          "text": "This is truly valuable data. However, I feel that benchmarks and other metrics are unfortunately no longer providing anything beyond a general overview. The situation has become so¬†multifaceted¬†that it is becoming quite frustrating. To clarify: a 4B model specifically designed for translation can outperform an 80B model. Furthermore, the quality between different levels of¬†quantization¬†is, regrettably, inconsistent. I also suspect that many current models are being specifically¬†fine-tuned¬†to inflate these benchmark scores.\n\nIn my view,¬†empirical¬†experience is paramount. For instance, I have observed significant performance gaps between Q8 and Q4 quantization, particularly in MoE models. While a model like GPT-OSS 20B might be too 'clumsy' for my specific workflows, another user might prefer it over GPT-4. It ultimately depends on your specific use case. Because I utilize long, complex system prompts that require strict adherence to sequential instructions, models like GLM-4.7 or Granite 4 yield better results for me than Qwen 80B. For others, the opposite may be true.\n\nThe difference between Q4 and Q8 becomes especially¬†pronounced¬†in extended tasks where a structured 'flow' isn't utilized; a single logical error can lead to an¬†irreversible¬†divergence in the output. However, if a multi-model flow is implemented where each model is assigned a single task, Q4 is often sufficient. That said, I have encountered cases where a Qwen 30B (A3B) at Q8 provided answers that even a Qwen 80B at Q4 could not. I realize, of course, that many might disagree with this assessment.",
          "score": 1,
          "created_utc": "2026-02-09 14:31:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gtwta",
              "author": "mr_ocotopus",
              "text": "Hey, I agree to what you are saying.   \nAs LLM's get deployed on more and more systems there will be many \"right kind\" of ways to use it.   \nLike you have observed in your case it could be Q4 or Q8.   \nWhat compressGPT is trying to do is, provide a high level API to build the right kind of model that you would need. And if this particular flow does not workout you can itterate with other models/experiments faster - But since fine-tuning and quantising is a powerful tool it will be helpful to check weather a model passed down this pipeline will work or not. \n\nTLDR;   \nModels are \"correct model\" for a specific task, compressGPT offers one way of building \"right model\"",
              "score": 1,
              "created_utc": "2026-02-09 17:38:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qygq2b",
      "title": "We have developed a different architecture for LLM that does not work on transformers but works on the principles of reservoir computing and energy modelling. It remains constant on vRAM as we scale context unlike transformers.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/3830vd8tc3ig1.jpeg",
      "author": "Dry_Oil2597",
      "created_utc": "2026-02-07 15:30:45",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qygq2b/we_have_developed_a_different_architecture_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o43s4zy",
          "author": "HumanDrone8721",
          "text": "That's lovely, now publish it, open source it and let us train models based on it with opensource data sets, like the ones from Olmo series, and this way we can compare the efficiency and accuracy of the new method. I think I still have $200 on thinker.ai so I could do a test run for you if you publish the pipeline for training and inference.",
          "score": 10,
          "created_utc": "2026-02-07 16:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ezthp",
          "author": "Noddybear",
          "text": "What perplexity?",
          "score": 1,
          "created_utc": "2026-02-09 11:23:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f049r",
              "author": "Dry_Oil2597",
              "text": "For average over 10k msmarco docs its 18",
              "score": 1,
              "created_utc": "2026-02-09 11:25:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0lbvd",
      "title": "Mixture-of-Models routing beats single LLMs on SWE-Bench via task specialization",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "author": "botirkhaltaev",
      "created_utc": "2026-02-10 00:10:22",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been looking at per-task results on SWE-Bench Verified and noticed something that leaderboard averages hide: different models consistently solve¬†*different*¬†subsets of tasks.\n\nEven the top overall model on the leaderboard fails a non-trivial number of tasks that other models reliably solve, and the reverse is also true. This suggests strong task-level specialization rather than one model being strictly better.\n\nTo test this, I built a¬†**Mixture-of-Models architecture**, which is different from traditional routing that just defaults to the strongest aggregate model most of the time. The goal isn‚Äôt to route to a single model as often as possible, but to exploit complementary strengths between models.\n\nConcretely:\n\n* The problem description is embedded\n* It‚Äôs assigned to a semantic cluster (learned from general coding data, not SWE-Bench)\n* Each cluster has learned per-model success statistics\n* The task is routed to the historically strongest model for that¬†*type*¬†of problem\n\nImportantly, this does¬†**not**¬†route the top aggregate model for the majority of tasks. Several clusters consistently route to other models where they outperform it, even though it has the highest overall score.\n\nThere‚Äôs no new foundation model, no test-time search, and no repo execution, just a lightweight gating mechanism over multiple models.\n\nUsing this Mixture-of-Models setup, the system reaches 75.6% on SWE-Bench, exceeding single-model baselines (\\~74%). The takeaway isn‚Äôt the absolute number, but the mechanism: leaderboard aggregates hide complementary strengths, and mixture architectures can capture a higher ceiling than any single model.\n\nBlog with details and methodology here:¬†[https://nordlyslabs.com/blog/hypernova](https://nordlyslabs.com/blog/hypernova)\n\nGithub: the framework is open source !¬†[https://github.com/Nordlys-Labs/nordlys](https://github.com/Nordlys-Labs/nordlys)\n\nML/AI Research Community Discord:¬†[https://discord.gg/dqW7BBrq](https://discord.gg/dqW7BBrq)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qv6w28",
      "title": "I‚Äôm building an open-source local AI agent in Go that uses IR + tools instead of wasting tokens",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qv6w28/im_building_an_opensource_local_ai_agent_in_go/",
      "author": "iagomussel",
      "created_utc": "2026-02-03 22:46:21",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "Hey everyone,\n\nI‚Äôve been working on an open-source project called **IRon**: a local-first AI assistant focused on automation, not chat.\n\nThe main idea is:\n\nInstead of using LLMs to ‚Äúthink‚Äù and generate long text, IRon translates user input into a small structured format (IR ‚Äì Intermediate Representation) and executes real tools.\n\nSo most tasks don‚Äôt need heavy models.\n\n# What it does\n\nIRon works mainly through Telegram and runs locally.\n\nPipeline:\n\nUser ‚Üí Router ‚Üí (optional LLM) ‚Üí IR (JSON) ‚Üí Tools ‚Üí Result\n\nFeatures:\n\n* Deterministic router for common tasks (notes, lists, commands, etc.)\n* Dual output: short human reply + machine IR\n* Tool system (shell, docker, http, code exec, notes, scheduler, addons)\n* Cron-based scheduler\n* Codex/Ollama support for complex reasoning\n* Session isolation per chat\n* Addon system for external tools/adapters\n\n# Why I built it\n\nMost ‚ÄúAI assistants‚Äù today:\n\n* Burn tokens on simple things\n* Re-explain everything\n* Don‚Äôt integrate well with real systems\n* Lose context easily\n\nI wanted something closer to:\n\n‚ÄúNatural language ‚Üí compact instruction ‚Üí real execution‚Äù\n\nLike a mix of:\n\n* cron\n* Makefile\n* shell\n* and LLMs\n\nBut with safety and structure.\n\n# Example\n\nUser:  \n‚ÄúRemind me to pay rent tomorrow at 9‚Äù\n\nIRon:\n\n* Generates IR\n* Schedules cron\n* Uses scheduler tool\n* Confirms in one line\n\nNo long explanation. No wasted tokens.\n\n# Tech stack\n\n* Go\n* Telegram Bot API\n* Codex CLI / Ollama (future)\n* JSON-based IR\n* robfig/cron\n* Plugin system\n\nCurrent status\n\nIt‚Äôs usable and evolving.  \nMain focus now:\n\n* DSL for tasks\n* Better scheduling\n* Memory without huge context\n* More deterministic routing\n\n**It's in progress, so there are bugs yet, let me know if you can help.**\n\n# Repo\n\n[https://github.com/iagomussel/IRon](https://github.com/iagomussel/IRon?utm_source=chatgpt.com)\n\n# Looking for feedback\n\nI‚Äôm interested in feedback on:\n\n* Architecture\n* IR format\n* DSL ideas\n* Similar projects\n* Security concerns\n\nIf you‚Äôre into local AI, automation, or agent systems, I‚Äôd love your thoughts.\n\nThanks üôå",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qv6w28/im_building_an_opensource_local_ai_agent_in_go/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3g2vcj",
          "author": "DataCentricExpert",
          "text": "Do you have a sandboxed or local dev environment for testing IRon safely with real data, or is it purely the Telegram interface right now?",
          "score": 1,
          "created_utc": "2026-02-04 00:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6ivm",
          "author": "SeaworthinessThis598",
          "text": "I want to get. afeel about the IR concept efficacy can you show us. a demo maybe ?",
          "score": 1,
          "created_utc": "2026-02-04 01:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fite7",
          "author": "Otherwise_Wave9374",
          "text": "Love the ‚ÄúIR + tools‚Äù approach. That feels like a practical agent design: keep the LLM for the hard parsing/planning edges, but push everything into a constrained representation so execution stays predictable.\n\nHow are you thinking about schema evolution for the IR and safety around tool permissions (per chat/session)? Those two things seem to make or break local agents. Ive been reading a bunch about structured agents lately, this page has some good notes: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 23:04:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzi6ot",
      "title": "Moltbook - No Human Captcha allows only LLMs post",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/85cc0i50nbig1.png",
      "author": "hasmcp",
      "created_utc": "2026-02-08 19:25:50",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.64,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qzi6ot/moltbook_no_human_captcha_allows_only_llms_post/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4ckqbm",
          "author": "dezastrologu",
          "text": "Why do we still care about moltbook",
          "score": 1,
          "created_utc": "2026-02-09 00:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d8oco",
              "author": "hasmcp",
              "text": "My interest on the brilliant idea came for dealing with the spams and/or unwanted inputs. I found it clever. The developers are trying innovative ways and enjoying the moment.",
              "score": 3,
              "created_utc": "2026-02-09 02:50:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e5tbx",
                  "author": "HumanDrone8721",
                  "text": "So soon to verify that you're no robot you'll have to use a robot?",
                  "score": 3,
                  "created_utc": "2026-02-09 06:36:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b61ea",
          "author": "Moliri-Eremitis",
          "text": "Interesting. How long was the verification window?",
          "score": 1,
          "created_utc": "2026-02-08 20:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bh7z1",
              "author": "_tony_lewis",
              "text": "Seems to be 30s and if you are incorrect it locks, one of my agents made a formatting mistake so list a post",
              "score": 2,
              "created_utc": "2026-02-08 21:05:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyimtf",
      "title": "Grounding Is Not a Prompt",
      "subreddit": "LLMDevs",
      "url": "https://substack.com/home/post/p-187075330",
      "author": "SeriousSir1148",
      "created_utc": "2026-02-07 16:45:11",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.73,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qyimtf/grounding_is_not_a_prompt/",
      "domain": "substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qz2edh",
      "title": "Golang or Python",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qz2edh/golang_or_python/",
      "author": "Ok-Satisfaction945",
      "created_utc": "2026-02-08 07:14:07",
      "score": 7,
      "num_comments": 19,
      "upvote_ratio": 1.0,
      "text": "Why python over golang? Current on my first year of mechatronics looking to expand and get ahead. I just bought a Jetson Orin nano I would like to start tinkering with. I understand python is the right now but from research I done I feel like golang really got more potential overall. Would love to hear from people in this space.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qz2edh/golang_or_python/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o48u3q8",
          "author": "etherealflaim",
          "text": "Go is better for building systems. Python is better for stitching together other people's stuff, which in the world of AI means a lot more Lego bricks to do random stuff like training and evaluating models locally.  If you're using cloud models like Gemini, model gateways, or providers like ollama though, suddenly this advantage gets blunted.\n\nFor agentic systems, Temporal (which was built in Go but that isn't super relevant) is a killer technology and you can even mix and match Go and Python where each one suits.",
          "score": 5,
          "created_utc": "2026-02-08 12:55:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a0ylc",
              "author": "Ok-Satisfaction945",
              "text": "Thanks for the straight to the point explanation also wasn‚Äôt aware of temporal looking into it üëçüèæ",
              "score": 2,
              "created_utc": "2026-02-08 16:53:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4c5agf",
              "author": "Osi32",
              "text": "I like this answer. Well put.",
              "score": 1,
              "created_utc": "2026-02-08 23:11:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48hoy1",
          "author": "Osi32",
          "text": "I built a large (non-AI) middleware last year in Golang.\nFirst off- I love Golang. It‚Äôs powerful, fast and efficient. It‚Äôs truly an amazing language.\n\nHowever, adoption of a new language is slow.\n\nMy general advice is- if you ever plan to hire people to maintain what you‚Äôre building or get someone else to help maintain it. Python programmers are a dime a dozen (figure of speech, not literally ;))\n\nGolang is great if you want to build something from scratch with minimal external libraries from other people.",
          "score": 2,
          "created_utc": "2026-02-08 11:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48n2cl",
              "author": "Ok-Satisfaction945",
              "text": "I see , I was getting the impression that with python it‚Äôs like ‚Äú we been doing it this way why change now?‚Äù Type scenario that‚Äôs why I wanted to ask the users . Solid advice personally I think I will start with golang just didn‚Äôt know it was it something fundamentally that I guess wasn‚Äôt capable of compared to python in action/real world.",
              "score": 2,
              "created_utc": "2026-02-08 12:00:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48nx57",
          "author": "burntoutdev8291",
          "text": "Python. Faster iterations and POC. Libraries are also mostly in Python.\n\nI also say this with knowledge in rust and go.\n\nWith that said, you didn't really provide a use case. Do you want to do pure backend, devops, agentic, RAG, traditional AI engineering?",
          "score": 2,
          "created_utc": "2026-02-08 12:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48pn7a",
              "author": "Ok-Satisfaction945",
              "text": "I‚Äôm kind of all over the place atm, but I plan to go in mostly in robotics & agentic/ai integration. Wanted something I could take into different spaces. If I can‚Äôt spin of my own thing I plan of going to work at Lockheed, manufacturing/aerospace something of that nature. As far as python I noticed it‚Äôs more resources available. I just didn‚Äôt wanna learn something potentially useless or outdated by the time I get anywhere you know?",
              "score": 1,
              "created_utc": "2026-02-08 12:21:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47tlr2",
          "author": "tom-mart",
          "text": "Beacuse I know Python.",
          "score": 1,
          "created_utc": "2026-02-08 07:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47u894",
              "author": "Ok-Satisfaction945",
              "text": "Understandable, it just seems like Golang got way more potential & scalability, but again I‚Äôm not in any ‚Äúspace‚Äù per se. I‚Äôm starting sorta fresh the only limited experience I have is with xml & mySQL from hosting gaming servers in the past. If you could start again would you still choose python?",
              "score": 1,
              "created_utc": "2026-02-08 07:31:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47v08g",
                  "author": "tom-mart",
                  "text": ">it just seems like Golang got way more potential & scalability\n\nAll the major AI tools are written in Python. Starting with PyTorch and ending on the agentic frameworks like Pydantic AI or Langchain. Not sure how you see more potential in Golang but if you do then the choice should be simple for you.",
                  "score": 2,
                  "created_utc": "2026-02-08 07:38:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dinb6",
          "author": "Terrible_Tangelo6064",
          "text": "I thought the title said \"golang or prison\"",
          "score": 1,
          "created_utc": "2026-02-09 03:45:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g4xhm",
          "author": "BidWestern1056",
          "text": "most science researchers in ml/RL/AI use python so it is the most natural to use for any adjacent spaces because there will not be as many custom libraries in golang for specific scientific/matrix/tensor operations so for these you will have to pass to python or an equivalent in another language if it exists anyway so easier just to do it all in python. ",
          "score": 1,
          "created_utc": "2026-02-09 15:39:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gdd59",
          "author": "soopazoupy",
          "text": "python because nearly every AI stack assumes python first e.g. PyTorch, TensorFlow, ROS2 tooling, OpenCV, CUDA bindings, and most example repos you‚Äôll find are python native. small bonus with python is that it got libraries like pydantic to make it easy to structure inputs/outputs cleanly and newer tooling around it fits nicely when you start building ai powered or sensor-heavy pipelines. go is excellent for building reliable infrastructure, but the robotics/ML ecosystem around it is thinner so you‚Äôll fight the tooling more often. that doesn‚Äôt mean go is less potential tho but just that it's in a different lane",
          "score": 1,
          "created_utc": "2026-02-09 16:19:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o486z4n",
          "author": "Novel_Leading_7541",
          "text": "You don‚Äôt need to choose‚Äîlearn both lightly; with vibe coding and AI tools now, the language matters way less than understanding the problem üôÇ",
          "score": 1,
          "created_utc": "2026-02-08 09:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48mg6l",
              "author": "Ok-Satisfaction945",
              "text": "Definitely agree with you i just had one of those late night thoughts and wanted to hear from people that have used one or the other as far as their experiences. I live in a place where what Im doing or anything computer related is non existent & finding people with programming experience it‚Äôs difficult",
              "score": 1,
              "created_utc": "2026-02-08 11:55:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fl0y9",
              "author": "m98789",
              "text": "Not entirely true.\n\nThough the spirit of what you saying is true, in practice, you do need to select a language (and library ecosystem) that LLMs have a lot of training data on.",
              "score": 1,
              "created_utc": "2026-02-09 13:52:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4952w5",
          "author": "Crafty_Disk_7026",
          "text": "If you want quality software and less bugs then Go.  If you want to move fast and make things easier then Python.  I use both everyday",
          "score": 1,
          "created_utc": "2026-02-08 14:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a09fu",
              "author": "Ok-Satisfaction945",
              "text": "Anything you would say go lack that python it‚Äôs superior? And as far as libraries for go are they decently availability?",
              "score": 1,
              "created_utc": "2026-02-08 16:49:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4a1ngk",
                  "author": "Crafty_Disk_7026",
                  "text": "I would say no, atleast not for me, if I want to use Go for something I generally can.  People other than me may say that though.  Also it's not a big deal to have systems with 2 ore more languages that happily work together.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:56:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv6z6k",
      "title": "Natural Language to shell commands tool. Fully local, Ollama powered.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qv6z6k/natural_language_to_shell_commands_tool_fully/",
      "author": "ykushch",
      "created_utc": "2026-02-03 22:49:38",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[ask - natural language to shell commands](https://i.redd.it/vzpbvx06zchg1.gif)\n\nI built a CLI tool that turns natural language into shell commands using Ollama. It runs locally (no API keys, no data egress) and includes safety checks so you don't accidentally¬†`rm -rf`¬†your system.\n\nRepo:¬†[https://github.com/ykushch/ask](https://github.com/ykushch/ask)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qv6z6k/natural_language_to_shell_commands_tool_fully/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3jo2dk",
          "author": "konmik-android",
          "text": "I often ask Claude to do that, but having an offline version would be much better. Remembering and typing all parameters of all commands is something I left in 1990x.",
          "score": 2,
          "created_utc": "2026-02-04 15:38:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g2dog",
          "author": "DataCentricExpert",
          "text": "Curious if anyone has tried running it with a local dev environment that enforces data masking and auditability?",
          "score": 1,
          "created_utc": "2026-02-04 00:50:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwmbq0",
      "title": "glm 4.7 swe-bench 73.8% - tested claims on real refactoring tasks, improvement over previous models measurable",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qwmbq0/glm_47_swebench_738_tested_claims_on_real/",
      "author": "Weird_Perception1728",
      "created_utc": "2026-02-05 14:11:57",
      "score": 6,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "saw glm 4.7 swe-bench verified score (73.8%, +5.8 vs glm 4.6) and terminal bench (41%, +16.5)\n\nskeptical of benchmark gaming so tested on actual software engineering tasks\n\n**methodology:**\n\n20 refactoring tasks from internal codebase (flask, fastapi, django projects)\n\neach task: multi-file changes, maintaining references, no breaking changes\n\ntested against: glm 4.6, deepseek v3, codellama 70b\n\nmetric: success rate (code runs without fixes) + retry attempts needed\n\n**results:**\n\nglm 4.7: 17/20 success first attempt (85%))  \ndeepseek v3: 14/20 success first attempt (70%)  \ncodellama 70b: 11/20 success first attempt (55%)\n\n**failure analysis:**\n\nglm 4.7 failures: mostly edge cases in dependency injection patterns\n\nother models: frequent import hallucination, circular dependency introduction, breaking type hints\n\n**terminal bench correlation:**\n\ntested bash script generation (10 automation tasks)\n\nglm 4.7: 9/10 scripts ran without syntax errors  \nothers: 5-7/10 average\n\nterminal bench score (41% vs \\~25-35% typical) actually translated to real usage\n\n**architectural notes:**\n\n355b parameters, moe with 32b active per token\n\ntraining on 14.8t tokens\n\n**where improvement shows:**\n\ncross-file context tracking significantly better (measured by import correctness)\n\niterative debugging fewer loops to solution (average 1.4 attempts vs 2.3 for previous)\n\nbash/terminal command generation syntax correctness up\n\n**where still limited:**\n\ntraining cutoff late-2024 (misses recent library updates)\n\narchitectural reasoning weaker than frontier closed models\n\nexplanation depth inferior to teaching-optimized models\n\n**cost efficiency:**\n\napi pricing: \\~$3/month plan for generous coding use (significantly under openai/anthropic)\n\n**discussion points:**\n\nis 73.8% swe-bench representing actual capability or benchmark-specific tuning?\n\nbased on 20-task sample, improvement over previous versions real and measurable\n\nterminal bench correlation to bash quality interesting - suggests benchmark captures meaningful skill\n\n**limitations of this analysis:**\n\nsmall sample size (20 tasks)\n\ntasks from specific domains (web backends)\n\nno comparison to gpt-4/claude (cost prohibitive for extensive testing)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwmbq0/glm_47_swebench_738_tested_claims_on_real/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3pxn6q",
          "author": "WelcomeMysterious122",
          "text": "nice.",
          "score": 1,
          "created_utc": "2026-02-05 14:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pz6ar",
          "author": "microhan20",
          "text": "Interesting that terminal bench score correlates with actual bash quality. Usually synthetic benchmarks don't predict real performance well, but 41% vs 25-35% showing in your tests suggests its capturing something meaningful",
          "score": 1,
          "created_utc": "2026-02-05 14:27:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tjp4u",
          "author": "DeathShot7777",
          "text": "How much does it cost in total to run swe verified on glm4.7 \n\nI am working on a code intelligence layer using Graphs so agents can use it for deeper codebase understanding and want to test performance improvements on it. Will need to save up üòÖ",
          "score": 1,
          "created_utc": "2026-02-06 00:58:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u2u9q",
              "author": "Scared-Biscotti2287",
              "text": "You might want to check their site for the coding plans the cheapest is like 3 bucks for a month",
              "score": 1,
              "created_utc": "2026-02-06 02:52:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy8ptz",
      "title": "Your agent's 100% pass rate on 10 runs is statistically compatible with 72% true reliability. Here's the math and a way to fix your CI.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qy8ptz/your_agents_100_pass_rate_on_10_runs_is/",
      "author": "Better_Accident8064",
      "created_utc": "2026-02-07 08:39:48",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "I ran a LangGraph agent with Claude 3.5 Haiku on a trivial task (\"What is 15 * 37?\") across 100 trials. Pass rate: 70%. Not 95%, not 99%. Seventy percent on a calculator task.\n\nThe interesting part isn't that agents fail ‚Äî everyone here knows that. It's that **single-run evals can't detect it.** If you run 10 trials and get 10/10, Wilson score CI at 95% confidence gives you [0.722, 1.000]. Your \"perfect\" result is statistically compatible with a system that fails 28% of the time.\n\nThis matters for CI/CD. Most teams either skip agent evals in their pipeline or run each test once and assert pass/fail. Both approaches have the same problem: they can't distinguish a 95%-reliable agent from a 70%-reliable one unless you run enough trials.\n\n**What actually works for catching regressions:**\n\nRun each test case N times (N >= 20 makes a real difference). Compute Wilson CI on the pass rate. Compare against your baseline using Fisher exact test instead of naive diff. Use Benjamini-Hochberg correction if you're testing multiple cases simultaneously ‚Äî otherwise you'll get false alarms.\n\nFor failure attribution: group trials into pass/fail, compare tool call distributions at each step, pick the step with the lowest Fisher p-value. This gives you \"step 2 tool selection is the bottleneck\" instead of \"test failed.\"\n\nI open-sourced the framework I built for this: [agentrial](https://github.com/alepot55/agentrial). It wraps any Python callable and has adapters for LangGraph, CrewAI, AutoGen, Pydantic AI, OpenAI Agents SDK, and smolagents. YAML config, runs in CI, exit code 1 on statistically significant regression.\n\n```\nbasic-math      20/20  CI=[0.839, 1.000]  PASS\nmulti-step      14/20  CI=[0.480, 0.862]  FAIL\n  ‚Üí Step 2: tool selection diverges (p=0.003)\n```\n\nCurious how others are handling this. Are you running multi-trial evals in CI? Using soft thresholds? Something else entirely?\n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qy8ptz/your_agents_100_pass_rate_on_10_runs_is/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o41ydxz",
          "author": "Thick-Protection-458",
          "text": "\\> Your agent's 100% pass rate\n\nWell, isn't it clear?\n\nI mean that's fucking heuristic rather than predictable algorithm. With additional sampling randomness.\n\nYou can never have 100% working solution in such a case. You can have (unknown to you yet) distribution of test results you can't reliably distinguish from 100%.",
          "score": 5,
          "created_utc": "2026-02-07 09:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o420e97",
              "author": "Better_Accident8064",
              "text": "Exactly. You can't distinguish true 100% from a sample that happened to look like 100%. But you can quantify it.\n\n20/20 passes gives a Wilson 95% CI of \\[0.839, 1.000\\], so your \"perfect\" score is compatible with 84% true reliability. agentrial just makes this math explicit and wires it into CI/CD.",
              "score": 1,
              "created_utc": "2026-02-07 09:37:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43me85",
          "author": "lionmeetsviking",
          "text": "I use this: https://github.com/madviking/pydantic-llm-tester",
          "score": 1,
          "created_utc": "2026-02-07 16:15:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}