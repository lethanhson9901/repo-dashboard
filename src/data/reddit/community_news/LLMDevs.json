{
  "metadata": {
    "last_updated": "2026-02-16 03:09:31",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 82,
    "file_size_bytes": 110721
  },
  "items": [
    {
      "id": "1r1oa4i",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:53",
      "score": 211,
      "num_comments": 33,
      "upvote_ratio": 0.93,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) ‚Äì 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt‚Äôs trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4s8hek",
          "author": "TylerDurdenFan",
          "text": ">The cleaning, chunking, and optimization challenges are exactly what excites me\n\nJust try to not get too excited around that material, mkay?",
          "score": 24,
          "created_utc": "2026-02-11 11:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sctpa",
              "author": "Cod3Conjurer",
              "text": "He he üòÇ",
              "score": -11,
              "created_utc": "2026-02-11 12:20:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xu1u2",
                  "author": "kexxty",
                  "text": "Bro...this is not a laughing matter",
                  "score": 2,
                  "created_utc": "2026-02-12 06:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tf78a",
          "author": "Significant-Crow-974",
          "text": "It would be marvellous to run this over the full set of unredacted files.\nI am hoping that the FBI who have illegally redacted information do not now delete that hoard of documents.\nI hope that when they finally manage to charge Trump and the Epstein class that they will be able to utilise a tool such as this to make their prosecutions more effective.\nWell done and Thank you!",
          "score": 7,
          "created_utc": "2026-02-11 15:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tgqt7",
              "author": "Cod3Conjurer",
              "text": "The goal here is purely technical, building better retrieval over large unstructured datasets.  \nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.  \n",
              "score": 3,
              "created_utc": "2026-02-11 15:59:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tqxae",
                  "author": "Significant-Crow-974",
                  "text": "Yes. Fully appreciate that. Actually, I created a similar RAG for the first tranche of documents. Just as an exercise to see how a RAG could cope with so many documents.\nI would say that it was a partial success. Great insight.",
                  "score": 2,
                  "created_utc": "2026-02-11 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4wndcx",
                  "author": "Klutzy_Celebration80",
                  "text": "Might be interesting to see if you could have it un-redact the documents",
                  "score": 1,
                  "created_utc": "2026-02-12 01:33:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r00lr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 6,
          "created_utc": "2026-02-11 05:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdqsc",
              "author": "Cod3Conjurer",
              "text": "That's over 1gb¬†\nYou can generate using my code¬†",
              "score": 0,
              "created_utc": "2026-02-11 07:07:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sy6d6",
                  "author": "Borkato",
                  "text": "Wait 1gb is nothing when the models are like 20gb+",
                  "score": 1,
                  "created_utc": "2026-02-11 14:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sf4yp",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -5,
                  "created_utc": "2026-02-11 12:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tfnfq",
          "author": "DaRandomStoner",
          "text": "I was hoping you had the newly released documents in this... until we get these new documents processed through an OCR and into an organized data structure, we can't really go through them properly. \n\nIt would cost a good amount to process all the new documents so that we can include them in databases like this... it's all just compute costs though. DeepSeek's OCR is open-source and can run on most PCs. If a bunch of people got together we could expand databases like this to include all the newly released docs...",
          "score": 6,
          "created_utc": "2026-02-11 15:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4th0by",
              "author": "Cod3Conjurer",
              "text": "Yeah, this version doesn‚Äôt include the newly released documents yet. If those are raw scans, they‚Äôd need OCR + structured parsing before indexing.  \nThe main cost is compute and storage, not complexity.  \nA collaborative effort could definitely speed that up, especially for batching OCR and preprocessing at scale.",
              "score": 3,
              "created_utc": "2026-02-11 16:00:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4reucl",
          "author": "kondasamy",
          "text": "I think you should checkout - [https://jmail.world/jemini](https://jmail.world/jemini)",
          "score": 6,
          "created_utc": "2026-02-11 07:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf5lj",
              "author": "Cod3Conjurer",
              "text": "Yeeha i have seen that¬†\nBut it is currently broken",
              "score": 1,
              "created_utc": "2026-02-11 07:20:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x1h4q",
          "author": "jdsweet653",
          "text": "Great app! What did your ingestion py look like for the db?",
          "score": 2,
          "created_utc": "2026-02-12 02:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjmq3",
              "author": "Cod3Conjurer",
              "text": "The ingestion was pretty simple, I loaded the cleaned JSON, chunked it (400 size, 80 overlap), deduped chunks using SHA-256 hashing, generated MiniLM embeddings, and upserted everything into ChromaDB with source metadata.",
              "score": 2,
              "created_utc": "2026-02-12 05:01:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yp6ur",
                  "author": "DanRan88",
                  "text": "Any idea on the size of the DB?¬†",
                  "score": 2,
                  "created_utc": "2026-02-12 11:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xgy3s",
          "author": "Big3gg",
          "text": "See if it knows how to make jerky",
          "score": 2,
          "created_utc": "2026-02-12 04:41:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj134",
              "author": "Cod3Conjurer",
              "text": "he he ü§£",
              "score": 0,
              "created_utc": "2026-02-12 04:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y92qs",
          "author": "DarKresnik",
          "text": "Amazing job. Now we need someone to \"find\" those 3m missing documents.",
          "score": 2,
          "created_utc": "2026-02-12 08:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zqs1i",
              "author": "Cod3Conjurer",
              "text": "Guess I‚Äôll have to OCR the entire publicly available dataset myself now, jokingü§£",
              "score": 1,
              "created_utc": "2026-02-12 15:14:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5afg66",
              "author": "MaliciousTent",
              "text": "Seeing docs are gmail, would be a shame if someone at Google let the data out.",
              "score": 1,
              "created_utc": "2026-02-14 04:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t10wl",
          "author": "StackSmashRepeat",
          "text": "So, have you come to terms with RAG being a dead end as far as real recall of memory works? Or are you just chunking and overlapping to a ridiculous point? I really don't think this is a sensible use of RAG. The LLM will at some point start hallucinating missing pieces from thin air, making this tool fairly unreliable for accuracy. \n\nPeople looking into these files need absolute accuracy.",
          "score": 1,
          "created_utc": "2026-02-11 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tfjmw",
              "author": "Cod3Conjurer",
              "text": "You‚Äôre right that absolute accuracy matters. That‚Äôs why this should be treated as an assistive search layer, not a final source of truth.\n\nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.",
              "score": 5,
              "created_utc": "2026-02-11 15:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tikha",
                  "author": "StackSmashRepeat",
                  "text": "Make it do an online search after it retrieves data from rag and provide a link directly to an online source. End users are dumb and some will believe anything the llm tells them.",
                  "score": 1,
                  "created_utc": "2026-02-11 16:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o538mx9",
          "author": "HarjjotSinghh",
          "text": "2m pages = my new love language.",
          "score": 1,
          "created_utc": "2026-02-13 01:44:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r49we9",
      "title": "AI Developer Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/mhyf0n56qdjg1.png",
      "author": "Main-Fisherman-2075",
      "created_utc": "2026-02-14 03:29:03",
      "score": 199,
      "num_comments": 28,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r49we9/ai_developer_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5c7ogy",
          "author": "TheDeadlyPretzel",
          "text": "Instructor is not an agent framework, rather it is a structured output inference library.\n\nOn the other hand, Atomic Agents which was built on top of instructor IS an agent framework: [https://github.com/BrainBlend-AI/atomic-agents](https://github.com/BrainBlend-AI/atomic-agents)",
          "score": 2,
          "created_utc": "2026-02-14 13:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eza1p",
              "author": "Main-Fisherman-2075",
              "text": "thanks for point that out",
              "score": 0,
              "created_utc": "2026-02-14 22:38:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gvvsr",
          "author": "walkingbiscuit",
          "text": "For Agent Development missing Google ADK, and i don't know where you want to put Chrome browser now, since in the beta release it has WebMCP",
          "score": 1,
          "created_utc": "2026-02-15 06:37:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hzhwd",
          "author": "kubrador",
          "text": "looking at this like it's supposed help me pick a tool but it just makes me feel like i'm colorblind at a rave",
          "score": 1,
          "created_utc": "2026-02-15 12:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kxir2",
          "author": "afucher",
          "text": "Missing [ECA](https://eca.dev/)",
          "score": 1,
          "created_utc": "2026-02-15 22:00:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5abluh",
          "author": "economicscar",
          "text": "Prime intellect missing under inference and compute",
          "score": 1,
          "created_utc": "2026-02-14 04:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br22k",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 0,
              "created_utc": "2026-02-14 11:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ccb1u",
                  "author": "Equity_Harbinger",
                  "text": "Can you share the latest one please (which is also less blurry, because when I zoom the image, words are blurry beyond recognition)\n\n\n(Thank you for your contributions)",
                  "score": 1,
                  "created_utc": "2026-02-14 14:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5atgtt",
          "author": "Live-Speech-1058",
          "text": "Antigravity?",
          "score": 1,
          "created_utc": "2026-02-14 06:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br5gi",
              "author": "Main-Fisherman-2075",
              "text": "I think I added it, I don't know why it's not there but definitely worth a try.",
              "score": 0,
              "created_utc": "2026-02-14 11:47:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bu98n",
          "author": "increasinglybold",
          "text": "Pi coding agent is great",
          "score": 1,
          "created_utc": "2026-02-14 12:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ezbgx",
              "author": "Main-Fisherman-2075",
              "text": "Will check it out",
              "score": 1,
              "created_utc": "2026-02-14 22:39:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bz0vl",
          "author": "Realistic-Damage2004",
          "text": "https://ainativedev.io/landscape\n\nHas been around for a while now. Is this published anywhere?",
          "score": 1,
          "created_utc": "2026-02-14 12:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5djaiy",
          "author": "Neferio1",
          "text": "Greptile is a very good code review tool",
          "score": 1,
          "created_utc": "2026-02-14 18:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ez8zy",
              "author": "Main-Fisherman-2075",
              "text": "1000%",
              "score": 1,
              "created_utc": "2026-02-14 22:38:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5fgo08",
              "author": "oXeNoN",
              "text": "How does it compare with other tools like CodeRabbit? Is CodeRabbit just spending more on marketing? üòÖ",
              "score": 0,
              "created_utc": "2026-02-15 00:24:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fuy08",
                  "author": "Neferio1",
                  "text": "We tested both and we choose Greptile over CodeRabbit just because Greptile can be ¬´¬†selfhosted¬†¬ª using Kubernetes or Docker. From a review perspective, Greptile and CodeRabbit are equivalent",
                  "score": 2,
                  "created_utc": "2026-02-15 01:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bn5o0",
          "author": "mcd0g",
          "text": "Warp under coding agents missing. They really need to step up their PR game",
          "score": 0,
          "created_utc": "2026-02-14 11:11:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br1u4",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 1,
              "created_utc": "2026-02-14 11:47:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bth4g",
          "author": "renntv",
          "text": "Great overview! Do you keep it on the web for linking, or just here on Reddit? ",
          "score": 0,
          "created_utc": "2026-02-14 12:07:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etdza",
              "author": "Main-Fisherman-2075",
              "text": "Hey I keep it here: but the content inside is still not very polished yet. https://www.keywordsai.co/market-map I will try to add all the description comparison price etc inside",
              "score": 1,
              "created_utc": "2026-02-14 22:05:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5c2b8u",
              "author": "Code_Exists_Here",
              "text": "Yeh same question from me.",
              "score": 0,
              "created_utc": "2026-02-14 13:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egtyh",
          "author": "funguslungusdungus",
          "text": "I need a link!",
          "score": 0,
          "created_utc": "2026-02-14 20:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etiuy",
              "author": "Main-Fisherman-2075",
              "text": "https://www.keywordsai.co/market-map here you go! I will try to update weekly and the content in it",
              "score": 1,
              "created_utc": "2026-02-14 22:06:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egxm5",
          "author": "Varqu",
          "text": "You can just use Claude Code.",
          "score": 0,
          "created_utc": "2026-02-14 20:57:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fkv68",
          "author": "Disastrous-Maybe2501",
          "text": "Mistral Vibe missing in coding agents",
          "score": 0,
          "created_utc": "2026-02-15 00:50:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4ylja",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:59:37",
      "score": 33,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5fifzx",
          "author": "Vearres17",
          "text": "The fact that you kept Gemma3 sliding-window attention in FP8 is impressive. I've seen some implementations that fall back to fp16 for the local attention layers I guess it bcz it can be tricky to handle",
          "score": 1,
          "created_utc": "2026-02-15 00:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fjkgw",
              "author": "Educational_Cry_7951",
              "text": "thanks tbf it was a pain for me too at first",
              "score": 1,
              "created_utc": "2026-02-15 00:42:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hcpmr",
          "author": "Delicious-One-5129",
          "text": "This is seriously impressive work. An actual NVFP4 first path on RTX 4090 without silent FP16 fallback is huge for people squeezing every GB out of Ada cards. The VRAM savings vs FP16 are especially compelling.",
          "score": 1,
          "created_utc": "2026-02-15 09:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hwkm4",
              "author": "Educational_Cry_7951",
              "text": "Thank you! ",
              "score": 1,
              "created_utc": "2026-02-15 12:23:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3kgpn",
      "title": "Rearchitecting LLMs ‚Äî pruning, distillation, and smaller domain models (MEAP)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "author": "ManningBooks",
      "created_utc": "2026-02-13 09:07:18",
      "score": 23,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "Hi r/LLMDevs,\n\nStjepan from Manning here. The mods said it's ok if I post this here. \n\nWe‚Äôve just released a book that‚Äôs very much aimed at the kinds of problems this community discusses all the time: what to do when a general-purpose LLM is technically impressive but awkward, expensive, or inefficient for your actual use case.\n\n**Rearchitecting LLMs** by Pere Martra  \n[https://www.manning.com/books/rearchitecting-llms](https://hubs.la/Q042-hLy0)\n\n[Rearchitecting LLMs by Pere Martra](https://preview.redd.it/vyy079zx78jg1.jpg?width=2213&format=pjpg&auto=webp&s=755a8b1ab1320ede5daedfa861d6ab8d1b0c5e5d)\n\nThe core idea of the book is simple but powerful: instead of treating open models as fixed artifacts, you can reshape them. Pere walks through structural techniques like targeted fine-tuning, pruning, and knowledge distillation to build smaller, cheaper, domain-focused models that still perform well on the tasks you care about.\n\nWhat makes this book interesting is how hands-on it gets. You‚Äôre not working with abstract toy networks. The examples focus on modifying widely used open models, such as Llama-3, Gemma, and Qwen. The focus is on understanding which parts of a model actually contribute to behavior, how to identify waste or redundancy, and how to remove or compress components without blindly wrecking performance.\n\nThere‚Äôs also some genuinely thoughtful material on combining behavioral analysis with structural changes. Instead of just cutting parameters and hoping for the best, the book explores ways to reason about why a modification works or fails. One section that tends to spark discussion is ‚Äúfair pruning,‚Äù where pruning is used not only for efficiency but also to reduce bias at the neuron level.\n\nIf you‚Äôre working on local models, cost-constrained deployments, or specialized SLMs, this book is very much in that territory. It‚Äôs written for people who are comfortable with LLM concepts and want to go deeper into how models can be reshaped rather than simply prompted.\n\n**For the** r/LLMDevs **community:**  \nYou can get **50% off** with the code **MLMARTRA50RE**.\n\nA quick note on availability: the book is currently in **MEAP (Manning Early Access Program)**. That means you get immediate access to the chapters as they‚Äôre written, along with updates as the manuscript evolves.\n\nHappy to bring the author to answer questions about the book, the techniques it covers, or the kinds of readers it‚Äôs best suited for. And I‚Äôd be curious to hear from folks here who are already doing pruning or distillation in practice ‚Äî what‚Äôs been harder than expected?\n\nI'm ready to give away 5 ebooks to the first five commenters who share their experience here.\n\nThank you all for having us. It feels great to be here.\n\nCheers,",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o54xstz",
          "author": "StackSmashRepeat",
          "text": "Would you list some common problems and terminologies that the book covers? I'll have a look if it peaks my interest.",
          "score": 4,
          "created_utc": "2026-02-13 09:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5529pk",
              "author": "ManningBooks",
              "text": "Hey, thanks for asking. Here are some examples of what the book covers:\n\n\\- End-to-End Model Re-architecting (Chapter 2): Transform Gemma-3-270M using depth pruning and knowledge distillation for a 10% speed increase while retaining 93-98% of original capabilities in a hands-on project.\n\n\\- Data-Driven Pruning (Chapters 4-5): Create two models from the same base (Qwen3-0.6B or Llama-3.2-1B): one for formal texts (WikiText) and another for short messages (SMS Spam), using activation analysis with PyTorch hooks to highlight domain-specific component importance.\n\n\\- Bias Auditing and Correction: In ethics chapters, perform a model \"cleanup\" using ablation frameworks and PCA visualization to identify and mitigate demographic biases, achieving fairness without full retraining.\n\n\\- Mini-Capstone: Utilize a small \"draft model\" to speed up LLM inference by quickly proposing tokens, validated by a larger model.\n\n\\- Capstone Project: Migrate an agent system from costly external APIs to a specialized local Small Language Model (SLM).\n\nHope this helps.",
              "score": 6,
              "created_utc": "2026-02-13 10:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56j1zc",
                  "author": "StackSmashRepeat",
                  "text": "This is quite interesting; you're trying to move local models away from the static model while staying within the static framework? I could basically train a model for my iPhone, let's say I export all my email and scrub PII, format for training data and then I could fine-tune to write mails that look somewhat within the realm of my own style? \n\nI haven't looked into training or fine tuning as I couldn't think of a personal use case for these tiny models, but like you're saying \"domain-focused\", gave a clearer picture.\n\nThis is a little over my current scope as I'm not even sure if I understood this correctly, but I've been thinking of ways to make a digital twin that could handle writing across multiple platforms. Was always thinking Id need a larger model to handle such a task because it sounds easy enough, but capturing the essence of one's writing is quite a complex task for llms. At least in my experience.\n\nThanks for the info.",
                  "score": 3,
                  "created_utc": "2026-02-13 15:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57f10h",
              "author": "pmartra",
              "text": "Hi u/StackSmashRepeat \n\nI'll give you a rough explanation and if you want more details just ask me. \n\nIn the book I explain an LLM optimization / customization pipeline. The pipeline basically consists of: \n\n1- Pruning (depth and width) which is removing parts of a model. \n\n2- Knowledge Distillation. Recovering the lost knowledge by transferring it from the base model to the pruned one. \n\n3- Finetuning on a specialized domain. \n\nDuring this pipeline you gain knowledge about how models work internally, since to decide which parts to remove we study how activations are produced detecting the mos important parts. \n\nWe take advantage of this knowledge to do surgical operations on the model in the more advanced chapters, like replacing attention layers or changing the behaviour of the model modifying the weights of some neurons. \n\nThe pipeline is very similar to what companies like Nvidia or Mistral follow to create their model families, but adapted to create specific models,  and using less data and processing capacity than they have.   \n  \nFor example in width pruning Mistral uses a completely dynamic model to detect which weights to remove, in the book we use a combination that rewards keeping neurons with high importance, so with much less data you can get an efficient model. \n\nAlthough it really seems very complicated we start from the simplest things, basic depth pruning, in chapter 2 you already remove parts of a model and recover the lost knowledge. \n\nFrom this base you build the knowledge that leads you to the more advanced techniques. \n\nThen there's a second intention which is to make cutting-edge research understandable, so in each chapter starting from the fourth, there's an explanation of which papers the chapter's code is based on and how we've adapted it when implementing it. \n\nFor example the development of width pruning in modern models like Llama or Gemma is based on a paper which we've changed a good part of the formulas to simplify it but keeping its general idea. \n\nI hope the explanation was useful! \n\nPere.",
              "score": 2,
              "created_utc": "2026-02-13 18:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fsrna",
                  "author": "h8mx",
                  "text": "Hey, your comment on this thread was auto-removed by Reddit as spam, but I approved it. Thanks for your input!",
                  "score": 2,
                  "created_utc": "2026-02-15 01:42:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57qobq",
          "author": "marm_alarm",
          "text": "I'm a subscriber to Manning and so I have access to all the MEAP content.  I am very interested in reading this book and will post my review here after I've taken a look!",
          "score": 2,
          "created_utc": "2026-02-13 19:14:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57u5v0",
          "author": "dextoz",
          "text": "Would love a copy and meap along!",
          "score": 1,
          "created_utc": "2026-02-13 19:31:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b0dln",
          "author": "boredaadvark",
          "text": "Excited for this and this resonates to what I want to explore. Keen on getting a copy. How complete is this book in terms of percentage?",
          "score": 1,
          "created_utc": "2026-02-14 07:29:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6n2y",
              "author": "pmartra",
              "text": "Hi, at this moment there are just 2 chapters, the third will be published next week. ",
              "score": 1,
              "created_utc": "2026-02-14 16:58:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1jr22",
      "title": "I'm super unemployed and have too much time so I built an open source SDK to build event-driven, distributed agents on Kafka",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1jr22/im_super_unemployed_and_have_too_much_time_so_i/",
      "author": "orange-cola",
      "created_utc": "2026-02-11 01:30:26",
      "score": 23,
      "num_comments": 12,
      "upvote_ratio": 0.75,
      "text": "I finally got around to building this SDK for event-driven agents. It's an idea I've been sitting on for a while. I finally started working on it and it's been super fun to develop.\n\nI made the SDK in order to decompose agents into independent, separate microservices (LLM inference, tools, and routing) that communicate asynchronously through Kafka. This way, agents, tool services, and downstream consumers all communicate asynchronously and can be deployed, adapted, and scaled completely independently.\n\nThe event-driven structure also makes connecting up and orchestrating multi-agent teams trivial. Although this functionality isn't yet implemented, I'll probably develop it soon (assuming I stay unemployed and continue to have free time on my hands).\n\nCheck it out and throw me a star if you found the project interesting!¬†[https://github.com/calf-ai/calfkit-sdk](https://github.com/calf-ai/calfkit-sdk)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1jr22/im_super_unemployed_and_have_too_much_time_so_i/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4q4hbc",
          "author": "qa_anaaq",
          "text": "Ah this is cool. I was obsessed with the idea of the event-driven approach to agents last year but never had to time to explore it. I‚Äôll be diving in. I always thought it‚Äôs a solid approach.",
          "score": 5,
          "created_utc": "2026-02-11 01:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q9ba4",
              "author": "orange-cola",
              "text": "Yea for sure--I think it's a must for production-ready agents. Let me know what you think when you try it out! Always love any feedback",
              "score": 1,
              "created_utc": "2026-02-11 02:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rd7in",
          "author": "His0kx",
          "text": "Seems really interesting on paper ! How did you manage the separate microservice part ? One thing I have struggled with is that even with quality gates/api contracts between agents, for the same phase, same prompt/tools the output could be different with different subagents (maybe more food for thoughts when you will start working on orchestrating multi agents ?)",
          "score": 3,
          "created_utc": "2026-02-11 07:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rqnc4",
              "author": "orange-cola",
              "text": "How I'm currently going about it is to think of multi-agent orchestrations as agents in a text-based groupchat. Internally, each agent can can call tools with different schemas but when it responds into the \"groupchat,\" the message will always be text based. This way, the schema between agents will always be predictable, and agents can still effectively coordinate and chat among eachother. This is just my initial naive implementation so I fully expect to grow this into something more sophisticated as different agent communication patterns emerge.",
              "score": 1,
              "created_utc": "2026-02-11 09:09:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rwdwp",
                  "author": "His0kx",
                  "text": "Okay I think you have the right intuition üòÖ, Claude code did the same on the last release (Agent team)",
                  "score": 2,
                  "created_utc": "2026-02-11 10:02:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzu1d",
          "author": "Far_Independent8754",
          "text": "This is exactly the direction the industry needs to move. Building monolithic agents is a dead end for production.\n\nI‚Äôve been preaching about this lately‚Äîwe need to stop the 'Prompt Alchemy' and move toward **Microagentic Stacking**. Your approach with Kafka is the perfect infrastructure for it because it enforces the decoupling that most people ignore.\n\nIf you are breaking down agents into independent services, you‚Äôve already won half the battle against 'reasoning decay'. I actually wrote a **Manifesto** on why this modular/stacked approach is the only way to scale without the whole thing collapsing into a 'Big Ball of Mud'.\n\nCheck it out if you want to see the architectural patterns I'm formalizing: üîó[https://github.com/ericmora/microagentic-stacking](https://github.com/ericmora/microagentic-stacking)\n\nCongrats on the SDK, man. Building in public while job hunting is the best way to show senior-level thinking. Starred! ‚≠ê",
          "score": 2,
          "created_utc": "2026-02-11 05:11:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4robrd",
              "author": "orange-cola",
              "text": "Thanks for the star! And agreed, I believe production-grade agents will inevitably have to move towards event-driven architecture as these agent systems scale.",
              "score": 1,
              "created_utc": "2026-02-11 08:47:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4r3sei",
          "author": "Sea-Sir-2985",
          "text": "the decomposition into separate microservices for inference, tools, and routing is smart... the main pain point with monolithic agent frameworks is that scaling one part means scaling everything, and a slow tool call blocks the whole pipeline\n\nkafka as the backbone makes the async part trivial but i'd be curious how you're handling the latency tradeoff. LLM agents are already slow from inference time so adding message queue overhead on top might make the end-to-end response time rough for interactive use cases. seems like it'd shine more for batch processing and background agent workflows where latency doesn't matter as much\n\nthe multi-agent orchestration part is where this could get really interesting though... being able to spin up independent agent services that communicate through events without tight coupling is way cleaner than most multi-agent frameworks that try to do everything in one process",
          "score": 2,
          "created_utc": "2026-02-11 05:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rpx1r",
              "author": "orange-cola",
              "text": "I agree. I think added overhead from message queueing is probably best managed by dynamic instance scaling that adjusts to the incoming traffic load. It's more on the operational side of things, and outside of the SDK's domain, but the good thing is there are already well-established technologies for this purpose.\n\nAlso totally agree on the background agent workflows part. For agent operations on continuous background data streams, this SDK can really shine.",
              "score": 1,
              "created_utc": "2026-02-11 09:02:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ynvo1",
          "author": "UseMoreBandwith",
          "text": "interesting. but what could the use-cases for something like that?  (other than hacking/ddos etc)",
          "score": 2,
          "created_utc": "2026-02-12 11:11:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54dbr2",
          "author": "arbiter_rise",
          "text": "I‚Äôm also very interested in the event-driven approach! I‚Äôve added a star to the repo as well.  \n  \nIt seems like this could be fully handled by another broker as well. Was there a specific reason you chose to use Kafka?\n\nWhat level of message volume is being handled by Kafka? I‚Äôm curious whether the context can be maintained reliably.",
          "score": 2,
          "created_utc": "2026-02-13 06:23:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r04wed",
      "title": "Observations From Using GPT-5.3 Codex and Claude Opus 4.6",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "author": "Arindam_200",
      "created_utc": "2026-02-09 13:58:50",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "I tested GPT-5.3 Codex and Claude Opus 4.6 shortly after release to see what actually happens once you stop prompting and start expecting results. Benchmarks are easy to read. Real execution is harder to fake.\n\nBoth models were given the same prompts and left alone to work. The difference showed up fast.\n\nCodex doesn‚Äôt hesitate. It commits early, makes reasonable calls on its own, and keeps moving until something usable exists. You don‚Äôt feel like you‚Äôre co-writing every step. You kick it off, check back, and review what came out. That‚Äôs convenient, but it also means you sometimes get decisions you didn‚Äôt explicitly ask for.\n\nOpus behaves almost the opposite way. It slows things down, checks its own reasoning, and tries to keep everything internally tidy. That extra caution shows up in the output. Things line up better, explanations make more sense, and fewer surprises appear at the end. The tradeoff is time.\n\nA few things stood out pretty clearly:\n\n* Codex optimizes for momentum, not elegance\n* Opus optimizes for coherence, not speed\n* Codex assumes you‚Äôll iterate anyway\n* Opus assumes you care about getting it right the first time\n\nThe interaction style changes because of that. Codex feels closer to delegating work. Opus feels closer to collaborating on it.\n\nNeither model felt ‚Äúsmarter‚Äù than the other. They just burn time in different places. Codex burns it after delivery. Opus burns it before.\n\nIf you care about moving fast and fixing things later, Codex fits that mindset. If you care about clean reasoning and fewer corrections, Opus makes more sense.\n\nI wrote a longer breakdown [here](https://www.tensorlake.ai/blog/claude-opus-4-6-vs-gpt-5-3-codex) with screenshots and timing details in the full post for anyone who wants the deeper context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4fmz12",
          "author": "swarmed100",
          "text": "Opus 4.6 reasons a lot longer than opus 4.5. One negative side I noticed from this is that it is \"better\" at finding delusional logic to explain why a set of facts that are clearly impossible \"make sense\", instead of concluding that some of the assumptions or inputs must be wrong since the set of facts are just impossible together.",
          "score": 6,
          "created_utc": "2026-02-09 14:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gbcxu",
              "author": "External-Yak-371",
              "text": "As a pro plan user I agree, but it also means my piddly allowance can nearly be consumed in one good planning session",
              "score": 3,
              "created_utc": "2026-02-09 16:10:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hrtb3",
                  "author": "Manfluencer10kultra",
                  "text": "u/External-Yak-371 One git commit request for small refactors in many files (similar refactors) was enough for 48%.  It did notice I missed 5 items that needed to be refactored then used like 20k tokens to fix it, and then it was absolutely perfect.\n\nToo bad it was 50% of my 5h allowance (you get about 9 x 5h on pro at 11% of weekly ...).  In that sense, it was absolutely worthless spending my tokens on it.  \nBut what if I used Sonnet for it? it would have been maybe worse.  \nAnd these are the things that you don't want to do yourself, and want to hand over to AI. You close off your session after a long day, forget to commit all those refactors, but still want a sensible commit instead of \"lots of fixes\".  \nEh this is where AI tooling should come in to save the day, but nope..",
                  "score": 1,
                  "created_utc": "2026-02-09 20:21:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gcjpr",
              "author": "cmndr_spanky",
              "text": "This is pretty worrying but makes sense. More reasoning doesn‚Äôt mean better results and often just causes useless ‚Äúthought loops‚Äù that at best just wastes more open credits, at worst fills up context causing it to loose touch with the original request details.\n\nThat said, I‚Äôve never been impressed with any of openAI‚Äôs models as coding agents, so I‚Äôd suspect opus is still better despite the flaws. We‚Äôll see I guess",
              "score": 2,
              "created_utc": "2026-02-09 16:15:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gfqe5",
          "author": "kubrador",
          "text": "so basically one model is a startup founder and the other is an engineering lead pretending to care about code review",
          "score": 2,
          "created_utc": "2026-02-09 16:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hrdaq",
          "author": "Manfluencer10kultra",
          "text": "Bruh, I switched to Codex (Free) and getting incredible usage just on Free and GPT-5.2-Codex High, not even 5.3 and it's just night and day.  Claude put me in a depressive mood, and now I'm back enjoying engineering again.",
          "score": 2,
          "created_utc": "2026-02-09 20:19:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r162ky",
      "title": "LLaDA2.1 vs Autoregressive Baselines: Is Diffusion Finally Competitive for Inference Throughput?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r162ky/llada21_vs_autoregressive_baselines_is_diffusion/",
      "author": "Ill_Awareness6706",
      "created_utc": "2026-02-10 16:51:41",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "Been digging into the LLaDA2.1 paper (arXiv:2602.08676) after seeing claims about diffusion LLMs hitting 892 TPS on HumanEval+. The numbers are interesting enough that I'm starting to wonder if we've been sleeping on diffusion models for inference workloads.\n\nQuick context for those unfamiliar: LLaDA2.1 introduces a dual decoding system (Speedy Mode vs Quality Mode) that lets you trade accuracy for throughput. The 100B model hits 892 TPS peak on coding benchmarks with quantization, while the 16B model averages 1071 TPS across nine benchmarks vs 302 TPS for Qwen3 8B. The key innovation is Token to Token editing that lets the model correct its own mistakes during generation.\n\nStandard absorbing state diffusion models have a fundamental problem: once a \\[MASK\\] token becomes a real token, it's locked in forever. LLaDA2.1 adds what they call an \"Editing Set\" alongside the \"Unmasking Set\" at each timestep. This means the model can retroactively fix errors during parallel decoding, which addresses the token inconsistency issues that have plagued diffusion LLMs. Speedy Mode generates a rough draft fast (aggressive M2T threshold), then T2T passes clean up the artifacts. Quality Mode uses conservative thresholds throughout for higher accuracy but slower generation.\n\nThe benchmark highlights: LLaDA2.1 Flash (100B) on HumanEval+ scores 89.63 in both modes, with Speedy Mode achieving 13.81 tokens per forward vs Quality Mode's 9.18 TPF. On LiveCodeBench, Speedy Mode hits 44.05 at 6.48 TPF while Quality Mode reaches 45.37 at 3.80 TPF. Peak throughput with their quantized setup: 891.74 TPS on HumanEval+, 801.48 TPS on BigCodeBench Full, 663.39 TPS on LiveCodeBench. The 16B Mini variant peaks at 1586.93 TPS on HumanEval+. Compared to baselines across nine benchmarks: LLaDA2.1 Mini averages 1071.2 TPS vs 597.1 for LLaDA2.0 Mini, 464.7 for Ling Mini 2.0, and 301.9 for Qwen3 8B. That's roughly 3.55x throughput comparing the 16B diffusion model against the 8B autoregressive baseline, so not exactly apples to apples on parameters, but the efficiency gap is notable. Reproducibility note: these numbers use customized SGLang with per block FP8 quantization.\n\nThey also have Multi Block Editing that trades throughput for accuracy: AIME 2025 jumps from 63.33 to 70.00 with MBE, ZebraLogic from 84.20 to 88.20. Average across 10 benchmarks: 72.67 at 5.14 TPF with MBE vs 70.69 at 5.82 TPF without.\n\nOn the training side, they built what they claim is the first large scale RL framework for diffusion LLMs using EBPO (ELBO based Block level Policy Optimization). The core problem is that sequence level log likelihood is intractable in block autoregressive models, so they use Vectorized Likelihood Estimation for parallelized bound computation. Interesting direction if you're thinking about fine tuning diffusion models beyond standard SFT, though I haven't dug deep into the training methodology yet.\n\nNow here's where I'm genuinely uncertain. The paper is upfront about the tradeoffs: Speedy Mode works well for code and math but can produce artifacts on general chat. They specifically mention n gram repetitions that self correction only partially fixes. So the question becomes: are these throughput numbers meaningful if you can only use Speedy Mode for narrow domains?\n\nTo make this concrete: say you're running a code completion service handling 10K requests per minute. At 302 TPS with Qwen3 8B, you need roughly 33 inference instances to keep up. At 1071 TPS with LLaDA2.1 Mini, that drops to about 10 instances. That's real infrastructure savings if the quality holds up in production. But I'm skeptical whether benchmark TPS translates to real world throughput given the different decoding dynamics, and whether the occasional n gram artifacts would tank user experience.\n\nPaper: [https://arxiv.org/abs/2602.08676](https://arxiv.org/abs/2602.08676)\n\nGitHub: [https://github.com/inclusionAI/LLaDA2.X](https://github.com/inclusionAI/LLaDA2.X)\n\nFor those running inference at scale: would 3x throughput on code generation be enough to justify adding a completely different model architecture to your stack? Or is the operational complexity not worth it until diffusion models close the gap on general tasks?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r162ky/llada21_vs_autoregressive_baselines_is_diffusion/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r5nil3",
      "title": "I built a CLI that extracts design systems from any live website",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "author": "Every_Chicken_1293",
      "created_utc": "2026-02-15 19:26:36",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I kept running into the same problem: I'd see a website I liked and want to build something with a similar design, but manually inspecting every color, font, spacing value, and component pattern was tedious.\n\nSo I built design-memory. You point it at a URL and it:  \n  \n\\- Crawls the page with Playwright  \n\\- Extracts colors, typography, spacing, border radius, elevation  \n\\- Captures all CSS custom properties (often 500-700+ variables)  \n\\- Detects Tailwind usage and top utility patterns  \n\\- Uses an LLM to interpret component recipes and layout structure  \n\\- Outputs a .design-memory/ folder of markdown files\n\nThe output is structured so you can paste it into Claude, Cursor, or ChatGPT and get a faithful recreation of the original design.\n\nIt also supports learning from screenshots, multi-page crawls, and diffing two design systems.\n\nSource: [https://github.com/memvid/design-memory](https://github.com/memvid/design-memory)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5lfrsg",
          "author": "NeverSkipSleepDay",
          "text": "Have you ever seen what a design system looks like?",
          "score": 1,
          "created_utc": "2026-02-15 23:42:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r041y4",
      "title": "A RAG Agent and their Types",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1r041y4",
      "author": "KarllsMarcel",
      "created_utc": "2026-02-09 13:21:33",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r041y4/a_rag_agent_and_their_types/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4fh4qy",
          "author": "flonnil",
          "text": "ballsy to use n8n screenshots in something that tries to sell you as a \"pro\".",
          "score": 2,
          "created_utc": "2026-02-09 13:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j8ojz",
          "author": "dezastrologu",
          "text": "Cool more LLM generated slop",
          "score": 1,
          "created_utc": "2026-02-10 00:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ptltc",
          "author": "SystemFlowStudio",
          "text": "One thing that helped me think about RAG agents more clearly was separating **retrieval responsibility** from **control responsibility**.\n\nRough breakdown I keep coming back to:\n\n**1. Passive RAG**  \nRetrieval is a single, fixed step (query ‚Üí retrieve ‚Üí answer). No iteration, no state. Easy to reason about, hard to scale to complex tasks.\n\n**2. Tool-augmented RAG agent**  \nThe agent decides *when* to retrieve vs act. Retrieval becomes a tool call, often interleaved with reasoning steps. This is where looping and stale context issues start to appear.\n\n**3. Planner-Executor RAG**  \nPlanner decomposes the task, executor performs steps, retrieval happens per step. Much more powerful, but you need guardrails or you get infinite plan-replan cycles.\n\n**4. Memory-augmented / stateful RAG**  \nRetrieval isn‚Äôt just documents ‚Äî it includes prior actions, summaries, or checkpoints. Great for long tasks, very easy to accidentally poison the context.\n\nWhat I‚Äôve noticed is most ‚ÄúRAG agent bugs‚Äù aren‚Äôt retrieval quality issues ‚Äî they‚Äôre **control-flow failures** (no termination condition, repeated tool calls, planner drift).\n\nCurious how others here are drawing these boundaries ‚Äî especially in production systems.",
          "score": 1,
          "created_utc": "2026-02-11 00:42:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2ree2",
      "title": "Lessons from building AI shopping assistant for 1B$+ skincare brand.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "author": "rudzienki",
      "created_utc": "2026-02-12 11:50:51",
      "score": 9,
      "num_comments": 9,
      "upvote_ratio": 0.85,
      "text": "Hey! I was recently hired to build an AI shopping assistant for a huge brand, 1B$+ in revenue. Unfortunately can't say which one is it (damn NDAs), but I thought I'd share some lessons. After the project CTO told me ‚ÄúWorking with you was the best AI investment in the last year‚Äù, so I guess it went well!\n\nI'm reposting this from my linkedin, so sorry for this \"linkedinish\" vibe:\n\nThe biggest secret was, surprise, surprise, **not** wasn‚Äôt fancy AI methods, complex RAG pipelines, and multi step workflows. In the end it was good prompts, a bunch of domain-specific tools and one subagent.  \n  \nThe secret was the process.  \n  \nI didn‚Äôt know anything about skincare so I had to learn about it. Even light understanding of the domain turned out EXTREMELY IMPORTANT since it allowed m to play around with an agent and have a good judgement whether it says good things. The fastest feedback loop is always \"in your head\".   \n  \nI built a domain-specific dashboard for the client. A collaborative environment where domain experts can play around with an agent, comment, feedback, etc. I took the idea from [Hamel Husain](https://x.com/HamelHusain) who said that [‚ÄúThe Most Important AI Investment is A Simple Data Viewer‚Äù.](https://x.com/i/status/1991903412997509372) He was damn right about it.   \n  \nThe last thing is something that is not talked much about but it should. We got hundreds of files about company knowledge. This knowledge is spread around big organisations like crazy. But if you really really understand the domain, if you really digest it all and ask a lot of questions, you‚Äôll be able to COMPRESS this knowledge. You‚Äôll find common stuff, remove dead ends, and really narrow it down to sth that expresses most about this company in smallest piece of text. This is your system prompt!! Why split context and add a potential point of failure if you can have MOST of the important stuff always in the system prompt? It‚Äôs crazy how well it works.  \n  \nOn the context engineering side we ended up with a great system prompt + a bunch of tools for getting info about products. I added one subagent for more complex stuff (routine building), but that was the only ‚Äúfancy‚Äù thing out there.  \n  \nI think the lesson here is that building agents is not hard on the technical level, and every developer can do it! The models do all the heavy lifting and they‚Äôre only getting better. The secret is understanding the domain and extracting the domain knowledge from people who know it. It's communication.\n\n  \nI'm curious:\n\nHave you built such \"customer support\"-related agents for your companies too? One thing that triggers me is amount of those giant SaaS companies that promises \"the super ultra duper ai agent\", and honestly? I think they don't have much secret sauce. Models are doing heavy lifting, and simple methods where heavy lifting is done by domain-specific knowledge trump general purpose ones. \n\nHere's what Malte from Vercel recently wrote btw:\n\nhttps://preview.redd.it/h2pjrjfix1jg1.png?width=1198&format=png&auto=webp&s=c8cd25ac93ee3a1b92cab153a1c591edbaf35d78\n\nIt somehow clicks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4ysvg2",
          "author": "HatApprehensive141",
          "text": "‚ÄúSecret sauce‚Äù = good prompts, domain tools, and actually understanding the business‚Ä¶ basically just doing your job properly.\n\nLots of companies hype up intergalactic RAG pipelines, but if you don‚Äôt compress real domain knowledge into a clear system, your agent is just an overconfident intern. The real edge isn‚Äôt the model magic, it‚Äôs the context quality.",
          "score": 7,
          "created_utc": "2026-02-12 11:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z7lpo",
          "author": "tom-mart",
          "text": "Wait till you discover the water is wet.",
          "score": 4,
          "created_utc": "2026-02-12 13:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ysv0w",
          "author": "kubrador",
          "text": "wow so the secret sauce was just... understanding the domain and writing good prompts. truly revolutionary stuff, might as well say the secret to cooking is using fresh ingredients and knowing what tastes good",
          "score": 2,
          "created_utc": "2026-02-12 11:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z2ese",
              "author": "rudzienki",
              "text": "I don't think simplicity is always obvious. There are many merchants of complexity out there who want to tell you otherwise.\n\nThat was the point of the post.",
              "score": 1,
              "created_utc": "2026-02-12 13:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51kyxj",
          "author": "nore_se_kra",
          "text": "Beyond the hype... interesting read despite the comments here. I dont think it hurts to tell the story of applied \"boring\" company specific domain knowledge one more time.",
          "score": 2,
          "created_utc": "2026-02-12 20:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51q54o",
          "author": "SamCRichard",
          "text": "What LLM did you use or are you routing between them\n\n",
          "score": 2,
          "created_utc": "2026-02-12 20:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55hde0",
              "author": "rudzienki",
              "text": "Main thread was optimized for latency so it was good-but-not-best model, sonnet territory.\n\nSubagent was supposed to reason over many products to analyse interactions, in this case we used the best reasoning model. Still was a bit too slow with max reasoning effort, so we ended up with the best model with mid reasoning effort.",
              "score": 1,
              "created_utc": "2026-02-13 12:19:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zc408",
          "author": "ampancha",
          "text": "You're right that domain knowledge compression matters more than complex RAG for quality. The gap I see in most \"it works\" agents is what happens at production scale: prompt injection attempts from real users, hallucinated product claims becoming liability, and cost spikes without per-user attribution. For a $1B brand those risks are where the actual work starts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-12 13:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bak7z",
          "author": "SeaOk5990",
          "text": "Nice work, I'd love practical tips on scaling personalization?",
          "score": 1,
          "created_utc": "2026-02-14 09:07:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r21425",
      "title": "I tracked how much time my team wastes re-explaining context. The number is embarrassing.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r21425/i_tracked_how_much_time_my_team_wastes/",
      "author": "arapkuliev",
      "created_utc": "2026-02-11 15:56:39",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 0.84,
      "text": "I got annoyed enough to actually measure this.  \n  \nFor two weeks I logged every time someone on my 6-person eng team had to re-explain context to someone else. Slack catch-ups, meeting recaps, \"here's what we decided and why\" conversations, onboarding someone into a thread.  \n  \n47 times per week. Average 8.5 minutes each. That's 6.5 hours/week of engineering time just... transferring context from one brain to another.  \n  \nAnd that's only the times someone *bothered*. How many times did someone just not explain, and the other person made a decision with half the picture?  \n  \nWe have Notion. We have Confluence. We have a wiki nobody reads. We have \"just search Slack\" which is a joke. None of it works because the context is either stale, unfindable, or so buried in noise that nobody trusts it.  \n  \nI've been experimenting with treating it as a memory problem instead of a documentation problem. Like, what if decisions and context got captured *as they happened* instead of someone writing a doc after the fact (which nobody does)?  \n  \nEarly results are promising but I'm curious: is anyone else measuring this? Or are we all just accepting \"let me catch you up\" as a normal part of work?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r21425/i_tracked_how_much_time_my_team_wastes/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4vfvzu",
          "author": "radarsat1",
          "text": "yes, humans talking to each other is a normal part of work.",
          "score": 14,
          "created_utc": "2026-02-11 21:35:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55yfuf",
              "author": "MannToots",
              "text": "I like when people succinctly prove they didn't read the whole thing.¬†¬†",
              "score": 0,
              "created_utc": "2026-02-13 14:01:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x7jvn",
          "author": "Nofoofro",
          "text": "It sounds like a culture, process and info architecture issue.¬†\n\nSo many of these posts have so little detail. So many words to say so little.",
          "score": 7,
          "created_utc": "2026-02-12 03:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vi0yz",
          "author": "WolfeheartGames",
          "text": "This is a problem of information theory and how communication works.",
          "score": 3,
          "created_utc": "2026-02-11 21:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50jv89",
          "author": "UncleRedz",
          "text": "May I ask how many products and/or feature areas your team is responsible for and the size of the team? Too big teams and self organization doesn't work, information starts to become siloed. Too many products and/or feature areas and the cognitive load increases to a point where it's simply not possible to keep track of everything by everyone. Then you need to split into sub-teams with specific areas of responsibilities, which you can only do if the team is big enough. So it's both organizational and resource constrainted. I guess you could call it a memory problem, but i'd say it's more about memory capacity then.",
          "score": 2,
          "created_utc": "2026-02-12 17:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z8bxu",
          "author": "PaddingCompression",
          "text": "This is my favorite part of vibe coding.\n\nLLMs are so obviously bad at context that I'm forced to be really good at explicit communication and organization of information.  These skills are just generally pretty useful in life.\n\nIt's like practicing delegation skills on hard mode.",
          "score": 1,
          "created_utc": "2026-02-12 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ztvpo",
          "author": "Mad_Tyrion",
          "text": "you are not alone :(",
          "score": 1,
          "created_utc": "2026-02-12 15:29:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5404zr",
          "author": "mimitwothree",
          "text": "Is that the problem or job stress? Let me out it this way. Your parents asked you what probably a billion time in a year to do something and u still didn't.\n\nShall I say lazy?\nCan't be bothered?\n\nNo bro. \n\nIt's just growing as a person . If this bothers u then trust me this is not the problem. \nThe problem is the job and you . \nRelax .",
          "score": 1,
          "created_utc": "2026-02-13 04:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56s9o2",
          "author": "gh0stwriter1234",
          "text": "Probably the most effective way to reduce this is have the correct person decide things... decentralized decision making is always going to be less efficient but it is also more robust at various levels as well as in distributing blame in the event of failure because instead of throwing one person under the bus and continuing to be bad everyone has to accept reality.\n\nSometimes being effective is counter productive in the long run because of the secondary effects.",
          "score": 1,
          "created_utc": "2026-02-13 16:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57bphs",
          "author": "siddu71",
          "text": "Don't optimize humans like robots or programming...",
          "score": 1,
          "created_utc": "2026-02-13 18:03:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b15ix",
          "author": "Illustrious-Echo1383",
          "text": "Culture problem. Amazon handles it nicely, in every meeting the organizer is expected to prepare a concise document on the topic of discussion. When meeting starts, 5 minutes is dedicated to read and ask silent questions (in the document). This does two things,\n1. Beings everyone up to speed without wasting too much time\n2. Drives the conversation forward without anyone realizing, by the silent q/a¬†",
          "score": 1,
          "created_utc": "2026-02-14 07:36:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4nj4y",
      "title": "16 single-file, zero-dependency implementations of the algorithms behind LLMs ‚Äî tokenization through speculative decoding. No frameworks, just the math.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/t4h1i8nbbhjg1.png",
      "author": "tom_mathews",
      "created_utc": "2026-02-14 15:32:01",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4nj4y/16_singlefile_zerodependency_implementations_of/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ho7tr",
          "author": "tom_mathews",
          "text": "The repo has been expanded from 16 to 30 scripts since the original post. Here's what's new:\n\n- **Foundations (7 ‚Üí 11):** Added BERT (bidirectional encoder), RNNs & GRUs (vanishing gradients + gating), CNNs (kernels, pooling, feature maps), GANs (generator vs. discriminator), VAEs (reparameterization trick), diffusion (denoising on point clouds), and an optimizer comparison (SGD vs. Momentum vs. RMSProp vs. Adam).\n\n- **Alignment (4 ‚Üí 9):** Added PPO (full RLHF reward ‚Üí policy loop), GRPO (DeepSeek's simplified approach), QLoRA (4-bit quantized fine-tuning), REINFORCE (vanilla policy gradients), Mixture of Experts (sparse routing), batch normalization, and dropout/regularization.\n\n- **Systems (5 ‚Üí 10):** Added paged attention (vLLM-style memory management), RoPE (rotary position embeddings), decoding strategies (greedy, top-k, top-p, beam, speculative ‚Äî all in one file), tensor & pipeline parallelism, activation checkpointing, and state space models (Mamba-style linear-time sequence modeling).\n\nSame constraints as before: every script is a single file, zero dependencies, trains and infers (or demonstrates forward-pass mechanics side-by-side), runs on CPU in minutes.\n\n[https://github.com/Mathews-Tom/no-magic](https://github.com/Mathews-Tom/no-magic)",
          "score": 1,
          "created_utc": "2026-02-15 11:10:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0lbvd",
      "title": "Mixture-of-Models routing beats single LLMs on SWE-Bench via task specialization",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "author": "botirkhaltaev",
      "created_utc": "2026-02-10 00:10:22",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been looking at per-task results on SWE-Bench Verified and noticed something that leaderboard averages hide: different models consistently solve¬†*different*¬†subsets of tasks.\n\nEven the top overall model on the leaderboard fails a non-trivial number of tasks that other models reliably solve, and the reverse is also true. This suggests strong task-level specialization rather than one model being strictly better.\n\nTo test this, I built a¬†**Mixture-of-Models architecture**, which is different from traditional routing that just defaults to the strongest aggregate model most of the time. The goal isn‚Äôt to route to a single model as often as possible, but to exploit complementary strengths between models.\n\nConcretely:\n\n* The problem description is embedded\n* It‚Äôs assigned to a semantic cluster (learned from general coding data, not SWE-Bench)\n* Each cluster has learned per-model success statistics\n* The task is routed to the historically strongest model for that¬†*type*¬†of problem\n\nImportantly, this does¬†**not**¬†route the top aggregate model for the majority of tasks. Several clusters consistently route to other models where they outperform it, even though it has the highest overall score.\n\nThere‚Äôs no new foundation model, no test-time search, and no repo execution, just a lightweight gating mechanism over multiple models.\n\nUsing this Mixture-of-Models setup, the system reaches 75.6% on SWE-Bench, exceeding single-model baselines (\\~74%). The takeaway isn‚Äôt the absolute number, but the mechanism: leaderboard aggregates hide complementary strengths, and mixture architectures can capture a higher ceiling than any single model.\n\nBlog with details and methodology here:¬†[https://nordlyslabs.com/blog/hypernova](https://nordlyslabs.com/blog/hypernova)\n\nGithub: the framework is open source !¬†[https://github.com/Nordlys-Labs/nordlys](https://github.com/Nordlys-Labs/nordlys)\n\nML/AI Research Community Discord:¬†[https://discord.gg/dqW7BBrq](https://discord.gg/dqW7BBrq)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2s3r4",
      "title": "I dont get mcp",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "author": "Yaar-Bhak",
      "created_utc": "2026-02-12 12:26:56",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 0.83,
      "text": "All I understood till now is - \n\nI'm calling an LLM api normally and now\nInstead of that I add something called MCP which sort of shows whatever tools i have? And then calls api \n\n\nI mean, dont AGENTS do the same thing? \n\nWhy use MCP? Apart from some standard which can call any tool or llm \n\nAnd I still dont get exactly where and how it works \n\nAnd WHY and WHEN should I be using mcp? \n\nI'm not understanding at all üò≠ Can someone please help\n\n",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4z3q6l",
          "author": "rudzienki",
          "text": "It's just a standardised way for companies to \"expose their tools\".\n\nIf you're Stripe you have a bunch of tools: \"do payment\", \"check invoices\" etc. If you want your agent to use them you can just... add them as tools to your agent. That's it. \n\nBut with MCP you can just say \"connect to stripe MCP\" and it automatically fetches all Stripe tools to be called. Stripe updates tools, you get update automatically.\n\nBut aside from that - no difference. \n\nBtw, MCP is much bigger protocol that handles more stuff than exposing tools, but in reality it's 99%, other uses didn't get much traction as far as I know.",
          "score": 7,
          "created_utc": "2026-02-12 13:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zrwup",
          "author": "fooz42",
          "text": "Service registry and discovery for remote procedure call is a wheel that gets reinvented every platform. It's not a revolution except in the sense the wheel gets reinvented every time the cycle turns, and now I'm getting dizzy from my metaphor.",
          "score": 12,
          "created_utc": "2026-02-12 15:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yy3mm",
          "author": "kubrador",
          "text": "mcp is basically \"what if we made tool use boring and standardized so literally any llm can talk to literally any tool without rewiring everything\" agents let your llm pick tools. mcp is the \\*protocol\\* so your llm doesn't need to know what tools exist. they just show up. it's the difference between \"here's a menu\" vs \"here's a standardized way to hand someone a menu\"\n\nyou need it when you're tired of writing custom integrations for every tool+llm combo. you don't need it if you're just bolting claude into your thing once and calling it a day.",
          "score": 5,
          "created_utc": "2026-02-12 12:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yzpvs",
              "author": "Yaar-Bhak",
              "text": "you think this can be used in production?\n\nand this means mcp would be used only in agentic flows right?",
              "score": 1,
              "created_utc": "2026-02-12 12:42:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54buzg",
          "author": "FoldedKatana",
          "text": "MCP is dead now. OpenClaw skills are where it's at",
          "score": 2,
          "created_utc": "2026-02-13 06:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0t1n",
          "author": "Cool_Fly_2030",
          "text": "The term ‚Äútool‚Äù in this thread is probably creating more confusion.\n\nMCP is a protocol to standardize how LLMs fetch context to ground and complete a task or generate a response more effectively.\n\nMCP servers are effectively APIs, where ‚Äútools‚Äù function as endpoints/routes to handle requests and execute logic on the server to do something and return the response to the LLM. \n\nWhen you register an MCP server in your client - VS code, Claude code, etc the LLM has a registry of tools to arbitrate between and delegate to if the description of the tool will solve its problem. \n\nThis is pretty powerful in agentic applications of LLMs because they can retrieve external context and perform actions in a fully automated loop.",
          "score": 2,
          "created_utc": "2026-02-14 19:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50wvs5",
          "author": "throwaway490215",
          "text": "MCPs are bullshit. They are a standard that basically tells the program run on your computer to prepend `some-tool --help` when you start a conversation, but with much more overhead, and **every conversation** even if you dont want to use `some-tool` this session. \n\nAnybody talking about credentials/authentication is a moron. \n\nJust add a \"Use `some-tool --help` to do X\" in your AGENTS.md and you're good.",
          "score": 2,
          "created_utc": "2026-02-12 18:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z5ouz",
          "author": "Astronos",
          "text": "it is function/tool calling over api",
          "score": 1,
          "created_utc": "2026-02-12 13:21:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zo2ke",
          "author": "Crafty_Disk_7026",
          "text": "MCP is just like an open ai spec the ai can read and know how to use your tool. It's literally just instruction manual",
          "score": 1,
          "created_utc": "2026-02-12 15:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tzq6",
          "author": "voidiciant",
          "text": "From what I understand:\n\nThe models have to be trained (and usually are, there is often a ‚Äûtool‚Äú tag on the downloads) to insert special keywords in their responses when a tool call is appropriate. \n\nThese keywords are intercepted by the runtime (the thing taking your input, converting to tokens,, etc) and the runtime performs the appropriate calls to the registered mcp tools (according to the protocol) and feeds back the tool-call results to the model, which in turn now incorporates them in the next response.\n\nAdditionally, and here I get fuzzy, the runtime generates a system prompt that contains a list of available MCP Tools, and the model is trained to understand this to generate the relevant keywords in the response based. \n\nMCP defines the protocols/API/formats. \n\nThat‚Äòs the gist for me",
          "score": 1,
          "created_utc": "2026-02-12 18:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51rpl9",
          "author": "CreepyValuable",
          "text": "MCP is kind of sort of a universal adapter to plug anything from ChatGPT to your toaster together.\n\nIt's not quite that straightforward and the actual interface is kind of clunky but it's pretty useful.\n\nFor example, my (not very good, but experimental so that's not important) AI uses it for things like a weather service, XiaoZhi AI esp support (essentially a smart speaker with a screen), VS Code integration and some other random things. It avoids needing a whole bunch of incompatible APIs.",
          "score": 1,
          "created_utc": "2026-02-12 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52svxj",
          "author": "Glum_Teaching8224",
          "text": "It's just tool using reference for the agent. ",
          "score": 1,
          "created_utc": "2026-02-13 00:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwht1",
          "author": "Electronic-Door7134",
          "text": "Good luck explaining to an auditor why your gave a 3rd party company full access to your company data (which is what happens without mcp)",
          "score": -3,
          "created_utc": "2026-02-12 15:41:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50avet",
              "author": "PaddingCompression",
              "text": "Wut",
              "score": 3,
              "created_utc": "2026-02-12 16:48:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2qfcz",
      "title": "Mix prompts instead of writing them by hand",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/5z7edpx9n1jg1.png",
      "author": "Everlier",
      "created_utc": "2026-02-12 10:55:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2qfcz/mix_prompts_instead_of_writing_them_by_hand/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r204ab",
      "title": "Intent Model",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "author": "Repulsive_Laugh_1875",
      "created_utc": "2026-02-11 15:18:54",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hi community,  \nthis is my first post here üôÇ\n\nI‚Äôm an experienced AI Engineer / AI DevOps Engineer / Consultant working for a well-known US-based company. I‚Äôd really appreciate your thoughts on a challenge I‚Äôm currently facing and whether you would approach it differently.\n\nUse-Case\n\nI‚Äôm building an **intent classifier** that must:\n\n* Run **on edge**\n* Stay around **\\~100ms latency**\n* Predict **1 out of 9 intent labels**\n* Consider **up to 2 previous conversation turns**\n\nThe environment is domain-specific (medical domain in reality), but to simplify, imagine a system controlling a car.\n\nExample:  \nYou have an intent like `lane_change`, and the user can request it in many different ways.\n\nCurrent Setup\n\n* Base model: **phi-3.5-mini-instruct**\n* Fine-tuned using **LoRA**\n* Model explicitly outputs only the intent token (e.g., `command_xyz`)\n* Each intent is mapped to a **single special token**\n* Almost no system prompt (removed to save tokens)\n\nPerformance:\n\n* \\~110ms latency (non-quantized) ‚Üí acceptable\n* \\~10 input tokens on average\n* \\~5 output tokens on average\n* 25k training samples\n* \\~95% accuracy\n\nSpeed is not the main issue ‚Äî I still have some room for token optimization and quantization if needed.\n\nthe real challenge -> the missing 5%.\n\nThe issue is **edge cases**.\n\nThe model operates in an open-input environment. The user can phrase requests in unlimited ways.\n\nFor example:  \nFor `lane_change`, there might be 30+ semantically equivalent variations. I built a synthetic data generation pipeline to create such variations and spent \\~2 weeks refining it. Evaluation suggests it's decent.\n\nBut:\n\nThere are still rare phrasings that the model hasn‚Äôt seen ‚Üí wrong intent prediction.\n\nOf course, I can:\n\n* Iteratively collect misclassifications\n* Add them to the training set\n* Retrain\n\nBut that‚Äôs slow and reactive.\n\nConstraints:\n\n* I could use a larger model (e.g., phi-4), and I‚Äôve tested it.\n* However, time-to-first-token for phi-4 is significantly slower.\n* Latency is more important than squeezing out a few extra percent of quality.\n\nSo scaling up model size isn‚Äôt ideal.\n\nMy questions to you:\n\nHow would you tackle the final 5%?\n\nI‚Äôd really appreciate hearing how others would approach this kind of edge, low-latency intent classification problem.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4tefqh",
          "author": "Swimming-Chip9582",
          "text": "Can you detect whether the output is likely to be an edge case or know when it's part of an uncertain category?  Perhaps you can fallback on a larger model and accept latency when it's unsure. So starting both the small and large concurrent, if the small finishes first and is all good just cancel the large one, if it's uncertain then wait for completion from the bigger model. ",
          "score": 4,
          "created_utc": "2026-02-11 15:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tll8h",
          "author": "FoodAccurate5414",
          "text": "You need to look into using very very very small models to handle edge cases. There are tons on hugging face. Run it along side your main model",
          "score": 2,
          "created_utc": "2026-02-11 16:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57q9",
              "author": "Repulsive_Laugh_1875",
              "text": "Can you recommend something or at least tell which ones you have in mind?",
              "score": 1,
              "created_utc": "2026-02-11 20:43:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t91xc",
          "author": "InfraScaler",
          "text": "Ok, so catching new ways of expressing the intent from your users is definitely reactive, but what about using bigger LLMs to generate those for you? Can you even use agents leveraging big LLMs to \"test\" and help prepare training for your system? Not necessarily cheap, but your employer may be able to afford it :-)",
          "score": 1,
          "created_utc": "2026-02-11 15:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tc616",
              "author": "Repulsive_Laugh_1875",
              "text": "It‚Äôs not the employer ‚Äî it‚Äôs the customer who has to pay for it üòâ\n\nJokes aside, thank you for your comment.\n\nI‚Äôm already using GPT-5.2-chat for the synthetic data generation. As mentioned, I‚Äôm currently achieving a full match rate  (intent plus parameters) of around 95%, and based on the latest metrics even closer to 97%, which I consider quite solid.\n\nThat‚Äôs why I don‚Äôt believe the data generation itself is the core issue here.\n\n  \n\\-------------------------- edited\n\n  \nin fact, I also thought about leveraging two agents to simulate such qustion answer things and try to figure out such edge cases. But this is costly.",
              "score": 2,
              "created_utc": "2026-02-11 15:38:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4thauy",
                  "author": "InfraScaler",
                  "text": ">But this is costly.\n\nNot for you! :P",
                  "score": 2,
                  "created_utc": "2026-02-11 16:02:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ybuxq",
                  "author": "BehindUAll",
                  "text": "Why is cost an issue? If you are using 2 classifiers, they are cheaper than LLMs and if you are using LLMs like Llama-3.1 8b or Mistral small etc. you will get great speed and is cheap too. Plus you can use groq, SambaNova or Cerebras for fast and cheap inference. And for LLMs the input will be like 10 words and output 1 word if you do your system prompt right. The cost goes way down.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:17:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vvjay",
          "author": "Charming_Support726",
          "text": "First I doubt that models with decoder only architecture are the best fit for this task.  They allways bring error classes which are unwanted in these scenarios.\n\nSecond I doubt, that you ever will reach 100% - but that's obvious\n\nThird: Encoder-Decoder architectures promise the best fit and efficiency for these tasks, but you'll never know ( we did back in 2020 over a 1000 intents with Rasa in a BERT-Style Intent detector went over 90% but appeared still flaky). T5Gemma Style Models could be a solution, but I got no experience on fine tuning them.\n\nFourth you could apply additional techniques like reranking or building a similarity distance to sample sentences to make sure that your generation is a valid result.  Maybe good to combine this approaches with multiple generations of the result.\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-11 22:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ysnco",
              "author": "Repulsive_Laugh_1875",
              "text": "To your fist point: I agree to this but I don¬¥t get what you mean by error classes? Do you mean just a wrong prediction label with this? If thats your point, you are right. These are the edge cases what I¬¥m talking about! It never responds in a different way, it just predicts sometimes the wrong label (edge cases).\n\n  \nTo your second point -> That is obvious and clear. Customer is already pretty satisfied with the solution, but I¬¥m not :D\n\n  \nthird: good hint. I need to look into them on how to really finetune them.\n\n  \nfourth: Indeed, I also wanted to test if you purely rely on similarity (history dependend intents excluded). I wanted to test if you can embedd those 25k and just detect the intent based on the similarity. I did something similar a few years back and this worked also very solid ;-) Now with the larger embedding representations, could also be an option.",
              "score": 1,
              "created_utc": "2026-02-12 11:51:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o51kkkq",
                  "author": "Charming_Support726",
                  "text": "Decoder only does not stick close to the input as encoder-decoder do. So you get all the things like hallucinations and similar. They also have issues generating \"i dont know\" tokens. \n\nIf you have an encoder part in the model like in a T5 the model is always working with a kind of embedding as knowledge representation for building a bridge to the decoder. \n\nThat's you are 90% there using an SLM, so these are my ideas to improve.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:24:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yfzud",
          "author": "tshawkins",
          "text": "If it is acceptable to have edge case resolution slower, you could push the cases that fail to resolve out to a bigger model, and push those requests and thier resolutions into a set of training data for your next primary model rebuild. Its a hybrid solution that uses both bigger model and retraining for the 5% only.",
          "score": 1,
          "created_utc": "2026-02-12 09:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u6y1l",
          "author": "TheBioto",
          "text": "Give Mistral 3b a shot and see how it works. \n\nI am currently doing something similar with endpoint nodes with small models. Mistral was the only one that was fast enough/could be guided enough to accomplish my needs.\n\nI don't have any suggestions for your edge cases issue, good luck!",
          "score": 1,
          "created_utc": "2026-02-11 18:01:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3hvkg",
      "title": "Observation: LLMs seem to have a \"Version 2.0\" bias when generating new UIs",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/rmkiv7qmh7jg1.jpeg",
      "author": "Routine_Connection8",
      "created_utc": "2026-02-13 06:29:02",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3hvkg/observation_llms_seem_to_have_a_version_20_bias/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r143o1",
      "title": "Cut Agent Cost By 90% By Fixing Context Bloat, Routing Models and Actually Using Caching [Claude]",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r143o1/cut_agent_cost_by_90_by_fixing_context_bloat/",
      "author": "thelastexcursion",
      "created_utc": "2026-02-10 15:40:15",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Started looking into this after my agent randomly fired off close to 1M tokens in a single request. Costs were unpredictable, rate limits were getting hammered, and I had no idea why.\n\nSo I instrumented everything, tracked token usage per action, and tested changes one at a time.\n\nLogged token counts per request, the model used, and the task it handled.\n\nRan multiple passes, compared baseline against each change, then cross-checked Anthropic's dashboard to see what actually got billed.\n\nAlso took screenshots of token graphs and fed them back into the agent to calibrate cost estimates. A bit meta, but it worked.\n\nKey finding #1: Context loading was the problem\n\nThe agent was loading all workspace and Slack session context on every single heartbeat. Once I stopped that, context-related tokens dropped \\~80%.\n\nAdded a \"new session\" command that archives old history in memory instead of resending it. Immediate difference.\n\nKey finding #2: Most tasks don't need the expensive model\n\nAfter classifying tasks, I routed about 75-85% of work to a cheap model, 10% to mid-tier, and only 3-5% to the expensive one.\n\nFile moves? CSV cleanup? Zero benefit from reasoning-heavy models.\n\nKey finding #3: Caching changed everything\n\nOne 6-hour multi-agent job was 95% cache-served and cost around $6 instead of an estimated $150.\n\nWhat surprised me most: the waste wasn't from complex tasks.\n\nIt was background behavior, heartbeats, retries, and context re-sends.\n\nOnce I controlled those, costs became predictable and boring.\n\nWhich is exactly what you want.\n\nToken efficiency isn't optimization. It's system design.\n\nCurious how others are handling cost control in agent setups.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r143o1/cut_agent_cost_by_90_by_fixing_context_bloat/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4n17b9",
          "author": "Fulgren09",
          "text": "Especially relevant when passing system prompts. Anthropic‚Äôs caching is unmatched. In services I design using Claude is strategic not just for quality but for this architectural feature.¬†",
          "score": 1,
          "created_utc": "2026-02-10 16:34:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n4nqs",
              "author": "thelastexcursion",
              "text": "Yes sir preach",
              "score": 1,
              "created_utc": "2026-02-10 16:50:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3ia0o",
      "title": "Launching Dhi-5B (compute optimally pre-trained from scratch)",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/iihkwu6rl7jg1.png",
      "author": "gradNorm",
      "created_utc": "2026-02-13 06:52:08",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3ia0o/launching_dhi5b_compute_optimally_pretrained_from/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54gqwa",
          "author": "gradNorm",
          "text": "Model available on HuggingFace: https://huggingface.co/Shaligram-Dewangan/Dhi-5B-Base",
          "score": 2,
          "created_utc": "2026-02-13 06:53:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o55pdgz",
          "author": "m98789",
          "text": "Can you publish your steps to create this from scratch? That may be more impactful than the model itself",
          "score": 1,
          "created_utc": "2026-02-13 13:11:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r36lxb",
      "title": "everyrow.io/screen: An intelligent pandas filter",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r36lxb/everyrowioscreen_an_intelligent_pandas_filter/",
      "author": "ddp26",
      "created_utc": "2026-02-12 21:50:27",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "(xpost from r/python)  \n  \nI extended pandas filtering to handle qualitative criteria you can't put in a `.query()` and screened 3600 job posts for remote friendly, senior roles with salaries disclosed**.**\n\nI built [everyrow.io/screen](http://everyrow.io/screen) ([docs](https://everyrow.io/docs/reference/SCREEN)), a Python SDK that adds qualitative operations to pandas DataFrames. The API pattern is: describe your criteria, pass in a DataFrame, get a DataFrame back, with all the LLM orchestration handled for you.\n\nHere's an example, filtering 3600 HN job posts for senior, remote-friendly, roles where the salaries are disclosed:\n\n    import asyncio\n    import pandas as pd\n    from pydantic import BaseModel, Field\n    from everyrow.ops import screen\n    \n    jobs = pd.read_csv(\"hn_jobs.csv\")  # 3,616 job postings\n    \n    class JobScreenResult(BaseModel):\n        qualifies: bool = Field(description=\"True if meets ALL criteria\")\n    \n    async def main():\n        result = await screen(\n            task=\"\"\"\n            A job posting qualifies if it meets ALL THREE criteria:\n    \n            1. Remote-friendly: Explicitly allows remote work, hybrid, WFH,\n               distributed teams, or \"work from anywhere\".\n    \n            2. Senior-level: Title contains Senior/Staff/Lead/Principal/Architect,\n               OR requires 5+ years experience, OR mentions \"founding engineer\".\n    \n            3. Salary disclosed: Specific compensation numbers are mentioned.\n               \"$150K-200K\" qualifies. \"Competitive\" or \"DOE\" does not.\n            \"\"\",\n            input=jobs,\n            response_model=JobScreenResult,\n        )\n    \n        qualified = result.data\n        print(f\"Qualified: {len(qualified)} of {len(jobs)}\")\n        return qualified\n    \n    qualified_jobs = asyncio.run(main())\n\nInterestingly, in early 2020, only 1.7% of job postings met all three criteria. By 2025, that number reached 14.5%.  \n  \nWithout using LLMs, the best you can do on this task is to keyword filter, e.g. for \"remote\", but this has a bunch of false positives for things like \"not remote!\"\n\nThe closest alternatives that use LLMs are probably LangChain-style chains where you write your own prompt and orchestrate the LLMs. But this example uses 3600 LLM calls (and everyrow supports web research agents), so this can get complex and expensive quickly.\n\n**Source code**: [github.com/futuresearch/everyrow-sdk](https://github.com/futuresearch/everyrow-sdk) \\- MIT licensed, Python 3.12+",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r36lxb/everyrowioscreen_an_intelligent_pandas_filter/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    }
  ]
}