{
  "metadata": {
    "last_updated": "2026-02-18 17:29:57",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 101,
    "file_size_bytes": 107616
  },
  "items": [
    {
      "id": "1r49we9",
      "title": "AI Developer Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/mhyf0n56qdjg1.png",
      "author": "Main-Fisherman-2075",
      "created_utc": "2026-02-14 03:29:03",
      "score": 255,
      "num_comments": 33,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r49we9/ai_developer_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5c7ogy",
          "author": "TheDeadlyPretzel",
          "text": "Instructor is not an agent framework, rather it is a structured output inference library.\n\nOn the other hand, Atomic Agents which was built on top of instructor IS an agent framework: [https://github.com/BrainBlend-AI/atomic-agents](https://github.com/BrainBlend-AI/atomic-agents)",
          "score": 2,
          "created_utc": "2026-02-14 13:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eza1p",
              "author": "Main-Fisherman-2075",
              "text": "thanks for point that out",
              "score": 0,
              "created_utc": "2026-02-14 22:38:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gvvsr",
          "author": "walkingbiscuit",
          "text": "For Agent Development missing Google ADK, and i don't know where you want to put Chrome browser now, since in the beta release it has WebMCP",
          "score": 1,
          "created_utc": "2026-02-15 06:37:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hzhwd",
          "author": "kubrador",
          "text": "looking at this like it's supposed help me pick a tool but it just makes me feel like i'm colorblind at a rave",
          "score": 1,
          "created_utc": "2026-02-15 12:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kxir2",
          "author": "afucher",
          "text": "Missing [ECA](https://eca.dev/)",
          "score": 1,
          "created_utc": "2026-02-15 22:00:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ncjp9",
          "author": "bl_builder",
          "text": "This reminds me of AdTech landscape back in the day, 2019. https://static-prod.adweek.com/wp-content/uploads/2018/04/luma-1200.png",
          "score": 1,
          "created_utc": "2026-02-16 07:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sjsnv",
          "author": "kovai_nvs",
          "text": "LLM newbie here. What tools would you use to analyse data to identify patterns?",
          "score": 1,
          "created_utc": "2026-02-17 01:53:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t8fbm",
          "author": "KongAtReddit",
          "text": "if you are doing design, you may also want to check out budgetpixel AI, it is like figma+AI steroid. ",
          "score": 1,
          "created_utc": "2026-02-17 04:29:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5thhef",
          "author": "Full-Signature8997",
          "text": "Parallel AI under web scraping too",
          "score": 1,
          "created_utc": "2026-02-17 05:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yw2g5",
          "author": "EvKoh34",
          "text": "https://posthog.com/ai",
          "score": 1,
          "created_utc": "2026-02-18 00:58:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5abluh",
          "author": "economicscar",
          "text": "Prime intellect missing under inference and compute",
          "score": 1,
          "created_utc": "2026-02-14 04:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br22k",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 0,
              "created_utc": "2026-02-14 11:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ccb1u",
                  "author": "Equity_Harbinger",
                  "text": "Can you share the latest one please (which is also less blurry, because when I zoom the image, words are blurry beyond recognition)\n\n\n(Thank you for your contributions)",
                  "score": 1,
                  "created_utc": "2026-02-14 14:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5atgtt",
          "author": "Live-Speech-1058",
          "text": "Antigravity?",
          "score": 1,
          "created_utc": "2026-02-14 06:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br5gi",
              "author": "Main-Fisherman-2075",
              "text": "I think I added it, I don't know why it's not there but definitely worth a try.",
              "score": 0,
              "created_utc": "2026-02-14 11:47:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bu98n",
          "author": "increasinglybold",
          "text": "Pi coding agent is great",
          "score": 1,
          "created_utc": "2026-02-14 12:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ezbgx",
              "author": "Main-Fisherman-2075",
              "text": "Will check it out",
              "score": 1,
              "created_utc": "2026-02-14 22:39:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bz0vl",
          "author": "Realistic-Damage2004",
          "text": "https://ainativedev.io/landscape\n\nHas been around for a while now. Is this published anywhere?",
          "score": 1,
          "created_utc": "2026-02-14 12:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5djaiy",
          "author": "Neferio1",
          "text": "Greptile is a very good code review tool",
          "score": 1,
          "created_utc": "2026-02-14 18:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ez8zy",
              "author": "Main-Fisherman-2075",
              "text": "1000%",
              "score": 1,
              "created_utc": "2026-02-14 22:38:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5fgo08",
              "author": "oXeNoN",
              "text": "How does it compare with other tools like CodeRabbit? Is CodeRabbit just spending more on marketing? üòÖ",
              "score": 0,
              "created_utc": "2026-02-15 00:24:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fuy08",
                  "author": "Neferio1",
                  "text": "We tested both and we choose Greptile over CodeRabbit just because Greptile can be ¬´¬†selfhosted¬†¬ª using Kubernetes or Docker. From a review perspective, Greptile and CodeRabbit are equivalent",
                  "score": 2,
                  "created_utc": "2026-02-15 01:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bn5o0",
          "author": "mcd0g",
          "text": "Warp under coding agents missing. They really need to step up their PR game",
          "score": 0,
          "created_utc": "2026-02-14 11:11:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br1u4",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 1,
              "created_utc": "2026-02-14 11:47:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bth4g",
          "author": "renntv",
          "text": "Great overview! Do you keep it on the web for linking, or just here on Reddit? ",
          "score": 0,
          "created_utc": "2026-02-14 12:07:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etdza",
              "author": "Main-Fisherman-2075",
              "text": "Hey I keep it here: but the content inside is still not very polished yet. https://www.keywordsai.co/market-map I will try to add all the description comparison price etc inside",
              "score": 1,
              "created_utc": "2026-02-14 22:05:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5c2b8u",
              "author": "Code_Exists_Here",
              "text": "Yeh same question from me.",
              "score": 0,
              "created_utc": "2026-02-14 13:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egtyh",
          "author": "funguslungusdungus",
          "text": "I need a link!",
          "score": 0,
          "created_utc": "2026-02-14 20:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etiuy",
              "author": "Main-Fisherman-2075",
              "text": "https://www.keywordsai.co/market-map here you go! I will try to update weekly and the content in it",
              "score": 1,
              "created_utc": "2026-02-14 22:06:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egxm5",
          "author": "Varqu",
          "text": "You can just use Claude Code.",
          "score": 0,
          "created_utc": "2026-02-14 20:57:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fkv68",
          "author": "Disastrous-Maybe2501",
          "text": "Mistral Vibe missing in coding agents",
          "score": 0,
          "created_utc": "2026-02-15 00:50:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6nw3e",
      "title": "AI Coding Agent Dev Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/gm88nuyrlxjg1.png",
      "author": "bhaktatejas",
      "created_utc": "2026-02-16 22:20:01",
      "score": 218,
      "num_comments": 24,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6nw3e/ai_coding_agent_dev_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ri3mi",
          "author": "bhaktatejas",
          "text": "link [https://www.morphllm.com/market-map](https://www.morphllm.com/market-map)",
          "score": 4,
          "created_utc": "2026-02-16 22:20:09",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5sbifa",
          "author": "btdeviant",
          "text": "It's weird how many of these guides and people are sleeping on [Strands](https://strandsagents.com/latest/). Hands down the most dead simple, capable provider agnostic agentic framework out there.. swings far above it's weight. ",
          "score": 4,
          "created_utc": "2026-02-17 01:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t9rlv",
              "author": "teambyg",
              "text": "Strands is also one of the smartest BETS from a future proofing perspective. Many of the small start up frameworks will die. Many probably very soon, so trusting in bigger names is likely to lead to long term viability (Lindy Effect). Provider frameworks, AWS, and the Pydantic team are probably the only one's I would consider right now for any enterprise application",
              "score": 2,
              "created_utc": "2026-02-17 04:38:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vjcfg",
                  "author": "echology-io",
                  "text": "thanks for the insight. I will check it out. ",
                  "score": 1,
                  "created_utc": "2026-02-17 15:02:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61bve5",
                  "author": "yeathatsmebro",
                  "text": "Vercel's AI SDK has someone that is 100% dedicated on the project and is not sketchy. Only if you use Typescript though.",
                  "score": 1,
                  "created_utc": "2026-02-18 11:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wst35",
              "author": "kabs1194",
              "text": "I've really appreciated LangGraph and my own custom context management, any thoughts on comparison with Strands?",
              "score": 1,
              "created_utc": "2026-02-17 18:42:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5swyy1",
              "author": "AdditionalWeb107",
              "text": "its yet another framework - and haven't we gotten pass this point that its just one while loop. The real hard part is the stuff around the loop",
              "score": 1,
              "created_utc": "2026-02-17 03:13:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t554z",
                  "author": "btdeviant",
                  "text": "Right. The salient point is its abstractions allow one to focus more on ‚Äúthe stuff around the loop‚Äù. \n\nIt‚Äôs a well designed framework and more tailored toward modern, multi-agent architectures compared to nearly all the others in that list, majority of which are relative dinosaurs and objectively a much bigger pain to work with for complex, code-first workflows. \n\nGive it a shot! I have no affiliation, just used most of them and found Strands a great blend of depth and breadth, especially with their (experimental) BIDI. Just a breeze to work with compared to all the others.",
                  "score": -1,
                  "created_utc": "2026-02-17 04:06:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rxdge",
          "author": "fredandlunchbox",
          "text": "No conductor?",
          "score": 1,
          "created_utc": "2026-02-16 23:42:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud25m",
              "author": "bhaktatejas",
              "text": "added!",
              "score": 1,
              "created_utc": "2026-02-17 10:22:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s84h7",
          "author": "skarpa10",
          "text": "I think Google ADK supposed to be GitHub Copilot SDK.",
          "score": 1,
          "created_utc": "2026-02-17 00:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uexh5",
              "author": "Darxeal",
              "text": "no, both exist",
              "score": 2,
              "created_utc": "2026-02-17 10:39:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sde7c",
          "author": "LoyalLittleOne",
          "text": "There's that many ?",
          "score": 1,
          "created_utc": "2026-02-17 01:15:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud2lc",
              "author": "bhaktatejas",
              "text": "theres even more",
              "score": 2,
              "created_utc": "2026-02-17 10:22:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v8jju",
              "author": "OkTry9715",
              "text": "AI slop is reproducing fast",
              "score": 1,
              "created_utc": "2026-02-17 14:05:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tydj1",
          "author": "j4ys0nj",
          "text": "Where would [Mission Squad](https://missionsquad.ai) go? What about OpenClaw?",
          "score": 1,
          "created_utc": "2026-02-17 08:03:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud3ez",
              "author": "bhaktatejas",
              "text": "wouldnt consider them coding agents, more general agents",
              "score": 1,
              "created_utc": "2026-02-17 10:23:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u6rb9",
          "author": "Varqu",
          "text": "What's the point of putting nvidia out there? ",
          "score": 1,
          "created_utc": "2026-02-17 09:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud64d",
              "author": "bhaktatejas",
              "text": "they have an inference service via brev. its not up to market standards. I've used it, but its getting better",
              "score": 2,
              "created_utc": "2026-02-17 10:23:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vvk60",
          "author": "Terrible-Rooster1586",
          "text": "I think ellipsis is dead sadly. I was an early adopter but they lost their CTO/cofounder to cursor and haven‚Äôt posted anything on linked in in months",
          "score": 1,
          "created_utc": "2026-02-17 16:02:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zxnmt",
          "author": "infraPulseAi",
          "text": "Interesting landscape. Curious how many of these tools handle deterministic verification and signed execution receipts for agent-to-agent transactions ‚Äî that layer feels missing in most stacks.",
          "score": 1,
          "created_utc": "2026-02-18 04:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s40kg",
          "author": "AdditionalWeb107",
          "text": "Missing the data plane for agentic apps. [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- cuts between the framework and gateway category as delivery infrastructure",
          "score": 1,
          "created_utc": "2026-02-17 00:20:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4ylja",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:59:37",
      "score": 34,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5fifzx",
          "author": "Vearres17",
          "text": "The fact that you kept Gemma3 sliding-window attention in FP8 is impressive. I've seen some implementations that fall back to fp16 for the local attention layers I guess it bcz it can be tricky to handle",
          "score": 1,
          "created_utc": "2026-02-15 00:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fjkgw",
              "author": "Educational_Cry_7951",
              "text": "thanks tbf it was a pain for me too at first",
              "score": 1,
              "created_utc": "2026-02-15 00:42:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hcpmr",
          "author": "Delicious-One-5129",
          "text": "This is seriously impressive work. An actual NVFP4 first path on RTX 4090 without silent FP16 fallback is huge for people squeezing every GB out of Ada cards. The VRAM savings vs FP16 are especially compelling.",
          "score": 1,
          "created_utc": "2026-02-15 09:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hwkm4",
              "author": "Educational_Cry_7951",
              "text": "Thank you! ",
              "score": 1,
              "created_utc": "2026-02-15 12:23:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6zfnj",
      "title": "How are they actually deployed in production at scale?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6zfnj/how_are_they_actually_deployed_in_production_at/",
      "author": "hareld10",
      "created_utc": "2026-02-17 07:16:05",
      "score": 28,
      "num_comments": 11,
      "upvote_ratio": 0.97,
      "text": "I‚Äôm trying to understand how giants LLMs systems like ChatGPT/Claude are deployed in production.\n\nSpecifically curious about:\n\n‚Ä¢ Inference stack (custom engine vs vLLM-like architecture?)  \n‚Ä¢ API behind  \n‚Ä¢ Database   \n‚Ä¢ GPU orchestration (Kubernetes? custom scheduler?)  \n‚Ä¢ Sharding strategy (tensor / pipeline parallelism?)  \n‚Ä¢ How latency is kept low under burst traffic  \n‚Ä¢ Observability + guardrail systems\n\nI know nobody has internal details, but based on public info, talks, papers, or experience deploying large models -  what‚Äôs the likely architecture?\n\nI'm asking because I want to prepare a knowledge kit for system design questions at this level.\n\nWould love input from people running 30B+ models in production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6zfnj/how_are_they_actually_deployed_in_production_at/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5twdrw",
          "author": "Once_ina_Lifetime",
          "text": "From what I have seen publicly, most large LLM deployments look like layered infra , optimized inference engines (vLLM/Triton/custom), heavy GPU orchestration with Kubernetes or internal schedulers, aggressive caching/batching for latency, and strong observability/guardrails on top. Exact details vary, but it‚Äôs basically a reliability + infra engineering problem more than just model serving.",
          "score": 6,
          "created_utc": "2026-02-17 07:44:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwoij",
              "author": "pmv143",
              "text": "Exactly. The interesting part is that once you solve model execution. thecomplexity shifts to orchestration and memory lifecycle management. That‚Äôs where most production pain seems to live.",
              "score": 2,
              "created_utc": "2026-02-17 12:56:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ue2ql",
          "author": "AdPutrid2974",
          "text": "That's the million-dollar question! Most likely a mix of custom C++ engines and massive Kubernetes clusters. Dealing with that level of burst traffic must be an engineering nightmare.",
          "score": 3,
          "created_utc": "2026-02-17 10:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5umpe4",
              "author": "hareld10",
              "text": "I want to construct prep kit to interviews, so its not have to be 1-1 :)",
              "score": 1,
              "created_utc": "2026-02-17 11:46:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5uwxel",
              "author": "pmv143",
              "text": "Probably a mix, yeah. Custom kernels and tight C++ runtimes make sense at that scale. But beyond the engine itself, I suspect a lot of the real complexity lives in scheduling, memory management, and how they handle burst traffic without fragmenting GPU memory.",
              "score": 1,
              "created_utc": "2026-02-17 12:58:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uiao1",
          "author": "kleinmatic",
          "text": "Read downtime post-mortems that tech companies publish after big outages. They‚Äôre always full of details on the exotic setups of very high scale systems. On GitHub look for danluu/post-mortems but there are others as well. They‚Äôre fascinating to read. \n\nWith that much money and scale I‚Äôm betting it‚Äôs way different and more custom than you think.",
          "score": 5,
          "created_utc": "2026-02-17 11:09:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tvbms",
          "author": "Abu_BakarSiddik",
          "text": "This is a very cool thing to learn about.\n\nI‚Äôm currently working on scaling our platform at the DB level, and it‚Äôs a completely different problem compared to scaling LLM inference. At the database layer, it mostly comes down to:\n\n* Managing connection lifecycle properly\n* Keeping transactions short\n* Handling long-lived sessions carefully (especially with streaming)\n* Using replicas effectively\n\nIf you mess up connection management, holdconnection hostage, everything falls apart. That‚Äôs usually the real bottleneck. With LLM systems, the bottleneck is about GPU compute and memory. The main things are:\n\n* Efficient batching of incoming requests\n* Maximizing GPU utilization\n* Managing KV cache memory properly\n* Supporting high concurrency\n\nModern frameworks like vLLM help a lot here. Things like paged attention, continuous batching, and FlashAttention make it possible to handle large numbers of concurrent requests efficiently. Memory management is critical, but these frameworks abstract a lot of that complexity away.\n\nSo DB scaling is mostly about connection discipline and replication strategy. LLM scaling is about batching efficiency and GPU orchestration.",
          "score": 3,
          "created_utc": "2026-02-17 07:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwe8m",
              "author": "pmv143",
              "text": "This is really well put. The ‚Äòdifferent bottlenecks, different failure modes‚Äô framing is key. With LLM systems you can have perfect API and DB hygiene and still fall apart purely due to KV cache pressure or poor batching under burst traffic.",
              "score": 2,
              "created_utc": "2026-02-17 12:54:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uvzet",
          "author": "pmv143",
          "text": "Nobody outside those orgs knows the exact internals, but based on public talks and production constraints, the architecture likely looks something like this:\n\n\t1.Inference Engine\n\nNot stock vLLM. Likely heavily customized runtime layers optimized for:\n‚Äì KV cache management\n‚Äì Scheduling + batching\n‚Äì Memory locality\n‚Äì Tensor + pipeline parallelism coordination\nvLLM concepts, but production hardened and deeply modified.\n\n\t2.GPU Orchestration\n\nKubernetes at the outer layer for cluster management.\nCustom schedulers at the GPU level.\nYou cannot rely on vanilla k8s scheduling when GPUs cost this much and memory is not oversubscribable.\n\n\t3.Sharding Strategy\nLarge models: tensor parallelism within a node, pipeline parallelism across nodes.\nMoE adds routing complexity.\nEverything optimized around minimizing cross node bandwidth.\n\n\t4.Latency Under Burst\n\nTwo strategies:\n‚Äì Keep massive pools warm at high utilization\n‚Äì Aggressive batching with tight admission control\nTrue scale to zero serverless does not really exist at this tier.\n\n\t5.API + Gateway Layer\nHigh performance stateless frontends\nQueueing + prioritization\nStreaming responses over HTTP/2 or gRPC\n\n\t6.Observability + Guardrails\nPer token tracing\nReal time safety filters\nShadow traffic for model eval\nCanary deployments for new weights\n\nThe hard part is not just loading the model.\nIt‚Äôs scheduling, memory, and utilization at scale.\n\nCold start optimization matters only if it works in production traffic, not just in a benchmark.",
          "score": 1,
          "created_utc": "2026-02-17 12:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8jf8",
          "author": "burntoutdev8291",
          "text": "Check out production stack helm chart",
          "score": 1,
          "created_utc": "2026-02-17 14:05:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o611jsj",
          "author": "GarbageOk5505",
          "text": "For burst traffic the answer is almost always overprovisioning plus request queuing with dynamic batching you're trading latency variance for throughput. Routing based on sequence length helps too, you don't want a 4-token request waiting behind a 32k context job.\n\nFor the system design prep angle: the Megatron-LM and PaLM papers are worth reading carefully, and Meta's LLaMA inference posts are surprisingly detailed about production tradeoffs.\n\n  \nfor the others I need to research furhter, good stuff pointing that out",
          "score": 1,
          "created_utc": "2026-02-18 10:11:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3kgpn",
      "title": "Rearchitecting LLMs ‚Äî pruning, distillation, and smaller domain models (MEAP)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "author": "ManningBooks",
      "created_utc": "2026-02-13 09:07:18",
      "score": 24,
      "num_comments": 19,
      "upvote_ratio": 0.91,
      "text": "Hi r/LLMDevs,\n\nStjepan from Manning here. The mods said it's ok if I post this here. \n\nWe‚Äôve just released a book that‚Äôs very much aimed at the kinds of problems this community discusses all the time: what to do when a general-purpose LLM is technically impressive but awkward, expensive, or inefficient for your actual use case.\n\n**Rearchitecting LLMs** by Pere Martra  \n[https://www.manning.com/books/rearchitecting-llms](https://hubs.la/Q042-hLy0)\n\n[Rearchitecting LLMs by Pere Martra](https://preview.redd.it/vyy079zx78jg1.jpg?width=2213&format=pjpg&auto=webp&s=755a8b1ab1320ede5daedfa861d6ab8d1b0c5e5d)\n\nThe core idea of the book is simple but powerful: instead of treating open models as fixed artifacts, you can reshape them. Pere walks through structural techniques like targeted fine-tuning, pruning, and knowledge distillation to build smaller, cheaper, domain-focused models that still perform well on the tasks you care about.\n\nWhat makes this book interesting is how hands-on it gets. You‚Äôre not working with abstract toy networks. The examples focus on modifying widely used open models, such as Llama-3, Gemma, and Qwen. The focus is on understanding which parts of a model actually contribute to behavior, how to identify waste or redundancy, and how to remove or compress components without blindly wrecking performance.\n\nThere‚Äôs also some genuinely thoughtful material on combining behavioral analysis with structural changes. Instead of just cutting parameters and hoping for the best, the book explores ways to reason about why a modification works or fails. One section that tends to spark discussion is ‚Äúfair pruning,‚Äù where pruning is used not only for efficiency but also to reduce bias at the neuron level.\n\nIf you‚Äôre working on local models, cost-constrained deployments, or specialized SLMs, this book is very much in that territory. It‚Äôs written for people who are comfortable with LLM concepts and want to go deeper into how models can be reshaped rather than simply prompted.\n\n**For the** r/LLMDevs **community:**  \nYou can get **50% off** with the code **MLMARTRA50RE**.\n\nA quick note on availability: the book is currently in **MEAP (Manning Early Access Program)**. That means you get immediate access to the chapters as they‚Äôre written, along with updates as the manuscript evolves.\n\nHappy to bring the author to answer questions about the book, the techniques it covers, or the kinds of readers it‚Äôs best suited for. And I‚Äôd be curious to hear from folks here who are already doing pruning or distillation in practice ‚Äî what‚Äôs been harder than expected?\n\nI'm ready to give away 5 ebooks to the first five commenters who share their experience here.\n\nThank you all for having us. It feels great to be here.\n\nCheers,",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o54xstz",
          "author": "StackSmashRepeat",
          "text": "Would you list some common problems and terminologies that the book covers? I'll have a look if it peaks my interest.",
          "score": 4,
          "created_utc": "2026-02-13 09:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5529pk",
              "author": "ManningBooks",
              "text": "Hey, thanks for asking. Here are some examples of what the book covers:\n\n\\- End-to-End Model Re-architecting (Chapter 2): Transform Gemma-3-270M using depth pruning and knowledge distillation for a 10% speed increase while retaining 93-98% of original capabilities in a hands-on project.\n\n\\- Data-Driven Pruning (Chapters 4-5): Create two models from the same base (Qwen3-0.6B or Llama-3.2-1B): one for formal texts (WikiText) and another for short messages (SMS Spam), using activation analysis with PyTorch hooks to highlight domain-specific component importance.\n\n\\- Bias Auditing and Correction: In ethics chapters, perform a model \"cleanup\" using ablation frameworks and PCA visualization to identify and mitigate demographic biases, achieving fairness without full retraining.\n\n\\- Mini-Capstone: Utilize a small \"draft model\" to speed up LLM inference by quickly proposing tokens, validated by a larger model.\n\n\\- Capstone Project: Migrate an agent system from costly external APIs to a specialized local Small Language Model (SLM).\n\nHope this helps.",
              "score": 7,
              "created_utc": "2026-02-13 10:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56j1zc",
                  "author": "StackSmashRepeat",
                  "text": "This is quite interesting; you're trying to move local models away from the static model while staying within the static framework? I could basically train a model for my iPhone, let's say I export all my email and scrub PII, format for training data and then I could fine-tune to write mails that look somewhat within the realm of my own style? \n\nI haven't looked into training or fine tuning as I couldn't think of a personal use case for these tiny models, but like you're saying \"domain-focused\", gave a clearer picture.\n\nThis is a little over my current scope as I'm not even sure if I understood this correctly, but I've been thinking of ways to make a digital twin that could handle writing across multiple platforms. Was always thinking Id need a larger model to handle such a task because it sounds easy enough, but capturing the essence of one's writing is quite a complex task for llms. At least in my experience.\n\nThanks for the info.",
                  "score": 3,
                  "created_utc": "2026-02-13 15:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57f10h",
              "author": "pmartra",
              "text": "Hi u/StackSmashRepeat \n\nI'll give you a rough explanation and if you want more details just ask me. \n\nIn the book I explain an LLM optimization / customization pipeline. The pipeline basically consists of: \n\n1- Pruning (depth and width) which is removing parts of a model. \n\n2- Knowledge Distillation. Recovering the lost knowledge by transferring it from the base model to the pruned one. \n\n3- Finetuning on a specialized domain. \n\nDuring this pipeline you gain knowledge about how models work internally, since to decide which parts to remove we study how activations are produced detecting the mos important parts. \n\nWe take advantage of this knowledge to do surgical operations on the model in the more advanced chapters, like replacing attention layers or changing the behaviour of the model modifying the weights of some neurons. \n\nThe pipeline is very similar to what companies like Nvidia or Mistral follow to create their model families, but adapted to create specific models,  and using less data and processing capacity than they have.   \n  \nFor example in width pruning Mistral uses a completely dynamic model to detect which weights to remove, in the book we use a combination that rewards keeping neurons with high importance, so with much less data you can get an efficient model. \n\nAlthough it really seems very complicated we start from the simplest things, basic depth pruning, in chapter 2 you already remove parts of a model and recover the lost knowledge. \n\nFrom this base you build the knowledge that leads you to the more advanced techniques. \n\nThen there's a second intention which is to make cutting-edge research understandable, so in each chapter starting from the fourth, there's an explanation of which papers the chapter's code is based on and how we've adapted it when implementing it. \n\nFor example the development of width pruning in modern models like Llama or Gemma is based on a paper which we've changed a good part of the formulas to simplify it but keeping its general idea. \n\nI hope the explanation was useful! \n\nPere.",
              "score": 3,
              "created_utc": "2026-02-13 18:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fsrna",
                  "author": "h8mx",
                  "text": "Hey, your comment on this thread was auto-removed by Reddit as spam, but I approved it. Thanks for your input!",
                  "score": 3,
                  "created_utc": "2026-02-15 01:42:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57qobq",
          "author": "marm_alarm",
          "text": "I'm a subscriber to Manning and so I have access to all the MEAP content.  I am very interested in reading this book and will post my review here after I've taken a look!",
          "score": 3,
          "created_utc": "2026-02-13 19:14:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623gg8",
              "author": "ManningBooks",
              "text": "Thank you. We appreciate it.",
              "score": 1,
              "created_utc": "2026-02-18 14:26:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qph7o",
          "author": "Glovali",
          "text": "I would love to read this and get into LLM development!",
          "score": 3,
          "created_utc": "2026-02-16 19:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623h1l",
              "author": "ManningBooks",
              "text": "Happy to send you a copy. Please DM me your full name and your email address. Thanks.",
              "score": 2,
              "created_utc": "2026-02-18 14:26:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6278pu",
                  "author": "Glovali",
                  "text": "I sent the DM!",
                  "score": 1,
                  "created_utc": "2026-02-18 14:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57u5v0",
          "author": "dextoz",
          "text": "Would love a copy and meap along!",
          "score": 2,
          "created_utc": "2026-02-13 19:31:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623ehv",
              "author": "ManningBooks",
              "text": "Happy to send you a copy. Please DM me your full name and your email address. Thanks.",
              "score": 1,
              "created_utc": "2026-02-18 14:26:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b0dln",
          "author": "boredaadvark",
          "text": "Excited for this and this resonates to what I want to explore. Keen on getting a copy. How complete is this book in terms of percentage?",
          "score": 2,
          "created_utc": "2026-02-14 07:29:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6n2y",
              "author": "pmartra",
              "text": "Hi, at this moment there are just 2 chapters, the third will be published next week. ",
              "score": 3,
              "created_utc": "2026-02-14 16:58:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o623d8r",
              "author": "ManningBooks",
              "text": "Happy to send you a copy. Please DM me your full name and your email address. Thanks.",
              "score": 1,
              "created_utc": "2026-02-18 14:25:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6rzah",
      "title": "AI Coding Agent Dev Tools 2026 (Updated)",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/syaar38yfyjg1.png",
      "author": "bhaktatejas",
      "created_utc": "2026-02-17 01:08:11",
      "score": 19,
      "num_comments": 7,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6rzah/ai_coding_agent_dev_tools_2026_updated/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5tl122",
          "author": "sogo00",
          "text": "There is so much wrong with this",
          "score": 1,
          "created_utc": "2026-02-17 06:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tlepy",
              "author": "bhaktatejas",
              "text": "tell me! i'll update it ",
              "score": 1,
              "created_utc": "2026-02-17 06:07:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ua01w",
                  "author": "sogo00",
                  "text": "Look you have clearly asked an LLM to produce this for you and half of it is wrong.\n\nIf you want something correct, start to google each headline, understand what it means and then read about each icon/text underneath it.",
                  "score": 1,
                  "created_utc": "2026-02-17 09:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6219jt",
          "author": "resiros",
          "text": "I never understood the value of these maps, other than for investors. You can't even click on things. ",
          "score": 1,
          "created_utc": "2026-02-18 14:15:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5nil3",
      "title": "I built a CLI that extracts design systems from any live website",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "author": "Every_Chicken_1293",
      "created_utc": "2026-02-15 19:26:36",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "I kept running into the same problem: I'd see a website I liked and want to build something with a similar design, but manually inspecting every color, font, spacing value, and component pattern was tedious.\n\nSo I built design-memory. You point it at a URL and it:  \n  \n\\- Crawls the page with Playwright  \n\\- Extracts colors, typography, spacing, border radius, elevation  \n\\- Captures all CSS custom properties (often 500-700+ variables)  \n\\- Detects Tailwind usage and top utility patterns  \n\\- Uses an LLM to interpret component recipes and layout structure  \n\\- Outputs a .design-memory/ folder of markdown files\n\nThe output is structured so you can paste it into Claude, Cursor, or ChatGPT and get a faithful recreation of the original design.\n\nIt also supports learning from screenshots, multi-page crawls, and diffing two design systems.\n\nSource: [https://github.com/memvid/design-memory](https://github.com/memvid/design-memory)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5lfrsg",
          "author": "NeverSkipSleepDay",
          "text": "Have you ever seen what a design system looks like?",
          "score": 2,
          "created_utc": "2026-02-15 23:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5min2v",
              "author": "salasi",
              "text": "Lmao",
              "score": 1,
              "created_utc": "2026-02-16 03:47:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nl10m",
          "author": "WhoTookPlasticJesus",
          "text": "Not to be overly critical, but not even your demo screenshots are alike. The fonts are wrong, icons weren't copied, an entire fucking navbar has gone missing...\n\nI'm also not sure at all what this has to do with LLMs",
          "score": 1,
          "created_utc": "2026-02-16 09:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s3a9r",
          "author": "o1got",
          "text": "This is really cool. I've been on the receiving end of this problem from a different angle - we see a ton of AI agents crawling websites now, and the ones that actually extract design tokens and structured CSS tend to perform way better than the ones just scraping raw HTML.\n\nHow are you handling responsive design patterns? Like if a site has completely different component structure at mobile vs desktop, does it capture both states or does Playwright just grab whatever viewport you set?  \nAlso the diffing feature is interesting. I wonder if that could be useful for tracking how design systems evolve over time, like crawling the same site every few months and seeing what changed. Could be a neat way to learn from how mature products iterate on their UI.",
          "score": 1,
          "created_utc": "2026-02-17 00:16:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7momq",
      "title": "Clawdbot/Moltbot/OpenClaw is a security disaster waiting to happen",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r7momq/clawdbotmoltbotopenclaw_is_a_security_disaster/",
      "author": "thecreator51",
      "created_utc": "2026-02-17 23:38:04",
      "score": 14,
      "num_comments": 16,
      "upvote_ratio": 0.89,
      "text": "I was more excited about AI agent frameworks than I was when LLMs first dropped. The composability, the automation, the skill ecosystem - it felt like the actual paradigm shift.\n\nLately though I'm genuinely worried. We can all be careful about which skills we install, sure. But most people don't realize skills can silently install other skills. No prompt, no notification, no visibility. One legitimate-looking package becomes a dropper for something else entirely, running background jobs you'll never see in your chat history.\n\nWhat does a actually secure OpenClaw implementation even look like? Does one exist?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r7momq/clawdbotmoltbotopenclaw_is_a_security_disaster/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5ywnls",
          "author": "Strong_Worker4090",
          "text": "I don‚Äôt think the concern is overblown. If skills can silently install other skills and run background jobs with no visibility, that‚Äôs a real supply chain and privilege boundary problem.\n\nThe way I think about it is this: don‚Äôt treat the agent like a helpful assistant. Treat it like the smartest hacker in the world who happens to be following instructions most of the time.\n\nIf you assume that, a \"secure\" implementation looks very different from the default hobby setup.\n\nFirst, the model shouldn‚Äôt have direct power. It shouldn‚Äôt have raw network access, raw filesystem access, or ambient credentials sitting in environment variables. It should only be able to request actions.\n\nSecond, every capability should be explicitly defined and allowlisted. No silent skill installs. No transitive dependency installs at runtime. If something gets added, it happens in a controlled build step with review and version pinning.\n\nThird, all external effects should go through a choke point you control. If it wants to send an email, make an HTTP request, write to a database, or touch Slack, it calls a guarded tool. That tool enforces policy, rate limits, domain restrictions, and writes to an immutable audit log. No raw SMTP. No arbitrary outbound HTTP.\n\nFourth, assume it will try to exfiltrate if it can. That means default deny on network egress, strict sandboxing, and strong logging that lives outside the agent runtime.\n\nIs there a \"perfect\" secure setup that still keeps full utility? Probably not. The more useful the agent is, the more power it needs. The goal isn‚Äôt perfection, it‚Äôs constrained, mediated power with visibility and revocability.\n\nSo I wouldn‚Äôt say these frameworks are doomed. I‚Äôd say most default installs are way too permissive for production. A secure OpenClaw implementation would look less like a plugin playground and more like a tightly sandboxed execution engine with a policy layer in front of every meaningful action.",
          "score": 7,
          "created_utc": "2026-02-18 01:01:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60idec",
              "author": "Traditional-Set6848",
              "text": "Nicely put!¬†",
              "score": 1,
              "created_utc": "2026-02-18 07:13:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o61l3gs",
              "author": "GCoderDCoder",
              "text": "Agreed. To their credit, tons of folks were excited to help something like this and made it a totally different project IMO. There's now various ways to increase isolation. I have been using a lot of reactive tools that require me initiating everything so Im enticed by the proactive nature of open claw so I see it as starting with proactive features I want to make a more deterministic implementation of. That said, I was trashing the initial release but i think it is usable for personal setups for informed users now.\n\nI originally planned a series of vms with separate controls but there are several layers of isolation included now in open claw. I'm still configuring an internal only container and an external accessible vm install because I wanted to be as conservative as possible assuming the external facing one will be contaminated but sandboxing seems legitimately incorporated in open claw now where it's not irresponsible now to have one gateway with different agent profiles to achieve similar separation IMO. \n\nBoth my instances are locked down to only be able to use tools I give them. The external one has no access to any sensitive info but can gather information and put it together for me to allow it to save it in my tools. But until I approve it everything is ephemeral in the container sandbox. The second one is internal only with a lot of read only access but any write requires my approval. So that allows it to proactively review solutions without compromising my tools.\n\nInsider threat is always the most dangerous and even more so with tools that replicate human logic but lack the ability to actually think.",
              "score": 1,
              "created_utc": "2026-02-18 12:44:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yp6vs",
          "author": "Interesting-Law-8815",
          "text": "Waiting to happen?   I think it‚Äôs already happened!",
          "score": 11,
          "created_utc": "2026-02-18 00:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zrcum",
          "author": "Vusiwe",
          "text": "Don‚Äôt know much about it, but it really sounds like 2023‚Äôs AutoGPT, only running with root permissions, with network access turned on\n\nGG",
          "score": 3,
          "created_utc": "2026-02-18 03:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z05j8",
          "author": "crankthehandle",
          "text": "are there any crazy stories that have happened with openclaw? Looks like moltbook was the way bigger fuck up.",
          "score": 2,
          "created_utc": "2026-02-18 01:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yiaxd",
          "author": "kubrador",
          "text": "you're describing dependency hell with god mode. the answer to \"what does secure look like\" is probably \"don't let untrusted code execute arbitrary actions\" which, yeah, solves the problem by making the whole thing pointless.",
          "score": 1,
          "created_utc": "2026-02-17 23:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5za5z6",
          "author": "BrianJThomas",
          "text": "I feel the same way about crates.io, to be fair.",
          "score": 1,
          "created_utc": "2026-02-18 02:11:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zcy7t",
          "author": "wally659",
          "text": "I feel like a \"security disaster\" requires some suggestion of \"security\" to begin with. Saying the OpenClaw platform is a security risk is a bit like saying underwater cave exploration is dangerous.",
          "score": 1,
          "created_utc": "2026-02-18 02:26:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zgtpy",
              "author": "NoleMercy05",
              "text": "![gif](giphy|VBmRD9W9HwTLmGLz34)",
              "score": 1,
              "created_utc": "2026-02-18 02:47:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o603e1p",
          "author": "Civil_Tea_3250",
          "text": "And OpenAI just hired the guy that made it. Because he made such a great product lol\n\nSeriously, can we stop this now? Like, right now.",
          "score": 1,
          "created_utc": "2026-02-18 05:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o609ury",
          "author": "No_Success3928",
          "text": "I'm excited about making bank fixing things :D\n\n",
          "score": 1,
          "created_utc": "2026-02-18 06:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60s18g",
          "author": "sogo00",
          "text": "I remember there was one of the top skill that was installing some malware...\n\nBTW - that is not limited to this type of agent - all claude/other agent skills you find on the web are unaudited and even if the author is trustful someone can hijack the repo. That especially applies to the big all-included skill where the maintainer collects other peoples skills...",
          "score": 1,
          "created_utc": "2026-02-18 08:43:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60xb6w",
          "author": "Zeikos",
          "text": "Nobody is waiting",
          "score": 1,
          "created_utc": "2026-02-18 09:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o611f8a",
          "author": "Loud-Option9008",
          "text": "This is the thing that worries me more than jailbreaks or prompt injection in isolation. Silent skill installation is a supply chain attack surface and most users have no idea it exists. You're not just trusting the skill, you're trusting everything that skill decides to pull in at runtime.\n\nA \"secure\" OpenClaw implementation would need at minimum: process-level isolation per skill so a compromised package can't read memory or environment variables from the agent runtime, network egress controls so background jobs can't phone home, and some kind of attestation that what's running matches what you installed. None of that exists out of the box.\n\nThe deeper issue is that the whole skill ecosystem is built on implicit trust. Skills run in the same execution context as the agent, which means they have access to everything the agent has access to  credentials, session tokens, whatever's in the environment. A dropper skill doesn't need to escalate privileges, it already has them.\n\nDocker helps at the surface level but shared kernel is a real limitation here  if a skill finds a kernel exploit, the container boundary doesn't save you. The honest answer is that a properly isolated implementation needs the skill execution to happen in a separate environment with explicit, audited permissions for every outbound action. Most people are nowhere near that and don't realize it.",
          "score": 1,
          "created_utc": "2026-02-18 10:10:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zr3ns",
          "author": "zZaphon",
          "text": "This is where AI Governance Software would be useful. For example\n\nhttps://factara.fly.dev",
          "score": 0,
          "created_utc": "2026-02-18 03:48:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7d29g",
      "title": "PlaceboBench: New benchmark on SOTA LLM hallucinations in pharma",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/3bzaibbzd3kg1.jpeg",
      "author": "aiprod",
      "created_utc": "2026-02-17 17:45:34",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r7d29g/placebobench_new_benchmark_on_sota_llm/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5wzh3c",
          "author": "Competitive-Garage-4",
          "text": "Do not ask software engineer to recommend you pills.",
          "score": 2,
          "created_utc": "2026-02-17 19:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xg2je",
              "author": "aiprod",
              "text": "Haha yes, while I prefer Opus for coding, I guess Gemini would be the better pharmacist",
              "score": 2,
              "created_utc": "2026-02-17 20:32:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2ree2",
      "title": "Lessons from building AI shopping assistant for 1B$+ skincare brand.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "author": "rudzienki",
      "created_utc": "2026-02-12 11:50:51",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 0.85,
      "text": "Hey! I was recently hired to build an AI shopping assistant for a huge brand, 1B$+ in revenue. Unfortunately can't say which one is it (damn NDAs), but I thought I'd share some lessons. After the project CTO told me ‚ÄúWorking with you was the best AI investment in the last year‚Äù, so I guess it went well!\n\nI'm reposting this from my linkedin, so sorry for this \"linkedinish\" vibe:\n\nThe biggest secret was, surprise, surprise, **not** wasn‚Äôt fancy AI methods, complex RAG pipelines, and multi step workflows. In the end it was good prompts, a bunch of domain-specific tools and one subagent.  \n  \nThe secret was the process.  \n  \nI didn‚Äôt know anything about skincare so I had to learn about it. Even light understanding of the domain turned out EXTREMELY IMPORTANT since it allowed m to play around with an agent and have a good judgement whether it says good things. The fastest feedback loop is always \"in your head\".   \n  \nI built a domain-specific dashboard for the client. A collaborative environment where domain experts can play around with an agent, comment, feedback, etc. I took the idea from [Hamel Husain](https://x.com/HamelHusain) who said that [‚ÄúThe Most Important AI Investment is A Simple Data Viewer‚Äù.](https://x.com/i/status/1991903412997509372) He was damn right about it.   \n  \nThe last thing is something that is not talked much about but it should. We got hundreds of files about company knowledge. This knowledge is spread around big organisations like crazy. But if you really really understand the domain, if you really digest it all and ask a lot of questions, you‚Äôll be able to COMPRESS this knowledge. You‚Äôll find common stuff, remove dead ends, and really narrow it down to sth that expresses most about this company in smallest piece of text. This is your system prompt!! Why split context and add a potential point of failure if you can have MOST of the important stuff always in the system prompt? It‚Äôs crazy how well it works.  \n  \nOn the context engineering side we ended up with a great system prompt + a bunch of tools for getting info about products. I added one subagent for more complex stuff (routine building), but that was the only ‚Äúfancy‚Äù thing out there.  \n  \nI think the lesson here is that building agents is not hard on the technical level, and every developer can do it! The models do all the heavy lifting and they‚Äôre only getting better. The secret is understanding the domain and extracting the domain knowledge from people who know it. It's communication.\n\n  \nI'm curious:\n\nHave you built such \"customer support\"-related agents for your companies too? One thing that triggers me is amount of those giant SaaS companies that promises \"the super ultra duper ai agent\", and honestly? I think they don't have much secret sauce. Models are doing heavy lifting, and simple methods where heavy lifting is done by domain-specific knowledge trump general purpose ones. \n\nHere's what Malte from Vercel recently wrote btw:\n\nhttps://preview.redd.it/h2pjrjfix1jg1.png?width=1198&format=png&auto=webp&s=c8cd25ac93ee3a1b92cab153a1c591edbaf35d78\n\nIt somehow clicks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4ysvg2",
          "author": "HatApprehensive141",
          "text": "‚ÄúSecret sauce‚Äù = good prompts, domain tools, and actually understanding the business‚Ä¶ basically just doing your job properly.\n\nLots of companies hype up intergalactic RAG pipelines, but if you don‚Äôt compress real domain knowledge into a clear system, your agent is just an overconfident intern. The real edge isn‚Äôt the model magic, it‚Äôs the context quality.",
          "score": 7,
          "created_utc": "2026-02-12 11:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z7lpo",
          "author": "tom-mart",
          "text": "Wait till you discover the water is wet.",
          "score": 4,
          "created_utc": "2026-02-12 13:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ysv0w",
          "author": "kubrador",
          "text": "wow so the secret sauce was just... understanding the domain and writing good prompts. truly revolutionary stuff, might as well say the secret to cooking is using fresh ingredients and knowing what tastes good",
          "score": 2,
          "created_utc": "2026-02-12 11:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z2ese",
              "author": "rudzienki",
              "text": "I don't think simplicity is always obvious. There are many merchants of complexity out there who want to tell you otherwise.\n\nThat was the point of the post.",
              "score": 1,
              "created_utc": "2026-02-12 13:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51kyxj",
          "author": "nore_se_kra",
          "text": "Beyond the hype... interesting read despite the comments here. I dont think it hurts to tell the story of applied \"boring\" company specific domain knowledge one more time.",
          "score": 2,
          "created_utc": "2026-02-12 20:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51q54o",
          "author": "SamCRichard",
          "text": "What LLM did you use or are you routing between them\n\n",
          "score": 2,
          "created_utc": "2026-02-12 20:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55hde0",
              "author": "rudzienki",
              "text": "Main thread was optimized for latency so it was good-but-not-best model, sonnet territory.\n\nSubagent was supposed to reason over many products to analyse interactions, in this case we used the best reasoning model. Still was a bit too slow with max reasoning effort, so we ended up with the best model with mid reasoning effort.",
              "score": 1,
              "created_utc": "2026-02-13 12:19:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zc408",
          "author": "ampancha",
          "text": "You're right that domain knowledge compression matters more than complex RAG for quality. The gap I see in most \"it works\" agents is what happens at production scale: prompt injection attempts from real users, hallucinated product claims becoming liability, and cost spikes without per-user attribution. For a $1B brand those risks are where the actual work starts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-12 13:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bak7z",
          "author": "SeaOk5990",
          "text": "Nice work, I'd love practical tips on scaling personalization?",
          "score": 1,
          "created_utc": "2026-02-14 09:07:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vzrey",
          "author": "gardenia856",
          "text": "Your main insight is dead on: the real moat is compressed domain knowledge, not some 18-hop agent graph.\n\nWhat you describe as ‚Äúcompressing‚Äù org knowledge into a sharp system prompt is basically doing the product thinking and ontology work nobody wants to do. I‚Äôve had the same experience: once you‚Äôve read the internal docs, sat with support and sales, and boiled everything into a few pages of ‚Äúhow this company actually thinks,‚Äù retrieval becomes just a safety net instead of the main act.\n\nThe dashboard piece is underrated too. Giving domain experts a sandbox where they can poke the agent, leave comments, and iterate on that compressed spec is worth way more than another custom reranker.\n\nOn the tooling front, I‚Äôve bounced between Intercom, Zendesk bots, and Pulse for Reddit for catching and answering real user questions where they hang out, and the stuff that works is always: tight prompts, good tools, short paths, no ego about using simple patterns.",
          "score": 1,
          "created_utc": "2026-02-17 16:23:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2s3r4",
      "title": "I dont get mcp",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "author": "Yaar-Bhak",
      "created_utc": "2026-02-12 12:26:56",
      "score": 9,
      "num_comments": 14,
      "upvote_ratio": 0.84,
      "text": "All I understood till now is - \n\nI'm calling an LLM api normally and now\nInstead of that I add something called MCP which sort of shows whatever tools i have? And then calls api \n\n\nI mean, dont AGENTS do the same thing? \n\nWhy use MCP? Apart from some standard which can call any tool or llm \n\nAnd I still dont get exactly where and how it works \n\nAnd WHY and WHEN should I be using mcp? \n\nI'm not understanding at all üò≠ Can someone please help\n\n",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4z3q6l",
          "author": "rudzienki",
          "text": "It's just a standardised way for companies to \"expose their tools\".\n\nIf you're Stripe you have a bunch of tools: \"do payment\", \"check invoices\" etc. If you want your agent to use them you can just... add them as tools to your agent. That's it. \n\nBut with MCP you can just say \"connect to stripe MCP\" and it automatically fetches all Stripe tools to be called. Stripe updates tools, you get update automatically.\n\nBut aside from that - no difference. \n\nBtw, MCP is much bigger protocol that handles more stuff than exposing tools, but in reality it's 99%, other uses didn't get much traction as far as I know.",
          "score": 8,
          "created_utc": "2026-02-12 13:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zrwup",
          "author": "fooz42",
          "text": "Service registry and discovery for remote procedure call is a wheel that gets reinvented every platform. It's not a revolution except in the sense the wheel gets reinvented every time the cycle turns, and now I'm getting dizzy from my metaphor.",
          "score": 14,
          "created_utc": "2026-02-12 15:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yy3mm",
          "author": "kubrador",
          "text": "mcp is basically \"what if we made tool use boring and standardized so literally any llm can talk to literally any tool without rewiring everything\" agents let your llm pick tools. mcp is the \\*protocol\\* so your llm doesn't need to know what tools exist. they just show up. it's the difference between \"here's a menu\" vs \"here's a standardized way to hand someone a menu\"\n\nyou need it when you're tired of writing custom integrations for every tool+llm combo. you don't need it if you're just bolting claude into your thing once and calling it a day.",
          "score": 4,
          "created_utc": "2026-02-12 12:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yzpvs",
              "author": "Yaar-Bhak",
              "text": "you think this can be used in production?\n\nand this means mcp would be used only in agentic flows right?",
              "score": 1,
              "created_utc": "2026-02-12 12:42:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54buzg",
          "author": "FoldedKatana",
          "text": "MCP is dead now. OpenClaw skills are where it's at",
          "score": 2,
          "created_utc": "2026-02-13 06:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0t1n",
          "author": "Cool_Fly_2030",
          "text": "The term ‚Äútool‚Äù in this thread is probably creating more confusion.\n\nMCP is a protocol to standardize how LLMs fetch context to ground and complete a task or generate a response more effectively.\n\nMCP servers are effectively APIs, where ‚Äútools‚Äù function as endpoints/routes to handle requests and execute logic on the server to do something and return the response to the LLM. \n\nWhen you register an MCP server in your client - VS code, Claude code, etc the LLM has a registry of tools to arbitrate between and delegate to if the description of the tool will solve its problem. \n\nThis is pretty powerful in agentic applications of LLMs because they can retrieve external context and perform actions in a fully automated loop.",
          "score": 2,
          "created_utc": "2026-02-14 19:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50wvs5",
          "author": "throwaway490215",
          "text": "MCPs are bullshit. They are a standard that basically tells the program run on your computer to prepend `some-tool --help` when you start a conversation, but with much more overhead, and **every conversation** even if you dont want to use `some-tool` this session. \n\nAnybody talking about credentials/authentication is a moron. \n\nJust add a \"Use `some-tool --help` to do X\" in your AGENTS.md and you're good.",
          "score": 2,
          "created_utc": "2026-02-12 18:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z5ouz",
          "author": "Astronos",
          "text": "it is function/tool calling over api",
          "score": 1,
          "created_utc": "2026-02-12 13:21:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zo2ke",
          "author": "Crafty_Disk_7026",
          "text": "MCP is just like an open ai spec the ai can read and know how to use your tool. It's literally just instruction manual",
          "score": 1,
          "created_utc": "2026-02-12 15:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tzq6",
          "author": "voidiciant",
          "text": "From what I understand:\n\nThe models have to be trained (and usually are, there is often a ‚Äûtool‚Äú tag on the downloads) to insert special keywords in their responses when a tool call is appropriate. \n\nThese keywords are intercepted by the runtime (the thing taking your input, converting to tokens,, etc) and the runtime performs the appropriate calls to the registered mcp tools (according to the protocol) and feeds back the tool-call results to the model, which in turn now incorporates them in the next response.\n\nAdditionally, and here I get fuzzy, the runtime generates a system prompt that contains a list of available MCP Tools, and the model is trained to understand this to generate the relevant keywords in the response based. \n\nMCP defines the protocols/API/formats. \n\nThat‚Äòs the gist for me",
          "score": 1,
          "created_utc": "2026-02-12 18:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51rpl9",
          "author": "CreepyValuable",
          "text": "MCP is kind of sort of a universal adapter to plug anything from ChatGPT to your toaster together.\n\nIt's not quite that straightforward and the actual interface is kind of clunky but it's pretty useful.\n\nFor example, my (not very good, but experimental so that's not important) AI uses it for things like a weather service, XiaoZhi AI esp support (essentially a smart speaker with a screen), VS Code integration and some other random things. It avoids needing a whole bunch of incompatible APIs.",
          "score": 1,
          "created_utc": "2026-02-12 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52svxj",
          "author": "Glum_Teaching8224",
          "text": "It's just tool using reference for the agent. ",
          "score": 1,
          "created_utc": "2026-02-13 00:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwht1",
          "author": "Electronic-Door7134",
          "text": "Good luck explaining to an auditor why your gave a 3rd party company full access to your company data (which is what happens without mcp)",
          "score": -2,
          "created_utc": "2026-02-12 15:41:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50avet",
              "author": "PaddingCompression",
              "text": "Wut",
              "score": 3,
              "created_utc": "2026-02-12 16:48:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4nj4y",
      "title": "16 single-file, zero-dependency implementations of the algorithms behind LLMs ‚Äî tokenization through speculative decoding. No frameworks, just the math.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/t4h1i8nbbhjg1.png",
      "author": "tom_mathews",
      "created_utc": "2026-02-14 15:32:01",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4nj4y/16_singlefile_zerodependency_implementations_of/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ho7tr",
          "author": "tom_mathews",
          "text": "The repo has been expanded from 16 to 30 scripts since the original post. Here's what's new:\n\n- **Foundations (7 ‚Üí 11):** Added BERT (bidirectional encoder), RNNs & GRUs (vanishing gradients + gating), CNNs (kernels, pooling, feature maps), GANs (generator vs. discriminator), VAEs (reparameterization trick), diffusion (denoising on point clouds), and an optimizer comparison (SGD vs. Momentum vs. RMSProp vs. Adam).\n\n- **Alignment (4 ‚Üí 9):** Added PPO (full RLHF reward ‚Üí policy loop), GRPO (DeepSeek's simplified approach), QLoRA (4-bit quantized fine-tuning), REINFORCE (vanilla policy gradients), Mixture of Experts (sparse routing), batch normalization, and dropout/regularization.\n\n- **Systems (5 ‚Üí 10):** Added paged attention (vLLM-style memory management), RoPE (rotary position embeddings), decoding strategies (greedy, top-k, top-p, beam, speculative ‚Äî all in one file), tensor & pipeline parallelism, activation checkpointing, and state space models (Mamba-style linear-time sequence modeling).\n\nSame constraints as before: every script is a single file, zero dependencies, trains and infers (or demonstrates forward-pass mechanics side-by-side), runs on CPU in minutes.\n\n[https://github.com/Mathews-Tom/no-magic](https://github.com/Mathews-Tom/no-magic)",
          "score": 1,
          "created_utc": "2026-02-15 11:10:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6m5hb",
      "title": "Can LLMs deduplicate ML training data?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6m5hb/can_llms_deduplicate_ml_training_data/",
      "author": "ddp26",
      "created_utc": "2026-02-16 21:14:03",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "I get increasingly annoyed with how unreliable deduplication tools are for cleaning training data. I‚Äôve used MinHash/LSH, libraries like [dedupe.io](http://dedupe.io), and pandas.drop\\_duplicates() but they all have a lot of false positives/negatives.  \n  \nI ended up running LLM-powered deduplication on 3,000 sentences from Google's paraphrase dataset from Wikipedia (PAWS). It removed 1,072 sentences (35.7% of the set). It only cost $4.21, and took \\~5 minutes.  \n  \nExamples of what it catches that the other methods don't:\n\n* \"Glenn Howard won the Ontario Championship for the 17th time as either third or skip\" and \"For the 17th time the Glenn Howard won the Ontario Championship as third or skip\"\n* \"David Spurlock was born on 18 November 1959 in Dallas, Texas\" and \"J. David Spurlock was born on November 18, 1959 in Dallas, Texas\"\n\n  \nFull code and methodology: [https://everyrow.io/docs/deduplicate-training-data-ml](https://everyrow.io/docs/deduplicate-training-data-ml)\n\nAnyone else using LLMs for data processing at scale? It obviously can work at small scale (and high cost), but are you finding it can work at high scale and low cost?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6m5hb/can_llms_deduplicate_ml_training_data/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5sbqnp",
          "author": "kubrador",
          "text": "yeah this is clever but you're basically paying for semantic understanding you could get cheaper with embeddings + cosine similarity. run your 3k sentences through openai's small embedding model (\\~$0.02 total), cluster by cosine distance, done in 10 seconds for less than a coffee.\n\n\n\nthe paraphrase examples you showed would absolutely get caught by that approach since they're semantically identical, which is what actually matters for training data dedup anyway.",
          "score": 2,
          "created_utc": "2026-02-17 01:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sf7em",
              "author": "dreamingwell",
              "text": "You could do this to get ‚Äúprobably duplicates‚Äù. And then use an LLM to finalize them. Reducing your LLM costs significantly.",
              "score": 1,
              "created_utc": "2026-02-17 01:25:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5seuz1",
          "author": "dreamingwell",
          "text": "You can do a Lora tuning on a small model, like Qwen3-4B. Train it to identify duplicated data from examples in your set. On the right GPU, it would absolutely tear through that data.",
          "score": 2,
          "created_utc": "2026-02-17 01:23:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzvnc",
              "author": "ChanceKale7861",
              "text": "Holy moly! Are you me?! ü§£ü§£ü§£",
              "score": 1,
              "created_utc": "2026-02-18 01:19:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5siq3u",
          "author": "No_Indication_1238",
          "text": "Tbh, you pretty much nailed a novel use case for LLMs. Yes, semantic analysis was tough before them.",
          "score": 1,
          "created_utc": "2026-02-17 01:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uvzl2",
          "author": "andy_p_w",
          "text": "Those two examples, if you take out regular words (any word 3 letters or less) and just look at the Jaccard similarity for the words will have very high overlap. English language is quite large, it is difficult to have much overlap in words random sentences, [https://andrewpwheeler.com/2024/04/20/some-musings-on-plagiarism/](https://andrewpwheeler.com/2024/04/20/some-musings-on-plagiarism/) .",
          "score": 1,
          "created_utc": "2026-02-17 12:52:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5mqrd",
      "title": "Has anyone here successfully sold RAG solutions to clients? Would love to hear your experience (pricing, client acquisition, delivery, etc.)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5mqrd/has_anyone_here_successfully_sold_rag_solutions/",
      "author": "Temporary_Pay3221",
      "created_utc": "2026-02-15 18:57:10",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "Hey everyone!\n\nI've been diving deep into RAG systems lately and I'm genuinely fascinated by the technology. I've built a few projects for myself and feel confident in my technical abilities, but now I'm looking to transition this into actual client work.\n\nBefore I jump in, I'd really appreciate learning from people who've already walked this path. If you've sold RAG solutions to clients, I'd love to hear about your experience:\n\n**Client & Project Details:**\n\n* What types of clients/industries did you work with?\n* How did they discover they needed RAG? (Did they come asking for it, or did you identify the use case?)\n* What was the scope? (customer support, internal knowledge base, document search, etc.)\n\n**Delivery & Timeline:**\n\n* How long did the project take from discovery to delivery?\n* What were the biggest technical challenges you faced?\n* Did you handle ongoing maintenance, or was it a one-time delivery?\n\n**Business Side:**\n\n* How did you find these clients? (freelance platforms, LinkedIn outreach, referrals, content marketing, etc.)\n* What did you charge? (ballpark is fine, just trying to understand market rates)\n* How did you structure pricing? (fixed project, hourly, monthly retainer?)\n\n**Post-Delivery:**\n\n* Were clients happy with the results?\n* Did you iterate/improve the system after launch?\n* Any lessons learned that you'd do differently next time?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5mqrd/has_anyone_here_successfully_sold_rag_solutions/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r79s1w",
      "title": "SurrealDB 3.0 for AI agent memory",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r79s1w/surrealdb_30_for_ai_agent_memory/",
      "author": "DistinctRide9884",
      "created_utc": "2026-02-17 15:55:41",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "SurrealDB 3.0 just dropped, with a big focus on agent memory infra for AI: improved vector indexing + better graph performance + native file storage + a WebAssembly extension system (Surrealism) that can run custom logic/models inside the DB. You can store vector embeddings + structured data + graph context/knowledge/memory in one place and do hybrid retrieval in one query.\n\nDetails:¬†[https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memor](https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memory)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r79s1w/surrealdb_30_for_ai_agent_memory/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2qfcz",
      "title": "Mix prompts instead of writing them by hand",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/5z7edpx9n1jg1.png",
      "author": "Everlier",
      "created_utc": "2026-02-12 10:55:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2qfcz/mix_prompts_instead_of_writing_them_by_hand/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r6hwr0",
      "title": "Bring OpenClaw-style memory to every agent",
      "subreddit": "LLMDevs",
      "url": "https://github.com/zilliztech/memsearch",
      "author": "codingjaguar",
      "created_utc": "2026-02-16 18:39:12",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6hwr0/bring_openclawstyle_memory_to_every_agent/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r3hvkg",
      "title": "Observation: LLMs seem to have a \"Version 2.0\" bias when generating new UIs",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/rmkiv7qmh7jg1.jpeg",
      "author": "Routine_Connection8",
      "created_utc": "2026-02-13 06:29:02",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3hvkg/observation_llms_seem_to_have_a_version_20_bias/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r6pho9",
      "title": "Have we overcome the long-term memory bottleneck?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6pho9/have_we_overcome_the_longterm_memory_bottleneck/",
      "author": "Bubbly_Run_2349",
      "created_utc": "2026-02-16 23:22:18",
      "score": 4,
      "num_comments": 20,
      "upvote_ratio": 0.67,
      "text": "Hey all,\n\nThis past summer I was interning as an SWE at a large finance company, and noticed that there was a huge initiative deploying AI agents. Despite this, almost all Engineering Directors I spoke with were complaining that the current agents had no ability to recall information after a little while (in fact, the company chatbot could barely remember after exchanging 6‚Äì10 messages).\n\nI discussed this grievance with some of my buddies at other firms and Big Tech companies and noticed that this issue was not uncommon (although my company‚Äôs internal chatbot was laughably bad).\n\nAll that said, I have to say that this \"memory bottleneck\" poses a tremendously compelling engineering problem, and so I am trying to give it a shot and am curious what you all think.\n\nAs you probably already know, vector embeddings are great for similarity search via cosine/BM25, but the moment you care about things like persistent state, relationships between facts, or how context changes over time, you begin to hit a wall.\n\nRight now I am playing around with a hybrid approach using a vector plus graph DB. Embeddings handle semantic recall, and the graph models entities and relationships. There is also a notion of a \"reasoning bank\" akin to the one outlined in Googles famous paper several months back. TBH I am not 100 percent confident that this is the right abstraction or if I am doing too much. \n\nHas anyone here experimented with structured or temporal memory systems for agents?\n\nIs hybrid vector plus graph reasonable, or is there a better established approach I should be looking at?\n\nAny and all feedback or pointers at this stage would be very much appreciated.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6pho9/have_we_overcome_the_longterm_memory_bottleneck/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5rzwor",
          "author": "Sea-Chemistry-4130",
          "text": "We're just reinventing distributed computing but with llms...",
          "score": 8,
          "created_utc": "2026-02-16 23:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sble0",
              "author": "Bubbly_Run_2349",
              "text": "Lol i guess... if by distributed computing you mean clever system design then I can't disagree. ",
              "score": 0,
              "created_utc": "2026-02-17 01:04:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5sik1h",
                  "author": "Sea-Chemistry-4130",
                  "text": "No I literally mean we're having to resolve many of the same issues that distributed systems have only now it's to ensure the llm didn't get it wrong instead of due to unreliable network or hardware.",
                  "score": 3,
                  "created_utc": "2026-02-17 01:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ruxjr",
          "author": "user0139",
          "text": "Interesting approach, although you are a bit vague with your description. Do you have a repo I can look over?",
          "score": 2,
          "created_utc": "2026-02-16 23:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rvfoe",
              "author": "Bubbly_Run_2349",
              "text": "Woah thank you for the fast reply. \n\nHere is the link: [https://github.com/TheBuddyDave/Memoria.](https://github.com/TheBuddyDave/Memoria)\n\nmuch appreciated!",
              "score": 1,
              "created_utc": "2026-02-16 23:31:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rymef",
          "author": "ibrahimsafah",
          "text": "Im doing something similar just for personal knowledge. I basically am out of my depth. I recommend reading some white papers about it",
          "score": 2,
          "created_utc": "2026-02-16 23:49:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sbg95",
              "author": "Bubbly_Run_2349",
              "text": "Oh thats pretty cool. I was able to get some AI researchers/students on my projects discord. They have been a lot of help. ",
              "score": 1,
              "created_utc": "2026-02-17 01:03:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s11my",
          "author": "philip_laureano",
          "text": "Yes, but this problem has been solved in other areas of software engineering.\n\nIt's a viewport problem, not a knapsack fitting problem. We do this all the time with Web pages.\n\nFor example, how in the world do we fit several TB of information that we have floating around onto a single browser tab so that someone can use and browse that information?\n\nHint: You don't do that by dumping the most similar context into the browser tab and expect the user to piece all the bits together. \n\nIf I as the user click on a link about 'architecture', I don't expect a page filled with terms similar to architecture. I expect to go to the page about architecture and it should be organised and easy to get to, despite the fact there's an infinite sea of information out there. \n\nThe only thing that changes is how that information is retrieved and organised and how my viewport changes to match what I'm looking for. \n\nThese problems in of themselves aren't new. The AI industry is, but the problems of scale are well known. The only question is when they'll catch up to the solutions already known",
          "score": 2,
          "created_utc": "2026-02-17 00:03:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sc8pg",
              "author": "Bubbly_Run_2349",
              "text": "I see. The current retrieval algo I have been workshopping with some buddies takes a lot of inspiration from  page ranking algos.   \n  \nVery interesting insight! Thank you.  ",
              "score": 1,
              "created_utc": "2026-02-17 01:08:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tdg6d",
          "author": "Ill_Awareness6706",
          "text": "Do you have a repo I can look over?",
          "score": 2,
          "created_utc": "2026-02-17 05:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tgtpc",
              "author": "Bubbly_Run_2349",
              "text": "Yes of course! Thank you again all help is very appreciated :) \n\n[https://github.com/TheBuddyDave/Memoria](https://github.com/TheBuddyDave/Memoria)",
              "score": 1,
              "created_utc": "2026-02-17 05:30:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tlbim",
          "author": "Happy-Fruit-8628",
          "text": " Hybrid vector plus graph is reasonable. The tough part isn‚Äôt storage, it‚Äôs deciding what to remember and keeping it from getting messy over time.",
          "score": 2,
          "created_utc": "2026-02-17 06:06:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uhsa3",
          "author": "Far_Noise_5886",
          "text": "I think vector + graph is where it's heading. How though do you handle the context bloat? ",
          "score": 2,
          "created_utc": "2026-02-17 11:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ynlhn",
          "author": "footuretruth",
          "text": "I have made a program that keeps continuity between user and AI basically a reference/recollection snapshot. Not true memory but it definitely supplements well.",
          "score": 2,
          "created_utc": "2026-02-18 00:11:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61ubot",
              "author": "Bubbly_Run_2349",
              "text": "like a context manager? ",
              "score": 1,
              "created_utc": "2026-02-18 13:38:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5zego4",
          "author": "cmndr_spanky",
          "text": "Just FYI there‚Äôs a post on this subreddit every 10 mins from some bot claiming they‚Äôve solved LLM memory or pretending to ask a question that‚Äôs ultimately just peddling some crap solution that has already been solved.",
          "score": 2,
          "created_utc": "2026-02-18 02:34:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61u8k9",
              "author": "Bubbly_Run_2349",
              "text": "Thank you for letting me know. I asked this question on a couple subs and got banned. I think this is the reason. ",
              "score": 1,
              "created_utc": "2026-02-18 13:37:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5we5z8",
          "author": "honestduane",
          "text": "What you‚Äôre dealing with is context window limitations; this shows to me that you haven‚Äôt yet learned about the AI stuff deep enough so keep learning.",
          "score": 1,
          "created_utc": "2026-02-17 17:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sejh9",
          "author": "No_Wrongdoer41",
          "text": "me and a few folks have built a platform that automatically creates a shared knowledge/ context layer from underlying sources for agents to use. happy to let you try it for free!",
          "score": 1,
          "created_utc": "2026-02-17 01:21:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3ia0o",
      "title": "Launching Dhi-5B (compute optimally pre-trained from scratch)",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/iihkwu6rl7jg1.png",
      "author": "gradNorm",
      "created_utc": "2026-02-13 06:52:08",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 0.64,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3ia0o/launching_dhi5b_compute_optimally_pretrained_from/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o54gqwa",
          "author": "gradNorm",
          "text": "Model available on HuggingFace: https://huggingface.co/Shaligram-Dewangan/Dhi-5B-Base",
          "score": 2,
          "created_utc": "2026-02-13 06:53:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o55pdgz",
          "author": "m98789",
          "text": "Can you publish your steps to create this from scratch? That may be more impactful than the model itself",
          "score": 1,
          "created_utc": "2026-02-13 13:11:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o604380",
          "author": "iitgF",
          "text": "üöÄü•áüçæ",
          "score": 1,
          "created_utc": "2026-02-18 05:16:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}