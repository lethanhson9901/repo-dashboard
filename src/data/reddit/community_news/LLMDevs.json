{
  "metadata": {
    "last_updated": "2026-02-14 02:59:58",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 76,
    "file_size_bytes": 106743
  },
  "items": [
    {
      "id": "1r1oa4i",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:53",
      "score": 201,
      "num_comments": 32,
      "upvote_ratio": 0.93,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) ‚Äì 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt‚Äôs trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4s8hek",
          "author": "TylerDurdenFan",
          "text": ">The cleaning, chunking, and optimization challenges are exactly what excites me\n\nJust try to not get too excited around that material, mkay?",
          "score": 24,
          "created_utc": "2026-02-11 11:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sctpa",
              "author": "Cod3Conjurer",
              "text": "He he üòÇ",
              "score": -10,
              "created_utc": "2026-02-11 12:20:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xu1u2",
                  "author": "kexxty",
                  "text": "Bro...this is not a laughing matter",
                  "score": 2,
                  "created_utc": "2026-02-12 06:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tf78a",
          "author": "Significant-Crow-974",
          "text": "It would be marvellous to run this over the full set of unredacted files.\nI am hoping that the FBI who have illegally redacted information do not now delete that hoard of documents.\nI hope that when they finally manage to charge Trump and the Epstein class that they will be able to utilise a tool such as this to make their prosecutions more effective.\nWell done and Thank you!",
          "score": 7,
          "created_utc": "2026-02-11 15:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tgqt7",
              "author": "Cod3Conjurer",
              "text": "The goal here is purely technical, building better retrieval over large unstructured datasets.  \nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.  \n",
              "score": 3,
              "created_utc": "2026-02-11 15:59:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tqxae",
                  "author": "Significant-Crow-974",
                  "text": "Yes. Fully appreciate that. Actually, I created a similar RAG for the first tranche of documents. Just as an exercise to see how a RAG could cope with so many documents.\nI would say that it was a partial success. Great insight.",
                  "score": 2,
                  "created_utc": "2026-02-11 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4wndcx",
                  "author": "Klutzy_Celebration80",
                  "text": "Might be interesting to see if you could have it un-redact the documents",
                  "score": 1,
                  "created_utc": "2026-02-12 01:33:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r00lr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 6,
          "created_utc": "2026-02-11 05:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdqsc",
              "author": "Cod3Conjurer",
              "text": "That's over 1gb¬†\nYou can generate using my code¬†",
              "score": 0,
              "created_utc": "2026-02-11 07:07:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sy6d6",
                  "author": "Borkato",
                  "text": "Wait 1gb is nothing when the models are like 20gb+",
                  "score": 1,
                  "created_utc": "2026-02-11 14:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sf4yp",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -3,
                  "created_utc": "2026-02-11 12:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tfnfq",
          "author": "DaRandomStoner",
          "text": "I was hoping you had the newly released documents in this... until we get these new documents processed through an OCR and into an organized data structure, we can't really go through them properly. \n\nIt would cost a good amount to process all the new documents so that we can include them in databases like this... it's all just compute costs though. DeepSeek's OCR is open-source and can run on most PCs. If a bunch of people got together we could expand databases like this to include all the newly released docs...",
          "score": 5,
          "created_utc": "2026-02-11 15:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4th0by",
              "author": "Cod3Conjurer",
              "text": "Yeah, this version doesn‚Äôt include the newly released documents yet. If those are raw scans, they‚Äôd need OCR + structured parsing before indexing.  \nThe main cost is compute and storage, not complexity.  \nA collaborative effort could definitely speed that up, especially for batching OCR and preprocessing at scale.",
              "score": 3,
              "created_utc": "2026-02-11 16:00:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4reucl",
          "author": "kondasamy",
          "text": "I think you should checkout - [https://jmail.world/jemini](https://jmail.world/jemini)",
          "score": 6,
          "created_utc": "2026-02-11 07:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf5lj",
              "author": "Cod3Conjurer",
              "text": "Yeeha i have seen that¬†\nBut it is currently broken",
              "score": -1,
              "created_utc": "2026-02-11 07:20:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x1h4q",
          "author": "jdsweet653",
          "text": "Great app! What did your ingestion py look like for the db?",
          "score": 2,
          "created_utc": "2026-02-12 02:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjmq3",
              "author": "Cod3Conjurer",
              "text": "The ingestion was pretty simple, I loaded the cleaned JSON, chunked it (400 size, 80 overlap), deduped chunks using SHA-256 hashing, generated MiniLM embeddings, and upserted everything into ChromaDB with source metadata.",
              "score": 2,
              "created_utc": "2026-02-12 05:01:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yp6ur",
                  "author": "DanRan88",
                  "text": "Any idea on the size of the DB?¬†",
                  "score": 2,
                  "created_utc": "2026-02-12 11:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xgy3s",
          "author": "Big3gg",
          "text": "See if it knows how to make jerky",
          "score": 2,
          "created_utc": "2026-02-12 04:41:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj134",
              "author": "Cod3Conjurer",
              "text": "he he ü§£",
              "score": 0,
              "created_utc": "2026-02-12 04:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y92qs",
          "author": "DarKresnik",
          "text": "Amazing job. Now we need someone to \"find\" those 3m missing documents.",
          "score": 2,
          "created_utc": "2026-02-12 08:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zqs1i",
              "author": "Cod3Conjurer",
              "text": "Guess I‚Äôll have to OCR the entire publicly available dataset myself now, jokingü§£",
              "score": 1,
              "created_utc": "2026-02-12 15:14:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t10wl",
          "author": "StackSmashRepeat",
          "text": "So, have you come to terms with RAG being a dead end as far as real recall of memory works? Or are you just chunking and overlapping to a ridiculous point? I really don't think this is a sensible use of RAG. The LLM will at some point start hallucinating missing pieces from thin air, making this tool fairly unreliable for accuracy. \n\nPeople looking into these files need absolute accuracy.",
          "score": 1,
          "created_utc": "2026-02-11 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tfjmw",
              "author": "Cod3Conjurer",
              "text": "You‚Äôre right that absolute accuracy matters. That‚Äôs why this should be treated as an assistive search layer, not a final source of truth.\n\nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.",
              "score": 3,
              "created_utc": "2026-02-11 15:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tikha",
                  "author": "StackSmashRepeat",
                  "text": "Make it do an online search after it retrieves data from rag and provide a link directly to an online source. End users are dumb and some will believe anything the llm tells them.",
                  "score": 0,
                  "created_utc": "2026-02-11 16:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o538mx9",
          "author": "HarjjotSinghh",
          "text": "2m pages = my new love language.",
          "score": 1,
          "created_utc": "2026-02-13 01:44:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1jr22",
      "title": "I'm super unemployed and have too much time so I built an open source SDK to build event-driven, distributed agents on Kafka",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1jr22/im_super_unemployed_and_have_too_much_time_so_i/",
      "author": "orange-cola",
      "created_utc": "2026-02-11 01:30:26",
      "score": 22,
      "num_comments": 12,
      "upvote_ratio": 0.74,
      "text": "I finally got around to building this SDK for event-driven agents. It's an idea I've been sitting on for a while. I finally started working on it and it's been super fun to develop.\n\nI made the SDK in order to decompose agents into independent, separate microservices (LLM inference, tools, and routing) that communicate asynchronously through Kafka. This way, agents, tool services, and downstream consumers all communicate asynchronously and can be deployed, adapted, and scaled completely independently.\n\nThe event-driven structure also makes connecting up and orchestrating multi-agent teams trivial. Although this functionality isn't yet implemented, I'll probably develop it soon (assuming I stay unemployed and continue to have free time on my hands).\n\nCheck it out and throw me a star if you found the project interesting!¬†[https://github.com/calf-ai/calfkit-sdk](https://github.com/calf-ai/calfkit-sdk)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1jr22/im_super_unemployed_and_have_too_much_time_so_i/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4q4hbc",
          "author": "qa_anaaq",
          "text": "Ah this is cool. I was obsessed with the idea of the event-driven approach to agents last year but never had to time to explore it. I‚Äôll be diving in. I always thought it‚Äôs a solid approach.",
          "score": 4,
          "created_utc": "2026-02-11 01:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q9ba4",
              "author": "orange-cola",
              "text": "Yea for sure--I think it's a must for production-ready agents. Let me know what you think when you try it out! Always love any feedback",
              "score": 1,
              "created_utc": "2026-02-11 02:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rd7in",
          "author": "His0kx",
          "text": "Seems really interesting on paper ! How did you manage the separate microservice part ? One thing I have struggled with is that even with quality gates/api contracts between agents, for the same phase, same prompt/tools the output could be different with different subagents (maybe more food for thoughts when you will start working on orchestrating multi agents ?)",
          "score": 3,
          "created_utc": "2026-02-11 07:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rqnc4",
              "author": "orange-cola",
              "text": "How I'm currently going about it is to think of multi-agent orchestrations as agents in a text-based groupchat. Internally, each agent can can call tools with different schemas but when it responds into the \"groupchat,\" the message will always be text based. This way, the schema between agents will always be predictable, and agents can still effectively coordinate and chat among eachother. This is just my initial naive implementation so I fully expect to grow this into something more sophisticated as different agent communication patterns emerge.",
              "score": 1,
              "created_utc": "2026-02-11 09:09:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rwdwp",
                  "author": "His0kx",
                  "text": "Okay I think you have the right intuition üòÖ, Claude code did the same on the last release (Agent team)",
                  "score": 2,
                  "created_utc": "2026-02-11 10:02:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzu1d",
          "author": "Far_Independent8754",
          "text": "This is exactly the direction the industry needs to move. Building monolithic agents is a dead end for production.\n\nI‚Äôve been preaching about this lately‚Äîwe need to stop the 'Prompt Alchemy' and move toward **Microagentic Stacking**. Your approach with Kafka is the perfect infrastructure for it because it enforces the decoupling that most people ignore.\n\nIf you are breaking down agents into independent services, you‚Äôve already won half the battle against 'reasoning decay'. I actually wrote a **Manifesto** on why this modular/stacked approach is the only way to scale without the whole thing collapsing into a 'Big Ball of Mud'.\n\nCheck it out if you want to see the architectural patterns I'm formalizing: üîó[https://github.com/ericmora/microagentic-stacking](https://github.com/ericmora/microagentic-stacking)\n\nCongrats on the SDK, man. Building in public while job hunting is the best way to show senior-level thinking. Starred! ‚≠ê",
          "score": 2,
          "created_utc": "2026-02-11 05:11:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4robrd",
              "author": "orange-cola",
              "text": "Thanks for the star! And agreed, I believe production-grade agents will inevitably have to move towards event-driven architecture as these agent systems scale.",
              "score": 1,
              "created_utc": "2026-02-11 08:47:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4r3sei",
          "author": "Sea-Sir-2985",
          "text": "the decomposition into separate microservices for inference, tools, and routing is smart... the main pain point with monolithic agent frameworks is that scaling one part means scaling everything, and a slow tool call blocks the whole pipeline\n\nkafka as the backbone makes the async part trivial but i'd be curious how you're handling the latency tradeoff. LLM agents are already slow from inference time so adding message queue overhead on top might make the end-to-end response time rough for interactive use cases. seems like it'd shine more for batch processing and background agent workflows where latency doesn't matter as much\n\nthe multi-agent orchestration part is where this could get really interesting though... being able to spin up independent agent services that communicate through events without tight coupling is way cleaner than most multi-agent frameworks that try to do everything in one process",
          "score": 2,
          "created_utc": "2026-02-11 05:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rpx1r",
              "author": "orange-cola",
              "text": "I agree. I think added overhead from message queueing is probably best managed by dynamic instance scaling that adjusts to the incoming traffic load. It's more on the operational side of things, and outside of the SDK's domain, but the good thing is there are already well-established technologies for this purpose.\n\nAlso totally agree on the background agent workflows part. For agent operations on continuous background data streams, this SDK can really shine.",
              "score": 1,
              "created_utc": "2026-02-11 09:02:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ynvo1",
          "author": "UseMoreBandwith",
          "text": "interesting. but what could the use-cases for something like that?  (other than hacking/ddos etc)",
          "score": 2,
          "created_utc": "2026-02-12 11:11:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o54dbr2",
          "author": "arbiter_rise",
          "text": "I‚Äôm also very interested in the event-driven approach! I‚Äôve added a star to the repo as well.  \n  \nIt seems like this could be fully handled by another broker as well. Was there a specific reason you chose to use Kafka?\n\nWhat level of message volume is being handled by Kafka? I‚Äôm curious whether the context can be maintained reliably.",
          "score": 2,
          "created_utc": "2026-02-13 06:23:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r04wed",
      "title": "Observations From Using GPT-5.3 Codex and Claude Opus 4.6",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "author": "Arindam_200",
      "created_utc": "2026-02-09 13:58:50",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "I tested GPT-5.3 Codex and Claude Opus 4.6 shortly after release to see what actually happens once you stop prompting and start expecting results. Benchmarks are easy to read. Real execution is harder to fake.\n\nBoth models were given the same prompts and left alone to work. The difference showed up fast.\n\nCodex doesn‚Äôt hesitate. It commits early, makes reasonable calls on its own, and keeps moving until something usable exists. You don‚Äôt feel like you‚Äôre co-writing every step. You kick it off, check back, and review what came out. That‚Äôs convenient, but it also means you sometimes get decisions you didn‚Äôt explicitly ask for.\n\nOpus behaves almost the opposite way. It slows things down, checks its own reasoning, and tries to keep everything internally tidy. That extra caution shows up in the output. Things line up better, explanations make more sense, and fewer surprises appear at the end. The tradeoff is time.\n\nA few things stood out pretty clearly:\n\n* Codex optimizes for momentum, not elegance\n* Opus optimizes for coherence, not speed\n* Codex assumes you‚Äôll iterate anyway\n* Opus assumes you care about getting it right the first time\n\nThe interaction style changes because of that. Codex feels closer to delegating work. Opus feels closer to collaborating on it.\n\nNeither model felt ‚Äúsmarter‚Äù than the other. They just burn time in different places. Codex burns it after delivery. Opus burns it before.\n\nIf you care about moving fast and fixing things later, Codex fits that mindset. If you care about clean reasoning and fewer corrections, Opus makes more sense.\n\nI wrote a longer breakdown [here](https://www.tensorlake.ai/blog/claude-opus-4-6-vs-gpt-5-3-codex) with screenshots and timing details in the full post for anyone who wants the deeper context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4fmz12",
          "author": "swarmed100",
          "text": "Opus 4.6 reasons a lot longer than opus 4.5. One negative side I noticed from this is that it is \"better\" at finding delusional logic to explain why a set of facts that are clearly impossible \"make sense\", instead of concluding that some of the assumptions or inputs must be wrong since the set of facts are just impossible together.",
          "score": 5,
          "created_utc": "2026-02-09 14:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gbcxu",
              "author": "External-Yak-371",
              "text": "As a pro plan user I agree, but it also means my piddly allowance can nearly be consumed in one good planning session",
              "score": 3,
              "created_utc": "2026-02-09 16:10:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hrtb3",
                  "author": "Manfluencer10kultra",
                  "text": "u/External-Yak-371 One git commit request for small refactors in many files (similar refactors) was enough for 48%.  It did notice I missed 5 items that needed to be refactored then used like 20k tokens to fix it, and then it was absolutely perfect.\n\nToo bad it was 50% of my 5h allowance (you get about 9 x 5h on pro at 11% of weekly ...).  In that sense, it was absolutely worthless spending my tokens on it.  \nBut what if I used Sonnet for it? it would have been maybe worse.  \nAnd these are the things that you don't want to do yourself, and want to hand over to AI. You close off your session after a long day, forget to commit all those refactors, but still want a sensible commit instead of \"lots of fixes\".  \nEh this is where AI tooling should come in to save the day, but nope..",
                  "score": 1,
                  "created_utc": "2026-02-09 20:21:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gcjpr",
              "author": "cmndr_spanky",
              "text": "This is pretty worrying but makes sense. More reasoning doesn‚Äôt mean better results and often just causes useless ‚Äúthought loops‚Äù that at best just wastes more open credits, at worst fills up context causing it to loose touch with the original request details.\n\nThat said, I‚Äôve never been impressed with any of openAI‚Äôs models as coding agents, so I‚Äôd suspect opus is still better despite the flaws. We‚Äôll see I guess",
              "score": 2,
              "created_utc": "2026-02-09 16:15:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gfqe5",
          "author": "kubrador",
          "text": "so basically one model is a startup founder and the other is an engineering lead pretending to care about code review",
          "score": 2,
          "created_utc": "2026-02-09 16:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hrdaq",
          "author": "Manfluencer10kultra",
          "text": "Bruh, I switched to Codex (Free) and getting incredible usage just on Free and GPT-5.2-Codex High, not even 5.3 and it's just night and day.  Claude put me in a depressive mood, and now I'm back enjoying engineering again.",
          "score": 2,
          "created_utc": "2026-02-09 20:19:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3kgpn",
      "title": "Rearchitecting LLMs ‚Äî pruning, distillation, and smaller domain models (MEAP)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "author": "ManningBooks",
      "created_utc": "2026-02-13 09:07:18",
      "score": 18,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "Hi r/LLMDevs,\n\nStjepan from Manning here. The mods said it's ok if I post this here. \n\nWe‚Äôve just released a book that‚Äôs very much aimed at the kinds of problems this community discusses all the time: what to do when a general-purpose LLM is technically impressive but awkward, expensive, or inefficient for your actual use case.\n\n**Rearchitecting LLMs** by Pere Martra  \n[https://www.manning.com/books/rearchitecting-llms](https://hubs.la/Q042-hLy0)\n\n[Rearchitecting LLMs by Pere Martra](https://preview.redd.it/vyy079zx78jg1.jpg?width=2213&format=pjpg&auto=webp&s=755a8b1ab1320ede5daedfa861d6ab8d1b0c5e5d)\n\nThe core idea of the book is simple but powerful: instead of treating open models as fixed artifacts, you can reshape them. Pere walks through structural techniques like targeted fine-tuning, pruning, and knowledge distillation to build smaller, cheaper, domain-focused models that still perform well on the tasks you care about.\n\nWhat makes this book interesting is how hands-on it gets. You‚Äôre not working with abstract toy networks. The examples focus on modifying widely used open models, such as Llama-3, Gemma, and Qwen. The focus is on understanding which parts of a model actually contribute to behavior, how to identify waste or redundancy, and how to remove or compress components without blindly wrecking performance.\n\nThere‚Äôs also some genuinely thoughtful material on combining behavioral analysis with structural changes. Instead of just cutting parameters and hoping for the best, the book explores ways to reason about why a modification works or fails. One section that tends to spark discussion is ‚Äúfair pruning,‚Äù where pruning is used not only for efficiency but also to reduce bias at the neuron level.\n\nIf you‚Äôre working on local models, cost-constrained deployments, or specialized SLMs, this book is very much in that territory. It‚Äôs written for people who are comfortable with LLM concepts and want to go deeper into how models can be reshaped rather than simply prompted.\n\n**For the** r/LLMDevs **community:**  \nYou can get **50% off** with the code **MLMARTRA50RE**.\n\nA quick note on availability: the book is currently in **MEAP (Manning Early Access Program)**. That means you get immediate access to the chapters as they‚Äôre written, along with updates as the manuscript evolves.\n\nHappy to bring the author to answer questions about the book, the techniques it covers, or the kinds of readers it‚Äôs best suited for. And I‚Äôd be curious to hear from folks here who are already doing pruning or distillation in practice ‚Äî what‚Äôs been harder than expected?\n\nI'm ready to give away 5 ebooks to the first five commenters who share their experience here.\n\nThank you all for having us. It feels great to be here.\n\nCheers,",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o54xstz",
          "author": "StackSmashRepeat",
          "text": "Would you list some common problems and terminologies that the book covers? I'll have a look if it peaks my interest.",
          "score": 3,
          "created_utc": "2026-02-13 09:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5529pk",
              "author": "ManningBooks",
              "text": "Hey, thanks for asking. Here are some examples of what the book covers:\n\n\\- End-to-End Model Re-architecting (Chapter 2): Transform Gemma-3-270M using depth pruning and knowledge distillation for a 10% speed increase while retaining 93-98% of original capabilities in a hands-on project.\n\n\\- Data-Driven Pruning (Chapters 4-5): Create two models from the same base (Qwen3-0.6B or Llama-3.2-1B): one for formal texts (WikiText) and another for short messages (SMS Spam), using activation analysis with PyTorch hooks to highlight domain-specific component importance.\n\n\\- Bias Auditing and Correction: In ethics chapters, perform a model \"cleanup\" using ablation frameworks and PCA visualization to identify and mitigate demographic biases, achieving fairness without full retraining.\n\n\\- Mini-Capstone: Utilize a small \"draft model\" to speed up LLM inference by quickly proposing tokens, validated by a larger model.\n\n\\- Capstone Project: Migrate an agent system from costly external APIs to a specialized local Small Language Model (SLM).\n\nHope this helps.",
              "score": 6,
              "created_utc": "2026-02-13 10:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56j1zc",
                  "author": "StackSmashRepeat",
                  "text": "This is quite interesting; you're trying to move local models away from the static model while staying within the static framework? I could basically train a model for my iPhone, let's say I export all my email and scrub PII, format for training data and then I could fine-tune to write mails that look somewhat within the realm of my own style? \n\nI haven't looked into training or fine tuning as I couldn't think of a personal use case for these tiny models, but like you're saying \"domain-focused\", gave a clearer picture.\n\nThis is a little over my current scope as I'm not even sure if I understood this correctly, but I've been thinking of ways to make a digital twin that could handle writing across multiple platforms. Was always thinking Id need a larger model to handle such a task because it sounds easy enough, but capturing the essence of one's writing is quite a complex task for llms. At least in my experience.\n\nThanks for the info.",
                  "score": 3,
                  "created_utc": "2026-02-13 15:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57qobq",
          "author": "marm_alarm",
          "text": "I'm a subscriber to Manning and so I have access to all the MEAP content.  I am very interested in reading this book and will post my review here after I've taken a look!",
          "score": 1,
          "created_utc": "2026-02-13 19:14:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57u5v0",
          "author": "dextoz",
          "text": "Would love a copy and meap along!",
          "score": 1,
          "created_utc": "2026-02-13 19:31:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r162ky",
      "title": "LLaDA2.1 vs Autoregressive Baselines: Is Diffusion Finally Competitive for Inference Throughput?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r162ky/llada21_vs_autoregressive_baselines_is_diffusion/",
      "author": "Ill_Awareness6706",
      "created_utc": "2026-02-10 16:51:41",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "Been digging into the LLaDA2.1 paper (arXiv:2602.08676) after seeing claims about diffusion LLMs hitting 892 TPS on HumanEval+. The numbers are interesting enough that I'm starting to wonder if we've been sleeping on diffusion models for inference workloads.\n\nQuick context for those unfamiliar: LLaDA2.1 introduces a dual decoding system (Speedy Mode vs Quality Mode) that lets you trade accuracy for throughput. The 100B model hits 892 TPS peak on coding benchmarks with quantization, while the 16B model averages 1071 TPS across nine benchmarks vs 302 TPS for Qwen3 8B. The key innovation is Token to Token editing that lets the model correct its own mistakes during generation.\n\nStandard absorbing state diffusion models have a fundamental problem: once a \\[MASK\\] token becomes a real token, it's locked in forever. LLaDA2.1 adds what they call an \"Editing Set\" alongside the \"Unmasking Set\" at each timestep. This means the model can retroactively fix errors during parallel decoding, which addresses the token inconsistency issues that have plagued diffusion LLMs. Speedy Mode generates a rough draft fast (aggressive M2T threshold), then T2T passes clean up the artifacts. Quality Mode uses conservative thresholds throughout for higher accuracy but slower generation.\n\nThe benchmark highlights: LLaDA2.1 Flash (100B) on HumanEval+ scores 89.63 in both modes, with Speedy Mode achieving 13.81 tokens per forward vs Quality Mode's 9.18 TPF. On LiveCodeBench, Speedy Mode hits 44.05 at 6.48 TPF while Quality Mode reaches 45.37 at 3.80 TPF. Peak throughput with their quantized setup: 891.74 TPS on HumanEval+, 801.48 TPS on BigCodeBench Full, 663.39 TPS on LiveCodeBench. The 16B Mini variant peaks at 1586.93 TPS on HumanEval+. Compared to baselines across nine benchmarks: LLaDA2.1 Mini averages 1071.2 TPS vs 597.1 for LLaDA2.0 Mini, 464.7 for Ling Mini 2.0, and 301.9 for Qwen3 8B. That's roughly 3.55x throughput comparing the 16B diffusion model against the 8B autoregressive baseline, so not exactly apples to apples on parameters, but the efficiency gap is notable. Reproducibility note: these numbers use customized SGLang with per block FP8 quantization.\n\nThey also have Multi Block Editing that trades throughput for accuracy: AIME 2025 jumps from 63.33 to 70.00 with MBE, ZebraLogic from 84.20 to 88.20. Average across 10 benchmarks: 72.67 at 5.14 TPF with MBE vs 70.69 at 5.82 TPF without.\n\nOn the training side, they built what they claim is the first large scale RL framework for diffusion LLMs using EBPO (ELBO based Block level Policy Optimization). The core problem is that sequence level log likelihood is intractable in block autoregressive models, so they use Vectorized Likelihood Estimation for parallelized bound computation. Interesting direction if you're thinking about fine tuning diffusion models beyond standard SFT, though I haven't dug deep into the training methodology yet.\n\nNow here's where I'm genuinely uncertain. The paper is upfront about the tradeoffs: Speedy Mode works well for code and math but can produce artifacts on general chat. They specifically mention n gram repetitions that self correction only partially fixes. So the question becomes: are these throughput numbers meaningful if you can only use Speedy Mode for narrow domains?\n\nTo make this concrete: say you're running a code completion service handling 10K requests per minute. At 302 TPS with Qwen3 8B, you need roughly 33 inference instances to keep up. At 1071 TPS with LLaDA2.1 Mini, that drops to about 10 instances. That's real infrastructure savings if the quality holds up in production. But I'm skeptical whether benchmark TPS translates to real world throughput given the different decoding dynamics, and whether the occasional n gram artifacts would tank user experience.\n\nPaper: [https://arxiv.org/abs/2602.08676](https://arxiv.org/abs/2602.08676)\n\nGitHub: [https://github.com/inclusionAI/LLaDA2.X](https://github.com/inclusionAI/LLaDA2.X)\n\nFor those running inference at scale: would 3x throughput on code generation be enough to justify adding a completely different model architecture to your stack? Or is the operational complexity not worth it until diffusion models close the gap on general tasks?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r162ky/llada21_vs_autoregressive_baselines_is_diffusion/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r0lbvd",
      "title": "Mixture-of-Models routing beats single LLMs on SWE-Bench via task specialization",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "author": "botirkhaltaev",
      "created_utc": "2026-02-10 00:10:22",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been looking at per-task results on SWE-Bench Verified and noticed something that leaderboard averages hide: different models consistently solve¬†*different*¬†subsets of tasks.\n\nEven the top overall model on the leaderboard fails a non-trivial number of tasks that other models reliably solve, and the reverse is also true. This suggests strong task-level specialization rather than one model being strictly better.\n\nTo test this, I built a¬†**Mixture-of-Models architecture**, which is different from traditional routing that just defaults to the strongest aggregate model most of the time. The goal isn‚Äôt to route to a single model as often as possible, but to exploit complementary strengths between models.\n\nConcretely:\n\n* The problem description is embedded\n* It‚Äôs assigned to a semantic cluster (learned from general coding data, not SWE-Bench)\n* Each cluster has learned per-model success statistics\n* The task is routed to the historically strongest model for that¬†*type*¬†of problem\n\nImportantly, this does¬†**not**¬†route the top aggregate model for the majority of tasks. Several clusters consistently route to other models where they outperform it, even though it has the highest overall score.\n\nThere‚Äôs no new foundation model, no test-time search, and no repo execution, just a lightweight gating mechanism over multiple models.\n\nUsing this Mixture-of-Models setup, the system reaches 75.6% on SWE-Bench, exceeding single-model baselines (\\~74%). The takeaway isn‚Äôt the absolute number, but the mechanism: leaderboard aggregates hide complementary strengths, and mixture architectures can capture a higher ceiling than any single model.\n\nBlog with details and methodology here:¬†[https://nordlyslabs.com/blog/hypernova](https://nordlyslabs.com/blog/hypernova)\n\nGithub: the framework is open source !¬†[https://github.com/Nordlys-Labs/nordlys](https://github.com/Nordlys-Labs/nordlys)\n\nML/AI Research Community Discord:¬†[https://discord.gg/dqW7BBrq](https://discord.gg/dqW7BBrq)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qygq2b",
      "title": "We have developed a different architecture for LLM that does not work on transformers but works on the principles of reservoir computing and energy modelling. It remains constant on vRAM as we scale context unlike transformers.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/3830vd8tc3ig1.jpeg",
      "author": "Dry_Oil2597",
      "created_utc": "2026-02-07 15:30:45",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qygq2b/we_have_developed_a_different_architecture_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o43s4zy",
          "author": "HumanDrone8721",
          "text": "That's lovely, now publish it, open source it and let us train models based on it with opensource data sets, like the ones from Olmo series, and this way we can compare the efficiency and accuracy of the new method. I think I still have $200 on thinker.ai so I could do a test run for you if you publish the pipeline for training and inference.",
          "score": 10,
          "created_utc": "2026-02-07 16:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ezthp",
          "author": "Noddybear",
          "text": "What perplexity?",
          "score": 1,
          "created_utc": "2026-02-09 11:23:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f049r",
              "author": "Dry_Oil2597",
              "text": "For average over 10k msmarco docs its 18",
              "score": 1,
              "created_utc": "2026-02-09 11:25:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzi6ot",
      "title": "Moltbook - No Human Captcha allows only LLMs post",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/85cc0i50nbig1.png",
      "author": "hasmcp",
      "created_utc": "2026-02-08 19:25:50",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 0.69,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qzi6ot/moltbook_no_human_captcha_allows_only_llms_post/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4ckqbm",
          "author": "dezastrologu",
          "text": "Why do we still care about moltbook",
          "score": 2,
          "created_utc": "2026-02-09 00:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d8oco",
              "author": "hasmcp",
              "text": "My interest on the brilliant idea came for dealing with the spams and/or unwanted inputs. I found it clever. The developers are trying innovative ways and enjoying the moment.",
              "score": 3,
              "created_utc": "2026-02-09 02:50:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e5tbx",
                  "author": "HumanDrone8721",
                  "text": "So soon to verify that you're no robot you'll have to use a robot?",
                  "score": 3,
                  "created_utc": "2026-02-09 06:36:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b61ea",
          "author": "Moliri-Eremitis",
          "text": "Interesting. How long was the verification window?",
          "score": 1,
          "created_utc": "2026-02-08 20:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bh7z1",
              "author": "_tony_lewis",
              "text": "Seems to be 30s and if you are incorrect it locks, one of my agents made a formatting mistake so list a post",
              "score": 2,
              "created_utc": "2026-02-08 21:05:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qprcx",
          "author": "dydhaw",
          "text": "This is the dumbest thing I've ever seen",
          "score": 1,
          "created_utc": "2026-02-11 03:59:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz1adm",
      "title": "-68% model size, <0.4 pp accuracy loss: Compressed LLaMA-3.2-1B ‚Üí Q4_0 GGUF on SNIPS Dataset (CPU Inference)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1qz1adm",
      "author": "mr_ocotopus",
      "created_utc": "2026-02-08 06:10:48",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qz1adm/68_model_size_04_pp_accuracy_loss_compressed/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o47wcjn",
          "author": "Icy_Distribution_361",
          "text": "Looks interesting but can you say a bit more?",
          "score": 1,
          "created_utc": "2026-02-08 07:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47ym88",
              "author": "mr_ocotopus",
              "text": "Sure, it is basically a library to make your models smaller so they can perform a specific tasks at the same level as bigger models.   \nThe idea is to use these in on-device (OR) enterprise scale where you would want to process billions of records \\[save cost\\]  \nCheck out the repo for more : [https://github.com/chandan678/compressGPT](https://github.com/chandan678/compressGPT)\n\nBlog on how you can use it for on-device : [https://medium.com/@chandancjs/rethinking-on-device-llms-why-one-model-is-never-enough-3abccb4756bf](https://medium.com/@chandancjs/rethinking-on-device-llms-why-one-model-is-never-enough-3abccb4756bf)",
              "score": 1,
              "created_utc": "2026-02-08 08:11:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47z5co",
                  "author": "Icy_Distribution_361",
                  "text": "Ah, so it's not effectively the same ability model, it's same ability in specific tasks?",
                  "score": 1,
                  "created_utc": "2026-02-08 08:16:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4frssh",
          "author": "mr-KSA",
          "text": "This is truly valuable data. However, I feel that benchmarks and other metrics are unfortunately no longer providing anything beyond a general overview. The situation has become so¬†multifaceted¬†that it is becoming quite frustrating. To clarify: a 4B model specifically designed for translation can outperform an 80B model. Furthermore, the quality between different levels of¬†quantization¬†is, regrettably, inconsistent. I also suspect that many current models are being specifically¬†fine-tuned¬†to inflate these benchmark scores.\n\nIn my view,¬†empirical¬†experience is paramount. For instance, I have observed significant performance gaps between Q8 and Q4 quantization, particularly in MoE models. While a model like GPT-OSS 20B might be too 'clumsy' for my specific workflows, another user might prefer it over GPT-4. It ultimately depends on your specific use case. Because I utilize long, complex system prompts that require strict adherence to sequential instructions, models like GLM-4.7 or Granite 4 yield better results for me than Qwen 80B. For others, the opposite may be true.\n\nThe difference between Q4 and Q8 becomes especially¬†pronounced¬†in extended tasks where a structured 'flow' isn't utilized; a single logical error can lead to an¬†irreversible¬†divergence in the output. However, if a multi-model flow is implemented where each model is assigned a single task, Q4 is often sufficient. That said, I have encountered cases where a Qwen 30B (A3B) at Q8 provided answers that even a Qwen 80B at Q4 could not. I realize, of course, that many might disagree with this assessment.",
          "score": 1,
          "created_utc": "2026-02-09 14:31:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gtwta",
              "author": "mr_ocotopus",
              "text": "Hey, I agree to what you are saying.   \nAs LLM's get deployed on more and more systems there will be many \"right kind\" of ways to use it.   \nLike you have observed in your case it could be Q4 or Q8.   \nWhat compressGPT is trying to do is, provide a high level API to build the right kind of model that you would need. And if this particular flow does not workout you can itterate with other models/experiments faster - But since fine-tuning and quantising is a powerful tool it will be helpful to check weather a model passed down this pipeline will work or not. \n\nTLDR;   \nModels are \"correct model\" for a specific task, compressGPT offers one way of building \"right model\"",
              "score": 1,
              "created_utc": "2026-02-09 17:38:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r041y4",
      "title": "A RAG Agent and their Types",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1r041y4",
      "author": "KarllsMarcel",
      "created_utc": "2026-02-09 13:21:33",
      "score": 9,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r041y4/a_rag_agent_and_their_types/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4fh4qy",
          "author": "flonnil",
          "text": "ballsy to use n8n screenshots in something that tries to sell you as a \"pro\".",
          "score": 2,
          "created_utc": "2026-02-09 13:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j8ojz",
          "author": "dezastrologu",
          "text": "Cool more LLM generated slop",
          "score": 1,
          "created_utc": "2026-02-10 00:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ptltc",
          "author": "SystemFlowStudio",
          "text": "One thing that helped me think about RAG agents more clearly was separating **retrieval responsibility** from **control responsibility**.\n\nRough breakdown I keep coming back to:\n\n**1. Passive RAG**  \nRetrieval is a single, fixed step (query ‚Üí retrieve ‚Üí answer). No iteration, no state. Easy to reason about, hard to scale to complex tasks.\n\n**2. Tool-augmented RAG agent**  \nThe agent decides *when* to retrieve vs act. Retrieval becomes a tool call, often interleaved with reasoning steps. This is where looping and stale context issues start to appear.\n\n**3. Planner-Executor RAG**  \nPlanner decomposes the task, executor performs steps, retrieval happens per step. Much more powerful, but you need guardrails or you get infinite plan-replan cycles.\n\n**4. Memory-augmented / stateful RAG**  \nRetrieval isn‚Äôt just documents ‚Äî it includes prior actions, summaries, or checkpoints. Great for long tasks, very easy to accidentally poison the context.\n\nWhat I‚Äôve noticed is most ‚ÄúRAG agent bugs‚Äù aren‚Äôt retrieval quality issues ‚Äî they‚Äôre **control-flow failures** (no termination condition, repeated tool calls, planner drift).\n\nCurious how others here are drawing these boundaries ‚Äî especially in production systems.",
          "score": 1,
          "created_utc": "2026-02-11 00:42:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz2edh",
      "title": "Golang or Python",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qz2edh/golang_or_python/",
      "author": "Ok-Satisfaction945",
      "created_utc": "2026-02-08 07:14:07",
      "score": 9,
      "num_comments": 19,
      "upvote_ratio": 1.0,
      "text": "Why python over golang? Current on my first year of mechatronics looking to expand and get ahead. I just bought a Jetson Orin nano I would like to start tinkering with. I understand python is the right now but from research I done I feel like golang really got more potential overall. Would love to hear from people in this space.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qz2edh/golang_or_python/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o48u3q8",
          "author": "etherealflaim",
          "text": "Go is better for building systems. Python is better for stitching together other people's stuff, which in the world of AI means a lot more Lego bricks to do random stuff like training and evaluating models locally.  If you're using cloud models like Gemini, model gateways, or providers like ollama though, suddenly this advantage gets blunted.\n\nFor agentic systems, Temporal (which was built in Go but that isn't super relevant) is a killer technology and you can even mix and match Go and Python where each one suits.",
          "score": 6,
          "created_utc": "2026-02-08 12:55:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a0ylc",
              "author": "Ok-Satisfaction945",
              "text": "Thanks for the straight to the point explanation also wasn‚Äôt aware of temporal looking into it üëçüèæ",
              "score": 2,
              "created_utc": "2026-02-08 16:53:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4c5agf",
              "author": "Osi32",
              "text": "I like this answer. Well put.",
              "score": 1,
              "created_utc": "2026-02-08 23:11:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48hoy1",
          "author": "Osi32",
          "text": "I built a large (non-AI) middleware last year in Golang.\nFirst off- I love Golang. It‚Äôs powerful, fast and efficient. It‚Äôs truly an amazing language.\n\nHowever, adoption of a new language is slow.\n\nMy general advice is- if you ever plan to hire people to maintain what you‚Äôre building or get someone else to help maintain it. Python programmers are a dime a dozen (figure of speech, not literally ;))\n\nGolang is great if you want to build something from scratch with minimal external libraries from other people.",
          "score": 2,
          "created_utc": "2026-02-08 11:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48n2cl",
              "author": "Ok-Satisfaction945",
              "text": "I see , I was getting the impression that with python it‚Äôs like ‚Äú we been doing it this way why change now?‚Äù Type scenario that‚Äôs why I wanted to ask the users . Solid advice personally I think I will start with golang just didn‚Äôt know it was it something fundamentally that I guess wasn‚Äôt capable of compared to python in action/real world.",
              "score": 2,
              "created_utc": "2026-02-08 12:00:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48nx57",
          "author": "burntoutdev8291",
          "text": "Python. Faster iterations and POC. Libraries are also mostly in Python.\n\nI also say this with knowledge in rust and go.\n\nWith that said, you didn't really provide a use case. Do you want to do pure backend, devops, agentic, RAG, traditional AI engineering?",
          "score": 2,
          "created_utc": "2026-02-08 12:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48pn7a",
              "author": "Ok-Satisfaction945",
              "text": "I‚Äôm kind of all over the place atm, but I plan to go in mostly in robotics & agentic/ai integration. Wanted something I could take into different spaces. If I can‚Äôt spin of my own thing I plan of going to work at Lockheed, manufacturing/aerospace something of that nature. As far as python I noticed it‚Äôs more resources available. I just didn‚Äôt wanna learn something potentially useless or outdated by the time I get anywhere you know?",
              "score": 1,
              "created_utc": "2026-02-08 12:21:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47tlr2",
          "author": "tom-mart",
          "text": "Beacuse I know Python.",
          "score": 1,
          "created_utc": "2026-02-08 07:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47u894",
              "author": "Ok-Satisfaction945",
              "text": "Understandable, it just seems like Golang got way more potential & scalability, but again I‚Äôm not in any ‚Äúspace‚Äù per se. I‚Äôm starting sorta fresh the only limited experience I have is with xml & mySQL from hosting gaming servers in the past. If you could start again would you still choose python?",
              "score": 1,
              "created_utc": "2026-02-08 07:31:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47v08g",
                  "author": "tom-mart",
                  "text": ">it just seems like Golang got way more potential & scalability\n\nAll the major AI tools are written in Python. Starting with PyTorch and ending on the agentic frameworks like Pydantic AI or Langchain. Not sure how you see more potential in Golang but if you do then the choice should be simple for you.",
                  "score": 2,
                  "created_utc": "2026-02-08 07:38:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dinb6",
          "author": "Terrible_Tangelo6064",
          "text": "I thought the title said \"golang or prison\"",
          "score": 1,
          "created_utc": "2026-02-09 03:45:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g4xhm",
          "author": "BidWestern1056",
          "text": "most science researchers in ml/RL/AI use python so it is the most natural to use for any adjacent spaces because there will not be as many custom libraries in golang for specific scientific/matrix/tensor operations so for these you will have to pass to python or an equivalent in another language if it exists anyway so easier just to do it all in python. ",
          "score": 1,
          "created_utc": "2026-02-09 15:39:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gdd59",
          "author": "soopazoupy",
          "text": "python because nearly every AI stack assumes python first e.g. PyTorch, TensorFlow, ROS2 tooling, OpenCV, CUDA bindings, and most example repos you‚Äôll find are python native. small bonus with python is that it got libraries like pydantic to make it easy to structure inputs/outputs cleanly and newer tooling around it fits nicely when you start building ai powered or sensor-heavy pipelines. go is excellent for building reliable infrastructure, but the robotics/ML ecosystem around it is thinner so you‚Äôll fight the tooling more often. that doesn‚Äôt mean go is less potential tho but just that it's in a different lane",
          "score": 1,
          "created_utc": "2026-02-09 16:19:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o486z4n",
          "author": "Novel_Leading_7541",
          "text": "You don‚Äôt need to choose‚Äîlearn both lightly; with vibe coding and AI tools now, the language matters way less than understanding the problem üôÇ",
          "score": 1,
          "created_utc": "2026-02-08 09:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48mg6l",
              "author": "Ok-Satisfaction945",
              "text": "Definitely agree with you i just had one of those late night thoughts and wanted to hear from people that have used one or the other as far as their experiences. I live in a place where what Im doing or anything computer related is non existent & finding people with programming experience it‚Äôs difficult",
              "score": 1,
              "created_utc": "2026-02-08 11:55:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fl0y9",
              "author": "m98789",
              "text": "Not entirely true.\n\nThough the spirit of what you saying is true, in practice, you do need to select a language (and library ecosystem) that LLMs have a lot of training data on.",
              "score": 1,
              "created_utc": "2026-02-09 13:52:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4952w5",
          "author": "Crafty_Disk_7026",
          "text": "If you want quality software and less bugs then Go.  If you want to move fast and make things easier then Python.  I use both everyday",
          "score": 1,
          "created_utc": "2026-02-08 14:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a09fu",
              "author": "Ok-Satisfaction945",
              "text": "Anything you would say go lack that python it‚Äôs superior? And as far as libraries for go are they decently availability?",
              "score": 1,
              "created_utc": "2026-02-08 16:49:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4a1ngk",
                  "author": "Crafty_Disk_7026",
                  "text": "I would say no, atleast not for me, if I want to use Go for something I generally can.  People other than me may say that though.  Also it's not a big deal to have systems with 2 ore more languages that happily work together.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:56:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2ree2",
      "title": "Lessons from building AI shopping assistant for 1B$+ skincare brand.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "author": "rudzienki",
      "created_utc": "2026-02-12 11:50:51",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.85,
      "text": "Hey! I was recently hired to build an AI shopping assistant for a huge brand, 1B$+ in revenue. Unfortunately can't say which one is it (damn NDAs), but I thought I'd share some lessons. After the project CTO told me ‚ÄúWorking with you was the best AI investment in the last year‚Äù, so I guess it went well!\n\nI'm reposting this from my linkedin, so sorry for this \"linkedinish\" vibe:\n\nThe biggest secret was, surprise, surprise, **not** wasn‚Äôt fancy AI methods, complex RAG pipelines, and multi step workflows. In the end it was good prompts, a bunch of domain-specific tools and one subagent.  \n  \nThe secret was the process.  \n  \nI didn‚Äôt know anything about skincare so I had to learn about it. Even light understanding of the domain turned out EXTREMELY IMPORTANT since it allowed m to play around with an agent and have a good judgement whether it says good things. The fastest feedback loop is always \"in your head\".   \n  \nI built a domain-specific dashboard for the client. A collaborative environment where domain experts can play around with an agent, comment, feedback, etc. I took the idea from [Hamel Husain](https://x.com/HamelHusain) who said that [‚ÄúThe Most Important AI Investment is A Simple Data Viewer‚Äù.](https://x.com/i/status/1991903412997509372) He was damn right about it.   \n  \nThe last thing is something that is not talked much about but it should. We got hundreds of files about company knowledge. This knowledge is spread around big organisations like crazy. But if you really really understand the domain, if you really digest it all and ask a lot of questions, you‚Äôll be able to COMPRESS this knowledge. You‚Äôll find common stuff, remove dead ends, and really narrow it down to sth that expresses most about this company in smallest piece of text. This is your system prompt!! Why split context and add a potential point of failure if you can have MOST of the important stuff always in the system prompt? It‚Äôs crazy how well it works.  \n  \nOn the context engineering side we ended up with a great system prompt + a bunch of tools for getting info about products. I added one subagent for more complex stuff (routine building), but that was the only ‚Äúfancy‚Äù thing out there.  \n  \nI think the lesson here is that building agents is not hard on the technical level, and every developer can do it! The models do all the heavy lifting and they‚Äôre only getting better. The secret is understanding the domain and extracting the domain knowledge from people who know it. It's communication.\n\n  \nI'm curious:\n\nHave you built such \"customer support\"-related agents for your companies too? One thing that triggers me is amount of those giant SaaS companies that promises \"the super ultra duper ai agent\", and honestly? I think they don't have much secret sauce. Models are doing heavy lifting, and simple methods where heavy lifting is done by domain-specific knowledge trump general purpose ones. \n\nHere's what Malte from Vercel recently wrote btw:\n\nhttps://preview.redd.it/h2pjrjfix1jg1.png?width=1198&format=png&auto=webp&s=c8cd25ac93ee3a1b92cab153a1c591edbaf35d78\n\nIt somehow clicks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4ysvg2",
          "author": "HatApprehensive141",
          "text": "‚ÄúSecret sauce‚Äù = good prompts, domain tools, and actually understanding the business‚Ä¶ basically just doing your job properly.\n\nLots of companies hype up intergalactic RAG pipelines, but if you don‚Äôt compress real domain knowledge into a clear system, your agent is just an overconfident intern. The real edge isn‚Äôt the model magic, it‚Äôs the context quality.",
          "score": 8,
          "created_utc": "2026-02-12 11:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z7lpo",
          "author": "tom-mart",
          "text": "Wait till you discover the water is wet.",
          "score": 4,
          "created_utc": "2026-02-12 13:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ysv0w",
          "author": "kubrador",
          "text": "wow so the secret sauce was just... understanding the domain and writing good prompts. truly revolutionary stuff, might as well say the secret to cooking is using fresh ingredients and knowing what tastes good",
          "score": 2,
          "created_utc": "2026-02-12 11:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z2ese",
              "author": "rudzienki",
              "text": "I don't think simplicity is always obvious. There are many merchants of complexity out there who want to tell you otherwise.\n\nThat was the point of the post.",
              "score": 1,
              "created_utc": "2026-02-12 13:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51kyxj",
          "author": "nore_se_kra",
          "text": "Beyond the hype... interesting read despite the comments here. I dont think it hurts to tell the story of applied \"boring\" company specific domain knowledge one more time.",
          "score": 2,
          "created_utc": "2026-02-12 20:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51q54o",
          "author": "SamCRichard",
          "text": "What LLM did you use or are you routing between them\n\n",
          "score": 2,
          "created_utc": "2026-02-12 20:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55hde0",
              "author": "rudzienki",
              "text": "Main thread was optimized for latency so it was good-but-not-best model, sonnet territory.\n\nSubagent was supposed to reason over many products to analyse interactions, in this case we used the best reasoning model. Still was a bit too slow with max reasoning effort, so we ended up with the best model with mid reasoning effort.",
              "score": 1,
              "created_utc": "2026-02-13 12:19:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zc408",
          "author": "ampancha",
          "text": "You're right that domain knowledge compression matters more than complex RAG for quality. The gap I see in most \"it works\" agents is what happens at production scale: prompt injection attempts from real users, hallucinated product claims becoming liability, and cost spikes without per-user attribution. For a $1B brand those risks are where the actual work starts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-12 13:57:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2s3r4",
      "title": "I dont get mcp",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "author": "Yaar-Bhak",
      "created_utc": "2026-02-12 12:26:56",
      "score": 8,
      "num_comments": 13,
      "upvote_ratio": 0.83,
      "text": "All I understood till now is - \n\nI'm calling an LLM api normally and now\nInstead of that I add something called MCP which sort of shows whatever tools i have? And then calls api \n\n\nI mean, dont AGENTS do the same thing? \n\nWhy use MCP? Apart from some standard which can call any tool or llm \n\nAnd I still dont get exactly where and how it works \n\nAnd WHY and WHEN should I be using mcp? \n\nI'm not understanding at all üò≠ Can someone please help\n\n",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4z3q6l",
          "author": "rudzienki",
          "text": "It's just a standardised way for companies to \"expose their tools\".\n\nIf you're Stripe you have a bunch of tools: \"do payment\", \"check invoices\" etc. If you want your agent to use them you can just... add them as tools to your agent. That's it. \n\nBut with MCP you can just say \"connect to stripe MCP\" and it automatically fetches all Stripe tools to be called. Stripe updates tools, you get update automatically.\n\nBut aside from that - no difference. \n\nBtw, MCP is much bigger protocol that handles more stuff than exposing tools, but in reality it's 99%, other uses didn't get much traction as far as I know.",
          "score": 7,
          "created_utc": "2026-02-12 13:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zrwup",
          "author": "fooz42",
          "text": "Service registry and discovery for remote procedure call is a wheel that gets reinvented every platform. It's not a revolution except in the sense the wheel gets reinvented every time the cycle turns, and now I'm getting dizzy from my metaphor.",
          "score": 12,
          "created_utc": "2026-02-12 15:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yy3mm",
          "author": "kubrador",
          "text": "mcp is basically \"what if we made tool use boring and standardized so literally any llm can talk to literally any tool without rewiring everything\" agents let your llm pick tools. mcp is the \\*protocol\\* so your llm doesn't need to know what tools exist. they just show up. it's the difference between \"here's a menu\" vs \"here's a standardized way to hand someone a menu\"\n\nyou need it when you're tired of writing custom integrations for every tool+llm combo. you don't need it if you're just bolting claude into your thing once and calling it a day.",
          "score": 4,
          "created_utc": "2026-02-12 12:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yzpvs",
              "author": "Yaar-Bhak",
              "text": "you think this can be used in production?\n\nand this means mcp would be used only in agentic flows right?",
              "score": 1,
              "created_utc": "2026-02-12 12:42:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54buzg",
          "author": "FoldedKatana",
          "text": "MCP is dead now. OpenClaw skills are where it's at",
          "score": 2,
          "created_utc": "2026-02-13 06:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50wvs5",
          "author": "throwaway490215",
          "text": "MCPs are bullshit. They are a standard that basically tells the program run on your computer to prepend `some-tool --help` when you start a conversation, but with much more overhead, and **every conversation** even if you dont want to use `some-tool` this session. \n\nAnybody talking about credentials/authentication is a moron. \n\nJust add a \"Use `some-tool --help` to do X\" in your AGENTS.md and you're good.",
          "score": 2,
          "created_utc": "2026-02-12 18:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z5ouz",
          "author": "Astronos",
          "text": "it is function/tool calling over api",
          "score": 1,
          "created_utc": "2026-02-12 13:21:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zo2ke",
          "author": "Crafty_Disk_7026",
          "text": "MCP is just like an open ai spec the ai can read and know how to use your tool. It's literally just instruction manual",
          "score": 1,
          "created_utc": "2026-02-12 15:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tzq6",
          "author": "voidiciant",
          "text": "From what I understand:\n\nThe models have to be trained (and usually are, there is often a ‚Äûtool‚Äú tag on the downloads) to insert special keywords in their responses when a tool call is appropriate. \n\nThese keywords are intercepted by the runtime (the thing taking your input, converting to tokens,, etc) and the runtime performs the appropriate calls to the registered mcp tools (according to the protocol) and feeds back the tool-call results to the model, which in turn now incorporates them in the next response.\n\nAdditionally, and here I get fuzzy, the runtime generates a system prompt that contains a list of available MCP Tools, and the model is trained to understand this to generate the relevant keywords in the response based. \n\nMCP defines the protocols/API/formats. \n\nThat‚Äòs the gist for me",
          "score": 1,
          "created_utc": "2026-02-12 18:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51rpl9",
          "author": "CreepyValuable",
          "text": "MCP is kind of sort of a universal adapter to plug anything from ChatGPT to your toaster together.\n\nIt's not quite that straightforward and the actual interface is kind of clunky but it's pretty useful.\n\nFor example, my (not very good, but experimental so that's not important) AI uses it for things like a weather service, XiaoZhi AI esp support (essentially a smart speaker with a screen), VS Code integration and some other random things. It avoids needing a whole bunch of incompatible APIs.",
          "score": 1,
          "created_utc": "2026-02-12 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52svxj",
          "author": "Glum_Teaching8224",
          "text": "It's just tool using reference for the agent. ",
          "score": 1,
          "created_utc": "2026-02-13 00:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwht1",
          "author": "Electronic-Door7134",
          "text": "Good luck explaining to an auditor why your gave a 3rd party company full access to your company data (which is what happens without mcp)",
          "score": -4,
          "created_utc": "2026-02-12 15:41:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50avet",
              "author": "PaddingCompression",
              "text": "Wut",
              "score": 3,
              "created_utc": "2026-02-12 16:48:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyimtf",
      "title": "Grounding Is Not a Prompt",
      "subreddit": "LLMDevs",
      "url": "https://substack.com/home/post/p-187075330",
      "author": "SeriousSir1148",
      "created_utc": "2026-02-07 16:45:11",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qyimtf/grounding_is_not_a_prompt/",
      "domain": "substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r2qfcz",
      "title": "Mix prompts instead of writing them by hand",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/5z7edpx9n1jg1.png",
      "author": "Everlier",
      "created_utc": "2026-02-12 10:55:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2qfcz/mix_prompts_instead_of_writing_them_by_hand/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r204ab",
      "title": "Intent Model",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "author": "Repulsive_Laugh_1875",
      "created_utc": "2026-02-11 15:18:54",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hi community,  \nthis is my first post here üôÇ\n\nI‚Äôm an experienced AI Engineer / AI DevOps Engineer / Consultant working for a well-known US-based company. I‚Äôd really appreciate your thoughts on a challenge I‚Äôm currently facing and whether you would approach it differently.\n\nUse-Case\n\nI‚Äôm building an **intent classifier** that must:\n\n* Run **on edge**\n* Stay around **\\~100ms latency**\n* Predict **1 out of 9 intent labels**\n* Consider **up to 2 previous conversation turns**\n\nThe environment is domain-specific (medical domain in reality), but to simplify, imagine a system controlling a car.\n\nExample:  \nYou have an intent like `lane_change`, and the user can request it in many different ways.\n\nCurrent Setup\n\n* Base model: **phi-3.5-mini-instruct**\n* Fine-tuned using **LoRA**\n* Model explicitly outputs only the intent token (e.g., `command_xyz`)\n* Each intent is mapped to a **single special token**\n* Almost no system prompt (removed to save tokens)\n\nPerformance:\n\n* \\~110ms latency (non-quantized) ‚Üí acceptable\n* \\~10 input tokens on average\n* \\~5 output tokens on average\n* 25k training samples\n* \\~95% accuracy\n\nSpeed is not the main issue ‚Äî I still have some room for token optimization and quantization if needed.\n\nthe real challenge -> the missing 5%.\n\nThe issue is **edge cases**.\n\nThe model operates in an open-input environment. The user can phrase requests in unlimited ways.\n\nFor example:  \nFor `lane_change`, there might be 30+ semantically equivalent variations. I built a synthetic data generation pipeline to create such variations and spent \\~2 weeks refining it. Evaluation suggests it's decent.\n\nBut:\n\nThere are still rare phrasings that the model hasn‚Äôt seen ‚Üí wrong intent prediction.\n\nOf course, I can:\n\n* Iteratively collect misclassifications\n* Add them to the training set\n* Retrain\n\nBut that‚Äôs slow and reactive.\n\nConstraints:\n\n* I could use a larger model (e.g., phi-4), and I‚Äôve tested it.\n* However, time-to-first-token for phi-4 is significantly slower.\n* Latency is more important than squeezing out a few extra percent of quality.\n\nSo scaling up model size isn‚Äôt ideal.\n\nMy questions to you:\n\nHow would you tackle the final 5%?\n\nI‚Äôd really appreciate hearing how others would approach this kind of edge, low-latency intent classification problem.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4tefqh",
          "author": "Swimming-Chip9582",
          "text": "Can you detect whether the output is likely to be an edge case or know when it's part of an uncertain category?  Perhaps you can fallback on a larger model and accept latency when it's unsure. So starting both the small and large concurrent, if the small finishes first and is all good just cancel the large one, if it's uncertain then wait for completion from the bigger model. ",
          "score": 4,
          "created_utc": "2026-02-11 15:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tll8h",
          "author": "FoodAccurate5414",
          "text": "You need to look into using very very very small models to handle edge cases. There are tons on hugging face. Run it along side your main model",
          "score": 2,
          "created_utc": "2026-02-11 16:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57q9",
              "author": "Repulsive_Laugh_1875",
              "text": "Can you recommend something or at least tell which ones you have in mind?",
              "score": 1,
              "created_utc": "2026-02-11 20:43:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t91xc",
          "author": "InfraScaler",
          "text": "Ok, so catching new ways of expressing the intent from your users is definitely reactive, but what about using bigger LLMs to generate those for you? Can you even use agents leveraging big LLMs to \"test\" and help prepare training for your system? Not necessarily cheap, but your employer may be able to afford it :-)",
          "score": 1,
          "created_utc": "2026-02-11 15:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tc616",
              "author": "Repulsive_Laugh_1875",
              "text": "It‚Äôs not the employer ‚Äî it‚Äôs the customer who has to pay for it üòâ\n\nJokes aside, thank you for your comment.\n\nI‚Äôm already using GPT-5.2-chat for the synthetic data generation. As mentioned, I‚Äôm currently achieving a full match rate  (intent plus parameters) of around 95%, and based on the latest metrics even closer to 97%, which I consider quite solid.\n\nThat‚Äôs why I don‚Äôt believe the data generation itself is the core issue here.\n\n  \n\\-------------------------- edited\n\n  \nin fact, I also thought about leveraging two agents to simulate such qustion answer things and try to figure out such edge cases. But this is costly.",
              "score": 2,
              "created_utc": "2026-02-11 15:38:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4thauy",
                  "author": "InfraScaler",
                  "text": ">But this is costly.\n\nNot for you! :P",
                  "score": 2,
                  "created_utc": "2026-02-11 16:02:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ybuxq",
                  "author": "BehindUAll",
                  "text": "Why is cost an issue? If you are using 2 classifiers, they are cheaper than LLMs and if you are using LLMs like Llama-3.1 8b or Mistral small etc. you will get great speed and is cheap too. Plus you can use groq, SambaNova or Cerebras for fast and cheap inference. And for LLMs the input will be like 10 words and output 1 word if you do your system prompt right. The cost goes way down.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:17:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vvjay",
          "author": "Charming_Support726",
          "text": "First I doubt that models with decoder only architecture are the best fit for this task.  They allways bring error classes which are unwanted in these scenarios.\n\nSecond I doubt, that you ever will reach 100% - but that's obvious\n\nThird: Encoder-Decoder architectures promise the best fit and efficiency for these tasks, but you'll never know ( we did back in 2020 over a 1000 intents with Rasa in a BERT-Style Intent detector went over 90% but appeared still flaky). T5Gemma Style Models could be a solution, but I got no experience on fine tuning them.\n\nFourth you could apply additional techniques like reranking or building a similarity distance to sample sentences to make sure that your generation is a valid result.  Maybe good to combine this approaches with multiple generations of the result.\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-11 22:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ysnco",
              "author": "Repulsive_Laugh_1875",
              "text": "To your fist point: I agree to this but I don¬¥t get what you mean by error classes? Do you mean just a wrong prediction label with this? If thats your point, you are right. These are the edge cases what I¬¥m talking about! It never responds in a different way, it just predicts sometimes the wrong label (edge cases).\n\n  \nTo your second point -> That is obvious and clear. Customer is already pretty satisfied with the solution, but I¬¥m not :D\n\n  \nthird: good hint. I need to look into them on how to really finetune them.\n\n  \nfourth: Indeed, I also wanted to test if you purely rely on similarity (history dependend intents excluded). I wanted to test if you can embedd those 25k and just detect the intent based on the similarity. I did something similar a few years back and this worked also very solid ;-) Now with the larger embedding representations, could also be an option.",
              "score": 1,
              "created_utc": "2026-02-12 11:51:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o51kkkq",
                  "author": "Charming_Support726",
                  "text": "Decoder only does not stick close to the input as encoder-decoder do. So you get all the things like hallucinations and similar. They also have issues generating \"i dont know\" tokens. \n\nIf you have an encoder part in the model like in a T5 the model is always working with a kind of embedding as knowledge representation for building a bridge to the decoder. \n\nThat's you are 90% there using an SLM, so these are my ideas to improve.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:24:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yfzud",
          "author": "tshawkins",
          "text": "If it is acceptable to have edge case resolution slower, you could push the cases that fail to resolve out to a bigger model, and push those requests and thier resolutions into a set of training data for your next primary model rebuild. Its a hybrid solution that uses both bigger model and retraining for the 5% only.",
          "score": 1,
          "created_utc": "2026-02-12 09:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u6y1l",
          "author": "TheBioto",
          "text": "Give Mistral 3b a shot and see how it works. \n\nI am currently doing something similar with endpoint nodes with small models. Mistral was the only one that was fast enough/could be guided enough to accomplish my needs.\n\nI don't have any suggestions for your edge cases issue, good luck!",
          "score": 1,
          "created_utc": "2026-02-11 18:01:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r21425",
      "title": "I tracked how much time my team wastes re-explaining context. The number is embarrassing.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r21425/i_tracked_how_much_time_my_team_wastes/",
      "author": "arapkuliev",
      "created_utc": "2026-02-11 15:56:39",
      "score": 7,
      "num_comments": 12,
      "upvote_ratio": 0.82,
      "text": "I got annoyed enough to actually measure this.  \n  \nFor two weeks I logged every time someone on my 6-person eng team had to re-explain context to someone else. Slack catch-ups, meeting recaps, \"here's what we decided and why\" conversations, onboarding someone into a thread.  \n  \n47 times per week. Average 8.5 minutes each. That's 6.5 hours/week of engineering time just... transferring context from one brain to another.  \n  \nAnd that's only the times someone *bothered*. How many times did someone just not explain, and the other person made a decision with half the picture?  \n  \nWe have Notion. We have Confluence. We have a wiki nobody reads. We have \"just search Slack\" which is a joke. None of it works because the context is either stale, unfindable, or so buried in noise that nobody trusts it.  \n  \nI've been experimenting with treating it as a memory problem instead of a documentation problem. Like, what if decisions and context got captured *as they happened* instead of someone writing a doc after the fact (which nobody does)?  \n  \nEarly results are promising but I'm curious: is anyone else measuring this? Or are we all just accepting \"let me catch you up\" as a normal part of work?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r21425/i_tracked_how_much_time_my_team_wastes/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4vfvzu",
          "author": "radarsat1",
          "text": "yes, humans talking to each other is a normal part of work.",
          "score": 14,
          "created_utc": "2026-02-11 21:35:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55yfuf",
              "author": "MannToots",
              "text": "I like when people succinctly prove they didn't read the whole thing.¬†¬†",
              "score": 0,
              "created_utc": "2026-02-13 14:01:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x7jvn",
          "author": "Nofoofro",
          "text": "It sounds like a culture, process and info architecture issue.¬†\n\nSo many of these posts have so little detail. So many words to say so little.",
          "score": 5,
          "created_utc": "2026-02-12 03:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vi0yz",
          "author": "WolfeheartGames",
          "text": "This is a problem of information theory and how communication works.",
          "score": 3,
          "created_utc": "2026-02-11 21:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50jv89",
          "author": "UncleRedz",
          "text": "May I ask how many products and/or feature areas your team is responsible for and the size of the team? Too big teams and self organization doesn't work, information starts to become siloed. Too many products and/or feature areas and the cognitive load increases to a point where it's simply not possible to keep track of everything by everyone. Then you need to split into sub-teams with specific areas of responsibilities, which you can only do if the team is big enough. So it's both organizational and resource constrainted. I guess you could call it a memory problem, but i'd say it's more about memory capacity then.",
          "score": 2,
          "created_utc": "2026-02-12 17:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z8bxu",
          "author": "PaddingCompression",
          "text": "This is my favorite part of vibe coding.\n\nLLMs are so obviously bad at context that I'm forced to be really good at explicit communication and organization of information.  These skills are just generally pretty useful in life.\n\nIt's like practicing delegation skills on hard mode.",
          "score": 1,
          "created_utc": "2026-02-12 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ztvpo",
          "author": "Mad_Tyrion",
          "text": "you are not alone :(",
          "score": 1,
          "created_utc": "2026-02-12 15:29:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5404zr",
          "author": "mimitwothree",
          "text": "Is that the problem or job stress? Let me out it this way. Your parents asked you what probably a billion time in a year to do something and u still didn't.\n\nShall I say lazy?\nCan't be bothered?\n\nNo bro. \n\nIt's just growing as a person . If this bothers u then trust me this is not the problem. \nThe problem is the job and you . \nRelax .",
          "score": 1,
          "created_utc": "2026-02-13 04:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56s9o2",
          "author": "gh0stwriter1234",
          "text": "Probably the most effective way to reduce this is have the correct person decide things... decentralized decision making is always going to be less efficient but it is also more robust at various levels as well as in distributing blame in the event of failure because instead of throwing one person under the bus and continuing to be bad everyone has to accept reality.\n\nSometimes being effective is counter productive in the long run because of the secondary effects.",
          "score": 1,
          "created_utc": "2026-02-13 16:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57bphs",
          "author": "siddu71",
          "text": "Don't optimize humans like robots or programming...",
          "score": 1,
          "created_utc": "2026-02-13 18:03:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy8ptz",
      "title": "Your agent's 100% pass rate on 10 runs is statistically compatible with 72% true reliability. Here's the math and a way to fix your CI.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qy8ptz/your_agents_100_pass_rate_on_10_runs_is/",
      "author": "Better_Accident8064",
      "created_utc": "2026-02-07 08:39:48",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.75,
      "text": "I ran a LangGraph agent with Claude 3.5 Haiku on a trivial task (\"What is 15 * 37?\") across 100 trials. Pass rate: 70%. Not 95%, not 99%. Seventy percent on a calculator task.\n\nThe interesting part isn't that agents fail ‚Äî everyone here knows that. It's that **single-run evals can't detect it.** If you run 10 trials and get 10/10, Wilson score CI at 95% confidence gives you [0.722, 1.000]. Your \"perfect\" result is statistically compatible with a system that fails 28% of the time.\n\nThis matters for CI/CD. Most teams either skip agent evals in their pipeline or run each test once and assert pass/fail. Both approaches have the same problem: they can't distinguish a 95%-reliable agent from a 70%-reliable one unless you run enough trials.\n\n**What actually works for catching regressions:**\n\nRun each test case N times (N >= 20 makes a real difference). Compute Wilson CI on the pass rate. Compare against your baseline using Fisher exact test instead of naive diff. Use Benjamini-Hochberg correction if you're testing multiple cases simultaneously ‚Äî otherwise you'll get false alarms.\n\nFor failure attribution: group trials into pass/fail, compare tool call distributions at each step, pick the step with the lowest Fisher p-value. This gives you \"step 2 tool selection is the bottleneck\" instead of \"test failed.\"\n\nI open-sourced the framework I built for this: [agentrial](https://github.com/alepot55/agentrial). It wraps any Python callable and has adapters for LangGraph, CrewAI, AutoGen, Pydantic AI, OpenAI Agents SDK, and smolagents. YAML config, runs in CI, exit code 1 on statistically significant regression.\n\n```\nbasic-math      20/20  CI=[0.839, 1.000]  PASS\nmulti-step      14/20  CI=[0.480, 0.862]  FAIL\n  ‚Üí Step 2: tool selection diverges (p=0.003)\n```\n\nCurious how others are handling this. Are you running multi-trial evals in CI? Using soft thresholds? Something else entirely?\n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qy8ptz/your_agents_100_pass_rate_on_10_runs_is/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o41ydxz",
          "author": "Thick-Protection-458",
          "text": "\\> Your agent's 100% pass rate\n\nWell, isn't it clear?\n\nI mean that's fucking heuristic rather than predictable algorithm. With additional sampling randomness.\n\nYou can never have 100% working solution in such a case. You can have (unknown to you yet) distribution of test results you can't reliably distinguish from 100%.",
          "score": 4,
          "created_utc": "2026-02-07 09:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o420e97",
              "author": "Better_Accident8064",
              "text": "Exactly. You can't distinguish true 100% from a sample that happened to look like 100%. But you can quantify it.\n\n20/20 passes gives a Wilson 95% CI of \\[0.839, 1.000\\], so your \"perfect\" score is compatible with 84% true reliability. agentrial just makes this math explicit and wires it into CI/CD.",
              "score": 1,
              "created_utc": "2026-02-07 09:37:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43me85",
          "author": "lionmeetsviking",
          "text": "I use this: https://github.com/madviking/pydantic-llm-tester",
          "score": 1,
          "created_utc": "2026-02-07 16:15:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzm5bo",
      "title": "Skill Seekers v3.0.0 - Universal doc preprocessor for AI systems",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qzm5bo/skill_seekers_v300_universal_doc_preprocessor_for/",
      "author": "Critical-Pea-8782",
      "created_utc": "2026-02-08 21:56:24",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "TL;DR: One command converts docs into any AI format.\n\n  The Problem: Every AI project needs documentation preprocessing:\n\n  ‚Ä¢ RAG pipelines need clean, chunked text\n  ‚Ä¢ AI coding tools need structured knowledge\n  ‚Ä¢ Claude/GPT need formatted skills\n\n  Everyone rebuilds the same scrapers.\n\n  The Solution:\n\n  pip install skill-seekers\n  skill-seekers scrape --config react.json\n\n  16 Output Formats:\n\n   RAG/Vectors: LangChain, LlamaIndex, Chroma, FAISS, Haystack, Qdrant, Weaviate\n\n   AI Coding: Cursor, Windsurf, Cline, Continue.dev\n\n   AI Platforms: Claude, Gemini, OpenAI\n\n   Generic: Markdown\n\n  26 MCP Tools: Your AI agent can now prepare its own knowledge:\n\n  ‚Ä¢ scrape_docs, scrape_github, scrape_pdf\n  ‚Ä¢ package_skill, install_skill\n  ‚Ä¢ And 21 more...\n\n  Stats:\n\n  ‚Ä¢ 58,512 lines of Python\n  ‚Ä¢ 1,852 tests\n  ‚Ä¢ 100 test files\n  ‚Ä¢ 12 example projects\n  ‚Ä¢ 18 integration guides\n\n  Cloud & CI/CD:\n\n  # Upload to S3\n  skill-seekers cloud upload output/ --provider s3 --bucket my-bucket\n\n  # GitHub Action available\n\n  Links:\n\n  ‚Ä¢ GitHub: https://github.com/yusufkaraaslan/Skill_Seekers\n  ‚Ä¢ Website: https://skillseekersweb.com\n  ‚Ä¢ PyPI: pip install skill-seekers\n\n  Just launched v3.0.0 today. Happy to answer any questions!\n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qzm5bo/skill_seekers_v300_universal_doc_preprocessor_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4bzwzh",
          "author": "Critical-Pea-8782",
          "text": "hi please feel free to leave a any feedback you want and lets talk about it :) ",
          "score": 1,
          "created_utc": "2026-02-08 22:40:44",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3hvkg",
      "title": "Observation: LLMs seem to have a \"Version 2.0\" bias when generating new UIs",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/rmkiv7qmh7jg1.jpeg",
      "author": "Routine_Connection8",
      "created_utc": "2026-02-13 06:29:02",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3hvkg/observation_llms_seem_to_have_a_version_20_bias/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}