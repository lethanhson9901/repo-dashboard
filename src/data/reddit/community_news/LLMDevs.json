{
  "metadata": {
    "last_updated": "2026-02-01 08:57:56",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 130,
    "file_size_bytes": 167803
  },
  "items": [
    {
      "id": "1qq4ted",
      "title": "Building opensource Zero Server Code Intelligence Engine",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/5o9xx8n8k9gg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-29 10:15:13",
      "score": 51,
      "num_comments": 35,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq4ted/building_opensource_zero_server_code_intelligence/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ewgla",
          "author": "Key-Contact-6524",
          "text": "Gorgeous",
          "score": 3,
          "created_utc": "2026-01-29 14:14:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f2o22",
              "author": "DeathShot7777",
              "text": "â¤ï¸ðŸ« ",
              "score": 1,
              "created_utc": "2026-01-29 14:46:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gn944",
          "author": "Happythen",
          "text": "Yea, you killed it with the visualizations, great work! Working on the same thing right now, implementing Graph RAG. Fun space right now for sure.",
          "score": 3,
          "created_utc": "2026-01-29 19:01:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2go6fj",
              "author": "DeathShot7777",
              "text": "Yup. Gets painful though when LLM starts going on a spree quering the graph and ends up its context window. Hard to solve but very rewarding",
              "score": 1,
              "created_utc": "2026-01-29 19:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jh12l",
          "author": "foobarrister",
          "text": "Very well done. How are you building the graph? Looks like Leiden based . .\n\n\nCurious why you didn't use tree-sitter or language specific tools like JavaParser for Java or Roslyn for dotnet etc..Â \n\n\nWouldn't they give you a better nodes and relationships vs heuristic approach like Leiden?",
          "score": 2,
          "created_utc": "2026-01-30 03:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd95j",
              "author": "DeathShot7777",
              "text": "I m using Tree sitters. Simplified explanation : Extract IMPORTS, CALLS, DEFINES relations of each file, this already creates an accurate knowledge graph. Next use leidens algo to divide it into clusters and label those clusters ( for example AuthHandler cluster ) next find out the entrypoint of each service and DFS into the CALL chain to get the process maps in each cluster.\n\nSo the graph is quite accurate for static analyses, for the stuff like dynamic imports, runtime stuff, the cluster and process map handles most of it. These also saves a lot of tokens since the tools itself r intelligent not depending too much on LLM figuring stiff out.",
              "score": 1,
              "created_utc": "2026-01-30 07:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2grvi1",
          "author": "talltad",
          "text": "I like this, I know nothing about Software Dev but I'm working on a few things right now so I guess I'm vibe coding.  I don't know if there's a use case within this that you're looking for but if there is I'd be glad to help if needed.  It's clear this is a substantial amount of work so best of luck man!",
          "score": 1,
          "created_utc": "2026-01-29 19:23:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gt9tk",
              "author": "DeathShot7777",
              "text": "Indeed its a substantial amount of work ðŸ˜­ but good kind of pain ðŸ« . \n\nIf u would try out the MCP and plug it into your vibecoding tool for example cursor , claude code, etc load up a project into it and ask the ai about the codebase or how it works or the architecture, it should be able to go into full technical and architectural depth. Knowing the architecture even if u dont know development will help a lot in your vibecoding journey",
              "score": 2,
              "created_utc": "2026-01-29 19:29:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2hxq37",
                  "author": "talltad",
                  "text": "Cool and thanks man, I'll give it a try",
                  "score": 2,
                  "created_utc": "2026-01-29 22:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gweo8",
          "author": "SloSuenos64",
          "text": "This is so cool! Hooking up my Cursor project now....",
          "score": 1,
          "created_utc": "2026-01-29 19:44:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h59a4",
              "author": "DeathShot7777",
              "text": "â¤ï¸ try creating detailed documentation using dumber model with gitnexus vs sota opus without gitnexus. For me it worked surprisingly well",
              "score": 3,
              "created_utc": "2026-01-29 20:26:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h87ok",
                  "author": "SloSuenos64",
                  "text": "Game-changer! I used to grep through every file and hope I didn't miss a dynamic import. Now, I can see the actual dependency graph!",
                  "score": 1,
                  "created_utc": "2026-01-29 20:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2h9i9u",
              "author": "SloSuenos64",
              "text": "Immediately found Structural Redundancy issues. Thank you!",
              "score": 2,
              "created_utc": "2026-01-29 20:47:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hh2cp",
                  "author": "DeathShot7777",
                  "text": "Your comments made my day. Thanks for trying it out â¤ï¸ðŸ« ",
                  "score": 2,
                  "created_utc": "2026-01-29 21:23:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i2rs0",
          "author": "kfawcett1",
          "text": "~~Is this sending my entire codebase through your servers? Are you storing the data?~~\n\nnvm, found the answer.\n\n* All processing happens in your browser\n* No code uploaded to any server\n* API keys stored in localStorage only\n* Open sourceâ€”audit the code yourself",
          "score": 1,
          "created_utc": "2026-01-29 23:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3ohc",
              "author": "DeathShot7777",
              "text": "Client sided everything. So costs me 0 to deploy, so u all get it for free ðŸ« . Just trying to take it to a product stage from the current cool demo stage",
              "score": 2,
              "created_utc": "2026-01-29 23:15:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2i5x67",
                  "author": "kfawcett1",
                  "text": "how does it perform with 1M+ LOC codebases?",
                  "score": 2,
                  "created_utc": "2026-01-29 23:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2jwsky",
          "author": "Repulsive-Memory-298",
          "text": "I mean it looks cool but is it useful to you? The examples in the demo vid do not seem very helpful at a glance",
          "score": 1,
          "created_utc": "2026-01-30 05:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kb8wb",
              "author": "DeathShot7777",
              "text": "For me, it helps me understand repos better than DeepWiki right now. \n\nAfter I am done with File watch feature it should be able to work in background increasing accuracy of coding agents without me having to hooking up the website every time",
              "score": 1,
              "created_utc": "2026-01-30 07:24:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2kbcea",
              "author": "DeathShot7777",
              "text": "Maybe i should have shown the MCP working in cursor in the video",
              "score": 1,
              "created_utc": "2026-01-30 07:25:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k0uz7",
          "author": "Striking-Bluejay6155",
          "text": "Very nice, this amounts to a knowledge graph. Which visualization library are u using to visualize this?",
          "score": 1,
          "created_utc": "2026-01-30 06:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kaw3z",
              "author": "DeathShot7777",
              "text": "Sigma js + ForceAtlas2 and some custom logic and hit and trial to reduce the clumping up of nodes",
              "score": 2,
              "created_utc": "2026-01-30 07:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kuaqt",
                  "author": "Striking-Bluejay6155",
                  "text": "Thank you",
                  "score": 2,
                  "created_utc": "2026-01-30 10:16:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k977b",
          "author": "slowlearningovrtime",
          "text": "Couldnâ€™t figure out the local LLM connection",
          "score": 1,
          "created_utc": "2026-01-30 07:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kb5dj",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/hnk1zn8mufgg1.png?width=407&format=png&auto=webp&s=67c054d2a1b0cb465b290b58f4ae823392167066\n\nblocked by my AV",
          "score": 1,
          "created_utc": "2026-01-30 07:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kbi6q",
              "author": "DeathShot7777",
              "text": "Huh. I have no idea why this happened even though its client sided",
              "score": 1,
              "created_utc": "2026-01-30 07:26:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ksgok",
          "author": "snirjka",
          "text": "Well done ðŸ‘ðŸ»\nCool af",
          "score": 1,
          "created_utc": "2026-01-30 10:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ksygl",
              "author": "DeathShot7777",
              "text": "â¤ï¸ trying to make this into a product from current cool demo stage ðŸ¤ž",
              "score": 2,
              "created_utc": "2026-01-30 10:04:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ktlqw",
          "author": "redvox27",
          "text": "Looks incredible man! I'll check it out later today, and share my thoughts if you find that helpful",
          "score": 1,
          "created_utc": "2026-01-30 10:10:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l1jiu",
              "author": "DeathShot7777",
              "text": "Thanks. Would be really helpful",
              "score": 1,
              "created_utc": "2026-01-30 11:18:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rsp6h",
          "author": "Fresh_State_1403",
          "text": "this looks fascinating. is it practical too?",
          "score": 1,
          "created_utc": "2026-01-31 11:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rstuz",
              "author": "DeathShot7777",
              "text": "Haiku 4.5 was able to produce better architecture docs compared to opus 4.5 using gitnexus MCP in cursor. So has potential",
              "score": 1,
              "created_utc": "2026-01-31 11:09:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoj60d",
      "title": "I relied on stateless retrieval for long-form agents. It failed after 50 turns. Hereâ€™s how Iâ€™m managing state now.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qoj60d/i_relied_on_stateless_retrieval_for_longform/",
      "author": "gt_roy_",
      "created_utc": "2026-01-27 16:48:00",
      "score": 32,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Full disclosure: Iâ€™m the dev behind this project.\n\nIn long-running agent sessions (\\~50â€“100 turns), I kept seeing the same failure mode: preferences established early would silently stop affecting generation, even though they were still retrievable. You build a cool agentic workflow, and it works great for the first few turns. By turn 60, it starts doing those statistical parlor tricks where it just ignores half your instructions or forgets a preference you established three sessions ago.\n\nThe problem is that stateless retrieval is, well, stateless. Itâ€™s fine for pulling static docs, but it doesn't actually 'learn' who the user is. You can try recursive summarization or sliding windows, but honestly, youâ€™re just burning tokens to delay inevitable instruction drift.\n\nI spent the last few months building a layer to handle long-term state properly. Iâ€™m calling it MemOS (probably an overloaded term, but it manages the lifecycle). Itâ€™s an MIT-licensed layer that sits between the agent and the LLM.\n\nWhy stateless retrieval isn't enough:\n\nThe first thing people ask is why not just use a Vector DB. They are great for storage, but they don't have a logic layer for state. If a user says 'I hate Python' in turn 5 and 'actually Iâ€™m starting to like Python' in turn 50, a standard search returns both. Itâ€™s a mess.\n\nMemOS handles the lifecycleâ€”it merges similar memories, moves old stuff to a 'MemVault' (cold storage), and resolves conflicts based on a freshness protocol.\n\nFacts vs. Preferences:\n\nI realized agents fail because they treat all context the same. I split them up:\n\n\\- Facts: Hard data (e.g., 'The project deadline is Friday')\n\n\\- Preferences: How the user wants things done (e.g., 'No unwraps in Rust, use safe error handling')\n\nWhen you hit addMessage, it extracts these into 'MemCubes' automatically so you don't have to manually tag everything.\n\nThe Implementation:\n\nI tried to keep the DX pretty simple, basically just a wrapper around your existing calls.\n\nfrom memos import MemClient\n\nclient = MemClient(api\\_key=\"your\\_key\") # or localhost\n\n\\# This extracts facts/prefs automatically in the background\n\nclient.add\\_message(\n\nuser\\_id=\"dev\\_123\",\n\nrole=\"user\",\n\ncontent=\"I'm on a Rust backend. Avoid unwraps, I want safe error handling.\"\n\n)\n\n\\# Retrieval prioritizes preferences and freshness\n\ncontext = client.search\\_memory(user\\_id=\"dev\\_123\", query=\"How to handle this Result?\")\n\nprint(context)\n\n\\# Output: \\[Preference: Avoids unwraps\\] \\[Fact: Working on Rust backend\\]\n\nLatency & 'Next-Scene Prediction':\n\nInjecting a massive history into every prompt is a great way to go broke and spike your latency. I added an async scheduling layer called Next-Scene Prediction. It basically predicts what memories the agent will need next based on the current convo trajectory and pre-loads them into the KV Cache.\n\nTech Stack:\n\nCore: Python / TypeScript\n\nInference: KV Cache acceleration + Async scheduling\n\nIntegrations: Claude MCP, Dify, Coze\n\nLicense: MIT (Self-hostable)\n\nSafety & Benchmarks:\n\nIâ€™m using a 'Memory Safety Protocol' to check for source verification and attribution. Testing it against the LoCoMo dataset shows way better recall for preferences than standard top-k retrieval.\n\nItâ€™s still early and definitely has some rough edges. If you want to poke around, the GitHub is open and thereâ€™s a playground to test the extraction logic.\n\nRepo / Docs:\n\n\\- Github:  https://github.com/MemTensor/MemOS\n\n\\- Docs:  https://memos-docs.openmem.net/cn",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qoj60d/i_relied_on_stateless_retrieval_for_longform/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o21ybfn",
          "author": "transfire",
          "text": "â€œThis extracts facts/prefs automatically in the backgroundâ€   Is it running an second LLM in the background to do this?",
          "score": 1,
          "created_utc": "2026-01-27 17:40:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jfflq",
              "author": "Asleep_Ad_7097",
              "text": "I conjecture, yes (considering that's how memory storage works in practice)",
              "score": 1,
              "created_utc": "2026-01-30 03:38:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fvium",
          "author": "Sweet121",
          "text": "Glad to see more people admitting that standard RAG is failing for agents, i found it one year ago!",
          "score": 1,
          "created_utc": "2026-01-29 16:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g5yjh",
          "author": "fijuro",
          "text": "The \"I hate Python\" vs \"I like Python\" conflict resolution is the real hurdle. Glad to see someone treating memory as a state machine rather than just a search problem",
          "score": 1,
          "created_utc": "2026-01-29 17:44:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoo1ho",
      "title": "Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qoo1ho/benchmark_of_qwen332b_reveals_12x_capacity_gain/",
      "author": "AIMultiple",
      "created_utc": "2026-01-27 19:35:07",
      "score": 28,
      "num_comments": 8,
      "upvote_ratio": 0.95,
      "text": "We ran 12,000+ MMLU-Pro questions and 2,000 inference runs to settle the quantization debate. INT4 serves 12x more users than BF16 while keeping 98% accuracy.\n\nBenchmarked Qwen3-32B across BF16/FP8/INT8/INT4 on a single H100. The memory savings translate directly to concurrent user capacity. Went from 4 users (BF16) to 47 users (INT4) at 4k context. Full methodology and raw numbers here: (https://research.aimultiple.com/llm-quantization/).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qoo1ho/benchmark_of_qwen332b_reveals_12x_capacity_gain/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o25bhuy",
          "author": "fijuro",
          "text": "I'm considering switching to this model",
          "score": 1,
          "created_utc": "2026-01-28 03:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25co5c",
              "author": "frgal",
              "text": "the same",
              "score": 1,
              "created_utc": "2026-01-28 03:23:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cinjv",
          "author": "pbalIII",
          "text": "Worth separating capacity from speed. INT4 shrinks weights, so you mostly buy KV cache headroom and that becomes more concurrent contexts. But tokens per second and quality don't always move in lockstep, it depends on prompts and batching.\n\nIf you're switching, I'd run three quick checks.\n\n- Eval on your own prompt set, not just generic benchmarks\n- Latency at your target QPS and batch size\n- Quant recipe and calibration data, bad calibration can cause cliffs\n\nDo that and INT4 is usually the cleanest cost win.",
          "score": 1,
          "created_utc": "2026-01-29 03:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gnabf",
              "author": "AIMultiple",
              "text": "Solid advice for production deployment!",
              "score": 1,
              "created_utc": "2026-01-29 19:01:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cyi44",
          "author": "justron",
          "text": "Cool, nice writeup!  \n  \n\\- For the evaluation datasets, it isn't obvious to me whether the different quantizations generated different scores. You might consider putting the response quality, or benchmark scores, into their own chart.  \n  \n\\- The \"Evidence 1: BF16 Initialization\" and \"Evidence 2: GPTQ-Int4 Initialization\" sections in the article are identical--is that intentional?",
          "score": 1,
          "created_utc": "2026-01-29 05:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gnl8w",
              "author": "AIMultiple",
              "text": "We'll add a dedicated accuracy comparison chart in v2 to make the quality differences clearer. The evidence section should show different values, might be a browser cache issue. Could you try a refresh and let me know if it still looks identical?",
              "score": 1,
              "created_utc": "2026-01-29 19:02:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gsjin",
                  "author": "justron",
                  "text": "Ah yes, the Evidence sections look different now, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-29 19:26:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22xvte",
          "author": "Infamous_Knee3576",
          "text": "Nice work and white papers . How does one get job a firm like yours ??",
          "score": 1,
          "created_utc": "2026-01-27 20:14:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmf35d",
      "title": "How do LLMs ACTUALLY work?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qmf35d/how_do_llms_actually_work/",
      "author": "LordAntares",
      "created_utc": "2026-01-25 09:53:15",
      "score": 26,
      "num_comments": 46,
      "upvote_ratio": 0.79,
      "text": "I've heard the \"it just does autocomplete based on statistical analyses\" argument a million times. Everybody acts like it's self explanatory and obvious but I can't quite make the connection.\n\n  \nI understand if somebody asks \"what's Tokyo's population\", how it would get you an answer. However, sometimes it almost seems like understands questions and I know that's not the case. I'll give you a couple of examples:\n\n1.  The \"how many Rs in strawberry\" famous question. Though it used to fail that one, it seems like it attempts reasoning somehow. I don't understand how statistical data analysis would lead it to go back and forth with you trying to solve the riddle.  I'm sure nobody actually asked that question online and had conversations like that.\n2.  How does it do math? Again, the problems you ask it can get very specific with an untried combination of numbers. Clearly it does something more than predict the words, no?\n3.  I usually slam it on its coding abilities; specifically semantic understanding of what needs to be done. I can understand boiler plate code etc. but just sometimes when I ask it to debug what went wrong in my code, it actually provides a seemingly thoughtful answer, solving the problem on a \"thinking\" level. Did it just see that reply somewhere? But how could it have deduced that was the problem from the code, unless someone somewhere asked the same sentence before pasting the code?\n4.  I ask it to roleplay as a custom character for a video game or whatever. I give him a custom set of instructions and a background etc. It seems to reply in character, and when it tries to, for example, reference his home town, it's not just like \" `\"Been a while since I've been in \" + hometown + \".\"`. It kind of makes up lore about it or uses alternative ways to reference it. How does it do that?\n\n  \nI know it's not magic, but I don't understand how it works. The general \"it's just a glorified autocomplete\" doesn't satisfy my curiosity. Can somebody explain to me how it does seemingly semantic things?\n\nThanks.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qmf35d/how_do_llms_actually_work/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1le2pp",
          "author": "UnbeliebteMeinung",
          "text": "[https://www.youtube.com/watch?v=D8GOeCFFby4](https://www.youtube.com/watch?v=D8GOeCFFby4)",
          "score": 27,
          "created_utc": "2026-01-25 10:00:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1llgrc",
              "author": "InTheEndEntropyWins",
              "text": "That's an amazing link, everyone should watch it.",
              "score": 4,
              "created_utc": "2026-01-25 11:05:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1llq33",
                  "author": "UnbeliebteMeinung",
                  "text": "I know. These questions come up all the time with these bad explanations \"its just the next token bro\".\n\nNow that you know the answer just copy the link when the next guy asks the same question :3",
                  "score": 4,
                  "created_utc": "2026-01-25 11:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mam60",
              "author": "LordAntares",
              "text": "This is fascinating, but essentially it says we haven't the slightest clue how they work.\n\nI also don't understand why they can decide to improve and pattern match what they haven't been coded to do.",
              "score": 3,
              "created_utc": "2026-01-25 14:04:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mcq87",
                  "author": "flextrek_whipsnake",
                  "text": "Yes, that is correct. We don't have a complete understanding of why this amount of scaling with these methods enables the models to achieve what they're able to achieve. That's the main reason why so many people, including myself, were skeptical of it for so long.\n\nAs always, the long answer is more complicated, but the short answer is we don't really know how they work.",
                  "score": 3,
                  "created_utc": "2026-01-25 14:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1n02ry",
                  "author": "roger_ducky",
                  "text": "The way I understood it is, it emulates the pattern matching part of the human brain. Not the same way, but close enough to work.\n\nWhatâ€™s actually happening when generating is:\n\nRead full question. Think about first word to write and writes it.\n\nRead full question and first word, think about the second word to write.\n\nKeep repeating until it looks complete.",
                  "score": 1,
                  "created_utc": "2026-01-25 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mx3kz",
              "author": "aletheus_compendium",
              "text": "thanks ðŸ™ðŸ» that's one i will save and share for sure. i've been giving this one out https://youtu.be/LPZh9BOjkQs \"Large Language Models explained briefly\" ðŸ¤™ðŸ»",
              "score": 1,
              "created_utc": "2026-01-25 15:56:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n71ei",
          "author": "infamouslycrocodile",
          "text": "I can give you a more intuitive explanation: the model is ONLY trained to complete the next token given the current set of words preceding it - contextually it leads strongly to a very likely next word appearing. \n\nIf you focus on the next word completion / autocomplete you blind yourself to the preceding context. Have it complete the next word or sentence, then delete that sentence / play with the preceding context and see what other sentence comes out instead. \n\nDoing this enough and at different conversation lengths has the model learn what to pay attention to and how to inch closer to the correct result regardless. \n\nIt reconfigures its weights to achieve this, it's not learning the answers at that point, they're just a side effect of the main goal of learning how to be more likely to say the right thing.\n\n---\n\nBecause there's a finite set of weights to configure, the model has to come up with a good way to cram all that information in so it distills the knowledge and the techniques to get to the answers which happens to be similar to how we learn but less advanced.\n\nThis is why the models can get mixed up and hallucinate - \"The capital of Japan is Paris\" - the data is close together but not wired up correctly but it will get better with more training. \n\nInference time scaling is just a higher order autocomplete: perhaps there was another thing it learnt - \"The capital of France is Paris, but wait - I said Paris was the capital of Japan so that can't be right\" - it can use other things it has been trained on to connect concepts out loud, this might correlate highly to similar lines of reasoning that the model can use as a tool for the current line of thinking.",
          "score": 6,
          "created_utc": "2026-01-25 16:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lmtgs",
          "author": "consultant82",
          "text": "I think for the math part, reasoning + agentic tool combination is foundation. LLM reasons that â€žmaybe I should use a calculator for given taskâ€œ and a calc is invoked for given task. \n\nThe llm journey started all â€žsimpleâ€œ step by step with text2vec embeddings (semantic of tokens, enabling a meaning space), neural networks (â€žgive me input parameters given on known output, so I can learn predicting for given problemâ€œ), contextualized token prediction (transformer architecture) and this base foundation is now fed with lot of bells and whistles around it (tooling, more effective models, rag, ..).",
          "score": 4,
          "created_utc": "2026-01-25 11:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lo32i",
          "author": "GCoderDCoder",
          "text": "I stumbled across this the other day. She does a good job concisely explaining the different types of logic and how \"logic\" works with LLMs. I have peers who keep saying they're just pattern matching tools and I'm not for the AI hype but that's not a sufficient or fair description. \n\nI prefer describing them as complex text generators with emerging logic capabilities due to the science and art of how we use the text the model was trained on. Words have meaning so \"understanding\" or heavily connecting with relationships between words allows LLMs to generate value based on those relationships\n\nhttps://youtu.be/qXtNvfxBzlk",
          "score": 3,
          "created_utc": "2026-01-25 11:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oky6h",
          "author": "TheRealStepBot",
          "text": "To the degree that someone thinks that itâ€™s fundamentally not what humans do, the explanation is wrong. But most ml people think thatâ€™s what humans do as well so when they use that phrase they are saying something different than most people hear. \n\nThe derisive itâ€™s â€œjustâ€ fancy autocomplete misunderstands almost every word in that sentence. Itâ€™s just fancy autocomplete in the same sense that aspects of what humans do is also just fancy autocomplete, and in that these models are almost certainly better at that aspect of cognition than at least the average person, if not most people.",
          "score": 3,
          "created_utc": "2026-01-25 20:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldo3p",
          "author": "kubrador",
          "text": "it's still autocomplete, just autocomplete that's absurdly good at pattern matching across billions of examples. when you ask \"how many Rs in strawberry\" it's seen enough \"let me think through this letter by letter\" responses that it's learned the \\*pattern\\* of reasoning, not actual reasoning.",
          "score": 6,
          "created_utc": "2026-01-25 09:56:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26c3e4",
          "author": "r-3141592-pi",
          "text": "A few answers have offered good video explainers, but they often get lost in architectural details and miss the fundamental purpose of training deep neural networks.\n\nLet me try a different approach. Since early neural networks were inspired by the brain, let's start with how our brains process concepts.\n\nWhen you hear \"What is Tokyo's population?\", your brain recognizes the question format, identifies \"Tokyo\", and understands \"population\". You learned these concepts by forming neural pathways that encode information. \"Encoding\" simply means electrical signals flow through specific networks of neurons. As the signal passes through each connection, it gets modified, eventually forming a representation that can be associated with that concept. Each time you think \"Tokyo\", roughly the same group of neurons activates. The more you know about Tokyo, the more connections form, adding more semantic features to that signal. When you hear \"Tokyo\", parts of your brain light up pathways that encode \"Japan\", \"capital\", or \"Japanese food\". These pathways generate a combined signal representing everything you associate with the city. The same happens for syntax and the meaning of \"population\".\n\nThese encodings aren't precise. The same pathway might activate similarly for \"elephant\" and \"hippopotamus\". This process relies on competition in which the best selection wins. Your brain uses the surrounding words to suppress the irrelevant meaning and amplify the correct one. This competition between signals allows you to select the precise concept intended.\n\nHow do LLMs work? LLMs use a next-token prediction task to force the network to build internal representations that encode semantic information about each word and its relationships to other words. These representations are high-dimensional vectors that arise from the activations of artificial neurons, usually calculated as the nonlinear transformation of their weighted sum of inputs. After training, these vectors capture recognizable features. In this vector space, \"Tokyo\" sits close to other capital cities, near \"Japan\" and \"Japanese food\", but far from unrelated concepts.\n\nDeep architectures generalize features. As input like \"Tokyo\" moves through the network, early layers encode basic syntactic features. Later layers use those outputs to encode more complex conceptual meanings, until middle layers represent sophisticated concepts we associate with \"Tokyo\". For your question \"What is Tokyo's population?\", the network builds representations for (capital city) + (Japan) + (statistics) + (punctuation) and many more. These representations can be added together while preserving their meanings. This gives the final part of the network enough information to know that the most likely next-token prediction should look like a response to a question about Japan's capital city and should return a number.\n\nThe multiple semantic meanings are added to a residual stream in LLMs, which carries the meaning from the initial input. More contributions are added as different layers process it. Attention mechanisms look at the other words in the sentence (like \"population\"), calculating their importance and relationship to \"Tokyo\". Then the architecture takes that information and adds it to the original \"Tokyo\" vector. In real models, features are \"smeared\" across many dimensions, but let's assume they are clean for this example.\n\nSay dimensions encode specific features: [feat_1 = capital city, feat_2 = animal, feat_3 = punctuation, feat_4 = color, feat_5 = statistics]. \"Tokyo\" might generate [0.9, 0.1, 0, 0, 0], a question mark [0, 0, 1.2, 0.1, 0], and \"statistics\" [0.2, 0.1, 0, 0, 1]. Adding these gives [1.1, 0.2, 1.2, 0.1, 1], which includes high values in dimensions for identifying the most important features of your question.\n\nWhen internal representations match our understanding of concepts, you have a world model that can be used for prediction. This model enables reasoning through supervised training and reinforcement learning. During reinforcement learning with verified rewards, the model is encouraged to follow logical steps that lead to correct answers. Without human intervention, it begins to increase its use of strategies that help it solve problems. That's where the reasoning traces you may have seen come from: backtracking after spotting a mistake (\"wait...\"), recognizing a promising idea (\"aha!\"), breaking a difficult problem into smaller, more manageable parts, elaborating a plan to solve a problem, expressing uncertainty, and considering alternative approaches. Additional training allows them to use tools, search the web, and read relevant literature to understand the problem better before trying to solve it.\n\nThe better the world model in the base model, the better the network will be able to understand what you want, solve problems, and behave in ways that are compatible with the context provided.\n\nAs you can see, regardless of whether we are talking about biological brains or LLMs, the objective is identical: to create an internal representation accurate enough to predict what comes next. It's not magic, but the fact that it works so incredibly well is almost magical.\n\nI hope that helps.",
          "score": 2,
          "created_utc": "2026-01-28 07:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nbd36",
          "author": "insulaTropicalis",
          "text": "When a person says you that an LLM is just a sophisticate autocomplete, ask them what they know about linear transformations and non-convex optimization. If they can't answer the questions, they are just repeating concepts they don't understand that they read on social media.",
          "score": 4,
          "created_utc": "2026-01-25 16:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m5o61",
          "author": "Ok-Lack-7216",
          "text": "The \"glorified autocomplete\" explanation is technically true but effectively useless because it ignores how the model decides what comes next.\n\nI actually just created a visual breakdown of this process that answers your specific examples:\n\n[https://youtu.be/x-XkExN6BkI](https://youtu.be/x-XkExN6BkI)\n\n1. The Strawberry Problem: This is a Tokenization issue. The AI doesn't see letters; it sees whole words (tokens) as single \"Lego bricks.\" It literally cannot \"see\" the letters inside the brick to count them.\n2. Roleplay & Coding: This works via the Attention Mechanism. The model doesn't just read left-to-right; it assigns a \"weight\" to previous instructions. When it generates a line of dialogue, it is mathematically \"attending\" to the character background you provided earlier, ensuring the prediction aligns with that context.\n\nItâ€™s not magic, but it is complex linear algebra. I traced a single prompt through the engine to show exactly how this works in the video.",
          "score": 2,
          "created_utc": "2026-01-25 13:36:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mb99s",
              "author": "LordAntares",
              "text": "I'm sorry, but this video is cringe.",
              "score": -2,
              "created_utc": "2026-01-25 14:08:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mibku",
                  "author": "Ok-Lack-7216",
                  "text": "Fair enough! I know the analogies (like the Grocery Store) aren't for everyone, but I wanted to try something different than the usual dry lectures. Thanks for giving it a shot anyway!",
                  "score": 3,
                  "created_utc": "2026-01-25 14:46:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1llcbt",
          "author": "InTheEndEntropyWins",
          "text": "The short answer is we don't know exactly how they work. We know the architecture, but how it actually works is based on its own learning and the networks are way too complex for us to understand what's it's learnt. But in some simple situations we have looked at the networks and understood what it's done.  \n\n\n>Sam Altman Says OpenAI Doesnâ€™t Fully Understand How GPT Works Despite Rapid Progress\nâ€œWe certainly have not solved interpretability,â€ Altman said.\n[https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/](https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/)\n\n>During that training process, they learn their own strategies to solve problems. These strategies are encoded in the billions of computations a model performs for every word it writes. They arrive inscrutable to us, the modelâ€™s developers. **This means that we donâ€™t understand how models do most of the things they do.**\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\nSo the Rs in strawberry is due to the fact it doesn't get each letter, the word strawberry is broken up into tokens like \"straw\" and \"berry\" and those turned into vectors. So all the LLM has is say two vectors and those vectors might not have anything about the letters in straw and berry. \n\n>How does it do math? \n\nThis is a really interesting question. Anthropic have done some studies on this exact question and for simple addition, they use a bespoke algorithm that has two parts an estimation part and an accuracy part. So it doesn't add up numbers like a human would normally do or how a human would program a computer would do. It's learnt this completely new method. \n\nIn terms of autocomplete, anthropic have demonstrated that it uses algorithms and multistep reasoning rather than just memorising data and looking things up. \n\n\n>Claude wasn't designed as a calculatorâ€”it was trained on text, not equipped with mathematical algorithms. Yet somehow, it can add numbers correctly \"in its head\". How does a system trained to predict the next word in a sequence learn to calculate, say, 36+59, without writing out each step?\n>\n>Maybe the answer is uninteresting: the model might have memorized massive addition tables and simply outputs the answer to any given sum because that answer is in its training data. Another possibility is that it follows the traditional longhand addition algorithms that we learn in school.\n>\n>Instead, we find that Claude employs multiple computational paths that work in parallel. One path computes a rough approximation of the answer and the other focuses on precisely determining the last digit of the sum. These paths interact and combine with one another to produce the final answer. Addition is a simple behavior, but understanding how it works at this level of detail, involving a mix of approximate and precise strategies, might teach us something about how Claude tackles more complex problems, too.\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\n\n>if asked \"What is the capital of the state where Dallas is located?\", a \"regurgitating\" model could just learn to output \"Austin\" without knowing the relationship between Dallas, Texas, and Austin. Perhaps, for example, it saw the exact same question and its answer during its training.\n>\nBut our research reveals something more sophisticated happening inside Claude. When we ask Claude a question requiring multi-step reasoning, we can identify intermediate conceptual steps in Claude's thinking process. In the Dallas example, we observe Claude first activating features representing \"Dallas is in Texas\" and then connecting this to a separate concept indicating that â€œthe capital of Texas is Austinâ€. In other words, the model isÂ combiningÂ independent facts to reach its answer rather than regurgitating a memorized response.\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\nThat anthropic article is really good and has other examples, worth a read.\n\nSomeone else also pasted this link, so I'd just emphasise it's an amazing video worth watching.\n\n# The most complex model we actually understand\nhttps://www.youtube.com/watch?v=D8GOeCFFby4",
          "score": 2,
          "created_utc": "2026-01-25 11:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ln556",
              "author": "LordAntares",
              "text": "Thanks for the detailed reply. So it's not \"just a fancy autocomplete\". It might be a VERY fancy autocomplete tho.\n\nI will have to watch that video I guess.",
              "score": 1,
              "created_utc": "2026-01-25 11:20:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ltpyq",
                  "author": "InTheEndEntropyWins",
                  "text": ">Thanks for the detailed reply. So it's not \"just a fancy autocomplete\". It might be a VERY fancy autocomplete tho.\n\nIf you want to think about it in those terms, then when humans write and say stuff it's just \"VERY fancy autocomplete\".",
                  "score": 4,
                  "created_utc": "2026-01-25 12:14:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1lhnz8",
          "author": "guigouz",
          "text": "https://www.theguardian.com/technology/ng-interactive/2023/nov/01/how-ai-chatbots-like-chatgpt-or-bard-work-visual-explainer",
          "score": 1,
          "created_utc": "2026-01-25 10:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ng6dh",
          "author": "keebmat",
          "text": "[https://www.youtube.com/watch?v=xiqtrtKxUzY](https://www.youtube.com/watch?v=xiqtrtKxUzY)\n\n  \nyou dont want to know...",
          "score": 1,
          "created_utc": "2026-01-25 17:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nlsvk",
          "author": "crone66",
          "text": "For your second point.\n\n\nLLMs are actually really bad at math in terms of actually calculating. At the beginning they were completly useless. Then they added all simple math operations (add, subtract, devide, multiply) for two numbers of up to 4 digits to the training data. But obviously people quickly noticed that LLM can't deal with numbers with more than 5 digits. Therefore, they added a calculator tool that AI can use. Use any local ai model without a calculator tool and they will fail really quickly.",
          "score": 1,
          "created_utc": "2026-01-25 17:42:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nq0ua",
              "author": "LordAntares",
              "text": "So what is the top linked video about? It explained how llms do adding, and it wasn't hard coded as you say.",
              "score": 1,
              "created_utc": "2026-01-25 18:00:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1obauo",
                  "author": "crone66",
                  "text": "Yes it's not hard coded but it also doesn't understand how these math operations actually work otherwise it would be able to apply the rules to any given numbers but it can't. The reason is that the higher the number are the less likely they are even included in the trainingsset. S8nce LLM are a model that outputs the token with the highest probability the math equation (including the result) needs to be in the trainingset otherwise it essentialy outputs a random number. For \"Thinking models\" which is essentially the same base model but with a two step approach where you try to first break down the initial prompt by planning out a path to solution which provides useful context to shift the probability in the right direction. This context might include solving strategies such as removing zeros. Since 12+13 is in the training data it can solve it now the context or the final stage just needs to figure out that it has to add 3 zeros to the end which is most likely part of the training set too and even probability wise very likely. Therefore thinking models can even solve stuff thats not part of there trainings set.The fun thing is the llm actually don't know if the answer is correct but it will tell you it's correct with out a doubt. Since the thinking models will still fail for slightly more complex math questions calculator tool calls exist because LLMs are still really bad at calculating since they simply don't \"understand\" logic.\n\n\n(some parts are simplified to make easier to understand)",
                  "score": 2,
                  "created_utc": "2026-01-25 19:28:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1onpbv",
          "author": "TheRealStepBot",
          "text": "Iâ€™d say the way they work is related to information compression. You canâ€™t complete the next word if you canâ€™t internalize a semantic idea of what is being conveyed. The way to achieve this is by creating an information bottleneck where you force compression. \n\nGiven enough model complexity there really is no obvious limit to how complex an idea can be passed through this compression system. Consequently itâ€™s auto complete yes but in order to complete every sentence ever written you must necessarily compress semantic understanding at some level.\n\nThe main reason people think these models â€œdonâ€™t understandâ€ is actually not related to what they do but how we train them to do it. \n\nTheir internal representations are entangled and not perfectly able to be isolated. This in turn leads to â€œhallucinationsâ€ and other artifacts that sometimes clouds what it is they are doing. \n\nThat we initialize and train them the way we do is mostly not because we canâ€™t conceive of better ways but because of the pragmatic convenience of the way we do it and the quite good performance we do get. We are still in the infancy of having enough compute to apply these techniques as we do so convenience is still a significant driver.\n\nAs the tech matures however and the compute available continues to grow different implementations will be tried that will likely significantly improve performance without necessarily invalidating the â€œitâ€™s just fancy autocompleteâ€ talking point.\n\nIn case you are interested Iâ€™m alluding to the ideas of continuous learning, and neuro evolutionary learning rather than direct gradient descent from random initialization. And thatâ€™s just assuming transformer architecture. There is so much more in the modern ml toolbox that just hast been brought to bear at the same scale as these models",
          "score": 1,
          "created_utc": "2026-01-25 20:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ovau1",
          "author": "Substantial_Sound272",
          "text": "A lot of folks don't know that the mathematical underpinnings of modern language models were discovered way back in 1948 in the absolutely incredible Shannon paper \"AMathematical Theory of Communication\". Section 6 titled \"Choice, Uncertainty, and Entropy\" is one of the most mind-blowing things I've ever read. He basically invents Information Theory and the concept of Entropy, which is directly used as a loss function for the training of modern Large Language Models, though he didn't know anything about neural networks or transformers or such.  Claude Shannon was an absolute genius. He is the person that Anthropic's Claude is named after, so if you really want to understand the math behind LLMs and you are at all mathematically inclined, check out section 6 of that paper!",
          "score": 1,
          "created_utc": "2026-01-25 20:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p9rmu",
          "author": "justinhj",
          "text": "For 2. most models are not good at arithmetic. The ones you use on a website or app usually have built in function calling for that.",
          "score": 1,
          "created_utc": "2026-01-25 22:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pghst",
          "author": "wheres-my-swingline",
          "text": "This thread is enjoyable",
          "score": 1,
          "created_utc": "2026-01-25 22:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21lnuu",
          "author": "Available_Cream_752",
          "text": "Check out Andrej Karpathy's YT channel",
          "score": 1,
          "created_utc": "2026-01-27 16:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lkkix",
          "author": "Kiansjet",
          "text": "https://youtu.be/wjZofJX0v4M\n\n2. It has learned how math works via pattern recognition by seeing a ton of varied examples. Crucially, it ideally should NOT natively try to answer the question. It is a probabilistic prediction algorithm and not a precise calculator, so what it SHOULD do during one of its reasoning/tool call phases is to invoke a hard coded calculator tool or code executor to do the calculation for it. \n\nI'm only answering 2 because it's the only one I think I have a decent answer for.\n\nForgive me for what may come off as condescension but it really is the probabilistic behavior explanation you've heard. Similar to how you likely learned to speak your native language, it's given a ton of examples of what native coherent, valid text looks like and views it as a series of blocks of text called tokens, and learns what kinds of tokens show up around other kinds.\n\nIt does not need to have seen a exact example of a scenario you're hitting it with because during it's training phase it gains an understanding of how tokens relate to other tokens. It knows what the critical thinking chain of thought for debugging code generally looks like and for the language you're using, from there depending on the tools it's given it can try different solutions depending on what it thinks, probabilistically, whether you yourself can understand internally how it saw a similarity or not, is the solution to your problem.",
          "score": 1,
          "created_utc": "2026-01-25 10:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lnmz5",
          "author": "superawesomepandacat",
          "text": "Magnets how do they work",
          "score": 1,
          "created_utc": "2026-01-25 11:24:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrqdds",
      "title": "[P] Trained a 67M-parameter transformer from scratch on M4 Mac Mini - 94% exact-match accuracy on CLI command generation",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "author": "Great_Fun7005",
      "created_utc": "2026-01-31 02:47:54",
      "score": 26,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I trained a small language model end-to-end on consumer hardware (M4 Mac Mini, 24GB RAM) and achieved 94% exact-match accuracy on CLI command generation.\n\n**Key details:**\n\n* Model: 67M parameters (12 layers, 512 hidden dim, RoPE, RMSNorm, SwiGLU)\n* Training: 204.8M tokens, \\~13 hours pretraining + 4 minutes fine-tuning\n* Hardware: Apple Silicon MPS, no discrete GPU\n* Cost: \\~$0.50 in electricity\n* Evaluation: Strict exact-match (no partial credit)\n\n**What worked:**\n\n* Modern architectural components (RoPE, RMSNorm, SwiGLU) are effective even at small scale\n* Marker-based output contracts for state signaling\n* Memory-mapped data loading to handle 200M+ tokens on limited RAM\n* Continual learning with evaluation gates that reject harmful updates\n\n**What failed (and why it matters):** All 6% of failures shared one pattern: early termination on symbol-dense patterns (regex, pipes, redirects). Not a reasoning failureâ€”a data coverage problem. Adding \\~500 targeted examples would likely fix most of these.\n\n**Takeaway:** For narrow, exact tasks with controllable domains, small models trained from scratch can be practical, inspectable, and cheap to iterate on. Data quality mattered more than scale.\n\nFull technical writeup with training logs, failure analysis, and code: [https://geddydukes.com/blog/tiny-llm](https://geddydukes.com/blog/tiny-llm)\n\nGitHub: [https://github.com/geddydukes/tiny\\_llm](https://github.com/geddydukes/tiny_llm)\n\nHappy to answer questions about training dynamics, architecture choices, or the evaluation setup.",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2sfg8j",
          "author": "radarsat1",
          "text": "The implementation is fairly clean, good job. I have a question though, this seems to be an unusual TransformerBlock forward function, did you get this from somewhere or is it a mistake or maybe your own idea?\n\n```\nÂ  Â  Â  Â  h1 = self.norm1(x)\nÂ  Â  Â  Â  h2 = self.norm2(x)\n\nÂ  Â  Â  Â  attn_out = self.attn(h1, attn_mask, rope_cos, rope_sin)\nÂ  Â  Â  Â  mlp_out = self.mlp(h2)\nÂ  Â  Â  Â Â \nÂ  Â  Â  Â  return x + self.dropout(attn_out) + self.dropout(mlp_out)\n```\n\nI'm referring to how it adds `attn_out` and `mlp_out` instead of feeding `attn_out` into `mlp`.",
          "score": 4,
          "created_utc": "2026-01-31 13:56:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2th04w",
              "author": "Great_Fun7005",
              "text": "Thanks, appreciate it. This is an intentional pre-norm parallel residual block: x + attn(norm(x)) + mlp(norm(x)). Attention and MLP run in parallel off the same residual stream (with separate RMSNorm) and are summed in a single update. Itâ€™s a known Transformer variant used in several modern decoder-only models, not a mistake or a novel invention.",
              "score": 2,
              "created_utc": "2026-01-31 17:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uinbj",
                  "author": "radarsat1",
                  "text": "Do you know offhand which variants use this? I actually checked the Llama code before posting just in case I was saying something dumb, but it seems to work there as I am used to. I guess I can plumb the transformers library a bit to find out but I'm curious about it, if you happen to know.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2q4kfm",
          "author": "Dense_Gate_5193",
          "text": "thanks i am training SLMs for work and this is helpful",
          "score": 2,
          "created_utc": "2026-01-31 02:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4qfl",
              "author": "Great_Fun7005",
              "text": "Glad to provide a helpful resource!",
              "score": 1,
              "created_utc": "2026-01-31 02:57:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2u5cre",
          "author": "HealthyCommunicat",
          "text": "Woah, I was literally talking about how bad some models are with just basic commands, like hooking up glm 4.7 flash to codex cli and ask it to find a fileâ€¦ watch it mess up the â€œfind . -name â€œ___â€â€ bash syntax 7 times before getting it right, or even editing a file i usually watch it struggle going through multiple different attempt methods until it just finally ends up on echoing it into the file lol\n\nThis is actually really cool, if someone was to take ur base and add upon it iâ€™d totally use it",
          "score": 1,
          "created_utc": "2026-01-31 19:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u8met",
              "author": "Great_Fun7005",
              "text": "Feel free to add onto it! I have some future iterations planned but have a couple of projects Iâ€™m working on before Iâ€™ll get back to this one.",
              "score": 1,
              "created_utc": "2026-01-31 19:19:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqgpt8",
      "title": "We did not see real prompt injection failures until our LLM app was in prod",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qqgpt8/we_did_not_see_real_prompt_injection_failures/",
      "author": "Zoniin",
      "created_utc": "2026-01-29 18:28:38",
      "score": 20,
      "num_comments": 25,
      "upvote_ratio": 0.93,
      "text": "I am a college student. Last summer I worked in SWE in the financial space and helped build a user facing AI chatbot that lived directly on the company website.\n\nBefore shipping, I mostly thought prompt injection was an academic or edge case concern. Then real users showed up.\n\nWithin days, people were actively trying to jailbreak the system. Mostly curiosity driven it seemed, but still bypassing system instructions, surfacing internal context, and pushing the model into behavior it was never supposed to exhibit.\n\nWe tried the usual fixes. Stronger system prompts, more guardrails, traditional MCP style controls, etc. They helped, but none of them actually solved the problem. The failures only showed up once the system was live and stateful, under real usage patterns you cannot *realistically* simulate in testing.\n\nWhat stuck with me is how easy this is to miss right now. A lot of developers are shipping LLM powered features quickly, treating prompt injection as a theoretical concern rather than a production risk. That was exactly my mindset before this experience. If you are not using AI when building (for most use cases) today, you are behind, but many of us are unknowingly deploying systems with real permissions and no runtime security model behind them.\n\nThis experience really got me in the deep end of all this stuff and is what pushed me to start building towards a solution to hopefully enhance my skills and knowledge along the way. I have made decent progress so far and just finished a website for it which I can share if anyone wants to see but I know people hate promo so I won't force it lol. My core belief is that prompt security cannot be solved purely at the prompt layer. You need runtime visibility into behavior, intent, and outputs.\n\nI am posting here mostly to get honest feedback.\n\nFor those building production LLM systems:\n\n* does runtime prompt abuse show up only after launch for you too\n* do you rely entirely on prompt design and tool gating, or something else\n* where do you see the biggest failure modes today\n\nHappy to share more details if useful. Genuinely curious how others here are approaching this issue and if it is a real problem for anyone else.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qqgpt8/we_did_not_see_real_prompt_injection_failures/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2gnfbv",
          "author": "tom-mart",
          "text": "We will have to teach you kids evwrythong from scratch\n\nRule number one of any public facing endpoint is that it will be abused and it will be exploited and it will be hacked. Those are not ifs, those are facts. When you create any public facing service, what it does is really a secondary concern. Your main issue is to think of any possible threat and mitigate it. For instance, I would never create anything that is public facing and doesn't require an account. Then you can detect harmful behaviour just by adding observing agent that looks at the chat interaction without being engaged in it and is able to lock user out when it detects any foul play. There are many other ways to implement basic chat security.",
          "score": 14,
          "created_utc": "2026-01-29 19:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2guo70",
              "author": "Zoniin",
              "text": "You are definitely not wrong on the core principle. Public endpoints will always be abused. The part that surprised me was how much harder this becomes with LLMs compared to traditional services. Auth and rate limiting help, but most of the failures we saw were not obviously malicious and came from normal users probing behavior rather than attacking infra. Observing agents and heuristics help too, sure, but they still rely on assumptions about intent that break down once prompts get stateful and context bleeds across turns. That gap between traditional endpoint security and model behavior is what caught me off guard and what I am trying to reason about more deeply.",
              "score": 5,
              "created_utc": "2026-01-29 19:36:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ha1vr",
          "author": "Strong_Worker4090",
          "text": "Yep, this matches what Iâ€™ve seen too.\n\nReal users instantly go â€œlol can I jailbreak itâ€ the second itâ€™s live. UAT almost never catches that because people are busy/timeboxed and test the happy path, not â€œlet me spend 2 hours trying to break it and set it on fire.â€\n\nWhat actually helped for us wasnâ€™t stronger prompts, it was moving controls out of the prompt layer:\n\n* **Treat the LLM like untrusted input**: ***EVERY*** tool call is server-side validated (auth checks, allowlists, strict schemas).\n* **Least privilege**: split tools into read-only vs write vs â€œdangerousâ€, and keep most sessions on the lowest tier.\n* **Data controls**: redact/classify sensitive stuff before it hits the model, and block obvious â€œdump the context / dump the docâ€ outputs. I have a couple free tools I've been using for this.\n* **Runtime visibility**: log tool calls + retrievals, rate limit probing patterns, and add a few (as many as you can) jailbreak tests that run continuously on real flows.\n\nPrompts still matter, but more as polish. The security model has to be runtime + permissions + data handling.\n\nBiggest failure modes Iâ€™ve seen: RAG leaks, tool misuse, and privilege confusion (â€œuser askedâ€ != â€œuser is allowedâ€).",
          "score": 8,
          "created_utc": "2026-01-29 20:50:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ni88w",
              "author": "Visionexe",
              "text": "How is this not complete and utter common sense?Â ",
              "score": 3,
              "created_utc": "2026-01-30 18:49:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nltkd",
                  "author": "Strong_Worker4090",
                  "text": "Yea I mean I think it is common sense at the conceptual level. The problem is execution. Most teams ship the demo version (prompt rules and a couple quick checks), then prod happens and you realize you need real gating, real permissioning, real logging, and real data handling. Each of those adds real engineering time and product friction, so it gets deprioritized until something breaks. That's showbiz baby",
                  "score": 1,
                  "created_utc": "2026-01-30 19:05:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hgzgp",
          "author": "darkwingdankest",
          "text": ">thought prompt injection was an academic or edge case concern \n\noh boy",
          "score": 5,
          "created_utc": "2026-01-29 21:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hetwb",
          "author": "Long_Complex_4395",
          "text": "Prompt injection and jailbreaks will always be present once itâ€™s out in the wild and one should prepare for it.\n\nOne thing to do is to test for known vulnerabilities before deploying to production, then brace for the unknown because people will always want to know how far they can go",
          "score": 2,
          "created_utc": "2026-01-29 21:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hbfe7",
          "author": "codemuncher",
          "text": "Itâ€™s a classic that developers never think of adversarial uses of their systems. â€œOh what do you mean someone pressed every key on the keyboard at once, and the is deleted everything? Thatâ€™s supposed to be impossible!â€\n\nThis is a tale as old as time.",
          "score": 1,
          "created_utc": "2026-01-29 20:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hjt3n",
          "author": "kubrador",
          "text": "yep, users are absolutely feral once they get access. the prompt layer stuff is basically security theater. it's like locking your front door while leaving the windows open.\n\n\n\nruntime visibility is the real move though. most teams i've talked to are basically doing nothing and hoping their guardrails hold, which is wild. they break immediately under actual adversarial use.",
          "score": 1,
          "created_utc": "2026-01-29 21:36:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmly",
              "author": "Zoniin",
              "text": "Yeah, that framing matches what I saw almost exactly. The prompt layer gives a false sense of safety, and once users start poking at stateful systems the cracks show fast lol. Iâ€™ll look into runtime security, do you have any tools or tips on that note? Some dude dropped one of the tools he used that actually looked pretty good but I am curious what you use for this.",
              "score": 1,
              "created_utc": "2026-01-29 22:13:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hm5t9",
          "author": "HealthyCommunicat",
          "text": "Can you give me some examples of what kind of prevention instructions/prompts youâ€™ve made and what/how itâ€™s being circumvented?",
          "score": 1,
          "created_utc": "2026-01-29 21:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iif34",
              "author": "Zoniin",
              "text": "yeah, a lot of it started with fairly standard stuff. strict system prompts about role, explicit â€œdo not reveal internal instructions,â€ tool usage constraints, and guardrails around what data could be accessed or returned. the circumvention was rarely a single prompt, it was usually gradual. things like multi turn probing that reframed the task, mixing benign requests with meta instructions, or steering the model to restate or summarize context in ways that effectively leaked system or RAG data. none of those looked obviously malicious in isolation, which is why they slipped past prompt level checks.",
              "score": 1,
              "created_utc": "2026-01-30 00:34:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iklp5",
                  "author": "HealthyCommunicat",
                  "text": "ive shipped a good amount of automation, only 3 of them being actual \"chat with the model\" kinda thing, and i went out of my way to take as much into consideration, i had gpt opus gemini3pro just speak with the agent nonstop and come up with as many possible situations and test to make certain that the model wont leak anything, if you can send me ur .md or post it online and tell me the model id i can try to help?",
                  "score": 1,
                  "created_utc": "2026-01-30 00:46:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hpp9m",
          "author": "gkarthi280",
          "text": "What really helps is observability. A lot of this stuff is hard to catch before production cuz LLMs  and agents are non-deterministic. You dont know whats going on under the hood. \n\nCheck out OpenTelemetry and pair it with an OTel compatible backend like SigNoz and you'll get detailed traces of every step the agent takes before it spits out its output. it helps you answer questions like:\n\n* what tools were being called\n* what inputs triggered them\n* how the model reasoned step-by-step\n\nIt doesn't solve prompt injection but makes it way easier to see failures and unintended behavior before they escalate, especially in prod.",
          "score": 1,
          "created_utc": "2026-01-29 22:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iywve",
          "author": "nmrk",
          "text": "There is an old internet saying, \"If your platform is full of assholes, and you could have done something to prevent it and didn't, *you're* the asshole.\"",
          "score": 1,
          "created_utc": "2026-01-30 02:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jln4z",
          "author": "gg6x3",
          "text": "The core concern here is that prompt engineering is not security. In my role building an enterprise GenAI platform, Iâ€™ve seen this play out in infrastructure design. We recently migrated from direct (global, might I add) endpoints to a backend proxy model for data residency compliance. This shift moves the security boundary from the 'prompt' to the 'network path.' By routing traffic through a controlled backend, we gain the runtime visibility needed to monitor behavior and enforce data residency in real-time. SSO protects the entrance, but the proxy protects the data flow. Relying on system prompts to prevent exfiltration is a production risk we hope to thwart by centralizing control at the proxy layer.\n\nNow if your endpoint is connected to internal tools (like a database or file search), that opens a whole new can of worms.",
          "score": 1,
          "created_utc": "2026-01-30 04:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qs37a",
          "author": "Analytics-Maken",
          "text": "System prompts and duardrails operate at the prompt layer, but they can't enforce permissions or validate calls. Move security out of the prompt layer, treat every LLM output as untrusted input. Use gate tools, they call servers side with strict auth checks, allowlist, and schema validation. Some ETL tools like Windsor.ai have them and enable more than 325 data sources through them.",
          "score": 1,
          "created_utc": "2026-01-31 05:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gseuz",
          "author": "ChanceKale7861",
          "text": "Prompt injection is a featureâ€¦ and you will not fix it. Itâ€™s part and parcel. Further, you need to go educate yourself on security and threat modeling and adversarial threats and then ensure your systems have the controls.\n\nHahahahahha so first time? And you didnâ€™t think about ANY of this beforehand? Same old thingâ€¦ speed to market trumps allâ€¦ except a good designâ€¦ ðŸ˜‚ðŸ˜‚ðŸ˜‚ \n\nIf this happened then it was warrantedâ€¦ \n\nAnyone can leverage tools to test this stuff and automate the security and E2E testing etc.",
          "score": -2,
          "created_utc": "2026-01-29 19:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gvbop",
              "author": "Zoniin",
              "text": "Fair reaction tbh. To be clear it wasn't that we thought about none of it. We did threat modeling, prompt hardening, etc. What surprised me was not that abuse happened but more so how much of it fell into gray areas that were hard to classify as malicious ahead of time and only emerged once the system was stateful and under real usage. Automated testing and E2E help, but they do not surface the same failure modes we saw once users started interacting freely. That gap was what I found interesting, not the idea that public systems get abused.",
              "score": 2,
              "created_utc": "2026-01-29 19:39:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2hn3yg",
                  "author": "Adept_Carpet",
                  "text": "Did you roll out to a small group first (including a micro group of employees directed to try and break it)? Or was it basically sent to all users at once?",
                  "score": 2,
                  "created_utc": "2026-01-29 21:52:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2iazfx",
          "author": "Far_Statistician1479",
          "text": "If you create a system where prompt injection can cause a problem, then youâ€™re a bad developer and designed a bad system.",
          "score": -3,
          "created_utc": "2026-01-29 23:54:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hh4vz",
          "author": "VisualForever2",
          "text": "I found this mcp service just recently that actually that checks prompts and blocks malicious attempts before they hit your llm. I think theyâ€™re relatively new to the space but Iâ€™ve had decent results in testing their system so Iâ€™ve started using them. You might want to check it out. Itâ€™s https://axiomsecurity.dev",
          "score": -5,
          "created_utc": "2026-01-29 21:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hqwbu",
              "author": "Zoniin",
              "text": "Appreciate you sharing that. More or less lines up pretty closely with the kinds of issues I was running into. Iâ€™ll spend some time testing it out thanks again for sharing. What specifically do you use this for if you donâ€™t mind my asking?",
              "score": 0,
              "created_utc": "2026-01-29 22:10:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qr71pv",
      "title": "Who still use LLMs in browser and copy paste those code in editior instead of using Code Agent?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "author": "monskull_",
      "created_utc": "2026-01-30 14:32:46",
      "score": 13,
      "num_comments": 31,
      "upvote_ratio": 0.78,
      "text": "Iâ€™m always excited to try new AI agents, but when the work gets serious, I usually go back to using LLMs in the browser, inline edits, or autocomplete. Agentsâ€”especially the Gemini CLIâ€”tend to mess things up and leave no trace of what they actually changed.\n\nThe ones that insist on 'planning' first, like Kiro or Antigravity, eventually over-code so much that I spend another hour just reverting their mistakes. I only want agents for specific, local scriptsâ€”like a Python tool for ActivityWatch that updates my calendar every hour or pings me if Iâ€™m wasting time on YouTube.\n\nI want to know is there something i am missing? like better way to code with agents?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2ly9nd",
          "author": "-rnr",
          "text": "wish I could help, agents always feel like pair programming with someone who wonâ€™t stop refactoring.",
          "score": 11,
          "created_utc": "2026-01-30 14:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3l9k",
          "author": "Zeikos",
          "text": "Me.  \nI use LLMs as a fancy google.  \nI like my IDE as uncluttered ad possible.",
          "score": 10,
          "created_utc": "2026-01-30 15:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ma6qd",
          "author": "dsartori",
          "text": "I enjoy using Cline in VSCode, but I rarely give it permission to write directly to files. This is an evolution of my approach based on my own setup and experience. Jesus take the wheel coding is usually the illusion of productivity more than the reality if you have any standards for your codebase.",
          "score": 6,
          "created_utc": "2026-01-30 15:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mf2kf",
          "author": "mdizak",
          "text": "\n\nI'm blind, so don't use any of the IDEs as none of them are accessible via screen reader.\n\n\n\nOnly time I really use code LLMs give me in the browser is using Gemini to correct typos basically.  Just in the last few months these LLMs have finally gotten good enough so I can bang out say a 300 line Rust struct, then just pass it to Gemini to fix all the syntax and braces / brackets errors, and have it actually work.  That's been really nice, and a huge time saver.\n\n\n\nOther than that, I don't ever use code from LLMs as I find it slopppy, overly verbose, and poor design choices.  That's expected, as these are just predictive machines trained on the entirety of the internet, so by design, you're going to get the most average, middle of the road code out there.\n\n\n\nI do however use Claude Code asisstant here and there.  If I just need something done for a data processing or training pipeline of some kind, and other things that won't be going into production, then I'll sometimes use that.   Although think I'll start steering away from that, because as per usual, when I begin leaning on these things more I end up realizing their screw ups ultimately cost me more time and stress than any initial development time savings I get.",
          "score": 5,
          "created_utc": "2026-01-30 15:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ndlhq",
              "author": "monskull_",
              "text": "How blind ppl code? It's very impressive too.",
              "score": 3,
              "created_utc": "2026-01-30 18:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2m4zkx",
          "author": "Adept_Carpet",
          "text": "This is making me feel better because I feel like I should be using agents but the chat works better.\n\n\nThe agents work alright if everything is set up in a certain way, but all the projects I work on have stuff that kills agents like unused old versions of code sitting in directories or documentation from other projects that is similar enough to be reused if you are a human but makes agents very confused.\n\n\nThat stuff shouldn't be there, but it is and I can't get rid of it solely to enable more vibe coding.",
          "score": 3,
          "created_utc": "2026-01-30 15:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m9c3v",
              "author": "monskull_",
              "text": "I thought i am missing something.",
              "score": 2,
              "created_utc": "2026-01-30 15:30:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nbosm",
          "author": "UncleRedz",
          "text": "I find that I keep moving back and forth between web chat copy/paste and Qwen Code / Copilot Agent Mode etc. What's often missing in the discussions here is what languages and tech stack you are using and the state of the code base. Let me provide an example, I tried to use agents to build a Flutter app from scratch and it was a horrible waste of time. After a week of evenings I gave up and used Gemini Pro on the web instead, that worked great and was many times faster.\n\nWhen you know how to write the code, but see that the AI generates about 80-90% correct code, it's at least for me,, faster to just do copy paste, fix the last 10-20%, than it is to watch the AI correct it self like a trainwreck, retry, test, compile, try to fix something completely unrelated, still fail and announce that it successfully completed the task, remind it and have it try again, until it succeeded but at the same time refactored a bunch of unrelated stuff and left piles of unused code from trial and errors.\n\nFunny thing, once my Flutter app was practically feature complete, I tried Qwen Code again, and this time it actually worked perfectly, my guess is that there were enough code there for it to understand what needed to be done and how to fit it in, as opposed to an empty or nearly empty code base.\n\nFor other projects, I've had better success, HTML/JavaScript/CSS seems to work rather well, and creating boilerplate code in C# also works well, when the right classes are in the context for the AI to know how things should be implemented.\n\nMy point is that how successful you are seems to depend on multiple factors, such as how well the model handles your specific tech stack, and even down to specific versions of libraries, in addition to how clean, large or small your codebase is. And I also suspect that people have different tolerances for watching the AI trainwreck trying to repair it's mistakes, versus doing it them self, and what quality of code they accept. \n\nAt the end of the day, most important, is that every developer needs to own the code that the AI generates, when main branch breaks, or bugs reach customers, it's not AI's fault, it's the developer who needs to own it and fix it.",
          "score": 3,
          "created_utc": "2026-01-30 18:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o52jp",
          "author": "radarsat1",
          "text": "I would like to jump back into using agents but gave up for now since Google cut off free Gemini API usage. Somehow the browser interface I can basically spam the model as much as I want, so I use that.\n\nSo I end up architecting things so that most changes can be done on a single file instead of requiring little changes in many places, leads to good structure anyway.\n\nI do kind of miss working with agents though, but it can get wild. I agree with people here that there is something more careful and controlled when forced into using copy-paste, but it is also annoying. I feel like there must be some yet to be discovered interface that is a happy medium.",
          "score": 3,
          "created_utc": "2026-01-30 20:34:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lzh8t",
          "author": "IONaut",
          "text": "Yeah I don't use agents. I have a couple VS code plugins that can be agentic but I generally don't let them edit code at all. I just have them for the convenience of having an interface in the sidebar. Usually I'll just have them refactor something or provide autocomplete or write me a boiler plate function that is short enough that I can vet it before running it. I also don't use online APIs and run everything locally with LM Studio.",
          "score": 2,
          "created_utc": "2026-01-30 14:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m22ol",
              "author": "monskull_",
              "text": "Same.",
              "score": 2,
              "created_utc": "2026-01-30 14:56:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lzq9t",
          "author": "SerDetestable",
          "text": "vscode with copilot and opus",
          "score": 2,
          "created_utc": "2026-01-30 14:44:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m2etv",
              "author": "monskull_",
              "text": "what is opus?",
              "score": 2,
              "created_utc": "2026-01-30 14:57:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2m2l6g",
                  "author": "SerDetestable",
                  "text": "Opus 4.5, anthropic reasoning flagship model.",
                  "score": 3,
                  "created_utc": "2026-01-30 14:58:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2oxvxl",
                  "author": "drinksbeerdaily",
                  "text": "Yeah, you need to try Opus 4.5 in Claude Code or Opencode.",
                  "score": 1,
                  "created_utc": "2026-01-30 22:54:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m6ibj",
          "author": "knownboyofno",
          "text": "Yea, that's a problem. I normally have to say something like be DRY and SOILD while following the current repo conventions. It changes a bit based on the model I am using. I always make a plan then make sure it looks good before i have it do the work. I normally have two git worktrees open to work on two features at a time.",
          "score": 2,
          "created_utc": "2026-01-30 15:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2manee",
          "author": "dondie8448",
          "text": "Cant trust the agents with my code! They messed up my work a couple of times. Never again. Rather do it myself than let them screw up.",
          "score": 2,
          "created_utc": "2026-01-30 15:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mc9d4",
          "author": "gman55075",
          "text": "I always feel like a code agent is asking for trouble, tho that may be because I'm not good enough at debugging and I use a conversational prompting style to plan, then fine down to pseudo, then code.  I actually ended up building my own browser-style desktop API wrapper that can receive code output as artifacts, let me review/ edit them, then copy and paste into my IDE.  Maybe not ideal for someone with better skills but for me it works.",
          "score": 2,
          "created_utc": "2026-01-30 15:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2me1on",
          "author": "darkwingdankest",
          "text": "rookies",
          "score": 2,
          "created_utc": "2026-01-30 15:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mewme",
          "author": "typeryu",
          "text": "I have been using Codex with 5.2 high and it is very good. Unlike Claude Code which kind of does its own thing sometimes, I can steer pretty well to a point where I am generally explaining in natural language what I want and it takes care of the syntax which means I still know the codebase as if I did it myself. I imagine this is what it feels like to drive those quadrupedal robots which you give it inputs like playing video games and the AI figures out where to place the feet.",
          "score": 2,
          "created_utc": "2026-01-30 15:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mod82",
          "author": "Awkward-Customer",
          "text": ">and leave no trace of what they actually changed  \n...\n\n>I spend another hour just reverting their mistakes  \n...  \nI want to know is there something i am missing? like better way to code with agents?\n\n  \nAre you using revision control with your code bases? The copy/pasting from the web is a huge waste of time, try roocode or cline in visual studio, make sure your code is all revision controlled, review the changes and commit the code frequently. Or use claude code directly if you don't want to use the IDE plugins.\n\nIt should be very clear what these tools are doing if you're using git to manage your projects.",
          "score": 2,
          "created_utc": "2026-01-30 16:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ne1kv",
              "author": "monskull_",
              "text": "I usually use git create new branch let them do what they want if did't work delete the branch. This only happen with Gemini-CLI you can't even find old code in vs code timeline",
              "score": 1,
              "created_utc": "2026-01-30 18:31:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2nfqwm",
                  "author": "Awkward-Customer",
                  "text": "Ok, but if you're using git how do you have no trace of what they actually changed? I'm just trying to understand the problem you're running into here because I find having the tools work with the code directly at least an order of magnitude faster and copy/pasting from the web interface.",
                  "score": 2,
                  "created_utc": "2026-01-30 18:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mou7c",
          "author": "AurumDaemonHD",
          "text": "Exactly the ai agent frameworks are token munchers that need handholding and fall apart if u look at them wrong.\n\nYesterday i posted on [locallama](https://www.reddit.com/r/LocalLLaMA/s/lR5inneFwF) that i built a fish shell script to manage context maybe you could try that.",
          "score": 2,
          "created_utc": "2026-01-30 16:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2os9ef",
          "author": "No_Afternoon_4260",
          "text": "My boss",
          "score": 2,
          "created_utc": "2026-01-30 22:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ow2uh",
          "author": "esaule",
          "text": "use git my friend!",
          "score": 2,
          "created_utc": "2026-01-30 22:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3qmk",
          "author": "BidWestern1056",
          "text": "incognide and npcsh\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 2,
          "created_utc": "2026-01-30 23:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p83fc",
          "author": "TheCientista",
          "text": "I architect with ChatGPT chatting on the browser. Get it to make a tightly controlled instruction block for Claude code and paste that in. CC does the work and produces a summary. ChatGPT checks it. I pay two subs. Itâ€™s been great so far and only getting better. CC needs controlling so chat holds it to account. I am always in the loop. I clean as I go along.",
          "score": 2,
          "created_utc": "2026-01-30 23:49:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3mft",
          "author": "No_Knee3385",
          "text": "When the IDE agent is rate limited",
          "score": 1,
          "created_utc": "2026-01-30 23:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2prszh",
          "author": "Johnlee01223",
          "text": "Same here.\n\nLLM, the quality of output correlates to the quality of input but in a larger codebase, these agents tend to add tons of unrelated stuffs to the context which ends up degrading the quality of the output unless everything is set up and documented in certain way.",
          "score": 1,
          "created_utc": "2026-01-31 01:40:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnpgcr",
      "title": "Stop manually iterating on agent prompts: I built an open-source offline analyzer based on Stanford's ACE that extracts prompt improvements from execution traces",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "author": "cheetguy",
      "created_utc": "2026-01-26 19:04:50",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "Some of you might have seen my [previous post](https://reddit.com/r/LLMDevs/comments/1obp91s/i_opensourced_stanfords_agentic_context/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: agentic system prompting from agent traces.\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's historical execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insight\n\n**How this compares to DSPy/GEPA:**\n\nWhile DSPy works best with structured data (input/output pairs), ACE is designed to work directly on execution traces (logs, conversations, markdown files) and keeps humans in the loop for review. Compared to GEPA, the ACE paper was able to show significant improvements on benchmarks.\n\n**Try it yourself:** [https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear your feedback if you do try it out",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o20ricf",
          "author": "JustViktorio",
          "text": "Why not to make a CLI tool from that so it could be callable in Claude Code / Codex / Open Code runtime?",
          "score": 2,
          "created_utc": "2026-01-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o217fmj",
              "author": "cheetguy",
              "text": "We're working on this. Hopefully we can release in the next couple of days. Join our Discord to stay updated: [https://discord.com/invite/mqCqH7sTyK](https://discord.com/invite/mqCqH7sTyK)",
              "score": 1,
              "created_utc": "2026-01-27 15:43:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpmtms",
      "title": "LAD-A2A: How AI agents find each other on local networks",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qpmtms/lada2a_how_ai_agents_find_each_other_on_local/",
      "author": "franzvill",
      "created_utc": "2026-01-28 20:22:21",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.94,
      "text": "AI agents are getting really good at doing things, but they're completely blind to their physical surroundings.\n\nIf you walk into a hotel and you have an AI assistant (like the Chatgpt mobile app), it has no idea there may be a concierge agent on the network that could help you book a spa, check breakfast times, or request late checkout. Same thing at offices, hospitals, cruise ships. The agents are there, but there's no way to discover them.\n\nA2A (Google's agent-to-agent protocol) handles how agents talk to each other. MCP handles how agents use tools. But neither answers a basic question: how do you find agents in the first place?\n\nSo I built LAD-A2A, a simple discovery protocol. When you connect to a Wi-Fi, your agent can automatically find what's available using mDNS (like how AirDrop finds nearby devices) or a standard HTTP endpoint.\n\nThe spec is intentionally minimal. I didn't want to reinvent A2A or create another complex standard. LAD-A2A just handles discovery, then hands off to A2A for actual communication.\n\nOpen source, Apache 2.0. Includes a working Python implementation you can run to see it in action. Repo can be found at franzvill/lad.\n\nCurious what people think!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qpmtms/lada2a_how_ai_agents_find_each_other_on_local/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2aon0h",
          "author": "Samrit_buildss",
          "text": "This is a nice framing of the discovery gap between agents vs tool usage. One question I had reading this: how do you think about *trust and scoping* once discovery is automatic? mDNS-style discovery works well for devices, but agents feel trickier e.g. avoiding accidental discovery across VLANs, or ensuring an agent only advertises capabilities to peers with the right trust context.\n\nCurious whether you see LAD-A2A as purely local / human-controlled environments (home, hotel, office), or something that could safely generalize beyond that with additional constraints.",
          "score": 2,
          "created_utc": "2026-01-28 21:50:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fi7hj",
          "author": "ChanceKale7861",
          "text": "Goodness this is refreshing. Appreciate that you augment a separate protocol. Using MCP/A2A/ANP in tandem to address multi agent aspects is key. Thanks for this!",
          "score": 2,
          "created_utc": "2026-01-29 15:57:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fik6p",
              "author": "franzvill",
              "text": "Thanks! I'm posting the links here if you want to read more:\n\n[https://github.com/franzvill/lad](https://github.com/franzvill/lad)\n\n[https://lad-a2a.org/](https://lad-a2a.org/)",
              "score": 2,
              "created_utc": "2026-01-29 15:59:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a4pr9",
          "author": "franzvill",
          "text": "Links:   \n[https://github.com/franzvill/lad](https://github.com/franzvill/lad)  \n[https://lad-a2a.org/](https://lad-a2a.org/)",
          "score": 1,
          "created_utc": "2026-01-28 20:22:50",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq72ap",
      "title": "Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1qq72ap",
      "author": "Charming_Group_2950",
      "created_utc": "2026-01-29 12:19:50",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq72ap/quantifying_hallucinations_by_calculating_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2l09rg",
          "author": "Mikasa0xdev",
          "text": "TrustifAI is smart. We need better LLM evaluation frameworks now.",
          "score": 1,
          "created_utc": "2026-01-30 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lipvq",
              "author": "Charming_Group_2950",
              "text": "Thanks! Glad you like it. Pls spread the word.",
              "score": 1,
              "created_utc": "2026-01-30 13:16:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmho0s",
      "title": "Does anyone know of tools that let you branch off AI conversations without cluttering the main chat?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qmho0s/does_anyone_know_of_tools_that_let_you_branch_off/",
      "author": "Nkt_31",
      "created_utc": "2026-01-25 12:18:50",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I've been using AI for research and I keep running into this annoying workflow issue. I'll be in the middle of a good conversation, then the AI mentions something technical or uses a term I don't fully understand. When I ask for clarification in the same chat, it just keeps adding to this long scrolling mess and I lose track of the main thread.\n\nLike yesterday I was asking about data validation methods and wanted to quickly understand what it meant in that context. But if I ask in the same conversation, now my main research chat has this tangent stuck in the middle of it, and the AI's context window gets filled with stuff that's not really relevant to my main question.\n\nI know some apps have \"fork\" features or conversation branching, but I haven't found anything that actually works well for this. Ideally I'd want to:\n\nâ€¢â   â Highlight a specific part of the AI's response \n\nâ€¢â   â Branch off into a separate mini-conversation just about that\n\nâ€¢â   â Keep that exploration isolated so it doesn't pollute the main chat\n\nâ€¢â   â Maybe save the key insight and attach it back to the original point\n\nDoes anything like this exist? Or am I just supposed to open 10 different chat windows and copy-paste context around like a caveman?\n\nWould genuinely appreciate any suggestions. This is driving me nuts.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qmho0s/does_anyone_know_of_tools_that_let_you_branch_off/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1s2n1s",
          "author": "chatpatahu",
          "text": "wait, this is exactly what I needed last week when I was going down research rabbit holes  \n  \nI think the issue is most AI chat interfaces are designed for casual use, not actual deep research where you need to explore tangents. Like the UI assumes you're just having one linear conversation, but real research doesn't work that way at all  \n  \nhave you looked into any of the self-hosted options? I feel like some of the open source stuff might have better features for this kind of workflow since they're built by people who actually do research with AI",
          "score": 2,
          "created_utc": "2026-01-26 07:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oozkh",
          "author": "throwaway490215",
          "text": "I'd like some improvement in this space as well, but I thought the 90% usecase was already available in most clients? \n\nClaude Code and OpenCode both allow you to --resume any chat, or jump back to a previous point and 'undo' parts of the chat.",
          "score": 1,
          "created_utc": "2026-01-25 20:29:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qm53f",
              "author": "The_Noble_Lie",
              "text": "And now /fork widely available.  I had my own fork / clone convo up until now",
              "score": 1,
              "created_utc": "2026-01-26 01:50:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pxinq",
          "author": "Remarkable_Training9",
          "text": "hmm... this is literally what I built! My extension lets you organize conversations with folders and tags, so you can branch off tangents without cluttering the main chat. Plus it is cross-platform (chatgpt + claude) and all data stored locally. DM me if you want to try it out!",
          "score": 1,
          "created_utc": "2026-01-25 23:48:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1riqtx",
          "author": "elbiot",
          "text": "Claude definitely has that. You edit a message and it branches the conversation. Pretty sure ChatGPT does too",
          "score": 1,
          "created_utc": "2026-01-26 04:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s9m1s",
          "author": "Acceptable_Driver655",
          "text": "This is huge actually. I've been doing the \"10 different chat windows\" method and it's such a mess\n\nThe biggest problem I have is when I'm 20 messages deep into a conversation about one topic, then I need to understand one specific thing the AI mentioned, but asking about it in the same thread derails everything. And starting a fresh chat means I lose all the prior context\n\nIf this research layers thing lets you branch off while keeping the main conversation focused, that's basically how my brain actually works when doing research. I don't think linearly - I explore tangents, gather insights, then come back to the main path\n\nGoing to try this out tonight. The fact that it's self-hosted is actually a plus for me because I'm working with some proprietary data and don't want it going through too many external services\n\nThanks for the tip",
          "score": 1,
          "created_utc": "2026-01-26 08:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sfnaj",
          "author": "Ok-Lack-7216",
          "text": "On ChatGPT, there is an option. click 'more actions (three dots)' at the end of the response and branch out. \n\nhttps://preview.redd.it/tb7vjnp5vnfg1.png?width=307&format=png&auto=webp&s=6e196b69c5098f7dfd799e2f1a029ced8c7c4af8",
          "score": 1,
          "created_utc": "2026-01-26 09:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u7a6x",
          "author": "Alternative_Nose_874",
          "text": "For what youâ€™re asking about, there *are* some options beyond just opening new tabs and copy-pasting. A few open source tools let you create actual branches or isolated threads off a main LLM conversation so your main context stays clean, for example Delta, a local app that lets you rewind and branch chats into different directions without polluting the original thread, and Context Branching SDK which is literally built to isolate exploration branches from the main conversation context.\n\nThereâ€™s also Multiversalchats on GitHub that treats messages as nodes and lets you branch/merge conversations like a flowchart, and some self-hosted research UIs (like KEA Research mentioned in the thread) that let you start â€œresearch layersâ€ off a highlighted point to explore without clutter.\n\nSo youâ€™re not stuck with copypaste. but most good branching workflows right now live in opensource/selfhosted tools rather than in mainstream chat UIs.",
          "score": 1,
          "created_utc": "2026-01-26 16:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vu1dp",
          "author": "Strong_Worker4090",
          "text": "I usually just create a project and create different threads like: strategy, strategy questions, mvp, mvp questions, etc.\n\nThat way the project maintains its context but you can ask clarify question in a different thread with shared project context. \n\nAn example is \n\nThread 1: â€œgive me my mvp roadmap over the next weekâ€\n\nThread 2: used to answer questions from these one without losing context in thread 1",
          "score": 1,
          "created_utc": "2026-01-26 20:12:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpbuhf",
      "title": "Local LLM deployment",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qpbuhf/local_llm_deployment/",
      "author": "Puzzleheaded-Ant1993",
      "created_utc": "2026-01-28 13:49:27",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.83,
      "text": "Ok I have little to no understanding on the topic, only basic programming skills and experience with LLMs. What is up with this recent craze over locally run LLMs and is it worth the hype. How is it possible these complex systems run on a tiny computers CPU/GPU with no interference with the cloud and does it make a difference if your running it in a 5k set up, a regular Mac, or what. It seems Claude has also had a â€˜fewâ€™ security breaches with folks leaving back doors into their own APIs. While other systems are simply lesser known but I donâ€™t have the knowledge, nor energy, to break down the safety of the code and these systems. If someone would be so kind to explain their thoughts on the topic, any basic info Iâ€™m missing or donâ€™t understand, etc. Feel free to nerd out, express anger, interest, Iâ€™m here for it all I just simply wish to understand this new era we find ourselves entering. ",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qpbuhf/local_llm_deployment/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2811gw",
          "author": "Rand_o",
          "text": "I run a 30B sized LLM locally on a 128 GB iGPU setup via AMD and it is decent. Ive spent the last 2 weeks learning how to set it all up, how it works, etc. Itâ€™s slower than cloud. If you wanna match cloud performance right now its probably $10k or more worth of equipment and you still wont exactly match claude or chatgpt. But I do think things are going to keep improving and we will eventually get to the point where running locally is extremely good. Right now itâ€™s almost there but not quite for the average person. Still impressive thoughÂ ",
          "score": 1,
          "created_utc": "2026-01-28 14:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o281kdx",
              "author": "Rand_o",
              "text": "As far as what device to get, depends what you wanna do. I got a framework desktop for $2k (which has jumped to nearly $3k for the same machine) but now I kinda wish I just got a mac studio instead. It works but it doesnt really have the performance it should imo. Or if you want a beast you need to drop serious money - the cost of a car lol",
              "score": 2,
              "created_utc": "2026-01-28 14:52:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29jn4l",
              "author": "Danzaar",
              "text": "How much slower is it?",
              "score": 1,
              "created_utc": "2026-01-28 18:49:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a69xe",
                  "author": "Rand_o",
                  "text": "it has acceptable speed for about 3 requests when I am trying to code with it. Each request will take 10-15 min. Past that - 4+ requests it takes an hour or more to complete. So I am trying to come up with techniques to do everything in small steps",
                  "score": 1,
                  "created_utc": "2026-01-28 20:29:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29dafl",
          "author": "attn-transformer",
          "text": "Local models are smaller, and often trained for a narrow use case.   \nOllama is a good place to start.",
          "score": 1,
          "created_utc": "2026-01-28 18:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bslc1",
          "author": "Clay_Ferguson",
          "text": "The main reason to use a local LLM (or SLM) is when you need privacy because you have customer data or other sensitive information that you don't want to send out across the web, or if not for privacy reasons then because there's just a vast amount of actual 'inference' (prompting) that you need to do over hundreds or thousands of files, perhaps , where it would get expensive to do it on a cloud paid service. \n\nwhat you lose is significant IQ points when you run locally, so if you have an extremely difficult problem to solve , or if you just want to write the best code possible and that's when you want to try to definitely use a best in class type SOTA model from an online provider. however, I might be exaggerating the loss of IQ points , because I think the local models might be only like one year behind the best LLMs in terms of their capabilities, so for most use cases the loss of IQ points is probably negligible .",
          "score": 1,
          "created_utc": "2026-01-29 01:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28fyq7",
          "author": "cmndr_spanky",
          "text": "The real use case is enterprise companies who donâ€™t want to send data over to a cloud hosted model like chatGPT / Claude. For simple agentic or document chat systems you can get nearly equiv performance out of smaller LLMs. So even running a much bigger local LLM thatâ€™s 100 to 200b sized might be worth it, but often 32b is even good enough. Secondarily, with high token usage the costs of using vendor hosted models is also going to sting (even a mid sized company), and running a local model on $10k+ hardware can still save money in the long run. A lot of money.",
          "score": 0,
          "created_utc": "2026-01-28 15:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o296fkq",
              "author": "DistributionOk6412",
              "text": "If you can run a good local model on a $10k hardware for a 20 ppl company i'm giving you $10k. The costs are extremely high and I'm sick of ppl with no experience making costs estimations",
              "score": 2,
              "created_utc": "2026-01-28 17:53:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28uj7h",
              "author": "czmax",
              "text": "Even this is sometimes overstated... sending data to a trusted cloud hosted model with appropriate legal/contractual/security protections to manage risk can work well. \n\nLike just general compute there are tons of reasons for local vs cloud. Most importantly, having the choice is a good thing for the industry. Although I've played with local models on my home systems I'm simply not investing enough in HW to make it as effective as a cloud solution for me. But I'm stoked to see the enthusiasm and work that made it possible for me to experiment with it. \n\nI hope the pendulum swings back. When local models are powerful enough, when HW is cheap enough, and when model architectures support true learning I'm hoping we see models that can develop over time to be good partners to individuals. When/if that happens I don't want to see vendor lock-in and prefer a more open ecosystem.",
              "score": 0,
              "created_utc": "2026-01-28 17:01:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o293kf2",
          "author": "burntoutdev8291",
          "text": "Mostly safety and data governance. The local models cannot beat the larger models, but for specific use cases they might be sufficient. A good RAG system doesn't really need strong models.\n\nAnother factor is cost but this needs analytics. Can you prove that your workload will save more from upfront hardware costs vs API? Because don't forget hardware is depreciating (without considering the RAM surges).",
          "score": 0,
          "created_utc": "2026-01-28 17:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2910y6",
          "author": "Western_Bread6931",
          "text": "i was just browsing through pottery barn minding my own biz when suddenly this ridiculous clown in a bright orange wig and neon shoes comes strolling in honking that absolute monster of a horn im not even kidding it was like he wanted to clear out the whole store\nand what does my body decide to do betrayal i pooped myself like hard im talking call the cleanup crew level everyones staring and im just standing there frozen praying for a hole to swallow me whole the worst part the clowns just laughing like its the funniest thing hes seen all day\npottery barn staff had to call my wife to come get me im never going back there again if anyone needs me ill be here rethinking life choices and burning these pants",
          "score": -5,
          "created_utc": "2026-01-28 17:29:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qom1i3",
      "title": "Do you use Evals?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qom1i3/do_you_use_evals/",
      "author": "InvestigatorAlert832",
      "created_utc": "2026-01-27 18:26:02",
      "score": 6,
      "num_comments": 11,
      "upvote_ratio": 0.88,
      "text": "Do people currently run evaluations on your prompt/workflow/agent?\n\nI used to just test manually when iterating, but it's getting difficult/unsustainable. I'm looking into evals recently, but it seems to be a lot of effort to setup & maintain, while producing results that're not super trustworthy.  \n  \nI'm curious how others see evals, and if there're any tips?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qom1i3/do_you_use_evals/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o23xq84",
          "author": "kubrador",
          "text": "yeah evals are the \"we should probably do this\" that everyone avoids until their thing breaks in production. manual testing works great until you ship something that makes you want to delete your github account.\n\nthe annoying part is you're right. setting them up sucks and they're still kinda made up. i'd start stupid though: just pick like 5 test cases that would kill you if they broke, throw them in a txt file, and check them when you change stuff. beats maintaining a whole framework that makes you feel productive while being wrong.\n\nonce you have that baseline of \"oh this actually caught something real,\" then maybe think about scaling it. brute forcing lcm calls through test cases is way cheaper than debugging user complaints.",
          "score": 4,
          "created_utc": "2026-01-27 22:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25jxm4",
              "author": "InvestigatorAlert832",
              "text": "Thanks for the suggestion! So for test cases do  you mean I should put a bunch of messages arrays in there, run LLM calls and evaluate responses manually?",
              "score": 1,
              "created_utc": "2026-01-28 04:05:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o24gmd4",
          "author": "3j141592653589793238",
          "text": "Whether you use evals is what often separates successful and unsuccessful projects. Start with small sets, you can expand them later. Whether it's trustworthy depends on the type of eval & problem you're trying to solve. E.g. if you use LLMs to predict a number w/ structured outputs you can have a direct eval that's as trustworthy as your data is.\n\n[deeplearning.ai](http://deeplearning.ai) agentic AI course by Andrew Ng has a good introduction into evals for LLMs\n\nAlso, not mentioned there but I find running evals multiple times and averaging out results helps to stabilise some of the non-determinism in LLMs, just make sure you use a different seed each time (matters a lot for models like Gemini).",
          "score": 3,
          "created_utc": "2026-01-28 00:34:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25j0dn",
              "author": "cmndr_spanky",
              "text": "Or you could do like 15mins of reading and not pay for a dumb course",
              "score": 2,
              "created_utc": "2026-01-28 04:00:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27earm",
                  "author": "3j141592653589793238",
                  "text": "But the course is free... it's also written by someone with lots of credentials in the field e.g. he's a co-founder of Google Brain, adjunct professor at Stanford alongside many other things. It's likely to be better than some AI generated Medium article.\n\nWorth mentioning, I'm not affiliated with the course in anyway.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:48:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25im7w",
              "author": "InvestigatorAlert832",
              "text": "Thanks for the tips and the course, I'll definitely check it out!\nYou mentioned that trustworthiness depends on the type of problem, I wonder whether you have any tips on eval for chatbot, whose answer/decision can not be necessarily checked by simple code?",
              "score": 1,
              "created_utc": "2026-01-28 03:57:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27eosl",
                  "author": "3j141592653589793238",
                  "text": "Check out the course, it explores a few different approaches e.g. programmatically calculated metric, LLM-as-a-judge. It really depends, what is the purpose of your chatbot.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:50:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22h4if",
          "author": "demaraje",
          "text": "Test sets",
          "score": 1,
          "created_utc": "2026-01-27 19:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25mevh",
          "author": "Bonnie-Chamberlin",
          "text": "You can try LLM-as-Judge framework. Use listwise or pairwise comparison instead of one-shot.",
          "score": 1,
          "created_utc": "2026-01-28 04:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dq3d",
          "author": "PurpleWho",
          "text": "You're right, evals are a pain to set up.\n\nI generally use a testing playground embedded in my editor, likeÂ [Mind Rig](https://mindrig.ai/)Â orÂ [vscode-ai-toolkit](https://github.com/microsoft/vscode-ai-toolkit),Â over a more formal Eval tool like PromptFoo, Braintrust, Arize, etc.   \n  \nUsing an editor extension makes the \"tweak prompt, run against dataset, review results\" loop much faster. I can run the prompt against a bunch of inputs, see all the outputs side-by-side, and catch regressions right away. Less setup hassle but more reliability than a mere vibe check.  \n   \nOnce your dataset grows past 20-30 scenarios, I just export the CSV of test scenarios to a more formal eval tool.",
          "score": 1,
          "created_utc": "2026-01-28 07:43:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnuyyj",
      "title": "Reducing token costs on autonomous LLM agents - how do you deal with it?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnuyyj/reducing_token_costs_on_autonomous_llm_agents_how/",
      "author": "PatateRonde",
      "created_utc": "2026-01-26 22:18:50",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "Hey,\n\nI'm working on a security testing tool that uses LLMs to autonomously analyze web apps. Basically the agent reasons, runs commands, analyzes responses, and adapts its approach as it goes.\n\nThe issue: It's stateless. Every API call needs the full conversation history so the model knows what's going on. After 20-30 turns, I'm easily hitting 50-100k tokens per request, and costs go through the roof  \n  \nWhat I've tried:\n\n  \\- Different models/providers (GPT-4o, GPT-5, GPT-5mini, GPT 5.2,  DeepSeek, DeepInfra with open-source models...)\n\n  \\- OpenAI's prompt caching (helps but cache expires)\n\n  \\- Context compression (summarizing old turns, truncating outputs, keeping only the last N messages)\n\n  \\- Periodic conversation summaries  \n\n\nThe problem is every approach has tradeoffs. Compress too much and the agent \"forgets\" what it already tried and goes in circles. Don't compress enough and it costs a fortune.  \n\n\nMy question:\n\nFor those working on autonomous agents or multi-turn LLM apps:\n\n  \\- How do you handle context growth on long sessions?\n\n  \\- Any clever tricks beyond basic compression?\n\n  \\- Have you found a good balance between keeping context and limiting costs?\n\n\n\nCurious to hear your experience if you've dealt with this kind of problem.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnuyyj/reducing_token_costs_on_autonomous_llm_agents_how/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1wohch",
          "author": "pmv143",
          "text": "Youâ€™ve basically hit the core limitation of stateless LLM APIs. Once agents become long running and toolusing, the real cost isnâ€™t actually tokens, itâ€™s repeatedly reconstructing state. Compression can help but itâ€™s lossy by definition. which is why agents loop or forget. One alternative pattern that work better is treating agent state as runtime state instead of prompt state . keep the model warm, preserve KV / execution context across turns, and only serialize when you truly need to suspend. That shifts the problem from prompt engineering to lifecycle management, but it avoids paying the full context price on every step.",
          "score": 4,
          "created_utc": "2026-01-26 22:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wueib",
              "author": "PatateRonde",
              "text": "That's a good point. I've been too focused on the prompt side and not enough on treating it as a runtime problem. I'm currently on commercial APIs OpenAI, DeepSeek... OpenAI's prompt caching now supports 24h retention which helps, but it still requires exact prefix matches and the cache can get invalidated easily when the conversation branches.\n\nI've been looking into self-hosted options. Looks like vLLM + LMCache is the go-to combo for this apparently it can give 3-10x improvements on multi-turn workloads by properly managing KV cache across turns. There's also llm-d for KV-cache aware routing if you're running multiple instances.  \n  \nHave you actually deployed something like this in production? My main concern is whether open-source models (Llama, Qwen, etc.) can match GPT-4o /GTP 5 quality for agentic tasks that require good reasoning and tool use. Trading 10x cost savings for an agent that hallucinates more doesn't seem worth it.\n\nTbh I'm not an expert, still figuring all this out as I go. But thanks for the insight, really helpful to shift my perspective on this.",
              "score": 1,
              "created_utc": "2026-01-26 22:56:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x1qb6",
                  "author": "pmv143",
                  "text": "Weâ€™ve deployed this pattern in production. but with a slightly different framing. KV cache reuse (vLLM, LMCache, cache aware routing) absolutely helps, but it still assumes a mostly warm, long lived process. For agentic workloads with branching, tool waits, and spiky traffic, the bigger win for us was treating each step as a short lived execution and externalizing state entirely. That way youâ€™re not paying tokens or GPUs to â€œrememberâ€ things that the system already knows. On model quality, weâ€™ve seen that open source models can match GPT 4 class reasoning for many agent loops when tool use is explicit and scoped, but for the hardest reasoning steps we still mix in frontier models. In practice it ends up being a hybrid system, not an either or tradeoff between cost and correctness.",
                  "score": 2,
                  "created_utc": "2026-01-26 23:33:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1xd0sz",
          "author": "tech2biz",
          "text": "This is 100% the same background we had. Built n8n first, then own agents. Then went on-prem but quality was bad (in some cases). Then tried hybrid but static routing was either high cast or bad output, we couldnâ€™t find the in between. And so developed dynamic cascading (cascadeflow), itâ€™s open source and on github. Tries small model first, cascades to large one if needed. You can work it in any infra you already have. Hope it helps you with this. Lmk",
          "score": 4,
          "created_utc": "2026-01-27 00:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o234utb",
              "author": "PatateRonde",
              "text": "Oh nice, that's exactly the rabbit hole I'm in right now. Thanks for sharing the repo, I'll check it out and try to plug it into my setup. Will let you know if I manage to get it working!",
              "score": 2,
              "created_utc": "2026-01-27 20:46:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o235201",
                  "author": "tech2biz",
                  "text": "Awesome! Lmk if I can help in any way :)",
                  "score": 1,
                  "created_utc": "2026-01-27 20:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1wua0d",
          "author": "thisdude415",
          "text": "The key, I think, is to build a representation of the workflow, and \"fill it in\" with context as the agents work. \n\nThe goal is to keep your orchestration agent's context minimally littered with low-level work\n\nIf you can truly figure it out, you've got a high paying job waiting for you at your choice of AI lab",
          "score": 2,
          "created_utc": "2026-01-26 22:56:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wrajd",
          "author": "EpochRaine",
          "text": "Can you not use a local model, LoRa train it on your tools and content, merge into the core model and use RAG and minimal context inejction?",
          "score": 1,
          "created_utc": "2026-01-26 22:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wt081",
              "author": "PatateRonde",
              "text": "Interesting idea, but I'm not sure it's viable for my use case. I don't have the infra to run a large model locally, and from what I've tested, smaller models really struggle with the kind of multi-step reasoning and tool chaining I need for security testing. They tend to hallucinate findings or go in circles way more than GPT-5 class models. Fine-tuning could help with tool familiarity, but I'm not sure it would fix the core reasoning gap. Have you seen good results with LoRA-tuned models on complex agentic tasks?",
              "score": 2,
              "created_utc": "2026-01-26 22:50:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x2c4d",
                  "author": "Technical-History104",
                  "text": "When dealing with smaller models, you would need to deconstruct the workflow into even smaller pieces, and also rely more on regular SW code to do much of the orchestration of the work across each of the pieces.  Can your prompts and context be broken down to achieve the same goal?",
                  "score": 2,
                  "created_utc": "2026-01-26 23:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zjgdk",
          "author": "AI_Data_Reporter",
          "text": "TPT (Tokens Per Thought) efficiency remains the primary bottleneck for autonomous agents. Implementation of Observation Masking at M=10 reduces redundant state reconstruction costs by 40% in multi-hop reasoning tasks. AgentDiet protocols further prune trajectory bloat. The 'Unreliability Tax'â€”the cost of model retries due to context fragmentationâ€”is often overlooked but accounts for 15% of total spend in stateless architectures. Shifting to KV-cache aware routing is the only viable path.",
          "score": 1,
          "created_utc": "2026-01-27 09:19:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o203gn5",
          "author": "Zeikos",
          "text": "In general performance/cost in software is minimized by doing less work.  \n\nShift as much as possible of the workflow away from using the LLM. Use it only when strictly necessary.",
          "score": 1,
          "created_utc": "2026-01-27 12:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2352ay",
              "author": "PatateRonde",
              "text": "Yeah fair point. I've probably been over-relying on the LLM for stuff that could be handled with simpler logic. Gonna look into offloading more of the workflow to deterministic code and only hitting the model when I actually need reasoning.",
              "score": 1,
              "created_utc": "2026-01-27 20:47:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2155bn",
          "author": "Mole-Transistor4440",
          "text": "If weâ€™re talking about LLM call costs suddenly spiking and wanting to rein that in - what kind of reduction are you actually aiming for? Like 5%, 10%, 20%, or something more aggressive?\n\nThere are a lot of smart folks here with real-world experience and plenty of optimization tricks, but without a target number itâ€™s hard to tell which ideas are worth suggesting and which ones wonâ€™t really move the needle for you.",
          "score": 1,
          "created_utc": "2026-01-27 15:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o235hvd",
              "author": "PatateRonde",
              "text": "Honestly I don't have a hard target in mind. Right now a 30-40 turn session can easily burn through $1-3 depending on the model, and that adds up fast when you're iterating a lot during dev.\n\nI'd be happy with anything that cuts that in half without sacrificing too much output quality. But really I'm just trying to find something sustainable where I'm not scared to hit \"run\" because I know it's gonna cost me.",
              "score": 1,
              "created_utc": "2026-01-27 20:49:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o233i6z",
          "author": "yaks18",
          "text": "Thereâ€™s a recent very interesting paper on Recursive Language Models (RLMs) that reframes the problem. Instead of stuffing everything into context, the model treats long text as external data, searches it, slices it, and only reasons over the parts it needs recursively calling itself on sub-chunks.\nSo the shift is from â€œgive the model everythingâ€ to â€œnavigate,  select, reason, compose.â€\n\nhttps://arxiv.org/pdf/2512.24601",
          "score": 1,
          "created_utc": "2026-01-27 20:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26eele",
          "author": "Accurate-Ad-7944",
          "text": "yeah the context bloat is brutal, especially with web interactions where DOM snapshots eat tokens like crazy. I ended up using a hybrid approach - I keep the full last 3-4 turns in detail, then maintaining a *really* condensed running log off just actions taken and key outcomes (like \"tested login endpoint - got 403\"). \n\nthe real hack for me was caching selectors and page structures separately so the agent doesn't need the full HTML every time. I actually started using Actionbook for this recently - it kinda automates that manual caching/action manual creation I was doing, which cut my tokens usage on browser tasks by a stupid amount. not sure if it fits security testing directly, but for reducing repetitive context it helped me break the cycle of agents re-analyzing the same UI elements.\n\nstill have to tune the compression aggressively though, and sometimes the agent doesn't get lost. it's a constant trade-off.",
          "score": 1,
          "created_utc": "2026-01-28 07:48:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rpqhn",
          "author": "Fit_Strawberry8480",
          "text": "Context compression is a tax you keep paying because youâ€™re trying to storeÂ *state*Â in a stateless API.Â \n\nWhatâ€™s worked better (pattern, not a silver bullet):Â \n\n1. move durable state out of the prompt (structured action log + artifacts, not prose)Â \n2. keep the modelâ€™s job small: plan/decide, not replay historyÂ \n3. enforce a hard budget contract (tokens/time/cost) and treat â€œbudget hitâ€ as a firstâ€‘class outcome, not an exception\n\nWe builtÂ **enzu**Â (OSS) exactly for (3) + typed outcomes/job-mode so long runs donâ€™t silently explode. Hard-stop demo:  \n[https://github.com/teilomillet/enzu/blob/main/examples/budget\\_hardstop\\_demo.py](https://github.com/teilomillet/enzu/blob/main/examples/budget_hardstop_demo.py)\n\nIf you share your target constraints (max $/run, max latency, desired recall), I can suggest a concrete state layout for your security agent.",
          "score": 1,
          "created_utc": "2026-01-31 10:40:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qop6h2",
      "title": "Initial opinions on KimiK2.5?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qop6h2/initial_opinions_on_kimik25/",
      "author": "RoadKill_11",
      "created_utc": "2026-01-27 20:15:05",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 0.75,
      "text": "Just saw the launch and was wondering what you guys think of it, considering making it the default LLM for our open-source coding agent.\n\n\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qop6h2/initial_opinions_on_kimik25/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o26b4lv",
          "author": "Comfortable-Sound944",
          "text": "I'm tired of testing these over hyped over benchmarked cheap models... I so wanted them to be as good as people say, but then they aren't even at the level to disappoint",
          "score": 2,
          "created_utc": "2026-01-28 07:20:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o232kmt",
          "author": "joaotolovi",
          "text": "For now, it's as good as Opus. I'm running complex tasks, giving the same prompt in Kimi K2.5 and Claude Opus, Kimi produces more accurate, structured code with fewer errors, Claude needs a few correction cycles, and in the end, Kimi seems to deliver higher quality.",
          "score": 2,
          "created_utc": "2026-01-27 20:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o293dmb",
              "author": "seunosewa",
              "text": "Which agent?",
              "score": 2,
              "created_utc": "2026-01-28 17:39:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2c8m5u",
              "author": "RedditSellsMyInfo",
              "text": "Are you using Kimi in Claude code or in a different harness? I'm having issues with it in long running vibecoding tasks.",
              "score": 1,
              "created_utc": "2026-01-29 02:37:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tgeo8",
                  "author": "gnaarw",
                  "text": "I'm using it in Kimi cli and apart from the occasional disconnect (twice over 3 dozen sessions) I had no issues...",
                  "score": 1,
                  "created_utc": "2026-01-31 17:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27nkrd",
          "author": "techperson1234",
          "text": "Anyone test it with tool usage?",
          "score": 1,
          "created_utc": "2026-01-28 13:41:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ov4y5",
          "author": "pbalIII",
          "text": "Swapped our agent from Opus to K2.5 last week on a multi-file refactor task. Observations so far:\n\n- Front-end gen is where it shines. Screenshot-to-code flows that took 3-4 cycles with Opus landed in one pass.\n- SWE-bench scores tell the real story: K2.5 trails Claude on verified benchmarks (64% vs 77%). For gnarly backend refactors, that gap shows.\n- Cost is wild. Same task ran roughly 8x cheaper. If you're iterating fast and can tolerate a few more correction rounds, that math works.\n\nFor a default agent, I'd pick based on workload. Heavy frontend or parallel tool use? K2.5. Production backend code where iteration cycles cost dev time? Opus still justifies the premium.",
          "score": 1,
          "created_utc": "2026-01-30 22:39:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25f92n",
          "author": "elllyphant",
          "text": "Elly from [Synthetic.new](http://Synthetic.new) here - we just supported Kimi K2.5 an hour ago and you can try it privacy-first w/ 40% discount (valid til 2/1) [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)   \n  \nWe never do discounts like this but want to try to ride the [Clawd.bot](http://Clawd.bot) hype and share our hard work with everyone! We're a tiny team of 3 people in SF. If you have any feedback or questions, we're actively on Discord [https://discord.gg/dssyuXeJ](https://discord.gg/dssyuXeJ)",
          "score": 1,
          "created_utc": "2026-01-28 03:38:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnrdn7",
      "title": "Implemented the world's most accurate LLM-based password guesser",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/47npbfkk3rfg1",
      "author": "Arsapen",
      "created_utc": "2026-01-26 20:10:26",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.65,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnrdn7/implemented_the_worlds_most_accurate_llmbased/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qrcxoj",
      "title": "Claude code's main success story is their tool design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "author": "Miclivs",
      "created_utc": "2026-01-30 18:03:34",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 0.72,
      "text": "Claude Code hit $1B in run-rate revenue.\n\nIts core architecture? Four primitives: read, write, edit, and bash.\n\nMeanwhile, most agent builders are drowning in specialized tools. One per domain object (hmm hmm 20+ tool MCPs..)\n\nThe difference comes down to one asymmetry:\n\n**Reading forgives schema ignorance. Writing punishes it.**\n\nWith reads, you can abstract away complexity. Wrap different APIs behind a unified interface. Normalize response shapes. The agent can be naive about what's underneath.\n\nWith writes, you can't hide the schema. The agent isn't consuming structureâ€”it's producing it. Every field, every constraint, every relationship needs to be explicit.\n\nUnless you model writes as files.\n\nFiles are a universal interface. The agent already knows JSON, YAML, markdown. The schema isn't embedded in your tool definitionsâ€”it's the file format itself.\n\nFour primitives. Not forty.\n\nWrote up the full breakdown with Vercel's d0 results: \n\nhttps://michaellivs.com/blog/architecture-behind-claude-code\n\nCurious if others have hit this same wall with write tools.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2n9nmx",
          "author": "pborenstein",
          "text": "I've been experimenting with other LLM code harnesses. Because I started with Claude Code, I tend to compare everything to it. \n\nThe first thing I missed was CC's command/skill structure. I realized that this is probably why I've never used an MCP. \n\nThe LLM can figure out what to do from my description. It deals with ambiguity better than code.",
          "score": 1,
          "created_utc": "2026-01-30 18:12:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2na2dy",
              "author": "Miclivs",
              "text": "I think thereâ€™s a good reason to compare everything to CCâ€¦",
              "score": 2,
              "created_utc": "2026-01-30 18:13:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2njp9b",
          "author": "steinernein",
          "text": "Why even bother with four? Just reduce it down to three and alter the shape of what the API accepts and rejects.",
          "score": 1,
          "created_utc": "2026-01-30 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p6wgf",
          "author": "kubrador",
          "text": "the files-as-api thesis is clever but i'm skeptical this scales past toy problems. reads forgiving schema ignorance works until the agent hallucinates a field that doesn't exist and you're debugging why it corrupted your production database.",
          "score": 1,
          "created_utc": "2026-01-30 23:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p7lbg",
              "author": "Miclivs",
              "text": "Oh, it works AMAZINGLY well for our data analytics agent.",
              "score": 1,
              "created_utc": "2026-01-30 23:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nb9sz",
          "author": "One-Neighborhood4868",
          "text": "Just make an agent team to help out choose what tools to use?",
          "score": 0,
          "created_utc": "2026-01-30 18:19:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo71l0",
      "title": "I built an SEO Content Agent Team that optimizes articles for Google AI Search",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qo71l0/i_built_an_seo_content_agent_team_that_optimizes/",
      "author": "Arindam_200",
      "created_utc": "2026-01-27 07:17:14",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "Iâ€™ve been working with multi-agent workflows and wanted to build something useful for real SEO work, so I put together an SEO Content Agent Team that helps optimize existing articles or generate SEO-ready content briefs before writing.\n\nThe system focuses on Google AI Search, including AI Mode and AI Overviews, instead of generic keyword stuffing.\n\nThe flow has a few clear stages:\n\n\\- Research Agent: Uses SerpAPI to analyze Google AI Mode, AI Overviews, keywords, questions, and competitors  \n\\- Strategy Agent: Clusters keywords, identifies search intent, and plans structure and gaps  \n\\- Editor Agent: Audits existing content or rewrites sections with natural keyword integration  \n\\- Coordinator: Agno orchestrates the agents into a single workflow\n\nYou can use it in two ways:\n\n1. Optimize an existing article from a URL or pasted content  \n2. Generate a full SEO content brief before writing, just from a topic\n\nEverything runs through a Streamlit UI with real-time progress and clean, document-style outputs. Hereâ€™s the stack I used to build it:\n\n\\- Agno for multi-agent orchestration  \n\\- Nebius for LLM inference  \n\\- SerpAPI for Google AI Mode and AI Overview data  \n\\- Streamlit for the UI\n\nAll reports are saved locally so teams can reuse them.\n\nThe project is intentionally focused and not a full SEO suite, but itâ€™s been useful for content refreshes and planning articles that actually align with how Google AI surfaces results now.\n\nIâ€™ve shared a full walkthrough here: [Demo](https://www.youtube.com/watch?v=BZwgey_YeF0)  \nAnd the code is here if you want to explore or extend it: [GitHub Repo](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/content_team_agent)\n\nWould love feedback on missing features or ideas to push this further.",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qo71l0/i_built_an_seo_content_agent_team_that_optimizes/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1z7nk9",
          "author": "Ok_Revenue9041",
          "text": "This setup is super practical for modern SEO, especially with Google leaning heavily into AI results. If you ever want to expand your optimization to large language model platforms like ChatGPT or Claude, checking out MentionDesk could help boost your brandâ€™s visibility in those answer engines too.",
          "score": 2,
          "created_utc": "2026-01-27 07:31:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z8nzh",
          "author": "kubrador",
          "text": "cool that you're solving for ai overviews instead of whatever seo was last year, but this is basically \"prompt engineer but it costs money and talks to serpapi\" which is fine, just don't oversell it as a content team when it's really a research + rewrite tool.",
          "score": 1,
          "created_utc": "2026-01-27 07:40:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qranzr",
      "title": "How do you prevent credential leaks to AI tools?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "author": "llm-60",
      "created_utc": "2026-01-30 16:45:04",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "How is your company handling employees pasting credentials/secrets into AI tools like ChatGPT or Copilot? Blocking tools entirely, using DLP, or just hoping for the best?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2p7nzh",
          "author": "kubrador",
          "text": "hoping for the best is the classic strategy, followed by a panicked all-hands email in 6 months when someone inevitably pastes a prod database url into claude.\n\nmost companies doing it right use a combo: dlp tools with regex patterns for common secrets, network blocks on the obvious stuff, and mandatory training that employees ignore until it happens to them.",
          "score": 3,
          "created_utc": "2026-01-30 23:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pvelf",
          "author": "Basic_Cat_1006",
          "text": "Whoever is hardcoding or announcing secrets out of the .env should not be a mile near your code base lmao.",
          "score": 2,
          "created_utc": "2026-01-31 02:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vg7g9",
              "author": "graymalkcat",
              "text": "Just as an aside, if you use Claude, it will open your .env which will send that to Anthropic. Maintain different sets of keys. (Itâ€™s not that Claude is evil but rather that it forgets that it itself is a cloud service)",
              "score": 1,
              "created_utc": "2026-01-31 22:55:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2n78m1",
          "author": "Miclivs",
          "text": "I made a thing for that! [https://github.com/Michaelliv/psst](https://github.com/Michaelliv/psst)",
          "score": 2,
          "created_utc": "2026-01-30 18:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q3ut4",
          "author": "cmndr_spanky",
          "text": "The same way you prevent employees from pasting their crediting into email, slack, GitHub etcâ€¦\n\nWhy is AI any different ?",
          "score": 1,
          "created_utc": "2026-01-31 02:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfpuh",
              "author": "konmik-android",
              "text": "You mean, you ask it politely and then it ignores you?",
              "score": 1,
              "created_utc": "2026-02-01 02:19:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2t46i5",
          "author": "Friendly_Hat_9545",
          "text": "We tried the \"training and hoping\" approach at first, which lasted exactly until someone almost pasted a Dev database connection string. That was a fun afternoon lol.\n\nNow we use inline DLP that scans before stuff reaches the chatbot. We went with iboss AI Chat Security because it looks into our existing network stack and just... works? Blocks the paste if it sniffs keys or PII patterns. We still allow Copilot and ChatGPT, but now with guardrails. It's not perfect but way better than crossing our fingers.\n\nTBH, blocking the tools entirely just leads to Shadow IT. You gotta let people use the tools but make it safe.",
          "score": 1,
          "created_utc": "2026-01-31 16:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vumac",
          "author": "Agreeable-Market-692",
          "text": "litellm as a proxy",
          "score": 1,
          "created_utc": "2026-02-01 00:15:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qodv1o",
      "title": "ClawdBot: Setup Guide + How to NOT Get Hacked",
      "subreddit": "LLMDevs",
      "url": "https://lukasniessen.medium.com/clawdbot-setup-guide-how-to-not-get-hacked-63bc951cbd90",
      "author": "trolleid",
      "created_utc": "2026-01-27 13:29:31",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qodv1o/clawdbot_setup_guide_how_to_not_get_hacked/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": []
    }
  ]
}