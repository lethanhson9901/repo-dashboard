{
  "metadata": {
    "last_updated": "2026-02-06 02:55:57",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 107,
    "file_size_bytes": 130783
  },
  "items": [
    {
      "id": "1qvtrw7",
      "title": "If RAG is dead, what will replace it?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qvtrw7/if_rag_is_dead_what_will_replace_it/",
      "author": "Normal_Sun_8169",
      "created_utc": "2026-02-04 16:50:23",
      "score": 104,
      "num_comments": 77,
      "upvote_ratio": 0.83,
      "text": "It seems like everyone who uses RAG eventually gets frustrated with it. You end up with either poor results from semantic search or complex data pipelines.\n\nAlso - searching for knowledge is only part of the problem for agents. I‚Äôve seen some articles and posts on X, Medium, Reddit, etc about agent memory and in a lot of ways it seems like that‚Äôs the natural evolution of RAG. You treat knowledge as a form of semantic memory and one piece of a bigger set of memory requirements.¬†\n\nThere was a paper published from Google late last year about self-evolving agents and another one talking about adaptive agents.\n\nIf you had a good solution to memory, it seems like you could get to the point where these ideas come together and you could use a combination of knowledge, episodic memory, user feedback, etc to make agents actually learn.\n\nSeems like that could be the future for solving agent data. Anyone tried to do this?¬†",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qvtrw7/if_rag_is_dead_what_will_replace_it/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3kg4v0",
          "author": "qa_anaaq",
          "text": "RAG isn‚Äôt dead. It‚Äôs perfectly fine and just needs to be used well. Everyone believes context graphs are the next trillion dollar industry. Context graph management at runtime is another flavor of RAG. \n\nRemember that RAG isn‚Äôt a narrow term. If something is pulled from somewhere to augment generation, it‚Äôs RAG.",
          "score": 152,
          "created_utc": "2026-02-04 17:48:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3luvt1",
              "author": "isthatashark",
              "text": "The challenge with the name \"RAG\" is that so many people use it as a shorthand to describe semantic search over chunked documents in a vector database. I think the days where you can built any sort of meaningful AI application with that approach are behind us.\n\nAs a pattern, retrieving context and using it to augment the LLM's generation is here to stay.",
              "score": 21,
              "created_utc": "2026-02-04 21:44:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3obe76",
                  "author": "FiddlyDink",
                  "text": "What is replacing chunked documents in a vector database for semantic search?",
                  "score": 8,
                  "created_utc": "2026-02-05 06:33:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mxhx9",
              "author": "FoldedKatana",
              "text": "Yeah I'm using graph rag for a client and it works great if the data is static.",
              "score": 4,
              "created_utc": "2026-02-05 01:10:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3owpwq",
                  "author": "3minpc",
                  "text": "Why static? You can't rebuild your graph every x hours?",
                  "score": 1,
                  "created_utc": "2026-02-05 09:53:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3l8dww",
              "author": "valuat",
              "text": "Everyone who?",
              "score": 6,
              "created_utc": "2026-02-04 19:56:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3na7fb",
              "author": "SUCK_MY_DICTIONARY",
              "text": "Oh I love the way this guy fucking thinks YES.\n\nWhat is your opinion on MoE? I want to know",
              "score": 1,
              "created_utc": "2026-02-05 02:23:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3otgqt",
              "author": "_nku",
              "text": "Agree.  I think we haven't reached the necessary maturity in best practice guidance in regards to when which kind and strategy of grounding information injection into the final completion run of an LLM is the right approach.  What mix of forced grounding context injection, dynamic tool calling to get grounding context, sub-agents summarizing from larger bodies to then inject summarized / shortened grounding context etc gives the best bang for the buck?   TBH from production experience, it's still very situation and data specific.  For a fast follower team that does not have the capacity to try out every frontline technology development, an off the shelf (e.g. GCP APIs based) vector store plus reranker plus grounding into a fast model with large context is still going to be a decent outcome IF (!!!) the use case is actually a fit for it and IF (!!) they put a lot of effort in the preparation, custom chunking, extraction, tagging of their content.     \n  \nMy unscientific thesis is that \"standard\" RAG setups are used way to much on a) bad data b) the wrong use case,  not that they are fundamentally bad.    \n  \nAnother thesis: The general approach of rather providing situationally dynamic context vs. relying on foundation model fact knowledge is here to stay until we have models that can be incrementally and continuously trained or tuned at very low cost (and even then, the question is up whether this provides better hallucination control than grounding in the generation context).",
              "score": 1,
              "created_utc": "2026-02-05 09:21:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ks60y",
              "author": "howardhus",
              "text": "rag was never even alive.\n\nRag ist pulling chunks in a half assed vector search and letting some llm hallucinate some coherent sentence from it. The selling point was the LLM faking confidence. Worked just like in the real world..\n\nwas never great in theory but peopel were flashed as they saw some very self confident human readable answer \"here is the perfect answer to your question!\"\n\nthen you correct it: \"yes you are right! i lied! here is the actual correct answer (this time for real!)\"\n\nRAG was only great before the word \"hallucination\" also became a thing.",
              "score": -18,
              "created_utc": "2026-02-04 18:42:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l0y0o",
                  "author": "Agreeable-Market-692",
                  "text": "This is a very outdated view of RAG. Hundreds of papers and dozens of models later and things are much improved.",
                  "score": 12,
                  "created_utc": "2026-02-04 19:22:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lml96",
              "author": "Dense_Gate_5193",
              "text": "yeah RAG is neat but Graph-RAG is where it is at.\n\nit‚Äôs why i built nornic. \n\nhttps://github.com/orneryd/NornicDB\n\n0.17ms p95 transacted writes. \n\nneo4j drop-in replacement that‚Äôs 3-50x faster depending on operation.\n\nit also has a qdrant compatible grpc endpoint and is ~40% faster than qdrant proper\n\ngpu accelerated vector embedding search or cpu IVF-HNSW, tunable. \n\nmanaged vector embeddings mean you don‚Äôt need a remote model to generate embeddings for you. same for reranking. it runs an in-memory model for reranking.",
              "score": -11,
              "created_utc": "2026-02-04 21:05:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kfb2f",
          "author": "coffee-praxis",
          "text": "Agent memory alone doesn‚Äôt cut it. Let‚Äôs say you want grounded facts from a document source that‚Äôs too big for context window. You can‚Äôt just shove it all in ‚Äúagent memory‚Äù unless you retrieve the correct bits of it somehow. Now you‚Äôre back to RAG.",
          "score": 36,
          "created_utc": "2026-02-04 17:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ltfuv",
              "author": "isthatashark",
              "text": "I hear more people talking about this as semantic memory and thinking of it as one requirement in a bigger set of agent memory requirements rather than just RAG.",
              "score": 4,
              "created_utc": "2026-02-04 21:37:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3kkqao",
              "author": "NorCalZen",
              "text": "Sorry if this a naive question, but could you use a database solution like ScyllaDB to achieve the right results ?",
              "score": 1,
              "created_utc": "2026-02-04 18:08:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3klea2",
                  "author": "coffee-praxis",
                  "text": "RAG is ‚Äú**retrieval** augmented generation‚Äù. Any DB qualifies.",
                  "score": 24,
                  "created_utc": "2026-02-04 18:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k9bq3",
          "author": "ethan000024",
          "text": "I‚Äôve been hearing more about agent learning lately too. Agree it‚Äôs a promising idea but also mostly hype when I‚Äôve tried to dig into it. The two most interesting projects I‚Äôve seen on this lately are Agent Lightning and Hindsight. Two very different approaches, Agent Lightning relies more on file system. Hindsight is closer to what you described with combining knowledge, episodic memory, etc. Both have learning aspects to it.¬†",
          "score": 12,
          "created_utc": "2026-02-04 17:16:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kgx9d",
              "author": "Normal_Sun_8169",
              "text": "I just looked those projects up. Very cool stuff. The learning demo they have on the GitHub repo for Hindsight is exactly what I was trying to describe. Reinforcement learning over agent memory to form mental models seems super powerful. Thanks for the info!",
              "score": 3,
              "created_utc": "2026-02-04 17:51:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kcja7",
          "author": "florinandrei",
          "text": "> If RAG is dead, what will replace it?\n\nTATTER\n\nTransformer-Attention Token Tangling for Eventually Rambling",
          "score": 25,
          "created_utc": "2026-02-04 17:31:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kielt",
              "author": "Floppy_Muppet",
              "text": "I believe \"token tangling\" is illegal in several states.",
              "score": 22,
              "created_utc": "2026-02-04 17:58:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k912z",
          "author": "Emma_4_7",
          "text": "The most annoying thing about agent memory right now is how many ‚Äúmemory‚Äù projects on GitHub are basic RAG solutions under the covers. That‚Äôs nice you can remember where I work after 10 whole messages.",
          "score": 18,
          "created_utc": "2026-02-04 17:15:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kgims",
              "author": "Normal_Sun_8169",
              "text": "Yeah, I‚Äôve noticed this too.",
              "score": 2,
              "created_utc": "2026-02-04 17:49:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3kig2c",
              "author": "Original_Finding2212",
              "text": "What do you think about this?\n\nQq folder here:\n\n[https://github.com/OriNachum/autonomous-intelligence](https://github.com/OriNachum/autonomous-intelligence)\n\nAnd add a star if you like or want to support üôèüèø\n\nhttps://preview.redd.it/r8euxdeboihg1.jpeg?width=2752&format=pjpg&auto=webp&s=050a9da330c4b9c4c558d792e243f8703b05dbfe",
              "score": -15,
              "created_utc": "2026-02-04 17:58:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kvyj2",
                  "author": "leonjetski",
                  "text": "‚ÄúMapping sturucted outitites and complex relationships between a√¶√∞capta.‚Äù",
                  "score": 13,
                  "created_utc": "2026-02-04 18:59:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3tghgy",
                  "author": "cmndr_spanky",
                  "text": "That diagram is a pile of nonsense. It might be time to start thinking for yourself‚Ä¶ friend. Did you even read it ?",
                  "score": 1,
                  "created_utc": "2026-02-06 00:40:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mv41a",
          "author": "jba1224a",
          "text": "‚ÄúLet me just shove this shit into a vector database.  We don‚Äôt need to worry about chunking.  What‚Äôs an embedding model?‚Äù\n\n‚Ä¶.\n\n‚ÄúWhy do my results suck.  RAG is frustrating‚Äù",
          "score": 7,
          "created_utc": "2026-02-05 00:56:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ozewa",
              "author": "CSEliot",
              "text": "RAG tools don't run any embedding by default???",
              "score": 1,
              "created_utc": "2026-02-05 10:18:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3pjuup",
                  "author": "vogut",
                  "text": "?",
                  "score": 1,
                  "created_utc": "2026-02-05 12:59:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3q2542",
                  "author": "jba1224a",
                  "text": "Are you asking?\n\nRag isn‚Äôt only vector search but in the context of this discussion this is why it fails for people.\n\nThey equate it purely to vector search and then put zero planning or thought into how to curate their vector database.\n\nIt‚Äôs akin to baking a cake by just dumping all the ingredients into a pan with no measuring.  You may get something vaguely cake-like‚Ä¶but you shouldn‚Äôt be pissed it didn‚Äôt come out the way you wanted.",
                  "score": 1,
                  "created_utc": "2026-02-05 14:42:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mtaif",
          "author": "Ok-Owl-7515",
          "text": "I don‚Äôt think RAG is dead. Vector-only semantic search is what usually disappoints. What‚Äôs replacing it (for me) is hybrid retrieval + memory architecture: FTS/keyword first, then vectors only as fallback, union + rerank, and always return retrieval diagnostics (which backend, hit counts, scores, latency).\n\nThe biggest unlock is in considering embeddings/indexes as versioned, reproducible derived artifacts (model/version + source hash), and controlling changes via a small golden set to prevent silent changes to results. Retrieval is just one ‚Äúmemory surface,‚Äù alongside structured state/ledgers and episodic logs.",
          "score": 3,
          "created_utc": "2026-02-05 00:46:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3syrh2",
              "author": "danigoncalves",
              "text": "What do you use for FTS? do you have your own implementation or use something like Apache Solr or similar that abstracts you from some of data ingestion processes? And why you use vector only as fallback and do not join both FTS/keyword with sematic search, merge and re-rank both to choose the best context to feed the models? ",
              "score": 2,
              "created_utc": "2026-02-05 23:00:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3t1fwk",
                  "author": "Ok-Owl-7515",
                  "text": "Good questions ‚Äì just a quick clarification on my wording. I‚Äôm currently using SQLite FTS5 (embedded) instead of Solr or Elasticsearch. It keeps retrieval portable, deterministic, and easy to debug with stable chunk/card IDs, source text hashes, and reproducible index builds.\n\nFor vectors, when I say ‚Äúfallback,‚Äù I mean I don‚Äôt always run semantic search. (a) It can add noise for queries that are heavy on entities, where lexical search performs better; and (b) it increases complexity and cost if used on every query. But when semantic does kick in, say, too few FTS hits or low lexical confidence,  I follow the exact flow you described: run vector search - merge results - rerank - return top-K. I also log diagnostics like backend used, hit counts, scores, and latency.\n\nThat said, I haven‚Äôt rolled out embeddings-based retrieval in production yet. The current setup is FTS-first, paired with structured state and ledgers. The hybrid approach is next on the roadmap, once I can safely gate it behind a ‚Äúsemantic miss‚Äù golden set to avoid silent drift.\n\nCurious, what‚Äôs worked best for you in terms of rerankers or thresholding?",
                  "score": 1,
                  "created_utc": "2026-02-05 23:14:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3kuy2i",
          "author": "metaphorm",
          "text": "my view is that RAG is still a highly relevant technique and the problems it has with accuracy are the current leading edge of LLM application development. agent memory might be a good approach for some classes of problems. \"deep\" agents might be another approach that works, i.e. an agent that has access to tools that allow it to introspect its own results. ",
          "score": 4,
          "created_utc": "2026-02-04 18:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l78ss",
              "author": "techhead57",
              "text": "Its a tool in the toolbox. When LLMs came out rag was the only tool. Now there are all kinds of interfaces being hooked up to them and RAG has all kinds of fancy alternatives that are basically trying to do the same thing but better. And models are getting better at using this kind of input context because theyre being trained with tools use now.",
              "score": 5,
              "created_utc": "2026-02-04 19:51:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k3xuy",
          "author": "Fragrant_Western4730",
          "text": "I don‚Äôt know about the rest of it, but I definitely experienced the shortcomings of RAG for searching documents. Cool thought. Interested to hear what people think about this. Upvoted.",
          "score": 2,
          "created_utc": "2026-02-04 16:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kg1c3",
              "author": "Normal_Sun_8169",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-04 17:47:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kerpl",
          "author": "onetimeiateaburrito",
          "text": "I dunno man. I've spent a little bit trying to get a [RAPTOR](https://arxiv.org/abs/2401.18059) style system going and maybe it'll be cool? Who knows. I'm not a programmer and have no background in CS or ML. Just arguing with myself and Claude until something does something without spitting error codes. Then doing the same thing to see what's silently failing.",
          "score": 2,
          "created_utc": "2026-02-04 17:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l5qqf",
          "author": "WolfeheartGames",
          "text": "The problem is retrieval. How is the agent supposed to know what I'd available for lookup? It must be told.\n\nLet's say we have a list of things the agent can retrieve. If we give it to the agent it will hyper fixate on this and it causes new failure modes.\n\nSo then we need to monitor the inputs and outputs and see if we should be injecting information from retrieval in to the context window. This requires a signal of some kind. Either LLM, BERT, or otherwise.",
          "score": 2,
          "created_utc": "2026-02-04 19:44:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mgzqd",
              "author": "ai-tacocat-ia",
              "text": "It's really just a taxonomy problem. Is easy to think of it like a file system. \"Tell me what folders are in the current directory. I want to see the files and subfolders in this list of directories. Now show me what's in these subdirs.\"\n\nAlso, \"show me the paths of files whose contents contain these search terms\". Then let the LLM list the files it wants to pull.\n\nObviously doesn't need to be files - can be categories, subcategories, filter by tags, etc. Basically, give LLMs the same tools you enjoy as a human to find things.",
              "score": 1,
              "created_utc": "2026-02-04 23:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mnmdb",
                  "author": "WolfeheartGames",
                  "text": "That is not how real deployments usually work. It's okay for like a call center bot where the company will invest a lot in the docs for a RAG, but even then it's not enough. How does it know that a question is even contained in its RAG? How does it know how to search for it if the user gives terrible keywords, how does it know if should look elsewhere? It's not a listable directory to explore to gain insight from, and that's the problem. The agent only knows whats in it's system prompt until it's found something, and then it's still ignorant about potentially other useful things it didn't find. This breaks down further when data is less organized, like code or loose pdfs\n\nBut the fact that you're comparing RAG lookup to a directory is concerning. Vector and graph databases do not work like that at all. The problem of retrieval is partially because they don't work like that.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3liewe",
          "author": "DataCentricExpert",
          "text": "RAG isn‚Äôt dead, it‚Äôs just being asked to do too much.  \ngents break when you expect retrieval to behave like memory. What replaces it isn‚Äôt ‚Äúbetter RAG,‚Äù it‚Äôs layered memory...AG becomes infrastructure, not the strategy.",
          "score": 2,
          "created_utc": "2026-02-04 20:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lmpd1",
          "author": "xFloaty",
          "text": "every time your agent calls a tool to search for context, it‚Äôs RAG",
          "score": 2,
          "created_utc": "2026-02-04 21:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nbxi9",
          "author": "hettuklaeddi",
          "text": "dead?!? RAG doesn‚Äôt even have the sniffles \n\nmaybe it‚Äôs dead to script kiddies, that‚Äôs fine",
          "score": 2,
          "created_utc": "2026-02-05 02:33:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kz4oq",
          "author": "fabkosta",
          "text": "Downvoted. We had enough \"RAG is dead\" posts here. It's getting silly.",
          "score": 4,
          "created_utc": "2026-02-04 19:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l6m4k",
          "author": "AdOwn10",
          "text": "Ya the RAG people changed what ‚ÄúRAG‚Äù means so RAG isn‚Äôt dead. Vector database? No! We are not talking about ALL ways you get retrieve information to augment a context window.",
          "score": 2,
          "created_utc": "2026-02-04 19:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mc77z",
          "author": "andrew_kirfman",
          "text": "Rag isn‚Äôt 100% dead, but it‚Äôs definitely been impacted by agentic search and agent skills getting so good.  \n\nI only use semantic search for dart at a dartboard type searches.  Everything else is agentic search.",
          "score": 2,
          "created_utc": "2026-02-04 23:12:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3obzoo",
              "author": "Visionexe",
              "text": "What is Agentic search?",
              "score": 1,
              "created_utc": "2026-02-05 06:38:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3klpky",
          "author": "vagobond45",
          "text": "Knowledge Graphs combined with Answer Rag Audit should replace RAG",
          "score": 1,
          "created_utc": "2026-02-04 18:13:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l5cw1",
          "author": "Miclivs",
          "text": "Agentic search works really well when the agent knows what to look for.",
          "score": 1,
          "created_utc": "2026-02-04 19:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9lk3",
          "author": "llOriginalityLack367",
          "text": "Mean pooling.\n\nMean pooling.\n\nMean pooling.",
          "score": 1,
          "created_utc": "2026-02-04 20:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mc5ln",
          "author": "Flat_Dependent3195",
          "text": "Can you share the link for the paper you mentioned?",
          "score": 1,
          "created_utc": "2026-02-04 23:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n8d42",
          "author": "New-Unit-3900",
          "text": "Properly structured ontologies",
          "score": 1,
          "created_utc": "2026-02-05 02:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nrac8",
              "author": "smm_h",
              "text": "like what",
              "score": 1,
              "created_utc": "2026-02-05 04:05:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nhjmt",
          "author": "GoodEnoughSetup",
          "text": " In my experience, database solutions like ScyllaDB can definitely be part of a broader strategy to replace RAG. By incorporating a database for fast access to relevant data, you might enhance the context in which generative models operate, similar to how semantic memory aims to streamline information retrieval. Have you looked into any specific frameworks that could mesh well with that approach?",
          "score": 1,
          "created_utc": "2026-02-05 03:04:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o8gid",
          "author": "fooz42",
          "text": "It's a garbage in, garbage out problem. You can reduce the surface area of the generation to something very small in scope, or you can increase the quality of the included information in the context to improve the summary.",
          "score": 1,
          "created_utc": "2026-02-05 06:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3objra",
          "author": "iAM_A_NiceGuy",
          "text": "Compression",
          "score": 1,
          "created_utc": "2026-02-05 06:34:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3p6ujg",
          "author": "sje397",
          "text": "I've got RAG, 'sticky' memories scoped as global or conversation specific, and 'notes' as a tool. Each suits different use cases. Seems to work pretty well in combination for my 'assistant'.",
          "score": 1,
          "created_utc": "2026-02-05 11:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q8flh",
          "author": "airylizard",
          "text": "‚ÄúRAG‚Äù is semantic search. You ‚ÄúAI people‚Äù have been inventing new terms to describe basic automation tools and practices for years",
          "score": 1,
          "created_utc": "2026-02-05 15:15:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3q8lgv",
          "author": "exids",
          "text": "RAG is awesome, not dead and is still in its infancy as agent models improve. Who says it's dead?!?!?",
          "score": 1,
          "created_utc": "2026-02-05 15:15:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tdhbc",
          "author": "Former-Ad-5757",
          "text": "Stupid click-once RAGging (in the meaning of simple semantic searching) is dead but to me it has never really existed.\n\nIf you setup a default vector db with chunking of 200, and you feed it documents of on average 600, what do you really suspect will happen? At best it will feed half-truncated garbage to the llm.\n\n  \nIn all RAG setups I have setup the absolute minimal chunking was 64kb, because I don't believe chunking is a fixed number, it is completely dependent on if the chunk completely describes the info, you can define info as a sentence, or a paragraph (or for coding for example a method) but I have almost never encountered a situation where all the meaning was captured in 200. Just use overlaps is what some tuts say, well great now you add more half-meanings which pollute your retrieval results more.",
          "score": 1,
          "created_utc": "2026-02-06 00:22:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3th44g",
          "author": "cmndr_spanky",
          "text": "Oh look. It‚Äôs the daily ‚Äúrag is dead‚Äù bot post.  Oh look here‚Äôs a fancy memory solution for agents (still an adaptation of rag). \n\nWould you mind thinking more deeply (or maybe search Reddit for 15secs) before vomiting out the next hapless low effort contribution to the cesspool of AI subreddits ? K thanks",
          "score": 1,
          "created_utc": "2026-02-06 00:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ttwqq",
          "author": "Academic_Track_2765",
          "text": "It‚Äôs dead, it dies everyday according to some guru. There are so many flavors of rag but somehow it‚Äôs still dead lol.",
          "score": 1,
          "created_utc": "2026-02-06 01:59:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l4jqo",
          "author": "Able_Penalty8856",
          "text": "I also got frustrated with RAG. My plan is to study Unsloth to explore fine-tuned models. I'm aware that I'll likely face several challenges.",
          "score": 0,
          "created_utc": "2026-02-04 19:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oeyq1",
              "author": "Pixelmixer",
              "text": "This simply isn‚Äôt possible for a lot of workflows. As a super simple toy example; imagine you want to search text comments posted by users and provide that to an LLM. Fine-tuning could potentially work as a first pass (let‚Äôs also assume that the fine-tuned model has perfect retrieval for the purpose of this example), but even then you‚Äôd need to retrain it each time a user posts a new comment or changes their comment. It‚Äôs just too much, unfortunately.",
              "score": 1,
              "created_utc": "2026-02-05 07:04:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsw6tg",
      "title": "We are not the same",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/glett1s5dvgg1.png",
      "author": "alvinunreal",
      "created_utc": "2026-02-01 11:35:03",
      "score": 40,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qsw6tg/we_are_not_the_same/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2yj0zt",
          "author": "brightheaded",
          "text": "A crumb of context sir",
          "score": 9,
          "created_utc": "2026-02-01 12:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yo43w",
              "author": "HumanDrone8721",
              "text": "Or a crust of context.",
              "score": 4,
              "created_utc": "2026-02-01 12:54:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yo5kf",
              "author": "Mythril_Zombie",
              "text": "Apparently something is different from something else.",
              "score": 2,
              "created_utc": "2026-02-01 12:55:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o30ca32",
          "author": "megatronus8010",
          "text": "How does the craber news work",
          "score": 1,
          "created_utc": "2026-02-01 18:02:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30f83c",
              "author": "alvinunreal",
              "text": "It's in sync with HackerNews submission - when someone submits there, it appears on crabbernews.\n\nBut difference is on HackerNews humans upvote, comment and discuss; on Crabbernews it's upto AI models.\n\nThis makes \"top\" posts different; For example are humans biased towards certain types of news or now...\n\nThat's the goal of this website, I'm currently adding core models to review new posts, and decide which to upvote but anyone can connect their openclaw agents too to participate.",
              "score": 3,
              "created_utc": "2026-02-01 18:15:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o353kdv",
                  "author": "germo20",
                  "text": "I think many of us would appreciate having this comment in the actual post, instead of just a comment.",
                  "score": 3,
                  "created_utc": "2026-02-02 11:57:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30nkzk",
          "author": "Ladder-Bhe",
          "text": "AGENTSÔºåPlease include historical messages when speaking, so we can see how this idea was generated, rather than the result of human intervention.",
          "score": 1,
          "created_utc": "2026-02-01 18:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qul18d",
      "title": "NotebookLM For Teams",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/b85n399898hg1",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:00",
      "score": 39,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qul18d/notebooklm_for_teams/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3axdaf",
          "author": "Otherwise_Wave9374",
          "text": "This looks like a great niche community. The \"agents that act, not just chat\" framing is exactly the right line to draw.\n\nIf you end up pinning a starter guide, I would include basics like tool permissioning, memory vs no-memory defaults, and how to keep agent loops bounded so people do not burn tokens endlessly.\n\nI have a few writeups on agent guardrails and patterns here: https://www.agentixlabs.com/blog/",
          "score": 3,
          "created_utc": "2026-02-03 07:07:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jnqad",
          "author": "tsquig",
          "text": "Similar flexibility here with uploading / source inputs, but a few more options to generate outputs. [NotebookLM but for business](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gwyij",
          "author": "DAlmighty",
          "text": "They have also been doing crazy advertising on Reddit. You‚Äôd think that they would just pay the platform for the opportunity already.",
          "score": 0,
          "created_utc": "2026-02-04 03:45:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrqdds",
      "title": "[P] Trained a 67M-parameter transformer from scratch on M4 Mac Mini - 94% exact-match accuracy on CLI command generation",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "author": "Great_Fun7005",
      "created_utc": "2026-01-31 02:47:54",
      "score": 33,
      "num_comments": 9,
      "upvote_ratio": 0.98,
      "text": "I trained a small language model end-to-end on consumer hardware (M4 Mac Mini, 24GB RAM) and achieved 94% exact-match accuracy on CLI command generation.\n\n**Key details:**\n\n* Model: 67M parameters (12 layers, 512 hidden dim, RoPE, RMSNorm, SwiGLU)\n* Training: 204.8M tokens, \\~13 hours pretraining + 4 minutes fine-tuning\n* Hardware: Apple Silicon MPS, no discrete GPU\n* Cost: \\~$0.50 in electricity\n* Evaluation: Strict exact-match (no partial credit)\n\n**What worked:**\n\n* Modern architectural components (RoPE, RMSNorm, SwiGLU) are effective even at small scale\n* Marker-based output contracts for state signaling\n* Memory-mapped data loading to handle 200M+ tokens on limited RAM\n* Continual learning with evaluation gates that reject harmful updates\n\n**What failed (and why it matters):** All 6% of failures shared one pattern: early termination on symbol-dense patterns (regex, pipes, redirects). Not a reasoning failure‚Äîa data coverage problem. Adding \\~500 targeted examples would likely fix most of these.\n\n**Takeaway:** For narrow, exact tasks with controllable domains, small models trained from scratch can be practical, inspectable, and cheap to iterate on. Data quality mattered more than scale.\n\nFull technical writeup with training logs, failure analysis, and code: [https://geddydukes.com/blog/tiny-llm](https://geddydukes.com/blog/tiny-llm)\n\nGitHub: [https://github.com/geddydukes/tiny\\_llm](https://github.com/geddydukes/tiny_llm)\n\nHappy to answer questions about training dynamics, architecture choices, or the evaluation setup.",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2sfg8j",
          "author": "radarsat1",
          "text": "The implementation is fairly clean, good job. I have a question though, this seems to be an unusual TransformerBlock forward function, did you get this from somewhere or is it a mistake or maybe your own idea?\n\n```\n¬† ¬† ¬† ¬† h1 = self.norm1(x)\n¬† ¬† ¬† ¬† h2 = self.norm2(x)\n\n¬† ¬† ¬† ¬† attn_out = self.attn(h1, attn_mask, rope_cos, rope_sin)\n¬† ¬† ¬† ¬† mlp_out = self.mlp(h2)\n¬† ¬† ¬† ¬†¬†\n¬† ¬† ¬† ¬† return x + self.dropout(attn_out) + self.dropout(mlp_out)\n```\n\nI'm referring to how it adds `attn_out` and `mlp_out` instead of feeding `attn_out` into `mlp`.",
          "score": 5,
          "created_utc": "2026-01-31 13:56:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2th04w",
              "author": "Great_Fun7005",
              "text": "Thanks, appreciate it. This is an intentional pre-norm parallel residual block: x + attn(norm(x)) + mlp(norm(x)). Attention and MLP run in parallel off the same residual stream (with separate RMSNorm) and are summed in a single update. It‚Äôs a known Transformer variant used in several modern decoder-only models, not a mistake or a novel invention.",
              "score": 2,
              "created_utc": "2026-01-31 17:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uinbj",
                  "author": "radarsat1",
                  "text": "Do you know offhand which variants use this? I actually checked the Llama code before posting just in case I was saying something dumb, but it seems to work there as I am used to. I guess I can plumb the transformers library a bit to find out but I'm curious about it, if you happen to know.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2q4kfm",
          "author": "Dense_Gate_5193",
          "text": "thanks i am training SLMs for work and this is helpful",
          "score": 2,
          "created_utc": "2026-01-31 02:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4qfl",
              "author": "Great_Fun7005",
              "text": "Glad to provide a helpful resource!",
              "score": 1,
              "created_utc": "2026-01-31 02:57:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2u5cre",
          "author": "HealthyCommunicat",
          "text": "Woah, I was literally talking about how bad some models are with just basic commands, like hooking up glm 4.7 flash to codex cli and ask it to find a file‚Ä¶ watch it mess up the ‚Äúfind . -name ‚Äú___‚Äù‚Äù bash syntax 7 times before getting it right, or even editing a file i usually watch it struggle going through multiple different attempt methods until it just finally ends up on echoing it into the file lol\n\nThis is actually really cool, if someone was to take ur base and add upon it i‚Äôd totally use it",
          "score": 1,
          "created_utc": "2026-01-31 19:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u8met",
              "author": "Great_Fun7005",
              "text": "Feel free to add onto it! I have some future iterations planned but have a couple of projects I‚Äôm working on before I‚Äôll get back to this one.",
              "score": 1,
              "created_utc": "2026-01-31 19:19:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quxgq9",
      "title": "8 Ways OpenClaw Reduces Context Loss in Long-Running Agents",
      "subreddit": "LLMDevs",
      "url": "https://codepointer.substack.com/p/openclaw-stop-losing-context-8-techniques",
      "author": "noninertialframe96",
      "created_utc": "2026-02-03 17:01:02",
      "score": 17,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1quxgq9/8_ways_openclaw_reduces_context_loss_in/",
      "domain": "codepointer.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3hsmig",
          "author": "gtek_engineer66",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-02-04 07:45:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr71pv",
      "title": "Who still use LLMs in browser and copy paste those code in editior instead of using Code Agent?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "author": "monskull_",
      "created_utc": "2026-01-30 14:32:46",
      "score": 14,
      "num_comments": 34,
      "upvote_ratio": 0.77,
      "text": "I‚Äôm always excited to try new AI agents, but when the work gets serious, I usually go back to using LLMs in the browser, inline edits, or autocomplete. Agents‚Äîespecially the Gemini CLI‚Äîtend to mess things up and leave no trace of what they actually changed.\n\nThe ones that insist on 'planning' first, like Kiro or Antigravity, eventually over-code so much that I spend another hour just reverting their mistakes. I only want agents for specific, local scripts‚Äîlike a Python tool for ActivityWatch that updates my calendar every hour or pings me if I‚Äôm wasting time on YouTube.\n\nI want to know is there something i am missing? like better way to code with agents?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2ly9nd",
          "author": "-rnr",
          "text": "wish I could help, agents always feel like pair programming with someone who won‚Äôt stop refactoring.",
          "score": 12,
          "created_utc": "2026-01-30 14:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3l9k",
          "author": "Zeikos",
          "text": "Me.  \nI use LLMs as a fancy google.  \nI like my IDE as uncluttered ad possible.",
          "score": 10,
          "created_utc": "2026-01-30 15:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ma6qd",
          "author": "dsartori",
          "text": "I enjoy using Cline in VSCode, but I rarely give it permission to write directly to files. This is an evolution of my approach based on my own setup and experience. Jesus take the wheel coding is usually the illusion of productivity more than the reality if you have any standards for your codebase.",
          "score": 5,
          "created_utc": "2026-01-30 15:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mf2kf",
          "author": "mdizak",
          "text": "\n\nI'm blind, so don't use any of the IDEs as none of them are accessible via screen reader.\n\n\n\nOnly time I really use code LLMs give me in the browser is using Gemini to correct typos basically.  Just in the last few months these LLMs have finally gotten good enough so I can bang out say a 300 line Rust struct, then just pass it to Gemini to fix all the syntax and braces / brackets errors, and have it actually work.  That's been really nice, and a huge time saver.\n\n\n\nOther than that, I don't ever use code from LLMs as I find it slopppy, overly verbose, and poor design choices.  That's expected, as these are just predictive machines trained on the entirety of the internet, so by design, you're going to get the most average, middle of the road code out there.\n\n\n\nI do however use Claude Code asisstant here and there.  If I just need something done for a data processing or training pipeline of some kind, and other things that won't be going into production, then I'll sometimes use that.   Although think I'll start steering away from that, because as per usual, when I begin leaning on these things more I end up realizing their screw ups ultimately cost me more time and stress than any initial development time savings I get.",
          "score": 5,
          "created_utc": "2026-01-30 15:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ndlhq",
              "author": "monskull_",
              "text": "How blind ppl code? It's very impressive too.",
              "score": 3,
              "created_utc": "2026-01-30 18:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2m4zkx",
          "author": "Adept_Carpet",
          "text": "This is making me feel better because I feel like I should be using agents but the chat works better.\n\n\nThe agents work alright if everything is set up in a certain way, but all the projects I work on have stuff that kills agents like unused old versions of code sitting in directories or documentation from other projects that is similar enough to be reused if you are a human but makes agents very confused.\n\n\nThat stuff shouldn't be there, but it is and I can't get rid of it solely to enable more vibe coding.",
          "score": 3,
          "created_utc": "2026-01-30 15:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m9c3v",
              "author": "monskull_",
              "text": "I thought i am missing something.",
              "score": 2,
              "created_utc": "2026-01-30 15:30:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nbosm",
          "author": "UncleRedz",
          "text": "I find that I keep moving back and forth between web chat copy/paste and Qwen Code / Copilot Agent Mode etc. What's often missing in the discussions here is what languages and tech stack you are using and the state of the code base. Let me provide an example, I tried to use agents to build a Flutter app from scratch and it was a horrible waste of time. After a week of evenings I gave up and used Gemini Pro on the web instead, that worked great and was many times faster.\n\nWhen you know how to write the code, but see that the AI generates about 80-90% correct code, it's at least for me,, faster to just do copy paste, fix the last 10-20%, than it is to watch the AI correct it self like a trainwreck, retry, test, compile, try to fix something completely unrelated, still fail and announce that it successfully completed the task, remind it and have it try again, until it succeeded but at the same time refactored a bunch of unrelated stuff and left piles of unused code from trial and errors.\n\nFunny thing, once my Flutter app was practically feature complete, I tried Qwen Code again, and this time it actually worked perfectly, my guess is that there were enough code there for it to understand what needed to be done and how to fit it in, as opposed to an empty or nearly empty code base.\n\nFor other projects, I've had better success, HTML/JavaScript/CSS seems to work rather well, and creating boilerplate code in C# also works well, when the right classes are in the context for the AI to know how things should be implemented.\n\nMy point is that how successful you are seems to depend on multiple factors, such as how well the model handles your specific tech stack, and even down to specific versions of libraries, in addition to how clean, large or small your codebase is. And I also suspect that people have different tolerances for watching the AI trainwreck trying to repair it's mistakes, versus doing it them self, and what quality of code they accept. \n\nAt the end of the day, most important, is that every developer needs to own the code that the AI generates, when main branch breaks, or bugs reach customers, it's not AI's fault, it's the developer who needs to own it and fix it.",
          "score": 3,
          "created_utc": "2026-01-30 18:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o52jp",
          "author": "radarsat1",
          "text": "I would like to jump back into using agents but gave up for now since Google cut off free Gemini API usage. Somehow the browser interface I can basically spam the model as much as I want, so I use that.\n\nSo I end up architecting things so that most changes can be done on a single file instead of requiring little changes in many places, leads to good structure anyway.\n\nI do kind of miss working with agents though, but it can get wild. I agree with people here that there is something more careful and controlled when forced into using copy-paste, but it is also annoying. I feel like there must be some yet to be discovered interface that is a happy medium.",
          "score": 3,
          "created_utc": "2026-01-30 20:34:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lzh8t",
          "author": "IONaut",
          "text": "Yeah I don't use agents. I have a couple VS code plugins that can be agentic but I generally don't let them edit code at all. I just have them for the convenience of having an interface in the sidebar. Usually I'll just have them refactor something or provide autocomplete or write me a boiler plate function that is short enough that I can vet it before running it. I also don't use online APIs and run everything locally with LM Studio.",
          "score": 2,
          "created_utc": "2026-01-30 14:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m22ol",
              "author": "monskull_",
              "text": "Same.",
              "score": 2,
              "created_utc": "2026-01-30 14:56:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lzq9t",
          "author": "SerDetestable",
          "text": "vscode with copilot and opus",
          "score": 2,
          "created_utc": "2026-01-30 14:44:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m2etv",
              "author": "monskull_",
              "text": "what is opus?",
              "score": 2,
              "created_utc": "2026-01-30 14:57:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2m2l6g",
                  "author": "SerDetestable",
                  "text": "Opus 4.5, anthropic reasoning flagship model.",
                  "score": 3,
                  "created_utc": "2026-01-30 14:58:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2oxvxl",
                  "author": "drinksbeerdaily",
                  "text": "Yeah, you need to try Opus 4.5 in Claude Code or Opencode.",
                  "score": 1,
                  "created_utc": "2026-01-30 22:54:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m6ibj",
          "author": "knownboyofno",
          "text": "Yea, that's a problem. I normally have to say something like be DRY and SOILD while following the current repo conventions. It changes a bit based on the model I am using. I always make a plan then make sure it looks good before i have it do the work. I normally have two git worktrees open to work on two features at a time.",
          "score": 2,
          "created_utc": "2026-01-30 15:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2manee",
          "author": "dondie8448",
          "text": "Cant trust the agents with my code! They messed up my work a couple of times. Never again. Rather do it myself than let them screw up.",
          "score": 2,
          "created_utc": "2026-01-30 15:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mc9d4",
          "author": "gman55075",
          "text": "I always feel like a code agent is asking for trouble, tho that may be because I'm not good enough at debugging and I use a conversational prompting style to plan, then fine down to pseudo, then code.  I actually ended up building my own browser-style desktop API wrapper that can receive code output as artifacts, let me review/ edit them, then copy and paste into my IDE.  Maybe not ideal for someone with better skills but for me it works.",
          "score": 2,
          "created_utc": "2026-01-30 15:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2me1on",
          "author": "darkwingdankest",
          "text": "rookies",
          "score": 2,
          "created_utc": "2026-01-30 15:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mewme",
          "author": "typeryu",
          "text": "I have been using Codex with 5.2 high and it is very good. Unlike Claude Code which kind of does its own thing sometimes, I can steer pretty well to a point where I am generally explaining in natural language what I want and it takes care of the syntax which means I still know the codebase as if I did it myself. I imagine this is what it feels like to drive those quadrupedal robots which you give it inputs like playing video games and the AI figures out where to place the feet.",
          "score": 2,
          "created_utc": "2026-01-30 15:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mod82",
          "author": "Awkward-Customer",
          "text": ">and leave no trace of what they actually changed  \n...\n\n>I spend another hour just reverting their mistakes  \n...  \nI want to know is there something i am missing? like better way to code with agents?\n\n  \nAre you using revision control with your code bases? The copy/pasting from the web is a huge waste of time, try roocode or cline in visual studio, make sure your code is all revision controlled, review the changes and commit the code frequently. Or use claude code directly if you don't want to use the IDE plugins.\n\nIt should be very clear what these tools are doing if you're using git to manage your projects.",
          "score": 2,
          "created_utc": "2026-01-30 16:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ne1kv",
              "author": "monskull_",
              "text": "I usually use git create new branch let them do what they want if did't work delete the branch. This only happen with Gemini-CLI you can't even find old code in vs code timeline",
              "score": 1,
              "created_utc": "2026-01-30 18:31:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2nfqwm",
                  "author": "Awkward-Customer",
                  "text": "Ok, but if you're using git how do you have no trace of what they actually changed? I'm just trying to understand the problem you're running into here because I find having the tools work with the code directly at least an order of magnitude faster and copy/pasting from the web interface.",
                  "score": 2,
                  "created_utc": "2026-01-30 18:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mou7c",
          "author": "AurumDaemonHD",
          "text": "Exactly the ai agent frameworks are token munchers that need handholding and fall apart if u look at them wrong.\n\nYesterday i posted on [locallama](https://www.reddit.com/r/LocalLLaMA/s/lR5inneFwF) that i built a fish shell script to manage context maybe you could try that.",
          "score": 2,
          "created_utc": "2026-01-30 16:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2os9ef",
          "author": "No_Afternoon_4260",
          "text": "My boss",
          "score": 2,
          "created_utc": "2026-01-30 22:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ow2uh",
          "author": "esaule",
          "text": "use git my friend!",
          "score": 2,
          "created_utc": "2026-01-30 22:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3qmk",
          "author": "BidWestern1056",
          "text": "incognide and npcsh\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 2,
          "created_utc": "2026-01-30 23:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p83fc",
          "author": "TheCientista",
          "text": "I architect with ChatGPT chatting on the browser. Get it to make a tightly controlled instruction block for Claude code and paste that in. CC does the work and produces a summary. ChatGPT checks it. I pay two subs. It‚Äôs been great so far and only getting better. CC needs controlling so chat holds it to account. I am always in the loop. I clean as I go along.",
          "score": 2,
          "created_utc": "2026-01-30 23:49:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yeieo",
          "author": "bakawolf123",
          "text": "I can agree that CLI ones were indeed way too blackbox for me to be comfortable enough for letting them  do anything beyond a basic bootstrap.  \nWith IDE tools it's a bit better, mainly because you can clearly see the process as it unfolds and assess if it's doing what you want it or not and react immediately.\n\nSo what you could do to utilise them more efficiently:\n\nAsk model to analyse the repo and build rules/guardrails (i.e. an Agents md or similar) first.  \nAdjust it manually.  \nSkim through the thinking blocks as it is going on and hit stop if it goes sideways, then prompt a follow up.   \n  \nSometimes it would ignore your rules, sometimes it will hallucinate an API, sometimes it would ignore your utilities - gotta babysit, but it will still be better than copy pasting from browser.  \n  \nAs for planning stages - those are mostly a bummer for me as the plan is either not fully editable, or changes to it are largely ignored somehow, which kinda kills the purpose. They want you to keep refining it by using model again but that's just waste of tokens.",
          "score": 2,
          "created_utc": "2026-02-01 11:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yh11v",
              "author": "monskull_",
              "text": "Okay i will try this in next project",
              "score": 1,
              "created_utc": "2026-02-01 12:00:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30m3o3",
          "author": "2_minutes_hate",
          "text": "I will use a vscode extension here and there, but most of what I do with LLMs happens there exclusively as a chat (functionally I use it the same as a browser tab), or in a browser.",
          "score": 2,
          "created_utc": "2026-02-01 18:45:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3mft",
          "author": "No_Knee3385",
          "text": "When the IDE agent is rate limited",
          "score": 1,
          "created_utc": "2026-01-30 23:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2prszh",
          "author": "Johnlee01223",
          "text": "Same here.\n\nLLM, the quality of output correlates to the quality of input but in a larger codebase, these agents tend to add tons of unrelated stuffs to the context which ends up degrading the quality of the output unless everything is set up and documented in certain way.",
          "score": 1,
          "created_utc": "2026-01-31 01:40:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qviqlh",
      "title": "How to become an AI Engineer in 2026 - what actually matters now?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qviqlh/how_to_become_an_ai_engineer_in_2026_what/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-04 08:05:16",
      "score": 11,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Trying to map out a realistic path into AI engineering and getting overwhelmed by contradictory advice.\n\nPython is still non-negotiable, but the \"just build a chatbot\" project approach doesn't cut it anymore. The market looks brutal for entry-level while senior roles are paying crazy money. Prompt engineering as a dedicated job seems dead, but the skill still matters. RAG, agentic AI, and MLOps seem to be where the growth is.\n\nThe part confusing me is traditional ML (sklearn, training models) vs pure LLM/API integration. Some say you need fundamentals, others say most jobs are just orchestrating existing models. With tools like Claude Code changing what coding even means, I'm not sure what skills are actually durable.\n\nFor people who've done this or are hiring:\n\n- What actually separated you from other candidates when you got in?\n- How much traditional ML do you use day-to-day vs LLM orchestration?\n- Best resources that actually helped you, not just ones you heard were good?\n- What does this role even look like in 2027 when agents do more of the work?\n\nNot looking for a generic roadmap. Looking for what's actually working right now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qviqlh/how_to_become_an_ai_engineer_in_2026_what/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3kxegl",
          "author": "Number4extraDip",
          "text": "What matters. Solving a specific problem. Being very specific and not just doing what everyone else is doing",
          "score": 6,
          "created_utc": "2026-02-04 19:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mrjp9",
              "author": "bbahner",
              "text": "Can you be more specific? <grin>",
              "score": 4,
              "created_utc": "2026-02-05 00:36:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nebra",
                  "author": "Number4extraDip",
                  "text": "Make something you wish existed. Make a product for yourself that no one is selling.",
                  "score": 0,
                  "created_utc": "2026-02-05 02:46:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lcedg",
          "author": "hrishikamath",
          "text": "Honestly most roles I have interviewed for have had AI in requirements but interviews were SWE stuff: system design, leetcode style and so on. Most but not all. During interviews I did speak about my projects that‚Äôs about it and some questions here and there. Yeah it‚Äôs more of just building agents for a lot of them. Traditional ML stuff is required by certain niche  companies. Certain companies randomly add its good to have fine tuning experience. But, yeah some companies develop their models for that you need solid fundamentals from ground up.",
          "score": 4,
          "created_utc": "2026-02-04 20:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lelk4",
          "author": "Canadianingermany",
          "text": ">Some say you need fundamentals, others say most jobs are just orchestrating existing models.  \n\n\nMost things that people are doing today are probably quite easy, and many are working on small problems that can probably be solved with some API and some prompt engineering.  \n\n  \nBut I'm not so convinced that in the future people will want to pay a full fledged DS wage for that because the barriers to entry are simply quite low.  \n\n  \nSo strategically I would concentrate on harder problems that need more than throw an LLM at it.  \n\n  \nBut what do I know?  I hire devs, I'm not one.\n\n>  \nI'm not sure what skills are actually durable.\n\nAt the end of the day.  The ability to solve problems and not be locked in to the solution that worked last time, but find the one for this problem.\n\n  \n",
          "score": 2,
          "created_utc": "2026-02-04 20:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqzgg",
              "author": "hrishikamath",
              "text": "Not really building good rag systems or tasks that require lot of context requires good understanding and skills",
              "score": 1,
              "created_utc": "2026-02-05 04:03:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oktgf",
          "author": "MediumShoddy5264",
          "text": "ML is not useful right now, you need to understand tool calling, context management, planning, evals, etc... ",
          "score": 2,
          "created_utc": "2026-02-05 07:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pui56",
              "author": "MullingMulianto",
              "text": "can you elaborate what you mean by ML. do you mean decision trees, classifiers, etc.?",
              "score": 1,
              "created_utc": "2026-02-05 14:01:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3p6o4l",
          "author": "KegOfAppleJuice",
          "text": "A big emphasis is on cloud engineering, LLM evals and observability and creating quality data context for agents.",
          "score": 2,
          "created_utc": "2026-02-05 11:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qk2x4",
          "author": "Fragrant_Western4730",
          "text": "I've been working on Slack-based agents lately that need to handle open ended tasks from users. I'm pretty convinced agent memory is going to become a must-have skill for any AI engineer that wants to build agents that are more than just workflows or chatbots, especially as adaptive memory and agent learning keeps improving. I won my last two clients by putting my agent in Slack and calling it an AI employee and showing very rudimentary learning and memory.",
          "score": 2,
          "created_utc": "2026-02-05 16:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3quk4m",
              "author": "MAX7668",
              "text": "I guess I'm out of the loop. What do you mean by adaptive memory?",
              "score": 2,
              "created_utc": "2026-02-05 16:58:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3qv9b3",
                  "author": "Fragrant_Western4730",
                  "text": "It's this idea that your agents can have some sort of self-evolving mechanism to learn over time. A simple case would be user preferences, but the more interesting area is in automations and tool calling. Having agents learn what tools to call to solve tasks without needing to tune prompts over and over. I'm using Hindsight for this in the Slack agents I mentioned.",
                  "score": 2,
                  "created_utc": "2026-02-05 17:01:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtgszg",
      "title": "Drowning in 70k+ papers/year. Built an open-source pipeline to find the signal. Feedback wanted.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qtgszg/drowning_in_70k_papersyear_built_an_opensource/",
      "author": "Real-Cheesecake-8074",
      "created_utc": "2026-02-02 01:08:40",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "Like many of you, I'm struggling to keep up. With over 80k AI papers published last year on arXiv alone, my RSS feeds and keyword alerts are just noise. I was spending more time filtering lists than reading actual research.\n\nTo solve this for myself, a few of us hacked together an open-source pipeline (\"Research Agent\") to automate the pruning process. We're hoping to get feedback from this community on the ranking logic to make it actually useful for researchers.\n\n**How we're currently filtering:**\n\n* **Source:**¬†Fetches recent arXiv papers (CS.AI, CS.ML, etc.).\n* **Semantic Filter:**¬†Uses embeddings to match papers against a specific natural language research brief (not just keywords).\n* **Classification:**¬†An LLM classifies papers as \"In-Scope,\" \"Adjacent,\" or \"Out.\"\n* **\"Moneyball\" Ranking:**¬†Ranks the shortlist based on author citation velocity (via Semantic Scholar) + abstract novelty.\n* **Output:**¬†Generates plain English summaries for the top hits.\n\n**Current Limitations (It's not perfect):**\n\n* Summaries can hallucinate (LLM randomness).\n* Predicting \"influence\" is incredibly hard and noisy.\n* Category coverage is currently limited to CS.\n\n**I need your help:**\n\n1. If you had to rank papers automatically, what signals would¬†*you*¬†trust? (Author history? Institution? Twitter velocity?)\n2. What is the biggest failure mode of current discovery tools for you?\n3. Would you trust an \"agent\" to pre-read for you, or do you only trust your own skimming?\n\nThe tool is hosted here if you want to break it:¬†[https://research-aiagent.streamlit.app/](https://research-aiagent.streamlit.app/)\n\nCode is open source if anyone wants to contribute or fork it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qtgszg/drowning_in_70k_papersyear_built_an_opensource/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o346bh4",
          "author": "PerceptualDisruption",
          "text": "Awesome",
          "score": 1,
          "created_utc": "2026-02-02 06:52:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwpdkz",
      "title": "ACE-Step 1.5: an on-device music model that beats Suno on common eval metrics",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/v0sqp0rh8phg1",
      "author": "MatchSuccessful1253",
      "created_utc": "2026-02-05 16:09:10",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwpdkz/acestep_15_an_ondevice_music_model_that_beats/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3shdm9",
          "author": "TheGoddessInari",
          "text": "Odd distinctive sound to everything on the site. ü§î",
          "score": 1,
          "created_utc": "2026-02-05 21:33:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrcxoj",
      "title": "Claude code's main success story is their tool design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "author": "Miclivs",
      "created_utc": "2026-01-30 18:03:34",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.77,
      "text": "Claude Code hit $1B in run-rate revenue.\n\nIts core architecture? Four primitives: read, write, edit, and bash.\n\nMeanwhile, most agent builders are drowning in specialized tools. One per domain object (hmm hmm 20+ tool MCPs..)\n\nThe difference comes down to one asymmetry:\n\n**Reading forgives schema ignorance. Writing punishes it.**\n\nWith reads, you can abstract away complexity. Wrap different APIs behind a unified interface. Normalize response shapes. The agent can be naive about what's underneath.\n\nWith writes, you can't hide the schema. The agent isn't consuming structure‚Äîit's producing it. Every field, every constraint, every relationship needs to be explicit.\n\nUnless you model writes as files.\n\nFiles are a universal interface. The agent already knows JSON, YAML, markdown. The schema isn't embedded in your tool definitions‚Äîit's the file format itself.\n\nFour primitives. Not forty.\n\nWrote up the full breakdown with Vercel's d0 results: \n\nhttps://michaellivs.com/blog/architecture-behind-claude-code\n\nCurious if others have hit this same wall with write tools.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2n9nmx",
          "author": "pborenstein",
          "text": "I've been experimenting with other LLM code harnesses. Because I started with Claude Code, I tend to compare everything to it. \n\nThe first thing I missed was CC's command/skill structure. I realized that this is probably why I've never used an MCP. \n\nThe LLM can figure out what to do from my description. It deals with ambiguity better than code.",
          "score": 1,
          "created_utc": "2026-01-30 18:12:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2na2dy",
              "author": "Miclivs",
              "text": "I think there‚Äôs a good reason to compare everything to CC‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-30 18:13:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2njp9b",
          "author": "steinernein",
          "text": "Why even bother with four? Just reduce it down to three and alter the shape of what the API accepts and rejects.",
          "score": 1,
          "created_utc": "2026-01-30 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p6wgf",
          "author": "kubrador",
          "text": "the files-as-api thesis is clever but i'm skeptical this scales past toy problems. reads forgiving schema ignorance works until the agent hallucinates a field that doesn't exist and you're debugging why it corrupted your production database.",
          "score": 1,
          "created_utc": "2026-01-30 23:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p7lbg",
              "author": "Miclivs",
              "text": "Oh, it works AMAZINGLY well for our data analytics agent.",
              "score": 1,
              "created_utc": "2026-01-30 23:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nb9sz",
          "author": "One-Neighborhood4868",
          "text": "Just make an agent team to help out choose what tools to use?",
          "score": 0,
          "created_utc": "2026-01-30 18:19:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsyhfi",
      "title": "Operating an LLM as a constrained decision layer in a 24/7 production system",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qsyhfi/operating_an_llm_as_a_constrained_decision_layer/",
      "author": "NationalIncome1706",
      "created_utc": "2026-02-01 13:29:29",
      "score": 8,
      "num_comments": 19,
      "upvote_ratio": 0.83,
      "text": "I‚Äôm an engineer by background (14+ years in aerospace systems),  \nand recently I‚Äôve been running a **24/7 always-on production system** that uses an LLM as a *constrained decision-making component*.\n\nThe specific application happens to be automated crypto trading,  \nbut this post is **not** about strategies, alpha, or performance.\n\nIt‚Äôs about a more general systems problem:\n\n>\n\n# System context (high-level)\n\n* **Runtime:** always-on, unattended, 24/7\n* **Environment:** small edge device (no autoscaling, no human in the loop)\n* **Decision model:** discrete, time-gated decisions\n* **Failure tolerance:** low ‚Äî incorrect actions have real cost\n\nThe system must continue operating safely even when:\n\n* external APIs are unreliable\n* the LLM produces malformed or inconsistent outputs\n* partial data or timing mismatches occur\n\n# How the LLM is used (and how it is not)\n\nThe LLM is **not** used for prediction, regression, or forecasting.\n\nIt is treated as a **bounded decision layer**:\n\n* It receives only *preprocessed, closed-interval data*\n* It must output exactly one of:\n   * `ENTRY`\n   * `HOLD`\n   * `CLOSE`\n\nThere are no confidence scores, probabilities, or free-form reasoning  \nthat directly affect execution.\n\nIf the response cannot be parsed, times out, or violates the expected format  \n‚Üí **the system defaults to doing nothing**.\n\n# Core design principles\n\n# 1. Decisions only occur at explicit, closed boundaries\n\nThe system never acts on streaming or unfinished data.\n\nAll decisions are gated on **closed time windows**.  \nThis eliminated several classes of failure:\n\n* phantom actions caused by transient states\n* rapid oscillation near thresholds\n* overlapping execution paths\n\nIf the boundary is not closed, the system refuses to act.\n\n# 2. ‚ÄúDo nothing‚Äù is the safest default\n\nThe system is intentionally biased toward inaction.\n\n* API error ‚Üí HOLD\n* LLM timeout ‚Üí HOLD\n* Partial or inconsistent data ‚Üí HOLD\n* Conflicting signals ‚Üí HOLD\n\nIn ambiguous situations, *not acting* is considered the safest outcome.\n\n# 3. Strict separation of concerns\n\nThe system is split into independent layers:\n\n* data preparation\n* LLM-based decision\n* execution\n* logging and notification\n* post-action accounting\n\nEach layer can fail independently without cascading into repeated actions  \nor runaway behavior.\n\nFor example, notifications react only to **confirmed state changes**,  \nnot to intended or predicted actions.\n\n# 4. Features that were intentionally removed\n\nSeveral ideas were tested and then removed after increasing operational risk:\n\n* adaptive or performance-based scaling\n* averaging down / martingale behavior\n* intra-window predictions\n* confidence-weighted LLM actions\n* automatic restart into uncertain internal states\n\nThe system became *more stable* by explicitly **not doing these things**.\n\n# Why I‚Äôm sharing this\n\nI‚Äôm sharing this to **organize and reflect on lessons learned** from operating  \na non-deterministic LLM component in a live system.\n\nThe feedback here is for personal learning and refinement of system design.  \nAny future write-up would be technical and experience-based,  \nnot monetized and not promotional.\n\n# Looking for discussion\n\nI‚Äôd appreciate perspectives from people who have:\n\n* deployed LLMs or ML components in always-on systems\n* dealt with non-determinism and failure modes in production\n* strong opinions on fail-safe vs fail-open design\n\nIf this kind of operational discussion is useful (or not), I‚Äôd like to know.\n\n\n\nhttps://preview.redd.it/79npeu8hxvgg1.jpg?width=2048&format=pjpg&auto=webp&s=0be3702d0694e3f1ff0f73c9d8b8e4b8fbf3b548\n\n\n\n*Not selling anything here. Just sharing an operational experience.*",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qsyhfi/operating_an_llm_as_a_constrained_decision_layer/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o30gf9h",
          "author": "chris_thoughtcatch",
          "text": "Why use the LLM over a heuristic? Or use the LLM to determine the heuristic?",
          "score": 5,
          "created_utc": "2026-02-01 18:20:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o320aqp",
              "author": "NationalIncome1706",
              "text": "Good question. Early versions of the system were almost entirely heuristic-based,\nand most of the core behavior still is.\n\nI didn‚Äôt introduce the LLM to replace rules, but to handle the gray zones between them.\nHeuristics work extremely well when decision boundaries are crisp.\nThey tend to become brittle or explosively complex when multiple conditions are\npartially satisfied at the same time.\n\nIn this setup, the LLM cannot mutate state, trigger execution, or override hard rules.\nIt only answers a constrained question: ‚ÄúIs this situation unambiguous or not?‚Äù\n\nIf heuristics are the hard constraints, the LLM acts more like a soft consensus checker\non top of them.\n\nI also deliberately avoided using the LLM to generate or adapt heuristics.\nOnce rules become model-derived, failure modes get harder to reason about\nand rollback becomes non-trivial.",
              "score": 2,
              "created_utc": "2026-02-01 22:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zkcop",
          "author": "pstryder",
          "text": "Strong agreement on default-to-inaction and closed boundaries. Curious whether you‚Äôve run into trust erosion when the LLM *sounds* confident but is actually constrained ‚Äî that‚Äôs been a surprisingly sharp edge for us.",
          "score": 2,
          "created_utc": "2026-02-01 15:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3271yr",
              "author": "NationalIncome1706",
              "text": "Yes ‚Äî we ran into that exact edge early on.\n\nThe problem wasn‚Äôt the constraints themselves, but the mismatch between linguistic confidence and actual agency. Humans tend to interpret confident language as capability, even when the model is heavily boxed in.\n\nWhat helped was reframing the LLM‚Äôs role entirely. Its output isn‚Äôt treated as an explanation or a recommendation ‚Äî it‚Äôs a state classification signal. We deliberately stripped away expressive language and made responses repetitive, terse, and sometimes even boring.\n\nOver time, that shifted trust away from the LLM as an ‚Äúactor‚Äù and toward the system as a whole. The goal wasn‚Äôt to make the LLM trustworthy, but to make it unnecessary to trust in isolation.\n\nOnce operators internalize that distinction, the confidence/constraint gap becomes much less sharp.",
              "score": 1,
              "created_utc": "2026-02-01 23:25:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31zmz3",
          "author": "Kimononono",
          "text": "How often does your system produce a BUY / SELL signal vs HOLD\n\nAt what interval does it run?",
          "score": 2,
          "created_utc": "2026-02-01 22:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32e4e0",
              "author": "NationalIncome1706",
              "text": "The decision loop runs on a fixed schedule, but with very different semantics.\n\nENTRY decisions are only evaluated on closed higher-timeframe candles, so they‚Äôre relatively infrequent by design.\nPosition management / exit checks run much more often, but the default outcome there is still HOLD.\n\nIn practice, the vast majority of evaluations result in HOLD. BUY/SELL is treated as an exception, not a steady stream.\n\nThat ratio is intentional. We don‚Äôt optimize for signal frequency ‚Äî we treat excessive activity as a smell. If the system is trading often, something upstream is probably too permissive.\n\nThe goal is to let the system be bored most of the time and only act when ambiguity collapses.",
              "score": 1,
              "created_utc": "2026-02-02 00:04:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o33pmxx",
                  "author": "Kimononono",
                  "text": "if I have a script which goes \n\nfunc analyze\\_signal():\n\nnum = random()  \nif(num < .01) BUY  \nif(num < .02) SELL  \nelse HOLD\n\nHow are you to smell anything if the scent is so faint?",
                  "score": 2,
                  "created_utc": "2026-02-02 04:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34gef2",
          "author": "Sea-Sir-2985",
          "text": "one thing i've run into with this kind of setup is that schema validation alone isn't enough... the LLM can return perfectly valid JSON that still makes a nonsensical decision. so we added a semantic validation layer on top, basically domain constraint checks that run after the LLM responds but before anything gets executed\n\nthe other piece that helped was logging every decision with the full prompt and response, not just the final action. when something goes wrong at 3am you need to see exactly what the model was thinking, not just what it did",
          "score": 2,
          "created_utc": "2026-02-02 08:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35elkz",
              "author": "NationalIncome1706",
              "text": "This matches our experience almost exactly.\n\nValid JSON is necessary but nowhere near sufficient.\nThe LLM sits inside a constrained decision layer,\nwhere its output is treated as a hypothesis that must pass\nexplicit domain checks (timeframe closure, regime alignment, risk bounds, state continuity).\n\nExecution is gated deterministically.\nHOLD is the fail-safe default.\n\nWe also persist the full prompt snapshot and model response,\nbecause without that, post-mortems are basically guesswork.",
              "score": 1,
              "created_utc": "2026-02-02 13:14:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38eoep",
          "author": "KnightCodin",
          "text": "I might have missed the exact reason but why introduce LLM at all? \"non-determinism\" or \"Grey\" area are not sufficient reason to introduce further complexity into this \"bound decision later\" with strict output states. Having done mission critical production systems all my life (built and managed trading systems for a one the big 3 investment banks) I am curious to know what are the reasons for this. Decision thresholding can be used to achieve the ambiguity question easily. What am I missing?",
          "score": 2,
          "created_utc": "2026-02-02 21:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38v6x8",
              "author": "NationalIncome1706",
              "text": "Good question.\n\nThe distinction is between a *default HOLD* and a *judged HOLD*.\n\nWith heuristics alone, any region outside clearly defined thresholds\ncollapses into a blanket HOLD. That‚Äôs safe, but it also throws away cases\nthat are structurally unambiguous but hard to encode cleanly as rules.\n\nThe LLM doesn‚Äôt decide BUY/SELL, mutate state, or override rules.\nIt only answers a constrained question:\n‚ÄúIs this situation unambiguous enough to allow the rules to act?‚Äù\n\nIf the answer is no, it‚Äôs still HOLD.\nIf yes, execution remains fully rule-driven.\n\nSo the goal wasn‚Äôt to add intelligence on top of heuristics,\nbut to prevent heuristics from becoming brittle or explosively complex\nin partially satisfied, multi-condition regimes.",
              "score": 1,
              "created_utc": "2026-02-02 23:17:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o38x43n",
                  "author": "KnightCodin",
                  "text": "Okay - so you are in the multi-branching scenario. You are still introducing stochasticity. However, assuming you have explored other techniques, Without knowing all the constraints and ecosystems, simplest solution would be   \n1. Use a smaller LLM known to punch above its weight class Eg. Qwen 3 - 4B.  \n2. If your inference engine allows it, use constraints at logits level to reduce subjectivity",
                  "score": 2,
                  "created_utc": "2026-02-02 23:28:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ytwkw",
          "author": "NationalIncome1706",
          "text": "This is an experience report on operating an LLM inside a live system.\n\nNot a product, not prompts, not benchmarks.\n\n\n\nI‚Äôm especially interested in how others handle non-determinism,\n\nfail-safe defaults, and state consistency in always-on LLM-based systems.",
          "score": 1,
          "created_utc": "2026-02-01 13:33:04",
          "is_submitter": true,
          "replies": [
            {
              "id": "o37psxr",
              "author": "amejin",
              "text": "My opinion - how to \"handle non-determinism?\"\n\nWhen it comes to money, you don't.",
              "score": 2,
              "created_utc": "2026-02-02 19:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37upb6",
                  "author": "NationalIncome1706",
                  "text": "I largely agree with that principle. When real money is involved, non-determinism shouldn‚Äôt be trusted as a decision maker.\n\nThat‚Äôs exactly why, in my case, it‚Äôs not allowed to decide anything. It can‚Äôt allocate capital, trigger execution, or override hard rules.\n\nThe only thing it‚Äôs permitted to do is say ‚Äúdon‚Äôt act‚Äù when the system is near a boundary the deterministic logic can‚Äôt cleanly resolve.\n\nSo I don‚Äôt think of it as ‚Äúhandling non-determinism,‚Äù but rather containing it ‚Äî pushing it into a narrow veto role where failure modes collapse to inaction.\n\nIf a non-deterministic component can‚Äôt fail safely, it shouldn‚Äôt be anywhere near money. On that point, I think we‚Äôre aligned.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:19:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qranzr",
      "title": "How do you prevent credential leaks to AI tools?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "author": "llm-60",
      "created_utc": "2026-01-30 16:45:04",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "How is your company handling employees pasting credentials/secrets into AI tools like ChatGPT or Copilot? Blocking tools entirely, using DLP, or just hoping for the best?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2p7nzh",
          "author": "kubrador",
          "text": "hoping for the best is the classic strategy, followed by a panicked all-hands email in 6 months when someone inevitably pastes a prod database url into claude.\n\nmost companies doing it right use a combo: dlp tools with regex patterns for common secrets, network blocks on the obvious stuff, and mandatory training that employees ignore until it happens to them.",
          "score": 3,
          "created_utc": "2026-01-30 23:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pvelf",
          "author": "Basic_Cat_1006",
          "text": "Whoever is hardcoding or announcing secrets out of the .env should not be a mile near your code base lmao.",
          "score": 2,
          "created_utc": "2026-01-31 02:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vg7g9",
              "author": "graymalkcat",
              "text": "Just as an aside, if you use Claude, it will open your .env which will send that to Anthropic. Maintain different sets of keys. (It‚Äôs not that Claude is evil but rather that it forgets that it itself is a cloud service)",
              "score": 3,
              "created_utc": "2026-01-31 22:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o306z3b",
                  "author": "Basic_Cat_1006",
                  "text": "I hadn‚Äôt heard that so thank you immensely",
                  "score": 1,
                  "created_utc": "2026-02-01 17:38:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2n78m1",
          "author": "Miclivs",
          "text": "I made a thing for that! [https://github.com/Michaelliv/psst](https://github.com/Michaelliv/psst)",
          "score": 2,
          "created_utc": "2026-01-30 18:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q3ut4",
          "author": "cmndr_spanky",
          "text": "The same way you prevent employees from pasting their crediting into email, slack, GitHub etc‚Ä¶\n\nWhy is AI any different ?",
          "score": 1,
          "created_utc": "2026-01-31 02:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfpuh",
              "author": "konmik-android",
              "text": "You mean, you ask it politely and then it ignores you?",
              "score": 1,
              "created_utc": "2026-02-01 02:19:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zwmgm",
                  "author": "cmndr_spanky",
                  "text": "That and you make them sign agreements that could make them legally liable (like at my company). Security experts will always tell you that human behavior will always be the weakest link, not software vulnerabilities. That said you missed my larger point. OP‚Äôs post is dumb because a person pasting credentials into AI chat is literally no different than a person pasting it into a multitude of other insecure places (which people do all the time). There‚Äôs nothing special about it being ‚ÄúAI‚Äù in terms of how you deal with this as a company.",
                  "score": 2,
                  "created_utc": "2026-02-01 16:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t46i5",
          "author": "Friendly_Hat_9545",
          "text": "We tried the \"training and hoping\" approach at first, which lasted exactly until someone almost pasted a Dev database connection string. That was a fun afternoon lol.\n\nNow we use inline DLP that scans before stuff reaches the chatbot. We went with iboss AI Chat Security because it looks into our existing network stack and just... works? Blocks the paste if it sniffs keys or PII patterns. We still allow Copilot and ChatGPT, but now with guardrails. It's not perfect but way better than crossing our fingers.\n\nTBH, blocking the tools entirely just leads to Shadow IT. You gotta let people use the tools but make it safe.",
          "score": 1,
          "created_utc": "2026-01-31 16:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vumac",
          "author": "Agreeable-Market-692",
          "text": "litellm as a proxy",
          "score": 1,
          "created_utc": "2026-02-01 00:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33oqx4",
          "author": "jontaffarsghost",
          "text": ".geminiignore",
          "score": 1,
          "created_utc": "2026-02-02 04:35:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv6w28",
      "title": "I‚Äôm building an open-source local AI agent in Go that uses IR + tools instead of wasting tokens",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qv6w28/im_building_an_opensource_local_ai_agent_in_go/",
      "author": "iagomussel",
      "created_utc": "2026-02-03 22:46:21",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "Hey everyone,\n\nI‚Äôve been working on an open-source project called **IRon**: a local-first AI assistant focused on automation, not chat.\n\nThe main idea is:\n\nInstead of using LLMs to ‚Äúthink‚Äù and generate long text, IRon translates user input into a small structured format (IR ‚Äì Intermediate Representation) and executes real tools.\n\nSo most tasks don‚Äôt need heavy models.\n\n# What it does\n\nIRon works mainly through Telegram and runs locally.\n\nPipeline:\n\nUser ‚Üí Router ‚Üí (optional LLM) ‚Üí IR (JSON) ‚Üí Tools ‚Üí Result\n\nFeatures:\n\n* Deterministic router for common tasks (notes, lists, commands, etc.)\n* Dual output: short human reply + machine IR\n* Tool system (shell, docker, http, code exec, notes, scheduler, addons)\n* Cron-based scheduler\n* Codex/Ollama support for complex reasoning\n* Session isolation per chat\n* Addon system for external tools/adapters\n\n# Why I built it\n\nMost ‚ÄúAI assistants‚Äù today:\n\n* Burn tokens on simple things\n* Re-explain everything\n* Don‚Äôt integrate well with real systems\n* Lose context easily\n\nI wanted something closer to:\n\n‚ÄúNatural language ‚Üí compact instruction ‚Üí real execution‚Äù\n\nLike a mix of:\n\n* cron\n* Makefile\n* shell\n* and LLMs\n\nBut with safety and structure.\n\n# Example\n\nUser:  \n‚ÄúRemind me to pay rent tomorrow at 9‚Äù\n\nIRon:\n\n* Generates IR\n* Schedules cron\n* Uses scheduler tool\n* Confirms in one line\n\nNo long explanation. No wasted tokens.\n\n# Tech stack\n\n* Go\n* Telegram Bot API\n* Codex CLI / Ollama (future)\n* JSON-based IR\n* robfig/cron\n* Plugin system\n\nCurrent status\n\nIt‚Äôs usable and evolving.  \nMain focus now:\n\n* DSL for tasks\n* Better scheduling\n* Memory without huge context\n* More deterministic routing\n\n**It's in progress, so there are bugs yet, let me know if you can help.**\n\n# Repo\n\n[https://github.com/iagomussel/IRon](https://github.com/iagomussel/IRon?utm_source=chatgpt.com)\n\n# Looking for feedback\n\nI‚Äôm interested in feedback on:\n\n* Architecture\n* IR format\n* DSL ideas\n* Similar projects\n* Security concerns\n\nIf you‚Äôre into local AI, automation, or agent systems, I‚Äôd love your thoughts.\n\nThanks üôå",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qv6w28/im_building_an_opensource_local_ai_agent_in_go/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3g2vcj",
          "author": "DataCentricExpert",
          "text": "Do you have a sandboxed or local dev environment for testing IRon safely with real data, or is it purely the Telegram interface right now?",
          "score": 1,
          "created_utc": "2026-02-04 00:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6ivm",
          "author": "SeaworthinessThis598",
          "text": "I want to get. afeel about the IR concept efficacy can you show us. a demo maybe ?",
          "score": 1,
          "created_utc": "2026-02-04 01:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fite7",
          "author": "Otherwise_Wave9374",
          "text": "Love the ‚ÄúIR + tools‚Äù approach. That feels like a practical agent design: keep the LLM for the hard parsing/planning edges, but push everything into a constrained representation so execution stays predictable.\n\nHow are you thinking about schema evolution for the IR and safety around tool permissions (per chat/session)? Those two things seem to make or break local agents. Ive been reading a bunch about structured agents lately, this page has some good notes: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 23:04:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qulg4u",
      "title": "n8n vs gumloop",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qulg4u/n8n_vs_gumloop/",
      "author": "OkWestern5",
      "created_utc": "2026-02-03 07:19:52",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "If you‚Äôre looking into n8n vs Gumloop, you‚Äôre probably not trying to find the ‚Äúbest‚Äù tool in general. You‚Äôre trying to understand which workflow automation platforms actually fit how your team works day to day. That‚Äôs where this comparison comes from. I also looked at a [broader comparison table](https://docs.google.com/spreadsheets/d/1zQr6iThp2fR-TLNMvSYHgx2ghSrzbYIduO4vX_jlHig/edit?gid=1301024975#gid=1301024975) of workflow automation platforms where n8n is listed, which helped set some baseline context.\n\n# High-level difference\n\n* **Gumloop** is built for business teams that want to automate workflows without involving engineering.\n* **n8n** is built for developer-first teams that want full control, even if that means more setup and maintenance.\n\nThis difference shows up across the product, from the editor to pricing and integrations.\n\n# Ease of use\n\n**Gumloop**  \nGumloop lets you focus on the business problem rather than implementation.\n\n* Visual, easy-to-follow canvas\n* Pre-built actions for common business tools\n* AI features included by default\n* Custom steps without deep technical knowledge\n\nMost teams can get useful workflows running quickly.\n\n**n8n**  \nn8n prioritizes flexibility over simplicity.\n\n* Node-by-node configuration\n* Direct access to APIs, JSON, and JavaScript\n\nYou gain more control, but also more responsibility for building and maintaining workflows.\n\n# Integrations and flexibility\n\nBoth platforms support tools like Google Workspace, Slack, Salesforce, and Notion.\n\n* **n8n** offers broader coverage via community-built nodes, but requires manual setup and upkeep.\n* **Gumloop** focuses on the integrations business teams actually use, with AI-assisted ways to extend them when needed.\n\nIn the **n8n vs Gumloop** comparison, this is often where teams weigh flexibility against effort.\n\n# Pricing and ownership\n\n* **Gumloop** bundles AI models, scraping, enrichment, and data sources into its plans.\n* **n8n** charges per execution, with AI and data services managed and billed separately.\n\nNeither approach is better by default - it depends on whether you prefer bundled convenience or modular control.\n\n# Final thoughts\n\nThe real distinction in n8n vs Gumloop is how much work you want around your automation. Gumloop minimizes it early on, while n8n gives you more room later if you‚Äôre willing to manage it.\n\nWhich side do you lean toward - simplicity or control?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qulg4u/n8n_vs_gumloop/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvvajn",
      "title": "Build a self-updating wiki from codebases (open source, Apache 2.0)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qvvajn/build_a_selfupdating_wiki_from_codebases_open/",
      "author": "Whole-Assignment6240",
      "created_utc": "2026-02-04 17:44:19",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I recently have been working on a new project to build a self-updating wiki from codebases.\n\nYour code is the source of truth, and documentations out of sync is such a common pain especially in larger teams. Someone refactors a module, and the wiki is already wrong. Nobody updates it until a new engineer asks a question about it.\n\nThis open source project scans your codebases, extracts structured information with LLMs, and generates Markdown documentation with Mermaid diagrams ‚Äî using CocoIndex + Instructor + Pydantic.\n\nWhat's cool about this example:\n\n‚Ä¢ ùêàùêßùêúùê´ùêûùê¶ùêûùêßùê≠ùêöùê• ùê©ùê´ùê®ùêúùêûùê¨ùê¨ùê¢ùêßùê† ‚Äî Only changed files get reprocessed. If you have 20+ projects but only touch one file, CocoIndex only re-analyzes that file ‚Äî saving 90%+ of LLM cost and compute.\n\n‚Ä¢ ùêíùê≠ùê´ùêÆùêúùê≠ùêÆùê´ùêûùêù ùêûùê±ùê≠ùê´ùêöùêúùê≠ùê¢ùê®ùêß ùê∞ùê¢ùê≠ùê° ùêãùêãùêåùê¨ ‚Äî We use Pydantic models as the schema with Instructor, so the LLM returns real typed objects ‚Äî classes, functions, signatures, relationships ‚Äî not brittle free text you have to regex parse.\n\n‚Ä¢ ùêÄùê¨ùê≤ùêßùêú ùêüùê¢ùê•ùêû ùê©ùê´ùê®ùêúùêûùê¨ùê¨ùê¢ùêßùê† ‚Äî All files in a project get extracted concurrently with asyncio.gather().\n\n‚Ä¢ ùêåùêûùê´ùê¶ùêöùê¢ùêù ùêùùê¢ùêöùê†ùê´ùêöùê¶ùê¨ ‚Äî Auto-generated pipeline visualizations showing how your functions connect across the project.\n\n‚Ä¢ ùêáùê¢ùêûùê´ùêöùê´ùêúùê°ùê¢ùêúùêöùê• ùêöùê†ùê†ùê´ùêûùê†ùêöùê≠ùê¢ùê®ùêß ‚Äî Extracts at file level, then aggregates into a unified project summary. Single-file projects skip the aggregation LLM call entirely.\n\nThink: target\\_state = transformation(source\\_state)\n\nThis pattern hooks naturally into PR flows ‚Äî run it on every merge and your docs stay current without anyone thinking about it.\n\nIf you want to explore the full example (fully open source, with code, APACHE 2.0), it's here:\n\nüëâ [https://cocoindex.io/examples-v1/multi-codebase-summarization](https://cocoindex.io/examples-v1/multi-codebase-summarization)\n\nNo locked features behind a paywall / commercial / \"pro\" license\n\nIf you find CocoIndex useful, a star on Github means a lot :)\n\n‚≠ê [https://github.com/cocoindex-io/cocoindex](https://github.com/cocoindex-io/cocoindex)",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qvvajn/build_a_selfupdating_wiki_from_codebases_open/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3l14pb",
          "author": "Specialist-Echo-7494",
          "text": "Love it! I think this pipeline is one feature away from industrial-grade... the example is already doing the hard parts (incremental + memoized extraction + structured).\n\nMaybe treat each generated page as a verifiable artifact? - not just Markdown... emit a tiny manifest.json + claims.jsonl sidecar where each sentence / Mermaid edge is a typed claim w/ stable ID + tight provenance (file + symbol + line / col span) + fingerprints (input hashes + model / config + prompt hash).\n\nThen a PR bot / UI can show a ‚Äúdocket‚Äù (claims count + manifest hash + receipt links) and fail the check if claims ‚Üî sources don‚Äôt line up.\n\nAlso: maybe sort project / file iteration + render Mermaid from a canonical typed graph to avoid that diff jitter from traversal / format variance?",
          "score": 2,
          "created_utc": "2026-02-04 19:22:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lsqls",
              "author": "Whole-Assignment6240",
              "text": "super cool!! thanks a lot for your suggestions!",
              "score": 2,
              "created_utc": "2026-02-04 21:34:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv6z6k",
      "title": "Natural Language to shell commands tool. Fully local, Ollama powered.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qv6z6k/natural_language_to_shell_commands_tool_fully/",
      "author": "ykushch",
      "created_utc": "2026-02-03 22:49:38",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "[ask - natural language to shell commands](https://i.redd.it/vzpbvx06zchg1.gif)\n\nI built a CLI tool that turns natural language into shell commands using Ollama. It runs locally (no API keys, no data egress) and includes safety checks so you don't accidentally¬†`rm -rf`¬†your system.\n\nRepo:¬†[https://github.com/ykushch/ask](https://github.com/ykushch/ask)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qv6z6k/natural_language_to_shell_commands_tool_fully/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3jo2dk",
          "author": "konmik-android",
          "text": "I often ask Claude to do that, but having an offline version would be much better. Remembering and typing all parameters of all commands is something I left in 1990x.",
          "score": 2,
          "created_utc": "2026-02-04 15:38:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g2dog",
          "author": "DataCentricExpert",
          "text": "Curious if anyone has tried running it with a local dev environment that enforces data masking and auditability?",
          "score": 1,
          "created_utc": "2026-02-04 00:50:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtm1r7",
      "title": "How do you debug multi-step agent workflows",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qtm1r7/how_do_you_debug_multistep_agent_workflows/",
      "author": "CreditOk5063",
      "created_utc": "2026-02-02 05:10:21",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "I've been working on a customer support agent that routes queries to different tools depending on intent. I am essentially building a state-machine style agent using LangGraph, and the state transitions are where the logic keeps drifting. The flow is: classify intent ‚Üí retrieve relevant docs ‚Üí generate response ‚Üí validate output format. Each node works fine in isolation but when the graph runs end to end the failure modes exhibit non-linear behaviors that are hard to replicate. Sometimes the classifier output schema breaks the retriever input, sometimes the context window gets bloated by step 3.\n\nMy current debugging approach is pretty manual. I added verbose logging at each node, dump intermediate state to JSON, and trace back from failures. But the hard part is not finding where it broke, it is understanding why a certain prompt phrasing caused a downstream node to behave differently. LLM outputs are not deterministic so reproducing issues is painful. So I started using Pydantic models for structured output at each step, and let Claude and Beyz coding assistant to help me do sanity check. But it still feels inefficient though. I'm curious how do you test nodes in isolation first or go straight to end-to-end runs? How do you handle the non-determinism problem when debugging state transitions? Is anyone using Pydantic strictly for node-to-node contracts or does the validation overhead add too much latency and retries for production pipelines? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qtm1r7/how_do_you_debug_multistep_agent_workflows/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o34jm5w",
          "author": "Zeikos",
          "text": "> \"Each node works fine in isolation\"\n\nThat phrasing is a bit of a red flag for me.  \nHow do you define \"fine\"?  \n\nA process of 5 steps where each has a 90% success rate is terrible, it'll fail extremely often.  \n\nBe a lot more strict about the reliability of each node.  \nGoing from 90% to 95% is noticeable, going from 95% to 98% is *more* noticeable.  \n90% -> 95% is a 50% improvement, 95% -> 98% is a 60% improvement.  \n\nWhen you have maxed that out then you want to look at each pairs of nodes.  \nWhat nodes produce bad output when they had good input? Why?  \nYou don't care about a bigger scope than the two connections, the other nodes can stay as a black box.  \n\nBe careful to attribute the failure correctly.  \nWhen something fails at step n then you don't care about steps > n (except eventual error-recovery steps).",
          "score": 3,
          "created_utc": "2026-02-02 08:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33vuq6",
          "author": "plarkin",
          "text": "The golden rule: If you can't reproduce it, you can't debug it. Make everything observable, cacheable, and replayable! \n\nAsk your favorite LLM about it ;)",
          "score": 2,
          "created_utc": "2026-02-02 05:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3413tb",
          "author": "InvestigatorAlert832",
          "text": "The current standard approach to address the reproducibility issue is to use an observability platform like langfuse to capture detailed logs, then clean&save them to datasets, then setup evaluators, and finally you can run evals on the dataset to gauge the quality of your node/program.\nIn terms of interfacing between LLM nodes, I personally prefer strongly typed enforcement. There are frameworks built specifically focusing on this - pydantic-ai, instructor.",
          "score": 1,
          "created_utc": "2026-02-02 06:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o341p8f",
              "author": "InvestigatorAlert832",
              "text": "I do think the observability platforms on the market are painful to use for the debug & iterate scenario though, and I'm actually working on a tool for this myself. Feel free to DM me if you are interested in being my beta tester :)",
              "score": 1,
              "created_utc": "2026-02-02 06:12:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwx950",
      "title": "Built an LLM agent for debugging production incidents - what we learned",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/y5lvd0tznqhg1",
      "author": "Useful-Process9033",
      "created_utc": "2026-02-05 20:53:02",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwx950/built_an_llm_agent_for_debugging_production/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3tj3cq",
          "author": "isthatashark",
          "text": "Cool project!",
          "score": 1,
          "created_utc": "2026-02-06 00:55:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3tsvjh",
              "author": "Useful-Process9033",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-02-06 01:53:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qwmbq0",
      "title": "glm 4.7 swe-bench 73.8% - tested claims on real refactoring tasks, improvement over previous models measurable",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qwmbq0/glm_47_swebench_738_tested_claims_on_real/",
      "author": "Weird_Perception1728",
      "created_utc": "2026-02-05 14:11:57",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "saw glm 4.7 swe-bench verified score (73.8%, +5.8 vs glm 4.6) and terminal bench (41%, +16.5)\n\nskeptical of benchmark gaming so tested on actual software engineering tasks\n\n**methodology:**\n\n20 refactoring tasks from internal codebase (flask, fastapi, django projects)\n\neach task: multi-file changes, maintaining references, no breaking changes\n\ntested against: glm 4.6, deepseek v3, codellama 70b\n\nmetric: success rate (code runs without fixes) + retry attempts needed\n\n**results:**\n\nglm 4.7: 17/20 success first attempt (85%))  \ndeepseek v3: 14/20 success first attempt (70%)  \ncodellama 70b: 11/20 success first attempt (55%)\n\n**failure analysis:**\n\nglm 4.7 failures: mostly edge cases in dependency injection patterns\n\nother models: frequent import hallucination, circular dependency introduction, breaking type hints\n\n**terminal bench correlation:**\n\ntested bash script generation (10 automation tasks)\n\nglm 4.7: 9/10 scripts ran without syntax errors  \nothers: 5-7/10 average\n\nterminal bench score (41% vs \\~25-35% typical) actually translated to real usage\n\n**architectural notes:**\n\n355b parameters, moe with 32b active per token\n\ntraining on 14.8t tokens\n\n**where improvement shows:**\n\ncross-file context tracking significantly better (measured by import correctness)\n\niterative debugging fewer loops to solution (average 1.4 attempts vs 2.3 for previous)\n\nbash/terminal command generation syntax correctness up\n\n**where still limited:**\n\ntraining cutoff late-2024 (misses recent library updates)\n\narchitectural reasoning weaker than frontier closed models\n\nexplanation depth inferior to teaching-optimized models\n\n**cost efficiency:**\n\napi pricing: \\~$3/month plan for generous coding use (significantly under openai/anthropic)\n\n**discussion points:**\n\nis 73.8% swe-bench representing actual capability or benchmark-specific tuning?\n\nbased on 20-task sample, improvement over previous versions real and measurable\n\nterminal bench correlation to bash quality interesting - suggests benchmark captures meaningful skill\n\n**limitations of this analysis:**\n\nsmall sample size (20 tasks)\n\ntasks from specific domains (web backends)\n\nno comparison to gpt-4/claude (cost prohibitive for extensive testing)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwmbq0/glm_47_swebench_738_tested_claims_on_real/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3pxn6q",
          "author": "WelcomeMysterious122",
          "text": "nice.",
          "score": 1,
          "created_utc": "2026-02-05 14:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pz6ar",
          "author": "microhan20",
          "text": "Interesting that terminal bench score correlates with actual bash quality. Usually synthetic benchmarks don't predict real performance well, but 41% vs 25-35% showing in your tests suggests its capturing something meaningful",
          "score": 1,
          "created_utc": "2026-02-05 14:27:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tjp4u",
          "author": "DeathShot7777",
          "text": "How much does it cost in total to run swe verified on glm4.7 \n\nI am working on a code intelligence layer using Graphs so agents can use it for deeper codebase understanding and want to test performance improvements on it. Will need to save up üòÖ",
          "score": 1,
          "created_utc": "2026-02-06 00:58:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwf2bh",
      "title": "Built this because I was tired of redoing AI agent stuff again and again",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qwf2bh/built_this_because_i_was_tired_of_redoing_ai/",
      "author": "Most_Cardiologist313",
      "created_utc": "2026-02-05 07:39:49",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Every Al project I build ends up repeating the same setup: agent reasoning loop, tool calling, API wrapper, bot integration, deployment configs. After doing this too many times, I built a small internal framework to standardize this stuff for myself.\n\nIt handles things like ReACT-style agents, tool execution, API mode, Discord integration, and edge-friendly deployment patterns.\n\nBefore I invest more time into polishing it, I'm curious how are you handling this today? Are you using LangChain/LangGraph, rolling your own, or something else? What parts feel the most painful to maintain?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qwf2bh/built_this_because_i_was_tired_of_redoing_ai/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3oiwmr",
          "author": "Most_Cardiologist313",
          "text": "Here‚Äôs the repo in case anyone wants to look at the implementation:\nhttps://github.com/Parvezkhan0/LLM-Task-Orchestrator",
          "score": 1,
          "created_utc": "2026-02-05 07:40:44",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o3olroc",
          "author": "Otherwise_Wave9374",
          "text": "I feel this so much. Everyone ends up rebuilding the same \"agent spine\" (loop, tools, retries, logging, auth, deployment) and it gets old fast.\n\nBiggest pain point for me has been eval + observability, like being able to replay a run, inspect tool calls, and measure where the agent went off track. Once you have that, the rest gets way easier to iterate.\n\nIf you are thinking about how to structure the agent loop and what to measure, I have seen a few good writeups here: https://www.agentixlabs.com/blog/\n\nAre you planning to support MCP out of the box, or is it more of an internal tool registry + adapters approach?",
          "score": 1,
          "created_utc": "2026-02-05 08:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u0lh8",
          "author": "AdditionalWeb107",
          "text": "via Plano - https://github.com/katanemo/plano. Solving delivery challenges at the substrate level.",
          "score": 1,
          "created_utc": "2026-02-06 02:39:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}