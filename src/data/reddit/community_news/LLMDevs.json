{
  "metadata": {
    "last_updated": "2026-03-02 09:15:17",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 100,
    "file_size_bytes": 144249
  },
  "items": [
    {
      "id": "1rfj3x5",
      "title": "Self Hosted LLM Tier List",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/p0ewesu10wlg1.png",
      "author": "Weves11",
      "created_utc": "2026-02-26 19:03:31",
      "score": 103,
      "num_comments": 19,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rfj3x5/self_hosted_llm_tier_list/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7l557b",
          "author": "Smooth_Astronaut855",
          "text": "How does one host 1T of parameters, even with quantization. Token generated per minute is lower and benchmarks are only slightly better. \nSo I wonder what makes it so desirable? ü§î",
          "score": 9,
          "created_utc": "2026-02-26 21:21:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nn3g4",
          "author": "MiyamotoMusashi7",
          "text": "No Qwen 3.5 122b, 35b, 27b?",
          "score": 5,
          "created_utc": "2026-02-27 06:07:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7sfwt0",
              "author": "itsjase",
              "text": "Or even glm 4.7 flash",
              "score": 1,
              "created_utc": "2026-02-27 23:27:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7zxzqc",
                  "author": "fatal57vr",
                  "text": "Yeah, glm 4.7 would be a solid addition! It has some impressive capabilities that could shake things up in the rankings.",
                  "score": 1,
                  "created_utc": "2026-03-01 04:06:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7nqd4d",
          "author": "stormy1one",
          "text": "Lmao - Who is this for?  Mixing KimiK2.5 and Phi 4 Mini in the same ranking makes zero sense.   The people that are considering those two models as ‚Äúwhat should I run‚Äù are apples and oranges.",
          "score": 6,
          "created_utc": "2026-02-27 06:34:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o83ba4s",
              "author": "Polysulfide-75",
              "text": "There‚Äôs a size filter.",
              "score": 1,
              "created_utc": "2026-03-01 18:12:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7o3e7y",
          "author": "kellybluey",
          "text": "I need a $500k NVIDIA rig to host them locally",
          "score": 2,
          "created_utc": "2026-02-27 08:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ooq5b",
          "author": "timbo2m",
          "text": "Qwen 3 coder next?",
          "score": 2,
          "created_utc": "2026-02-27 11:43:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7tyhn6",
          "author": "illuminato8",
          "text": "Gemma 27b should be in A. Very impressive model.",
          "score": 1,
          "created_utc": "2026-02-28 05:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8454q0",
          "author": "isko990",
          "text": "How strong do you need PC for Kimi 1T?",
          "score": 1,
          "created_utc": "2026-03-01 20:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84vb88",
          "author": "TaylorHu",
          "text": "Source: Trust me bro.",
          "score": 1,
          "created_utc": "2026-03-01 22:56:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ntac0",
          "author": "tom-mart",
          "text": "And yet the most work in my set up is done by Qwen3-4b.",
          "score": 1,
          "created_utc": "2026-02-27 06:58:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ygqdo",
              "author": "HaysamKING1",
              "text": "It's soo good for it's size, there's hop we will get qwen3.5 4b or 7b",
              "score": 1,
              "created_utc": "2026-02-28 22:42:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o80039c",
              "author": "old_mikser",
              "text": "Would you mind to tell what exact work it does?",
              "score": 1,
              "created_utc": "2026-03-01 04:21:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o80gsso",
                  "author": "tom-mart",
                  "text": "Intent classification - analising all new messages and current context to decide if it's a continuation of previous request or new request. New request are classified into one of the possible existing intents and user request is added to relevant pipelines.\n\nLanguage recognition - all messages from users that don't have accounts receive a rejection response in the language they wrote in (users use mix of European languages.\n\nTool use and RAG - using tools to interact with various APIs, based on user identity and intent. Retrieving data from databases, updating DB records, creating tasks in the queueing tool, etc.\n\nAssigning heavier workloads to specialist agent - requesting reports, document summaries and such.",
                  "score": 1,
                  "created_utc": "2026-03-01 06:31:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7tkiji",
          "author": "MikeLPU",
          "text": "And this picture is not dumb. Approved üëç.",
          "score": 0,
          "created_utc": "2026-02-28 03:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7zhkff",
          "author": "julianmatos",
          "text": "Here's a better source that helps you figure out what you can actually run [https://www.localllm.run/](https://www.localllm.run/)",
          "score": 0,
          "created_utc": "2026-03-01 02:21:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdyu2f",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Was Wrong.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdyu2f/we_benchmarked_7_chunking_strategies_most_best/",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-25 01:03:00",
      "score": 74,
      "num_comments": 17,
      "upvote_ratio": 0.93,
      "text": "If you've built a RAG system, you've had the chunking conversation. Somebody on your team (or [a Medium post](https://medium.com/%40adnanmasood/chunking-strategies-for-retrieval-augmented-generation-rag-a-comprehensive-guide-5522c4ea2a90)) told you to \"just use 512 tokens with 50-token overlap\" or \"semantic chunking is strictly better.\"\n\nWe (hello from the R&D team at Vecta!) decided to test these claims. We created a small corpus of real academic papers spanning AI, astrophysics, mathematics, economics, social science, physics, chemistry, and computer vision. Then, we ran every document through seven different chunking strategies and measured retrieval quality and downstream answer accuracy.\n\nCritically, we designed the evaluation to be¬†fair: each strategy retrieves a different number of chunks, calibrated so that every strategy gets approximately¬†2,000 tokens of context¬†in the generation prompt. This eliminates the confound where strategies with larger chunks get more context per retrieval, and ensures we're measuring chunking quality, not context window size.\n\nThe \"boring\" strategies won. The hyped strategies failed. And the relationship between chunk granularity and answer quality is more nuanced than most advice suggests.\n\n# Setup\n\n# Corpus\n\nWe assembled a diverse corpus of 50 academic papers (905,746 total tokens) deliberately spanning similar disciplines, writing styles, and document structures: Papers ranged from 3 to 112 pages and included technical dense mathematical proofs pertaining to fundamental ML research. All PDFs were converted to clean markdown using¬†[MarkItDown](https://github.com/microsoft/markitdown), with OCR artifacts and single-character fragments stripped before chunking.\n\n# Chunking Strategies Tested\n\n1. **Fixed-size, 512 tokens**, 50-token overlap\n2. **Fixed-size, 1024 tokens**, 100-token overlap\n3. **Recursive character splitting**, LangChain-style¬†`RecursiveCharacterTextSplitter`¬†at 512 tokens\n4. **Semantic chunking**, embedding-based boundary detection (cosine similarity threshold 0.7)\n5. **Document-structure-aware**, splitting on markdown headings/sections, max 1024 tokens\n6. **Page-per-chunk**, one chunk per PDF page, using MarkItDown's form-feed (`\\f`) page boundaries\n7. **Proposition chunking**, LLM-decomposed atomic propositions following¬†[Dense X Retrieval](https://arxiv.org/abs/2312.06648)¬†with the paper's exact extraction prompt\n\nAll chunks were embedded with¬†`text-embedding-3-small`¬†and stored in local ChromaDB. Answer generation used¬†`gemini-2.5-flash-lite`¬†via OpenRouter. We generated 30 ground-truth Q&A pairs using Vecta's synthetic benchmark pipeline.\n\n# Equal Context Budget: Adaptive Retrieval k\n\nMost chunking benchmarks use a fixed top-k (e.g., k=10) for all strategies. This is fundamentally unfair: if fixed-1024 retrieves 10 chunks, the generator sees \\~10,000 tokens of context; if proposition chunking retrieves 10 chunks at 17 tokens each, the generator gets \\~170 tokens. The larger-chunk strategy wins by default because it gets more context, not because its chunking is better.\n\nWe fix this by computing an¬†adaptive k¬†for each strategy. This targets \\~2,000 tokens of retrieved context for every strategy. The computed values:\n\n|Strategy|Avg Tokens/Chunk|Adaptive k|Expected Context|\n|:-|:-|:-|:-|\n|Page-per-Chunk|961|2|\\~1,921|\n|Doc-Structure|937|2|\\~1,873|\n|Fixed 1024|658|3|\\~1,974|\n|Fixed 512|401|5|\\~2,007|\n|Recursive 512|397|5|\\~1,984|\n|Semantic|43|46|\\~1,983|\n|Proposition|17|115|\\~2,008|\n\nNow every strategy gets \\~2,000 tokens to work with. Differences in accuracy reflect genuine chunking quality, not context budget.\n\n# How We Score Retrieval: Precision, Recall, and F1\n\nWe evaluate retrieval at two granularities:¬†page-level¬†(did we retrieve the right pages?) and¬†document-level¬†(did we retrieve the right documents?). At each level, the core metrics are precision, recall, and F1.\n\nLet¬†R¬†be the set of retrieved items (pages or documents) and¬†G¬†be the set of ground-truth relevant items.\n\nPrecision measures: of everything we retrieved, what fraction was actually relevant? A retriever that returns 5 pages, 4 of which contain the answer, has a precision of 0.8. High precision means low noise in the context window.\n\nRecall measures: of everything that¬†*was*¬†relevant, what fraction did we find? If 3 pages contain the answer and we retrieved 2 of them, recall is 0.67. High recall means we're not missing important information.\n\nF1 is the harmonic mean of precision and recall. It penalizes strategies that trade one for the other and rewards balanced retrieval.\n\nPage-level metrics tell you whether you're pulling the right¬†*passages*. Document-level metrics tell you whether you're pulling from the right¬†*sources*. A strategy can score high page-level recall (finding many relevant pages) while scoring low document-level precision (those pages are scattered across too many irrelevant documents). As we'll see, the tension between these two levels is one of the main findings.\n\n# Results\n\n# The Big Picture\n\n[Figure 1:¬†Complete metrics heatmap. Green is good, red is bad.](https://www.runvecta.com/blog/chunking/metrics_heatmap.png)\n\n|Strategy|k|Doc F1|Page F1|Accuracy|Groundedness|\n|:-|:-|:-|:-|:-|:-|\n|**Recursive 512**|5|0.86|**0.92**|**0.69**|0.81|\n|**Fixed 512**|5|0.85|0.88|0.67|**0.85**|\n|**Fixed 1024**|3|**0.88**|0.72|0.61|0.86|\n|**Doc-Structure**|2|0.88|0.69|0.52|0.84|\n|**Page-per-Chunk**|2|0.88|0.69|0.57|0.81|\n|**Semantic**|46|0.42|0.91|0.54|0.81|\n|**Proposition**|115|0.27|**0.97**|0.51|**0.87**|\n\n**Recursive splitting wins on accuracy (69%) and page-level retrieval (0.92 F1).**¬†The 512-token strategies lead on generation quality, while larger-chunk strategies lead on document-level retrieval but fall behind on accuracy.\n\n# Finding 1: Recursive and Fixed Splitting Often Outperforms Fancier Strategies\n\n[Figure 2:¬†Accuracy and groundedness by strategy. Recursive and fixed 512 lead on accuracy.](https://www.runvecta.com/blog/chunking/generation_quality.png)\n\nLangChain's¬†`RecursiveCharacterTextSplitter`¬†at 512 tokens achieved the highest accuracy (**69%**) across all seven strategies. Fixed 512 was close behind at 67%. Both strategies use 5 retrieved chunks for \\~2,000 tokens of context.\n\nWhy does recursive splitting edge out plain fixed-size? It tries to break at natural boundaries, paragraph breaks, then sentence breaks, then word breaks. On academic text, this preserves logical units: a complete paragraph about a method, a full equation derivation, a complete results discussion. The generator gets chunks that make semantic sense, not arbitrary windows that may cut mid-sentence.\n\nRecursive 512 also achieved the best page-level F1 (**0.92**), meaning it reliably finds the right pages¬†*and*¬†produces accurate answers from them.\n\n# Finding 2: The Granularity-Retrieval Tradeoff Is Real\n\n[Figure 3:¬†Radar chart, recursive 512 (orange) has the fullest coverage. Large-chunk strategies skew toward doc retrieval but lose on accuracy.](https://www.runvecta.com/blog/chunking/radar_comparison.png)\n\nWith a 2,000-token budget, a clear tradeoff emerges:\n\n* **Smaller chunks (k=5)**¬†achieve higher accuracy (67-69%) because 5 retrieval slots let you sample from 5 different locations in the corpus, each precisely targeted\n* **Larger chunks (k=2-3)**¬†achieve higher document F1 (0.88) because each retrieved chunk spans more of the relevant document, but the generator gets fewer, potentially less focused passages\n\nFixed 1024 scored the best document F1 (**0.88**) but only 61% accuracy. With just k=3, you get 3 large passages, great for document coverage, but if even one of those passages isn't well-targeted, you've wasted a third of your context budget.\n\n# Finding 3: Semantic Chunking Collapses at Scale\n\n[Figure 4:¬†Chunk size distribution. Semantic and proposition chunking produce extremely small fragments.](https://www.runvecta.com/blog/chunking/chunk_size_distribution.png)\n\nSemantic chunking produced¬†17,481 chunks averaging 43 tokens¬†across 50 papers. With k=46, the retriever samples from 46 different tiny chunks. The result: only¬†54% accuracy¬†and¬†0.42 document F1.\n\nHigh page F1 (0.91) reveals what's happening: the retriever¬†*finds the right pages*¬†by sampling many tiny chunks from across the corpus. But document-level retrieval collapses because those 46 chunks come from dozens of different documents, diluting precision. And accuracy suffers because 46 disconnected sentences don't form a coherent narrative for the generator.\n\n**The fundamental problem:**¬†semantic chunking optimizes for retrieval-boundary purity at the expense of context coherence. Each chunk is a \"clean\" semantic unit, but a single sentence chunk may lack the surrounding context needed for generation.\n\n# Finding 4: The Page-Level Retrieval Story\n\n[Figure 5:¬†Page-level precision-recall tradeoff. Recursive 512 achieves the best balance.](https://www.runvecta.com/blog/chunking/precision_recall.png)\n\n[Figure 6:¬†Page-level and document-level F1. The two metrics tell different stories.](https://www.runvecta.com/blog/chunking/retrieval_f1.png)\n\nPage-level and document-level retrieval tell opposite stories under constrained context:\n\n* Fine-grained strategies¬†(proposition k=115, semantic k=46) achieve high page F1 (0.91-0.97) by sampling many pages, but low doc F1 (0.27-0.42) because those pages come from too many documents\n* Coarse strategies¬†(page-chunk k=2, doc-structure k=2) achieve high doc F1 (0.88) by retrieving fewer, more relevant documents, but lower page F1 (0.69) because 2 chunks can only cover 2 pages\n\nRecursive 512 at k=5 hits the best balance: 0.92 page F1 and 0.86 doc F1. Five chunks is enough to sample multiple relevant pages while still concentrating on a few documents.\n\n[Figure 7:¬†Document-level precision, recall, and F1 detail. Large-chunk strategies lead on precision; fine-grained strategies lead on recall.](https://www.runvecta.com/blog/chunking/document_level.png)\n\n# What This Means for Your RAG System\n\n# The Short Version\n\n1. Use recursive character splitting at 512 tokens.¬†It scored the highest accuracy (69%), best page F1 (0.92), and strong doc F1 (0.86). It's the best all-around strategy on academic text.\n2. Fixed-size 512 is a strong runner-up¬†with 67% accuracy and the highest groundedness among the top performers (85%).\n3. If document-level retrieval matters most, use fixed-1024 or page-per-chunk (0.88 doc F1), but accept lower accuracy (57-61%).\n4. Don't use semantic chunking on academic text.¬†It fragments too aggressively (43 avg tokens) and collapses on document retrieval (0.42 F1).\n5. Don't use proposition chunking for general RAG.¬†51% accuracy isn't production-ready. It's only viable if you value groundedness over correctness.\n6. When benchmarking, equalize the context budget.¬†Fixed top-k comparisons are misleading. Use adaptive k = round(target\\_tokens / avg\\_chunk\\_tokens).\n\n# Why Academic Papers Specifically?\n\nWe deliberately chose to saturate the academic paper region of the embedding space with 50 papers spanning 10+ disciplines. When your knowledge base contains papers that all discuss \"evaluation,\" \"metrics,\" \"models,\" and \"performance,\" the retriever has to make fine-grained distinctions. That's when chunking quality matters most.\n\nIn a mixed corpus of recipes and legal contracts, even bad chunking might work because the embedding distances between domains are large. Academic papers are the¬†*hard case*¬†for chunking, and if a strategy works here, it'll work on easier data too.\n\n# How We Measured This (And How You Can Too)\n\nMy team built¬†[Vecta](https://www.runvecta.com/)¬†specifically to meet the need for precise RAG evaluation software. It generates synthetic benchmark Q&A pairs across multiple semantic granularities, then measures precision, recall, F1, accuracy, and groundedness against your actual retrieval pipeline.\n\nThe benchmarks in this post were generated and evaluated using Vecta's SDK (`pip install vecta`)\n\n# Limitations, Experiment Design, and Further Work\n\nThis experiment was deliberately small-scale: 50 papers, 30 synthetic Q&A pairs, one embedding model, one retriever, one generator. That's by design. We wanted something reproducible that a single engineer could rerun in an afternoon, not a months-long research project. The conclusions should be read with that scope in mind.\n\nSynthetic benchmarks are not human benchmarks.¬†Our ground-truth Q&A pairs were generated by Vecta's own pipeline, which means there's an inherent alignment between how questions are formed and how they're evaluated. Human-authored questions would be a stronger test. That said, Vecta's benchmark generation does produce complex multi-hop queries that require synthesizing information across multiple chunks and document locations, so these aren't trivially easy questions that favor any one strategy by default.\n\nOne pipeline, one result.¬†Everything here runs on¬†text-embedding-3-small, ChromaDB, and¬†gemini-2.5-flash-lite. Swap any of those components and the rankings could shift. We fully acknowledge this. Running the same experiment across multiple embedding models, vector databases, and generators would be valuable follow-up work, and it's on our roadmap.\n\nThe equal context budget is a deliberate constraint, not a flaw.¬†Some readers may object that semantic and proposition chunking are \"meant\" to be paired with rerankers, fusion, or hierarchical aggregation. But if a chunking strategy only works when combined with additional infrastructure, that's important to know. Equal context budgets ensure we're comparing chunking quality at roughly equal generation cost. A strategy that requires a reranker to be competitive is a more expensive strategy, and that should factor into the decision.\n\nSemantic chunking was not intentionally handicapped.¬†Our semantic chunking produced fragments averaging 43 tokens, which is smaller than most production deployments would target. This was likely due to a poorly tuned cosine similarity threshold (0.7) rather than any deliberate sabotage. But that's actually the point: semantic chunking requires careful threshold tuning, merging heuristics, and often parent-child retrieval to work well. When those aren't perfectly dialed in, it degrades badly. Recursive splitting, by contrast, produced strong results with default parameters. The brittleness of semantic chunking under imperfect tuning is itself a finding.\n\n**What we'd like to do next:**\n\n* Rerun the experiment with human-authored Q&A pairs alongside the synthetic benchmark\n* Test across multiple embedding models (`text-embedding-3-large`, open-source alternatives) and generators (GPT-4o, Claude, Llama)\n* Add reranking and hierarchical retrieval stages, then measure whether the rankings change when every strategy gets access to the same post-retrieval pipeline\n* Expand the corpus beyond academic papers to contracts, documentation, support tickets, and other common RAG domains\n* Test semantic chunking with properly tuned thresholds, chunk merging, and sliding windows to establish its ceiling\n\nIf you run any of these experiments yourself, we'd genuinely like to see the results.\n\nHave a chunking strategy that worked surprisingly well (or badly) for you? We'd love to hear about it. Reach out via DM!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdyu2f/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o79553a",
          "author": "250umdfail",
          "text": "Your experiments were designed for the standard chunking strategies to win. All your papers are independent, and share very little context with each other- why would you need a complicated RAG setup? I'm assuming your queries were simple too, like what is x, explain y etc.\n\nIf you have a complicated set of documents, that have a dependence graph, a cross reference network, and ask questions that need traversing the entire knowledge graph, things would be very different. Queries like: how did x evolve over time?, how does y compare to z, explain u in terms of v, etc. are where the advanced chunking, embedding, and linking shine.\n\nAlso I'm not sure F1 scores are a good way to grade your chunking strategies. Most modern systems use a high recall phase followed by a high precision stage over those recalled documents. You could possibly use a human or a larger model to find the similarity between their answer, and that of your model to score your strategies.",
          "score": 17,
          "created_utc": "2026-02-25 02:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79n5vk",
              "author": "Mythril_Zombie",
              "text": "Where is the list of papers?",
              "score": 1,
              "created_utc": "2026-02-25 04:00:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7akqqn",
              "author": "SmihtJonh",
              "text": "Recursive ensemble NER can mitigate some, but I agree that chunking in itself I prone to lossy semantics",
              "score": 1,
              "created_utc": "2026-02-25 08:26:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cgsq8",
              "author": "Confident-Honeydew66",
              "text": "This is a very well-thought-out comment, so first of all thank you.\n\n>\\>If you have a complicated set of documents, that have a dependence graph, a cross reference network, and ask questions that need traversing the entire knowledge graph, things would be very different.\n\nI agree, and acknowledge this is a serious blind spot in this analysis. Currently pushing for a second experiment using a more complex and inter-dependent dataset that requires multi-hop reasoning over similar-looking chunks.\n\n>\\>I'm not sure F1 scores are a good way to grade your chunking strategies.\n\nF1 score is often left behind in favor of recall, but precision, which is admittedly less important, ultimately determines if the downstream LLM is prone to context rot. Ideally, both recall and precision are maximized in the final retrieved chunks, hence our use of F1 as a benchmark.\n\n>\\>Most modern systems use a high recall phase followed by a high precision stage over those recalled documents.\n\nBy the end of these stages, the F1 should still be high. It is irrelevant if the retriever has multiple stages -- our analysis only considers the end result of the retrieval process.",
              "score": 1,
              "created_utc": "2026-02-25 16:01:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7dvscz",
                  "author": "250umdfail",
                  "text": "F1 treats both the recall and precision stages equally. In a multistage system, the stages are dependent and F1 is highly misleading, especially because RAG is a ranking system not a classifier. There are other scores you might want to consider like precision@K or NDCG.",
                  "score": 1,
                  "created_utc": "2026-02-25 19:53:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79ifgi",
          "author": "Caesarr",
          "text": "Interesting analysis. Chunking on paragraphs (i.e. human-defined atomic blocks of semantic meaning) would also be worth testing, no?",
          "score": 2,
          "created_utc": "2026-02-25 03:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ajq0u",
          "author": "Healthy_Library1357",
          "text": "Two quick thoughts:  \nParagraph length variance matters. In papers, some paragraphs are 80 tokens, others 600+. Without a cap + light overlap, you‚Äôll get unstable context budgets and retrieval noise. It might improve coherence but hurt coverage. If avg paragraph is \\~300‚Äì400 tokens, your adaptive k probably lands around 5‚Äì6 for a 2k token budget. That‚Äôs similar to Recursive 512, so gains may be marginal unless paragraphs align unusually well with answer spans.\n\nWould be interesting to see paragraph + max 768 cap vs Recursive 512 head-to-head. My bet: small lift in groundedness, similar accuracy, slightly worse page recall.",
          "score": 1,
          "created_utc": "2026-02-25 08:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7atbpn",
          "author": "ManufacturerWeird161",
          "text": "Finally seeing someone test this instead of just repeating advice. Our team also found that a fixed 512-token approach tanked performance on our legal docs, where citations at the end of sections were critical for context. Your findings on parent-child hierarchies match our internal testing.",
          "score": 1,
          "created_utc": "2026-02-25 09:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1usv",
          "author": "Ok_Bedroom_5088",
          "text": "if you chunk, semantic will always win",
          "score": 1,
          "created_utc": "2026-02-25 11:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmes0",
          "author": "Snoo_24581",
          "text": "This is a great point. Thanks for sharing!",
          "score": 1,
          "created_utc": "2026-02-25 13:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cha4e",
          "author": "Confident-Honeydew66",
          "text": "Saw someone ask for a TLDR in the comments before it got deleted.\n\nTLDR: keep it simple, stupid",
          "score": 1,
          "created_utc": "2026-02-25 16:04:08",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7cjcxl",
          "author": "sophie_zlngr",
          "text": "You've hit your limit ¬∑ resets 4am (Europe/Vienna)",
          "score": 1,
          "created_utc": "2026-02-25 16:13:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjvf2",
          "author": "Suspicious-Name4273",
          "text": "For source code chunking the story might be different. There are semantic chunking strategies for source code that are structure-aware, like this: https://chunkhound.github.io/under-the-hood/#design-philosophy\n\nWould be interesting if this has any real benefit to other chunking strategies or even dumb fulltext search.",
          "score": 1,
          "created_utc": "2026-02-25 16:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cyfs1",
          "author": "sophie_zlngr",
          "text": "You've hit your limit ¬∑ resets 4am (Europe/Vienna)",
          "score": 1,
          "created_utc": "2026-02-25 17:22:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d1a6i",
          "author": "AmphibianNo9959",
          "text": "Really appreciate you sharing this. This equal context budget approach is super smart and something I wish more benchmarks would adopt. It‚Äôs wild how much the best practice advice falls apart under a fair comparison.",
          "score": 1,
          "created_utc": "2026-02-25 17:35:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1repniu",
      "title": "OpenAI is a textbook example of Conway's Law",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1repniu/openai_is_a_textbook_example_of_conways_law/",
      "author": "robertgambee",
      "created_utc": "2026-02-25 20:57:56",
      "score": 28,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "There's a principle in software design called Conway's Law: organizations design systems that mirror their own communication structures (AKA shipping their org charts).\n\nOpenAI has two endpoints which do largely similar things: their older `chat/completions` API and the newer `responses` one. (Not to mention their even older `completions` endpoint that's now deprecated.)\n\nBoth let you generate text, call tools, and produce structured output. And at first glance, they look quite similar. But as you dig deeper, the differences quickly appear. Take structured outputs as an example. With `chat/completions`, you write:\n\n    {\n      \"response_format\": {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n          \"name\": \"Response\",\n          \"description\": \"A response to the user's question\",\n          \"schema\": {\"type\": \"object\", \"properties\": ...}\n        }\n      }\n    }\n\nBut for `responses`, it needs to look like this:\n\n    {\n      \"text\": {\n        \"format\": {\n          \"type\": \"json_schema\",\n          \"name\": \"Response\",\n          \"description\": \"A response to the user's question\",\n          \"schema\": {\"type\": \"object\", \"properties\": ...}\n        }\n      }\n    }\n\nI see no reason why these need to be different. It makes me wonder if they're deliberately making it difficult to migrate from one endpoint to the other. And the docs don't explain this! They only have a couple of examples, at least one of which is incorrect. I had to read the source code in their Python package to figure it out.\n\nGoogle suffers from this too. Their Gemini API rejects JSON Schema with `{\"type\": \"array\", \"items\": {}}` (a valid schema meaning \"array of anything\"). Their official Python package silently rewrites the schema to make it compliant before sending. I like to imagine that someone on the Python package team got fed up with backend team for not addressing this and decided to fix it themselves.\n\nI admit that this isn't surprising for fast-moving orgs who are shipping features quickly. But it does put a lot of burden on developers to deal with lots of little quirks. And it makes me wonder what's going on inside these places.\n\nI wrote up [some more examples](https://everyrow.io/blog/llm-provider-quirks) of odd quirks in LLM provider APIs. Which ones have you had to deal with?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1repniu/openai_is_a_textbook_example_of_conways_law/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7elqpl",
          "author": "BuddhasFinger",
          "text": "Everything is a textbook example of Conway's Law because it's a \\*law\\* :-) \n\nOpenAI is not that bad, actually. Look at Antropic. You can clearly see the point in time when they hired the CRO. ",
          "score": 6,
          "created_utc": "2026-02-25 21:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eja8z",
          "author": "ddp26",
          "text": "I feel like OpenAI does deprecate things a lot (like 4o). Why don't they deprecate the completions one?",
          "score": 0,
          "created_utc": "2026-02-25 21:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f566v",
              "author": "Mysterious-Rent7233",
              "text": "OpenAI does NOT deprecate things in the API that enterprise users depend upon, actually.\n\nDavinci from November 2022, is still available in their APIs:\n\n  \n[https://developers.openai.com/api/docs/models/davinci-002](https://developers.openai.com/api/docs/models/davinci-002)\n\n",
              "score": 4,
              "created_utc": "2026-02-25 23:33:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7emxpk",
              "author": "Purple-Programmer-7",
              "text": "They‚Äôre looking at ALL data.\n\nAnd that includes the number of requests to each endpoint.\n\nThey‚Äôre not going to depreciate anything until a critical mass has migrated.",
              "score": 3,
              "created_utc": "2026-02-25 22:00:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7gvv68",
              "author": "robogame_dev",
              "text": "Deprecating completions API would be nuts because it‚Äôs the #1 interoperable API standard in use across multiple providers / 3rd party tools. \n\nOnce most providers and tools fully support Responses, you could deprecate - but for now, completions is still the #1 API to target if you‚Äôre building production AI systems because if you target Responses, you‚Äôll reduce your interoperability for essentially no gain.",
              "score": 2,
              "created_utc": "2026-02-26 05:54:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rhqmd8",
      "title": "Sleeping LLM: persistent memory for local LLMs through weight editing and sleep consolidation",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rhqmd8/sleeping_llm_persistent_memory_for_local_llms/",
      "author": "vbaranov",
      "created_utc": "2026-03-01 07:02:56",
      "score": 28,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "I built a system where a local LLM learns facts from conversation and retains them across restarts. No RAG, no vector DB, no context stuffing. The knowledge is in the weights.\n\n**How it works:**\n\n* **Wake**: You chat normally. Facts are extracted and injected into MLP weights via MEMIT (Mass-Editing Memory in Transformers). Single forward pass, instant recall, no training.\n* **Sleep**: An 8-step pipeline audits which memories degraded, refreshes them with null-space constraints, then trains LoRA on the active facts and fuses it into the model. Each fact independently tracks whether LoRA absorbed it. If yes, MEMIT dissolves (scale 1.0 ‚Üí 0.5 ‚Üí 0.1 ‚Üí 0.0). If not, MEMIT stays as a safety net.\n\n**Why this was hard:**\n\nMEMIT has a capacity ceiling. The 8B model sustains recall up to \\~13 facts, then collapses at fact 14 (phase transition, not gradual decay). The obvious fix is LoRA consolidation, but RLHF fights back: a single LoRA training pass degrades chat recall by 37% on 8B. I call this the\"alignment tax.\"\n\nThe solution: cumulative fusing. Each sleep cycle trains on the already-fused model from the last cycle. Starting loss drops from 2.91 to 0.62 by cycle 2. The alignment tax is per-pass, not absolute. Multiple small shifts succeed where one big shift fails.\n\n**Results (Llama 3.1 8B, 4-bit, 2√óH100):**\n\n* 100% fact advancement at 5/10/15/20 facts\n* 1.00 chat recall at all scales\n* MEMIT edits dissolve on schedule, buffer is renewable\n* Effective lifetime capacity: unbounded\n\nAlso runs on MacBook Air M3 (3B model, reduced capacity).\n\n**Links:**\n\n* Code:¬†[https://github.com/vbario/sleeping-llm](https://github.com/vbario/sleeping-llm)\n* Paper:¬†[https://doi.org/10.5281/zenodo.18779159](https://doi.org/10.5281/zenodo.18779159)\n* Discussion on LocalLLaMA:¬†[https://www.reddit.com/r/LocalLLaMA/comments/1rewz9p/comment/o7gupjt/](https://www.reddit.com/r/LocalLLaMA/comments/1rewz9p/comment/o7gupjt/)\n\n6 papers covering the full journey. Happy to answer implementation questions.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rhqmd8/sleeping_llm_persistent_memory_for_local_llms/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o80n45n",
          "author": "coloradical5280",
          "text": "this is potentially a nice bridge to Time-Test Training + State Space Models.  In in my pretend toy demo i updated weights during sleeping as well (i've literally never told anyone about this repo, ever, you'll see why [https://github.com/DMontgomery40/ttt\\_ssm\\_eval](https://github.com/DMontgomery40/ttt_ssm_eval) ) but you caught me in a moment of vulnerability i guess. \n\nNice work. ",
          "score": 5,
          "created_utc": "2026-03-01 07:27:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80obeh",
              "author": "vbaranov",
              "text": "This is really interesting...thank you for sharing. I see you even have the chat replaying into the core model in sleep. It looks like there's a convergence to what the optimal (maybe only) memory mechanism can be. I'm going to have to explore your repo more deeply. Downloaded a copy :) I will update you with thoughts soon.",
              "score": 3,
              "created_utc": "2026-03-01 07:38:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o80rhff",
                  "author": "coloradical5280",
                  "text": "Yeah it's a \"toy demo\" in every sense of the term lol, but TTT architecture really interestes me, and a lot of the thought was more around \"how would make this so it isn't instantly breakable by the wrong person with a keyboard\", and ironically, those were the most embarrasing pieces of the whole mini-project.  Like please don't even look at the regex for jailbreaking üòÇ  The idea was more around what TTT (with semi/prentend SSM backbone) would do with it.  \n\nIt's very much in a broken unfinished state, life happened, got busy, etc, you've motivated me to get back at it someday soon though. ",
                  "score": 2,
                  "created_utc": "2026-03-01 08:08:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o80sa4a",
                  "author": "coloradical5280",
                  "text": "what is interesting though is our two differnet approaches to the delta on weights being changed, with your RLHF and my thing just saying \"whoa that gradient diff is a bit extreme I don't like that\".....   And both approaches make sense in context, yours being better for memory, mine obviously focused on safety only and not considering functionality of any other kind really.",
                  "score": 2,
                  "created_utc": "2026-03-01 08:16:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o80m6hk",
          "author": "HarrityRandall",
          "text": "Wow this is very interesting‚Ä¶\n\nI understand it is a kind of fine tuning you are doing? Does it have any side effects on output like with FT? How do you handle that?",
          "score": 1,
          "created_utc": "2026-03-01 07:18:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o80nuc9",
              "author": "vbaranov",
              "text": "This is MEMIT insertion and LoRA optimization, which is slightly different. The main side effect is degradation - a rise in perplexity. If we can control that, we're good :)",
              "score": 3,
              "created_utc": "2026-03-01 07:34:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o81l264",
                  "author": "Tiny_Arugula_5648",
                  "text": "It's a really excellent PoC.. However I think you should be clearer about the degradation problem most people here don't understand the limitations & what the trade offs are. The people in this sub will read this as problem solved and don't understand that it's not really viable for real world use due how the model degrades and how that compounds over time.\n\nAFAIK the general consensus in data science community  is that the transformer architecture can't get continuously trained because it's extremely fragile and cost prohibitive. Until we have a massive change in architecture; there is no overcoming that, it's a foundational challenge. It's good to be clear that this doesn't overcome that limitation.",
                  "score": 3,
                  "created_utc": "2026-03-01 12:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o82skhq",
          "author": "saijanai",
          "text": "Wake me when you implement Transcendental Meditation, not sleep.",
          "score": 1,
          "created_utc": "2026-03-01 16:43:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdmxi9",
      "title": "Are large language models actually generalizing, or are we just seeing extremely sophisticated memorization in a double descent regime?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdmxi9/are_large_language_models_actually_generalizing/",
      "author": "InevitableRespond494",
      "created_utc": "2026-02-24 17:40:25",
      "score": 24,
      "num_comments": 18,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve been trying to sharpen my intuition about large language models and I‚Äôd genuinely appreciate input from people who work in ML or have a strong technical background. I‚Äôm not looking for hype or anti-AI rhetoric, just a sober technical discussion.\n\nHere‚Äôs what I keep circling around:\n\nLLMs are trained on next-token prediction. At the most fundamental level, the objective is to predict the next word given previous context. That means the training paradigm is imitation. The system is optimized to produce text that statistically resembles the text it has seen before. So I keep wondering: if the objective is imitation, isn‚Äôt the best possible outcome simply a very good imitation? In other words, something that behaves as if it understands, while internally just modeling probability distributions over language?\n\nWhen people talk about ‚Äúemergent understanding,‚Äù I‚Äôm unsure how to interpret that. Is that a real structural property of the model, or are we projecting understanding onto a system that is just very good at approximating linguistic structure?\n\nAnother thing that bothers me is memorization versus generalization. We know there are documented cases of LLMs reproducing copyrighted text, reconstructing code snippets from known repositories, or instantly recognizing classic riddles and bias tests. That clearly demonstrates that memorization exists at non-trivial levels. My question is: how do we rigorously distinguish large-scale memorization from genuine abstraction? When models have hundreds of billions of parameters and are trained on massive internet-scale corpora, how confident are we that scaling is producing true generalization rather than a more distributed and statistically smoothed form of memorization?\n\nThis connects to overfitting and double descent. Classical ML intuition would suggest that when model capacity approaches or exceeds dataset complexity, overfitting becomes a serious concern. Yet modern deep networks, including LLMs, operate in highly overparameterized regimes and still generalize surprisingly well. The double descent phenomenon suggests that after the interpolation threshold, performance improves again as capacity increases further. I understand the empirical evidence for double descent in various domains, but I still struggle with what that really means here. Is the second descent genuinely evidence of abstraction and structure learning, or are we simply in a regime of extremely high-dimensional interpolation that looks like generalization because the data manifold is densely covered?\n\nThen there‚Äôs the issue of out-of-distribution behavior. In my own experiments, when I formulate problems that are genuinely new, not just paraphrased or slightly modified from common patterns, models often start to hallucinate or lose coherence. Especially in mathematics or formal reasoning, if the structure isn‚Äôt already well represented in the training distribution, performance degrades quickly. Is that a fundamental limitation of text-only systems? Is it a data quality issue? A scaling issue? Or does it reflect the absence of a grounded world model?\n\nThat leads to the grounding problem more broadly. Pure language models have no sensorimotor interaction with the world. They don‚Äôt perceive, manipulate, or causally intervene in physical systems. They don‚Äôt have multimodal grounding unless explicitly extended. Can a system trained purely on text ever develop robust causal understanding, or are we mistaking linguistic coherence for a world model? When a model explains what happens if you tilt a table and a phone slides off, is it reasoning about physics or statistically reproducing common narrative patterns about objects and gravity?\n\nI‚Äôm also curious about evaluation practices. With web-scale datasets, how strictly are training and evaluation corpora separated? How do we confidently prevent benchmark contamination when the training data is effectively ‚Äúthe internet‚Äù? In closed-source systems especially, how much of our trust relies on company self-reporting? I‚Äôm not implying fraud, but the scale makes rigorous guarantees seem extremely challenging.\n\nThere‚Äôs also the question of model size relative to data. Rough back-of-the-envelope reasoning suggests that the total volume of publicly available text on the internet is finite and large but not astronomically large compared to modern parameter counts. Given enough capacity, is it theoretically possible for models to internally encode enormous portions of the training corpus? Are LLMs best understood as knowledge compressors, as structure learners, or as extremely advanced semantic search systems embedded in a generative architecture?\n\nBeyond the technical layer, I think incentives matter. There is massive economic pressure in this space. Investment cycles, competition between companies, and the race narrative around AGI inevitably shape communication. Are there structural incentives that push capability claims upward? Even without malicious intent, does the funding environment bias evaluation standards or public framing?\n\nFinally, I wonder how much of the perceived intelligence is psychological. Humans are extremely prone to anthropomorphize coherent language. If a system speaks fluently and consistently, we instinctively attribute intention and understanding. To what extent is the ‚Äúwow factor‚Äù a cognitive illusion on our side rather than a deep ontological shift on the model‚Äôs side?\n\nAnd then there‚Äôs the resource question. Training and deploying large models consumes enormous computational and energy resources. Are we seeing diminishing returns masked by scale? Is the current trajectory sustainable from a systems perspective?\n\nSo my core question is this: are modern LLMs genuinely learning abstract structure in a way that meaningfully transcends interpolation, or are we observing extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime that happens to look intelligent?\n\nI‚Äôd really appreciate technically grounded perspectives. Not hype, not dismissal, just careful reasoning from people who‚Äôve worked close to these systems.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdmxi9/are_large_language_models_actually_generalizing/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o76bwrm",
          "author": "hymn_7-62",
          "text": "You raise good questions and I'm interested in answers, sadly I dont think we'll get lucky with someone who actually knows their shit.",
          "score": 8,
          "created_utc": "2026-02-24 17:55:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76jy1x",
          "author": "McMonty",
          "text": "First off: Great post!\n\nMy Ask: You'll need to define\n\n\\> meaningfully transcends interpolation\n\nI think a lot of research in the AI field was in this area during the early AI stages that pre-dated NNs decades ago. Personally, I've always liked Hofstadter's takes on AI such as those in \"I am a strange loop\". I doubt you'll find much better answers to \"what even is generalization\" than in his writing(GEB and \"Surfaces and Essences\" are also great!).  \n  \nBut although he was initially skeptical of LLMs, he has changed his tone a bit in these past few years to start to question if the recursive elements present in LLMs has hit a turning point where we should be questioning what it is that we've created: [https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai](https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai)\n\nMy own 2 cents: There is something to LLMs beyond just memorization, but its constrained still in a way that differs from how our own brains are constrained(Ultimately, our own ability to generalize is still subject to limits). I might even go as far as to say that I'd consider LLMs to be \"capable of consciousness\" to some extent - although I don't think I'd say that they are \"alive\". They are in a weird space where all of our definitions start to break down and are severely lacking nuance to describe the variety of possible forms of cognition. Similar things happen when you really peel back the layers between different forms of animal minds and compare them with human ones, but this is even weirder.",
          "score": 8,
          "created_utc": "2026-02-24 18:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78kcxf",
          "author": "PresentSituation8736",
          "text": "1. The \"World Model\" vs. High-Dimensional Interpolation You asked if models are genuinely learning abstract structure or just operating in an overparameterized interpolation regime. The consensus among interpretability researchers (looking at things like mechanistic interpretability and induction heads) is: It‚Äôs both, but leaning heavily toward sophisticated interpolation. LLMs do learn abstract representations. They don't just memorize strings of text; they build latent features for concepts (e.g., a \"gender\" direction, a \"formality\" vector, or coding syntax trees). To predict the next token efficiently across petabytes of data, the network must compress the data. And the best way to compress data is to discover the underlying generative rules. However, this does not equal a causal \"World Model.\" When the model describes a phone sliding off a tilted table, it is not running a physics engine in its latent space. It is navigating the semantic topology of how humans talk about physics. This is why LLMs fail so catastrophically on Out-of-Distribution (OOD) reasoning, spatial tasks, or novel math. If the solution isn't densely represented in the training manifold, the model cannot extrapolate. It can only interpolate. \n\n2. Memorization vs. Abstraction (The Double Descent Reality) You brought up double descent. In the overparameterized regime, models perfectly fit the training data (memorization) and then find the \"simplest\" function that interpolates between those points (generalization). But here is the dirty secret of modern LLMs: the training data is so massive that the \"data manifold\" covers almost every common human thought. What looks like zero-shot generalization to us is often just the model finding a latent bridge between two memorized concepts. It is \"generalizing,\" but strictly within the convex hull of human internet text. \n\n3. The Benchmark Contamination Crisis You asked: \"How strictly are training and evaluation corpora separated?\" They aren't. This is the biggest open secret in the industry right now. With web-scale scraping, almost every classic riddle, math problem, and coding test is in the training data. Companies try to de-duplicate and filter, but it is practically impossible to prevent \"data leakage\" entirely. Many \"emergent capabilities\" reported in 2023 were later debunked as the models simply having seen the test set during training. This is why closed-source claims must be taken with a massive grain of salt.\n\n 4. The Anthropomorphic Illusion & Incentives Your point about the ELIZA effect (anthropomorphism) is the psychological engine driving the current hype cycle. We are evolutionarily hardwired to attribute consciousness to fluent language. When an LLM uses the word \"I\", our brains immediately project a mind onto it. Combine this cognitive bias with the VC funding environment, and you get a toxic incentive structure. Companies are incentivized to frame sophisticated statistical pattern-matching as \"sparks of AGI\" because that unlocks billions in computing budgets. If they admitted, \"We built a lossy, trillion-parameter semantic search engine,\" the valuations would crash. The Conclusion To answer your core question: Modern LLMs are highly advanced, lossy knowledge compressors. They do learn structural abstractions of language (grammar, tone, logic structures), but they use these structures to perform statistical pattern completion. \n\nThey lack grounded causality, they cannot reliably extrapolate outside their training distribution, and their \"reasoning\" is a simulation driven by the linguistic shadows of human thought. It is a breathtaking engineering achievement, but your intuition is correct: we are largely confusing linguistic coherence for ontological intelligence. Keep pulling on these threads. The industry needs this level of skepticism right now.",
          "score": 3,
          "created_utc": "2026-02-25 00:20:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ceova",
              "author": "AI-Agent-geek",
              "text": "Great response",
              "score": 1,
              "created_utc": "2026-02-25 15:52:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77tjxi",
          "author": "Bulky-Flamingo9898",
          "text": ">In other words, something that behaves as if it understands, while internally just modeling probability distributions over language?\n\nI think ‚Äúbehaves as if it understands‚Äù isn‚Äôt really distinguishable from mimicking language patterns in general. So as the better the models get at generating language similar to the training set it will inevitably sound more human and as though it understands. \n\n>are we projecting understanding onto a system that is just very good at approximating linguistic structure?\n\nPartly that but the models do seem to have properties that seem to imply they are doing more than simply spewing back  training material. One thing that struck me early is how well llms seem to be able to rhyme and so if you ask it for a song it will create awful doggrel but it does rhyme. Hard to square the behaviour without thinking that it must in some sense be storing information about the sounds of words along with meaning and is able to invoke this in certain contexts. Not sure this has to be understanding but it‚Äôs related and seems deeper than the stochastic parrot caricature  \n\n>Or does it reflect the absence of a grounded world model? \n\nI would maybe characterise the optimistic view as being if you feed a big enough model enough data it will work out something near a world model itself but as you point out memoization is also happening and what seems to be tough is to encourage good world model building. Training purely for next word probably is close to diminishing returns \n\n>are modern LLMs genuinely learning abstract structure in a way that meaningfully transcends interpolation, or are we observing extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime that happens to look intelligent? \n\nThere seems to be something more than straight interpolation going on, but not enough to make me think AGI is just around the corner",
          "score": 2,
          "created_utc": "2026-02-24 21:59:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79yfxa",
          "author": "MrRandom04",
          "text": "A well-supported rebuttal to the idea that autoregressive language models cannot really learn global reasoning, planning and abstraction: https://arxiv.org/abs/2512.15605",
          "score": 2,
          "created_utc": "2026-02-25 05:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ysup",
              "author": "MrRandom04",
              "text": "This paper, combined with the idea that sufficiently advanced broad RLVR post-training can allow well-documented generalization of capabilities to reach past human expert levels, is essentially the real research bet that the frontier labs are making with their current strides towards AGI.",
              "score": 2,
              "created_utc": "2026-02-25 05:19:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77xai4",
          "author": "theOmnipotentKiller",
          "text": "I think your perspective is based on how GPT-3 (circa 2022) was trained. Saying that LLMs do just next token prediction implies that pre-training is the only thing that matters. \n\nWe have gone through 4 distinct phases in post training since then\n- RLHF\n- structured json grammars\n- test-time search\n- (now) reinforcement learning on tool call sequences\n\nThis should make your question a lot simpler. Next token predictor GPT-3 still felt like BERT. Models today are so different. The above list I gave is like the top highlights of post training, there‚Äôs a lot more going on that we prolly don‚Äôt even know. World models are being actively used to do better RL for agents right now for example.\n\nI think to understand pre-training you have to understand DPO. Next token prediction captured a lot of interesting behaviors in hard to elicit ways. Everything after has been a slow grind of finding the right eval harness and collect enough data to make that micro-behavior a macro-behavior through painstaking manual effort and hopefully some synthetic generation hacks. \n\nAs for true generalization, my only metric for that is how much revenue will Anthropic print per sector of the economy. I am an empiricist and I think the free markets will let you know if things are generalizing or not. It‚Äôs easy to fall for investor posturing optics so you really have to dig to know if they are. Anthropic for the most part has been honest in their communications based on what I have seen on the ground, other labs don‚Äôt share as much as them. \n\nThis question is much better suited for r/mlscaling - you‚Äôll get better answers there. Model training is a gated profession so us LLM Devs can just conjecture and hope the next models just work. Evals went out the window in mid 2025 so it‚Äôs all just vibes here now. Learning theory and all that is tech we hope the labs figure out.",
          "score": 3,
          "created_utc": "2026-02-24 22:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76qew6",
          "author": "visarga",
          "text": "\\> extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime\n\nan extremely sophisticated dunce that is fooling everyone? ",
          "score": 1,
          "created_utc": "2026-02-24 18:59:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77kb64",
              "author": "pab_guy",
              "text": "I think it's hard for people to accept that with intelligence, the map IS the territory.",
              "score": 2,
              "created_utc": "2026-02-24 21:17:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79op5m",
          "author": "thisdude415",
          "text": "IMO the fact that LLMs are able to easily work with random UUIDs (which are by definition never before seen in their training data) demonstrates that there is something beyond memorization.",
          "score": 1,
          "created_utc": "2026-02-25 04:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7yth8c",
          "author": "midaslibrary",
          "text": "Imagine an advanced compression algorithm. It only stores every nth pixel of an image, we‚Äôll call these features. It can reconstruct the image sensibly from said features. But it also constructs novel, sensible images from those features. \n\nWhat you asked at the end was much more sophisticated tho. Are you familiar with manifold topology and the idea of a universal function approximator?",
          "score": 1,
          "created_utc": "2026-02-28 23:56:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o773u2j",
          "author": "dmter",
          "text": "neural networks are just emergent virtual machines that utilize layer machinery to emerge code that satisfies the training data\n\nf.ex. in some image processing nn there are actual image processing algorithms running between layers and nn learns to process input images by giving correct parameters to these algorithms and then it does some math on those results which is also emergent.\n\nsame is done inside llm but unlike image processing, people have no idea how it works so they just assume it's magic. hence hyperscaling fallacy.",
          "score": 1,
          "created_utc": "2026-02-24 20:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77k5ad",
          "author": "pab_guy",
          "text": "TL;DR\n\nHowever, note that your entire perceptual world as a human is recalling learned patterns.  The magic happens when we combine or exchange ideas across disciplines.\n\nFor background on this type of thing I recommend \"Everything is a remix\" and the  veritasium video on expertise.\n\nOne example: a chess grandmaster can memorize pieces on a chessboard very well.  But if you put pieces on the board in a way that doesn't reflect how a real game would play out (novel placements) the grandmaster's advantage evaporates.  The skill is based on memorizing and recognizing patterns.",
          "score": 0,
          "created_utc": "2026-02-24 21:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77oiy7",
              "author": "Officer_Trevor_Cory",
              "text": "\"One example: a chess grandmaster can memorize pieces on a chessboard very well. But if you put pieces on the board in a way that doesn't reflect how a real game would play out (novel placements) the grandmaster's advantage evaporates. The skill is based on memorizing and recognizing patterns.\"\n\nwell this is not a good example. grandmasters play freestyle, start from a massive advantage and adapt extremely quickly.",
              "score": 1,
              "created_utc": "2026-02-24 21:36:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77wa1x",
                  "author": "pab_guy",
                  "text": "How do you think they adapt extremely quickly? Because they know the patterns!",
                  "score": 1,
                  "created_utc": "2026-02-24 22:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rg1yuj",
      "title": "Convert any web page to markdown and save crazy tokens",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rg1yuj/convert_any_web_page_to_markdown_and_save_crazy/",
      "author": "Safe_Ad_8485",
      "created_utc": "2026-02-27 09:18:16",
      "score": 22,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "As an AI builder, I've been frustrated with how bloated HTML from web pages eats up LLM tokens, think feeding a full Wikipedia article to Grok or Claude and watching your API costs skyrocket. LLMs love clean markdown, so I created **web-to-markdown**, a simple NPM package that scrapes and converts any webpage to optimized markdown.\n\n# Quick Install & Use\n\n    npm i web-to-markdown\n\nThen in your code:\n\nJavaScript\n\n    const { convertWebToMarkdown } = require('web-to-markdown');\n    \n    convertWebToMarkdown('https://example.com').then(markdown => {\n      console.log(markdown);\n    });\n\n# Shocking Benchmarks\n\nI ran tests on popular sites like Kubernetes documentation.\n\nFull demo and results in this video: [Original Announcement on X](https://x.com/nidhisinghattri/status/2026942204774895773)\n\n# Update: Chrome Extension Coming Soon!\n\nJust shipped a Chrome extension version for one-click conversions, it's in review and should be live soon. Stay tuned! [Update Post on X](https://x.com/nidhisinghattri/status/2027307842311802990)\n\nThis is open-source and free hence feedback welcome! \n\nNPM: [web-to-markdown on NPM](https://www.npmjs.com/package/web-to-markdown) \n\nThanks for checking it out! ",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rg1yuj/convert_any_web_page_to_markdown_and_save_crazy/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7olvph",
          "author": "Tema_Art_7777",
          "text": "How does it deal with javascript in web pages?",
          "score": 2,
          "created_utc": "2026-02-27 11:20:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7pwjuf",
              "author": "Safe_Ad_8485",
              "text": "for rendered javascript, using a headless Chromium instance. there's a flag --browser, you can find the details in the README for this.",
              "score": 2,
              "created_utc": "2026-02-27 15:55:03",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7opfqu",
              "author": "SleepAffectionate268",
              "text": "you don't need js for AI",
              "score": -3,
              "created_utc": "2026-02-27 11:49:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7p1a5f",
                  "author": "diabloman8890",
                  "text": "Still need to see the rendered JS content, not just the HTML, is what I think they meant",
                  "score": 2,
                  "created_utc": "2026-02-27 13:11:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oaf4b",
          "author": "Charming_Support726",
          "text": "I regularly use this one: [https://www.reddit.com/r/mcp/comments/1qknhxi/from\\_searxngmcp\\_to\\_searxncrawl/](https://www.reddit.com/r/mcp/comments/1qknhxi/from_searxngmcp_to_searxncrawl/)\n\nIt provides MCP and CLI for converting to markdown and privacy aware searching",
          "score": 1,
          "created_utc": "2026-02-27 09:36:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oe2gk",
              "author": "Safe_Ad_8485",
              "text": "Thanks for sharing, going to check this out.   \n  \nThe above tool is available as a CLI, and chrome extension is in the pipeline.",
              "score": 1,
              "created_utc": "2026-02-27 10:11:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7q97my",
          "author": "L8_Bloom3r",
          "text": "remindme! 1 month",
          "score": 1,
          "created_utc": "2026-02-27 16:54:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7q9dga",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 month on [**2026-03-27 16:54:37 UTC**](http://www.wolframalpha.com/input/?i=2026-03-27%2016:54:37%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LLMDevs/comments/1rg1yuj/convert_any_web_page_to_markdown_and_save_crazy/o7q97my/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLLMDevs%2Fcomments%2F1rg1yuj%2Fconvert_any_web_page_to_markdown_and_save_crazy%2Fo7q97my%2F%5D%0A%0ARemindMe%21%202026-03-27%2016%3A54%3A37%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201rg1yuj)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-27 16:55:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7reqta",
          "author": "This_Organization382",
          "text": "> think feeding a full Wikipedia article to Grok or Claude and watching your API costs skyrocket. \n\nWikipedia offers an API...",
          "score": 1,
          "created_utc": "2026-02-27 20:14:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7uj4kz",
              "author": "Safe_Ad_8485",
              "text": "API for ?",
              "score": 0,
              "created_utc": "2026-02-28 08:12:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rnp1w",
          "author": "No-Cucumber4564",
          "text": "I use defuddle and readibility in the JS ecosystem. From my experience defuddle is more aggresive, which might result in problems sometimes but saves tokens :)",
          "score": 1,
          "created_utc": "2026-02-27 20:59:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86yw12",
          "author": "Swimming-Chip9582",
          "text": "How does this compare to existing projects? Is there any reason why this is better than the existing alternatives?\n\n[https://github.com/mixmark-io/turndown](https://github.com/mixmark-io/turndown)  \n",
          "score": 1,
          "created_utc": "2026-03-02 07:28:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg5ybv",
      "title": "8 AI Agent Concepts I Wish I Knew as a Beginner",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rg5ybv/8_ai_agent_concepts_i_wish_i_knew_as_a_beginner/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-27 12:57:28",
      "score": 17,
      "num_comments": 9,
      "upvote_ratio": 0.74,
      "text": "Building an AI agent is easy. Building one that actually works reliably in production is where most people hit a wall.\n\nYou can spin up an agent in a weekend. Connect an LLM, add some tools, include conversation history and it seems intelligent. But when you give it real workloads it starts overthinking simple tasks, spiraling into recursive reasoning loops, and quietly multiplying API calls until costs explode.\n\nBeen building agents for a while and figured I'd share the architectural concepts that actually matter when you're trying to move past prototypes.\n\nMCP is the universal plugin layer: Model Context Protocol lets you implement tool integrations once and any MCP-compatible agent can use them automatically. Think API standardization but for agent tooling. Instead of custom integrations for every framework you write it once.\n\nTool calling vs function calling seem identical but aren't: Function calling is deterministic where the LLM generates parameters and your code executes the function immediately. Tool calling is iterative where the agent decides when and how to invoke tools, can chain multiple calls together, and adapts based on intermediate results. Start with function calling for simple workflows, upgrade to tool calling when you need iterative reasoning.\n\nAgentic loops and termination conditions are where most production agents fail catastrophically:The decision loop continues until task complete but without proper termination you get infinite loops, premature exits, resource exhaustion, or stuck states where agents repeat failed actions indefinitely. Use resource budgets as hard limits for safety, goal achievement as primary termination for quality, and loop detection to prevent stuck states for reliability.\n\nMemory architecture isn't just dump everything in a vector database: Production systems need layered memory. Short-term is your context window. Medium-term is session cache with recent preferences, entities mentioned, ongoing task state, and recent failures to avoid repeating. Long-term is vector DB. Research shows lost-in-the-middle phenomenon where information in the middle 50 percent of context has 30 to 40 percent lower retrieval accuracy than beginning or end.\n\nContext window management matters even with 200k tokens: Large context doesn't solve problems it delays them. Information placement affects retreval. First 10 percent of context gets 87 percent retrieval accuracy. Middle 50 percent gets 52 percent. Last 10 percent gets 81 percent. Use hierarchical structure first, add compression when costs matter, reserve multi-pass for complex analytical tasks.\n\nRAG with agents requires knowing when to retrieve: Before embedding extract structured information for better precision, metadata filtering, and proper context. Auto-retrieve always has high latency and low precision. Agent-directed retrieval has variable latency but high precision. Iterative has very high latency but very high precision. Match strategy to use case.\n\nMulti-agent orchestration has three main patterns: Sequential pipeline moves tasks through fixed chain of specialized agents, works for linear workflows but iteration is expensive. Hierarchical manager-worker has coordinator that breaks down tasks and assigns to workers, good for parallelizable problems but manager needs domain expertise. Peer-to-peer has agents communicating directly, flexible but can fall into endless clarification loops without boundaries.\n\nProduction readiness is about architecture not just models: Standards like MCP are emerging, models getting cheaper and faster, but the fundamental challenges around memory management, cost control, and error handling remain architectural problems that frameworks alone won't solve.\n\nAnyway figured this might save someone else the painful learning curve. These concepts separate prototypes that work in demos from systems you can actually trust in production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rg5ybv/8_ai_agent_concepts_i_wish_i_knew_as_a_beginner/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7p88db",
          "author": "Virtual_Substance_36",
          "text": "Wait what tool calling and function calling are different? I don't think so",
          "score": 10,
          "created_utc": "2026-02-27 13:51:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uw4i1",
          "author": "lucasbennett_1",
          "text": "agentic loops sound easy on paper but they wreck most production setups.. without tight termination rules the agent just spins and racks up costs until you kill it manually..\n\nmax steps plus goal check every few turns plus repeat action detection is the bare minimum.. function calling keeps things deterministic early on before jumping to full tool calling.",
          "score": 1,
          "created_utc": "2026-02-28 10:17:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o86p2bu",
          "author": "Own_Professional6525",
          "text": "These insights on production-grade agents are really practical. Curious, in your experience, which single factor-memory architecture, loop termination, or multi-agent orchestration-tends to cause the most failures in real deployments?",
          "score": 1,
          "created_utc": "2026-03-02 06:01:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8747pf",
          "author": "nikunjverma11",
          "text": "This is solid. Most people underestimate termination conditions and memory layering until they watch costs spike. I‚Äôve found separating planning from execution helps a lot too ‚Äî spec and constraints in Traycer, then Claude or GPT handles the loop with clear budgets and exit criteria. Architecture beats model choice every time.",
          "score": 1,
          "created_utc": "2026-03-02 08:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pjp5r",
          "author": "kubrador",
          "text": "\"building an ai agent is easy\" followed by eight paragraphs of why it's actually not is some classic \"actually it's complicated\" energy",
          "score": 0,
          "created_utc": "2026-02-27 14:52:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7r69sb",
              "author": "saintpetejackboy",
              "text": "Eh, I dunno, I thought the post was super basic just common sense stuff - this is like \"intro to agents\" level stuff, at best - very surface level and doesn't touch on any of the actual complex stuff. It is lengthy, but all the stuff is very basic.",
              "score": 2,
              "created_utc": "2026-02-27 19:31:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ryj2q",
                  "author": "MrBigStuff1r",
                  "text": "What is the actual complex stuff?¬†",
                  "score": 2,
                  "created_utc": "2026-02-27 21:53:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7oz6ap",
          "author": "Independent-Cost-971",
          "text": "I explained these visually in a blog if anyone's interested:¬†[https://kudra.ai/8-ai-agent-concepts-every-ai-developer-needs-in-2026-visually-explained/](https://kudra.ai/8-ai-agent-concepts-every-ai-developer-needs-in-2026-visually-explained/)",
          "score": 0,
          "created_utc": "2026-02-27 12:57:52",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnc0f",
      "title": "Running RAG on 512MB RAM: OOM Kills, Deadlocks, Telemetry Bugs and the Fixes",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/f9r207sydhlg1",
      "author": "Lazy-Kangaroo-573",
      "created_utc": "2026-02-24 17:54:37",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Discussion üí≠ ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdnc0f/running_rag_on_512mb_ram_oom_kills_deadlocks/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7el8gj",
          "author": "Everlier",
          "text": "I'm advising against using LangChain where I can, yours is another example where they crested meaningless abstraction that only adds to the complexity overhead while covering a very simple operation",
          "score": 2,
          "created_utc": "2026-02-25 21:52:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gej3n",
              "author": "Lazy-Kangaroo-573",
              "text": "Glad to see someone who gets the real production pain!",
              "score": 2,
              "created_utc": "2026-02-26 03:52:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdk8vu",
      "title": "Giving AI agents direct access to production data feels like a disaster waiting to happen",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdk8vu/giving_ai_agents_direct_access_to_production_data/",
      "author": "Then_Respect_1964",
      "created_utc": "2026-02-24 16:04:57",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "I've been building AI agents that interact with real systems (databases, internal APIs, tools, etc.)\n\nAnd I can't shake this feeling that we're repeating early cloud/security mistakes‚Ä¶ but faster.\n\nRight now, most setups look like:\n- give the agent database/tool access\n- wrap it in some prompts\n- maybe add logging\n- hope it behaves\n\nThat's‚Ä¶ not a security model.\n\nIf a human engineer had this level of access, we'd have:\n- RBAC / scoped permissions\n- approvals for sensitive actions\n- audit trails\n- data masking (PII, financials, etc.)\n- short-lived credentials\n\nBut for agents?\n\nWe're basically doing:\n\n> \"hey GPT, please be careful with production data\"\n\nThat feels insane.\n\nSo I started digging into this more seriously and experimenting with a different approach:\n\nInstead of trusting the agent, treat it like an untrusted actor and put a control layer in between.\n\nSomething that:\n- intercepts queries/tool calls at runtime\n- enforces policies (not prompts)\n- can require approval before sensitive access\n- masks or filters data automatically\n- issues temporary, scoped access instead of full credentials\n\nBasically:\n\ndon't let the agent *touch* real data unless it's explicitly allowed.\n\nCurious how others are thinking about this.\n\nIf you're running agents against real data:\n- are you just trusting prompts?\n- do you have any real enforcement layer?\n- or is everyone quietly accepting the risk right now?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdk8vu/giving_ai_agents_direct_access_to_production_data/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o75rriw",
          "author": "cmh_ender",
          "text": "agreed, boundries are crazy important. look at this video (tech with tim) he deployed clawbot but put a lot of safe guards in place.\n\n[https://www.youtube.com/watch?v=NO-bOryZoTE](https://www.youtube.com/watch?v=NO-bOryZoTE)\n\n  \nWe use ai agents with our code base right now but they can't (no permission) to approve prs, so they can create new branches and tag humans for review but can't actually deploy anything. that's been very helpful in keeping down mistakes. \n\n",
          "score": 5,
          "created_utc": "2026-02-24 16:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75qs3i",
          "author": "Fulgren09",
          "text": "I was an MCP doomer for months until I had the bright idea to build a conversational UI for my app.¬†\n\nAfter days of agonizingly building protocols that explain the api orchestration to accomplish task in my app, it works with Claude Sonnet.¬†\n\nWhat I learned is whoever is exposing their system to an external AI will have strong opinions on which paths it can walk in and which rooms it can enter.¬†\n\nNot saying it‚Äôs 100% fool proof but the experience of building this and the power of conversational UI gave me a lot of confidence that ppl aren‚Äôt just opening up their app free for all style.¬†\n",
          "score": 2,
          "created_utc": "2026-02-24 16:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75rh3s",
          "author": "DryRelationship1330",
          "text": "agree. give it to an employee who leaves the USB key of it at Panera, can't write an expression in excel that doesn't violate order-of-operations and sends a PDF of it to his co-workers to pick up the work...",
          "score": 1,
          "created_utc": "2026-02-24 16:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76752e",
          "author": "GullibleNarwhal",
          "text": "I am a tech-savvy non-coder who has been vibe-coding lately (geez that's a lot of hyphens) and I am terrified of integrating agents, or providing an agent accidentally with permissions that would potentially allow it to wipe a drive accidentally or worse. I am curious everyone's thoughts on if me prompting to provide read-only access to an agentic model running locally if it is truly constraining an agentic model.\n\nFrom what I have heard, if you are not truly sandboxing and running an agent via a VPS with its own accounts you specifically created for it, you are asking for trouble. Thoughts? Am I being gaslit by AI telling me I am properly safe-guarding agent implementation?\n\n![gif](giphy|iRcpZYWqYcJDuPMICy)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 17:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76toas",
          "author": "Maleficent_Pair4920",
          "text": "You need an audit log for each time an Agent has touched prod data same as humans ",
          "score": 1,
          "created_utc": "2026-02-24 19:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79r50w",
          "author": "kdhkillah",
          "text": "Deterministic layers of security are absolutely essential, but yes it seems like many are just trusting prompts (+ tools and any libraries the agents decide to pull), too caught up in hype to acknowledge the risks.  2026 is going to be full of bonkers breaches & skill/tool/MCP injections. This [npm package hallucination](https://www.aikido.dev/blog/agent-skills-spreading-hallucinated-npx-commands) article was eye opening for me last month.",
          "score": 1,
          "created_utc": "2026-02-25 04:26:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bk2tu",
          "author": "Efficient_Loss_9928",
          "text": "The enforcement layer is the human engineer's permission.\n\nI don't know why would you ever grant LLMs system admin access, you always provide it the same permission as the user.\n\nAnd honestly I have never seen a setup like that, it is always delegated access so agents inherit the person's access rights. Can you provide some concrete examples? Like that is so weird, I have worked with companies from startups to military, I have never seen people just grant LLMs non-delegated permissions.",
          "score": 1,
          "created_utc": "2026-02-25 13:15:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c2s0t",
          "author": "DecodeBytes",
          "text": "You may want to checkout [https://github.com/always-further/nono](https://github.com/always-further/nono) \\- disclaimer, one of the maintainers",
          "score": 1,
          "created_utc": "2026-02-25 14:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ddb79",
          "author": "TroubledSquirrel",
          "text": "You're not wrong and the comparison to early cloud security is apt, we're in the \"just open port 22 to everything and we'll fix it later\" phase of agent development.\n\nI've been building a memory and context system that agents interact with and security couldn't be an afterthought because the system handles sensitive personal and professional context across sessions. So I had to actually solve some of this rather than defer it.\n\nA few things I learned building it out:\n\nPrompt-based trust is not a security model, it's a liability. The enforcement has to happen at the infrastructure layer, not the instruction layer. An agent that's told to be careful with PII will be careful right up until it isn't, and you won't know which time that is in advance.\n\nPolicy engines need to be separate from the agent entirely. I ended up building a policy layer that intercepts before any data touches the agent, not just logging what happened but actively making decisions about what the agent is allowed to see in the first place. The agent operates on already-filtered context, not raw data.\n\nPII scrubbing at ingestion rather than at output is the move. By the time you're trying to mask data on the way out you've already lost. Strip it or tokenize it before it enters the system.\n\nAudit trails need to be tamper-evident not just present. Logs you can modify are just a false sense of security. I implemented hash chained audit records so any tampering is detectable.\n\nThe hardest part honestly isn't the technical implementation. It's that most people building agents right now are moving fast enough that security feels like a tax on velocity. Until something goes wrong it's invisible work.\n\nThe risk isn't being quietly accepted so much as quietly not thought about yet.",
          "score": 1,
          "created_utc": "2026-02-25 18:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fx7u4",
          "author": "Aggressive_Poet3228",
          "text": "You‚Äôre describing exactly the right architecture. Treating the agent as an untrusted actor with a control layer between it and system action is the key insight.\nI‚Äôve been building this exact approach as an open source project. The core principle is the same as yours: the model proposes, the governance layer decides.\nWhat‚Äôs implemented:\n\t‚àô\tEvery action passes through a policy engine before execution. The model can‚Äôt bypass it\n\t‚àô\tActions are classified into risk tiers (T0 through T3) with proportional governance. Low-risk actions get O(1) cache lookups. Irreversible actions (delete, deploy, send) require explicit owner approval\n\t‚àô\tPII redaction runs through a local model before any content reaches external APIs. It operates on content patterns, not on the model‚Äôs interpretation of intent\n\t‚àô\tEvery action produces a durable receipt. If there‚Äôs no receipt, it didn‚Äôt happen\n\t‚àô\tThe governance document is immutable at runtime. The running system cannot weaken its own constraints\nThe Agents of Chaos paper published this week red-teamed agents without any of these controls and documented 11 vulnerability classes. I did a full case-by-case mapping of each failure to the architectural defense that would have caught it: https://projectlancelot.dev/answer-to-chaos\nRepo: https://projectlancelot.dev\nTo answer your questions directly: no, prompts are not enough. Yes, you need real enforcement. And yes, most people are quietly accepting the risk right now.",
          "score": 1,
          "created_utc": "2026-02-26 02:11:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd5tbq",
      "title": "there‚Äôs a new open source tool for checking ai agent security.... is it okay to share here?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rd5tbq/theres_a_new_open_source_tool_for_checking_ai/",
      "author": "Accomplished-Wall375",
      "created_utc": "2026-02-24 04:34:17",
      "score": 13,
      "num_comments": 8,
      "upvote_ratio": 0.93,
      "text": "hey everyone,\n\ncame across a newly released free, open source tool designed to help developers and security teams evaluate the security of ai agents‚Äô skills, tools, and integrations. it focuses on spotting issues like overly broad permissions, unsafe tool access, and weak guardrails before anything goes live in production.\n\nthere‚Äôs also a podcast episode that dives deeper into ai security, emerging risks, and where the tech is heading:  \n[https://open.spotify.com/show/5c2sTWoqHEYLrXfLLegvek](https://open.spotify.com/show/5c2sTWoqHEYLrXfLLegvek)\n\ncurious... if this would be the right place to share the repo and get feedback from the community.\n\n**Edit: S**ince everyone was asking for the link...here it is \"[Caterpiller](https://caterpillar.alice.io/)\" that scan AI agent skills for¬†security threats and btw its an open source tool...please share your feedback and thankuu for being kinder.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rd5tbq/theres_a_new_open_source_tool_for_checking_ai/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o73owa8",
          "author": "Bmaxtubby1",
          "text": "It‚Äôs probably fine to share, especially since it‚Äôs open source and directly relevant to AI agents and security. But context matters. If you just paste a repo link and a podcast, it can look like promotion. If you explain what the tool actually does, how it evaluates permissions, and where it might fail, it feels like a genuine discussion.\n\nAlso, if it‚Äôs your project, say that upfront. Most dev communities are okay with creators sharing their own tools as long as they‚Äôre transparent and open to feedback. Hidden promotion is what usually triggers backlash.",
          "score": 4,
          "created_utc": "2026-02-24 08:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o794s65",
          "author": "m2845",
          "text": "Feel free to post it!",
          "score": 2,
          "created_utc": "2026-02-25 02:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73010q",
          "author": "Aggravating_Log9704",
          "text": "The podcast sounds nice but the repo is where the meat is. Does it support custom threat models? A big issue with AI security platforms is they assume one size fits all. Real teams have very different risk profiles. If it lets you plug in bespoke rules or simulated attacks that is legit.",
          "score": 1,
          "created_utc": "2026-02-24 04:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o734lct",
          "author": "Any_Artichoke7750",
          "text": "I‚Äôd love to know if it integrates with CI/CD. Static analysis during dev is fine, but the real win is catching risky permissions before deploy. If it hooks into GitHub actions or similar it‚Äôs already ahead of most toy tools.",
          "score": 1,
          "created_utc": "2026-02-24 05:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o739cuz",
          "author": "Severe_Part_5120",
          "text": "this looks promising. i have been building some personal ai projects that interact with api‚Äôs and local scripts, and I‚Äôve had zero way to test how safe they are. even simple things like accidentally exposing api keys or letting an agent delete something it shouldn‚Äôt can be a huge problem. a tool like this seems like it could be really valuable for people testing things in a sandbox environment before going live.",
          "score": 1,
          "created_utc": "2026-02-24 05:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75czpr",
          "author": "penguinzb1",
          "text": "the permissions-first approach is the right starting point, but with agents the structural analysis only catches a subset of the real exposure. the rest shows up when you run it against actual inputs. an agent that looks well-scoped at the permission level will still take unexpected actions under specific input combinations nobody mapped out during design. the behavioral gaps only surface when you run it through the scenarios that actually show up in your traffic before shipping.",
          "score": 1,
          "created_utc": "2026-02-24 15:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75kcju",
          "author": "Sea-Sir-2985",
          "text": "the permissions-first approach is a good start but the real risk with agents is runtime behavior not just static config... an agent can have perfectly scoped permissions and still do unexpected things depending on what inputs it gets. i've seen agents with read-only access still cause problems by flooding APIs with requests\n\nthe CI/CD integration question is the right one though, catching risky permissions before deploy rather than after something goes wrong is where the actual value is",
          "score": 1,
          "created_utc": "2026-02-24 15:51:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rejx0p",
      "title": "safe-py-runner: Secure & lightweight Python execution for LLM Agents",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rejx0p/safepyrunner_secure_lightweight_python_execution/",
      "author": "adarsh_maurya",
      "created_utc": "2026-02-25 17:36:22",
      "score": 11,
      "num_comments": 10,
      "upvote_ratio": 0.87,
      "text": "AI is getting smarter every day. Instead of building a specific \"tool\" for every tiny task, it's becoming more efficient to just let the AI write a Python script. But how do you run that code without risking your host machine or dealing with the friction of Docker during development?\n\nI built `safe-py-runner` to be the lightweight \"security seatbelt\" for developers building AI agents and Proof of Concepts (PoCs).\n\n# What My Project Does\n\nIt allows you to execute AI-generated Python snippets in a restricted subprocess with \"guardrails\" that you control via simple TOML policies.\n\n* **Reduce Tool-Calls:** Instead of making 10 different tools for math, string parsing, or data formatting, give your agent a `python_interpreter` tool powered by this runner.\n* **Resource Guardrails:** Prevents the AI from accidentally freezing your server with an infinite loop or crashing it with a memory-heavy operation.\n* **Access Control:** Explicitly whitelist or blacklist modules (e.g., allow `datetime`, block `os`).\n* **Local-First:** No need to manage heavy Docker images just to run a math script during your prototyping phase.\n\n# Target Audience\n\n* **PoC Developers:** If you are building an agent and want to move fast without the \"extra layer\" of Docker overhead yet.\n* **Production Teams:** Use this **inside** a Docker container for \"Defense in Depth\"‚Äîadding a second layer of code-level security inside your isolated environment.\n* **Tool Builders:** Anyone trying to reduce the number of hardcoded functions they have to maintain for their LLM.\n\n# Comparison\n\n|**Feature**|**eval() / exec()**|**safe-py-runner**|**Pyodide (WASM)**|**Docker**|\n|:-|:-|:-|:-|:-|\n|**Speed to Setup**|Instant|**Seconds**|Moderate|Minutes|\n|**Overhead**|None|**Very Low**|Moderate|High|\n|**Security**|None|**Policy-Based**|Very High|Isolated VM/Container|\n|**Best For**|Testing only|**Fast AI Prototyping**|Browser Apps|Production-scale|\n\n# Getting Started\n\n**Installation:**\n\nBash\n\n    pip install safe-py-runner\n\n**GitHub Repository:**\n\n[https://github.com/adarsh9780/safe-py-runner](https://github.com/adarsh9780/safe-py-runner)\n\nThis is meant to be a pragmatic tool for the \"Agentic\" era. If you‚Äôre tired of writing boilerplate tools and want to let your LLM actually use the Python skills it was trained on‚Äîsafely‚Äîgive this a shot.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rejx0p/safepyrunner_secure_lightweight_python_execution/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7d2qsh",
          "author": "Crafty_Disk_7026",
          "text": "Nice I made something similar if you want some inspiration https://github.com/imran31415/codemode_python_benchmark/blob/main/sandbox/executor.py",
          "score": 1,
          "created_utc": "2026-02-25 17:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d6s2c",
              "author": "adarsh_maurya",
              "text": "very interesting, it uses restricted python and thus you can control dunder level security threats using this. I am thinking to add this in future. thank you.",
              "score": 1,
              "created_utc": "2026-02-25 18:00:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7d7hpo",
                  "author": "Crafty_Disk_7026",
                  "text": "Yes!  Curious to see what approach you end up choosing!",
                  "score": 1,
                  "created_utc": "2026-02-25 18:03:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7d7hgq",
          "author": "penguinzb1",
          "text": "policy-based is solid for anticipated patterns. the gaps show up when agent-generated code hits the ones nobody pre-specified.",
          "score": 1,
          "created_utc": "2026-02-25 18:03:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d8zmi",
              "author": "adarsh_maurya",
              "text": "Yes. But there are some things which everyone should use like memory limit, time limit and restrictions to network. This reduces the boiler plate for developer and they can just focus on developing agent rather than setting up infra layer. But i agree with your point.",
              "score": 1,
              "created_utc": "2026-02-25 18:10:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gpkfl",
          "author": "xenos__25",
          "text": "What possible use case or need do you think this have? I can guide my ai agent to create tools and use them, what does creating a separate env bring into picture?",
          "score": 1,
          "created_utc": "2026-02-26 05:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gqym3",
              "author": "adarsh_maurya",
              "text": "I am not sure I fully understand this comment, but what I interpreted is this: the same agent which will create tool, it will execute it as well. If that's what you are saying, even then you'd need an environment to execute it right? this provides that environment with controllable limits. Imagine you have an agent on a Fast Api server, and it created a code and executed it on the same runtime which is hosting Fast Api, this would block your app to its end user because the agent is busy executing a code. If it doesn't, the agent might use libraries which it was not allowed to.\n\nThis comment is purely based on my interpretation of your question, if it is wrong please let me know.",
              "score": 2,
              "created_utc": "2026-02-26 05:16:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jmsph",
                  "author": "xenos__25",
                  "text": "Thanks! That answers my question",
                  "score": 1,
                  "created_utc": "2026-02-26 17:06:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ow445",
          "author": "MicroAppFounder",
          "text": "I've been messing around with agents for a bit and the tool sprawl is real. I started out writing individual functions for everything‚Äîdate parsing, math, cleaning strings‚Äîand it became a bit of a nightmare to maintain. I actually tried running a local interpreter but got paranoid about the agent accidentally nuking my dev folder. This looks like a nice middle ground that doesn't require me to spin up a whole container just to see if a script works. The TOML approach is pretty clever for keeping things readable.",
          "score": 1,
          "created_utc": "2026-02-27 12:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rxffv",
              "author": "adarsh_maurya",
              "text": "I had the same issue. Please try it and let me know. I added docker support for extra security and keeping the api same.",
              "score": 1,
              "created_utc": "2026-02-27 21:47:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdk3g4",
      "title": "I built a lightweight long-term memory engine for LLMs because I was tired of goldfish memory",
      "subreddit": "LLMDevs",
      "url": "https://github.com/RaffaelFerro/synapse",
      "author": "porrabelo",
      "created_utc": "2026-02-24 15:59:31",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdk3g4/i_built_a_lightweight_longterm_memory_engine_for/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o766vcv",
          "author": "porrabelo",
          "text": "I‚Äôm eager to know the results! Thank you!!",
          "score": 2,
          "created_utc": "2026-02-24 17:32:33",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o75u0fh",
          "author": "GullibleNarwhal",
          "text": "I am super intrigued as I am currently trying to Frankenstein together multiple models. I currently have an embedded router model for user input intent determination, a brain or language model for response generation that can be swapped, and vision models for image processing that can also be swapped. I have tried to build out a contextual memory for the brain by having it save \"memories\" of conversations, and then summarize once it reaches a certain threshold. I have yet to build enough of a record to test the memory system though. I am curious how this might integrate into it. Are you offering this open source?",
          "score": 1,
          "created_utc": "2026-02-24 16:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75v5i5",
              "author": "porrabelo",
              "text": "That Sounds like a fun challenge!\nYes, it is [open source](https://github.com/RaffaelFerro/synapse) (MIT license) \nPlease try it and give me your feedback!",
              "score": 1,
              "created_utc": "2026-02-24 16:39:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75vl0x",
                  "author": "GullibleNarwhal",
                  "text": "I will give it a shot and see if I can integrate it into my app. Curious if you have had any success with testing in just by prompting local llms, or are you connecting via an API?",
                  "score": 1,
                  "created_utc": "2026-02-24 16:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bqqa5",
              "author": "Dense_Gate_5193",
              "text": "you should check out NornicDB i have the entire rag pipeline including embedding the original query, RRF + rerank down to 7ms including http transport on a 1m embedding corpus.\n\nhttps://github.com/orneryd/NornicDB",
              "score": 1,
              "created_utc": "2026-02-25 13:52:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dt46i",
                  "author": "GullibleNarwhal",
                  "text": "That looks pretty interesting too. I am working on implementing a router model that determines the user's prompt intent, and routes the flow. I noticed on your GitHub that it shows conversational memory half-life is around 7 days. Is there any way to modify or configure if you would prefer for it to have more conversational memory retrieval capabilities?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:41:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bqtcc",
          "author": "Dense_Gate_5193",
          "text": "you should check out NornicDB https://github.com/orneryd/NornicDB/graphs/traffic",
          "score": 1,
          "created_utc": "2026-02-25 13:53:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rff0hk",
      "title": "How to build a knowledge graph for AI",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rff0hk/how_to_build_a_knowledge_graph_for_ai/",
      "author": "DistinctRide9884",
      "created_utc": "2026-02-26 16:36:43",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.83,
      "text": "Hi everyone, I‚Äôve been experimenting with building a knowledge graph for AI systems, and I wanted to share some of the key takeaways from the process.\n\nWhen building AI applications (especially RAG or agent-based systems), a lot of focus goes into embeddings and vector search. But one thing that becomes clear pretty quickly is that semantic similarity alone isn‚Äôt always enough - especially when you need structured reasoning, entity relationships, or explainability.\n\nSo I explored how to build a proper knowledge graph that can work alongside vector search instead of replacing it.\n\nThe idea was to:\n\n* Extract entities from documents\n* Infer relationships between them\n* Store everything in a graph structure\n* Combine that with semantic retrieval for hybrid reasoning\n\nOne of the most interesting parts was thinking about how to move from ‚Äúunstructured text chunks‚Äù to structured, queryable knowledge. That means:\n\n* Designing node types (entities, concepts, etc.)\n* Designing edge types (relationships)\n* Deciding what gets inferred by the LLM vs. what remains deterministic\n* Keeping the system flexible enough to evolve\n\nI used:\n\n**SurrealDB**: a multi-model database built in Rust that supports graph, document, vector, relational, and more - all in one engine. This makes it possible to store raw documents, extracted entities, inferred relationships, and embeddings together without stitching multiple databases. I combined vector + graph search (i.e. semantic similarity with graph traversal), enabling hybrid queries and retrieval.\n\n**GPT-5.2**: for entity extraction and relationship inference. The LLM helps turn raw text into structured graph data.\n\n**Conclusion**\n\nOne of the biggest insights is that knowledge graphs are extremely practical for AI apps when you want better explainability, structured reasoning, more precise filtering and long-term memory.\n\nIf you're building AI systems and feel limited by ‚Äúchunk + embed + retrieve,‚Äù adding a graph layer can dramatically change what your system is capable of.\n\nI wrote a full walkthrough explaining the architecture, modelling decisions, and implementation details¬†[here](https://surrealdb.com/blog/how-to-build-a-knowledge-graph-for-ai).\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rff0hk/how_to_build_a_knowledge_graph_for_ai/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7ogumd",
          "author": "entheosoul",
          "text": "Nice, this could actually help the project I'm working on that maps entities between each other (contacts, projects, orgs, engagements) which are populated by SQL and embeddings (similarity search)\n\nRight now I use programmatic sql functions to connect these as a entity knowledge graph and was considering graphQL. Empirica is open source MIT license though, so not sure if I could use your solution for that, but I'd like to try it...",
          "score": 2,
          "created_utc": "2026-02-27 10:36:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o48gl",
          "author": "robogame_dev",
          "text": "Note: for future posts please clarify to the reader any commercial or promotional relationship to recommended products, e.g. adding (I work here) or something like that when you bring up surrealdb.",
          "score": 1,
          "created_utc": "2026-02-27 08:37:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7o6gnk",
          "author": "robogame_dev",
          "text": "Vis a vis the graph and entity extraction, do you have a recommended approach for de conflicting info or handling hypotheticals or conditionals?\n\nI‚Äôve been struggling to figure out how to preserve conditional information through extraction - for example, ‚Äúif it wasn‚Äôt John it was someone who looked like him‚Äù - do I tag into my existing entity for John, create a new one for the possible unknown person, both - and how to prevent chaos when recalling from data that has conditional / hypothetical entries?",
          "score": 1,
          "created_utc": "2026-02-27 08:58:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7sejff",
          "author": "the-ai-scientist",
          "text": "The hybrid vector + graph approach is underexplored and genuinely powerful. One thing worth adding to the framing: knowledge graphs also help with a failure mode that pure RAG handles poorly ‚Äî multi-hop reasoning, where answering a question requires traversing several relationships that might not co-occur in any single chunk.\n\nThe SurrealDB choice is interesting. Curious whether you ran into any performance tradeoffs between the graph traversal and vector search at query time ‚Äî that tends to be where hybrid architectures get complicated in production.",
          "score": 1,
          "created_utc": "2026-02-27 23:19:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7z4ndi",
          "author": "Trekker23",
          "text": "These days you don't need a full multi-model database server for this ‚Äî you can use lightweight, in-memory graph systems like KGLite (Rust-based, embedded, with built-in vector search) that run as an MCP server. The agent just queries the graph directly through Cypher, no ETL pipeline into a separate database, no connection management. You define your nodes and edges, and the graph is ready to traverse and search in the same process. For a lot of AI agent use cases, that's all you need ‚Äî graph traversal, semantic search, and pattern matching without the operational overhead.",
          "score": 1,
          "created_utc": "2026-03-01 01:01:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rglkse",
      "title": "Built a KV cache for tool schemas ‚Äî 29x faster TTFT, 62M fewer tokens/day processed",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rglkse/built_a_kv_cache_for_tool_schemas_29x_faster_ttft/",
      "author": "PlayfulLingonberry73",
      "created_utc": "2026-02-27 22:49:58",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "If you're running tool-calling models in production, your GPU is re-processing the same tool definitions on every request. I built a cache to stop that.\n\n\n\nContextCache hashes your tool schemas, caches the KV states from prefill, and only processes the user query on subsequent requests. The tool definitions never go through the model again.\n\n\n\nAt 50 tools: 29x TTFT speedup, 6,215 tokens skipped per request (99% of the prompt). Cached latency stays flat at \\~200ms no matter how many tools you load.\n\n\n\nThe one gotcha: you have to cache all tools together, not individually. Per-tool caching breaks cross-tool attention and accuracy tanks to 10%. Group caching matches full prefill quality exactly.\n\n\n\nBenchmarked on Qwen3-8B (4-bit) on a single RTX 3090 Ti. Should work with any transformer model ‚Äî the caching is model-agnostic, only prompt formatting is model-specific.\n\n\n\nCode: [https://github.com/spranab/contextcache](https://github.com/spranab/contextcache)\n\nPaper: [https://zenodo.org/records/18795189](https://zenodo.org/records/18795189)\n\nhttps://preview.redd.it/5fkm1dde94mg1.png?width=3363&format=png&auto=webp&s=2cd7f3bf937eddc8e7330ba14422c59170580531\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rglkse/built_a_kv_cache_for_tool_schemas_29x_faster_ttft/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7xa537",
          "author": "GeneralMiao",
          "text": "Building primitives for tool calls is the easiest defined path - what tools were you calling here? Anything sourcing direct from a DB or mostly just API hits",
          "score": 2,
          "created_utc": "2026-02-28 18:55:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xb0l0",
              "author": "PlayfulLingonberry73",
              "text": "If you think enterprise level, I have seen tools with a large token counts. And with less context there are high chances of hallucination and wrong tool selections. The idea is not to send large system prompts or tool list on each session. Rather the tools will be cached when there is a new deployment and the system prompt will only refer to that cache id. So reducing the counts drastically. ",
              "score": 2,
              "created_utc": "2026-02-28 18:59:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7xf5i8",
                  "author": "GeneralMiao",
                  "text": "Yeah agreed, in practice reading 90 days of inbox data is going to obviously take a ton more context than a single SOQL query for a group of 10 accounts. If you‚Äôre running an agent, boxing in primitives (repeatable action multi tool calls) saves a ton on context life in your window",
                  "score": 2,
                  "created_utc": "2026-02-28 19:20:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rf61ln",
      "title": "Vibe hardware design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rf61ln/vibe_hardware_design/",
      "author": "Interesting-Tune-295",
      "created_utc": "2026-02-26 09:48:46",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.79,
      "text": "Hi hello , so i made a tool to see if we can lower the barrier of entry into hardware prototyping \n\ni.e type in i want an Iphone and you get an iphone delivered next week / three days after\n\n  \nThe reason i decided to do this is after observing really talented staff at a hospital in kenya state that they were running low on UV beds used to treat infants born with jaundice. the technical solution on my end seemed obvious due to my background in engineering but to them it was another bag of worms\n\nPs. I hope i am not shilling , i just find this idea really unque and would love to see your views on it [https://blankdesign-peach.vercel.app/](https://blankdesign-peach.vercel.app/) , there is no signups, payment or anything.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rf61ln/vibe_hardware_design/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7mb6rf",
          "author": "the-scream-i-scrumpt",
          "text": "So these sensors and microcontrollers and stuff... Are you going to assemble them for me after I place an order? Like you just ship me the finished product?\n\nWhat if I hate it? When I vibe code a project I get instant feedback, but this is like 3 or 4 weeks feedback (at best)",
          "score": 2,
          "created_utc": "2026-02-27 01:02:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7o7lkr",
              "author": "Interesting-Tune-295",
              "text": "This is something people have been asking after using the site...some proposed solution is including a rendered simulation for the product. \n\nAlso with the place an order, i initially made it with the idea that i can either use my network of people with 3D printers, CNC machinist and assemblers to fulfill the order.\n\nAnd it typically takes like a week or so",
              "score": 1,
              "created_utc": "2026-02-27 09:09:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ref4j2",
      "title": "I Intercepted 3,177 API Calls Across 4 AI Coding Tools. Here's What's Actually Filling Your Context Window",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1ref4j2/i_intercepted_3177_api_calls_across_4_ai_coding/",
      "author": "wouldacouldashoulda",
      "created_utc": "2026-02-25 14:45:29",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "I was curious so spent a lot of time analysing context usage amongst a few CLI‚Äôs. I found some pretty interesting strategies being used, but mainly it was the inefficiencies that were most noticeable. \n\nhttps://theredbeard.io/blog/i-intercepted-3177-api-calls-across-4-ai-coding-tools/",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ref4j2/i_intercepted_3177_api_calls_across_4_ai_coding/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7c60dd",
          "author": "Fantastic_Climate_90",
          "text": "Nice project \n\nWhy not Gemini 3 pro?",
          "score": 1,
          "created_utc": "2026-02-25 15:11:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7cj14j",
              "author": "wouldacouldashoulda",
              "text": "Lots of rate limiting issues when i was doing this research. Working on a followup with gemini 3 though.",
              "score": 1,
              "created_utc": "2026-02-25 16:12:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7rs64v",
          "author": "UncleRedz",
          "text": "Any chance of testing GitHub CoPilot CLI or GitHub CoPilot for VSCode? My impression is that it's more lean and works well even with local models.",
          "score": 1,
          "created_utc": "2026-02-27 21:21:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgh9ro",
      "title": "Give a man an automated fishing pipeline‚Ä¶ üé£",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/dxbcyouof3mg1.jpeg",
      "author": "Vivarium_dev",
      "created_utc": "2026-02-27 20:03:25",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rgh9ro/give_a_man_an_automated_fishing_pipeline/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7vza5j",
          "author": "c-u-in-da-ballpit",
          "text": "Jesus idk how to even read twitter chronologically anymore",
          "score": 8,
          "created_utc": "2026-02-28 14:59:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7yfb55",
              "author": "gefahr",
              "text": "This is some awful collage of screenshots. It doesn't look that natively, ever. You can also tell because of the X logo in the top right of the one tweet.",
              "score": 2,
              "created_utc": "2026-02-28 22:34:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7zcv6j",
          "author": "low_depo",
          "text": "This is true.\n\nNow for free or for 20$ a month you have access to unlimited knowledge, personal teacher on any subject in any language. \n\nPeople who are knowledge hungry will accelerate and people who were con/coom -sumers well... they will consumer more, more netflix, more yt, more ai slop.",
          "score": 3,
          "created_utc": "2026-03-01 01:52:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xy3kn",
          "author": "Acceptable-Milk-314",
          "text": "wet-ware wam wusage",
          "score": 1,
          "created_utc": "2026-02-28 21:01:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xy7ci",
              "author": "Vivarium_dev",
              "text": "How much should I dedotate?",
              "score": 1,
              "created_utc": "2026-02-28 21:01:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7u0u2l",
          "author": "robogame_dev",
          "text": "Yup, I read an interview with a pro gamer once. He said that hour for hour, he didn‚Äôt play more time than amateurs do - the difference was each hour he played was focussed on deliberately improving, so his 1000 hours of gameplay is mostly training and skill development, while a typical amateurs 1000 hours is mostly coasting and passing the time.\n\nSame game, same time input, totally divergent outcomes.",
          "score": 1,
          "created_utc": "2026-02-28 05:32:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdfqxw",
      "title": "206 models. 30 providers. One command to find what runs on your hardware",
      "subreddit": "LLMDevs",
      "url": "https://github.com/AlexsJones/llmfit",
      "author": "low_effort-username",
      "created_utc": "2026-02-24 13:06:56",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdfqxw/206_models_30_providers_one_command_to_find_what/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o74zoab",
          "author": "Accomplished-View952",
          "text": "Loved this utility. blazing fast. ",
          "score": 2,
          "created_utc": "2026-02-24 14:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o755866",
              "author": "low_effort-username",
              "text": "Thanks, tried to keep it simple, and Rust ü¶Ä",
              "score": 1,
              "created_utc": "2026-02-24 14:39:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75iugz",
          "author": "Daredevvll",
          "text": "Wow looks so nice. Congrats. I'm wondering if it can measure gguf models and can include hf custom models?",
          "score": 2,
          "created_utc": "2026-02-24 15:44:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75r3aa",
              "author": "low_effort-username",
              "text": "Yes, I pulled 200 of the most popular from HF, but we add whatever you want, as long as I can get the data on the model. ",
              "score": 2,
              "created_utc": "2026-02-24 16:21:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7wdqqj",
                  "author": "Daredevvll",
                  "text": "Great! I will try asap üëèüèª",
                  "score": 1,
                  "created_utc": "2026-02-28 16:13:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7m3z8u",
          "author": "astrokat79",
          "text": "can you add support for llama.cpp ?",
          "score": 1,
          "created_utc": "2026-02-27 00:22:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1riaeyl",
      "title": "Finance Agent: Improved retrieval accuracy from 50% to 91% on finance bench\nShowcase",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1riaeyl/finance_agent_improved_retrieval_accuracy_from_50/",
      "author": "hrishikamath",
      "created_utc": "2026-03-01 21:57:13",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.82,
      "text": "Built a open source financial research agent for querying SEC filings (10-Ks are 60k tokens each, so stuffing them into context is not practical at scale).  \nBasic open source embeddings, no OCR and no finetuning. Just good old RAG and good engineering around these constraints. Yet decent enough latency.\n\nStarted with naive RAG at 50%, ended at 91% on FinanceBench. The biggest wins in order:\n\n1. Separating text and table retrieval\n2. Cross-encoder reranking after aggressive retrieval (100 chunks down to 20)\n3. Hierarchical search over SEC sections instead of the full document\n4. Switching to agentic RAG with iterative retrieval and memory, each iteration builds on the previous answer\n\nThe constraint that shaped everything. To compensate I retrieved more chunks, use re ranker, and used a strong open source model.\n\nBenchmarked with LLM-as-judge against FinanceBench golden truths. The judge has real failure modes (rounding differences, verbosity penalties) so calibrating the prompt took more time than expected.\n\nFull writeup:¬†[https://kamathhrishi.substack.com/p/building-agentic-rag-for-financial](https://kamathhrishi.substack.com/p/building-agentic-rag-for-financial)\n\nGithub:¬†[https://github.com/kamathhrishi/finance-agent](https://github.com/kamathhrishi/finance-agent)\n\n",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1riaeyl/finance_agent_improved_retrieval_accuracy_from_50/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdymax",
      "title": "Memory made my agent smarter‚Ä¶ then slowly made it wrong",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdymax/memory_made_my_agent_smarter_then_slowly_made_it/",
      "author": "jak_kkk",
      "created_utc": "2026-02-25 00:53:56",
      "score": 6,
      "num_comments": 14,
      "upvote_ratio": 0.88,
      "text": "I‚Äôve been running an internal agent that helps summarize ongoing work across days.  \n At first persistent memory fixed everything. It stopped repeating questions and actually followed context between sessions.\n\nAfter a few weeks the behavior changed in a subtle way.  \n It didn‚Äôt forget it relied too much on conclusions that used to be true. The environment changed but its confidence didn‚Äôt.\n\nNow I‚Äôm realizing the hard problem isn‚Äôt remembering, it‚Äôs updating what the agent thinks it already knows.\n\nCurious how people handle this in long running systems.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdymax/memory_made_my_agent_smarter_then_slowly_made_it/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7annkf",
          "author": "patchfoot02",
          "text": "Large context windows/rag/mcp memory tools/etc definitely seem to get too loud. It dominates llm thinking even when it isn't helpful. Obviously to do anything useful you can't keep their context blank, but managing it is a question of balancing retention vs analysis quality. That old data will constantly drag the llm into mental culda sacs they can't escape and will keep repeating info from them.\n\n\nThe solution with current models for memory seems to be sub agents. Don't pollute the main models inference with a bunch of irrelevant data. Delegate it to a sub agent to have it go through and determine what's relevant to the current prompt and only return that. Maximize free space for the final analysis so the old data doesn't overwhelm the main model",
          "score": 3,
          "created_utc": "2026-02-25 08:54:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jmkpk",
              "author": "singh_taranjeet",
              "text": "I agree that dumping everything into the main context just creates cognitive noise and weird attractor states. Sub agents help, but the deeper fix is structured, typed memory with relevance scoring and decay so the system learns what to surface and what to forget instead of blindly forwarding chunks.\n\nThat orchestration layer is essentially what we‚Äôve been building at Mem0, because without active memory governance agents inevitably drift",
              "score": 1,
              "created_utc": "2026-02-26 17:05:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78qvz8",
          "author": "fatmax5",
          "text": "We saw the same thing. The agent wasn‚Äôt hallucinating, it was repeating logic that used to work.",
          "score": 2,
          "created_utc": "2026-02-25 00:54:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7c3aqt",
              "author": "Fantastic_Climate_90",
              "text": "Mode collapse",
              "score": 1,
              "created_utc": "2026-02-25 14:58:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7f5hhl",
              "author": "jak_kkk",
              "text": "Yeah exactly, success becomes sticky.",
              "score": 1,
              "created_utc": "2026-02-25 23:35:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7f5oza",
                  "author": "fatmax5",
                  "text": "We started using Hindsight mainly because it can revise earlier conclusions instead of stacking them forever.",
                  "score": 1,
                  "created_utc": "2026-02-25 23:36:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o78t65t",
          "author": "doomslice",
          "text": "We saw the same thing. It was about that time that I noticed this agent was about eight stories tall and was a crustacean from the Protozoic Era!",
          "score": 2,
          "created_utc": "2026-02-25 01:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o794q90",
          "author": "GloomyPop5387",
          "text": "Recency\nIf info is older than x revalidate that info\n",
          "score": 1,
          "created_utc": "2026-02-25 02:13:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79woxp",
          "author": "allisonmaybe",
          "text": "I'm obsessed with this problem lately. So much so that I've build out the MVAC stack (Memory, Vault, Activation, Communication). It's not about the memories it stores, you just dont have enough layers of complexity on top of that information to keep that memory up to date.\n\n\nCheck out hifathom.com. it's a work in progress. Memento is the memory tool that comes with everything you need to keep an up to date memory (The M). The VA, and part of the C will be released later this week.\n\n\nnpx memento-cli init",
          "score": 1,
          "created_utc": "2026-02-25 05:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7crog1",
          "author": "chiffon-",
          "text": "Is it adding in expired code too?",
          "score": 1,
          "created_utc": "2026-02-25 16:51:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f1zkz",
          "author": "midaslibrary",
          "text": "I‚Äôm playing around with a rag framework for this, it basically implements a rolling context window and I‚Äôm sure I could apply it to scratchpad/.md",
          "score": 1,
          "created_utc": "2026-02-25 23:16:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f5a56",
          "author": "One-Two-218",
          "text": "Feels like memory drift rather than memory loss.",
          "score": 1,
          "created_utc": "2026-02-25 23:34:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f5kks",
              "author": "jak_kkk",
              "text": "Good term honestly, drift is exactly what it feels like in production.",
              "score": 1,
              "created_utc": "2026-02-25 23:36:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7f5cnp",
          "author": "Oliver19234",
          "text": "Most memory systems store history, not beliefs. Agents need a way to unlearn, not just remember.",
          "score": 1,
          "created_utc": "2026-02-25 23:34:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}