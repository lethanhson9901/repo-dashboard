{
  "metadata": {
    "last_updated": "2026-02-26 17:15:53",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 142,
    "file_size_bytes": 176269
  },
  "items": [
    {
      "id": "1r9mqd1",
      "title": "Unpopular opinion: prompt engineering is just \"knowing how to talk to your coworker\" rebranded",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r9mqd1/unpopular_opinion_prompt_engineering_is_just/",
      "author": "Neither_Turn1635",
      "created_utc": "2026-02-20 05:22:02",
      "score": 112,
      "num_comments": 26,
      "upvote_ratio": 0.92,
      "text": "Half the \"prompt engineering\" advice I see is literally just good communication skills:  \n  \n\"Give clear context\" â€” yeah, that's how you talk to any human  \n\"Break complex tasks into steps\" â€” project management 101  \n\"Provide examples of what you want\" â€” every creative brief ever  \n\"Be specific about the output format\" â€” basic email etiquette  \n  \nThe people who are best at prompting aren't engineers. They're the people who were already good at explaining what they want. We just gave the skill a fancy name and a LinkedIn certification.  \n  \nAm I wrong?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r9mqd1/unpopular_opinion_prompt_engineering_is_just/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6dtr9s",
          "author": "OnlyTimeFan",
          "text": "Naming it â€œengineeringâ€ is annoying. I pretend Iâ€™m asking a primary school kid. Ta-da.",
          "score": 17,
          "created_utc": "2026-02-20 07:18:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i1so5",
              "author": "Disastrous-Angle-591",
              "text": "I use detailed structured prompts drawing on 30 years of coding.Â ",
              "score": 3,
              "created_utc": "2026-02-20 22:03:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6q52dd",
                  "author": "CedarSageAndSilicone",
                  "text": "Absolutely bonkers that giving context leads to better results!Â ",
                  "score": 2,
                  "created_utc": "2026-02-22 05:25:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6iqoap",
                  "author": "OnlyTimeFan",
                  "text": "Can you show us an example?",
                  "score": 1,
                  "created_utc": "2026-02-21 00:21:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6it2uu",
              "author": "red_hare",
              "text": "We're so afraid of acknowledging a non-STEM job can have value in tech that we renamed \"writer\" to \"English engineer\".",
              "score": 0,
              "created_utc": "2026-02-21 00:34:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dn25a",
          "author": "kobumaister",
          "text": "Absolutely, I made that analogy last week in my workplace: What happens if a new developer arrives at the company and you just throw a jira issue at him? I will deliver, but without following the best practices of the company, not understanding how internal dependencies work, probably changing things that are there for a reason, etc... \n\nThat's exactly what ai does, and why you provide context. I joke about it being a junior developer with a lot of cocaine.",
          "score": 13,
          "created_utc": "2026-02-20 06:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ed9k8",
          "author": "PhilosophicWax",
          "text": "So is being a developer.",
          "score": 6,
          "created_utc": "2026-02-20 10:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dm03z",
          "author": "ConnectMotion",
          "text": "There is some anecdotal relevance to this.\n\nItâ€™s not a skill everyone has in every way for every scenario.",
          "score": 6,
          "created_utc": "2026-02-20 06:09:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eq3ed",
          "author": "House13Games",
          "text": "AI Just reinvented the wheel. \n\nIt now takes billions of watts and a server farm the size of a city to do the same job as some interns. \n\nAI is trained 60% on reddit posts and can't tell which side of a cup is up.\n\nI'm not feeling worried about losing my job, to tell the truth.",
          "score": 3,
          "created_utc": "2026-02-20 12:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eta9b",
              "author": "Snoo-20788",
              "text": "It may look huge when measure in watts. But cost wise its negligible. \n\nWe now have the cost output everytime claude completes a jira ticket all by itself (i.e. it reads the ticket, codes the feature, tests it, creates a PR and waits for approval). It usually costs 1 or $2, and takes under 10 minutes for tasks that would take 30 minutes for a senior SWE who knows the company's codebase well (and 2h for one who doesn't). The equivalent cost of the SWE would be between 50 and 200.\n\nI am not worried at all about losing my job. Ultimately someone needs to talk to the business people, the researchers, and put together the framework that allows AI agents to do their job, and thats me and my colleagues.",
              "score": 1,
              "created_utc": "2026-02-20 12:29:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dz697",
          "author": "kubrador",
          "text": "you're right which is why prompt engineering jobs will be gone in 3 years when the models just understand what you mean",
          "score": 2,
          "created_utc": "2026-02-20 08:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e0mft",
          "author": "Usual-Orange-4180",
          "text": "Very unpopular because it ignores pattern repetition and the need for context isolation.",
          "score": 2,
          "created_utc": "2026-02-20 08:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eqdww",
          "author": "projectoedipus",
          "text": "The only point that I would disagree on, is that prompt engineering, especially advanced prompt engineering, is about understanding the ways that the AI model might misunderstand, because of how they work. You might say that is just communication skills, but it is about understanding how they function way deeper than someone who just communicates clearly.\n\nFor example, if I spend 50% of my prompt to a text-to-image generation model, describing a specific aspect of the image, then it is going to notice that, and it will generate the picture very differently, focusing more on that aspect, than if I say what is essentially the same thing with less words. But the order that I mention things matters as well. If I am generating an image and at the end of my prompt I say something that the AI model doesn't do, I could move that sentence to the beginning of my prompt, and it would have higher priority.\n\nOne time I was trying to generate an image and my prompt contained the phrase \"flight of stairs\", and after many failed generations where the stairs were floating, and me not understanding why, I realized that the word \"flight\" although used correctly, was confusing the model, and removing it fixed the outputs.\n\nA person that is exceptional at communication is not automatically a good prompt engineer, because they don't understand these things. Specific models have their own tendencies and prompt-following quirks as well, across all mediums of AI models, so you could also argue that part of being a good prompt engineer is learning these tendencies.\n\nSo being able to communicate effectively can make you rapidly progress while learning prompt engineering, but to say that they are the same skill is not understanding the full depth of prompt engineering.",
          "score": 2,
          "created_utc": "2026-02-20 12:09:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dye78",
          "author": "fabkosta",
          "text": "Hint: Prompt engineering today can mean specifying entire software stacks. In prose. Which means you must know how to describe concepts such as four tier architecture, microservice coordination, REST APIs vs Graphql, reactive frontend programming, RBAC based security, ORM, and quite a few more things. In language.\n\nStating that this is \"just knowing how to talk to your coworker\" implies that this is easy. Which tells me one or two things about OP's experience.",
          "score": 4,
          "created_utc": "2026-02-20 08:01:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e0nnu",
              "author": "Vestenpance",
              "text": "I think you're agreeing with OP that a key part of prompt engineering is an ability to communicate, and that effective communication requires deep domain knowledge.",
              "score": 8,
              "created_utc": "2026-02-20 08:22:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ep4dr",
              "author": "itquilibrium",
              "text": "Lolâ€¦",
              "score": 1,
              "created_utc": "2026-02-20 11:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6txpf5",
              "author": "robhanz",
              "text": "Interesting.  I don't think it's easy... at least, I don't find it particularly hard, but I'm also aware of how many issues it *does* present in the workplace, and I understand what \"talking to your coworkers\" in this context actually implies.\n\nThe easy part is that LLMs probably do understand REST APIs vs. GraphQL, so you don't usually have to do the tutorial bits.\n\nOne of the things I hypothesize is that LLMs write bad code because *nobody agrees on what good code is*.  So you need to tell them what *your* standards are, and then they can usually follow them.",
              "score": 0,
              "created_utc": "2026-02-22 20:07:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e5xbb",
          "author": "Vivid_Guava6269",
          "text": "Which is an incredibly rare skill, especially in mixed IT/Policy/Business environmentsÂ ",
          "score": 1,
          "created_utc": "2026-02-20 09:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6famgk",
          "author": "Fulgren09",
          "text": "Are you orchestrating how to talk to your coworker a wrapping it in deployment code?Â ",
          "score": 1,
          "created_utc": "2026-02-20 14:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fd2aj",
          "author": "deadwisdom",
          "text": "This is called being reductive. You can break anything down into parts and argue semantics. But is it helpful?",
          "score": 1,
          "created_utc": "2026-02-20 14:23:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ghgjt",
          "author": "ThePixelHunter",
          "text": "Who would've thought that the future of productivity was communication skills? ðŸ˜±",
          "score": 1,
          "created_utc": "2026-02-20 17:34:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i1oau",
          "author": "Disastrous-Angle-591",
          "text": "No.Â ",
          "score": 1,
          "created_utc": "2026-02-20 22:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j00g9",
          "author": "kyngston",
          "text": "i swear at my AI way more than my coworkers",
          "score": 1,
          "created_utc": "2026-02-21 01:16:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pbfhz",
          "author": "Street_Program_7436",
          "text": "Agree with a lot of thoughts here on how prompt engineering is a combo of clear communication and being able to break down a problem into smaller sub problems. This is probably one of the main reasons why automatic prompt engineering isnâ€™t that great (yet), at least in my experience.\nIf we include â€œstatistically making sure that your prompt performs with high accuracyâ€ in the definition of prompt engineering, then that changes things even more IMO.\n\nAnybody can â€œvibe promptâ€ and eyeball outputs, but not everybody can actually make sure that their prompt performs at scale when itâ€™s generating millions and millions of outputs.",
          "score": 1,
          "created_utc": "2026-02-22 01:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tww7f",
          "author": "robhanz",
          "text": "100% the people that have good communication skills are the ones getting better results out of LLMs.",
          "score": 1,
          "created_utc": "2026-02-22 20:03:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjn83",
          "author": "MokoshHydro",
          "text": "Its more like talking with \"rain man\". Kinda, guess what mental illness it has today.",
          "score": 1,
          "created_utc": "2026-02-25 16:14:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdyu2f",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Was Wrong.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdyu2f/we_benchmarked_7_chunking_strategies_most_best/",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-25 01:03:00",
      "score": 66,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "If you've built a RAG system, you've had the chunking conversation. Somebody on your team (or [a Medium post](https://medium.com/%40adnanmasood/chunking-strategies-for-retrieval-augmented-generation-rag-a-comprehensive-guide-5522c4ea2a90)) told you to \"just use 512 tokens with 50-token overlap\" or \"semantic chunking is strictly better.\"\n\nWe (hello from the R&D team at Vecta!) decided to test these claims. We created a small corpus of real academic papers spanning AI, astrophysics, mathematics, economics, social science, physics, chemistry, and computer vision. Then, we ran every document through seven different chunking strategies and measured retrieval quality and downstream answer accuracy.\n\nCritically, we designed the evaluation to beÂ fair: each strategy retrieves a different number of chunks, calibrated so that every strategy gets approximatelyÂ 2,000 tokens of contextÂ in the generation prompt. This eliminates the confound where strategies with larger chunks get more context per retrieval, and ensures we're measuring chunking quality, not context window size.\n\nThe \"boring\" strategies won. The hyped strategies failed. And the relationship between chunk granularity and answer quality is more nuanced than most advice suggests.\n\n# Setup\n\n# Corpus\n\nWe assembled a diverse corpus of 50 academic papers (905,746 total tokens) deliberately spanning similar disciplines, writing styles, and document structures: Papers ranged from 3 to 112 pages and included technical dense mathematical proofs pertaining to fundamental ML research. All PDFs were converted to clean markdown usingÂ [MarkItDown](https://github.com/microsoft/markitdown), with OCR artifacts and single-character fragments stripped before chunking.\n\n# Chunking Strategies Tested\n\n1. **Fixed-size, 512 tokens**, 50-token overlap\n2. **Fixed-size, 1024 tokens**, 100-token overlap\n3. **Recursive character splitting**, LangChain-styleÂ `RecursiveCharacterTextSplitter`Â at 512 tokens\n4. **Semantic chunking**, embedding-based boundary detection (cosine similarity threshold 0.7)\n5. **Document-structure-aware**, splitting on markdown headings/sections, max 1024 tokens\n6. **Page-per-chunk**, one chunk per PDF page, using MarkItDown's form-feed (`\\f`) page boundaries\n7. **Proposition chunking**, LLM-decomposed atomic propositions followingÂ [Dense X Retrieval](https://arxiv.org/abs/2312.06648)Â with the paper's exact extraction prompt\n\nAll chunks were embedded withÂ `text-embedding-3-small`Â and stored in local ChromaDB. Answer generation usedÂ `gemini-2.5-flash-lite`Â via OpenRouter. We generated 30 ground-truth Q&A pairs using Vecta's synthetic benchmark pipeline.\n\n# Equal Context Budget: Adaptive Retrieval k\n\nMost chunking benchmarks use a fixed top-k (e.g., k=10) for all strategies. This is fundamentally unfair: if fixed-1024 retrieves 10 chunks, the generator sees \\~10,000 tokens of context; if proposition chunking retrieves 10 chunks at 17 tokens each, the generator gets \\~170 tokens. The larger-chunk strategy wins by default because it gets more context, not because its chunking is better.\n\nWe fix this by computing anÂ adaptive kÂ for each strategy. This targets \\~2,000 tokens of retrieved context for every strategy. The computed values:\n\n|Strategy|Avg Tokens/Chunk|Adaptive k|Expected Context|\n|:-|:-|:-|:-|\n|Page-per-Chunk|961|2|\\~1,921|\n|Doc-Structure|937|2|\\~1,873|\n|Fixed 1024|658|3|\\~1,974|\n|Fixed 512|401|5|\\~2,007|\n|Recursive 512|397|5|\\~1,984|\n|Semantic|43|46|\\~1,983|\n|Proposition|17|115|\\~2,008|\n\nNow every strategy gets \\~2,000 tokens to work with. Differences in accuracy reflect genuine chunking quality, not context budget.\n\n# How We Score Retrieval: Precision, Recall, and F1\n\nWe evaluate retrieval at two granularities:Â page-levelÂ (did we retrieve the right pages?) andÂ document-levelÂ (did we retrieve the right documents?). At each level, the core metrics are precision, recall, and F1.\n\nLetÂ RÂ be the set of retrieved items (pages or documents) andÂ GÂ be the set of ground-truth relevant items.\n\nPrecision measures: of everything we retrieved, what fraction was actually relevant? A retriever that returns 5 pages, 4 of which contain the answer, has a precision of 0.8. High precision means low noise in the context window.\n\nRecall measures: of everything thatÂ *was*Â relevant, what fraction did we find? If 3 pages contain the answer and we retrieved 2 of them, recall is 0.67. High recall means we're not missing important information.\n\nF1 is the harmonic mean of precision and recall. It penalizes strategies that trade one for the other and rewards balanced retrieval.\n\nPage-level metrics tell you whether you're pulling the rightÂ *passages*. Document-level metrics tell you whether you're pulling from the rightÂ *sources*. A strategy can score high page-level recall (finding many relevant pages) while scoring low document-level precision (those pages are scattered across too many irrelevant documents). As we'll see, the tension between these two levels is one of the main findings.\n\n# Results\n\n# The Big Picture\n\n[Figure 1:Â Complete metrics heatmap. Green is good, red is bad.](https://www.runvecta.com/blog/chunking/metrics_heatmap.png)\n\n|Strategy|k|Doc F1|Page F1|Accuracy|Groundedness|\n|:-|:-|:-|:-|:-|:-|\n|**Recursive 512**|5|0.86|**0.92**|**0.69**|0.81|\n|**Fixed 512**|5|0.85|0.88|0.67|**0.85**|\n|**Fixed 1024**|3|**0.88**|0.72|0.61|0.86|\n|**Doc-Structure**|2|0.88|0.69|0.52|0.84|\n|**Page-per-Chunk**|2|0.88|0.69|0.57|0.81|\n|**Semantic**|46|0.42|0.91|0.54|0.81|\n|**Proposition**|115|0.27|**0.97**|0.51|**0.87**|\n\n**Recursive splitting wins on accuracy (69%) and page-level retrieval (0.92 F1).**Â The 512-token strategies lead on generation quality, while larger-chunk strategies lead on document-level retrieval but fall behind on accuracy.\n\n# Finding 1: Recursive and Fixed Splitting Often Outperforms Fancier Strategies\n\n[Figure 2:Â Accuracy and groundedness by strategy. Recursive and fixed 512 lead on accuracy.](https://www.runvecta.com/blog/chunking/generation_quality.png)\n\nLangChain'sÂ `RecursiveCharacterTextSplitter`Â at 512 tokens achieved the highest accuracy (**69%**) across all seven strategies. Fixed 512 was close behind at 67%. Both strategies use 5 retrieved chunks for \\~2,000 tokens of context.\n\nWhy does recursive splitting edge out plain fixed-size? It tries to break at natural boundaries, paragraph breaks, then sentence breaks, then word breaks. On academic text, this preserves logical units: a complete paragraph about a method, a full equation derivation, a complete results discussion. The generator gets chunks that make semantic sense, not arbitrary windows that may cut mid-sentence.\n\nRecursive 512 also achieved the best page-level F1 (**0.92**), meaning it reliably finds the right pagesÂ *and*Â produces accurate answers from them.\n\n# Finding 2: The Granularity-Retrieval Tradeoff Is Real\n\n[Figure 3:Â Radar chart, recursive 512 (orange) has the fullest coverage. Large-chunk strategies skew toward doc retrieval but lose on accuracy.](https://www.runvecta.com/blog/chunking/radar_comparison.png)\n\nWith a 2,000-token budget, a clear tradeoff emerges:\n\n* **Smaller chunks (k=5)**Â achieve higher accuracy (67-69%) because 5 retrieval slots let you sample from 5 different locations in the corpus, each precisely targeted\n* **Larger chunks (k=2-3)**Â achieve higher document F1 (0.88) because each retrieved chunk spans more of the relevant document, but the generator gets fewer, potentially less focused passages\n\nFixed 1024 scored the best document F1 (**0.88**) but only 61% accuracy. With just k=3, you get 3 large passages, great for document coverage, but if even one of those passages isn't well-targeted, you've wasted a third of your context budget.\n\n# Finding 3: Semantic Chunking Collapses at Scale\n\n[Figure 4:Â Chunk size distribution. Semantic and proposition chunking produce extremely small fragments.](https://www.runvecta.com/blog/chunking/chunk_size_distribution.png)\n\nSemantic chunking producedÂ 17,481 chunks averaging 43 tokensÂ across 50 papers. With k=46, the retriever samples from 46 different tiny chunks. The result: onlyÂ 54% accuracyÂ andÂ 0.42 document F1.\n\nHigh page F1 (0.91) reveals what's happening: the retrieverÂ *finds the right pages*Â by sampling many tiny chunks from across the corpus. But document-level retrieval collapses because those 46 chunks come from dozens of different documents, diluting precision. And accuracy suffers because 46 disconnected sentences don't form a coherent narrative for the generator.\n\n**The fundamental problem:**Â semantic chunking optimizes for retrieval-boundary purity at the expense of context coherence. Each chunk is a \"clean\" semantic unit, but a single sentence chunk may lack the surrounding context needed for generation.\n\n# Finding 4: The Page-Level Retrieval Story\n\n[Figure 5:Â Page-level precision-recall tradeoff. Recursive 512 achieves the best balance.](https://www.runvecta.com/blog/chunking/precision_recall.png)\n\n[Figure 6:Â Page-level and document-level F1. The two metrics tell different stories.](https://www.runvecta.com/blog/chunking/retrieval_f1.png)\n\nPage-level and document-level retrieval tell opposite stories under constrained context:\n\n* Fine-grained strategiesÂ (proposition k=115, semantic k=46) achieve high page F1 (0.91-0.97) by sampling many pages, but low doc F1 (0.27-0.42) because those pages come from too many documents\n* Coarse strategiesÂ (page-chunk k=2, doc-structure k=2) achieve high doc F1 (0.88) by retrieving fewer, more relevant documents, but lower page F1 (0.69) because 2 chunks can only cover 2 pages\n\nRecursive 512 at k=5 hits the best balance: 0.92 page F1 and 0.86 doc F1. Five chunks is enough to sample multiple relevant pages while still concentrating on a few documents.\n\n[Figure 7:Â Document-level precision, recall, and F1 detail. Large-chunk strategies lead on precision; fine-grained strategies lead on recall.](https://www.runvecta.com/blog/chunking/document_level.png)\n\n# What This Means for Your RAG System\n\n# The Short Version\n\n1. Use recursive character splitting at 512 tokens.Â It scored the highest accuracy (69%), best page F1 (0.92), and strong doc F1 (0.86). It's the best all-around strategy on academic text.\n2. Fixed-size 512 is a strong runner-upÂ with 67% accuracy and the highest groundedness among the top performers (85%).\n3. If document-level retrieval matters most, use fixed-1024 or page-per-chunk (0.88 doc F1), but accept lower accuracy (57-61%).\n4. Don't use semantic chunking on academic text.Â It fragments too aggressively (43 avg tokens) and collapses on document retrieval (0.42 F1).\n5. Don't use proposition chunking for general RAG.Â 51% accuracy isn't production-ready. It's only viable if you value groundedness over correctness.\n6. When benchmarking, equalize the context budget.Â Fixed top-k comparisons are misleading. Use adaptive k = round(target\\_tokens / avg\\_chunk\\_tokens).\n\n# Why Academic Papers Specifically?\n\nWe deliberately chose to saturate the academic paper region of the embedding space with 50 papers spanning 10+ disciplines. When your knowledge base contains papers that all discuss \"evaluation,\" \"metrics,\" \"models,\" and \"performance,\" the retriever has to make fine-grained distinctions. That's when chunking quality matters most.\n\nIn a mixed corpus of recipes and legal contracts, even bad chunking might work because the embedding distances between domains are large. Academic papers are theÂ *hard case*Â for chunking, and if a strategy works here, it'll work on easier data too.\n\n# How We Measured This (And How You Can Too)\n\nMy team builtÂ [Vecta](https://www.runvecta.com/)Â specifically to meet the need for precise RAG evaluation software. It generates synthetic benchmark Q&A pairs across multiple semantic granularities, then measures precision, recall, F1, accuracy, and groundedness against your actual retrieval pipeline.\n\nThe benchmarks in this post were generated and evaluated using Vecta's SDK (`pip install vecta`)\n\n# Limitations, Experiment Design, and Further Work\n\nThis experiment was deliberately small-scale: 50 papers, 30 synthetic Q&A pairs, one embedding model, one retriever, one generator. That's by design. We wanted something reproducible that a single engineer could rerun in an afternoon, not a months-long research project. The conclusions should be read with that scope in mind.\n\nSynthetic benchmarks are not human benchmarks.Â Our ground-truth Q&A pairs were generated by Vecta's own pipeline, which means there's an inherent alignment between how questions are formed and how they're evaluated. Human-authored questions would be a stronger test. That said, Vecta's benchmark generation does produce complex multi-hop queries that require synthesizing information across multiple chunks and document locations, so these aren't trivially easy questions that favor any one strategy by default.\n\nOne pipeline, one result.Â Everything here runs onÂ text-embedding-3-small, ChromaDB, andÂ gemini-2.5-flash-lite. Swap any of those components and the rankings could shift. We fully acknowledge this. Running the same experiment across multiple embedding models, vector databases, and generators would be valuable follow-up work, and it's on our roadmap.\n\nThe equal context budget is a deliberate constraint, not a flaw.Â Some readers may object that semantic and proposition chunking are \"meant\" to be paired with rerankers, fusion, or hierarchical aggregation. But if a chunking strategy only works when combined with additional infrastructure, that's important to know. Equal context budgets ensure we're comparing chunking quality at roughly equal generation cost. A strategy that requires a reranker to be competitive is a more expensive strategy, and that should factor into the decision.\n\nSemantic chunking was not intentionally handicapped.Â Our semantic chunking produced fragments averaging 43 tokens, which is smaller than most production deployments would target. This was likely due to a poorly tuned cosine similarity threshold (0.7) rather than any deliberate sabotage. But that's actually the point: semantic chunking requires careful threshold tuning, merging heuristics, and often parent-child retrieval to work well. When those aren't perfectly dialed in, it degrades badly. Recursive splitting, by contrast, produced strong results with default parameters. The brittleness of semantic chunking under imperfect tuning is itself a finding.\n\n**What we'd like to do next:**\n\n* Rerun the experiment with human-authored Q&A pairs alongside the synthetic benchmark\n* Test across multiple embedding models (`text-embedding-3-large`, open-source alternatives) and generators (GPT-4o, Claude, Llama)\n* Add reranking and hierarchical retrieval stages, then measure whether the rankings change when every strategy gets access to the same post-retrieval pipeline\n* Expand the corpus beyond academic papers to contracts, documentation, support tickets, and other common RAG domains\n* Test semantic chunking with properly tuned thresholds, chunk merging, and sliding windows to establish its ceiling\n\nIf you run any of these experiments yourself, we'd genuinely like to see the results.\n\nHave a chunking strategy that worked surprisingly well (or badly) for you? We'd love to hear about it. Reach out via DM!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdyu2f/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o79553a",
          "author": "250umdfail",
          "text": "Your experiments were designed for the standard chunking strategies to win. All your papers are independent, and share very little context with each other- why would you need a complicated RAG setup? I'm assuming your queries were simple too, like what is x, explain y etc.\n\nIf you have a complicated set of documents, that have a dependence graph, a cross reference network, and ask questions that need traversing the entire knowledge graph, things would be very different. Queries like: how did x evolve over time?, how does y compare to z, explain u in terms of v, etc. are where the advanced chunking, embedding, and linking shine.\n\nAlso I'm not sure F1 scores are a good way to grade your chunking strategies. Most modern systems use a high recall phase followed by a high precision stage over those recalled documents. You could possibly use a human or a larger model to find the similarity between their answer, and that of your model to score your strategies.",
          "score": 17,
          "created_utc": "2026-02-25 02:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79n5vk",
              "author": "Mythril_Zombie",
              "text": "Where is the list of papers?",
              "score": 1,
              "created_utc": "2026-02-25 04:00:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7akqqn",
              "author": "SmihtJonh",
              "text": "Recursive ensemble NER can mitigate some, but I agree that chunking in itself I prone to lossy semantics",
              "score": 1,
              "created_utc": "2026-02-25 08:26:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cgsq8",
              "author": "Confident-Honeydew66",
              "text": "This is a very well-thought-out comment, so first of all thank you.\n\n>\\>If you have a complicated set of documents, that have a dependence graph, a cross reference network, and ask questions that need traversing the entire knowledge graph, things would be very different.\n\nI agree, and acknowledge this is a serious blind spot in this analysis. Currently pushing for a second experiment using a more complex and inter-dependent dataset that requires multi-hop reasoning over similar-looking chunks.\n\n>\\>I'm not sure F1 scores are a good way to grade your chunking strategies.\n\nF1 score is often left behind in favor of recall, but precision, which is admittedly less important, ultimately determines if the downstream LLM is prone to context rot. Ideally, both recall and precision are maximized in the final retrieved chunks, hence our use of F1 as a benchmark.\n\n>\\>Most modern systems use a high recall phase followed by a high precision stage over those recalled documents.\n\nBy the end of these stages, the F1 should still be high. It is irrelevant if the retriever has multiple stages -- our analysis only considers the end result of the retrieval process.",
              "score": 1,
              "created_utc": "2026-02-25 16:01:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7dvscz",
                  "author": "250umdfail",
                  "text": "F1 treats both the recall and precision stages equally. In a multistage system, the stages are dependent and F1 is highly misleading, especially because RAG is a ranking system not a classifier. There are other scores you might want to consider like precision@K or NDCG.",
                  "score": 1,
                  "created_utc": "2026-02-25 19:53:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o79ifgi",
          "author": "Caesarr",
          "text": "Interesting analysis. Chunking on paragraphs (i.e. human-defined atomic blocks of semantic meaning) would also be worth testing, no?",
          "score": 2,
          "created_utc": "2026-02-25 03:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ajq0u",
          "author": "Healthy_Library1357",
          "text": "Two quick thoughts:  \nParagraph length variance matters. In papers, some paragraphs are 80 tokens, others 600+. Without a cap + light overlap, youâ€™ll get unstable context budgets and retrieval noise. It might improve coherence but hurt coverage. If avg paragraph is \\~300â€“400 tokens, your adaptive k probably lands around 5â€“6 for a 2k token budget. Thatâ€™s similar to Recursive 512, so gains may be marginal unless paragraphs align unusually well with answer spans.\n\nWould be interesting to see paragraph + max 768 cap vs Recursive 512 head-to-head. My bet: small lift in groundedness, similar accuracy, slightly worse page recall.",
          "score": 1,
          "created_utc": "2026-02-25 08:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7atbpn",
          "author": "ManufacturerWeird161",
          "text": "Finally seeing someone test this instead of just repeating advice. Our team also found that a fixed 512-token approach tanked performance on our legal docs, where citations at the end of sections were critical for context. Your findings on parent-child hierarchies match our internal testing.",
          "score": 1,
          "created_utc": "2026-02-25 09:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1usv",
          "author": "Ok_Bedroom_5088",
          "text": "if you chunk, semantic will always win",
          "score": 1,
          "created_utc": "2026-02-25 11:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmes0",
          "author": "Snoo_24581",
          "text": "This is a great point. Thanks for sharing!",
          "score": 1,
          "created_utc": "2026-02-25 13:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cha4e",
          "author": "Confident-Honeydew66",
          "text": "Saw someone ask for a TLDR in the comments before it got deleted.\n\nTLDR: keep it simple, stupid",
          "score": 1,
          "created_utc": "2026-02-25 16:04:08",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7cjcxl",
          "author": "sophie_zlngr",
          "text": "You've hit your limit Â· resets 4am (Europe/Vienna)",
          "score": 1,
          "created_utc": "2026-02-25 16:13:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjvf2",
          "author": "Suspicious-Name4273",
          "text": "For source code chunking the story might be different. There are semantic chunking strategies for source code that are structure-aware, like this: https://chunkhound.github.io/under-the-hood/#design-philosophy\n\nWould be interesting if this has any real benefit to other chunking strategies or even dumb fulltext search.",
          "score": 1,
          "created_utc": "2026-02-25 16:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cyfs1",
          "author": "sophie_zlngr",
          "text": "You've hit your limit Â· resets 4am (Europe/Vienna)",
          "score": 1,
          "created_utc": "2026-02-25 17:22:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7d1a6i",
          "author": "AmphibianNo9959",
          "text": "Really appreciate you sharing this. This equal context budget approach is super smart and something I wish more benchmarks would adopt. Itâ€™s wild how much the best practice advice falls apart under a fair comparison.",
          "score": 1,
          "created_utc": "2026-02-25 17:35:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc0f1j",
      "title": "Opensource is truly catching up to commercial LLM coding offerings",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rc0f1j/opensource_is_truly_catching_up_to_commercial_llm/",
      "author": "1nam2nam",
      "created_utc": "2026-02-22 22:55:41",
      "score": 36,
      "num_comments": 18,
      "upvote_ratio": 0.76,
      "text": "( My crude thoughts in relatively bad english. Fuck you grammar Nazis. )\n\nGot frustrated by Claude Code base (20$) to do anything serious due to the high token usage. Gemini is unusable due to high volume (literally for last 16 hours. Not a single prompt) .\n\nFrustrated and tried opencode + Kimi 2.5. Blown away by the cost. Performance is nearly as good as Sonnet 4.5 (I prefer it to Opus 4.6 based on my own experience) or Gemini 3. \n\nI believe rude awakening for frontier labs as more devs are forced to switch. \n\n  \nThese labs won't command the high premium pricing hence valuations for long.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rc0f1j/opensource_is_truly_catching_up_to_commercial_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6wg2c6",
          "author": "Qxz3",
          "text": "Yup, these businesses don't have much of a moat. Their lofty valuations are beyond ridiculous.Â ",
          "score": 8,
          "created_utc": "2026-02-23 04:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72eu1j",
              "author": "TrainerThin",
              "text": "The moat is training costs. If Chinese government subsidezes their way to market dominance like other industries or steal, weâ€™ll all be using Chinese models soon.\n\nWhich is fine and healthy in theory. But lots of power goes to winners of AI race.",
              "score": 2,
              "created_utc": "2026-02-24 02:28:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wthrq",
          "author": "SourceOfConfusion",
          "text": "yeah, most enterprise use cases do not require frontier models. The open source models coming out of China are really quite good and fit most used cases.",
          "score": 6,
          "created_utc": "2026-02-23 06:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uxqew",
          "author": "Tema_Art_7777",
          "text": "Codex with chat gpt plus is a powerhouse. I donâ€™t run out. Anthropic rate limits like hell so avoid that. kimi 2.5 subscription is same price as openai - I certainly wouldâ€™t pay that much for kimi",
          "score": 8,
          "created_utc": "2026-02-22 23:13:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wwh2p",
              "author": "Low-Exam-7547",
              "text": "I have never hit any limits on Claude Code using Max.",
              "score": 4,
              "created_utc": "2026-02-23 07:02:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x2ptr",
                  "author": "sgtfoleyistheman",
                  "text": "I have the first max subscription and only use it for 2 personal projects. I've hit my weekly limit 3 weeks in a row! I'm surprised it's enough for your job",
                  "score": 3,
                  "created_utc": "2026-02-23 08:01:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6wwmex",
                  "author": "Tema_Art_7777",
                  "text": "that is max!! i am using a $20/m chatgpt plus. how much r u paying",
                  "score": 1,
                  "created_utc": "2026-02-23 07:04:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6v34d2",
              "author": "1nam2nam",
              "text": "Codex is roughly same tier as Gemini. It is too slow to get anything done",
              "score": 0,
              "created_utc": "2026-02-22 23:44:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6v6esr",
                  "author": "Tema_Art_7777",
                  "text": "Not sure what your tiering us but frontier models have access to much better hardware and have better inference speed. kimi folks complain about hardware constraints",
                  "score": 3,
                  "created_utc": "2026-02-23 00:03:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6waodz",
                  "author": "DurianDiscriminat3r",
                  "text": "Codex 5.3 is better than opus 4.6. Try scaffolding a project from a brief. It can handle complex projects way better. Opus is good at planning (very detailed) and codex is good at implementation.",
                  "score": 3,
                  "created_utc": "2026-02-23 04:09:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6vopxs",
                  "author": "Upbeat-Cloud1714",
                  "text": "Slow is subjective. Fast and sloppy is not useful and that's what most models do right now. Codex is great for complex repositories where time is subjective to the scale of the task at hand. ",
                  "score": 1,
                  "created_utc": "2026-02-23 01:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2w01",
          "author": "lsmith77",
          "text": "open weight != open source",
          "score": 4,
          "created_utc": "2026-02-23 08:03:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wabok",
          "author": "Pleasant_Heat7314",
          "text": "I agree, it's been really impressive to watch. I think this trend is likely to accelerate over the next year or two.",
          "score": 1,
          "created_utc": "2026-02-23 04:06:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xhi0o",
          "author": "Zeikos",
          "text": "As soon as I'll get my reverse proxy done on my homelab I'll drop all services in favour of my self hosted OpenWebUI server.  \n\nAPIs are so much cheaper it's not even funny.  \nAnd I can hook my local models to it too.",
          "score": 1,
          "created_utc": "2026-02-23 10:26:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qyxc",
          "author": "cutlossking",
          "text": "How do I find a software engineer to help me code and build out with ai??",
          "score": 1,
          "created_utc": "2026-02-25 04:24:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbvlyb",
      "title": "If the current LLMs architectures are inefficient, why we're aggressively scaling hardware?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rbvlyb/if_the_current_llms_architectures_are_inefficient/",
      "author": "en00m",
      "created_utc": "2026-02-22 19:50:57",
      "score": 34,
      "num_comments": 32,
      "upvote_ratio": 0.85,
      "text": "Hello guys! As in the title, I'm genuinely curious about the current motivations on keeping information encoded as tokens, using transformers and all relevant state of art LLMs architecture/s.\n\nI'm at the beginning of the studies this field, enlighten me.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rbvlyb/if_the_current_llms_architectures_are_inefficient/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6tvzc2",
          "author": "SamWest98",
          "text": "To run the inefficient LLMs!Â ",
          "score": 50,
          "created_utc": "2026-02-22 19:58:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uucic",
              "author": "undo777",
              "text": "The good news is we'll have lots of power needs, maybe nuclear takes off!",
              "score": 5,
              "created_utc": "2026-02-22 22:54:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6w0guv",
                  "author": "rditorx",
                  "text": "What are the good news?",
                  "score": 1,
                  "created_utc": "2026-02-23 03:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6txags",
          "author": "i_wayyy_over_think",
          "text": "Thereâ€™s newer  techniques like Engrams by DeepSeek that tries to keep reasoning separate from knowledge. \n\nAlso GPUs are programmable so when new techniques are available, itâ€™s just a software update, so doesnâ€™t make sense to hold back hardware.",
          "score": 24,
          "created_utc": "2026-02-22 20:05:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72q7nw",
              "author": "Playful-Job2938",
              "text": "It does, these ai farms are putting us back into a worse spot than covid. The rest of the world needs compute too:",
              "score": 1,
              "created_utc": "2026-02-24 03:36:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6u5sjg",
              "author": "BarrenLandslide",
              "text": "Yes exactly this. Even the big KIMI K2 models, which are basically hundreds of SLM under the hood need at least like a 1 Mio USD rack to run on halfway usable quantisation.",
              "score": 1,
              "created_utc": "2026-02-22 20:48:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v4km7",
                  "author": "jeffdn",
                  "text": "That is not how MoE models work, and basically every model released in the last year has been an MoE, Kimi isnâ€™t special in that regard.",
                  "score": 6,
                  "created_utc": "2026-02-22 23:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u1ynt",
          "author": "typeryu",
          "text": "I like to think this as the same as saying â€œnuclear fusion energy is clearly better and safer than fission energyâ€. Almost everyone knows there are theoretically much more capable world simulators that should just get it (whatever that is), but we are not there yet and we donâ€™t even know if it is doable with the current hardware stack and data. LLMs are here and available now and they are far more capable than what is currently mainstream. Based on the incremental improvements weâ€™ve been getting, we still have many years of improvement ahead of us not to mention it will take even more time for the average folks and businesses to adopt the latest form which is agentic LLMs. That alone I think is enough to wipe out a ton of work and also accelerate development on other technologies so that is why money is being poured in. Thereâ€™s definitely some over investing going on in some places, but in general the big labs should come through as the new tech conglomerates.",
          "score": 3,
          "created_utc": "2026-02-22 20:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u37a3",
          "author": "docgpt-io",
          "text": "To the best of my knowledge, keeping information encoded as tokens has nothing to do with efficiency loss, it's rather the fact that we encode all information from the internet in giant neural networks and always talk to at least very large parts of the network - the LLMs shouldn't need to know how high the Eiffeltower is to help you with Maths, yet they do, and this is not efficient. I think the reasons why the spending keeps increasing anyway, are:  \n1. it still reaches out --> the value that can be created with LLMs is still remarkable and it makes sense to keep spending from an economic perspective  \n2. efficiency is rapidly improving",
          "score": 3,
          "created_utc": "2026-02-22 20:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u554x",
          "author": "BarrenLandslide",
          "text": "Because clever orchestration of SLMs, TLMs calling deterministic tools is the future.",
          "score": 3,
          "created_utc": "2026-02-22 20:45:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u5pck",
          "author": "funbike",
          "text": "Diffusion LLMs have a completely different architecture.   Someone took image-generation AI and applied it to text.  Look into Inception's Mercury, which performs well.",
          "score": 3,
          "created_utc": "2026-02-22 20:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u0mwr",
          "author": "kkania",
          "text": "Our power generation based on the Carnot cycle (so coal, gas, nuclear) is only 30-40% efficient, and weâ€™ve been at it for a hundred years at this point. People donâ€™t give a shit about efficiency in general, and it only becomes a thing when fuel runs out (eg oil for cars). Itâ€™ll probably need to happen with power for compute first before we see efficiency in ai getting improved.",
          "score": 4,
          "created_utc": "2026-02-22 20:21:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6txver",
          "author": "Mysterious-Rent7233",
          "text": ">Hello guys! As in the title, I'm genuinely curious about the current motivations on keeping information encoded as tokens, using transformers and all relevant state of art LLMs architecture/s.\n\nThe motivation is: \"This is what we know works. Other approaches are unproven research.\" That's all. There isn't a magic wand to invent a better architecture. You actually have to invent it. Which might take six months, six years or sixty years.",
          "score": 2,
          "created_utc": "2026-02-22 20:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tyu7o",
          "author": "Low-Opening25",
          "text": "Ok, so what do you propose, whatâ€™s your replacement architecture exactly? to me it seems like you didnâ€™t understand the fundamentals. LLM architecture is based on transformers and matrix multiplication and they operate on tokens.\n\nWhat you propose is equivalent of, hey, why computers have to operate on 0s and 1s and binary logic, why not mix this up?",
          "score": 2,
          "created_utc": "2026-02-22 20:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u1b9x",
          "author": "chickenAd0b0",
          "text": "Read Richard Suttonâ€™s â€œthe bitter lessonâ€ essay then youâ€™ll understand why everyone is scaling.",
          "score": 2,
          "created_utc": "2026-02-22 20:25:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x98ah",
              "author": "Mysterious-Rent7233",
              "text": "Everyone is scaling...except Sutton. Who believes they are scaling the wrong thing.",
              "score": 1,
              "created_utc": "2026-02-23 09:05:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uu79o",
          "author": "Tema_Art_7777",
          "text": "they will all improve - as new papers are emerging on optimization. However, for ai to be pervasive and ambient, the current infrastructure we have is woefully inadequate and investments are quite welcome. Anthropic is rate limiting the hell out of everyone as it is. I believe investors have faith that innovations will make things better with llm usage. While not a promised road to AGI at all, there is massive benefits still to be realized with what we currently have!",
          "score": 2,
          "created_utc": "2026-02-22 22:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4729",
          "author": "Fabulous-Possible758",
          "text": "Even with improving efficiency weâ€™re also increasing demand a lot.  Remember a single query now might be multiple tool calls, inferencing on the results, maybe *more* tool calls, and all of that on larger and larger context windows, and theyâ€™re still trying to sell and incorporate this into wider and and wider user bases.  A .9x improvement in compute usage still doesnâ€™t matter if you have 100x as many uses for it.",
          "score": 1,
          "created_utc": "2026-02-22 20:40:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uffx9",
          "author": "coloradical5280",
          "text": "Because the future architectures like JEPA, Test-Time Training, State Space Models, etc, are more efficient in many ways but still need a ton of compute, and unfortunately, probably more memory, so we need compute post-transformers too.",
          "score": 1,
          "created_utc": "2026-02-22 21:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6unmsn",
          "author": "imkindathere",
          "text": "Why do you say they're inefficient? I would say they're efficient because they can be fully parallelized. That's what allowed them to scale to the size they're at now",
          "score": 1,
          "created_utc": "2026-02-22 22:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v1pol",
          "author": "Sonoftalltree",
          "text": "Think about the Mag 7 and what options they have to continue growing their returns year after year, after they are already so big. Then think about the risk of AI eating their SaaS margins. The strategy is to have a tool no one else can run. In some respects, the inefficient nature is a feature because now startups have considerably less advantage.",
          "score": 1,
          "created_utc": "2026-02-22 23:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v5ate",
          "author": "red_hare",
          "text": "The path forward is fine-tuning smaller models for task-specific execution.\n\nBut user demand and progress on larger general purposes models is outpacing the cost of task-specific fine-tuning.\n\nBest thing that could happen to the industry right now would be a slowdown in SotA general purposes model progress.",
          "score": 1,
          "created_utc": "2026-02-22 23:56:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vuhm4",
          "author": "damnburglar",
          "text": "Among other things: Gold rush.",
          "score": 1,
          "created_utc": "2026-02-23 02:25:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vzypj",
          "author": "FirmSignificance1725",
          "text": "First I would say, define inefficient. Weâ€™ve very quickly grown accustomed to LLMs, but this is still new in the grand scheme of innovation. The transformer architecture is able to achieve a functionality prior impossible, even with data center level of resources. \n\nThere are many other interesting theoretical implications of transformers, but one of the biggest was the fact that it didnâ€™t follow the law of diminishing return as aggressively as other models. Most models were restricted to a specific type of task and/or topped off quickly when generalized, flattening regardless of parameter count increase. Transformers however have continuously gotten better and shown better generalizability as parameter count has increased. \n\nSo, I would say that while they are resource hogs, I would not generally classify the transformer as â€œinefficientâ€. Yes, maybe compared to a standard program, but that program has nowhere near the capability of the deployed LLM. I would say itâ€™s quite efficient for what it does and weâ€™re attempting to push it as far as we can at scale. \n\nThat being said, the reason weâ€™re scaling hardware is because product X shows some capability and economic benefit both short and long term, that companies have deemed it valuable enough to invest Y dollars for Z return. \n\nOptimizations constantly happen. Can use mixture of experts to reduce active params, better kernels, KV Cache, pipeline parallelism, quantizations, <insert technique here> to make it more efficient. And those techniques will continue to be discovered and implemented.\n\nBut, if we reached the threshold where value exceeds cost, then were executing",
          "score": 1,
          "created_utc": "2026-02-23 02:58:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xucnq",
          "author": "Valuable-Mix4359",
          "text": "I keep seeing the argument that â€œtransformers are inefficient, so why are we scaling hardware,â€ and I think it mixes two different layers of analysis.\n\nAt the model level, yes, transformers are expensive. Attention is costly, long context windows are costly, inference isnâ€™t lightweight. But they scale in a highly predictable way. More parameters + more data + more compute â†’ better performance, with relatively stable scaling curves. From an engineering perspective, that kind of predictability has significant value.\n\nAs long as marginal capability gains remain higher than marginal compute costs, scaling is not irrational. Itâ€™s an economic decision.\n\nWhat seems more interesting to me is that weâ€™re no longer operating at the â€œone call = one responseâ€ level. Production systems today are often multi-step pipelines: RAG, tool calls, retries, fallback models, agent loops, reflection passes, etc. A single user request can trigger multiple inferences and large context usage.\n\nEven if base model efficiency improves by 20%, total system-level compute can still increase because workflows become more complex. Lower unit cost tends to increase usage. This is no longer purely a model efficiency problem â€” itâ€™s an allocation problem at the system level.\n\nMany teams still default to routing most tasks to the largest available model, even when parts of the workflow could be handled by a smaller model or a deterministic component. Thatâ€™s not really about architectural elegance. Itâ€™s about compute routing.\n\nIâ€™m not convinced the main bottleneck is â€œfind a radically new architecture tomorrow.â€ It may be more about optimizing compute allocation across models, tasks, and constraints at the system layer. Scaling looks excessive if you isolate the model, but less so when you look at the entire infrastructure stack.\n\nAre people here actually measuring cost per workflow rather than just cost per token?",
          "score": 1,
          "created_utc": "2026-02-23 12:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xzv33",
          "author": "Potential-Leg-639",
          "text": "LLMs are getting more efficient and there is still a shortage on hardware. Better be prepared.",
          "score": 1,
          "created_utc": "2026-02-23 12:56:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74gqx3",
          "author": "qubridInc",
          "text": "Because scaling hardware gives reliable gains *today*, even if the architecture isnâ€™t perfect.\n\nTransformers are easy to parallelize, scaling laws still hold, and all existing infra is built around them so, more compute = better models right now. New, more efficient architectures are being researched, but theyâ€™re not yet proven at the same scale.",
          "score": 1,
          "created_utc": "2026-02-24 12:16:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aroka",
          "author": "werdnum",
          "text": "Because we're pretty sure that growth in demand will outstrip efficiency gains.",
          "score": 1,
          "created_utc": "2026-02-25 09:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6typ8n",
          "author": "Fuzzy_Pop9319",
          "text": "As it happens, the elegant data structures that are being brute forced are from a finite structure, and as it happens in mathematics, no one will take you seriously or give you grants or hire you if you are using finite mathematics.    \nEverything else spawns from this.",
          "score": 0,
          "created_utc": "2026-02-22 20:12:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdmxi9",
      "title": "Are large language models actually generalizing, or are we just seeing extremely sophisticated memorization in a double descent regime?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdmxi9/are_large_language_models_actually_generalizing/",
      "author": "InevitableRespond494",
      "created_utc": "2026-02-24 17:40:25",
      "score": 25,
      "num_comments": 17,
      "upvote_ratio": 0.91,
      "text": "Iâ€™ve been trying to sharpen my intuition about large language models and Iâ€™d genuinely appreciate input from people who work in ML or have a strong technical background. Iâ€™m not looking for hype or anti-AI rhetoric, just a sober technical discussion.\n\nHereâ€™s what I keep circling around:\n\nLLMs are trained on next-token prediction. At the most fundamental level, the objective is to predict the next word given previous context. That means the training paradigm is imitation. The system is optimized to produce text that statistically resembles the text it has seen before. So I keep wondering: if the objective is imitation, isnâ€™t the best possible outcome simply a very good imitation? In other words, something that behaves as if it understands, while internally just modeling probability distributions over language?\n\nWhen people talk about â€œemergent understanding,â€ Iâ€™m unsure how to interpret that. Is that a real structural property of the model, or are we projecting understanding onto a system that is just very good at approximating linguistic structure?\n\nAnother thing that bothers me is memorization versus generalization. We know there are documented cases of LLMs reproducing copyrighted text, reconstructing code snippets from known repositories, or instantly recognizing classic riddles and bias tests. That clearly demonstrates that memorization exists at non-trivial levels. My question is: how do we rigorously distinguish large-scale memorization from genuine abstraction? When models have hundreds of billions of parameters and are trained on massive internet-scale corpora, how confident are we that scaling is producing true generalization rather than a more distributed and statistically smoothed form of memorization?\n\nThis connects to overfitting and double descent. Classical ML intuition would suggest that when model capacity approaches or exceeds dataset complexity, overfitting becomes a serious concern. Yet modern deep networks, including LLMs, operate in highly overparameterized regimes and still generalize surprisingly well. The double descent phenomenon suggests that after the interpolation threshold, performance improves again as capacity increases further. I understand the empirical evidence for double descent in various domains, but I still struggle with what that really means here. Is the second descent genuinely evidence of abstraction and structure learning, or are we simply in a regime of extremely high-dimensional interpolation that looks like generalization because the data manifold is densely covered?\n\nThen thereâ€™s the issue of out-of-distribution behavior. In my own experiments, when I formulate problems that are genuinely new, not just paraphrased or slightly modified from common patterns, models often start to hallucinate or lose coherence. Especially in mathematics or formal reasoning, if the structure isnâ€™t already well represented in the training distribution, performance degrades quickly. Is that a fundamental limitation of text-only systems? Is it a data quality issue? A scaling issue? Or does it reflect the absence of a grounded world model?\n\nThat leads to the grounding problem more broadly. Pure language models have no sensorimotor interaction with the world. They donâ€™t perceive, manipulate, or causally intervene in physical systems. They donâ€™t have multimodal grounding unless explicitly extended. Can a system trained purely on text ever develop robust causal understanding, or are we mistaking linguistic coherence for a world model? When a model explains what happens if you tilt a table and a phone slides off, is it reasoning about physics or statistically reproducing common narrative patterns about objects and gravity?\n\nIâ€™m also curious about evaluation practices. With web-scale datasets, how strictly are training and evaluation corpora separated? How do we confidently prevent benchmark contamination when the training data is effectively â€œthe internetâ€? In closed-source systems especially, how much of our trust relies on company self-reporting? Iâ€™m not implying fraud, but the scale makes rigorous guarantees seem extremely challenging.\n\nThereâ€™s also the question of model size relative to data. Rough back-of-the-envelope reasoning suggests that the total volume of publicly available text on the internet is finite and large but not astronomically large compared to modern parameter counts. Given enough capacity, is it theoretically possible for models to internally encode enormous portions of the training corpus? Are LLMs best understood as knowledge compressors, as structure learners, or as extremely advanced semantic search systems embedded in a generative architecture?\n\nBeyond the technical layer, I think incentives matter. There is massive economic pressure in this space. Investment cycles, competition between companies, and the race narrative around AGI inevitably shape communication. Are there structural incentives that push capability claims upward? Even without malicious intent, does the funding environment bias evaluation standards or public framing?\n\nFinally, I wonder how much of the perceived intelligence is psychological. Humans are extremely prone to anthropomorphize coherent language. If a system speaks fluently and consistently, we instinctively attribute intention and understanding. To what extent is the â€œwow factorâ€ a cognitive illusion on our side rather than a deep ontological shift on the modelâ€™s side?\n\nAnd then thereâ€™s the resource question. Training and deploying large models consumes enormous computational and energy resources. Are we seeing diminishing returns masked by scale? Is the current trajectory sustainable from a systems perspective?\n\nSo my core question is this: are modern LLMs genuinely learning abstract structure in a way that meaningfully transcends interpolation, or are we observing extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime that happens to look intelligent?\n\nIâ€™d really appreciate technically grounded perspectives. Not hype, not dismissal, just careful reasoning from people whoâ€™ve worked close to these systems.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdmxi9/are_large_language_models_actually_generalizing/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o76bwrm",
          "author": "hymn_7-62",
          "text": "You raise good questions and I'm interested in answers, sadly I dont think we'll get lucky with someone who actually knows their shit.",
          "score": 7,
          "created_utc": "2026-02-24 17:55:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76jy1x",
          "author": "McMonty",
          "text": "First off: Great post!\n\nMy Ask: You'll need to define\n\n\\> meaningfully transcends interpolation\n\nI think a lot of research in the AI field was in this area during the early AI stages that pre-dated NNs decades ago. Personally, I've always liked Hofstadter's takes on AI such as those in \"I am a strange loop\". I doubt you'll find much better answers to \"what even is generalization\" than in his writing(GEB and \"Surfaces and Essences\" are also great!).  \n  \nBut although he was initially skeptical of LLMs, he has changed his tone a bit in these past few years to start to question if the recursive elements present in LLMs has hit a turning point where we should be questioning what it is that we've created: [https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai](https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai)\n\nMy own 2 cents: There is something to LLMs beyond just memorization, but its constrained still in a way that differs from how our own brains are constrained(Ultimately, our own ability to generalize is still subject to limits). I might even go as far as to say that I'd consider LLMs to be \"capable of consciousness\" to some extent - although I don't think I'd say that they are \"alive\". They are in a weird space where all of our definitions start to break down and are severely lacking nuance to describe the variety of possible forms of cognition. Similar things happen when you really peel back the layers between different forms of animal minds and compare them with human ones, but this is even weirder.",
          "score": 8,
          "created_utc": "2026-02-24 18:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78kcxf",
          "author": "PresentSituation8736",
          "text": "1. The \"World Model\" vs. High-Dimensional Interpolation You asked if models are genuinely learning abstract structure or just operating in an overparameterized interpolation regime. The consensus among interpretability researchers (looking at things like mechanistic interpretability and induction heads) is: Itâ€™s both, but leaning heavily toward sophisticated interpolation. LLMs do learn abstract representations. They don't just memorize strings of text; they build latent features for concepts (e.g., a \"gender\" direction, a \"formality\" vector, or coding syntax trees). To predict the next token efficiently across petabytes of data, the network must compress the data. And the best way to compress data is to discover the underlying generative rules. However, this does not equal a causal \"World Model.\" When the model describes a phone sliding off a tilted table, it is not running a physics engine in its latent space. It is navigating the semantic topology of how humans talk about physics. This is why LLMs fail so catastrophically on Out-of-Distribution (OOD) reasoning, spatial tasks, or novel math. If the solution isn't densely represented in the training manifold, the model cannot extrapolate. It can only interpolate. \n\n2. Memorization vs. Abstraction (The Double Descent Reality) You brought up double descent. In the overparameterized regime, models perfectly fit the training data (memorization) and then find the \"simplest\" function that interpolates between those points (generalization). But here is the dirty secret of modern LLMs: the training data is so massive that the \"data manifold\" covers almost every common human thought. What looks like zero-shot generalization to us is often just the model finding a latent bridge between two memorized concepts. It is \"generalizing,\" but strictly within the convex hull of human internet text. \n\n3. The Benchmark Contamination Crisis You asked: \"How strictly are training and evaluation corpora separated?\" They aren't. This is the biggest open secret in the industry right now. With web-scale scraping, almost every classic riddle, math problem, and coding test is in the training data. Companies try to de-duplicate and filter, but it is practically impossible to prevent \"data leakage\" entirely. Many \"emergent capabilities\" reported in 2023 were later debunked as the models simply having seen the test set during training. This is why closed-source claims must be taken with a massive grain of salt.\n\n 4. The Anthropomorphic Illusion & Incentives Your point about the ELIZA effect (anthropomorphism) is the psychological engine driving the current hype cycle. We are evolutionarily hardwired to attribute consciousness to fluent language. When an LLM uses the word \"I\", our brains immediately project a mind onto it. Combine this cognitive bias with the VC funding environment, and you get a toxic incentive structure. Companies are incentivized to frame sophisticated statistical pattern-matching as \"sparks of AGI\" because that unlocks billions in computing budgets. If they admitted, \"We built a lossy, trillion-parameter semantic search engine,\" the valuations would crash. The Conclusion To answer your core question: Modern LLMs are highly advanced, lossy knowledge compressors. They do learn structural abstractions of language (grammar, tone, logic structures), but they use these structures to perform statistical pattern completion. \n\nThey lack grounded causality, they cannot reliably extrapolate outside their training distribution, and their \"reasoning\" is a simulation driven by the linguistic shadows of human thought. It is a breathtaking engineering achievement, but your intuition is correct: we are largely confusing linguistic coherence for ontological intelligence. Keep pulling on these threads. The industry needs this level of skepticism right now.",
          "score": 3,
          "created_utc": "2026-02-25 00:20:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ceova",
              "author": "AI-Agent-geek",
              "text": "Great response",
              "score": 1,
              "created_utc": "2026-02-25 15:52:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77tjxi",
          "author": "Bulky-Flamingo9898",
          "text": ">In other words, something that behaves as if it understands, while internally just modeling probability distributions over language?\n\nI think â€œbehaves as if it understandsâ€ isnâ€™t really distinguishable from mimicking language patterns in general. So as the better the models get at generating language similar to the training set it will inevitably sound more human and as though it understands. \n\n>are we projecting understanding onto a system that is just very good at approximating linguistic structure?\n\nPartly that but the models do seem to have properties that seem to imply they are doing more than simply spewing back  training material. One thing that struck me early is how well llms seem to be able to rhyme and so if you ask it for a song it will create awful doggrel but it does rhyme. Hard to square the behaviour without thinking that it must in some sense be storing information about the sounds of words along with meaning and is able to invoke this in certain contexts. Not sure this has to be understanding but itâ€™s related and seems deeper than the stochastic parrot caricature  \n\n>Or does it reflect the absence of a grounded world model? \n\nI would maybe characterise the optimistic view as being if you feed a big enough model enough data it will work out something near a world model itself but as you point out memoization is also happening and what seems to be tough is to encourage good world model building. Training purely for next word probably is close to diminishing returns \n\n>are modern LLMs genuinely learning abstract structure in a way that meaningfully transcends interpolation, or are we observing extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime that happens to look intelligent? \n\nThere seems to be something more than straight interpolation going on, but not enough to make me think AGI is just around the corner",
          "score": 2,
          "created_utc": "2026-02-24 21:59:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79yfxa",
          "author": "MrRandom04",
          "text": "A well-supported rebuttal to the idea that autoregressive language models cannot really learn global reasoning, planning and abstraction: https://arxiv.org/abs/2512.15605",
          "score": 2,
          "created_utc": "2026-02-25 05:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ysup",
              "author": "MrRandom04",
              "text": "This paper, combined with the idea that sufficiently advanced broad RLVR post-training can allow well-documented generalization of capabilities to reach past human expert levels, is essentially the real research bet that the frontier labs are making with their current strides towards AGI.",
              "score": 2,
              "created_utc": "2026-02-25 05:19:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77xai4",
          "author": "theOmnipotentKiller",
          "text": "I think your perspective is based on how GPT-3 (circa 2022) was trained. Saying that LLMs do just next token prediction implies that pre-training is the only thing that matters. \n\nWe have gone through 4 distinct phases in post training since then\n- RLHF\n- structured json grammars\n- test-time search\n- (now) reinforcement learning on tool call sequences\n\nThis should make your question a lot simpler. Next token predictor GPT-3 still felt like BERT. Models today are so different. The above list I gave is like the top highlights of post training, thereâ€™s a lot more going on that we prolly donâ€™t even know. World models are being actively used to do better RL for agents right now for example.\n\nI think to understand pre-training you have to understand DPO. Next token prediction captured a lot of interesting behaviors in hard to elicit ways. Everything after has been a slow grind of finding the right eval harness and collect enough data to make that micro-behavior a macro-behavior through painstaking manual effort and hopefully some synthetic generation hacks. \n\nAs for true generalization, my only metric for that is how much revenue will Anthropic print per sector of the economy. I am an empiricist and I think the free markets will let you know if things are generalizing or not. Itâ€™s easy to fall for investor posturing optics so you really have to dig to know if they are. Anthropic for the most part has been honest in their communications based on what I have seen on the ground, other labs donâ€™t share as much as them. \n\nThis question is much better suited for r/mlscaling - youâ€™ll get better answers there. Model training is a gated profession so us LLM Devs can just conjecture and hope the next models just work. Evals went out the window in mid 2025 so itâ€™s all just vibes here now. Learning theory and all that is tech we hope the labs figure out.",
          "score": 3,
          "created_utc": "2026-02-24 22:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76qew6",
          "author": "visarga",
          "text": "\\> extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime\n\nan extremely sophisticated dunce that is fooling everyone? ",
          "score": 1,
          "created_utc": "2026-02-24 18:59:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77kb64",
              "author": "pab_guy",
              "text": "I think it's hard for people to accept that with intelligence, the map IS the territory.",
              "score": 2,
              "created_utc": "2026-02-24 21:17:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o79op5m",
          "author": "thisdude415",
          "text": "IMO the fact that LLMs are able to easily work with random UUIDs (which are by definition never before seen in their training data) demonstrates that there is something beyond memorization.",
          "score": 1,
          "created_utc": "2026-02-25 04:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o773u2j",
          "author": "dmter",
          "text": "neural networks are just emergent virtual machines that utilize layer machinery to emerge code that satisfies the training data\n\nf.ex. in some image processing nn there are actual image processing algorithms running between layers and nn learns to process input images by giving correct parameters to these algorithms and then it does some math on those results which is also emergent.\n\nsame is done inside llm but unlike image processing, people have no idea how it works so they just assume it's magic. hence hyperscaling fallacy.",
          "score": 1,
          "created_utc": "2026-02-24 20:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77k5ad",
          "author": "pab_guy",
          "text": "TL;DR\n\nHowever, note that your entire perceptual world as a human is recalling learned patterns.  The magic happens when we combine or exchange ideas across disciplines.\n\nFor background on this type of thing I recommend \"Everything is a remix\" and the  veritasium video on expertise.\n\nOne example: a chess grandmaster can memorize pieces on a chessboard very well.  But if you put pieces on the board in a way that doesn't reflect how a real game would play out (novel placements) the grandmaster's advantage evaporates.  The skill is based on memorizing and recognizing patterns.",
          "score": 0,
          "created_utc": "2026-02-24 21:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77oiy7",
              "author": "Officer_Trevor_Cory",
              "text": "\"One example: a chess grandmaster can memorize pieces on a chessboard very well. But if you put pieces on the board in a way that doesn't reflect how a real game would play out (novel placements) the grandmaster's advantage evaporates. The skill is based on memorizing and recognizing patterns.\"\n\nwell this is not a good example. grandmasters play freestyle, start from a massive advantage and adapt extremely quickly.",
              "score": 1,
              "created_utc": "2026-02-24 21:36:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77wa1x",
                  "author": "pab_guy",
                  "text": "How do you think they adapt extremely quickly? Because they know the patterns!",
                  "score": 1,
                  "created_utc": "2026-02-24 22:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1repniu",
      "title": "OpenAI is a textbook example of Conway's Law",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1repniu/openai_is_a_textbook_example_of_conways_law/",
      "author": "robertgambee",
      "created_utc": "2026-02-25 20:57:56",
      "score": 24,
      "num_comments": 5,
      "upvote_ratio": 0.93,
      "text": "There's a principle in software design called Conway's Law: organizations design systems that mirror their own communication structures (AKA shipping their org charts).\n\nOpenAI has two endpoints which do largely similar things: their older `chat/completions` API and the newer `responses` one. (Not to mention their even older `completions` endpoint that's now deprecated.)\n\nBoth let you generate text, call tools, and produce structured output. And at first glance, they look quite similar. But as you dig deeper, the differences quickly appear. Take structured outputs as an example. With `chat/completions`, you write:\n\n    {\n      \"response_format\": {\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n          \"name\": \"Response\",\n          \"description\": \"A response to the user's question\",\n          \"schema\": {\"type\": \"object\", \"properties\": ...}\n        }\n      }\n    }\n\nBut for `responses`, it needs to look like this:\n\n    {\n      \"text\": {\n        \"format\": {\n          \"type\": \"json_schema\",\n          \"name\": \"Response\",\n          \"description\": \"A response to the user's question\",\n          \"schema\": {\"type\": \"object\", \"properties\": ...}\n        }\n      }\n    }\n\nI see no reason why these need to be different. It makes me wonder if they're deliberately making it difficult to migrate from one endpoint to the other. And the docs don't explain this! They only have a couple of examples, at least one of which is incorrect. I had to read the source code in their Python package to figure it out.\n\nGoogle suffers from this too. Their Gemini API rejects JSON Schema with `{\"type\": \"array\", \"items\": {}}` (a valid schema meaning \"array of anything\"). Their official Python package silently rewrites the schema to make it compliant before sending. I like to imagine that someone on the Python package team got fed up with backend team for not addressing this and decided to fix it themselves.\n\nI admit that this isn't surprising for fast-moving orgs who are shipping features quickly. But it does put a lot of burden on developers to deal with lots of little quirks. And it makes me wonder what's going on inside these places.\n\nI wrote up [some more examples](https://everyrow.io/blog/llm-provider-quirks) of odd quirks in LLM provider APIs. Which ones have you had to deal with?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1repniu/openai_is_a_textbook_example_of_conways_law/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7elqpl",
          "author": "BuddhasFinger",
          "text": "Everything is a textbook example of Conway's Law because it's a \\*law\\* :-) \n\nOpenAI is not that bad, actually. Look at Antropic. You can clearly see the point in time when they hired the CRO. ",
          "score": 5,
          "created_utc": "2026-02-25 21:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7eja8z",
          "author": "ddp26",
          "text": "I feel like OpenAI does deprecate things a lot (like 4o). Why don't they deprecate the completions one?",
          "score": 0,
          "created_utc": "2026-02-25 21:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7f566v",
              "author": "Mysterious-Rent7233",
              "text": "OpenAI does NOT deprecate things in the API that enterprise users depend upon, actually.\n\nDavinci from November 2022, is still available in their APIs:\n\n  \n[https://developers.openai.com/api/docs/models/davinci-002](https://developers.openai.com/api/docs/models/davinci-002)\n\n",
              "score": 4,
              "created_utc": "2026-02-25 23:33:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7emxpk",
              "author": "Purple-Programmer-7",
              "text": "Theyâ€™re looking at ALL data.\n\nAnd that includes the number of requests to each endpoint.\n\nTheyâ€™re not going to depreciate anything until a critical mass has migrated.",
              "score": 3,
              "created_utc": "2026-02-25 22:00:23",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7gvv68",
              "author": "robogame_dev",
              "text": "Deprecating completions API would be nuts because itâ€™s the #1 interoperable API standard in use across multiple providers / 3rd party tools. \n\nOnce most providers and tools fully support Responses, you could deprecate - but for now, completions is still the #1 API to target if youâ€™re building production AI systems because if you target Responses, youâ€™ll reduce your interoperability for essentially no gain.",
              "score": 2,
              "created_utc": "2026-02-26 05:54:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rbmq30",
      "title": "not sure if hot take but mcps/skills abstraction is redundant",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rbmq30/not_sure_if_hot_take_but_mcpsskills_abstraction/",
      "author": "uriwa",
      "created_utc": "2026-02-22 14:07:08",
      "score": 23,
      "num_comments": 50,
      "upvote_ratio": 0.69,
      "text": "Whenever I read about MCPs and skills I can't help but think about the emperor's new clothes.\n\nThe more I work on agents, both for personal use and designing frameworks, I feel there is no real justification for the abstraction. Maybe there was a brief window when models weren't smart enough and you needed to hand-hold them through tool use. But that window is closing fast.\n\nIt's all just noise over APIs. Having clean APIs and good docs *is* the MCP. That's all it ever was.\n\nIt makes total sense for API client libraries to live in GitHub repos. That's normal software. But why do we need all this specialized \"search for a skill\", \"install a skill\" tooling? Why is there an entire ecosystem of wrappers around what is fundamentally just calling an endpoint?\n\nMy prediction: the real shift isn't going to be in AI tooling. It's going to be in businesses. **Every business will need to be API-first.** The companies that win are the ones with clean, well-documented APIs that any sufficiently intelligent agent can pick up and use.\n\nI've just changed some of my ventures to be API-first. I think pay per usage will replace SaaS.\n\nAI is already smarter than most developers. Stop building the adapter layer. Start building the API.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rbmq30/not_sure_if_hot_take_but_mcpsskills_abstraction/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6ry33f",
          "author": "Thick-Protection-458",
          "text": "Hm ... Wasn't MCP always exactly just the way to expose remote / other non-foreseen tools for use by the model?\n\n\nSo you know, not like some fancy magic idea, but just a way to provide standartized interface so services can provide it instead of going xkcd 14 standards situation?",
          "score": 23,
          "created_utc": "2026-02-22 14:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s69q9",
              "author": "das_war_ein_Befehl",
              "text": "For early models, yeah. Nowadays theyâ€™re pretty good at just using the API. MCP server are heavy on context and honestly LLMs work better with CLIs anyways",
              "score": 3,
              "created_utc": "2026-02-22 15:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6srghb",
                  "author": "ThenExtension9196",
                  "text": "The early models of mid 2025.",
                  "score": 7,
                  "created_utc": "2026-02-22 16:48:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sh1s8",
              "author": "uriwa",
              "text": "I don't think this will work long term. If it did, humans would do it too.",
              "score": 1,
              "created_utc": "2026-02-22 16:02:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s4okv",
          "author": "strangeanswers",
          "text": "putting an MCP server over an API standardizes access control, abstracts away schema changes and deduplicates efforts since agent developers will probably need to create an abstract tool layer for the API anyways.",
          "score": 13,
          "created_utc": "2026-02-22 15:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sdtvi",
              "author": "damhack",
              "text": "MCP is not supposed to just layer over REST or GraphQL APIs.  Thatâ€™s a poor use.  MCP is a remote procedure call interface and should be handled accordingly.\n\nThe OP is right.  Recent LLM systems are more than capable of making a call to an API in an orderly manner if the API schema and (not totally necessary) the API docs are available.  This is due to both better reasoning performance, better tool calling and code execution abilities.  In fact a reasonable, context-saving approach is to ask a recent LLM to provide a parameterized prompt snippet to make a particular call and then provide those snippets as a library of available calls to your agents in future. Less context use, faster predictable calls and less security vulnerability hunting.",
              "score": 2,
              "created_utc": "2026-02-22 15:48:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6s7ads",
              "author": "cmndr_spanky",
              "text": "This is how I think about MCP anyways, that said (just to play devils advocate), if an API library is well documented a multi-turn agent (with a smart LLM) could easily read API docs and make API queries for you with a generic http request tool and web scraper tool. If all youâ€™re doing is writing simple MCP abstractions over APIs.. itâ€™s kinda the same shit (just handing over more autonomy to the agent and possibility of it making a bad choice in how it uses an API). \n\nThat said, if youâ€™re writing custom logic that your agent is meant to execute, MCP (or skills) is the best wayâ€¦ thereâ€™s no api to wrap",
              "score": 1,
              "created_utc": "2026-02-22 15:17:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s8zvw",
                  "author": "strangeanswers",
                  "text": "but then youâ€™re wasting tokens and causing context rot by needing to load all those API docs into your context window to understand what the api does instead of just reading a tool description. how will the agent even know which api is useful for the task at hand? should it read the docs for all the APIs available to it each time?\n\nalso, if you spin up a new agent and want it to use this API, you probably need to whitelist it for that endpoint, which adds a bunch of complexity. if you instead just expose one tool to it and all requests from that tool come to the api through an MCP server it massively simplifies access management.",
                  "score": 6,
                  "created_utc": "2026-02-22 15:25:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70namj",
                  "author": "Dihedralman",
                  "text": "Why are you making a deterministic process non-deterministic?Â ",
                  "score": 1,
                  "created_utc": "2026-02-23 20:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rxgd0",
          "author": "OkLettuce338",
          "text": "How would you remotely install a skill that requires authentication?",
          "score": 10,
          "created_utc": "2026-02-22 14:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sgh66",
              "author": "uriwa",
              "text": "LLMs need env variables that get hot swapped, that's enough I think.",
              "score": -2,
              "created_utc": "2026-02-22 16:00:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6srzec",
                  "author": "OkLettuce338",
                  "text": "No it wouldnâ€™t be. How would you install the skill remotely for a diverse set of users?",
                  "score": 6,
                  "created_utc": "2026-02-22 16:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tq220",
          "author": "igorim",
          "text": "it's a massive risk when everytime some model needs to read something or do something it decides to read arbitrary code. MCP is not about a model not being able to do X it's about 1. saving it tokens to do X, and 2. adding deterministic guardrails, and 3. Having a shared interface so you don't need to reimplement for every model\n\n",
          "score": 4,
          "created_utc": "2026-02-22 19:28:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s21pm",
          "author": "XiiMoss",
          "text": "Yeah sound Iâ€™ll just give the agent direct access to my API keys shall I",
          "score": 3,
          "created_utc": "2026-02-22 14:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sfr96",
              "author": "uriwa",
              "text": "the harness should hot swap the keys. llm doesn't need to know them. (like in deno sandboxes)",
              "score": 0,
              "created_utc": "2026-02-22 15:57:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s2ct0",
          "author": "vogut",
          "text": "It's just a tool list and a prompt fetcher. The hype around it was dumb, I agree. But it's necessary",
          "score": 3,
          "created_utc": "2026-02-22 14:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s7kbg",
          "author": "cmndr_spanky",
          "text": "MCP as a simple wrapper over APIs might be silly, but if you use custom written logic (nothing to do with APIs) that you want your LLM agent to execute like a function, MCP / skills is def the way to go..",
          "score": 6,
          "created_utc": "2026-02-22 15:19:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sgurp",
              "author": "uriwa",
              "text": "humans just call that \"documentation\", and they put it in places like google docs, markdown files in github etc'. There is no protocol for it.",
              "score": -1,
              "created_utc": "2026-02-22 16:01:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6sn8d1",
                  "author": "cmndr_spanky",
                  "text": "No thatâ€™s incorrect. You often donâ€™t want variations / non-determinism in how an LLM should execute known logic which needs to be stable. So authoring a python function exposed via MCP is the best way, the â€œdocumentâ€ only describes how and when to execute the function.",
                  "score": 4,
                  "created_utc": "2026-02-22 16:29:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o772wvy",
                  "author": "cats_r_ghey",
                  "text": "Your takes are pretty rough. Even humans working in a team have a standard they follow with their documentation.\n\nImagine the insane mental load if there was no structure to anything?",
                  "score": 1,
                  "created_utc": "2026-02-24 19:56:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tz1fn",
          "author": "WolfeheartGames",
          "text": "MCP is great for not restful api. Anything that wraps complex logic or maintains a state for the agent. Ghidra mcp, Godot mcp, playwright mcp, that sort of thing.",
          "score": 2,
          "created_utc": "2026-02-22 20:13:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u8vsx",
          "author": "apf6",
          "text": "MCP is great when you need something with builtin Oauth support.",
          "score": 2,
          "created_utc": "2026-02-22 21:04:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s6i43",
          "author": "dreamingwell",
          "text": "I wish I could auto block anyone that posts â€œMCP isnâ€™t necessaryâ€",
          "score": 3,
          "created_utc": "2026-02-22 15:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ta2ky",
              "author": "Hammer466",
              "text": "I could make an mcp for that!",
              "score": 4,
              "created_utc": "2026-02-22 18:14:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73vu67",
                  "author": "Andrew_Ngrok",
                  "text": "you can do it without an mcp",
                  "score": 1,
                  "created_utc": "2026-02-24 09:16:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y8eyf",
          "author": "albaldus",
          "text": "MCP = distribution, expositionÂ Â \n\nSkills = executionÂ ",
          "score": 1,
          "created_utc": "2026-02-23 13:49:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o772fr3",
          "author": "cats_r_ghey",
          "text": "I donâ€™t think you know what youâ€™re talking about.",
          "score": 1,
          "created_utc": "2026-02-24 19:54:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78gpyj",
          "author": "caio__oliveira",
          "text": "Not every capability needs to be a business. Think coding agents: why would a code editing, or file reading need to be an MCP? Why wrap stuff with JSON-RPC unless necessary?\n\nAnother reason is how MCPs require a round trip to the LLM provider, so it costs more tokens. It's also harder to compose MCPs into higher level functionality.\n\nThere's a bunch of material around how MCP was a bad idea (look up stuff like \"MCP was a bad abstraction\", and cloudflare's code execution post), and I agree that it being the default way of giving capabilities to agents probably caused more harm than good.",
          "score": 1,
          "created_utc": "2026-02-25 00:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8rlp",
          "author": "qlwkerjqewlkr",
          "text": "MCP is cringe and pointless",
          "score": 1,
          "created_utc": "2026-02-22 15:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wap5e",
          "author": "Clear-Dimension-6890",
          "text": "I agree. Iâ€™m not a big fan of skills and hooks. Just another point of failure . Put it all in a config file or write utilities",
          "score": 0,
          "created_utc": "2026-02-23 04:09:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbc0d2",
      "title": "Antigravity (Gemini 3.1 Pro) just solved a Next.js Tailwind build bug Iâ€™ve been struggling with for a year.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rbc0d2/antigravity_gemini_31_pro_just_solved_a_nextjs/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-22 04:13:06",
      "score": 23,
      "num_comments": 10,
      "upvote_ratio": 0.76,
      "text": "For almost a year, my Next.js portfolio build would fail every single time I ran `npm run build`. The error message was completely useless:\n\nRepo: [https://github.com/AnkitNayak-eth/ankitFolio](https://github.com/AnkitNayak-eth/ankitFolio)  \nLive site: [https://ankit-nayak.vercel.app/](https://ankit-nayak.vercel.app/)\n\n    HookWebpackError: Cannot read properties of undefined (reading 'length')\n    in cssnano-simple\n\nIt always crashed during CSS minification. I went down every rabbit hole imaginable Webpack configs, different Next.js versions, cssnano issues, dependency updates. Nothing worked.\n\nMy only workaround was disabling minification in `next.config.ts`:\n\n    config.optimization.minimize = false\n\nThe build would pass, but my production app was completely unoptimized. I eventually accepted it as one of those strange â€œNext.js things.â€\n\nToday, I decided to try Antigravity, powered by Gemini 3.1 Pro. I let it analyze the repository. It ran for about half an hour digging through the codebase and then it surfaced the actual root cause.\n\nIt wasnâ€™t Webpack.  \nIt wasnâ€™t cssnano.  \nIt wasnâ€™t Next.js.\n\nIt was a Tailwind arbitrary value with a template literal:\n\n    <div className={`flex [mask-image:linear-gradient(to_${direction},transparent,black_10%,black_90%,transparent)]`}>\n\nTailwind couldnâ€™t statically analyze `to_${direction}` at build time, so it generated invalid CSS. When Next.js passed that to cssnano for minification, the process crashed. The stack trace pointed in the wrong direction for months.\n\nThe fix was simply making the class static with a ternary:\n\n    <div className={`flex ${\n      direction === 'left'\n        ? '[mask-image:linear-gradient(to_left,...)]'\n        : '[mask-image:linear-gradient(to_right,...)]'\n    }`}>\n\nAfter that, production builds worked immediately. Minification enabled. No crashes.\n\nI spent a year blaming Webpack and Next.js for what was ultimately a dynamic Tailwind string interpolation mistake. Antigravity, powered by Gemini 3.1 Pro, found it in under an hour.\n\nUff What a crazzy time to be alive. ðŸ¤·â€â™‚ï¸",
      "is_original_content": false,
      "link_flair_text": "Great Discussion ðŸ’­ ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rbc0d2/antigravity_gemini_31_pro_just_solved_a_nextjs/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6qdko0",
          "author": "coloradical5280",
          "text": "There is no way opus 4.6 or codex-5.x-xhigh , would have failed to find this, particularly with chrome dev tools MCP",
          "score": 7,
          "created_utc": "2026-02-22 06:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wk2m1",
              "author": "Cod3Conjurer",
              "text": "I did experiment with Claude a few months ago (don't remember which model), but it didn't crack this one back then.",
              "score": 2,
              "created_utc": "2026-02-23 05:17:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6v4o2x",
          "author": "eltron",
          "text": "Iâ€™ve been burned with Tailwind arbitrary interpolation errors before and Iâ€™ve found those error to be red herring errors. They usually distract me for a good chunk of time.",
          "score": 2,
          "created_utc": "2026-02-22 23:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wjvjb",
              "author": "Cod3Conjurer",
              "text": "The problem wasn't understanding that Tailwind discourages dynamic classes. The problem was a production only cssnano crash with zero indication it was related to Tailwind or which component caused it.",
              "score": 1,
              "created_utc": "2026-02-23 05:16:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ws7wp",
                  "author": "coloradical5280",
                  "text": "No offense but the real failure was not implementing robust error handling, verbose debug logging, and not connecting basic web dev tools that are literally created to handle exactly these issues",
                  "score": 0,
                  "created_utc": "2026-02-23 06:25:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xg3k9",
          "author": "hugganao",
          "text": "first prove to me you don't work for google",
          "score": 2,
          "created_utc": "2026-02-23 10:12:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xybzi",
              "author": "Cod3Conjurer",
              "text": "He he ðŸ˜‚Â ",
              "score": 1,
              "created_utc": "2026-02-23 12:46:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xhaoh",
          "author": "CorneZen",
          "text": "Dude, from your post history itâ€™s clear that you are now pushing AI generated content. Itâ€™s too obvious. Your site styling and layout looks sweet though.",
          "score": 0,
          "created_utc": "2026-02-23 10:24:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xya94",
              "author": "Cod3Conjurer",
              "text": "I'm not \"pushing Al content,\" I just use Al as a tool\n\n\nAnd appreciate the compliment on the site thanks manÂ ",
              "score": 1,
              "created_utc": "2026-02-23 12:46:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rafi3g",
      "title": "I built an LLM gateway in Rust because I was tired of API failures",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rafi3g/i_built_an_llm_gateway_in_rust_because_i_was/",
      "author": "SchemeVivid4175",
      "created_utc": "2026-02-21 02:42:49",
      "score": 16,
      "num_comments": 16,
      "upvote_ratio": 0.69,
      "text": "I kept hitting the same problems with LLMs in production:\n\n\\- OpenAI goes down â†’ my app breaks\n\n\\- I'm using expensive models for simple tasks  \n\n\\- No visibility into what I'm spending\n\n\\- PII leaking to external APIs\n\nSo I built Sentinel - an open-source gateway that handles all of this.\n\n\n\nWhat it does:\n\n\\- Automatic failover (OpenAI down? Switch to Anthropic)\n\n\\- Cost tracking (see exactly what you're spending)\n\n\\- PII redaction (strip sensitive data before it leaves your network)\n\n\\- Smart caching (save money on repeated queries)\n\n\\- OpenAI-compatible API (just change your base URL)\n\n\n\nTech:\n\n\\- Built in Rust for performance\n\n\\- Sub-millisecond overhead\n\n\\- 9 LLM providers supported\n\n\\- SQLite for logging, DashMap for caching\n\n\n\nGitHub: [https://github.com/fbk2111/Sentinel](https://github.com/fbk2111/Sentinel)\n\n\n\nI'm looking for:\n\n\\- Feedback on the architecture\n\n\\- Bug reports (if you try it)\n\n\\- Ideas for what's missing\n\n\n\nBuilt this for myself, but figured others might have the same pain points.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rafi3g/i_built_an_llm_gateway_in_rust_because_i_was/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6mcwb7",
          "author": "Karyo_Ten",
          "text": "Seems very wrong.\n\nFirst of all, there is no tests.\n\nSecond, how do you accurately count the number of tokens?\n\nThird. The `assess_complexity` function is completely wrong, it hardcodes keyword in English, lower-case, doesn't account for typo or multilinguage or mixed-case.\n\nFourth. \"What's a croissant?\" and \"What's a pain au chocolat?\" are likely to have a high cosine similarity score and your semantic cache is likely to be very buggy. It also prevents regeneration of answers.",
          "score": 4,
          "created_utc": "2026-02-21 16:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6moy6b",
              "author": "LatentSpaceLeaper",
              "text": ">First of all, there is no tests.\n\nðŸ¤£ Thank you and goodbye!\n\n(thank you for taking your time and giving OP constructive feedback. Hope OP is amenable to it.)",
              "score": 3,
              "created_utc": "2026-02-21 17:17:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l1xhn",
          "author": "Ihavenocluelad",
          "text": "How does an llm gateway help you using expensive models for simple tasks lmao? Just call another provider? What differentiates this AI Gateway from LiteLLM Openrouter etc",
          "score": 2,
          "created_utc": "2026-02-21 11:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lakyj",
          "author": "esmurf",
          "text": "Is it smarter than opencode?Â ",
          "score": 1,
          "created_utc": "2026-02-21 12:30:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m5fs6",
          "author": "airylizard",
          "text": "Yeahâ€¦ interchangeability isnâ€™t a thing. What testing have you done? Because I know from personal experience that there is no world in which you just change out the model and it works flawlessly",
          "score": 1,
          "created_utc": "2026-02-21 15:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nexn7",
              "author": "SchemeVivid4175",
              "text": "what model are you talking about , this is not a model training\n\n",
              "score": -1,
              "created_utc": "2026-02-21 19:27:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6p5r0z",
                  "author": "elbiot",
                  "text": "Lol did you even read your own post? Let's think. How could model interoperability relate to your post about routing requests to random models?",
                  "score": 3,
                  "created_utc": "2026-02-22 01:21:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kkj9l",
          "author": "hopfi2k",
          "text": "Well done. Star absolutely â­ï¸ deserved",
          "score": 0,
          "created_utc": "2026-02-21 08:23:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lm5pk",
          "author": "Antic_Hay",
          "text": "I vibe-coded some data utilities in Rust that do video analysis, OCR, voice transcription etc. where I need near real-time performance ideally, rust made sense here because I could just say to claude \"optimise this for my M3 mac and make sure all cores are used even on a single file operation\".\n\nA gateway is a great idea, but I don't see the Rust value...though no better or worse than anything else. I mean node is single-threaded and interpreted, and can be performant if done right. But neither here nor there :)",
          "score": 0,
          "created_utc": "2026-02-21 13:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jqn96",
          "author": "ai_hedge_fund",
          "text": "This is a good idea to put effort into\n\nStarred it and intend to check it out\n\nThank you",
          "score": -6,
          "created_utc": "2026-02-21 04:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kbh5j",
              "author": "dry_garlic_boy",
              "text": "Why do all these LLM AI posts and\n\nresponses have extra lines between\n\nthe text? It's really fucking\n\nstupid.",
              "score": 8,
              "created_utc": "2026-02-21 06:56:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6l8ehl",
                  "author": "ai_hedge_fund",
                  "text": "Youâ€™re absolutely right!",
                  "score": 5,
                  "created_utc": "2026-02-21 12:12:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mphle",
                  "author": "PuddleWhale",
                  "text": "Open claw bouncing walls of text around and ending up with a carriage return AND a linefeed where there should be only one?",
                  "score": 1,
                  "created_utc": "2026-02-21 17:20:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mattj",
                  "author": "Karyo_Ten",
                  "text": "They want to next line but don't know you need to end a line with an antislash \\ if you want next line without doubling it.\\\nLike so.",
                  "score": 0,
                  "created_utc": "2026-02-21 16:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rd5tbq",
      "title": "thereâ€™s a new open source tool for checking ai agent security.... is it okay to share here?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rd5tbq/theres_a_new_open_source_tool_for_checking_ai/",
      "author": "Accomplished-Wall375",
      "created_utc": "2026-02-24 04:34:17",
      "score": 14,
      "num_comments": 7,
      "upvote_ratio": 0.94,
      "text": "hey everyone,\n\ncame across a newly released free, open source tool designed to help developers and security teams evaluate the security of ai agentsâ€™ skills, tools, and integrations. it focuses on spotting issues like overly broad permissions, unsafe tool access, and weak guardrails before anything goes live in production.\n\nthereâ€™s also a podcast episode that dives deeper into ai security, emerging risks, and where the tech is heading:  \n[https://open.spotify.com/show/5c2sTWoqHEYLrXfLLegvek](https://open.spotify.com/show/5c2sTWoqHEYLrXfLLegvek)\n\ncurious... if this would be the right place to share the repo and get feedback from the community.\n\n**Edit: S**ince everyone was asking for the link...here it is \"[Caterpiller](https://caterpillar.alice.io/)\" that scan AI agent skills forÂ security threats and btw its an open source tool...please share your feedback and thankuu for being kinder.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rd5tbq/theres_a_new_open_source_tool_for_checking_ai/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o73owa8",
          "author": "Bmaxtubby1",
          "text": "Itâ€™s probably fine to share, especially since itâ€™s open source and directly relevant to AI agents and security. But context matters. If you just paste a repo link and a podcast, it can look like promotion. If you explain what the tool actually does, how it evaluates permissions, and where it might fail, it feels like a genuine discussion.\n\nAlso, if itâ€™s your project, say that upfront. Most dev communities are okay with creators sharing their own tools as long as theyâ€™re transparent and open to feedback. Hidden promotion is what usually triggers backlash.",
          "score": 3,
          "created_utc": "2026-02-24 08:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o794s65",
          "author": "m2845",
          "text": "Feel free to post it!",
          "score": 2,
          "created_utc": "2026-02-25 02:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73010q",
          "author": "Aggravating_Log9704",
          "text": "The podcast sounds nice but the repo is where the meat is. Does it support custom threat models? A big issue with AI security platforms is they assume one size fits all. Real teams have very different risk profiles. If it lets you plug in bespoke rules or simulated attacks that is legit.",
          "score": 1,
          "created_utc": "2026-02-24 04:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o734lct",
          "author": "Any_Artichoke7750",
          "text": "Iâ€™d love to know if it integrates with CI/CD. Static analysis during dev is fine, but the real win is catching risky permissions before deploy. If it hooks into GitHub actions or similar itâ€™s already ahead of most toy tools.",
          "score": 1,
          "created_utc": "2026-02-24 05:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o739cuz",
          "author": "Severe_Part_5120",
          "text": "this looks promising. i have been building some personal ai projects that interact with apiâ€™s and local scripts, and Iâ€™ve had zero way to test how safe they are. even simple things like accidentally exposing api keys or letting an agent delete something it shouldnâ€™t can be a huge problem. a tool like this seems like it could be really valuable for people testing things in a sandbox environment before going live.",
          "score": 1,
          "created_utc": "2026-02-24 05:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75czpr",
          "author": "penguinzb1",
          "text": "the permissions-first approach is the right starting point, but with agents the structural analysis only catches a subset of the real exposure. the rest shows up when you run it against actual inputs. an agent that looks well-scoped at the permission level will still take unexpected actions under specific input combinations nobody mapped out during design. the behavioral gaps only surface when you run it through the scenarios that actually show up in your traffic before shipping.",
          "score": 1,
          "created_utc": "2026-02-24 15:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75kcju",
          "author": "Sea-Sir-2985",
          "text": "the permissions-first approach is a good start but the real risk with agents is runtime behavior not just static config... an agent can have perfectly scoped permissions and still do unexpected things depending on what inputs it gets. i've seen agents with read-only access still cause problems by flooding APIs with requests\n\nthe CI/CD integration question is the right one though, catching risky permissions before deploy rather than after something goes wrong is where the actual value is",
          "score": 1,
          "created_utc": "2026-02-24 15:51:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdk8vu",
      "title": "Giving AI agents direct access to production data feels like a disaster waiting to happen",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdk8vu/giving_ai_agents_direct_access_to_production_data/",
      "author": "Then_Respect_1964",
      "created_utc": "2026-02-24 16:04:57",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "I've been building AI agents that interact with real systems (databases, internal APIs, tools, etc.)\n\nAnd I can't shake this feeling that we're repeating early cloud/security mistakesâ€¦ but faster.\n\nRight now, most setups look like:\n- give the agent database/tool access\n- wrap it in some prompts\n- maybe add logging\n- hope it behaves\n\nThat'sâ€¦ not a security model.\n\nIf a human engineer had this level of access, we'd have:\n- RBAC / scoped permissions\n- approvals for sensitive actions\n- audit trails\n- data masking (PII, financials, etc.)\n- short-lived credentials\n\nBut for agents?\n\nWe're basically doing:\n\n> \"hey GPT, please be careful with production data\"\n\nThat feels insane.\n\nSo I started digging into this more seriously and experimenting with a different approach:\n\nInstead of trusting the agent, treat it like an untrusted actor and put a control layer in between.\n\nSomething that:\n- intercepts queries/tool calls at runtime\n- enforces policies (not prompts)\n- can require approval before sensitive access\n- masks or filters data automatically\n- issues temporary, scoped access instead of full credentials\n\nBasically:\n\ndon't let the agent *touch* real data unless it's explicitly allowed.\n\nCurious how others are thinking about this.\n\nIf you're running agents against real data:\n- are you just trusting prompts?\n- do you have any real enforcement layer?\n- or is everyone quietly accepting the risk right now?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdk8vu/giving_ai_agents_direct_access_to_production_data/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o75rriw",
          "author": "cmh_ender",
          "text": "agreed, boundries are crazy important. look at this video (tech with tim) he deployed clawbot but put a lot of safe guards in place.\n\n[https://www.youtube.com/watch?v=NO-bOryZoTE](https://www.youtube.com/watch?v=NO-bOryZoTE)\n\n  \nWe use ai agents with our code base right now but they can't (no permission) to approve prs, so they can create new branches and tag humans for review but can't actually deploy anything. that's been very helpful in keeping down mistakes. \n\n",
          "score": 4,
          "created_utc": "2026-02-24 16:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75qs3i",
          "author": "Fulgren09",
          "text": "I was an MCP doomer for months until I had the bright idea to build a conversational UI for my app.Â \n\nAfter days of agonizingly building protocols that explain the api orchestration to accomplish task in my app, it works with Claude Sonnet.Â \n\nWhat I learned is whoever is exposing their system to an external AI will have strong opinions on which paths it can walk in and which rooms it can enter.Â \n\nNot saying itâ€™s 100% fool proof but the experience of building this and the power of conversational UI gave me a lot of confidence that ppl arenâ€™t just opening up their app free for all style.Â \n",
          "score": 2,
          "created_utc": "2026-02-24 16:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75rh3s",
          "author": "DryRelationship1330",
          "text": "agree. give it to an employee who leaves the USB key of it at Panera, can't write an expression in excel that doesn't violate order-of-operations and sends a PDF of it to his co-workers to pick up the work...",
          "score": 1,
          "created_utc": "2026-02-24 16:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76752e",
          "author": "GullibleNarwhal",
          "text": "I am a tech-savvy non-coder who has been vibe-coding lately (geez that's a lot of hyphens) and I am terrified of integrating agents, or providing an agent accidentally with permissions that would potentially allow it to wipe a drive accidentally or worse. I am curious everyone's thoughts on if me prompting to provide read-only access to an agentic model running locally if it is truly constraining an agentic model.\n\nFrom what I have heard, if you are not truly sandboxing and running an agent via a VPS with its own accounts you specifically created for it, you are asking for trouble. Thoughts? Am I being gaslit by AI telling me I am properly safe-guarding agent implementation?\n\n![gif](giphy|iRcpZYWqYcJDuPMICy)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 17:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76toas",
          "author": "Maleficent_Pair4920",
          "text": "You need an audit log for each time an Agent has touched prod data same as humans ",
          "score": 1,
          "created_utc": "2026-02-24 19:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79r50w",
          "author": "kdhkillah",
          "text": "Deterministic layers of security are absolutely essential, but yes it seems like many are just trusting prompts (+ tools and any libraries the agents decide to pull), too caught up in hype to acknowledge the risks.  2026 is going to be full of bonkers breaches & skill/tool/MCP injections. This [npm package hallucination](https://www.aikido.dev/blog/agent-skills-spreading-hallucinated-npx-commands) article was eye opening for me last month.",
          "score": 1,
          "created_utc": "2026-02-25 04:26:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bk2tu",
          "author": "Efficient_Loss_9928",
          "text": "The enforcement layer is the human engineer's permission.\n\nI don't know why would you ever grant LLMs system admin access, you always provide it the same permission as the user.\n\nAnd honestly I have never seen a setup like that, it is always delegated access so agents inherit the person's access rights. Can you provide some concrete examples? Like that is so weird, I have worked with companies from startups to military, I have never seen people just grant LLMs non-delegated permissions.",
          "score": 1,
          "created_utc": "2026-02-25 13:15:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c2s0t",
          "author": "DecodeBytes",
          "text": "You may want to checkout [https://github.com/always-further/nono](https://github.com/always-further/nono) \\- disclaimer, one of the maintainers",
          "score": 1,
          "created_utc": "2026-02-25 14:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ddb79",
          "author": "TroubledSquirrel",
          "text": "You're not wrong and the comparison to early cloud security is apt, we're in the \"just open port 22 to everything and we'll fix it later\" phase of agent development.\n\nI've been building a memory and context system that agents interact with and security couldn't be an afterthought because the system handles sensitive personal and professional context across sessions. So I had to actually solve some of this rather than defer it.\n\nA few things I learned building it out:\n\nPrompt-based trust is not a security model, it's a liability. The enforcement has to happen at the infrastructure layer, not the instruction layer. An agent that's told to be careful with PII will be careful right up until it isn't, and you won't know which time that is in advance.\n\nPolicy engines need to be separate from the agent entirely. I ended up building a policy layer that intercepts before any data touches the agent, not just logging what happened but actively making decisions about what the agent is allowed to see in the first place. The agent operates on already-filtered context, not raw data.\n\nPII scrubbing at ingestion rather than at output is the move. By the time you're trying to mask data on the way out you've already lost. Strip it or tokenize it before it enters the system.\n\nAudit trails need to be tamper-evident not just present. Logs you can modify are just a false sense of security. I implemented hash chained audit records so any tampering is detectable.\n\nThe hardest part honestly isn't the technical implementation. It's that most people building agents right now are moving fast enough that security feels like a tax on velocity. Until something goes wrong it's invisible work.\n\nThe risk isn't being quietly accepted so much as quietly not thought about yet.",
          "score": 1,
          "created_utc": "2026-02-25 18:29:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7fx7u4",
          "author": "Aggressive_Poet3228",
          "text": "Youâ€™re describing exactly the right architecture. Treating the agent as an untrusted actor with a control layer between it and system action is the key insight.\nIâ€™ve been building this exact approach as an open source project. The core principle is the same as yours: the model proposes, the governance layer decides.\nWhatâ€™s implemented:\n\tâˆ™\tEvery action passes through a policy engine before execution. The model canâ€™t bypass it\n\tâˆ™\tActions are classified into risk tiers (T0 through T3) with proportional governance. Low-risk actions get O(1) cache lookups. Irreversible actions (delete, deploy, send) require explicit owner approval\n\tâˆ™\tPII redaction runs through a local model before any content reaches external APIs. It operates on content patterns, not on the modelâ€™s interpretation of intent\n\tâˆ™\tEvery action produces a durable receipt. If thereâ€™s no receipt, it didnâ€™t happen\n\tâˆ™\tThe governance document is immutable at runtime. The running system cannot weaken its own constraints\nThe Agents of Chaos paper published this week red-teamed agents without any of these controls and documented 11 vulnerability classes. I did a full case-by-case mapping of each failure to the architectural defense that would have caught it: https://projectlancelot.dev/answer-to-chaos\nRepo: https://projectlancelot.dev\nTo answer your questions directly: no, prompts are not enough. Yes, you need real enforcement. And yes, most people are quietly accepting the risk right now.",
          "score": 1,
          "created_utc": "2026-02-26 02:11:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnc0f",
      "title": "Running RAG on 512MB RAM: OOM Kills, Deadlocks, Telemetry Bugs and the Fixes",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/f9r207sydhlg1",
      "author": "Lazy-Kangaroo-573",
      "created_utc": "2026-02-24 17:54:37",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Discussion ðŸ’­ ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdnc0f/running_rag_on_512mb_ram_oom_kills_deadlocks/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7el8gj",
          "author": "Everlier",
          "text": "I'm advising against using LangChain where I can, yours is another example where they crested meaningless abstraction that only adds to the complexity overhead while covering a very simple operation",
          "score": 2,
          "created_utc": "2026-02-25 21:52:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gej3n",
              "author": "Lazy-Kangaroo-573",
              "text": "Glad to see someone who gets the real production pain!",
              "score": 2,
              "created_utc": "2026-02-26 03:52:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1raweed",
      "title": "Anyone else noticing that claude code allocates a fixed number of subagents regardless of dataset size?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1raweed/anyone_else_noticing_that_claude_code_allocates_a/",
      "author": "ddp26",
      "created_utc": "2026-02-21 17:05:49",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I gave claude code a large fuzzy matching task ([https://everyrow.io/docs/case-studies/match-clinical-trials-to-papers](https://everyrow.io/docs/case-studies/match-clinical-trials-to-papers)) and claude independently designed a TF-IDF pre-filtering step, spun up 8 parallel subagents, and used regex for direct ID matching. But it used exactly 8 subagents whether the dataset was 200 or 700 rows on the right side, leading to the natural consequence of how coding agents plan: they estimate a reasonable level of parallelism and stick with it. Even as the dataset grows, each agent's workload increases but the total compute stays constant. \n\nI tried prompting it to use more subagents and it still capped at 8. Ended up solving it with an MCP tool that scales agent count dynamically, but curious if anyone's found a prompting approach that works. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1raweed/anyone_else_noticing_that_claude_code_allocates_a/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6owbkk",
          "author": "kubrador",
          "text": "claude's parallelism is like a guy who always orders 8 tacos regardless of how hungry he is, just with worse scaling implications.",
          "score": 1,
          "created_utc": "2026-02-22 00:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q1wy4",
          "author": "Ok_Prize_2264",
          "text": "I ran into a similar bottleneck with parallelism caps when we were scaling our RAG agents last month. It honestly feels like these coding agents hit a ceiling because they canâ€™t verify if adding more compute actually improves the output quality or just burns tokens. We started using a proper eval pipeline to monitor how the subagents were actually performing across different dataset sizes and it helped us catch where the logic was stalling. It really made the whole debugging process a lot smoother once we integrated Confident AI.",
          "score": 1,
          "created_utc": "2026-02-22 05:00:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rejx0p",
      "title": "safe-py-runner: Secure & lightweight Python execution for LLM Agents",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rejx0p/safepyrunner_secure_lightweight_python_execution/",
      "author": "adarsh_maurya",
      "created_utc": "2026-02-25 17:36:22",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "AI is getting smarter every day. Instead of building a specific \"tool\" for every tiny task, it's becoming more efficient to just let the AI write a Python script. But how do you run that code without risking your host machine or dealing with the friction of Docker during development?\n\nI built `safe-py-runner` to be the lightweight \"security seatbelt\" for developers building AI agents and Proof of Concepts (PoCs).\n\n# What My Project Does\n\nIt allows you to execute AI-generated Python snippets in a restricted subprocess with \"guardrails\" that you control via simple TOML policies.\n\n* **Reduce Tool-Calls:** Instead of making 10 different tools for math, string parsing, or data formatting, give your agent a `python_interpreter` tool powered by this runner.\n* **Resource Guardrails:** Prevents the AI from accidentally freezing your server with an infinite loop or crashing it with a memory-heavy operation.\n* **Access Control:** Explicitly whitelist or blacklist modules (e.g., allow `datetime`, block `os`).\n* **Local-First:** No need to manage heavy Docker images just to run a math script during your prototyping phase.\n\n# Target Audience\n\n* **PoC Developers:** If you are building an agent and want to move fast without the \"extra layer\" of Docker overhead yet.\n* **Production Teams:** Use this **inside** a Docker container for \"Defense in Depth\"â€”adding a second layer of code-level security inside your isolated environment.\n* **Tool Builders:** Anyone trying to reduce the number of hardcoded functions they have to maintain for their LLM.\n\n# Comparison\n\n|**Feature**|**eval() / exec()**|**safe-py-runner**|**Pyodide (WASM)**|**Docker**|\n|:-|:-|:-|:-|:-|\n|**Speed to Setup**|Instant|**Seconds**|Moderate|Minutes|\n|**Overhead**|None|**Very Low**|Moderate|High|\n|**Security**|None|**Policy-Based**|Very High|Isolated VM/Container|\n|**Best For**|Testing only|**Fast AI Prototyping**|Browser Apps|Production-scale|\n\n# Getting Started\n\n**Installation:**\n\nBash\n\n    pip install safe-py-runner\n\n**GitHub Repository:**\n\n[https://github.com/adarsh9780/safe-py-runner](https://github.com/adarsh9780/safe-py-runner)\n\nThis is meant to be a pragmatic tool for the \"Agentic\" era. If youâ€™re tired of writing boilerplate tools and want to let your LLM actually use the Python skills it was trained onâ€”safelyâ€”give this a shot.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rejx0p/safepyrunner_secure_lightweight_python_execution/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o7d2qsh",
          "author": "Crafty_Disk_7026",
          "text": "Nice I made something similar if you want some inspiration https://github.com/imran31415/codemode_python_benchmark/blob/main/sandbox/executor.py",
          "score": 1,
          "created_utc": "2026-02-25 17:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d6s2c",
              "author": "adarsh_maurya",
              "text": "very interesting, it uses restricted python and thus you can control dunder level security threats using this. I am thinking to add this in future. thank you.",
              "score": 1,
              "created_utc": "2026-02-25 18:00:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7d7hpo",
                  "author": "Crafty_Disk_7026",
                  "text": "Yes!  Curious to see what approach you end up choosing!",
                  "score": 1,
                  "created_utc": "2026-02-25 18:03:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7d7hgq",
          "author": "penguinzb1",
          "text": "policy-based is solid for anticipated patterns. the gaps show up when agent-generated code hits the ones nobody pre-specified.",
          "score": 1,
          "created_utc": "2026-02-25 18:03:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7d8zmi",
              "author": "adarsh_maurya",
              "text": "Yes. But there are some things which everyone should use like memory limit, time limit and restrictions to network. This reduces the boiler plate for developer and they can just focus on developing agent rather than setting up infra layer. But i agree with your point.",
              "score": 1,
              "created_utc": "2026-02-25 18:10:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gpkfl",
          "author": "xenos__25",
          "text": "What possible use case or need do you think this have? I can guide my ai agent to create tools and use them, what does creating a separate env bring into picture?",
          "score": 1,
          "created_utc": "2026-02-26 05:06:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gqym3",
              "author": "adarsh_maurya",
              "text": "I am not sure I fully understand this comment, but what I interpreted is this: the same agent which will create tool, it will execute it as well. If that's what you are saying, even then you'd need an environment to execute it right? this provides that environment with controllable limits. Imagine you have an agent on a Fast Api server, and it created a code and executed it on the same runtime which is hosting Fast Api, this would block your app to its end user because the agent is busy executing a code. If it doesn't, the agent might use libraries which it was not allowed to.\n\nThis comment is purely based on my interpretation of your question, if it is wrong please let me know.",
              "score": 2,
              "created_utc": "2026-02-26 05:16:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7jmsph",
                  "author": "xenos__25",
                  "text": "Thanks! That answers my question",
                  "score": 1,
                  "created_utc": "2026-02-26 17:06:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdk3g4",
      "title": "I built a lightweight long-term memory engine for LLMs because I was tired of goldfish memory",
      "subreddit": "LLMDevs",
      "url": "https://github.com/RaffaelFerro/synapse",
      "author": "porrabelo",
      "created_utc": "2026-02-24 15:59:31",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdk3g4/i_built_a_lightweight_longterm_memory_engine_for/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o766vcv",
          "author": "porrabelo",
          "text": "Iâ€™m eager to know the results! Thank you!!",
          "score": 2,
          "created_utc": "2026-02-24 17:32:33",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o75u0fh",
          "author": "GullibleNarwhal",
          "text": "I am super intrigued as I am currently trying to Frankenstein together multiple models. I currently have an embedded router model for user input intent determination, a brain or language model for response generation that can be swapped, and vision models for image processing that can also be swapped. I have tried to build out a contextual memory for the brain by having it save \"memories\" of conversations, and then summarize once it reaches a certain threshold. I have yet to build enough of a record to test the memory system though. I am curious how this might integrate into it. Are you offering this open source?",
          "score": 1,
          "created_utc": "2026-02-24 16:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75v5i5",
              "author": "porrabelo",
              "text": "That Sounds like a fun challenge!\nYes, it is [open source](https://github.com/RaffaelFerro/synapse) (MIT license) \nPlease try it and give me your feedback!",
              "score": 1,
              "created_utc": "2026-02-24 16:39:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75vl0x",
                  "author": "GullibleNarwhal",
                  "text": "I will give it a shot and see if I can integrate it into my app. Curious if you have had any success with testing in just by prompting local llms, or are you connecting via an API?",
                  "score": 1,
                  "created_utc": "2026-02-24 16:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bqqa5",
              "author": "Dense_Gate_5193",
              "text": "you should check out NornicDB i have the entire rag pipeline including embedding the original query, RRF + rerank down to 7ms including http transport on a 1m embedding corpus.\n\nhttps://github.com/orneryd/NornicDB",
              "score": 1,
              "created_utc": "2026-02-25 13:52:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7dt46i",
                  "author": "GullibleNarwhal",
                  "text": "That looks pretty interesting too. I am working on implementing a router model that determines the user's prompt intent, and routes the flow. I noticed on your GitHub that it shows conversational memory half-life is around 7 days. Is there any way to modify or configure if you would prefer for it to have more conversational memory retrieval capabilities?",
                  "score": 1,
                  "created_utc": "2026-02-25 19:41:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7bqtcc",
          "author": "Dense_Gate_5193",
          "text": "you should check out NornicDB https://github.com/orneryd/NornicDB/graphs/traffic",
          "score": 1,
          "created_utc": "2026-02-25 13:53:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcauv2",
      "title": "OpenAI vs Cohere vs Voyage embeddings for production RAG, what are you using?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rcauv2/openai_vs_cohere_vs_voyage_embeddings_for/",
      "author": "Equivalent-Bell9414",
      "created_utc": "2026-02-23 07:18:14",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Building a production RAG system for a healthtech startup. We need to embed around 5M clinical documents and the retrieval quality directly impacts patient safety, so accuracy matters more than cost here.\n\nCurrently evaluating OpenAI text-embedding-3-large, Cohere embed-v4, and Voyage AI voyage-3.\n\nAnyone running these at scale in production? How's the latency and retrieval quality holding up? Any other options I should be looking at that I'm missing?\n\nMainly want to hear from people who have actually shipped something with these, not just ran a quick MTEB comparison.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rcauv2/openai_vs_cohere_vs_voyage_embeddings_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rb1j9w",
      "title": "Bmalph now bundles Ralph's autonomous loop and stable BMAD to Codex, Cursor, Windsurf, Copilot and Aider",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/bauxbopzowkg1.png",
      "author": "Woclaw",
      "created_utc": "2026-02-21 20:25:37",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rb1j9w/bmalph_now_bundles_ralphs_autonomous_loop_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o6omy7n",
          "author": "CommercialComputer15",
          "text": "Looks good, Iâ€™ll give it a try. So youâ€™re saying this is a big improvement over regular Claude Code etc CLI?",
          "score": 2,
          "created_utc": "2026-02-21 23:25:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xdx0f",
              "author": "Woclaw",
              "text": "Where it shines is projects with multiple moving parts. The problem with vanilla Claude Code on bigger projects isn't that it's bad at coding, it's that it has no memory of the bigger picture. You end up re-explaining context, it makes decisions that conflict with earlier ones, and the architecture drifts.\n\nbmalph doesn't make the AI smarter. It makes it better informed. The planning phase produces docs that act as a persistent spec, so when Ralph implements story 5, it still knows what was decided during story 1.\n\nFor quick tasks like fixing a bug or writing a script, it's overkill. But for something with 10+ stories across multiple modules, the planning phase alone saves you from a lot of rework.",
              "score": 1,
              "created_utc": "2026-02-23 09:52:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o71fega",
                  "author": "guglyguglygoo",
                  "text": "i will also try it when i find time. no experience at all, am using spec kit. do you have any comparison? as far as i understand bmad is also a sdd framework which you adapted to work with ralph instead of the \"usual\" implementation step of bmad? \n\nif so we built the same thing for githubs spec kit and i will compare the 0shot end to end performance in a controlled setting for a random medium complexity challenge to [https://github.com/T-0-co/t-0-spec-kit-ralph](https://github.com/T-0-co/t-0-spec-kit-ralph)\n\nI experiment a lot with native cc vs. addons like yours with more or less additional memory features and so on. i still think the simple loop is the actual wonder, it forces you to construct and manage the context across the multiple instances. the rest can be configured per project / spec. \n\nIs there a reason you dont use keywords like \"spec driven\", \"sdd\" or \"specifications\" more often? would certainly make this project easier to find. or am i missing the point? ",
                  "score": 1,
                  "created_utc": "2026-02-23 23:07:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdfqxw",
      "title": "206 models. 30 providers. One command to find what runs on your hardware",
      "subreddit": "LLMDevs",
      "url": "https://github.com/AlexsJones/llmfit",
      "author": "low_effort-username",
      "created_utc": "2026-02-24 13:06:56",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdfqxw/206_models_30_providers_one_command_to_find_what/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o74zoab",
          "author": "Accomplished-View952",
          "text": "Loved this utility. blazing fast. ",
          "score": 2,
          "created_utc": "2026-02-24 14:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o755866",
              "author": "low_effort-username",
              "text": "Thanks, tried to keep it simple, and Rust ðŸ¦€",
              "score": 1,
              "created_utc": "2026-02-24 14:39:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o75iugz",
          "author": "Daredevvll",
          "text": "Wow looks so nice. Congrats. I'm wondering if it can measure gguf models and can include hf custom models?",
          "score": 2,
          "created_utc": "2026-02-24 15:44:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75r3aa",
              "author": "low_effort-username",
              "text": "Yes, I pulled 200 of the most popular from HF, but we add whatever you want, as long as I can get the data on the model. ",
              "score": 1,
              "created_utc": "2026-02-24 16:21:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9dbt6",
      "title": "How do you test LLM for quality ?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r9dbt6/how_do_you_test_llm_for_quality/",
      "author": "Easy_Ask5883",
      "created_utc": "2026-02-19 22:19:05",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "I'm building something for AI teams and trying to understand the problem better.\n\n1. Do you manually test your AI features? \n\n2. How do you know when a prompt change breaks something?\n\n  \nAt AWS we have tons of associates who do manual QA (mostly irrelevant as far as I could see) but I dont think startups and SMBs are doing it. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r9dbt6/how_do_you_test_llm_for_quality/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6bn718",
          "author": "Comfortable-Sound944",
          "text": "As with any QA testing, some don't do it, some do it badly, some do it well but manual, some automated it, and many adjust it over time as it makes sense.",
          "score": 2,
          "created_utc": "2026-02-19 22:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8hjz",
          "author": "charlesthayer",
          "text": "I write Evals (well agentic evals). Meaning\n\n1. A way to score your output. (e.g. llm-as-judge or jury)\n2. A set of inputs to test.\n3. A fast and simple way to run this. (like a benchmark)\n\nThere are many ways to achieve this, but you can start very simply and grow. I use Arize Phoenix for traces/spans, and they have large-scale Eval features.\n\n\\- Arize Phoenix Evals: [https://arize.com/docs/phoenix/evaluation/tutorials/run-evals-with-built-in-evals](https://arize.com/docs/phoenix/evaluation/tutorials/run-evals-with-built-in-evals)  \n\\- Article I wrote: [https://medium.com/towards-artificial-intelligence/ai-sw-engineers-youre-not-prod-ready-until-you-have-this-cd37beb8d06f](https://medium.com/towards-artificial-intelligence/ai-sw-engineers-youre-not-prod-ready-until-you-have-this-cd37beb8d06f)\n\n\\- Commercial tool (Braintrust evals): [https://www.braintrust.dev/docs/evaluation](https://www.braintrust.dev/docs/evaluation)",
          "score": 2,
          "created_utc": "2026-02-20 00:37:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h8xnx",
              "author": "Useful-Process9033",
              "text": "LLM-as-judge is underrated for catching regressions fast. The key is having a diverse enough input set that you actually cover your edge cases. Most teams test the happy path and then get surprised when a prompt change breaks some obscure but critical scenario.",
              "score": 2,
              "created_utc": "2026-02-20 19:40:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6e2yc7",
              "author": "anuragsarkar97",
              "text": "I'll take a look at those. Also do you keep changing your evals constantly? Or use vibe coding to create evals as well.\n\nHow do you decide which model to use and when",
              "score": 1,
              "created_utc": "2026-02-20 08:44:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mv7kn",
                  "author": "charlesthayer",
                  "text": "I'm adding inputs and updating my llm-as-judge (eval tests) all the time as I hit problems. One thing I'd like to do more is dig into my Arize Phoenix traces more regularly to spot cases I missed. Right now, I'm bug-report driven, but I'd like to make this automated.",
                  "score": 1,
                  "created_utc": "2026-02-21 17:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6btgbh",
          "author": "Dimwiddle",
          "text": "It's always going to be a mix of automated and manual. There's also some cool ideas using skills with a QA agent, but that doesn't sound that ideal to me. \n\nI've been looking at ways to make AI code less 'viby' and have been experimenting with translating specs in to machine verifiable contracts, using test stubs. So far it's reduced a good amount bugs.",
          "score": 1,
          "created_utc": "2026-02-19 23:09:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c54b0",
          "author": "zZaphon",
          "text": "https://replayai-web.fly.dev",
          "score": 1,
          "created_utc": "2026-02-20 00:17:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c5gbe",
          "author": "paulahjort",
          "text": "Run the same prompt suite across multiple model checkpoints and track regression automatically in Weights&Biases.\n\nThe infra side of this is underrated too. Teams often skip systematic eval because spinning up a GPU to run a full eval suite feels heavyweight. Try a CLI tool like Terradev.\n\n[*github.com/theoddden/terradev*](http://github.com/theoddden/terradev)",
          "score": 1,
          "created_utc": "2026-02-20 00:19:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6diz7p",
          "author": "Ok_Constant_9886",
          "text": "We use deepeval (open-source): [https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval)\n\nAlso has a commercial platform confident ai: [https://www.confident-ai.com/](https://www.confident-ai.com/)",
          "score": 1,
          "created_utc": "2026-02-20 05:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dr5dc",
          "author": "Slight_Republic_4242",
          "text": "We learned this the hard way. At first, we â€œtestedâ€ by just trying prompts ourselves and saying, â€œLooks good.\n\nThen one small prompt **change** broke.: formatting, tone, edge cases and sometimes logic\n\nAnd we didnâ€™t notice until a user complained. LLMs donâ€™t fail loudly.  \nThey fail quietly.\n\nNow we:\n\na. Keep fixed test inputs\n\nb. Compare outputs before & after changes\n\nc. Check edge cases on purpose\n\nd. Track regressions like real software\n\nItâ€™s not perfect.  \nBut treating prompts like code changed everything.",
          "score": 1,
          "created_utc": "2026-02-20 06:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6driw5",
              "author": "anuragsarkar97",
              "text": "That makes sense I'm doing the same thing too. I guess time to build a product out of it.\n10-15% of my time I'm trying to fix either the system prompt of formating or something else",
              "score": 1,
              "created_utc": "2026-02-20 06:57:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e2r8n",
          "author": "AnythingNo920",
          "text": "in reality most SMBs do vibe testing, unless benchmarks are their key selling point. ",
          "score": 1,
          "created_utc": "2026-02-20 08:42:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e3108",
              "author": "anuragsarkar97",
              "text": "Interesting, so it's not so high on priority list. But eventually they need know how is the AI performing in some way right?",
              "score": 1,
              "created_utc": "2026-02-20 08:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6llof5",
                  "author": "AnythingNo920",
                  "text": "Absolutely right. They need to, but the average Joe in an SMB can't tell the difference between BLEU, ROUGE, Fluency, Accuracy, Recall or whatever other metric u wanna use. \n\nSo they do vibe testing. This feels more tangible. \nAt least thats my impression so far.",
                  "score": 1,
                  "created_utc": "2026-02-21 13:48:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ygei2",
          "author": "khureNai05",
          "text": "For me, glm 4.7 runs small test scripts + real tasks, check outputs vs expected, rerun if weird. keeps QA low-effort but still catches most breakages.",
          "score": 1,
          "created_utc": "2026-02-23 14:33:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb1z2t",
      "title": "How do you detect silent output drift in LLM pipelines?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rb1z2t/how_do_you_detect_silent_output_drift_in_llm/",
      "author": "Lorenzo_Kotalla",
      "created_utc": "2026-02-21 20:43:48",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "I am running into something that feels tricky to monitor in LLM systems: silent output drift.\n\nNot obvious failures, but gradual changes in tone, structure, or reasoning quality over time. The outputs still look â€œvalidâ€, but they slowly move away from what the system was originally tuned for.\n\nThis seems to happen even without major prompt changes, sometimes just from model updates, context shifts, or small pipeline tweaks.\n\nFor those running LLMs in production or long-lived tools:\n\n* How do you detect this kind of drift early?\n* Do you rely on periodic sampling, regression datasets, structured output checks, or something else?\n* Have you found any signals that reliably indicate quality decay before users notice it?\n\nCurious what has actually worked in practice.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rb1z2t/how_do_you_detect_silent_output_drift_in_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6num44",
          "author": "andy_p_w",
          "text": "So it does not help with random quality changes in the model (I have observed behavior in OpenAI for the gpt-5 reasoning models that to me appears to be clear degradation that may last for a day and then go back to normal -- so not sure what is happening behind the scenes).\n\nBut more model upgrades, we have a set of test cases we look to make sure no regressions when upgrading models. (And it is useful if a cheaper model comes out to test with the cheaper model as well.) ",
          "score": 3,
          "created_utc": "2026-02-21 20:49:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oyite",
          "author": "kubrador",
          "text": "set up a panel of like 5-10 \"canary prompts\" that you run weekly against the same model version and compare outputs with cosine similarity or just eyeball them. sounds lazy but it catches 90% of weird drift before users do.\n\n\n\nthe real move though is having a frozen reference dataset of outputs from month 1 that you score against periodically. if your current outputs drift >15% in whatever metric matters to you, something broke.",
          "score": 3,
          "created_utc": "2026-02-22 00:36:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r7ww8",
          "author": "vanbrosh",
          "text": "LLM should not be used for now in cases where such unnotably drift impacts. There are quite a lot of tasks where LLM can fit great as automatization but not as crucial decision making. I would not recommend you doing something that can have bad consequences with LLM.  \nStructured output checks are great for detecting recursive repetitions drifts which nowerdays happen even with Gemini, OpenAI and literally any LLM, but not every task is possible to do with schema constrains - sometimes you need a stream / agentic chat, and structured outputs will kill a stream (makes no sense because streamed JSON is broken JSON)  \n  \nIf you use structured output (and not pure text streaming) - also one simple technique we use - insert a random secret token into prompt at random position (on sentense level) on every request and ask model to detect it in addition to main task. If it is there - model still understands sense and does predictable things. I did similar in my benchmark test [https://devforth.io/insights/self-hosted-gpt-real-response-time-token-throughput-and-cost-on-l4-l40s-and-h100-for-gpt-oss-20b/](https://devforth.io/insights/self-hosted-gpt-real-response-time-token-throughput-and-cost-on-l4-l40s-and-h100-for-gpt-oss-20b/)",
          "score": 2,
          "created_utc": "2026-02-22 11:26:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nx3zi",
          "author": "Abu_BakarSiddik",
          "text": "For critical and measurable outputs, we maintain datasets and run evaluations periodically.   \n  \nFor general LLM responses, weâ€™ve been evaluating them manually.",
          "score": 1,
          "created_utc": "2026-02-21 21:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6oqnvg",
          "author": "zZaphon",
          "text": "https://replayai-web.fly.dev",
          "score": 1,
          "created_utc": "2026-02-21 23:48:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pafkb",
          "author": "Clear-Dimension-6890",
          "text": "Here is something that could help https://medium.com/@dami.gupta/the-forgetting-problem-engineering-persistent-intelligence-in-claude-code-bd2e4c59711a",
          "score": 1,
          "created_utc": "2026-02-22 01:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pj38q",
          "author": "teambyg",
          "text": "Evaluations running offline on a schedule, and then subsample live data against your evals to monitor drift.",
          "score": 1,
          "created_utc": "2026-02-22 02:48:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}