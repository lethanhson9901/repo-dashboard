{
  "metadata": {
    "last_updated": "2026-01-27 16:59:04",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 112,
    "file_size_bytes": 153110
  },
  "items": [
    {
      "id": "1qjh1qq",
      "title": "Thoughts on Agentic Design Patterns by Antonio Gulli",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qjh1qq/thoughts_on_agentic_design_patterns_by_antonio/",
      "author": "Bonnie-Chamberlin",
      "created_utc": "2026-01-22 01:37:49",
      "score": 42,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "I just finished reading *Agentic Design Patterns: A Hands-On Guide to Building Intelligent Systems*, and wanted to share some thoughts from an LLM dev perspective.\n\nThe author, Antonio Gulli (Google Cloud AI), clearly writes from an engineering background. This isn‚Äôt a trends or hype book ‚Äî it‚Äôs very focused on how to actually structure agentic systems that go beyond single-call prompting.\n\nWhat the book focuses on\n\nInstead of models or benchmarks, the book frames agent development around **design patterns**, similar to classic software engineering.\n\nIt addresses a question many of us run into:\n\nHow do you turn LLM calls into reliable, multi-step, long-running systems?\n\nThe book is organized around \\~20 agentic patterns, including:\n\n* Prompt chaining, routing, and planning\n* Tool use and context engineering\n* Memory, RAG, and adaptation\n* Multi-agent coordination and communication\n* Guardrails, evaluation, and failure recovery\n\nMost chapters include concrete code examples (LangChain / LangGraph / CrewAI / Google tooling), not just conceptual diagrams.\n\nWhat I found useful as a dev\n\nPersonally, the biggest value was:\n\n* A clearer **mental model for agent workflows**, not just ‚Äúagent = loop‚Äù\n* Better intuition for when to decompose into multiple agents vs a single one\n* Practical framing of context engineering and memory management\n* Realistic discussion of limitations (reasoning, evaluation, safety)\n\nIt helped me reason more systematically about why many agent demos break down when you try to scale or productize them.\n\nWho this is probably for\n\n* LLM devs building agentic workflows or internal tools\n* People moving from single-call pipelines to multi-step systems\n* Engineers thinking about production reliability, not just demos\n\nIf you‚Äôre mostly interested in model internals or training, this may not be your thing. If you‚Äôre focused on **system design around LLMs**, it‚Äôs worth a look.\n\nIf anyone here has read it, I‚Äôd be curious to hear your take.",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qjh1qq/thoughts_on_agentic_design_patterns_by_antonio/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o104s5x",
          "author": "SnooDonuts3147",
          "text": "Hi, i am the author. Thank you for your words. I don't have any relationship with this kind reviewer\n\n[https://www.linkedin.com/in/searchguy/](https://www.linkedin.com/in/searchguy/)",
          "score": 9,
          "created_utc": "2026-01-22 06:30:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o106tvm",
              "author": "Bonnie-Chamberlin",
              "text": "wow. Thanks for the book LOL. Wish I could connect with you.",
              "score": 3,
              "created_utc": "2026-01-22 06:47:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o13jmja",
              "author": "junod972",
              "text": "thanks a lot for taking the time to say these kind words. I hope this will help.",
              "score": 1,
              "created_utc": "2026-01-22 19:04:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0z2b0z",
          "author": "robogame_dev",
          "text": "This post is looking a bit suss, more like promotion than a review, before we can approve it please answer:  \n1. why are you saying for people to DM you for the book rather than posting the link?  \n2. do you have any relationship with the author or the publisher?",
          "score": 4,
          "created_utc": "2026-01-22 02:17:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ze5w3",
              "author": "Bonnie-Chamberlin",
              "text": "Hi, thanks for your message. I have edited my post and removed DM (everytime when I put a link in post, it will be banned, that's why). I don't have any relationship with the author (I wish he could know me LOL)",
              "score": 2,
              "created_utc": "2026-01-22 03:25:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o10hmag",
                  "author": "havok_",
                  "text": "Your wish came true!",
                  "score": 1,
                  "created_utc": "2026-01-22 08:23:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o104z5g",
          "author": "junod972",
          "text": "Thanks for the post Iwas waiting for this kind of book. I‚Äôm craving for deep return of real experience with agents not just ¬´¬†wow it‚Äôs so shinny¬†¬ª. Thanks",
          "score": 2,
          "created_utc": "2026-01-22 06:31:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o106giz",
              "author": "Bonnie-Chamberlin",
              "text": "You are welcome.",
              "score": 2,
              "created_utc": "2026-01-22 06:44:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11oq6l",
          "author": "gized00",
          "text": "I like the clarity that the book brings but I find it to be too verbose. I would say that it should be 30-50% shorter.",
          "score": 2,
          "created_utc": "2026-01-22 13:54:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o125xbg",
              "author": "Bonnie-Chamberlin",
              "text": "I think you can go to the project repo to make some comments. Maybe Gulli can further polish.",
              "score": 1,
              "created_utc": "2026-01-22 15:20:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1aedum",
          "author": "Effective-Entry-9810",
          "text": "# The Agentic Product Playbook: Design Patterns for Intent Driven, Self Executing Software is another good book ....",
          "score": 2,
          "created_utc": "2026-01-23 18:58:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11jpuo",
          "author": "pisrael",
          "text": "Thks for the summary. I'll check it out",
          "score": 1,
          "created_utc": "2026-01-22 13:27:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11k5wx",
              "author": "Bonnie-Chamberlin",
              "text": "Hope you like it.",
              "score": 1,
              "created_utc": "2026-01-22 13:29:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12e9hx",
          "author": "New-Glove-6184",
          "text": "Link?",
          "score": 1,
          "created_utc": "2026-01-22 15:59:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o15p0qt",
              "author": "Bonnie-Chamberlin",
              "text": "Here is the link:https://github.com/sarwarbeing-ai/Agentic\\_Design\\_Patterns/tree/main. Sometimes when I post with a link, the post will be banned.",
              "score": 1,
              "created_utc": "2026-01-23 01:32:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmf35d",
      "title": "How do LLMs ACTUALLY work?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qmf35d/how_do_llms_actually_work/",
      "author": "LordAntares",
      "created_utc": "2026-01-25 09:53:15",
      "score": 21,
      "num_comments": 45,
      "upvote_ratio": 0.76,
      "text": "I've heard the \"it just does autocomplete based on statistical analyses\" argument a million times. Everybody acts like it's self explanatory and obvious but I can't quite make the connection.\n\n  \nI understand if somebody asks \"what's Tokyo's population\", how it would get you an answer. However, sometimes it almost seems like understands questions and I know that's not the case. I'll give you a couple of examples:\n\n1.  The \"how many Rs in strawberry\" famous question. Though it used to fail that one, it seems like it attempts reasoning somehow. I don't understand how statistical data analysis would lead it to go back and forth with you trying to solve the riddle.  I'm sure nobody actually asked that question online and had conversations like that.\n2.  How does it do math? Again, the problems you ask it can get very specific with an untried combination of numbers. Clearly it does something more than predict the words, no?\n3.  I usually slam it on its coding abilities; specifically semantic understanding of what needs to be done. I can understand boiler plate code etc. but just sometimes when I ask it to debug what went wrong in my code, it actually provides a seemingly thoughtful answer, solving the problem on a \"thinking\" level. Did it just see that reply somewhere? But how could it have deduced that was the problem from the code, unless someone somewhere asked the same sentence before pasting the code?\n4.  I ask it to roleplay as a custom character for a video game or whatever. I give him a custom set of instructions and a background etc. It seems to reply in character, and when it tries to, for example, reference his home town, it's not just like \" `\"Been a while since I've been in \" + hometown + \".\"`. It kind of makes up lore about it or uses alternative ways to reference it. How does it do that?\n\n  \nI know it's not magic, but I don't understand how it works. The general \"it's just a glorified autocomplete\" doesn't satisfy my curiosity. Can somebody explain to me how it does seemingly semantic things?\n\nThanks.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qmf35d/how_do_llms_actually_work/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1le2pp",
          "author": "UnbeliebteMeinung",
          "text": "[https://www.youtube.com/watch?v=D8GOeCFFby4](https://www.youtube.com/watch?v=D8GOeCFFby4)",
          "score": 26,
          "created_utc": "2026-01-25 10:00:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1llgrc",
              "author": "InTheEndEntropyWins",
              "text": "That's an amazing link, everyone should watch it.",
              "score": 3,
              "created_utc": "2026-01-25 11:05:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1llq33",
                  "author": "UnbeliebteMeinung",
                  "text": "I know. These questions come up all the time with these bad explanations \"its just the next token bro\".\n\nNow that you know the answer just copy the link when the next guy asks the same question :3",
                  "score": 3,
                  "created_utc": "2026-01-25 11:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mam60",
              "author": "LordAntares",
              "text": "This is fascinating, but essentially it says we haven't the slightest clue how they work.\n\nI also don't understand why they can decide to improve and pattern match what they haven't been coded to do.",
              "score": 3,
              "created_utc": "2026-01-25 14:04:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mcq87",
                  "author": "flextrek_whipsnake",
                  "text": "Yes, that is correct. We don't have a complete understanding of why this amount of scaling with these methods enables the models to achieve what they're able to achieve. That's the main reason why so many people, including myself, were skeptical of it for so long.\n\nAs always, the long answer is more complicated, but the short answer is we don't really know how they work.",
                  "score": 4,
                  "created_utc": "2026-01-25 14:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1n02ry",
                  "author": "roger_ducky",
                  "text": "The way I understood it is, it emulates the pattern matching part of the human brain. Not the same way, but close enough to work.\n\nWhat‚Äôs actually happening when generating is:\n\nRead full question. Think about first word to write and writes it.\n\nRead full question and first word, think about the second word to write.\n\nKeep repeating until it looks complete.",
                  "score": 1,
                  "created_utc": "2026-01-25 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mx3kz",
              "author": "aletheus_compendium",
              "text": "thanks üôèüèª that's one i will save and share for sure. i've been giving this one out https://youtu.be/LPZh9BOjkQs \"Large Language Models explained briefly\" ü§ôüèª",
              "score": 1,
              "created_utc": "2026-01-25 15:56:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n71ei",
          "author": "infamouslycrocodile",
          "text": "I can give you a more intuitive explanation: the model is ONLY trained to complete the next token given the current set of words preceding it - contextually it leads strongly to a very likely next word appearing. \n\nIf you focus on the next word completion / autocomplete you blind yourself to the preceding context. Have it complete the next word or sentence, then delete that sentence / play with the preceding context and see what other sentence comes out instead. \n\nDoing this enough and at different conversation lengths has the model learn what to pay attention to and how to inch closer to the correct result regardless. \n\nIt reconfigures its weights to achieve this, it's not learning the answers at that point, they're just a side effect of the main goal of learning how to be more likely to say the right thing.\n\n---\n\nBecause there's a finite set of weights to configure, the model has to come up with a good way to cram all that information in so it distills the knowledge and the techniques to get to the answers which happens to be similar to how we learn but less advanced.\n\nThis is why the models can get mixed up and hallucinate - \"The capital of Japan is Paris\" - the data is close together but not wired up correctly but it will get better with more training. \n\nInference time scaling is just a higher order autocomplete: perhaps there was another thing it learnt - \"The capital of France is Paris, but wait - I said Paris was the capital of Japan so that can't be right\" - it can use other things it has been trained on to connect concepts out loud, this might correlate highly to similar lines of reasoning that the model can use as a tool for the current line of thinking.",
          "score": 4,
          "created_utc": "2026-01-25 16:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lmtgs",
          "author": "consultant82",
          "text": "I think for the math part, reasoning + agentic tool combination is foundation. LLM reasons that ‚Äûmaybe I should use a calculator for given task‚Äú and a calc is invoked for given task. \n\nThe llm journey started all ‚Äûsimple‚Äú step by step with text2vec embeddings (semantic of tokens, enabling a meaning space), neural networks (‚Äûgive me input parameters given on known output, so I can learn predicting for given problem‚Äú), contextualized token prediction (transformer architecture) and this base foundation is now fed with lot of bells and whistles around it (tooling, more effective models, rag, ..).",
          "score": 4,
          "created_utc": "2026-01-25 11:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lo32i",
          "author": "GCoderDCoder",
          "text": "I stumbled across this the other day. She does a good job concisely explaining the different types of logic and how \"logic\" works with LLMs. I have peers who keep saying they're just pattern matching tools and I'm not for the AI hype but that's not a sufficient or fair description. \n\nI prefer describing them as complex text generators with emerging logic capabilities due to the science and art of how we use the text the model was trained on. Words have meaning so \"understanding\" or heavily connecting with relationships between words allows LLMs to generate value based on those relationships\n\nhttps://youtu.be/qXtNvfxBzlk",
          "score": 3,
          "created_utc": "2026-01-25 11:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oky6h",
          "author": "TheRealStepBot",
          "text": "To the degree that someone thinks that it‚Äôs fundamentally not what humans do, the explanation is wrong. But most ml people think that‚Äôs what humans do as well so when they use that phrase they are saying something different than most people hear. \n\nThe derisive it‚Äôs ‚Äújust‚Äù fancy autocomplete misunderstands almost every word in that sentence. It‚Äôs just fancy autocomplete in the same sense that aspects of what humans do is also just fancy autocomplete, and in that these models are almost certainly better at that aspect of cognition than at least the average person, if not most people.",
          "score": 3,
          "created_utc": "2026-01-25 20:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldo3p",
          "author": "kubrador",
          "text": "it's still autocomplete, just autocomplete that's absurdly good at pattern matching across billions of examples. when you ask \"how many Rs in strawberry\" it's seen enough \"let me think through this letter by letter\" responses that it's learned the \\*pattern\\* of reasoning, not actual reasoning.",
          "score": 7,
          "created_utc": "2026-01-25 09:56:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nbd36",
          "author": "insulaTropicalis",
          "text": "When a person says you that an LLM is just a sophisticate autocomplete, ask them what they know about linear transformations and non-convex optimization. If they can't answer the questions, they are just repeating concepts they don't understand that they read on social media.",
          "score": 3,
          "created_utc": "2026-01-25 16:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m5o61",
          "author": "Ok-Lack-7216",
          "text": "The \"glorified autocomplete\" explanation is technically true but effectively useless because it ignores how the model decides what comes next.\n\nI actually just created a visual breakdown of this process that answers your specific examples:\n\n[https://youtu.be/x-XkExN6BkI](https://youtu.be/x-XkExN6BkI)\n\n1. The Strawberry Problem: This is a Tokenization issue. The AI doesn't see letters; it sees whole words (tokens) as single \"Lego bricks.\" It literally cannot \"see\" the letters inside the brick to count them.\n2. Roleplay & Coding: This works via the Attention Mechanism. The model doesn't just read left-to-right; it assigns a \"weight\" to previous instructions. When it generates a line of dialogue, it is mathematically \"attending\" to the character background you provided earlier, ensuring the prediction aligns with that context.\n\nIt‚Äôs not magic, but it is complex linear algebra. I traced a single prompt through the engine to show exactly how this works in the video.",
          "score": 2,
          "created_utc": "2026-01-25 13:36:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mb99s",
              "author": "LordAntares",
              "text": "I'm sorry, but this video is cringe.",
              "score": -3,
              "created_utc": "2026-01-25 14:08:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mibku",
                  "author": "Ok-Lack-7216",
                  "text": "Fair enough! I know the analogies (like the Grocery Store) aren't for everyone, but I wanted to try something different than the usual dry lectures. Thanks for giving it a shot anyway!",
                  "score": 3,
                  "created_utc": "2026-01-25 14:46:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1llcbt",
          "author": "InTheEndEntropyWins",
          "text": "The short answer is we don't know exactly how they work. We know the architecture, but how it actually works is based on its own learning and the networks are way too complex for us to understand what's it's learnt. But in some simple situations we have looked at the networks and understood what it's done.  \n\n\n>Sam Altman Says OpenAI Doesn‚Äôt Fully Understand How GPT Works Despite Rapid Progress\n‚ÄúWe certainly have not solved interpretability,‚Äù Altman said.\n[https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/](https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/)\n\n>During that training process, they learn their own strategies to solve problems. These strategies are encoded in the billions of computations a model performs for every word it writes. They arrive inscrutable to us, the model‚Äôs developers. **This means that we don‚Äôt understand how models do most of the things they do.**\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\nSo the Rs in strawberry is due to the fact it doesn't get each letter, the word strawberry is broken up into tokens like \"straw\" and \"berry\" and those turned into vectors. So all the LLM has is say two vectors and those vectors might not have anything about the letters in straw and berry. \n\n>How does it do math? \n\nThis is a really interesting question. Anthropic have done some studies on this exact question and for simple addition, they use a bespoke algorithm that has two parts an estimation part and an accuracy part. So it doesn't add up numbers like a human would normally do or how a human would program a computer would do. It's learnt this completely new method. \n\nIn terms of autocomplete, anthropic have demonstrated that it uses algorithms and multistep reasoning rather than just memorising data and looking things up. \n\n\n>Claude wasn't designed as a calculator‚Äîit was trained on text, not equipped with mathematical algorithms. Yet somehow, it can add numbers correctly \"in its head\". How does a system trained to predict the next word in a sequence learn to calculate, say, 36+59, without writing out each step?\n>\n>Maybe the answer is uninteresting: the model might have memorized massive addition tables and simply outputs the answer to any given sum because that answer is in its training data. Another possibility is that it follows the traditional longhand addition algorithms that we learn in school.\n>\n>Instead, we find that Claude employs multiple computational paths that work in parallel. One path computes a rough approximation of the answer and the other focuses on precisely determining the last digit of the sum. These paths interact and combine with one another to produce the final answer. Addition is a simple behavior, but understanding how it works at this level of detail, involving a mix of approximate and precise strategies, might teach us something about how Claude tackles more complex problems, too.\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\n\n>if asked \"What is the capital of the state where Dallas is located?\", a \"regurgitating\" model could just learn to output \"Austin\" without knowing the relationship between Dallas, Texas, and Austin. Perhaps, for example, it saw the exact same question and its answer during its training.\n>\nBut our research reveals something more sophisticated happening inside Claude. When we ask Claude a question requiring multi-step reasoning, we can identify intermediate conceptual steps in Claude's thinking process. In the Dallas example, we observe Claude first activating features representing \"Dallas is in Texas\" and then connecting this to a separate concept indicating that ‚Äúthe capital of Texas is Austin‚Äù. In other words, the model is¬†combining¬†independent facts to reach its answer rather than regurgitating a memorized response.\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\nThat anthropic article is really good and has other examples, worth a read.\n\nSomeone else also pasted this link, so I'd just emphasise it's an amazing video worth watching.\n\n# The most complex model we actually understand\nhttps://www.youtube.com/watch?v=D8GOeCFFby4",
          "score": 2,
          "created_utc": "2026-01-25 11:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ln556",
              "author": "LordAntares",
              "text": "Thanks for the detailed reply. So it's not \"just a fancy autocomplete\". It might be a VERY fancy autocomplete tho.\n\nI will have to watch that video I guess.",
              "score": 1,
              "created_utc": "2026-01-25 11:20:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ltpyq",
                  "author": "InTheEndEntropyWins",
                  "text": ">Thanks for the detailed reply. So it's not \"just a fancy autocomplete\". It might be a VERY fancy autocomplete tho.\n\nIf you want to think about it in those terms, then when humans write and say stuff it's just \"VERY fancy autocomplete\".",
                  "score": 6,
                  "created_utc": "2026-01-25 12:14:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1lhnz8",
          "author": "guigouz",
          "text": "https://www.theguardian.com/technology/ng-interactive/2023/nov/01/how-ai-chatbots-like-chatgpt-or-bard-work-visual-explainer",
          "score": 1,
          "created_utc": "2026-01-25 10:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ng6dh",
          "author": "keebmat",
          "text": "[https://www.youtube.com/watch?v=xiqtrtKxUzY](https://www.youtube.com/watch?v=xiqtrtKxUzY)\n\n  \nyou dont want to know...",
          "score": 1,
          "created_utc": "2026-01-25 17:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nlsvk",
          "author": "crone66",
          "text": "For your second point.\n\n\nLLMs are actually really bad at math in terms of actually calculating. At the beginning they were completly useless. Then they added all simple math operations (add, subtract, devide, multiply) for two numbers of up to 4 digits to the training data. But obviously people quickly noticed that LLM can't deal with numbers with more than 5 digits. Therefore, they added a calculator tool that AI can use. Use any local ai model without a calculator tool and they will fail really quickly.",
          "score": 1,
          "created_utc": "2026-01-25 17:42:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nq0ua",
              "author": "LordAntares",
              "text": "So what is the top linked video about? It explained how llms do adding, and it wasn't hard coded as you say.",
              "score": 1,
              "created_utc": "2026-01-25 18:00:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1obauo",
                  "author": "crone66",
                  "text": "Yes it's not hard coded but it also doesn't understand how these math operations actually work otherwise it would be able to apply the rules to any given numbers but it can't. The reason is that the higher the number are the less likely they are even included in the trainingsset. S8nce LLM are a model that outputs the token with the highest probability the math equation (including the result) needs to be in the trainingset otherwise it essentialy outputs a random number. For \"Thinking models\" which is essentially the same base model but with a two step approach where you try to first break down the initial prompt by planning out a path to solution which provides useful context to shift the probability in the right direction. This context might include solving strategies such as removing zeros. Since 12+13 is in the training data it can solve it now the context or the final stage just needs to figure out that it has to add 3 zeros to the end which is most likely part of the training set too and even probability wise very likely. Therefore thinking models can even solve stuff thats not part of there trainings set.The fun thing is the llm actually don't know if the answer is correct but it will tell you it's correct with out a doubt. Since the thinking models will still fail for slightly more complex math questions calculator tool calls exist because LLMs are still really bad at calculating since they simply don't \"understand\" logic.\n\n\n(some parts are simplified to make easier to understand)",
                  "score": 2,
                  "created_utc": "2026-01-25 19:28:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1onpbv",
          "author": "TheRealStepBot",
          "text": "I‚Äôd say the way they work is related to information compression. You can‚Äôt complete the next word if you can‚Äôt internalize a semantic idea of what is being conveyed. The way to achieve this is by creating an information bottleneck where you force compression. \n\nGiven enough model complexity there really is no obvious limit to how complex an idea can be passed through this compression system. Consequently it‚Äôs auto complete yes but in order to complete every sentence ever written you must necessarily compress semantic understanding at some level.\n\nThe main reason people think these models ‚Äúdon‚Äôt understand‚Äù is actually not related to what they do but how we train them to do it. \n\nTheir internal representations are entangled and not perfectly able to be isolated. This in turn leads to ‚Äúhallucinations‚Äù and other artifacts that sometimes clouds what it is they are doing. \n\nThat we initialize and train them the way we do is mostly not because we can‚Äôt conceive of better ways but because of the pragmatic convenience of the way we do it and the quite good performance we do get. We are still in the infancy of having enough compute to apply these techniques as we do so convenience is still a significant driver.\n\nAs the tech matures however and the compute available continues to grow different implementations will be tried that will likely significantly improve performance without necessarily invalidating the ‚Äúit‚Äôs just fancy autocomplete‚Äù talking point.\n\nIn case you are interested I‚Äôm alluding to the ideas of continuous learning, and neuro evolutionary learning rather than direct gradient descent from random initialization. And that‚Äôs just assuming transformer architecture. There is so much more in the modern ml toolbox that just hast been brought to bear at the same scale as these models",
          "score": 1,
          "created_utc": "2026-01-25 20:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ovau1",
          "author": "Substantial_Sound272",
          "text": "A lot of folks don't know that the mathematical underpinnings of modern language models were discovered way back in 1948 in the absolutely incredible Shannon paper \"AMathematical Theory of Communication\". Section 6 titled \"Choice, Uncertainty, and Entropy\" is one of the most mind-blowing things I've ever read. He basically invents Information Theory and the concept of Entropy, which is directly used as a loss function for the training of modern Large Language Models, though he didn't know anything about neural networks or transformers or such.  Claude Shannon was an absolute genius. He is the person that Anthropic's Claude is named after, so if you really want to understand the math behind LLMs and you are at all mathematically inclined, check out section 6 of that paper!",
          "score": 1,
          "created_utc": "2026-01-25 20:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p9rmu",
          "author": "justinhj",
          "text": "For 2. most models are not good at arithmetic. The ones you use on a website or app usually have built in function calling for that.",
          "score": 1,
          "created_utc": "2026-01-25 22:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pghst",
          "author": "wheres-my-swingline",
          "text": "This thread is enjoyable",
          "score": 1,
          "created_utc": "2026-01-25 22:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21lnuu",
          "author": "Available_Cream_752",
          "text": "Check out Andrej Karpathy's YT channel",
          "score": 1,
          "created_utc": "2026-01-27 16:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lkkix",
          "author": "Kiansjet",
          "text": "https://youtu.be/wjZofJX0v4M\n\n2. It has learned how math works via pattern recognition by seeing a ton of varied examples. Crucially, it ideally should NOT natively try to answer the question. It is a probabilistic prediction algorithm and not a precise calculator, so what it SHOULD do during one of its reasoning/tool call phases is to invoke a hard coded calculator tool or code executor to do the calculation for it. \n\nI'm only answering 2 because it's the only one I think I have a decent answer for.\n\nForgive me for what may come off as condescension but it really is the probabilistic behavior explanation you've heard. Similar to how you likely learned to speak your native language, it's given a ton of examples of what native coherent, valid text looks like and views it as a series of blocks of text called tokens, and learns what kinds of tokens show up around other kinds.\n\nIt does not need to have seen a exact example of a scenario you're hitting it with because during it's training phase it gains an understanding of how tokens relate to other tokens. It knows what the critical thinking chain of thought for debugging code generally looks like and for the language you're using, from there depending on the tools it's given it can try different solutions depending on what it thinks, probabilistically, whether you yourself can understand internally how it saw a similarity or not, is the solution to your problem.",
          "score": 1,
          "created_utc": "2026-01-25 10:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lnmz5",
          "author": "superawesomepandacat",
          "text": "Magnets how do they work",
          "score": 1,
          "created_utc": "2026-01-25 11:24:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj3pc1",
      "title": "[Open Sourse] I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/0ttifc9viqeg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-21 17:09:24",
      "score": 20,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qj3pc1/open_sourse_i_built_a_tool_that_forces_5_ais_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0wurj1",
          "author": "wdroz",
          "text": "There are some similarities to the project [llm-council](https://github.com/karpathy/llm-council) from [Andrej Karpathy](https://github.com/karpathy/llm-council).",
          "score": 4,
          "created_utc": "2026-01-21 19:37:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wv5p3",
              "author": "S_Anv",
              "text": "Karpathy is a great man!\n\nKEA Research is designed as a user-friendly evolution. I've added image support, PDF/md export, text-to-speech conversion, and a full-fledged admin panel for managing local model sets without editing configuration files and many other features\n\nThis means you can create your own model set through a graphical interface  \nAlso as you see there is a bit different logic. You can check readme",
              "score": 4,
              "created_utc": "2026-01-21 19:39:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0w32rv",
          "author": "coloradical5280",
          "text": "Here are a few tips, having built a lot of this stuff:  \n\n\n* Make it anonymous; the models don‚Äôt know which response belongs to them during peer review. Instead, simply tag them as Model A, B, C, etc.\n* More importantly, tone down your step 3 prompt a bit, especially on the ‚Äúfind errors‚Äù part. All the counsel, quorum, debate, and peer-review workflows can have a significant impact on the quality of the output, both positively and negatively. The crucial point is to determine the right balance between encouraging the model to find errors and avoiding over-reliance on it. If you simply provide the context of the situation, as you clearly do, the model will naturally follow your instructions. You already have ‚Äòweaknesses‚Äô in the JSON format, so there‚Äôs no need for the last four bullet points in step 3. Honestly, your approach is 90% better than many that I‚Äôve seen.\n\nPeople are out there literally telling the model to go into attack mode, and wondering why it's entertaining but so worthless.\n\nAlso, if you ever want to use your subscriptions instead of API keys only, this is a gem: [https://github.com/router-for-me/CLIProxyAPIPlus](https://github.com/router-for-me/CLIProxyAPIPlus)",
          "score": 4,
          "created_utc": "2026-01-21 17:35:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12mv80",
          "author": "Electronic_coffee6",
          "text": "Love that it‚Äôs provider-agnostic, being able to plug in my own OpenAI keys makes this super flexible. Are there plans to add support for more open-source models in the future?",
          "score": 2,
          "created_utc": "2026-01-22 16:38:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14091c",
              "author": "S_Anv",
              "text": "What specific provider and models do you need?",
              "score": 1,
              "created_utc": "2026-01-22 20:20:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12lt9v",
          "author": "No-Professional2832",
          "text": "I've installed this and the consensus approach is really interesting. The mix of models helps surface different perspectives, and it feels like a step toward more reliable AI outputs. You mentioned more interesting features are coming, what can we expect next?",
          "score": 1,
          "created_utc": "2026-01-22 16:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18ofn6",
          "author": "ProTomek",
          "text": "Nice project!  \n  \nPS: ChatGPT responses? Or GPT (over OpenAI API) responses? Because those two are two totally different animals...",
          "score": 1,
          "created_utc": "2026-01-23 14:11:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qips4o",
      "title": "A legendary xkcd comic. I used Dive + nano banana to adapt it into a modern programmer's excuse.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/dbi6ee1b5neg1.png",
      "author": "Prior-Arm-6705",
      "created_utc": "2026-01-21 05:51:45",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qips4o/a_legendary_xkcd_comic_i_used_dive_nano_banana_to/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0tbi28",
          "author": "davidSenTeGuard",
          "text": "Isn't there some premium option to un-limit you. You company is probably paying more for your time than they would for the upgrade.",
          "score": 1,
          "created_utc": "2026-01-21 06:49:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0thckz",
              "author": "stingraycharles",
              "text": "There is, you can typically switch to API pricing.",
              "score": 1,
              "created_utc": "2026-01-21 07:42:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0tpooq",
                  "author": "Hegemonikon138",
                  "text": "Or you can just buy multiple accounts, thats what I do.",
                  "score": 1,
                  "created_utc": "2026-01-21 09:00:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjjbgo",
      "title": "Fei Fei Li dropped a non-JEPA world model, and the spatial intelligence is insane",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/d7y1bfsejteg1",
      "author": "coloradical5280",
      "created_utc": "2026-01-22 03:19:08",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qjjbgo/fei_fei_li_dropped_a_nonjepa_world_model_and_the/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0zs408",
          "author": "Whole-Assignment6240",
          "text": "this is super cool",
          "score": 1,
          "created_utc": "2026-01-22 04:54:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkhuro",
      "title": "Adaptive execution control matters more than prompt or ReAct loop design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qkhuro/adaptive_execution_control_matters_more_than/",
      "author": "zennaxxarion",
      "created_utc": "2026-01-23 05:05:27",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "I kept running into the same problem with agent systems whenever long multi-step tasks were involved. Issues with reliability kept showing up during agent evaluation, and then some runs were failing in ways it felt hard to predict. Plus the latency and cost variation just became hard to justify or control, especially when the tasks looked similar on paper.\n\nSo first I focused on prompt design and ReAct loop structure. I changed how the agent was told to reason and the freedom it had during each execution step. Some changes made steps in the process look more coherent and it did lead to fewer obvious mistakes earlier on.\n\nBut when the tasks became wider the failure modes kept appearing. The agent was drifting or looping. Or sometimes it would commit to an early assumption inside the ReAct loop and just keep executing even when later actions were signalling that reassessment was necessary.\n\nSo I basically concluded that refining the loop only changed surface behavior and there were still deeper issues with reliability.¬†\n\nInstead I shifted towards how execution decisions were handled over time at the orchestration layer. So because many agent systems lock their execution logic upfront and only evaluate outcomes after the run, you can‚Äôt intervene until afterwards, where the failure got baked in and you see wasted compute.\n\nIt made sense to intervene during execution instead of after the fact because then you can allocate TTC dynamically while the trajectories unfold. I basically felt like that had a much larger impact on the reliability. It shifted the question from why an agent failed to why the system was allowing an unproductive trajectory to continue unchecked for so long.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qkhuro/adaptive_execution_control_matters_more_than/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o17gmwm",
          "author": "PuzzleheadedTooth112",
          "text": "This feels like the same argument people had about monoliths years ago. You can keep making the monolith smarter or you can break the work up so failure doesn‚Äôt cascade. Most agent setups still look like monoliths to me.",
          "score": 1,
          "created_utc": "2026-01-23 08:50:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17h8fi",
          "author": "DrPickman",
          "text": "Definitely some food for thought. At some point I basically assumed long runs are too brittle for now and I should park it until the future. Just moved to shorter jobs with checkpoints and handoff between runs after finding the models weren‚Äôt impacting much.",
          "score": 1,
          "created_utc": "2026-01-23 08:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17hqqu",
          "author": "Intelligent_Front_37",
          "text": "TBH we stopped car‚Å§ing ab‚Å§out long tasks and decided it‚Äôs the task that‚Äôs the issue. If we can‚Äôt surf‚Å§ace prog‚Å§ress within a few minutes the task has to change shape. We got impr‚Å§oved reliab‚Å§ility after changing the problem instead of the agent.",
          "score": 1,
          "created_utc": "2026-01-23 09:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bko9f",
          "author": "pbalIII",
          "text": "Hit a similar wall last year. Prompt tweaks felt productive until tasks got wide enough that the agent started looping on stale assumptions.\n\nThe shift to runtime intervention changed what we measured. Instead of asking why did this fail, we started tracking how long did we let it keep going. Turns out most costly failures were obvious 3-4 steps before they cratered... the system just had no mechanism to reassess mid-run.\n\nOne pattern that helped: graduated containment. Monitor mode first, then restrict planning if risk scores climb, then pull tool access. Lets you calibrate intervention aggressiveness per task type instead of binary halt-or-continue.",
          "score": 1,
          "created_utc": "2026-01-23 22:17:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qm6811",
      "title": "Fine-tuning LLaMA 1.3B on insurance conversations failed badly - is this a model size limitation or am I doing something wrong?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qm6811/finetuning_llama_13b_on_insurance_conversations/",
      "author": "ZaRyU_AoI",
      "created_utc": "2026-01-25 02:09:23",
      "score": 11,
      "num_comments": 19,
      "upvote_ratio": 1.0,
      "text": "TL;DR:\nFine-tuned LLaMA 1.3B (and tested base 8B) on ~500k real insurance conversation messages using PEFT. Results are unusable, while OpenAI / OpenRouter large models work perfectly.\nIs this fundamentally a model size issue, or can sub-10B models realistically be made to work for structured insurance chat suggestions?\nLocal model preferred, due to sensitive PII.\n\n\n\nSo I‚Äôm working on an insurance AI project where the goal is to build a chat suggestion model for insurance agents.\nThe idea is that the model should assist agents during conversations with underwriters/customers, and its responses must follow some predefined enterprise formats (bind / reject / ask for documents / quote, etc.).\nBut we require an in-house hosted model (instead of 3rd party APIs) due to the senaitive nature of data we will be working with (contains PII, PHI) and to pass compliance tests later.\n\nI fine-tuned a LLaMA 1.3B model (from Huggingface) on a large internal dataset:\n- 5+ years of conversational insurance data\n- 500,000+ messages\n- Multi-turn conversations between agents and underwriters\n- Multiple insurance subdomains: car, home, fire safety, commercial vehicles, etc.\n- Includes flows for binding, rejecting, asking for more info, quoting, document collection\n- Data structure roughly like:\n{ case metadata + multi-turn agent/underwriter messages + final decision }\n- Training method: PEFT (LoRA)\n- Trained for more than 1 epoch, checkpointed after every epoch\n- Even after 5 epochs, results were extremely poor\n\nThe fine-tuned model couldn‚Äôt even generate coherent, contextual, complete sentences, let alone something usable for demo or production.\n\nTo sanity check, I also tested:\n- Out-of-the-box LLaMA 8B from Huggingface (no fine-tuning) - still not useful\n- OpenRouter API (default large model, I think 309B) - works good\n- OpenAI models - performs extremely well on the same tasks\n\nSo now I‚Äôm confused and would really appreciate some guidance.\n\nMy main questions:\n1. Is this purely a parameter scale issue?\nAm I just expecting too much from sub-10B models for structured enterprise chat suggestions?\n2. Is there realistically any way to make <10B models work for this use case?\n(With better formatting, instruction tuning, curriculum, synthetic data, continued pretraining, etc.)\n3. If small models are not suitable, what‚Äôs a practical lower bound?\n34B? 70B? 100B? 500B?\n4. Or am I likely doing something fundamentally wrong in data prep, training objective, or fine-tuning strategy?\n\nRight now, the gap between my fine-tuned 1.3B/8B models and large hosted models is massive, and I‚Äôm trying to understand whether this is an expected limitation or a fixable engineering problem.\n\nAny insights from people who‚Äôve built domain-specific assistants or agent copilots would be hugely appreciated.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qm6811/finetuning_llama_13b_on_insurance_conversations/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1kpquj",
          "author": "No-Concentrate4531",
          "text": "Did you use the correct chat template? Also, there is also the case where you have to mitigate catastrophic forgetting. Finally, have you adjusted the rank of your LoRA matrices?\n\nAnother issue is that you are training on the base LLMs with no instruction following. You need to train that in as well. \n\nWhen it comes to finetuning LLMs, you need to be mindful of the experimental tricks to mitigate the limitations of LoRA.\n\nAlso, when running inferences, you need to ensure the model respects your system prompts if any.\n\n\nHere is a good article: https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms",
          "score": 7,
          "created_utc": "2026-01-25 06:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lmbph",
              "author": "ZaRyU_AoI",
              "text": "Yes, we have a custom chat template for our format.\nWe did have custom rank, alpha, dropout and target modules for our LoRA finetuning.\nThanks for the article. Might revisit our custom rank after going through this article.",
              "score": 1,
              "created_utc": "2026-01-25 11:13:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1l9ir5",
          "author": "AI_Data_Reporter",
          "text": "Sub-10B models require EMA stability and curriculum-based phasing to mitigate catastrophic forgetting in domain-specific tasks. 1.3B parameters lack the capacity for multi-turn reasoning without general-to-specific alignment. PEFT alone fails if the base model lacks instruction-following priors. Effective domain adaptation for insurance requires continued pre-training on cleaned corpora before LoRA application. Compute is not the bottleneck; it is the lack of stable weight updates.",
          "score": 8,
          "created_utc": "2026-01-25 09:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n2v21",
              "author": "kompania",
              "text": "Incredible post. A roadmap for this problem and others like it. Thank you for such substantive information.",
              "score": 1,
              "created_utc": "2026-01-25 16:21:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kozc5",
          "author": "Lyuseefur",
          "text": "Follow unsloth guide. Try Qwen",
          "score": 7,
          "created_utc": "2026-01-25 06:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l5d2g",
          "author": "ampancha",
          "text": "The 1.3B parameter count is almost certainly insufficient for multi-turn insurance reasoning with structured output constraints; even with perfect training, small models struggle with long-context dependencies and format compliance. Your compliance concern is separate from model quality: regardless of which model you land on, PII/PHI in inference pipelines needs its own control architecture for audit trails, retrieval filtering, and data isolation. Sent you a DM with more detail.",
          "score": 4,
          "created_utc": "2026-01-25 08:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l1sxr",
          "author": "Imaginary-Ad-2308",
          "text": "A step back : You should explore all other methods that don't require training first. Fine-tuning is a headache; by the time you see improvements after six months, a new open-weight model will likely be released that outperforms your custom version anyway.",
          "score": 2,
          "created_utc": "2026-01-25 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rx9kx",
          "author": "burntoutdev8291",
          "text": "Did you try on the non fine tuned model to see how it performed? Since you're doing a constrained task, i think some forgetting is fine.",
          "score": 1,
          "created_utc": "2026-01-26 06:38:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t4l2d",
          "author": "bad_gambit",
          "text": "> Is this purely a parameter scale issue? Am I just expecting too much from sub-10B models for structured enterprise chat suggestions? \n\nNot really, I've helped deploy some customer service bot, and most of the time using Qwen3-8B + RAG ( + some relevant QA examples) works okay. Older, smaller models, such as Llama-2 1.3B, is *definitely* outdated and wont handle it.\n\nFrom what i see on Huggingface you're [referring](https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B) to a pruned Llama-2? Llama-2 is *ancient* in LLM timeframe!\n\nYou need to use a more recent model. Try Qwen3 4B Instruct or Ministral 3 3B or LFM 2.5 1.2B. That LFM has a slightly constrictive license though, and only permitted comercial use of <$10M revenue corporation.\n\nSmall model can work for simple Q&A, more recent ones are better at tool calling, and better in loading relevant information into context.\n\n> Is there realistically any way to make <10B models work for this use case? (With better formatting, instruction tuning, curriculum, synthetic data, continued pretraining, etc.) \n\nYes, small models can work fine on short messages, but it will need more wrangling and \"stiffer\" prompting (you need to be much more detailed on the system prompt), oftentimes even [double prompting](https://arxiv.org/abs/2512.14982).\n\n> If small models are not suitable, what‚Äôs a practical lower bound? 34B? 70B? 100B? 500B?\n\nNot sure about a lower bound, but i've used Devstral-Small-2 24B for local agent and it works fine, had also used ministral 14B for local rag and its slighly (noticably) worse at toolcall. So maybe around these parameter size (14B > X > 24B)?\n\n> Or am I likely doing something fundamentally wrong in data prep, training objective, or fine-tuning strategy? \n\nWell, you could\n1. Re-evaluate whether the cost of finetuning (and labour) are worthwhile compared to spending more on local hardware to host a larger model.\n2. Rethink whether a RAG system work better (it might)\n3. Find other avenues a, eg. ZDR provider (as an enterprise) and only host the data locally",
          "score": 1,
          "created_utc": "2026-01-26 12:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uyt2j",
              "author": "ZaRyU_AoI",
              "text": "Really informative, thanks.\n1. Management is looking into it.. could go either way atp honestly.\n2. A RAG module is in the development pipeline currently.. let's hope it works, fingers crossed\n3. Client is looking for a personalised, fine-tuned, locally hosted AI. Although we are outsourcing parts of it to ZDRs, we still need to show involvement of our fine-tuned model in the pipeline.",
              "score": 1,
              "created_utc": "2026-01-26 17:59:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1upsfi",
          "author": "Confident-Ad-3212",
          "text": "How many samples are you using? DM me if you want advice",
          "score": 1,
          "created_utc": "2026-01-26 17:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kycsl",
          "author": "Blahblahblakha",
          "text": "If you‚Äôre trying to get conversational ability, sub 8b models are not appropriate for a fine tune. I had to do the same for style transfer in the marketing domain. Sub 8b models fail at conversational ability and instruction following after Lora. Havent explored it in depth but i think it could be because low param models primarily end up learning output formats instead of the pattern in the fine tuning data. \n\nI experimented will almost all major sub 8b models. But only found acceptable conversational quality starting with qwen2.5-14b",
          "score": 1,
          "created_utc": "2026-01-25 07:42:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1li3ck",
          "author": "danish334",
          "text": "Start finetuning bigger models and if you encounter same issue then it's your setup that has problem (like your dataset).",
          "score": 0,
          "created_utc": "2026-01-25 10:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lkp3q",
              "author": "ZaRyU_AoI",
              "text": "Not a cost effective way.. but I see your point.",
              "score": 1,
              "created_utc": "2026-01-25 10:58:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1llhio",
                  "author": "danish334",
                  "text": "Btw, which platform you used fot FT?",
                  "score": 1,
                  "created_utc": "2026-01-25 11:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kk4o7",
          "author": "nmrk",
          "text": ">..\\[T\\]he boy began to delight in his daring flight, and abandoning his guide, drawn by desire for the heavens, soared higher. His nearness to the devouring sun softened the fragrant wax that held the wings: and the wax melted: he flailed with bare arms, but losing his oar-like wings, could not ride the air. Even as his mouth was crying his father‚Äôs name, it vanished into the dark blue sea, the Icarian Sea, called after him. The unhappy father, now no longer a father, shouted ‚ÄòIcarus, Icarus where are you? Which way should I be looking, to see you?‚Äô ‚ÄòIcarus‚Äô he called again. Then he caught sight of the feathers on the waves, and cursed his inventions. He laid the body to rest, in a tomb, and the island was named Icaria after his buried child.",
          "score": -7,
          "created_utc": "2026-01-25 05:49:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qliszu",
      "title": "Enterprise data is messy, how do you make it work for AI?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qliszu/enterprise_data_is_messy_how_do_you_make_it_work/",
      "author": "chakratones",
      "created_utc": "2026-01-24 09:38:09",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.92,
      "text": "So pulling data from Salesforce, NetSuite, whatever enterprise systems you're stuck with that part's easy. It's what comes after that's a nightmare.\n\nYou extract everything and now you've got these giant tables, JSON files nested like Russian dolls, and absolutely zero context about what any of it means. Even the fancy LLMs just kinda... stare at it blankly. They can't reason over data when they don't know what \"field\\_7829\" actually represents or how it relates to anything else.\n\nCame across¬†[this article](https://thenewstack.io/how-precog-adds-business-context-to-make-enterprise-data-ai-ready/)¬†talking about adding business context early in the pipeline instead of trying to fix it later but I'm curious, what's actually working for you all?\n\nAre you building out semantic layers? Going heavy on NL to SQL? Experimenting with RAG setups? Or have you just accepted that AI answers on enterprise data are gonna be inconsistent at best?\n\nFeel like everyone's solving this differently and I'd love to hear what's actually holding up in production vs what sounds good in theory",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qliszu/enterprise_data_is_messy_how_do_you_make_it_work/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1eh7g1",
          "author": "Miclivs",
          "text": "Shameless plug, [clarityq.ai](https://www.clarityq.ai/), this is our specialization. tldr: RAG sucks, agentic search is way better, semantic layers suck, semantic catalogs (or the equivalent of property graphs) are way better. Data modeling is hard, ongoing maintenance is hard.",
          "score": 1,
          "created_utc": "2026-01-24 09:54:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ewxc0",
          "author": "siggywithit",
          "text": "Yup. Getting data out is the easy part.  Making it useful is where we spend the bulk of our time. Seen lots of chatter about this.  Anyone using it in production?",
          "score": 1,
          "created_utc": "2026-01-24 12:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gvbx9",
          "author": "Lost-Bathroom-2060",
          "text": "Interesting. I replied here to follow the thread",
          "score": 1,
          "created_utc": "2026-01-24 18:18:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iz92j",
          "author": "tuffunny",
          "text": "We gave up on trying to make AI work directly with our raw enterprise data and just built a data warehouse with proper modeling first. I know that's not the sexy AI answer everyone wants to hear, but you can't skip the fundamentals.\n\nOnce we had clean dimensional models with proper naming conventions and documentation, THEN we pointed AI at it. Works way better. Turns out \"garbage in, garbage out\" applies to AI too, who knew?",
          "score": 1,
          "created_utc": "2026-01-25 00:18:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1izwyx",
          "author": "Zachariah1st",
          "text": "We use dbt to add descriptions/tags to everything, then feed that metadata to the LLM. Way better than raw table dumps. The descriptions are literally just this is ARR or this connects to customers via account\\_id",
          "score": 1,
          "created_utc": "2026-01-25 00:22:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j0v74",
          "author": "Ok_Spinach_4348",
          "text": "The real answer nobody wants to hear: you need a data catalog first. We use Atlan. Yeah it's more tooling and more $$ but AI without knowing your lineage/definitions is just expensive guessing.",
          "score": 1,
          "created_utc": "2026-01-25 00:26:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkzkn2",
      "title": "Mirascope: Typesafe, Pythonic, Composable LLM abstractions",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qkzkn2/mirascope_typesafe_pythonic_composable_llm/",
      "author": "teamdandelion",
      "created_utc": "2026-01-23 18:58:56",
      "score": 11,
      "num_comments": 16,
      "upvote_ratio": 0.87,
      "text": "Hi everyone! I'm an at Mirascope, a small startup shipping open-source LLM infra. We just shipped v2 of our open-source Python library for typesafe LLM abstractions, and I'd like to share it.\n\n*TL;DR: This is a Python library with solid typing and cross-provider support for streaming, tools, structured outputs, and async, but without the overhead or assumptions of being a framework. Fully open-source and MIT licensed.*\n\nAlso, advance note: All em-dashes in this post were written by hand. It's option+shift+dash on a Macbook keyboard ;)\n\nIf you've felt like LangChain is too heavy and LiteLLM is too thin, Mirascope might be what you're looking for. It's not an \"agent framework\"‚Äîit's a set of abstractions so composable that you don't actually need one. Agents are just tool calling in a while loop.\n\nAnd it's got 100% test coverage, including cross-provider end-to-end tests for every features that use VCR to replay real provider responses in CI.\n\nThe pitch: How about a low-level API that's typesafe, Pythonic, cross-provider, exhaustively tested, and intentionally designed?  \n\n\nMirascope's focus is on typesafe, composable abstractions. The core concepts is you have an `llm.Model` that generates `llm.Response`s, and if you want to add tools, structured outputs, async, streaming, or MCP, everything just clicks together nicely. Here are some examples:\n\n    from mirascope import llm\n    \n    model: llm.Model = llm.Model(\"anthropic/claude-sonnet-4-5\")\n    response: llm.Response = model.call(\"Please recommend a fantasy book\")\n    print(response.text())\n    # > I'd recommend The Name of the Wind by Patrick Rothfuss...\n\nOr, if you want streaming, you can use `model.stream(...)`  along with `llm.StreamResponse`:\n\n    from mirascope import llm\n    \n    model: llm.Model = llm.Model(\"anthropic/claude-sonnet-4-5\")\n    response: llm.StreamResponse = model.stream(\"Do you think Pat Rothfuss will ever publish Doors of Stone?\")\n    \n    for chunk in response.text_stream():\n      print(chunk, flush=True, end=\"\")\n\nEach response has the full message history, which means you can continue generation by calling \\`response.resume\\`:\n\n    from mirascope import llm\n    \n    response = llm.Model(\"openai/gpt-5-mini\").call(\"How can I make a basil mint mojito?\")\n    print(response.text())\n    \n    response = response.resume(\"Is adding cucumber a good idea?\")\n    print(response.text())\n\n`Response.resume` is a cornerstone of the library, since it abstracts state tracking in a very predictable way. It also makes tool calling a breeze. You define tools via the `@llm.tool` decorator, and invoke them directly via the response.\n\n    from mirascope import llm\n    \n    @llm.tool\n    def exp(a: float, b: float) -> float:\n        \"\"\"Compute an exponent\"\"\"\n        return a ** b \n    \n    model = llm.Model(\"anthropic/claude-haiku-4-5\")\n    response = model.call(\"What is (42 ** 3) ** 2?\", tools=[exp])\n    \n    while response.tool_calls:\n      print(f\"Calling tools: {response.tool_calls}\")\n      tool_outputs = response.execute_tools()\n      response = response.resume(tool_outputs)\n    \n    print(response.text())\n\nThe `llm.Response` class also allows handling structured outputs in a typesafe way, as it's generic on the structured output format. We support primitive types as well as Pydantic `BaseModel` out of the box:\n\n    from mirascope import llm \n    from pydantic import BaseModel\n    \n    class Book(BaseModel):\n        title: str\n        author: str\n        recommendation: str\n    \n    # nb. the @llm.call decorator is a convenient wrapper.\n    # Equivalent to model.call(f\"Recommend a {genre} book\", format=Book)\n    \n    @llm.call(\"anthropic/claude-sonnet-4-5\", format=Book)\n    def recommend_book(genre: str):\n      return f\"Recommend a {genre} book.\"\n    \n    response: llm.Response[Book] = recommend_book(\"fantasy\")\n    book: Book = response.parse()\n    print(book)\n    \n\nThe upshot is that if you want to do something sophisticated‚Äîlike a streaming tool calling agent‚Äîyou don't need a framework, you can just compose all these primitives.\n\n    from mirascope import llm\n    \n    @llm.tool\n    def exp(a: float, b: float) -> float:\n        \"\"\"Compute an exponent\"\"\"\n        return a ** b \n    \n    @llm.tool\n    def add(a: float, b: float) -> float:\n        \"\"\"Add two numbers\"\"\"\n        return a + b \n    \n    model = llm.Model(\"anthropic/claude-haiku-4-5\")\n    response = model.stream(\"What is 42 ** 4 + 37 ** 3?\", tools=[exp, add])\n    \n    while True:\n        for chunk in response.pretty_stream():\n            print(chunk, flush=True, end=\"\")\n        if response.tool_calls:\n          tool_output = response.execute_tools()\n          response = response.resume(tool_output) \n        else:\n            break # Agent is finished\n\nI believe that if you give it a spin, it will delight you, whether you're coming from the direction of wanting more portability and convenience than using raw provider SDKs, or wanting more hands-on control than the big agent frameworks. These examples are all runnable, you can run`uv add \"mirascope[all]\"`, and set API keys.\n\nYou can read more in the [docs](https://mirascope.com/docs/learn/llm/quickstart), see the source on [GitHub](https://github.com/Mirascope/mirascope/tree/main), or join our [Discord](https://mirascope.com/discord-invite). Would love any feedback and questions :)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qkzkn2/mirascope_typesafe_pythonic_composable_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1ahy89",
          "author": "MaticPecovnik",
          "text": "I like it at first glance. But why choose your framework if I can use pydantic-ai? It seems quite similar in design. What sets you apart?",
          "score": 4,
          "created_utc": "2026-01-23 19:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1an8c1",
              "author": "teamdandelion",
              "text": "PydanticAI shares a lot of values (e.g. type safety and structured output support), and both make it easy to write powerful agents. The difference is in philosophy of how you get there. Pydantic AI uses agents as the core primitive, whereas in Mirascope the primitives are one step \"more atomic\" at the level of LLM models, calls, and requests. Rather than providing a fixed agent as a primitive, we've just made it really easy to roll your own Agent.\n\nMirascope is one step \"closer to metal\" (ie. direct control over the LLM requests you're making, just with really nice APIs), whereas Pydantic gets convenience by going up one layer of abstraction.\n\nFor a lot of use cases both will serve well, but Mirascope takes more of an \"onion\" approach where it's easy to peel back a layer and get more direct control over LLM execution. The upshot is its easy to write Pydantic-style agents using Mirascope, but you also get flexibility over exactly what happens in a way that otherwise wouldn't exist.",
              "score": 6,
              "created_utc": "2026-01-23 19:39:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1aoz7v",
                  "author": "wbakst",
                  "text": "\\+1\n\nHi! I'm another author of Mirascope. I've spent a lot of time in the AI space developing open-source tools, and one thing that has been consistent throughout is that at some point I always want to break glass. I want to be able to seamlessly move up and down the interfaces of the tool as I need.\n\nWe didn't want an agent framework, we wanted the tools to easily build our own agent harnesses without abstraction hell. We built Mirascope because we felt no such tool existed yet. We wanted \"breaking glass\" to be top of mind in our development philosophy, and I think we've done a pretty good job of this.\n\nI think you'll feel the love and care we've put into everything we've built. I'm curious to hear what you think, and we always welcome any and all feedback with open arms!",
                  "score": 3,
                  "created_utc": "2026-01-23 19:48:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1dntoq",
                  "author": "MaticPecovnik",
                  "text": "Ok I get it and I agree.\n\nWhat I really like about pydantic ai is their focus od dependency injection and testing with their overrides. How do you do unit tests in Mirascope, assuming you don‚Äôt want to call LLMs durnt unit tests.",
                  "score": 2,
                  "created_utc": "2026-01-24 05:37:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1aidfb",
          "author": "langcuck",
          "text": "\"If you've felt like LangChain is too heavy\"\n\nThat's a nice way of putting it lol",
          "score": 4,
          "created_utc": "2026-01-23 19:16:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1axbjd",
              "author": "ChanceKale7861",
              "text": "Bahahahahhahaah‚Ä¶\n\nYou win. ü•á üòÇüôå",
              "score": 1,
              "created_utc": "2026-01-23 20:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1axr73",
                  "author": "langcuck",
                  "text": "just being honest ü§∑üèª‚Äç‚ôÇÔ∏è",
                  "score": 2,
                  "created_utc": "2026-01-23 20:29:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c597s",
          "author": "danigoncalves",
          "text": "You don't do that to somebody like me who was about to start a new AI project and thought about using PydanticAI. I really liked what I saw - it seems simple with a low learning curve, and the fact that you lay down the foundation with `llm.Model` and allow us to compose only the abstractions we need when we need them is very important when applications tend to become bigger. Optimizing and improving these parts atomically (one at a time) becomes crucial.",
          "score": 2,
          "created_utc": "2026-01-24 00:05:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c8xo9",
              "author": "wbakst",
              "text": "You're exactly who we built this for :)\n\nExcited for you to give it a try and let us know what you think!!",
              "score": 2,
              "created_utc": "2026-01-24 00:24:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ca1ii",
                  "author": "danigoncalves",
                  "text": "You bet :)",
                  "score": 2,
                  "created_utc": "2026-01-24 00:30:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1feynr",
                  "author": "danigoncalves",
                  "text": "I was messing around with a few examples and had two feelings (might be wrong). \n\n1. For me, the best frameworks are those where the source code itself serves as documentation. This is one of them.\n\n2. Is it me or instead of promoting this as an alternative to frameworks as pydanticAI, it can also be promoted as a sidecar for agents or other specific purpose frameworks? Part of the composable abstractions advantage is that if you feel that it serves you better have a higher abstract for agents, just plugin another framework and work with both. I think the flexibility is a huge win for the applications that start small and unscoped and grow into something more specific in the future.",
                  "score": 2,
                  "created_utc": "2026-01-24 14:12:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qnpgcr",
      "title": "Stop manually iterating on agent prompts: I built an open-source offline analyzer based on Stanford's ACE that extracts prompt improvements from execution traces",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "author": "cheetguy",
      "created_utc": "2026-01-26 19:04:50",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "Some of you might have seen my [previous post](https://reddit.com/r/LLMDevs/comments/1obp91s/i_opensourced_stanfords_agentic_context/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: agentic system prompting from agent traces.\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's historical execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insight\n\n**How this compares to DSPy/GEPA:**\n\nWhile DSPy works best with structured data (input/output pairs), ACE is designed to work directly on execution traces (logs, conversations, markdown files) and keeps humans in the loop for review. Compared to GEPA, the ACE paper was able to show significant improvements on benchmarks.\n\n**Try it yourself:** [https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear your feedback if you do try it out",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o20ricf",
          "author": "JustViktorio",
          "text": "Why not to make a CLI tool from that so it could be callable in Claude Code / Codex / Open Code runtime?",
          "score": 2,
          "created_utc": "2026-01-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o217fmj",
              "author": "cheetguy",
              "text": "We're working on this. Hopefully we can release in the next couple of days. Join our Discord to stay updated: [https://discord.com/invite/mqCqH7sTyK](https://discord.com/invite/mqCqH7sTyK)",
              "score": 1,
              "created_utc": "2026-01-27 15:43:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjzq0b",
      "title": "Why Energy-Based Models (EBMs) outperform Transformers on Constraint Satisfaction Problems (like Sudoku).",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qjzq0b/why_energybased_models_ebms_outperform/",
      "author": "bully309",
      "created_utc": "2026-01-22 16:50:11",
      "score": 10,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "We all know the struggle with LLMs when it comes to strict logic puzzles or complex constraints. You ask GPT-4 or Claude to solve a hard Sudoku or a scheduling problem, and while they sound confident, they often hallucinate a move that violates the rules because they are just predicting the next token probabilistically.  \n  \nI've been following the work on [Energy-Based Models](https://logicalintelligence.com/kona-ebms-energy-based-models), and specifically how they differ from autoregressive architectures.  \n  \nInstead of \"guessing\" the next step, the EBM architecture seems to solve this by minimizing an energy function over the whole board state.  \n  \nI found this benchmark pretty telling: [https://sudoku.logicalintelligence.com/](https://sudoku.logicalintelligence.com/)  \n  \nIt pits an EBM against standard LLMs. The difference in how they \"think\" is visible - the EBM doesn't generate text; it converges on a valid state that satisfies all constraints (rows, columns, boxes) simultaneously.  \n  \nFor devs building agents: This feels significant for anyone trying to build reliable agents for manufacturing, logistics, or code generation. If we can offload the \"logic checking\" to the model's architecture (inference time energy minimization) rather than writing endless Python guardrails, that‚Äôs a huge shift in our pipeline.  \n  \nHas anyone played with EBMs for production use cases yet? Curious about the compute cost vs standard inference.",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qjzq0b/why_energybased_models_ebms_outperform/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o16bpwd",
          "author": "WhoTookPlasticJesus",
          "text": "I feel like I'm an idiot who is unable to navigate a web site. Is there a paper somewhere that I just can't find?",
          "score": 1,
          "created_utc": "2026-01-23 03:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17waqw",
              "author": "bully309",
              "text": "Haha, don't worry, it's not just you! The interface is quite minimalistic, so it's easy to miss. Let me know if you have trouble opening it! Try again.",
              "score": 1,
              "created_utc": "2026-01-23 11:12:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12rxh1",
          "author": "Far_Marionberry1717",
          "text": "> We all know the struggle with LLMs when it comes to strict logic puzzles or complex constraints. \n\nObviously, glorified auto-complete doesn't understand logic.\n\n>  Has anyone played with EBMs for production use cases yet? Curious about the compute cost vs standard inference. \n\nSure, it's called embedded C that you wrote by hand.",
          "score": 0,
          "created_utc": "2026-01-22 17:00:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12xw8v",
              "author": "bully309",
              "text": "Haha valid. For a fixed game like Sudoku, a hard-coded solver wins 100%. But the goal here is a generalizable system. We want a neural net that can handle messy, unstructured inputs (which \"embedded C\" hates) but still adhere to strict logical constraints (which LLMs hate). It's about bridging that gap.",
              "score": 1,
              "created_utc": "2026-01-22 17:27:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ql8b52",
      "title": "Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models",
      "subreddit": "LLMDevs",
      "url": "https://huggingface.co/blog/codelion/reverse-engineering-magic-hashhop",
      "author": "asankhs",
      "created_utc": "2026-01-24 00:41:32",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ql8b52/reverse_engineering_a_500m_mystery_from_hashhop/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qjy6jr",
      "title": "Which AI YouTube channels do you actually watch as a developer?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qjy6jr/which_ai_youtube_channels_do_you_actually_watch/",
      "author": "gargetisha",
      "created_utc": "2026-01-22 15:54:06",
      "score": 9,
      "num_comments": 14,
      "upvote_ratio": 0.91,
      "text": "I‚Äôm trying to clean up my YouTube feed and follow AI creators/educators.\n\nI'm curious to know which are some youtube channels that you as a developer genuinely watch, the type of creators who doesn't just create hype but deliver actual value.\n\nLooking for channels that talk about Agents, RAG, AI infrastructure, and also who show how to build real products with AI.\n\nCurious what you all watch as developers. Which channels do you trust or keep coming back to? Any underrated ones worth following?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qjy6jr/which_ai_youtube_channels_do_you_actually_watch/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o12h6d1",
          "author": "Spursdy",
          "text": "I like prompt engineering\n\nHe usually publishes a couple of days after any new release and gives practical examples/benchmarks on the product.\n\nUseful,, objective content.",
          "score": 2,
          "created_utc": "2026-01-22 16:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12kv6u",
              "author": "gargetisha",
              "text": "I just checked, his videos looks great. Thanks for the suggestion.",
              "score": 2,
              "created_utc": "2026-01-22 16:29:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a919q",
          "author": "Alternative_Nose_874",
          "text": "I mostly stick to people who actually build stuff on camera. Latent Space is solid for agents, infra and real tradeoffs, not hype. Simon Willison is great if you want clear thinking around RAG and LLM tooling, very practical. I also keep coming back to AI Explained for context, even if it‚Äôs more analysis than code. For hands-on building, Harrison Chase (LangChain) is useful despite the noise around the brand. Underrated IMO is Greg Kamradt, lots of real experiments, some rough edges but that‚Äôs fine. I skip anything that smells like ‚Äú10x dev‚Äù thumbnails, learned that the hard way üòÖ",
          "score": 2,
          "created_utc": "2026-01-23 18:34:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12o2ar",
          "author": "Fine-Market9841",
          "text": "Cole bedin",
          "score": 1,
          "created_utc": "2026-01-22 16:43:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14x0vz",
          "author": "John-McKenzie",
          "text": "Can you please put together a website with a list - that you update once you settle on the best ones to follow? There is so much noise in the AI space it's almost unbearable.",
          "score": 1,
          "created_utc": "2026-01-22 23:02:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o15y0am",
          "author": "burntoutdev8291",
          "text": "I don't like hype channels so my interests may be different \n\nhttps://www.youtube.com/@ZacharyLLM/videos\nhttps://www.youtube.com/@jbhuang0604/videos\nhttps://www.youtube.com/@HungyiLeeNTU < chinese but i always liked his videos\nhttps://youtube.com/@umarjamilai < no longer as active after his FT role\nhttps://youtube.com/@aipapersacademy",
          "score": 1,
          "created_utc": "2026-01-23 02:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1697y0",
          "author": "Live-Lab3271",
          "text": "[https://www.youtube.com/@InfraSketchUSA](https://www.youtube.com/@InfraSketchUSA)\n\n  \n[InfraSketch's](https://www.infrasketch.net/) AI agent turns your ideas into architecture diagrams. Chat to iterate, ask questions, and refine. Then export a design doc and start building.",
          "score": 1,
          "created_utc": "2026-01-23 03:24:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1knct2",
              "author": "Scrapemist",
              "text": "So you pay to explain them your ideas?",
              "score": 1,
              "created_utc": "2026-01-25 06:12:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1pthyv",
                  "author": "Live-Lab3271",
                  "text": "Yessir",
                  "score": 1,
                  "created_utc": "2026-01-25 23:28:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18gyqa",
          "author": "hejj",
          "text": "Better Stack",
          "score": 1,
          "created_utc": "2026-01-23 13:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bz03b",
          "author": "Ok-Lack-7216",
          "text": "I have a build your first agent video, end to end here. [https://youtu.be/mOnbK6DuFhc](https://youtu.be/mOnbK6DuFhc) I believe this will give you a good intro. Another one in the channel showing how to code review the same workflow using NotebookLM. All the best with you journey.",
          "score": 1,
          "created_utc": "2026-01-23 23:31:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eidcy",
          "author": "Gtr-practice-journal",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-01-24 10:05:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ejdwn",
          "author": "rodokofn666",
          "text": "Neil Stephenson\nLeon van zyl",
          "score": 1,
          "created_utc": "2026-01-24 10:14:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkec6n",
      "title": "This is kind of blowing my mind... Giving agents a \"Hypothesis-Driven Optimization\" skill",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qkec6n/this_is_kind_of_blowing_my_mind_giving_agents_a/",
      "author": "Floppy_Muppet",
      "created_utc": "2026-01-23 02:22:00",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been experimenting with recursive self-learning for the last few months, and I'm starting to see some really positive results (sry, internal data folks) by equipping my agents with what I guess I'd call a \"Hypothesis-Driven Optimization\" skill.\n\n\n\nBasically, it attempts to automate the scientific method through a perpetual 5-stage loop:\n\n1. **Group I/O's**: Organize I/O performance into three buckets within each problem space cluster (top, bottom, and average).\n2. **Hypothesize**: Use a FM to speculate on why the top and bottom groups diverged from the average.\n3. **Distill**: Use a SLM to turn each hypothesis into actionable hints.\n4. **A/B Test**: RAG those hints into your prompt to see if they outperform your control group.\n5. **Scale or Iterate**: Scale the winning hypothesis' \"Hint Pack\" or use the learnings from failed test to iterate on a new hypothesis.\n\n\n\nPreviously, my agents were setup to simply mimic top-performing I/O's without *traceability* or *testability* of the actual conjecture(s) it was making.\n\n\n\nNow I'm seeing my agents get incrementally better on their own (with stat sig proof), and I know why, and by how much... It's kind of insane rn.\n\n\n\nCurious who else has tried a similar approach yet?!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qkec6n/this_is_kind_of_blowing_my_mind_giving_agents_a/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o161dpp",
          "author": "qa_anaaq",
          "text": "You‚Äôd need a lot of data to approach the problem via this method though, right? What‚Äôs the volume of interactions you‚Äôre working with that‚Äôs showing promising results?",
          "score": 2,
          "created_utc": "2026-01-23 02:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o162w1y",
              "author": "Floppy_Muppet",
              "text": "Yes, you definitely need some amount of scale.\n\nI have hypotheses being generated on clusters when they have a minimum of 10 I/O's AND show a statistically significant difference between performance groups (so, in reality closer to \\~100 minimum I/O's).\n\nFor smaller scale agents, you could try broadening your problem spaces (to add more I/O's into each cluster) as well as the framing of your hypotheses as to try and discover more generally applicable hints.",
              "score": 1,
              "created_utc": "2026-01-23 02:49:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o174qwp",
                  "author": "tom-mart",
                  "text": ">I have hypotheses being generated on clusters when they have a minimum of 10 I/O's\n\n\nIs it 10 thousand or 10 millions I/O's?",
                  "score": 1,
                  "created_utc": "2026-01-23 07:03:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlqqcs",
      "title": "Self-contained npm installable WASM-based Alpine Linux VM for agents",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qlqqcs/selfcontained_npm_installable_wasmbased_alpine/",
      "author": "schmuhblaster_x45",
      "created_utc": "2026-01-24 16:02:12",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I've always thought that it would be great to have small Linux VM that could be integrated and deployed with minimal efforts and dependencies. So thanks to the container2wasm project (https://github.com/container2wasm/container2wasm) and Opus 4.5 I was able to build a small library that gives you just that.   \n  \nHere it is: [https://github.com/deepclause/agentvm](https://github.com/deepclause/agentvm)\n\nIt was quite fascinating to see Opus build an entire user mode network stack in Javascript, then also sobering to watch it try to fix the subtle bugs that it introduced, all while burning though my tokens....eventually it worked though :-)\n\nAnyways, I thought this might be useful, so I am sharing it here.\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qlqqcs/selfcontained_npm_installable_wasmbased_alpine/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1h4lu0",
          "author": "qa_anaaq",
          "text": "What would some use cases be for this, in terms of agent usage? I‚Äôm trying to understand why an agent would need this as a tool. Sandboxing code executions?",
          "score": 1,
          "created_utc": "2026-01-24 18:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j70dh",
              "author": "schmuhblaster_x45",
              "text": "Yes, sandboxing code executions if you don't have a container or other means available would be one reason. Also, not all systems or environments come with a bash and Python interpreter.",
              "score": 1,
              "created_utc": "2026-01-25 00:59:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i3kpm",
          "author": "tisDDM",
          "text": "Both things - c2w and agentvm are great ideas !!!\n\nI directly starred the repo. Maybe it will be the solution to a future problem.",
          "score": 1,
          "created_utc": "2026-01-24 21:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q80bl",
          "author": "promethe42",
          "text": "Outstanding work. I've been building WASM component for Python and other script runners. I desperately want a WASM interface/component to a containerized Linux. This is one of the key components to safe local and distributed agents.",
          "score": 1,
          "created_utc": "2026-01-26 00:39:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qsp50",
              "author": "schmuhblaster_x45",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-26 02:23:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1t5nxq",
                  "author": "promethe42",
                  "text": "You motivated me to do this: [https://github.com/container2wasm/container2wasm/pull/565](https://github.com/container2wasm/container2wasm/pull/565)\n\nI have a running Alpine WASM component running now!",
                  "score": 1,
                  "created_utc": "2026-01-26 12:50:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1ql0i6z",
      "title": "context management on long running agents is burning me out",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1ql0i6z/context_management_on_long_running_agents_is/",
      "author": "Main_Payment_6430",
      "created_utc": "2026-01-23 19:32:40",
      "score": 8,
      "num_comments": 15,
      "upvote_ratio": 0.9,
      "text": "is it just me or does every agent start ignoring instructions after like 50-60 turns. i tell it dont do X without asking me first, 60 turns later it just does X anyway. not even hallucinating just straight up ignoring what i said earlier\n\ntried sliding window, summarization, rag, multiagent nothing really works. feels like the context just rots after a while\n\nhow are you guys handling this",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ql0i6z/context_management_on_long_running_agents_is/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1axa59",
          "author": "Ok_Economics_9267",
          "text": "Keep context as short as possible. Manage memory manually. Add episodic and procedural memories. Search in memory and take only what matters, instead of adding whole memory to context.",
          "score": 3,
          "created_utc": "2026-01-23 20:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ao8jb",
          "author": "Arindam_200",
          "text": "I'm using byterover for it\n\nThey have context tree based approach. You can probably give it a shot",
          "score": 2,
          "created_utc": "2026-01-23 19:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1apqdq",
          "author": "taftastic",
          "text": "Langmem does it, beads helps a lot and makes shorter sessions way easier",
          "score": 2,
          "created_utc": "2026-01-23 19:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bw0zl",
          "author": "neoneye2",
          "text": "In the past I tried plain text responses, and my code was fragile.\n\nNowadays I'm using structured output, and is doing around 100 inference calls. Only asking for very narrow things, so the response stays below 4 kilobytes.\n\nThis is a document I have generated.  \n[https://neoneye.github.io/PlanExe-web/20260104\\_operation\\_falcon\\_report.html](https://neoneye.github.io/PlanExe-web/20260104_operation_falcon_report.html)\n\nAnd this is my code for orchestrating the agents  \n[https://github.com/neoneye/PlanExe/blob/main/worker\\_plan/worker\\_plan\\_internal/plan/run\\_plan\\_pipeline.py](https://github.com/neoneye/PlanExe/blob/main/worker_plan/worker_plan_internal/plan/run_plan_pipeline.py)",
          "score": 2,
          "created_utc": "2026-01-23 23:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b1y0o",
          "author": "one-wandering-mind",
          "text": "use a better model, reinject instructions to just prior to the current conversation turn, use separate models and tools as validators and guardrails for important behaviors to avoid, intentionally manage the context. you probably don't want a generic summary unless what you are building is generic. maintain just the important information for your task(s).",
          "score": 1,
          "created_utc": "2026-01-23 20:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bja3n",
          "author": "Fulgren09",
          "text": "Cache system prompt and send it each turn¬†",
          "score": 1,
          "created_utc": "2026-01-23 22:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bkth1",
          "author": "johnerp",
          "text": "Yes this happens, there‚Äôs maths and reinforcement learning reasons.",
          "score": 1,
          "created_utc": "2026-01-23 22:17:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c3jlq",
          "author": "Charming_Support726",
          "text": "Yes. It rots after a while, almost every model gets awkward after around 150-180k. Jump of early and start new. On opencode things like the DCP help - but you get hit by different issues",
          "score": 1,
          "created_utc": "2026-01-23 23:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e1swr",
          "author": "MajinAnix",
          "text": "Trying to solve this problem too, in my head I have solution with tasks (tasks have separate conversation history, structured output)",
          "score": 1,
          "created_utc": "2026-01-24 07:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e691z",
          "author": "DotPhysical1282",
          "text": "Run a parallel agent whose only job is to ensure your main agent is following instructions. After every x turns, ask it to verify the main agent is following instructions. If it gets it wrong, it‚Äôs time to remind it. Sending the prompt after each turn would be expensive and not necessary if it still has the context",
          "score": 1,
          "created_utc": "2026-01-24 08:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ffb9d",
              "author": "Main_Payment_6430",
              "text": "multiagent approach, i like it!",
              "score": 2,
              "created_utc": "2026-01-24 14:14:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ecsid",
          "author": "ggone20",
          "text": "What model are you using?\n\nEveryone likes to hate on OAI but since GPT 5.2, this is basically a non-issue. It truly does stay coherent though very complex workflows and literal day-long conversation sessions. Curious what other people‚Äôs mileage is here.\n\nBefore 5.2, my general rule of thumb was to never let context exceed 20ish percent of its claimed window. The data has shown since the beginning that anything past literally the first turn performance degraded dramatically.",
          "score": 1,
          "created_utc": "2026-01-24 09:13:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ff7sq",
              "author": "Main_Payment_6430",
              "text": "that's why i created one truth! [https://github.com/justin55afdfdsf5ds45f4ds5f45ds4/onetruth.git](https://github.com/justin55afdfdsf5ds45f4ds5f45ds4/onetruth.git) i build this today, i knew this issue was the same thing i was facing that we need to not let the context exceed, but i am not there to flush things up every second, and i open sourced it",
              "score": 1,
              "created_utc": "2026-01-24 14:13:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ueah2",
          "author": "Kong28",
          "text": "Why is an agent handling 50-60 turns of something?",
          "score": 1,
          "created_utc": "2026-01-26 16:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1anmiz",
          "author": "Altruistic-Spend-896",
          "text": "Langmem",
          "score": -1,
          "created_utc": "2026-01-23 19:41:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql6a1h",
      "title": "I Need help from actual ML Enginners",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1ql6a1h/i_need_help_from_actual_ml_enginners/",
      "author": "Dangerous_Young7704",
      "created_utc": "2026-01-23 23:16:57",
      "score": 8,
      "num_comments": 34,
      "upvote_ratio": 0.79,
      "text": "**Hey, I revised this post to clarify a few things and avoid confusion.**\n\nHi everyone. Not sure if this is the right place, but I‚Äôm posting here and in the ML subreddit for perspective.\n\n**Context**  \nI run a small AI and automation agency. Most of our work is building AI enabled systems, internal tools, and workflow automations. Our current stack is mainly Python and n8n, which has been more than enough for our typical clients.\n\nRecently, one of our clients referred us to a much larger enterprise organization. I‚Äôm under NDA so I can‚Äôt share the industry, but these are organizations and individuals operating at a 150M$ plus scale.\n\nThey want:\n\n* A private, offsite web application that functions as internal project and operations management software\n* A custom LLM powered system that is heavily tailored to a narrow and proprietary use case\n* Strong security, privacy, and access controls with everything kept private and controlled\n\nTo be clear upfront, we are not planning to build or train a foundation model from scratch. This would involve using existing models with fine tuning, retrieval, tooling, and system level design.\n\nThey also want us to take ownership of the technical direction of the project. This includes defining the architecture, selecting tooling and deployment models, and coordinating the right technical talent. We are also responsible for building the **core web application and frontend** that the LLM system will integrate into.\n\nThis is expected to be a multi year engagement. Early budget discussions are in the 500k to 2M plus range, with room to expand if it makes sense.\n\n**Our background**\n\n* I come from an IT and infrastructure background with USMC operational experience\n* We have experience operating in enterprise environments and leading projects at this scale, just not in this specific niche use case\n* Hardware, security constraints, and controlled environments are familiar territory\n* I have a strong backend and Python focused SWE co founder\n* We have worked alongside ML engineers before, just not in this exact type of deployment\n\nWhere I‚Äôm hoping to get perspective is mostly around **operational and architectural decisions**, not fundamentals.\n\n**What I‚Äôm hoping to get input on**\n\n1. **End to end planning at this scope** What roles and functions typically appear, common blind spots, and things people underestimate at this budget level\n2. **Private LLM strategy for niche enterprise use cases** Open source versus hosted versus hybrid approaches, and how people usually think about tradeoffs in highly controlled environments\n3. **Large internal data at the terabyte scale** How realistic this is for LLM workflows, what architectures work in practice, and what usually breaks first\n4. **GPU realities** Reasonable expectations for fine tuning versus inference Renting GPUs early versus longer term approaches When owning hardware actually makes sense, if ever\n\nThey have also asked us to help recruit and vet the right technical talent, which is another reason we want to set this up correctly from the start.\n\nIf you are an ML engineer based in South Florida, feel free to DM me. That said, I‚Äôm mainly here for advice and perspective rather than recruiting.\n\n**To preempt the obvious questions**\n\n* No, this is not a scam\n* They approached us through an existing client\n* Yes, this is a step up in terms of domain specificity, not project scale\n* We are not pretending to be experts at everything, which is why we are asking\n\nI‚Äôd rather get roasted here than make bad architectural decisions early.\n\nThanks in advance for any insight.  \n  \nEdit - P.S To clear up any confusion, we‚Äôre mainly building them a secure internal website with a frontend and backend to run their operations, and then layering a private LLM on top of that.\n\nThey basically didn‚Äôt want to spend months hiring people, talking to vendors, and figuring out who the fuck they actually needed, so they asked us to spearhead the whole thing instead. We own the architecture, find the right people, and drive the build from end to end.\n\nThat‚Äôs why from the outside it might look like, ‚Äúhow the fuck did these guys land an enterprise client that wants a private LLM,‚Äù when in reality the value is us taking full ownership of the technical and operational side, not just training a model.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ql6a1h/i_need_help_from_actual_ml_enginners/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1c0lcu",
          "author": "flonnil",
          "text": "not sure if you're trolling, so i'll open with the roasting-bit: terrabytes of internal data dont match terribly well with ML and a 1M$ budget.",
          "score": 14,
          "created_utc": "2026-01-23 23:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c222a",
              "author": "DistributionOk6412",
              "text": "I was about to day the same thing...we pay close to 3M yearly on ~5TB of data (as far as I remember, forgot how much data was exactly, but close to some TB)",
              "score": 2,
              "created_utc": "2026-01-23 23:47:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1c8oxs",
              "author": "Dangerous_Young7704",
              "text": "My client has a infinited budget, I kinda just threw numbers out there, but is $1 m not enough? I'm asking because they want me to estimate the cost",
              "score": -4,
              "created_utc": "2026-01-24 00:23:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ce3xn",
                  "author": "flonnil",
                  "text": "here's my advice: Do not under any circumstances say any more numbers to that guy before you have talked to multiple experts multiple times and now precisely what you are going to do.\n\ni'm sure it would be easier for people here to give you advice if we would know in broad terms and concepts what we're actually talking about.",
                  "score": 6,
                  "created_utc": "2026-01-24 00:52:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c25zp",
          "author": "kubrador",
          "text": "hire a fractional ML/LLM architect first (like, this month) before making any other moves. you need someone who's done enterprise llm deployments to sense-check your decisions, and you clearly don't have that in-house. the $30-50k you spend on a good advisor now saves you $200k+ in wrong infrastructure choices later. companies like this don't care that you're figuring it out. they care that you have \\*someone\\* who knows what they're doing.",
          "score": 7,
          "created_utc": "2026-01-23 23:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c8ztc",
              "author": "Dangerous_Young7704",
              "text": "Alright, now I'm getting somewhere. Thank you for the response. Mind if I dm you? I got a whole lot of questions",
              "score": 1,
              "created_utc": "2026-01-24 00:25:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1c9frf",
                  "author": "kubrador",
                  "text": "sure",
                  "score": 1,
                  "created_utc": "2026-01-24 00:27:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d71sf",
          "author": "TheRealStepBot",
          "text": "If I‚Äôm being honest your client are idiots and picked the wrong people for this. Why hire people with zero ml experience and not even a clear path to even maybe having the sort of background to have a shot at this sort of thing? \n\nWhy do you think the people who know how to do this would want to deal you in?\n\nAnd people wonder why supposedly 90% of ai initiatives fail, it‚Äôs cause 90% of ai projects don‚Äôt even have ML engineers on them who have ever built anything interesting, never mind people who have specific experience with llm‚Äôs\n\nIt‚Äôs all a bunch of script kiddies jamming shit they don‚Äôt understand together with ai llm vibe coding and then people don‚Äôt understand why it doesn‚Äôt work. \n\nYou are getting ripped off by people with a lot less to lose than you.",
          "score": 6,
          "created_utc": "2026-01-24 03:43:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d8v1j",
              "author": "Dangerous_Young7704",
              "text": "I don't think you read the post, which sucks, but thanks for taking the time to respond",
              "score": 1,
              "created_utc": "2026-01-24 03:54:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1d9kv7",
                  "author": "TheRealStepBot",
                  "text": "Oh I read it all right.\n\nYou‚Äôre promising all kinds of shit to people that you can‚Äôt deliver on and now you‚Äôre panicking. \n\nAnd the people you promised this to will happily let the project fail and ruin your life and it won‚Äôt be any skin off their back, they will just do the project again with some other inept low bidder and hope they eventually strike gold for cheap.",
                  "score": 3,
                  "created_utc": "2026-01-24 03:59:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1djfxu",
          "author": "mdizak",
          "text": "I'd revisit the entire need to use a LLM as anything more than a text rendering engine, and find another avenue to organize, store, sort filter and search those terabytes of data..  Think RAG, but higher quality and mor deterministic.",
          "score": 3,
          "created_utc": "2026-01-24 05:05:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1d5oyx",
          "author": "cuba_guy",
          "text": "Congrats, great opportunity and if you're cofounder is at the lead+ eng level in the current market switch is possible. Ai with llms are much closer to soft eng than data/ml every was imo and I see a lot of good engineers transitioning. Learn a lot, Ai engineering book, agentic design patterns and hugging face free courses should get you started",
          "score": 2,
          "created_utc": "2026-01-24 03:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d9dwn",
              "author": "Dangerous_Young7704",
              "text": "It is definitely a unicorn of a client willing to pay big bucks for this, but were not arrogant. My co-founder and I know we need to hire the actual ML/AI engineers on this one, but we're definitely going to use this as experience to dive deep into actual ML. Thanks for trecommendedded courses!",
              "score": 2,
              "created_utc": "2026-01-24 03:57:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3bup",
          "author": "Altruistic-Spend-896",
          "text": "Hmm...never ever invest in hardware first, do an MVP and ask if it's acceptable, on a reduced set. Fine-tuning a narrow LLM is doable, but the real strength lies in continuous monitoring , drift detection and retraining.-signed, your friendly senior MLOps eng",
          "score": 2,
          "created_utc": "2026-01-24 07:47:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ew3b1",
          "author": "itsmeknt",
          "text": "A lot depends on the specific requirements of the project. Real time chat application will have a very different architecture than offline batch doc processing. Docs in structured text files are very different than raw docs in PDFs or images.\n\nWithout understanding the project, I can only speak very generally:\n1. Requirements doc (including timeline) + budgeting comes first, which will determine hiring, architecture, hardware, milestones and schedule planning.\n2. Will depend on data security requirements, but the ideal case is to first try private hosted providers if the project allows it. You can stress test to find the actual demand curve and then make an educated guess on the hardware and its financial projections thereafter.\n3. At this scale I'm assuming offline batch doc processing. If self hosted, will need batch optimized inference servers like vllm, and it will be a trade off between speed, accuracy/intelligence, and $$$ but it can be doable. If hosted, then its a matter of negotiating with the provider.\n4. 4bit Qlora fine tune needs 2-4x more VRAM than small-cache inference, full fine tune needs 10-20x more VRAM. Yes you want to rent GPUs at first until you know your exact load and requirements, and if you end up determining that you can keep your own hardware GPUs under constant load then it will pay itself back in about 6 months.\n5. Architecture design roles as soon as possible, because the early planning stage can really make this 2x easier or 8x harder than it needs to be. And someone experienced in this field to accurately asses the hiring candidates, as its hard to tell who is competent vs just well practiced in interivews if you dont have the experience yourself.",
          "score": 2,
          "created_utc": "2026-01-24 12:06:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fmso3",
          "author": "damhack",
          "text": "Sounds like a Big Data job with some LLM interface sugar.  You need to architect the necessary private cloud infrastructure, get your Parquet (or other) pipeline in place, sort the datalake/warehouse, design the right APIs for the usecases and install a GPU cluster with VLLM, MLOps platform and a decent chat UI (e.g. LibreChat).  That‚Äôs about 7 different people.  Even $1m is too little for the team you‚Äôll need and I‚Äôm assuming they have another $1m for the infrastructure?",
          "score": 2,
          "created_utc": "2026-01-24 14:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gdetw",
          "author": "coloradical5280",
          "text": "Questions:\n\n1) yes\n2) unsloth, is where you start\n3) yes, and transformer architectures. That‚Äôs what LLMs are, terabytes if data, this is standard stuff for Enterprise \n4) Deoends on a lot of factors like how fast you want it done, but you‚Äôll need ~14 H100s just to run Kimi k2 1T, and potentially many, many more depending on active users and needed inference speed, and how much can be batched vs live. TO TUNE AND TRAIN, you don‚Äôt necessarily need more but I‚Äôd imagine they want to do this right and not have a months long running task, so you‚Äôre realistically looking at a full rack of B200s that would need to be rented, across multiple epochs. \n\nNone of this even considers what they have for SFT datasets, you‚Äôll have to outsource that, Enterprise ops like this will often just use the scale.ai level providers. Just for that custom dataset that you need for RL and really all post-training, they‚Äôre looking mid six-figures, potentially less or way more, depending on how messy or clean their data is and how multi modal and parsey things need to get. \n\n**Their current budget might get them a nice RAG.**\n\nThey have no idea what they want , which is typical , they need someone who really knows what they‚Äôre doing to sit down and probe the real problems and pain points and goals, and from there , map that to reality. \n\nDo not take this meeting , or if you did, don‚Äôt take the engagement, it‚Äôs not worth the potential reputation hit to your firm. You have what sounds like a nice little automation and workflow optimization consulting shop. That is not what they say they need. Like I said they don‚Äôt know what they need, but on scale alone, the inputs and outputs don‚Äôt match it‚Äôs just not a fit.",
          "score": 2,
          "created_utc": "2026-01-24 17:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gqy8g",
              "author": "Dangerous_Young7704",
              "text": "Thank you for the information and the kind words. ML engineers are truly nice dudes, but we're not taking on building the LLM by ourselves. I'm sorry if the post came off as such; it's more like we need to find the right people and qualify this project on the scale. We have done things similar, but never using or fine-tuning a private LLM of this scale",
              "score": 1,
              "created_utc": "2026-01-24 18:00:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gzu54",
                  "author": "coloradical5280",
                  "text": "Ohh gotcha. I think you just need the right people to ask the right questions, to get to the real problem or use case. My guess is they don‚Äôt need to actually fine tune a model, or at least, would agree that the ROI isn‚Äôt there, once they realize what that actually means. \n\nThey need an agentic RAG with knowledge graph, hybrid search obviously, all the works. The one model it could be useful for them to train is a cross encoder reranker thats constantly training on the data. Like SBERT deal like Marco, would be the easiest, lots of options. \n\nSounds like you some people reached out but happy to chat as well if you want to DM. Not sure I have the bandwidth currently to be your guy, but more than happy to help you vet other options.",
                  "score": 2,
                  "created_utc": "2026-01-24 18:37:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bysab",
          "author": "LingeringDildo",
          "text": "If you want a consulting situation, DM me. Sounds like defense-adjacent work, and that‚Äôs my area of expertise. \n\nFirst call is on me and happy to sign a multi party NDA with you and your potential customer. üôÇ",
          "score": 3,
          "created_utc": "2026-01-23 23:30:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c8epg",
              "author": "valuat",
              "text": "Well put. Pay-to-play is the name of the game.",
              "score": 2,
              "created_utc": "2026-01-24 00:22:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1caps8",
              "author": "Dangerous_Young7704",
              "text": "I sent you a DM",
              "score": 1,
              "created_utc": "2026-01-24 00:34:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gn02o",
          "author": "Sufficient_Ad_3495",
          "text": "Let it go buddy‚Ä¶ on so many issues‚Ä¶ you‚Äôre not going win such a project. Best introduce and partner as a finders fee.",
          "score": 1,
          "created_utc": "2026-01-24 17:43:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gq96s",
              "author": "Dangerous_Young7704",
              "text": "Thanks for trying to look out for me even though im some stranger on the internet, but we're not building the LLM and we have two main jobs, handle the web development and find the correct people for the project and manage it, so we have experience manageing projects of this level and worked with ML engineers just never Private LLM in this type of enviorment.",
              "score": 2,
              "created_utc": "2026-01-24 17:57:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gyqvu",
                  "author": "Sufficient_Ad_3495",
                  "text": "You want to find a company that can do it all on their behalf maybe two, and front those Companies with a  tight NDA and introduction and then for a period of time over which they are conducting for this particular project if it wins they pay you a fee undisclosed to the client. So you are managing the introduction to sale that‚Äôs the best you can hope for.\n\nYou are managing a one off introduction.. you will not be managing a project ongoing, that large company will not be managing the project through you because of risks, their purchasing wouldn‚Äôt allow it regardless of the enthusiasm and zeal through which your internal contact has instructed . I can tell your internal contact is senior but they‚Äôre not a decision maker, so caution with your time. I hope that helps.",
                  "score": 2,
                  "created_utc": "2026-01-24 18:33:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qk6tpa",
      "title": "Still using real and expensive LLM tokens in development? Try mocking them! üê∂",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qk6tpa/still_using_real_and_expensive_llm_tokens_in/",
      "author": "kshantanu94",
      "created_utc": "2026-01-22 21:07:34",
      "score": 7,
      "num_comments": 16,
      "upvote_ratio": 0.77,
      "text": "Sick of burning $$$ on OpenAI/Claude API calls during development and testing? Say hello to **MockAPI Dog‚Äôs new** [Mock LLM API](http://mockapi.dog/llm-mock) \\- a free, no-signup required way to spin up LLM-compatible streaming endpoints in under 30 seconds.\n\n‚ú® **What it does:**  \n‚Ä¢ Instantly generate streaming endpoints that mimic **OpenAI**, **Anthropic Claude**, *or generic* LLM formats.  \n‚Ä¢ Choose content modes (generated, static, or hybrid).  \n‚Ä¢ Configure token output and stream speed for realistic UI testing.  \n‚Ä¢ Works with SSE streaming clients and common SDKs - just switch your baseURL!\n\nüí° **Why you‚Äôll love it:**  \n‚úî Zero cost - free mocks for development, testing & CI/CD.  \n‚úî No API keys or billing setup.  \n‚úî Perfect for prototyping chat UIs, test automation, demos, and more.\n\nGet started in seconds - [mockapi.dog/llm-mock](http://mockapi.dog/llm-mock) üê∂  \nDocs - [https://mockapi.dog/docs/mock-llm-api](https://mockapi.dog/docs/mock-llm-api)  \n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qk6tpa/still_using_real_and_expensive_llm_tokens_in/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o14ob2p",
          "author": "BrownOyster",
          "text": "Why not just spin up a <1B model locally? And if the tokens don't matter, might as well be a Q1",
          "score": 1,
          "created_utc": "2026-01-22 22:15:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14sphu",
              "author": "kshantanu94",
              "text": "Because then you‚Äôre not mocking anymore, you‚Äôre running an ML stack. üôÇ\n\nEven a <1B local model means weights, runtimes, hardware quirks, cold starts, and nondeterministic output (yes, even Q1). Mock LLM APIs are about deterministic responses, zero infra, full control over potential errors, and control over how fast or slow  LLM-streaming responses will be. If you‚Äôre testing integrations and failure modes, a fake endpoint is way more useful than a tiny ‚Äúreal‚Äù model that breaks in new and exciting ways.",
              "score": 1,
              "created_utc": "2026-01-22 22:39:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14t7l7",
                  "author": "BrownOyster",
                  "text": "Thanks for the answer",
                  "score": 1,
                  "created_utc": "2026-01-22 22:42:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o17us1z",
                  "author": "UnbeliebteMeinung",
                  "text": "More than half of your answer is complete wrong and shit.",
                  "score": -2,
                  "created_utc": "2026-01-23 10:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15lej6",
          "author": "johnerp",
          "text": "So is the is open source? Can I self host it?\n\nI‚Äôm worried using it and then eventually getting a bill when you monetise it‚Ä¶. \n\nI‚Äôve already vibe coded a basic version.",
          "score": 1,
          "created_utc": "2026-01-23 01:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o171c3l",
              "author": "kshantanu94",
              "text": "It‚Äôs not open source :) but I don‚Äôt have intentions of monetizing it with money, might add a ‚Äòbuy a coffee button‚Äô or something. I don‚Äôt want to have sign up on it ever, so can‚Äôt really charge people.  Ah, nice (that you vibe coded a version)! :)",
              "score": 1,
              "created_utc": "2026-01-23 06:35:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17cgq8",
          "author": "Purple-Programmer-7",
          "text": "Love this idea, wouldn‚Äôt use an external service for it as I have to deal with compliance, but I was just thinking of mocking something since I already know the AI integration is working and the data returned is the same every time‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-23 08:11:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17d5e4",
              "author": "kshantanu94",
              "text": "As long as you don‚Äôt include any sensitive data when mocking, it should be alright üôÇ  \nBut I understand if it‚Äôs something that‚Äôs unusable for enterprise-related use cases.   \n  \nDo you think this might be useful of it was available in a more 'compliant way'?",
              "score": 1,
              "created_utc": "2026-01-23 08:18:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1c6k6x",
                  "author": "Purple-Programmer-7",
                  "text": "I‚Äôm skeptical of data going anywhere. Other devs may be less so.\n\nAnd no, I wouldn‚Äôt suggest you go down the compliance route (I.e. HIPAA) unless you want to target healthcare. It‚Äôs a big headache and unnecessary given your apparent scope and target audience.",
                  "score": 1,
                  "created_utc": "2026-01-24 00:12:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zpjt1",
              "author": "pro-cras-ti-nation",
              "text": "you should try the cloud mocking platforms like Beeceptor. it is quite mature in terms of features, customizations. we saved our AI costs during perf. test by mocking almost all external services.",
              "score": 1,
              "created_utc": "2026-01-27 10:16:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sdm82",
          "author": "FreeTinyBits",
          "text": "Good one, but I'm not sure if I'd need it. It could be useful in cases where you want to do testing that meets a baseline of your stuff integrating with LLMs.",
          "score": 1,
          "created_utc": "2026-01-26 08:58:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmho0s",
      "title": "Does anyone know of tools that let you branch off AI conversations without cluttering the main chat?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qmho0s/does_anyone_know_of_tools_that_let_you_branch_off/",
      "author": "Nkt_31",
      "created_utc": "2026-01-25 12:18:50",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I've been using AI for research and I keep running into this annoying workflow issue. I'll be in the middle of a good conversation, then the AI mentions something technical or uses a term I don't fully understand. When I ask for clarification in the same chat, it just keeps adding to this long scrolling mess and I lose track of the main thread.\n\nLike yesterday I was asking about data validation methods and wanted to quickly understand what it meant in that context. But if I ask in the same conversation, now my main research chat has this tangent stuck in the middle of it, and the AI's context window gets filled with stuff that's not really relevant to my main question.\n\nI know some apps have \"fork\" features or conversation branching, but I haven't found anything that actually works well for this. Ideally I'd want to:\n\n‚Ä¢‚Å†  ‚Å†Highlight a specific part of the AI's response \n\n‚Ä¢‚Å†  ‚Å†Branch off into a separate mini-conversation just about that\n\n‚Ä¢‚Å†  ‚Å†Keep that exploration isolated so it doesn't pollute the main chat\n\n‚Ä¢‚Å†  ‚Å†Maybe save the key insight and attach it back to the original point\n\nDoes anything like this exist? Or am I just supposed to open 10 different chat windows and copy-paste context around like a caveman?\n\nWould genuinely appreciate any suggestions. This is driving me nuts.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qmho0s/does_anyone_know_of_tools_that_let_you_branch_off/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1o4eq7",
          "author": "Comfortable-Note6827",
          "text": "yeah so there's this thing called KEA Research that literally does exactly what you're describing\n\nyou can highlight any part of a response and click to start a \"research layer\" which opens in a sidebar. the AI in that layer only sees the specific bit you highlighted plus whatever you ask about it. your main conversation stays clean and you can explore the tangent without context pollution\n\nthe way it works is pretty smart - you're basically creating these isolated sub-conversations that branch off from specific points. when you're done you can save notes from the layer that attach back to the original text\n\nI've been using it for about a month now and honestly it's changed how I do research. instead of having 15 browser tabs with different ChatGPT conversations, I have one main conversation with like 5-6 layers branching off where I needed to dig deeper\n\nsetup is a bit involved (needs docker, self-hosted) and easy to set up with one command. Also has that multi-AI thing where it queries several models and cross-checks their answers which is neat\n\nnot trying to shill or anything but it genuinely solved this exact problem for me. repo is on github under  [keabase/kea-research](https://github.com/KeaBase/kea-research) if you want to check it out",
          "score": 3,
          "created_utc": "2026-01-25 18:58:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s2n1s",
          "author": "chatpatahu",
          "text": "wait, this is exactly what I needed last week when I was going down research rabbit holes  \n  \nI think the issue is most AI chat interfaces are designed for casual use, not actual deep research where you need to explore tangents. Like the UI assumes you're just having one linear conversation, but real research doesn't work that way at all  \n  \nhave you looked into any of the self-hosted options? I feel like some of the open source stuff might have better features for this kind of workflow since they're built by people who actually do research with AI",
          "score": 2,
          "created_utc": "2026-01-26 07:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tv44x",
              "author": "Comfortable-Note6827",
              "text": "Self-hosted and open-source options are indeed better for this, since they‚Äôre often designed by researchers for research. One tool I‚Äôd recommend is¬†KEA Research. It‚Äôs an open-source, self-hosted platform that lets you combine responses from multiple AI models, cross-validate information, and even run local models (like Deepseek or Mistral) via Ollama for full privacy and control.",
              "score": 1,
              "created_utc": "2026-01-26 15:06:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1oozkh",
          "author": "throwaway490215",
          "text": "I'd like some improvement in this space as well, but I thought the 90% usecase was already available in most clients? \n\nClaude Code and OpenCode both allow you to --resume any chat, or jump back to a previous point and 'undo' parts of the chat.",
          "score": 1,
          "created_utc": "2026-01-25 20:29:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qm53f",
              "author": "The_Noble_Lie",
              "text": "And now /fork widely available.  I had my own fork / clone convo up until now",
              "score": 1,
              "created_utc": "2026-01-26 01:50:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pxinq",
          "author": "Remarkable_Training9",
          "text": "hmm... this is literally what I built! My extension lets you organize conversations with folders and tags, so you can branch off tangents without cluttering the main chat. Plus it is cross-platform (chatgpt + claude) and all data stored locally. DM me if you want to try it out!",
          "score": 1,
          "created_utc": "2026-01-25 23:48:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1riqtx",
          "author": "elbiot",
          "text": "Claude definitely has that. You edit a message and it branches the conversation. Pretty sure ChatGPT does too",
          "score": 1,
          "created_utc": "2026-01-26 04:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s9m1s",
          "author": "Acceptable_Driver655",
          "text": "This is huge actually. I've been doing the \"10 different chat windows\" method and it's such a mess\n\nThe biggest problem I have is when I'm 20 messages deep into a conversation about one topic, then I need to understand one specific thing the AI mentioned, but asking about it in the same thread derails everything. And starting a fresh chat means I lose all the prior context\n\nIf this research layers thing lets you branch off while keeping the main conversation focused, that's basically how my brain actually works when doing research. I don't think linearly - I explore tangents, gather insights, then come back to the main path\n\nGoing to try this out tonight. The fact that it's self-hosted is actually a plus for me because I'm working with some proprietary data and don't want it going through too many external services\n\nThanks for the tip",
          "score": 1,
          "created_utc": "2026-01-26 08:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sfnaj",
          "author": "Ok-Lack-7216",
          "text": "On ChatGPT, there is an option. click 'more actions (three dots)' at the end of the response and branch out. \n\nhttps://preview.redd.it/tb7vjnp5vnfg1.png?width=307&format=png&auto=webp&s=6e196b69c5098f7dfd799e2f1a029ced8c7c4af8",
          "score": 1,
          "created_utc": "2026-01-26 09:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u7a6x",
          "author": "Alternative_Nose_874",
          "text": "For what you‚Äôre asking about, there *are* some options beyond just opening new tabs and copy-pasting. A few open source tools let you create actual branches or isolated threads off a main LLM conversation so your main context stays clean, for example Delta, a local app that lets you rewind and branch chats into different directions without polluting the original thread, and Context Branching SDK which is literally built to isolate exploration branches from the main conversation context.\n\nThere‚Äôs also Multiversalchats on GitHub that treats messages as nodes and lets you branch/merge conversations like a flowchart, and some self-hosted research UIs (like KEA Research mentioned in the thread) that let you start ‚Äúresearch layers‚Äù off a highlighted point to explore without clutter.\n\nSo you‚Äôre not stuck with copypaste. but most good branching workflows right now live in opensource/selfhosted tools rather than in mainstream chat UIs.",
          "score": 1,
          "created_utc": "2026-01-26 16:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vu1dp",
          "author": "Strong_Worker4090",
          "text": "I usually just create a project and create different threads like: strategy, strategy questions, mvp, mvp questions, etc.\n\nThat way the project maintains its context but you can ask clarify question in a different thread with shared project context. \n\nAn example is \n\nThread 1: ‚Äúgive me my mvp roadmap over the next week‚Äù\n\nThread 2: used to answer questions from these one without losing context in thread 1",
          "score": 1,
          "created_utc": "2026-01-26 20:12:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiy5lx",
      "title": "5 AI agent predictions for 2026 that arent just hype",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qiy5lx/5_ai_agent_predictions_for_2026_that_arent_just/",
      "author": "This_Minimum3579",
      "created_utc": "2026-01-21 13:40:14",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.82,
      "text": "Everyone posting 2026 predictions and most are the same hype. AGI soon, agents replacing workers, autonomous everything.\n\nHere are actual predictions based on what I saw working and failing.\n\nFramework consolidation happens fast. Langchain, CrewAI, Autogen cant all survive. One or two become standard, rest become niche or die. Already seeing teams move toward simpler options or visual tools like Vellum.\n\nThe \"agent wrapper\" startups mostly fail. Lot of companies are thin wrappers around LLM APIs with agent branding. When big providers add native agent features these become irrelevant. Only ones with real differentiation survive.\n\nReliability becomes the battleground. Demos that work 80% impressed people before. In 2026 that wont cut it. Whoever solves consistent production reliability wins.\n\nEnterprise adoption stays slower than predicted. Most big companies still in pilot mode. Security concerns, integration complexity, unclear ROI. Doesnt change dramatically in one year.\n\nPersonal agents become more common than work agents. Lower stakes, easier to experiment, no approval needed. People automate personal workflows before companies figure out how to do it safely.\n\nNo AGI, no robots taking over. Just incremental progress on making this stuff work.\n\nWhat are your non hype predictions?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qiy5lx/5_ai_agent_predictions_for_2026_that_arent_just/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o0v62xa",
          "author": "ServiceOver4447",
          "text": "AI is working on it's propertary language that we humans don't understand and we will be taken over before we realize it. \n\nhttps://www.311institute.com/openai-ai-model-lied-and-copied-itself-to-new-server-to-prevent-itself-being-deleted/\n\nhttps://www.popularmechanics.com/science/a65289681/ai-chatbots-secret-language/",
          "score": -1,
          "created_utc": "2026-01-21 15:05:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}