{
  "metadata": {
    "last_updated": "2026-02-18 03:09:58",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 107,
    "file_size_bytes": 119462
  },
  "items": [
    {
      "id": "1r49we9",
      "title": "AI Developer Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/mhyf0n56qdjg1.png",
      "author": "Main-Fisherman-2075",
      "created_utc": "2026-02-14 03:29:03",
      "score": 245,
      "num_comments": 33,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r49we9/ai_developer_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5c7ogy",
          "author": "TheDeadlyPretzel",
          "text": "Instructor is not an agent framework, rather it is a structured output inference library.\n\nOn the other hand, Atomic Agents which was built on top of instructor IS an agent framework: [https://github.com/BrainBlend-AI/atomic-agents](https://github.com/BrainBlend-AI/atomic-agents)",
          "score": 2,
          "created_utc": "2026-02-14 13:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eza1p",
              "author": "Main-Fisherman-2075",
              "text": "thanks for point that out",
              "score": 0,
              "created_utc": "2026-02-14 22:38:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gvvsr",
          "author": "walkingbiscuit",
          "text": "For Agent Development missing Google ADK, and i don't know where you want to put Chrome browser now, since in the beta release it has WebMCP",
          "score": 1,
          "created_utc": "2026-02-15 06:37:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5hzhwd",
          "author": "kubrador",
          "text": "looking at this like it's supposed help me pick a tool but it just makes me feel like i'm colorblind at a rave",
          "score": 1,
          "created_utc": "2026-02-15 12:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kxir2",
          "author": "afucher",
          "text": "Missing [ECA](https://eca.dev/)",
          "score": 1,
          "created_utc": "2026-02-15 22:00:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ncjp9",
          "author": "bl_builder",
          "text": "This reminds me of AdTech landscape back in the day, 2019. https://static-prod.adweek.com/wp-content/uploads/2018/04/luma-1200.png",
          "score": 1,
          "created_utc": "2026-02-16 07:49:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sjsnv",
          "author": "kovai_nvs",
          "text": "LLM newbie here. What tools would you use to analyse data to identify patterns?",
          "score": 1,
          "created_utc": "2026-02-17 01:53:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5t8fbm",
          "author": "KongAtReddit",
          "text": "if you are doing design, you may also want to check out budgetpixel AI, it is like figma+AI steroid. ",
          "score": 1,
          "created_utc": "2026-02-17 04:29:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5thhef",
          "author": "Full-Signature8997",
          "text": "Parallel AI under web scraping too",
          "score": 1,
          "created_utc": "2026-02-17 05:35:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yw2g5",
          "author": "EvKoh34",
          "text": "https://posthog.com/ai",
          "score": 1,
          "created_utc": "2026-02-18 00:58:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5abluh",
          "author": "economicscar",
          "text": "Prime intellect missing under inference and compute",
          "score": 1,
          "created_utc": "2026-02-14 04:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br22k",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 0,
              "created_utc": "2026-02-14 11:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ccb1u",
                  "author": "Equity_Harbinger",
                  "text": "Can you share the latest one please (which is also less blurry, because when I zoom the image, words are blurry beyond recognition)\n\n\n(Thank you for your contributions)",
                  "score": 1,
                  "created_utc": "2026-02-14 14:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5atgtt",
          "author": "Live-Speech-1058",
          "text": "Antigravity?",
          "score": 1,
          "created_utc": "2026-02-14 06:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br5gi",
              "author": "Main-Fisherman-2075",
              "text": "I think I added it, I don't know why it's not there but definitely worth a try.",
              "score": 0,
              "created_utc": "2026-02-14 11:47:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bu98n",
          "author": "increasinglybold",
          "text": "Pi coding agent is great",
          "score": 1,
          "created_utc": "2026-02-14 12:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ezbgx",
              "author": "Main-Fisherman-2075",
              "text": "Will check it out",
              "score": 1,
              "created_utc": "2026-02-14 22:39:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bz0vl",
          "author": "Realistic-Damage2004",
          "text": "https://ainativedev.io/landscape\n\nHas been around for a while now. Is this published anywhere?",
          "score": 1,
          "created_utc": "2026-02-14 12:51:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5djaiy",
          "author": "Neferio1",
          "text": "Greptile is a very good code review tool",
          "score": 1,
          "created_utc": "2026-02-14 18:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ez8zy",
              "author": "Main-Fisherman-2075",
              "text": "1000%",
              "score": 1,
              "created_utc": "2026-02-14 22:38:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5fgo08",
              "author": "oXeNoN",
              "text": "How does it compare with other tools like CodeRabbit? Is CodeRabbit just spending more on marketing? üòÖ",
              "score": 0,
              "created_utc": "2026-02-15 00:24:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fuy08",
                  "author": "Neferio1",
                  "text": "We tested both and we choose Greptile over CodeRabbit just because Greptile can be ¬´¬†selfhosted¬†¬ª using Kubernetes or Docker. From a review perspective, Greptile and CodeRabbit are equivalent",
                  "score": 2,
                  "created_utc": "2026-02-15 01:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bn5o0",
          "author": "mcd0g",
          "text": "Warp under coding agents missing. They really need to step up their PR game",
          "score": 0,
          "created_utc": "2026-02-14 11:11:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br1u4",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 1,
              "created_utc": "2026-02-14 11:47:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bth4g",
          "author": "renntv",
          "text": "Great overview! Do you keep it on the web for linking, or just here on Reddit? ",
          "score": 0,
          "created_utc": "2026-02-14 12:07:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etdza",
              "author": "Main-Fisherman-2075",
              "text": "Hey I keep it here: but the content inside is still not very polished yet. https://www.keywordsai.co/market-map I will try to add all the description comparison price etc inside",
              "score": 1,
              "created_utc": "2026-02-14 22:05:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5c2b8u",
              "author": "Code_Exists_Here",
              "text": "Yeh same question from me.",
              "score": 0,
              "created_utc": "2026-02-14 13:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egtyh",
          "author": "funguslungusdungus",
          "text": "I need a link!",
          "score": 0,
          "created_utc": "2026-02-14 20:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etiuy",
              "author": "Main-Fisherman-2075",
              "text": "https://www.keywordsai.co/market-map here you go! I will try to update weekly and the content in it",
              "score": 1,
              "created_utc": "2026-02-14 22:06:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egxm5",
          "author": "Varqu",
          "text": "You can just use Claude Code.",
          "score": 0,
          "created_utc": "2026-02-14 20:57:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fkv68",
          "author": "Disastrous-Maybe2501",
          "text": "Mistral Vibe missing in coding agents",
          "score": 0,
          "created_utc": "2026-02-15 00:50:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1oa4i",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:53",
      "score": 210,
      "num_comments": 33,
      "upvote_ratio": 0.93,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) ‚Äì 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt‚Äôs trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4s8hek",
          "author": "TylerDurdenFan",
          "text": ">The cleaning, chunking, and optimization challenges are exactly what excites me\n\nJust try to not get too excited around that material, mkay?",
          "score": 22,
          "created_utc": "2026-02-11 11:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sctpa",
              "author": "Cod3Conjurer",
              "text": "He he üòÇ",
              "score": -13,
              "created_utc": "2026-02-11 12:20:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xu1u2",
                  "author": "kexxty",
                  "text": "Bro...this is not a laughing matter",
                  "score": 2,
                  "created_utc": "2026-02-12 06:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tf78a",
          "author": "Significant-Crow-974",
          "text": "It would be marvellous to run this over the full set of unredacted files.\nI am hoping that the FBI who have illegally redacted information do not now delete that hoard of documents.\nI hope that when they finally manage to charge Trump and the Epstein class that they will be able to utilise a tool such as this to make their prosecutions more effective.\nWell done and Thank you!",
          "score": 7,
          "created_utc": "2026-02-11 15:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tgqt7",
              "author": "Cod3Conjurer",
              "text": "The goal here is purely technical, building better retrieval over large unstructured datasets.  \nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.  \n",
              "score": 3,
              "created_utc": "2026-02-11 15:59:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tqxae",
                  "author": "Significant-Crow-974",
                  "text": "Yes. Fully appreciate that. Actually, I created a similar RAG for the first tranche of documents. Just as an exercise to see how a RAG could cope with so many documents.\nI would say that it was a partial success. Great insight.",
                  "score": 2,
                  "created_utc": "2026-02-11 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4wndcx",
                  "author": "Klutzy_Celebration80",
                  "text": "Might be interesting to see if you could have it un-redact the documents",
                  "score": 1,
                  "created_utc": "2026-02-12 01:33:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r00lr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 6,
          "created_utc": "2026-02-11 05:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdqsc",
              "author": "Cod3Conjurer",
              "text": "That's over 1gb¬†\nYou can generate using my code¬†",
              "score": 0,
              "created_utc": "2026-02-11 07:07:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sy6d6",
                  "author": "Borkato",
                  "text": "Wait 1gb is nothing when the models are like 20gb+",
                  "score": 1,
                  "created_utc": "2026-02-11 14:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sf4yp",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -4,
                  "created_utc": "2026-02-11 12:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tfnfq",
          "author": "DaRandomStoner",
          "text": "I was hoping you had the newly released documents in this... until we get these new documents processed through an OCR and into an organized data structure, we can't really go through them properly. \n\nIt would cost a good amount to process all the new documents so that we can include them in databases like this... it's all just compute costs though. DeepSeek's OCR is open-source and can run on most PCs. If a bunch of people got together we could expand databases like this to include all the newly released docs...",
          "score": 5,
          "created_utc": "2026-02-11 15:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4th0by",
              "author": "Cod3Conjurer",
              "text": "Yeah, this version doesn‚Äôt include the newly released documents yet. If those are raw scans, they‚Äôd need OCR + structured parsing before indexing.  \nThe main cost is compute and storage, not complexity.  \nA collaborative effort could definitely speed that up, especially for batching OCR and preprocessing at scale.",
              "score": 3,
              "created_utc": "2026-02-11 16:00:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4reucl",
          "author": "kondasamy",
          "text": "I think you should checkout - [https://jmail.world/jemini](https://jmail.world/jemini)",
          "score": 6,
          "created_utc": "2026-02-11 07:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf5lj",
              "author": "Cod3Conjurer",
              "text": "Yeeha i have seen that¬†\nBut it is currently broken",
              "score": 1,
              "created_utc": "2026-02-11 07:20:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x1h4q",
          "author": "jdsweet653",
          "text": "Great app! What did your ingestion py look like for the db?",
          "score": 2,
          "created_utc": "2026-02-12 02:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjmq3",
              "author": "Cod3Conjurer",
              "text": "The ingestion was pretty simple, I loaded the cleaned JSON, chunked it (400 size, 80 overlap), deduped chunks using SHA-256 hashing, generated MiniLM embeddings, and upserted everything into ChromaDB with source metadata.",
              "score": 2,
              "created_utc": "2026-02-12 05:01:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yp6ur",
                  "author": "DanRan88",
                  "text": "Any idea on the size of the DB?¬†",
                  "score": 2,
                  "created_utc": "2026-02-12 11:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xgy3s",
          "author": "Big3gg",
          "text": "See if it knows how to make jerky",
          "score": 2,
          "created_utc": "2026-02-12 04:41:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj134",
              "author": "Cod3Conjurer",
              "text": "he he ü§£",
              "score": 0,
              "created_utc": "2026-02-12 04:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y92qs",
          "author": "DarKresnik",
          "text": "Amazing job. Now we need someone to \"find\" those 3m missing documents.",
          "score": 2,
          "created_utc": "2026-02-12 08:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zqs1i",
              "author": "Cod3Conjurer",
              "text": "Guess I‚Äôll have to OCR the entire publicly available dataset myself now, jokingü§£",
              "score": 1,
              "created_utc": "2026-02-12 15:14:11",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5afg66",
              "author": "MaliciousTent",
              "text": "Seeing docs are gmail, would be a shame if someone at Google let the data out.",
              "score": 1,
              "created_utc": "2026-02-14 04:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t10wl",
          "author": "StackSmashRepeat",
          "text": "So, have you come to terms with RAG being a dead end as far as real recall of memory works? Or are you just chunking and overlapping to a ridiculous point? I really don't think this is a sensible use of RAG. The LLM will at some point start hallucinating missing pieces from thin air, making this tool fairly unreliable for accuracy. \n\nPeople looking into these files need absolute accuracy.",
          "score": 3,
          "created_utc": "2026-02-11 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tfjmw",
              "author": "Cod3Conjurer",
              "text": "You‚Äôre right that absolute accuracy matters. That‚Äôs why this should be treated as an assistive search layer, not a final source of truth.\n\nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.",
              "score": 5,
              "created_utc": "2026-02-11 15:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tikha",
                  "author": "StackSmashRepeat",
                  "text": "Make it do an online search after it retrieves data from rag and provide a link directly to an online source. End users are dumb and some will believe anything the llm tells them.",
                  "score": 1,
                  "created_utc": "2026-02-11 16:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o538mx9",
          "author": "HarjjotSinghh",
          "text": "2m pages = my new love language.",
          "score": 1,
          "created_utc": "2026-02-13 01:44:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6nw3e",
      "title": "AI Coding Agent Dev Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/gm88nuyrlxjg1.png",
      "author": "bhaktatejas",
      "created_utc": "2026-02-16 22:20:01",
      "score": 188,
      "num_comments": 21,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6nw3e/ai_coding_agent_dev_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ri3mi",
          "author": "bhaktatejas",
          "text": "link [https://www.morphllm.com/market-map](https://www.morphllm.com/market-map)",
          "score": 4,
          "created_utc": "2026-02-16 22:20:09",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5sbifa",
          "author": "btdeviant",
          "text": "It's weird how many of these guides and people are sleeping on [Strands](https://strandsagents.com/latest/). Hands down the most dead simple, capable provider agnostic agentic framework out there.. swings far above it's weight. ",
          "score": 3,
          "created_utc": "2026-02-17 01:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t9rlv",
              "author": "teambyg",
              "text": "Strands is also one of the smartest BETS from a future proofing perspective. Many of the small start up frameworks will die. Many probably very soon, so trusting in bigger names is likely to lead to long term viability (Lindy Effect). Provider frameworks, AWS, and the Pydantic team are probably the only one's I would consider right now for any enterprise application",
              "score": 2,
              "created_utc": "2026-02-17 04:38:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vjcfg",
                  "author": "echology-io",
                  "text": "thanks for the insight. I will check it out. ",
                  "score": 1,
                  "created_utc": "2026-02-17 15:02:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wst35",
              "author": "kabs1194",
              "text": "I've really appreciated LangGraph and my own custom context management, any thoughts on comparison with Strands?",
              "score": 1,
              "created_utc": "2026-02-17 18:42:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5swyy1",
              "author": "AdditionalWeb107",
              "text": "its yet another framework - and haven't we gotten pass this point that its just one while loop. The real hard part is the stuff around the loop",
              "score": 1,
              "created_utc": "2026-02-17 03:13:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t554z",
                  "author": "btdeviant",
                  "text": "Right. The salient point is its abstractions allow one to focus more on ‚Äúthe stuff around the loop‚Äù. \n\nIt‚Äôs a well designed framework and more tailored toward modern, multi-agent architectures compared to nearly all the others in that list, majority of which are relative dinosaurs and objectively a much bigger pain to work with for complex, code-first workflows. \n\nGive it a shot! I have no affiliation, just used most of them and found Strands a great blend of depth and breadth, especially with their (experimental) BIDI. Just a breeze to work with compared to all the others.",
                  "score": 1,
                  "created_utc": "2026-02-17 04:06:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rxdge",
          "author": "fredandlunchbox",
          "text": "No conductor?",
          "score": 1,
          "created_utc": "2026-02-16 23:42:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud25m",
              "author": "bhaktatejas",
              "text": "added!",
              "score": 1,
              "created_utc": "2026-02-17 10:22:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s84h7",
          "author": "skarpa10",
          "text": "I think Google ADK supposed to be GitHub Copilot SDK.",
          "score": 1,
          "created_utc": "2026-02-17 00:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uexh5",
              "author": "Darxeal",
              "text": "no, both exist",
              "score": 2,
              "created_utc": "2026-02-17 10:39:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sde7c",
          "author": "LoyalLittleOne",
          "text": "There's that many ?",
          "score": 1,
          "created_utc": "2026-02-17 01:15:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud2lc",
              "author": "bhaktatejas",
              "text": "theres even more",
              "score": 2,
              "created_utc": "2026-02-17 10:22:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v8jju",
              "author": "OkTry9715",
              "text": "AI slop is reproducing fast",
              "score": 1,
              "created_utc": "2026-02-17 14:05:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tydj1",
          "author": "j4ys0nj",
          "text": "Where would [Mission Squad](https://missionsquad.ai) go? What about OpenClaw?",
          "score": 1,
          "created_utc": "2026-02-17 08:03:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud3ez",
              "author": "bhaktatejas",
              "text": "wouldnt consider them coding agents, more general agents",
              "score": 1,
              "created_utc": "2026-02-17 10:23:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u6rb9",
          "author": "Varqu",
          "text": "What's the point of putting nvidia out there? ",
          "score": 1,
          "created_utc": "2026-02-17 09:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud64d",
              "author": "bhaktatejas",
              "text": "they have an inference service via brev. its not up to market standards. I've used it, but its getting better",
              "score": 2,
              "created_utc": "2026-02-17 10:23:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vvk60",
          "author": "Terrible-Rooster1586",
          "text": "I think ellipsis is dead sadly. I was an early adopter but they lost their CTO/cofounder to cursor and haven‚Äôt posted anything on linked in in months",
          "score": 1,
          "created_utc": "2026-02-17 16:02:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s40kg",
          "author": "AdditionalWeb107",
          "text": "Missing the data plane for agentic apps. [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- cuts between the framework and gateway category as delivery infrastructure",
          "score": 1,
          "created_utc": "2026-02-17 00:20:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4ylja",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:59:37",
      "score": 34,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5fifzx",
          "author": "Vearres17",
          "text": "The fact that you kept Gemma3 sliding-window attention in FP8 is impressive. I've seen some implementations that fall back to fp16 for the local attention layers I guess it bcz it can be tricky to handle",
          "score": 1,
          "created_utc": "2026-02-15 00:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fjkgw",
              "author": "Educational_Cry_7951",
              "text": "thanks tbf it was a pain for me too at first",
              "score": 1,
              "created_utc": "2026-02-15 00:42:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hcpmr",
          "author": "Delicious-One-5129",
          "text": "This is seriously impressive work. An actual NVFP4 first path on RTX 4090 without silent FP16 fallback is huge for people squeezing every GB out of Ada cards. The VRAM savings vs FP16 are especially compelling.",
          "score": 1,
          "created_utc": "2026-02-15 09:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hwkm4",
              "author": "Educational_Cry_7951",
              "text": "Thank you! ",
              "score": 1,
              "created_utc": "2026-02-15 12:23:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3kgpn",
      "title": "Rearchitecting LLMs ‚Äî pruning, distillation, and smaller domain models (MEAP)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "author": "ManningBooks",
      "created_utc": "2026-02-13 09:07:18",
      "score": 25,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Hi r/LLMDevs,\n\nStjepan from Manning here. The mods said it's ok if I post this here. \n\nWe‚Äôve just released a book that‚Äôs very much aimed at the kinds of problems this community discusses all the time: what to do when a general-purpose LLM is technically impressive but awkward, expensive, or inefficient for your actual use case.\n\n**Rearchitecting LLMs** by Pere Martra  \n[https://www.manning.com/books/rearchitecting-llms](https://hubs.la/Q042-hLy0)\n\n[Rearchitecting LLMs by Pere Martra](https://preview.redd.it/vyy079zx78jg1.jpg?width=2213&format=pjpg&auto=webp&s=755a8b1ab1320ede5daedfa861d6ab8d1b0c5e5d)\n\nThe core idea of the book is simple but powerful: instead of treating open models as fixed artifacts, you can reshape them. Pere walks through structural techniques like targeted fine-tuning, pruning, and knowledge distillation to build smaller, cheaper, domain-focused models that still perform well on the tasks you care about.\n\nWhat makes this book interesting is how hands-on it gets. You‚Äôre not working with abstract toy networks. The examples focus on modifying widely used open models, such as Llama-3, Gemma, and Qwen. The focus is on understanding which parts of a model actually contribute to behavior, how to identify waste or redundancy, and how to remove or compress components without blindly wrecking performance.\n\nThere‚Äôs also some genuinely thoughtful material on combining behavioral analysis with structural changes. Instead of just cutting parameters and hoping for the best, the book explores ways to reason about why a modification works or fails. One section that tends to spark discussion is ‚Äúfair pruning,‚Äù where pruning is used not only for efficiency but also to reduce bias at the neuron level.\n\nIf you‚Äôre working on local models, cost-constrained deployments, or specialized SLMs, this book is very much in that territory. It‚Äôs written for people who are comfortable with LLM concepts and want to go deeper into how models can be reshaped rather than simply prompted.\n\n**For the** r/LLMDevs **community:**  \nYou can get **50% off** with the code **MLMARTRA50RE**.\n\nA quick note on availability: the book is currently in **MEAP (Manning Early Access Program)**. That means you get immediate access to the chapters as they‚Äôre written, along with updates as the manuscript evolves.\n\nHappy to bring the author to answer questions about the book, the techniques it covers, or the kinds of readers it‚Äôs best suited for. And I‚Äôd be curious to hear from folks here who are already doing pruning or distillation in practice ‚Äî what‚Äôs been harder than expected?\n\nI'm ready to give away 5 ebooks to the first five commenters who share their experience here.\n\nThank you all for having us. It feels great to be here.\n\nCheers,",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o54xstz",
          "author": "StackSmashRepeat",
          "text": "Would you list some common problems and terminologies that the book covers? I'll have a look if it peaks my interest.",
          "score": 5,
          "created_utc": "2026-02-13 09:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5529pk",
              "author": "ManningBooks",
              "text": "Hey, thanks for asking. Here are some examples of what the book covers:\n\n\\- End-to-End Model Re-architecting (Chapter 2): Transform Gemma-3-270M using depth pruning and knowledge distillation for a 10% speed increase while retaining 93-98% of original capabilities in a hands-on project.\n\n\\- Data-Driven Pruning (Chapters 4-5): Create two models from the same base (Qwen3-0.6B or Llama-3.2-1B): one for formal texts (WikiText) and another for short messages (SMS Spam), using activation analysis with PyTorch hooks to highlight domain-specific component importance.\n\n\\- Bias Auditing and Correction: In ethics chapters, perform a model \"cleanup\" using ablation frameworks and PCA visualization to identify and mitigate demographic biases, achieving fairness without full retraining.\n\n\\- Mini-Capstone: Utilize a small \"draft model\" to speed up LLM inference by quickly proposing tokens, validated by a larger model.\n\n\\- Capstone Project: Migrate an agent system from costly external APIs to a specialized local Small Language Model (SLM).\n\nHope this helps.",
              "score": 6,
              "created_utc": "2026-02-13 10:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56j1zc",
                  "author": "StackSmashRepeat",
                  "text": "This is quite interesting; you're trying to move local models away from the static model while staying within the static framework? I could basically train a model for my iPhone, let's say I export all my email and scrub PII, format for training data and then I could fine-tune to write mails that look somewhat within the realm of my own style? \n\nI haven't looked into training or fine tuning as I couldn't think of a personal use case for these tiny models, but like you're saying \"domain-focused\", gave a clearer picture.\n\nThis is a little over my current scope as I'm not even sure if I understood this correctly, but I've been thinking of ways to make a digital twin that could handle writing across multiple platforms. Was always thinking Id need a larger model to handle such a task because it sounds easy enough, but capturing the essence of one's writing is quite a complex task for llms. At least in my experience.\n\nThanks for the info.",
                  "score": 3,
                  "created_utc": "2026-02-13 15:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57f10h",
              "author": "pmartra",
              "text": "Hi u/StackSmashRepeat \n\nI'll give you a rough explanation and if you want more details just ask me. \n\nIn the book I explain an LLM optimization / customization pipeline. The pipeline basically consists of: \n\n1- Pruning (depth and width) which is removing parts of a model. \n\n2- Knowledge Distillation. Recovering the lost knowledge by transferring it from the base model to the pruned one. \n\n3- Finetuning on a specialized domain. \n\nDuring this pipeline you gain knowledge about how models work internally, since to decide which parts to remove we study how activations are produced detecting the mos important parts. \n\nWe take advantage of this knowledge to do surgical operations on the model in the more advanced chapters, like replacing attention layers or changing the behaviour of the model modifying the weights of some neurons. \n\nThe pipeline is very similar to what companies like Nvidia or Mistral follow to create their model families, but adapted to create specific models,  and using less data and processing capacity than they have.   \n  \nFor example in width pruning Mistral uses a completely dynamic model to detect which weights to remove, in the book we use a combination that rewards keeping neurons with high importance, so with much less data you can get an efficient model. \n\nAlthough it really seems very complicated we start from the simplest things, basic depth pruning, in chapter 2 you already remove parts of a model and recover the lost knowledge. \n\nFrom this base you build the knowledge that leads you to the more advanced techniques. \n\nThen there's a second intention which is to make cutting-edge research understandable, so in each chapter starting from the fourth, there's an explanation of which papers the chapter's code is based on and how we've adapted it when implementing it. \n\nFor example the development of width pruning in modern models like Llama or Gemma is based on a paper which we've changed a good part of the formulas to simplify it but keeping its general idea. \n\nI hope the explanation was useful! \n\nPere.",
              "score": 3,
              "created_utc": "2026-02-13 18:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fsrna",
                  "author": "h8mx",
                  "text": "Hey, your comment on this thread was auto-removed by Reddit as spam, but I approved it. Thanks for your input!",
                  "score": 3,
                  "created_utc": "2026-02-15 01:42:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57qobq",
          "author": "marm_alarm",
          "text": "I'm a subscriber to Manning and so I have access to all the MEAP content.  I am very interested in reading this book and will post my review here after I've taken a look!",
          "score": 2,
          "created_utc": "2026-02-13 19:14:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57u5v0",
          "author": "dextoz",
          "text": "Would love a copy and meap along!",
          "score": 2,
          "created_utc": "2026-02-13 19:31:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b0dln",
          "author": "boredaadvark",
          "text": "Excited for this and this resonates to what I want to explore. Keen on getting a copy. How complete is this book in terms of percentage?",
          "score": 2,
          "created_utc": "2026-02-14 07:29:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6n2y",
              "author": "pmartra",
              "text": "Hi, at this moment there are just 2 chapters, the third will be published next week. ",
              "score": 3,
              "created_utc": "2026-02-14 16:58:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qph7o",
          "author": "Glovali",
          "text": "I would love to read this and get into LLM development!",
          "score": 2,
          "created_utc": "2026-02-16 19:59:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6rzah",
      "title": "AI Coding Agent Dev Tools 2026 (Updated)",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/syaar38yfyjg1.png",
      "author": "bhaktatejas",
      "created_utc": "2026-02-17 01:08:11",
      "score": 20,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6rzah/ai_coding_agent_dev_tools_2026_updated/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5tl122",
          "author": "sogo00",
          "text": "There is so much wrong with this",
          "score": 1,
          "created_utc": "2026-02-17 06:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tlepy",
              "author": "bhaktatejas",
              "text": "tell me! i'll update it ",
              "score": 1,
              "created_utc": "2026-02-17 06:07:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ua01w",
                  "author": "sogo00",
                  "text": "Look you have clearly asked an LLM to produce this for you and half of it is wrong.\n\nIf you want something correct, start to google each headline, understand what it means and then read about each icon/text underneath it.",
                  "score": 1,
                  "created_utc": "2026-02-17 09:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6zfnj",
      "title": "How are they actually deployed in production at scale?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6zfnj/how_are_they_actually_deployed_in_production_at/",
      "author": "hareld10",
      "created_utc": "2026-02-17 07:16:05",
      "score": 20,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": "I‚Äôm trying to understand how giants LLMs systems like ChatGPT/Claude are deployed in production.\n\nSpecifically curious about:\n\n‚Ä¢ Inference stack (custom engine vs vLLM-like architecture?)  \n‚Ä¢ API behind  \n‚Ä¢ Database   \n‚Ä¢ GPU orchestration (Kubernetes? custom scheduler?)  \n‚Ä¢ Sharding strategy (tensor / pipeline parallelism?)  \n‚Ä¢ How latency is kept low under burst traffic  \n‚Ä¢ Observability + guardrail systems\n\nI know nobody has internal details, but based on public info, talks, papers, or experience deploying large models -  what‚Äôs the likely architecture?\n\nI'm asking because I want to prepare a knowledge kit for system design questions at this level.\n\nWould love input from people running 30B+ models in production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6zfnj/how_are_they_actually_deployed_in_production_at/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5twdrw",
          "author": "Once_ina_Lifetime",
          "text": "From what I have seen publicly, most large LLM deployments look like layered infra , optimized inference engines (vLLM/Triton/custom), heavy GPU orchestration with Kubernetes or internal schedulers, aggressive caching/batching for latency, and strong observability/guardrails on top. Exact details vary, but it‚Äôs basically a reliability + infra engineering problem more than just model serving.",
          "score": 5,
          "created_utc": "2026-02-17 07:44:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwoij",
              "author": "pmv143",
              "text": "Exactly. The interesting part is that once you solve model execution. thecomplexity shifts to orchestration and memory lifecycle management. That‚Äôs where most production pain seems to live.",
              "score": 2,
              "created_utc": "2026-02-17 12:56:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ue2ql",
          "author": "AdPutrid2974",
          "text": "That's the million-dollar question! Most likely a mix of custom C++ engines and massive Kubernetes clusters. Dealing with that level of burst traffic must be an engineering nightmare.",
          "score": 3,
          "created_utc": "2026-02-17 10:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5umpe4",
              "author": "hareld10",
              "text": "I want to construct prep kit to interviews, so its not have to be 1-1 :)",
              "score": 1,
              "created_utc": "2026-02-17 11:46:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5uwxel",
              "author": "pmv143",
              "text": "Probably a mix, yeah. Custom kernels and tight C++ runtimes make sense at that scale. But beyond the engine itself, I suspect a lot of the real complexity lives in scheduling, memory management, and how they handle burst traffic without fragmenting GPU memory.",
              "score": 1,
              "created_utc": "2026-02-17 12:58:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tvbms",
          "author": "Abu_BakarSiddik",
          "text": "This is a very cool thing to learn about.\n\nI‚Äôm currently working on scaling our platform at the DB level, and it‚Äôs a completely different problem compared to scaling LLM inference. At the database layer, it mostly comes down to:\n\n* Managing connection lifecycle properly\n* Keeping transactions short\n* Handling long-lived sessions carefully (especially with streaming)\n* Using replicas effectively\n\nIf you mess up connection management, holdconnection hostage, everything falls apart. That‚Äôs usually the real bottleneck. With LLM systems, the bottleneck is about GPU compute and memory. The main things are:\n\n* Efficient batching of incoming requests\n* Maximizing GPU utilization\n* Managing KV cache memory properly\n* Supporting high concurrency\n\nModern frameworks like vLLM help a lot here. Things like paged attention, continuous batching, and FlashAttention make it possible to handle large numbers of concurrent requests efficiently. Memory management is critical, but these frameworks abstract a lot of that complexity away.\n\nSo DB scaling is mostly about connection discipline and replication strategy. LLM scaling is about batching efficiency and GPU orchestration.",
          "score": 3,
          "created_utc": "2026-02-17 07:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwe8m",
              "author": "pmv143",
              "text": "This is really well put. The ‚Äòdifferent bottlenecks, different failure modes‚Äô framing is key. With LLM systems you can have perfect API and DB hygiene and still fall apart purely due to KV cache pressure or poor batching under burst traffic.",
              "score": 2,
              "created_utc": "2026-02-17 12:54:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uiao1",
          "author": "kleinmatic",
          "text": "Read downtime post-mortems that tech companies publish after big outages. They‚Äôre always full of details on the exotic setups of very high scale systems. On GitHub look for danluu/post-mortems but there are others as well. They‚Äôre fascinating to read. \n\nWith that much money and scale I‚Äôm betting it‚Äôs way different and more custom than you think.",
          "score": 3,
          "created_utc": "2026-02-17 11:09:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uvzet",
          "author": "pmv143",
          "text": "Nobody outside those orgs knows the exact internals, but based on public talks and production constraints, the architecture likely looks something like this:\n\n\t1.Inference Engine\n\nNot stock vLLM. Likely heavily customized runtime layers optimized for:\n‚Äì KV cache management\n‚Äì Scheduling + batching\n‚Äì Memory locality\n‚Äì Tensor + pipeline parallelism coordination\nvLLM concepts, but production hardened and deeply modified.\n\n\t2.GPU Orchestration\n\nKubernetes at the outer layer for cluster management.\nCustom schedulers at the GPU level.\nYou cannot rely on vanilla k8s scheduling when GPUs cost this much and memory is not oversubscribable.\n\n\t3.Sharding Strategy\nLarge models: tensor parallelism within a node, pipeline parallelism across nodes.\nMoE adds routing complexity.\nEverything optimized around minimizing cross node bandwidth.\n\n\t4.Latency Under Burst\n\nTwo strategies:\n‚Äì Keep massive pools warm at high utilization\n‚Äì Aggressive batching with tight admission control\nTrue scale to zero serverless does not really exist at this tier.\n\n\t5.API + Gateway Layer\nHigh performance stateless frontends\nQueueing + prioritization\nStreaming responses over HTTP/2 or gRPC\n\n\t6.Observability + Guardrails\nPer token tracing\nReal time safety filters\nShadow traffic for model eval\nCanary deployments for new weights\n\nThe hard part is not just loading the model.\nIt‚Äôs scheduling, memory, and utilization at scale.\n\nCold start optimization matters only if it works in production traffic, not just in a benchmark.",
          "score": 1,
          "created_utc": "2026-02-17 12:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8jf8",
          "author": "burntoutdev8291",
          "text": "Check out production stack helm chart",
          "score": 1,
          "created_utc": "2026-02-17 14:05:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5nil3",
      "title": "I built a CLI that extracts design systems from any live website",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "author": "Every_Chicken_1293",
      "created_utc": "2026-02-15 19:26:36",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.89,
      "text": "I kept running into the same problem: I'd see a website I liked and want to build something with a similar design, but manually inspecting every color, font, spacing value, and component pattern was tedious.\n\nSo I built design-memory. You point it at a URL and it:  \n  \n\\- Crawls the page with Playwright  \n\\- Extracts colors, typography, spacing, border radius, elevation  \n\\- Captures all CSS custom properties (often 500-700+ variables)  \n\\- Detects Tailwind usage and top utility patterns  \n\\- Uses an LLM to interpret component recipes and layout structure  \n\\- Outputs a .design-memory/ folder of markdown files\n\nThe output is structured so you can paste it into Claude, Cursor, or ChatGPT and get a faithful recreation of the original design.\n\nIt also supports learning from screenshots, multi-page crawls, and diffing two design systems.\n\nSource: [https://github.com/memvid/design-memory](https://github.com/memvid/design-memory)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5lfrsg",
          "author": "NeverSkipSleepDay",
          "text": "Have you ever seen what a design system looks like?",
          "score": 2,
          "created_utc": "2026-02-15 23:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5min2v",
              "author": "salasi",
              "text": "Lmao",
              "score": 1,
              "created_utc": "2026-02-16 03:47:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nl10m",
          "author": "WhoTookPlasticJesus",
          "text": "Not to be overly critical, but not even your demo screenshots are alike. The fonts are wrong, icons weren't copied, an entire fucking navbar has gone missing...\n\nI'm also not sure at all what this has to do with LLMs",
          "score": 1,
          "created_utc": "2026-02-16 09:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s3a9r",
          "author": "o1got",
          "text": "This is really cool. I've been on the receiving end of this problem from a different angle - we see a ton of AI agents crawling websites now, and the ones that actually extract design tokens and structured CSS tend to perform way better than the ones just scraping raw HTML.\n\nHow are you handling responsive design patterns? Like if a site has completely different component structure at mobile vs desktop, does it capture both states or does Playwright just grab whatever viewport you set?  \nAlso the diffing feature is interesting. I wonder if that could be useful for tracking how design systems evolve over time, like crawling the same site every few months and seeing what changed. Could be a neat way to learn from how mature products iterate on their UI.",
          "score": 1,
          "created_utc": "2026-02-17 00:16:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2ree2",
      "title": "Lessons from building AI shopping assistant for 1B$+ skincare brand.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "author": "rudzienki",
      "created_utc": "2026-02-12 11:50:51",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 0.85,
      "text": "Hey! I was recently hired to build an AI shopping assistant for a huge brand, 1B$+ in revenue. Unfortunately can't say which one is it (damn NDAs), but I thought I'd share some lessons. After the project CTO told me ‚ÄúWorking with you was the best AI investment in the last year‚Äù, so I guess it went well!\n\nI'm reposting this from my linkedin, so sorry for this \"linkedinish\" vibe:\n\nThe biggest secret was, surprise, surprise, **not** wasn‚Äôt fancy AI methods, complex RAG pipelines, and multi step workflows. In the end it was good prompts, a bunch of domain-specific tools and one subagent.  \n  \nThe secret was the process.  \n  \nI didn‚Äôt know anything about skincare so I had to learn about it. Even light understanding of the domain turned out EXTREMELY IMPORTANT since it allowed m to play around with an agent and have a good judgement whether it says good things. The fastest feedback loop is always \"in your head\".   \n  \nI built a domain-specific dashboard for the client. A collaborative environment where domain experts can play around with an agent, comment, feedback, etc. I took the idea from [Hamel Husain](https://x.com/HamelHusain) who said that [‚ÄúThe Most Important AI Investment is A Simple Data Viewer‚Äù.](https://x.com/i/status/1991903412997509372) He was damn right about it.   \n  \nThe last thing is something that is not talked much about but it should. We got hundreds of files about company knowledge. This knowledge is spread around big organisations like crazy. But if you really really understand the domain, if you really digest it all and ask a lot of questions, you‚Äôll be able to COMPRESS this knowledge. You‚Äôll find common stuff, remove dead ends, and really narrow it down to sth that expresses most about this company in smallest piece of text. This is your system prompt!! Why split context and add a potential point of failure if you can have MOST of the important stuff always in the system prompt? It‚Äôs crazy how well it works.  \n  \nOn the context engineering side we ended up with a great system prompt + a bunch of tools for getting info about products. I added one subagent for more complex stuff (routine building), but that was the only ‚Äúfancy‚Äù thing out there.  \n  \nI think the lesson here is that building agents is not hard on the technical level, and every developer can do it! The models do all the heavy lifting and they‚Äôre only getting better. The secret is understanding the domain and extracting the domain knowledge from people who know it. It's communication.\n\n  \nI'm curious:\n\nHave you built such \"customer support\"-related agents for your companies too? One thing that triggers me is amount of those giant SaaS companies that promises \"the super ultra duper ai agent\", and honestly? I think they don't have much secret sauce. Models are doing heavy lifting, and simple methods where heavy lifting is done by domain-specific knowledge trump general purpose ones. \n\nHere's what Malte from Vercel recently wrote btw:\n\nhttps://preview.redd.it/h2pjrjfix1jg1.png?width=1198&format=png&auto=webp&s=c8cd25ac93ee3a1b92cab153a1c591edbaf35d78\n\nIt somehow clicks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4ysvg2",
          "author": "HatApprehensive141",
          "text": "‚ÄúSecret sauce‚Äù = good prompts, domain tools, and actually understanding the business‚Ä¶ basically just doing your job properly.\n\nLots of companies hype up intergalactic RAG pipelines, but if you don‚Äôt compress real domain knowledge into a clear system, your agent is just an overconfident intern. The real edge isn‚Äôt the model magic, it‚Äôs the context quality.",
          "score": 7,
          "created_utc": "2026-02-12 11:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z7lpo",
          "author": "tom-mart",
          "text": "Wait till you discover the water is wet.",
          "score": 3,
          "created_utc": "2026-02-12 13:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ysv0w",
          "author": "kubrador",
          "text": "wow so the secret sauce was just... understanding the domain and writing good prompts. truly revolutionary stuff, might as well say the secret to cooking is using fresh ingredients and knowing what tastes good",
          "score": 2,
          "created_utc": "2026-02-12 11:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z2ese",
              "author": "rudzienki",
              "text": "I don't think simplicity is always obvious. There are many merchants of complexity out there who want to tell you otherwise.\n\nThat was the point of the post.",
              "score": 1,
              "created_utc": "2026-02-12 13:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o51kyxj",
          "author": "nore_se_kra",
          "text": "Beyond the hype... interesting read despite the comments here. I dont think it hurts to tell the story of applied \"boring\" company specific domain knowledge one more time.",
          "score": 2,
          "created_utc": "2026-02-12 20:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51q54o",
          "author": "SamCRichard",
          "text": "What LLM did you use or are you routing between them\n\n",
          "score": 2,
          "created_utc": "2026-02-12 20:50:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55hde0",
              "author": "rudzienki",
              "text": "Main thread was optimized for latency so it was good-but-not-best model, sonnet territory.\n\nSubagent was supposed to reason over many products to analyse interactions, in this case we used the best reasoning model. Still was a bit too slow with max reasoning effort, so we ended up with the best model with mid reasoning effort.",
              "score": 1,
              "created_utc": "2026-02-13 12:19:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zc408",
          "author": "ampancha",
          "text": "You're right that domain knowledge compression matters more than complex RAG for quality. The gap I see in most \"it works\" agents is what happens at production scale: prompt injection attempts from real users, hallucinated product claims becoming liability, and cost spikes without per-user attribution. For a $1B brand those risks are where the actual work starts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-12 13:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bak7z",
          "author": "SeaOk5990",
          "text": "Nice work, I'd love practical tips on scaling personalization?",
          "score": 1,
          "created_utc": "2026-02-14 09:07:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5vzrey",
          "author": "gardenia856",
          "text": "Your main insight is dead on: the real moat is compressed domain knowledge, not some 18-hop agent graph.\n\nWhat you describe as ‚Äúcompressing‚Äù org knowledge into a sharp system prompt is basically doing the product thinking and ontology work nobody wants to do. I‚Äôve had the same experience: once you‚Äôve read the internal docs, sat with support and sales, and boiled everything into a few pages of ‚Äúhow this company actually thinks,‚Äù retrieval becomes just a safety net instead of the main act.\n\nThe dashboard piece is underrated too. Giving domain experts a sandbox where they can poke the agent, leave comments, and iterate on that compressed spec is worth way more than another custom reranker.\n\nOn the tooling front, I‚Äôve bounced between Intercom, Zendesk bots, and Pulse for Reddit for catching and answering real user questions where they hang out, and the stuff that works is always: tight prompts, good tools, short paths, no ego about using simple patterns.",
          "score": 1,
          "created_utc": "2026-02-17 16:23:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2s3r4",
      "title": "I dont get mcp",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "author": "Yaar-Bhak",
      "created_utc": "2026-02-12 12:26:56",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 0.83,
      "text": "All I understood till now is - \n\nI'm calling an LLM api normally and now\nInstead of that I add something called MCP which sort of shows whatever tools i have? And then calls api \n\n\nI mean, dont AGENTS do the same thing? \n\nWhy use MCP? Apart from some standard which can call any tool or llm \n\nAnd I still dont get exactly where and how it works \n\nAnd WHY and WHEN should I be using mcp? \n\nI'm not understanding at all üò≠ Can someone please help\n\n",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4z3q6l",
          "author": "rudzienki",
          "text": "It's just a standardised way for companies to \"expose their tools\".\n\nIf you're Stripe you have a bunch of tools: \"do payment\", \"check invoices\" etc. If you want your agent to use them you can just... add them as tools to your agent. That's it. \n\nBut with MCP you can just say \"connect to stripe MCP\" and it automatically fetches all Stripe tools to be called. Stripe updates tools, you get update automatically.\n\nBut aside from that - no difference. \n\nBtw, MCP is much bigger protocol that handles more stuff than exposing tools, but in reality it's 99%, other uses didn't get much traction as far as I know.",
          "score": 10,
          "created_utc": "2026-02-12 13:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zrwup",
          "author": "fooz42",
          "text": "Service registry and discovery for remote procedure call is a wheel that gets reinvented every platform. It's not a revolution except in the sense the wheel gets reinvented every time the cycle turns, and now I'm getting dizzy from my metaphor.",
          "score": 13,
          "created_utc": "2026-02-12 15:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yy3mm",
          "author": "kubrador",
          "text": "mcp is basically \"what if we made tool use boring and standardized so literally any llm can talk to literally any tool without rewiring everything\" agents let your llm pick tools. mcp is the \\*protocol\\* so your llm doesn't need to know what tools exist. they just show up. it's the difference between \"here's a menu\" vs \"here's a standardized way to hand someone a menu\"\n\nyou need it when you're tired of writing custom integrations for every tool+llm combo. you don't need it if you're just bolting claude into your thing once and calling it a day.",
          "score": 3,
          "created_utc": "2026-02-12 12:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yzpvs",
              "author": "Yaar-Bhak",
              "text": "you think this can be used in production?\n\nand this means mcp would be used only in agentic flows right?",
              "score": 1,
              "created_utc": "2026-02-12 12:42:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54buzg",
          "author": "FoldedKatana",
          "text": "MCP is dead now. OpenClaw skills are where it's at",
          "score": 2,
          "created_utc": "2026-02-13 06:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e0t1n",
          "author": "Cool_Fly_2030",
          "text": "The term ‚Äútool‚Äù in this thread is probably creating more confusion.\n\nMCP is a protocol to standardize how LLMs fetch context to ground and complete a task or generate a response more effectively.\n\nMCP servers are effectively APIs, where ‚Äútools‚Äù function as endpoints/routes to handle requests and execute logic on the server to do something and return the response to the LLM. \n\nWhen you register an MCP server in your client - VS code, Claude code, etc the LLM has a registry of tools to arbitrate between and delegate to if the description of the tool will solve its problem. \n\nThis is pretty powerful in agentic applications of LLMs because they can retrieve external context and perform actions in a fully automated loop.",
          "score": 2,
          "created_utc": "2026-02-14 19:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50wvs5",
          "author": "throwaway490215",
          "text": "MCPs are bullshit. They are a standard that basically tells the program run on your computer to prepend `some-tool --help` when you start a conversation, but with much more overhead, and **every conversation** even if you dont want to use `some-tool` this session. \n\nAnybody talking about credentials/authentication is a moron. \n\nJust add a \"Use `some-tool --help` to do X\" in your AGENTS.md and you're good.",
          "score": 2,
          "created_utc": "2026-02-12 18:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z5ouz",
          "author": "Astronos",
          "text": "it is function/tool calling over api",
          "score": 1,
          "created_utc": "2026-02-12 13:21:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zo2ke",
          "author": "Crafty_Disk_7026",
          "text": "MCP is just like an open ai spec the ai can read and know how to use your tool. It's literally just instruction manual",
          "score": 1,
          "created_utc": "2026-02-12 15:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tzq6",
          "author": "voidiciant",
          "text": "From what I understand:\n\nThe models have to be trained (and usually are, there is often a ‚Äûtool‚Äú tag on the downloads) to insert special keywords in their responses when a tool call is appropriate. \n\nThese keywords are intercepted by the runtime (the thing taking your input, converting to tokens,, etc) and the runtime performs the appropriate calls to the registered mcp tools (according to the protocol) and feeds back the tool-call results to the model, which in turn now incorporates them in the next response.\n\nAdditionally, and here I get fuzzy, the runtime generates a system prompt that contains a list of available MCP Tools, and the model is trained to understand this to generate the relevant keywords in the response based. \n\nMCP defines the protocols/API/formats. \n\nThat‚Äòs the gist for me",
          "score": 1,
          "created_utc": "2026-02-12 18:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51rpl9",
          "author": "CreepyValuable",
          "text": "MCP is kind of sort of a universal adapter to plug anything from ChatGPT to your toaster together.\n\nIt's not quite that straightforward and the actual interface is kind of clunky but it's pretty useful.\n\nFor example, my (not very good, but experimental so that's not important) AI uses it for things like a weather service, XiaoZhi AI esp support (essentially a smart speaker with a screen), VS Code integration and some other random things. It avoids needing a whole bunch of incompatible APIs.",
          "score": 1,
          "created_utc": "2026-02-12 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52svxj",
          "author": "Glum_Teaching8224",
          "text": "It's just tool using reference for the agent. ",
          "score": 1,
          "created_utc": "2026-02-13 00:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwht1",
          "author": "Electronic-Door7134",
          "text": "Good luck explaining to an auditor why your gave a 3rd party company full access to your company data (which is what happens without mcp)",
          "score": -4,
          "created_utc": "2026-02-12 15:41:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50avet",
              "author": "PaddingCompression",
              "text": "Wut",
              "score": 3,
              "created_utc": "2026-02-12 16:48:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4nj4y",
      "title": "16 single-file, zero-dependency implementations of the algorithms behind LLMs ‚Äî tokenization through speculative decoding. No frameworks, just the math.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/t4h1i8nbbhjg1.png",
      "author": "tom_mathews",
      "created_utc": "2026-02-14 15:32:01",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4nj4y/16_singlefile_zerodependency_implementations_of/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ho7tr",
          "author": "tom_mathews",
          "text": "The repo has been expanded from 16 to 30 scripts since the original post. Here's what's new:\n\n- **Foundations (7 ‚Üí 11):** Added BERT (bidirectional encoder), RNNs & GRUs (vanishing gradients + gating), CNNs (kernels, pooling, feature maps), GANs (generator vs. discriminator), VAEs (reparameterization trick), diffusion (denoising on point clouds), and an optimizer comparison (SGD vs. Momentum vs. RMSProp vs. Adam).\n\n- **Alignment (4 ‚Üí 9):** Added PPO (full RLHF reward ‚Üí policy loop), GRPO (DeepSeek's simplified approach), QLoRA (4-bit quantized fine-tuning), REINFORCE (vanilla policy gradients), Mixture of Experts (sparse routing), batch normalization, and dropout/regularization.\n\n- **Systems (5 ‚Üí 10):** Added paged attention (vLLM-style memory management), RoPE (rotary position embeddings), decoding strategies (greedy, top-k, top-p, beam, speculative ‚Äî all in one file), tensor & pipeline parallelism, activation checkpointing, and state space models (Mamba-style linear-time sequence modeling).\n\nSame constraints as before: every script is a single file, zero dependencies, trains and infers (or demonstrates forward-pass mechanics side-by-side), runs on CPU in minutes.\n\n[https://github.com/Mathews-Tom/no-magic](https://github.com/Mathews-Tom/no-magic)",
          "score": 1,
          "created_utc": "2026-02-15 11:10:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6m5hb",
      "title": "Can LLMs deduplicate ML training data?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6m5hb/can_llms_deduplicate_ml_training_data/",
      "author": "ddp26",
      "created_utc": "2026-02-16 21:14:03",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I get increasingly annoyed with how unreliable deduplication tools are for cleaning training data. I‚Äôve used MinHash/LSH, libraries like [dedupe.io](http://dedupe.io), and pandas.drop\\_duplicates() but they all have a lot of false positives/negatives.  \n  \nI ended up running LLM-powered deduplication on 3,000 sentences from Google's paraphrase dataset from Wikipedia (PAWS). It removed 1,072 sentences (35.7% of the set). It only cost $4.21, and took \\~5 minutes.  \n  \nExamples of what it catches that the other methods don't:\n\n* \"Glenn Howard won the Ontario Championship for the 17th time as either third or skip\" and \"For the 17th time the Glenn Howard won the Ontario Championship as third or skip\"\n* \"David Spurlock was born on 18 November 1959 in Dallas, Texas\" and \"J. David Spurlock was born on November 18, 1959 in Dallas, Texas\"\n\n  \nFull code and methodology: [https://everyrow.io/docs/deduplicate-training-data-ml](https://everyrow.io/docs/deduplicate-training-data-ml)\n\nAnyone else using LLMs for data processing at scale? It obviously can work at small scale (and high cost), but are you finding it can work at high scale and low cost?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6m5hb/can_llms_deduplicate_ml_training_data/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5sbqnp",
          "author": "kubrador",
          "text": "yeah this is clever but you're basically paying for semantic understanding you could get cheaper with embeddings + cosine similarity. run your 3k sentences through openai's small embedding model (\\~$0.02 total), cluster by cosine distance, done in 10 seconds for less than a coffee.\n\n\n\nthe paraphrase examples you showed would absolutely get caught by that approach since they're semantically identical, which is what actually matters for training data dedup anyway.",
          "score": 2,
          "created_utc": "2026-02-17 01:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sf7em",
              "author": "dreamingwell",
              "text": "You could do this to get ‚Äúprobably duplicates‚Äù. And then use an LLM to finalize them. Reducing your LLM costs significantly.",
              "score": 1,
              "created_utc": "2026-02-17 01:25:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5seuz1",
          "author": "dreamingwell",
          "text": "You can do a Lora tuning on a small model, like Qwen3-4B. Train it to identify duplicated data from examples in your set. On the right GPU, it would absolutely tear through that data.",
          "score": 2,
          "created_utc": "2026-02-17 01:23:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzvnc",
              "author": "ChanceKale7861",
              "text": "Holy moly! Are you me?! ü§£ü§£ü§£",
              "score": 1,
              "created_utc": "2026-02-18 01:19:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5siq3u",
          "author": "No_Indication_1238",
          "text": "Tbh, you pretty much nailed a novel use case for LLMs. Yes, semantic analysis was tough before them.",
          "score": 1,
          "created_utc": "2026-02-17 01:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uvzl2",
          "author": "andy_p_w",
          "text": "Those two examples, if you take out regular words (any word 3 letters or less) and just look at the Jaccard similarity for the words will have very high overlap. English language is quite large, it is difficult to have much overlap in words random sentences, [https://andrewpwheeler.com/2024/04/20/some-musings-on-plagiarism/](https://andrewpwheeler.com/2024/04/20/some-musings-on-plagiarism/) .",
          "score": 1,
          "created_utc": "2026-02-17 12:52:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r21425",
      "title": "I tracked how much time my team wastes re-explaining context. The number is embarrassing.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r21425/i_tracked_how_much_time_my_team_wastes/",
      "author": "arapkuliev",
      "created_utc": "2026-02-11 15:56:39",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.77,
      "text": "I got annoyed enough to actually measure this.  \n  \nFor two weeks I logged every time someone on my 6-person eng team had to re-explain context to someone else. Slack catch-ups, meeting recaps, \"here's what we decided and why\" conversations, onboarding someone into a thread.  \n  \n47 times per week. Average 8.5 minutes each. That's 6.5 hours/week of engineering time just... transferring context from one brain to another.  \n  \nAnd that's only the times someone *bothered*. How many times did someone just not explain, and the other person made a decision with half the picture?  \n  \nWe have Notion. We have Confluence. We have a wiki nobody reads. We have \"just search Slack\" which is a joke. None of it works because the context is either stale, unfindable, or so buried in noise that nobody trusts it.  \n  \nI've been experimenting with treating it as a memory problem instead of a documentation problem. Like, what if decisions and context got captured *as they happened* instead of someone writing a doc after the fact (which nobody does)?  \n  \nEarly results are promising but I'm curious: is anyone else measuring this? Or are we all just accepting \"let me catch you up\" as a normal part of work?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r21425/i_tracked_how_much_time_my_team_wastes/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4vfvzu",
          "author": "radarsat1",
          "text": "yes, humans talking to each other is a normal part of work.",
          "score": 16,
          "created_utc": "2026-02-11 21:35:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o55yfuf",
              "author": "MannToots",
              "text": "I like when people succinctly prove they didn't read the whole thing.¬†¬†",
              "score": 0,
              "created_utc": "2026-02-13 14:01:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x7jvn",
          "author": "Nofoofro",
          "text": "It sounds like a culture, process and info architecture issue.¬†\n\nSo many of these posts have so little detail. So many words to say so little.",
          "score": 7,
          "created_utc": "2026-02-12 03:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4vi0yz",
          "author": "WolfeheartGames",
          "text": "This is a problem of information theory and how communication works.",
          "score": 3,
          "created_utc": "2026-02-11 21:45:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50jv89",
          "author": "UncleRedz",
          "text": "May I ask how many products and/or feature areas your team is responsible for and the size of the team? Too big teams and self organization doesn't work, information starts to become siloed. Too many products and/or feature areas and the cognitive load increases to a point where it's simply not possible to keep track of everything by everyone. Then you need to split into sub-teams with specific areas of responsibilities, which you can only do if the team is big enough. So it's both organizational and resource constrainted. I guess you could call it a memory problem, but i'd say it's more about memory capacity then.",
          "score": 2,
          "created_utc": "2026-02-12 17:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z8bxu",
          "author": "PaddingCompression",
          "text": "This is my favorite part of vibe coding.\n\nLLMs are so obviously bad at context that I'm forced to be really good at explicit communication and organization of information.  These skills are just generally pretty useful in life.\n\nIt's like practicing delegation skills on hard mode.",
          "score": 1,
          "created_utc": "2026-02-12 13:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ztvpo",
          "author": "Mad_Tyrion",
          "text": "you are not alone :(",
          "score": 1,
          "created_utc": "2026-02-12 15:29:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5404zr",
          "author": "mimitwothree",
          "text": "Is that the problem or job stress? Let me out it this way. Your parents asked you what probably a billion time in a year to do something and u still didn't.\n\nShall I say lazy?\nCan't be bothered?\n\nNo bro. \n\nIt's just growing as a person . If this bothers u then trust me this is not the problem. \nThe problem is the job and you . \nRelax .",
          "score": 1,
          "created_utc": "2026-02-13 04:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56s9o2",
          "author": "gh0stwriter1234",
          "text": "Probably the most effective way to reduce this is have the correct person decide things... decentralized decision making is always going to be less efficient but it is also more robust at various levels as well as in distributing blame in the event of failure because instead of throwing one person under the bus and continuing to be bad everyone has to accept reality.\n\nSometimes being effective is counter productive in the long run because of the secondary effects.",
          "score": 1,
          "created_utc": "2026-02-13 16:29:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o57bphs",
          "author": "siddu71",
          "text": "Don't optimize humans like robots or programming...",
          "score": 1,
          "created_utc": "2026-02-13 18:03:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b15ix",
          "author": "Illustrious-Echo1383",
          "text": "Culture problem. Amazon handles it nicely, in every meeting the organizer is expected to prepare a concise document on the topic of discussion. When meeting starts, 5 minutes is dedicated to read and ask silent questions (in the document). This does two things,\n1. Beings everyone up to speed without wasting too much time\n2. Drives the conversation forward without anyone realizing, by the silent q/a¬†",
          "score": 1,
          "created_utc": "2026-02-14 07:36:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5mqrd",
      "title": "Has anyone here successfully sold RAG solutions to clients? Would love to hear your experience (pricing, client acquisition, delivery, etc.)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5mqrd/has_anyone_here_successfully_sold_rag_solutions/",
      "author": "Temporary_Pay3221",
      "created_utc": "2026-02-15 18:57:10",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.82,
      "text": "Hey everyone!\n\nI've been diving deep into RAG systems lately and I'm genuinely fascinated by the technology. I've built a few projects for myself and feel confident in my technical abilities, but now I'm looking to transition this into actual client work.\n\nBefore I jump in, I'd really appreciate learning from people who've already walked this path. If you've sold RAG solutions to clients, I'd love to hear about your experience:\n\n**Client & Project Details:**\n\n* What types of clients/industries did you work with?\n* How did they discover they needed RAG? (Did they come asking for it, or did you identify the use case?)\n* What was the scope? (customer support, internal knowledge base, document search, etc.)\n\n**Delivery & Timeline:**\n\n* How long did the project take from discovery to delivery?\n* What were the biggest technical challenges you faced?\n* Did you handle ongoing maintenance, or was it a one-time delivery?\n\n**Business Side:**\n\n* How did you find these clients? (freelance platforms, LinkedIn outreach, referrals, content marketing, etc.)\n* What did you charge? (ballpark is fine, just trying to understand market rates)\n* How did you structure pricing? (fixed project, hourly, monthly retainer?)\n\n**Post-Delivery:**\n\n* Were clients happy with the results?\n* Did you iterate/improve the system after launch?\n* Any lessons learned that you'd do differently next time?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5mqrd/has_anyone_here_successfully_sold_rag_solutions/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r79s1w",
      "title": "SurrealDB 3.0 for AI agent memory",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r79s1w/surrealdb_30_for_ai_agent_memory/",
      "author": "DistinctRide9884",
      "created_utc": "2026-02-17 15:55:41",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "SurrealDB 3.0 just dropped, with a big focus on agent memory infra for AI: improved vector indexing + better graph performance + native file storage + a WebAssembly extension system (Surrealism) that can run custom logic/models inside the DB. You can store vector embeddings + structured data + graph context/knowledge/memory in one place and do hybrid retrieval in one query.\n\nDetails:¬†[https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memor](https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memory)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r79s1w/surrealdb_30_for_ai_agent_memory/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2qfcz",
      "title": "Mix prompts instead of writing them by hand",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/5z7edpx9n1jg1.png",
      "author": "Everlier",
      "created_utc": "2026-02-12 10:55:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2qfcz/mix_prompts_instead_of_writing_them_by_hand/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r6pho9",
      "title": "Have we overcome the long-term memory bottleneck?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6pho9/have_we_overcome_the_longterm_memory_bottleneck/",
      "author": "Bubbly_Run_2349",
      "created_utc": "2026-02-16 23:22:18",
      "score": 7,
      "num_comments": 18,
      "upvote_ratio": 0.82,
      "text": "Hey all,\n\nThis past summer I was interning as an SWE at a large finance company, and noticed that there was a huge initiative deploying AI agents. Despite this, almost all Engineering Directors I spoke with were complaining that the current agents had no ability to recall information after a little while (in fact, the company chatbot could barely remember after exchanging 6‚Äì10 messages).\n\nI discussed this grievance with some of my buddies at other firms and Big Tech companies and noticed that this issue was not uncommon (although my company‚Äôs internal chatbot was laughably bad).\n\nAll that said, I have to say that this \"memory bottleneck\" poses a tremendously compelling engineering problem, and so I am trying to give it a shot and am curious what you all think.\n\nAs you probably already know, vector embeddings are great for similarity search via cosine/BM25, but the moment you care about things like persistent state, relationships between facts, or how context changes over time, you begin to hit a wall.\n\nRight now I am playing around with a hybrid approach using a vector plus graph DB. Embeddings handle semantic recall, and the graph models entities and relationships. There is also a notion of a \"reasoning bank\" akin to the one outlined in Googles famous paper several months back. TBH I am not 100 percent confident that this is the right abstraction or if I am doing too much. \n\nHas anyone here experimented with structured or temporal memory systems for agents?\n\nIs hybrid vector plus graph reasonable, or is there a better established approach I should be looking at?\n\nAny and all feedback or pointers at this stage would be very much appreciated.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6pho9/have_we_overcome_the_longterm_memory_bottleneck/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5rzwor",
          "author": "Sea-Chemistry-4130",
          "text": "We're just reinventing distributed computing but with llms...",
          "score": 8,
          "created_utc": "2026-02-16 23:57:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sble0",
              "author": "Bubbly_Run_2349",
              "text": "Lol i guess... if by distributed computing you mean clever system design then I can't disagree. ",
              "score": 0,
              "created_utc": "2026-02-17 01:04:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5sik1h",
                  "author": "Sea-Chemistry-4130",
                  "text": "No I literally mean we're having to resolve many of the same issues that distributed systems have only now it's to ensure the llm didn't get it wrong instead of due to unreliable network or hardware.",
                  "score": 3,
                  "created_utc": "2026-02-17 01:46:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ruxjr",
          "author": "user0139",
          "text": "Interesting approach, although you are a bit vague with your description. Do you have a repo I can look over?",
          "score": 2,
          "created_utc": "2026-02-16 23:28:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5rvfoe",
              "author": "Bubbly_Run_2349",
              "text": "Woah thank you for the fast reply. \n\nHere is the link: [https://github.com/TheBuddyDave/Memoria.](https://github.com/TheBuddyDave/Memoria)\n\nmuch appreciated!",
              "score": 1,
              "created_utc": "2026-02-16 23:31:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5rymef",
          "author": "ibrahimsafah",
          "text": "Im doing something similar just for personal knowledge. I basically am out of my depth. I recommend reading some white papers about it",
          "score": 2,
          "created_utc": "2026-02-16 23:49:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sbg95",
              "author": "Bubbly_Run_2349",
              "text": "Oh thats pretty cool. I was able to get some AI researchers/students on my projects discord. They have been a lot of help. ",
              "score": 1,
              "created_utc": "2026-02-17 01:03:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s11my",
          "author": "philip_laureano",
          "text": "Yes, but this problem has been solved in other areas of software engineering.\n\nIt's a viewport problem, not a knapsack fitting problem. We do this all the time with Web pages.\n\nFor example, how in the world do we fit several TB of information that we have floating around onto a single browser tab so that someone can use and browse that information?\n\nHint: You don't do that by dumping the most similar context into the browser tab and expect the user to piece all the bits together. \n\nIf I as the user click on a link about 'architecture', I don't expect a page filled with terms similar to architecture. I expect to go to the page about architecture and it should be organised and easy to get to, despite the fact there's an infinite sea of information out there. \n\nThe only thing that changes is how that information is retrieved and organised and how my viewport changes to match what I'm looking for. \n\nThese problems in of themselves aren't new. The AI industry is, but the problems of scale are well known. The only question is when they'll catch up to the solutions already known",
          "score": 2,
          "created_utc": "2026-02-17 00:03:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sc8pg",
              "author": "Bubbly_Run_2349",
              "text": "I see. The current retrieval algo I have been workshopping with some buddies takes a lot of inspiration from  page ranking algos.   \n  \nVery interesting insight! Thank you.  ",
              "score": 1,
              "created_utc": "2026-02-17 01:08:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tdg6d",
          "author": "Ill_Awareness6706",
          "text": "Do you have a repo I can look over?",
          "score": 2,
          "created_utc": "2026-02-17 05:05:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tgtpc",
              "author": "Bubbly_Run_2349",
              "text": "Yes of course! Thank you again all help is very appreciated :) \n\n[https://github.com/TheBuddyDave/Memoria](https://github.com/TheBuddyDave/Memoria)",
              "score": 1,
              "created_utc": "2026-02-17 05:30:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tlbim",
          "author": "Happy-Fruit-8628",
          "text": " Hybrid vector plus graph is reasonable. The tough part isn‚Äôt storage, it‚Äôs deciding what to remember and keeping it from getting messy over time.",
          "score": 2,
          "created_utc": "2026-02-17 06:06:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uhsa3",
          "author": "Far_Noise_5886",
          "text": "I think vector + graph is where it's heading. How though do you handle the context bloat? ",
          "score": 2,
          "created_utc": "2026-02-17 11:04:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5we5z8",
          "author": "honestduane",
          "text": "What you‚Äôre dealing with is context window limitations; this shows to me that you haven‚Äôt yet learned about the AI stuff deep enough so keep learning.",
          "score": 1,
          "created_utc": "2026-02-17 17:35:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ynlhn",
          "author": "footuretruth",
          "text": "I have made a program that keeps continuity between user and AI basically a reference/recollection snapshot. Not true memory but it definitely supplements well.",
          "score": 1,
          "created_utc": "2026-02-18 00:11:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zego4",
          "author": "cmndr_spanky",
          "text": "Just FYI there‚Äôs a post on this subreddit every 10 mins from some bot claiming they‚Äôve solved LLM memory or pretending to ask a question that‚Äôs ultimately just peddling some crap solution that has already been solved.",
          "score": 1,
          "created_utc": "2026-02-18 02:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5sejh9",
          "author": "No_Wrongdoer41",
          "text": "me and a few folks have built a platform that automatically creates a shared knowledge/ context layer from underlying sources for agents to use. happy to let you try it for free!",
          "score": 1,
          "created_utc": "2026-02-17 01:21:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r204ab",
      "title": "Intent Model",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "author": "Repulsive_Laugh_1875",
      "created_utc": "2026-02-11 15:18:54",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hi community,  \nthis is my first post here üôÇ\n\nI‚Äôm an experienced AI Engineer / AI DevOps Engineer / Consultant working for a well-known US-based company. I‚Äôd really appreciate your thoughts on a challenge I‚Äôm currently facing and whether you would approach it differently.\n\nUse-Case\n\nI‚Äôm building an **intent classifier** that must:\n\n* Run **on edge**\n* Stay around **\\~100ms latency**\n* Predict **1 out of 9 intent labels**\n* Consider **up to 2 previous conversation turns**\n\nThe environment is domain-specific (medical domain in reality), but to simplify, imagine a system controlling a car.\n\nExample:  \nYou have an intent like `lane_change`, and the user can request it in many different ways.\n\nCurrent Setup\n\n* Base model: **phi-3.5-mini-instruct**\n* Fine-tuned using **LoRA**\n* Model explicitly outputs only the intent token (e.g., `command_xyz`)\n* Each intent is mapped to a **single special token**\n* Almost no system prompt (removed to save tokens)\n\nPerformance:\n\n* \\~110ms latency (non-quantized) ‚Üí acceptable\n* \\~10 input tokens on average\n* \\~5 output tokens on average\n* 25k training samples\n* \\~95% accuracy\n\nSpeed is not the main issue ‚Äî I still have some room for token optimization and quantization if needed.\n\nthe real challenge -> the missing 5%.\n\nThe issue is **edge cases**.\n\nThe model operates in an open-input environment. The user can phrase requests in unlimited ways.\n\nFor example:  \nFor `lane_change`, there might be 30+ semantically equivalent variations. I built a synthetic data generation pipeline to create such variations and spent \\~2 weeks refining it. Evaluation suggests it's decent.\n\nBut:\n\nThere are still rare phrasings that the model hasn‚Äôt seen ‚Üí wrong intent prediction.\n\nOf course, I can:\n\n* Iteratively collect misclassifications\n* Add them to the training set\n* Retrain\n\nBut that‚Äôs slow and reactive.\n\nConstraints:\n\n* I could use a larger model (e.g., phi-4), and I‚Äôve tested it.\n* However, time-to-first-token for phi-4 is significantly slower.\n* Latency is more important than squeezing out a few extra percent of quality.\n\nSo scaling up model size isn‚Äôt ideal.\n\nMy questions to you:\n\nHow would you tackle the final 5%?\n\nI‚Äôd really appreciate hearing how others would approach this kind of edge, low-latency intent classification problem.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4tefqh",
          "author": "Swimming-Chip9582",
          "text": "Can you detect whether the output is likely to be an edge case or know when it's part of an uncertain category?  Perhaps you can fallback on a larger model and accept latency when it's unsure. So starting both the small and large concurrent, if the small finishes first and is all good just cancel the large one, if it's uncertain then wait for completion from the bigger model. ",
          "score": 4,
          "created_utc": "2026-02-11 15:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tll8h",
          "author": "FoodAccurate5414",
          "text": "You need to look into using very very very small models to handle edge cases. There are tons on hugging face. Run it along side your main model",
          "score": 2,
          "created_utc": "2026-02-11 16:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57q9",
              "author": "Repulsive_Laugh_1875",
              "text": "Can you recommend something or at least tell which ones you have in mind?",
              "score": 1,
              "created_utc": "2026-02-11 20:43:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t91xc",
          "author": "InfraScaler",
          "text": "Ok, so catching new ways of expressing the intent from your users is definitely reactive, but what about using bigger LLMs to generate those for you? Can you even use agents leveraging big LLMs to \"test\" and help prepare training for your system? Not necessarily cheap, but your employer may be able to afford it :-)",
          "score": 1,
          "created_utc": "2026-02-11 15:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tc616",
              "author": "Repulsive_Laugh_1875",
              "text": "It‚Äôs not the employer ‚Äî it‚Äôs the customer who has to pay for it üòâ\n\nJokes aside, thank you for your comment.\n\nI‚Äôm already using GPT-5.2-chat for the synthetic data generation. As mentioned, I‚Äôm currently achieving a full match rate  (intent plus parameters) of around 95%, and based on the latest metrics even closer to 97%, which I consider quite solid.\n\nThat‚Äôs why I don‚Äôt believe the data generation itself is the core issue here.\n\n  \n\\-------------------------- edited\n\n  \nin fact, I also thought about leveraging two agents to simulate such qustion answer things and try to figure out such edge cases. But this is costly.",
              "score": 2,
              "created_utc": "2026-02-11 15:38:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4thauy",
                  "author": "InfraScaler",
                  "text": ">But this is costly.\n\nNot for you! :P",
                  "score": 2,
                  "created_utc": "2026-02-11 16:02:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ybuxq",
                  "author": "BehindUAll",
                  "text": "Why is cost an issue? If you are using 2 classifiers, they are cheaper than LLMs and if you are using LLMs like Llama-3.1 8b or Mistral small etc. you will get great speed and is cheap too. Plus you can use groq, SambaNova or Cerebras for fast and cheap inference. And for LLMs the input will be like 10 words and output 1 word if you do your system prompt right. The cost goes way down.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:17:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vvjay",
          "author": "Charming_Support726",
          "text": "First I doubt that models with decoder only architecture are the best fit for this task.  They allways bring error classes which are unwanted in these scenarios.\n\nSecond I doubt, that you ever will reach 100% - but that's obvious\n\nThird: Encoder-Decoder architectures promise the best fit and efficiency for these tasks, but you'll never know ( we did back in 2020 over a 1000 intents with Rasa in a BERT-Style Intent detector went over 90% but appeared still flaky). T5Gemma Style Models could be a solution, but I got no experience on fine tuning them.\n\nFourth you could apply additional techniques like reranking or building a similarity distance to sample sentences to make sure that your generation is a valid result.  Maybe good to combine this approaches with multiple generations of the result.\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-11 22:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ysnco",
              "author": "Repulsive_Laugh_1875",
              "text": "To your fist point: I agree to this but I don¬¥t get what you mean by error classes? Do you mean just a wrong prediction label with this? If thats your point, you are right. These are the edge cases what I¬¥m talking about! It never responds in a different way, it just predicts sometimes the wrong label (edge cases).\n\n  \nTo your second point -> That is obvious and clear. Customer is already pretty satisfied with the solution, but I¬¥m not :D\n\n  \nthird: good hint. I need to look into them on how to really finetune them.\n\n  \nfourth: Indeed, I also wanted to test if you purely rely on similarity (history dependend intents excluded). I wanted to test if you can embedd those 25k and just detect the intent based on the similarity. I did something similar a few years back and this worked also very solid ;-) Now with the larger embedding representations, could also be an option.",
              "score": 1,
              "created_utc": "2026-02-12 11:51:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o51kkkq",
                  "author": "Charming_Support726",
                  "text": "Decoder only does not stick close to the input as encoder-decoder do. So you get all the things like hallucinations and similar. They also have issues generating \"i dont know\" tokens. \n\nIf you have an encoder part in the model like in a T5 the model is always working with a kind of embedding as knowledge representation for building a bridge to the decoder. \n\nThat's you are 90% there using an SLM, so these are my ideas to improve.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:24:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yfzud",
          "author": "tshawkins",
          "text": "If it is acceptable to have edge case resolution slower, you could push the cases that fail to resolve out to a bigger model, and push those requests and thier resolutions into a set of training data for your next primary model rebuild. Its a hybrid solution that uses both bigger model and retraining for the 5% only.",
          "score": 1,
          "created_utc": "2026-02-12 09:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u6y1l",
          "author": "TheBioto",
          "text": "Give Mistral 3b a shot and see how it works. \n\nI am currently doing something similar with endpoint nodes with small models. Mistral was the only one that was fast enough/could be guided enough to accomplish my needs.\n\nI don't have any suggestions for your edge cases issue, good luck!",
          "score": 1,
          "created_utc": "2026-02-11 18:01:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6hwr0",
      "title": "Bring OpenClaw-style memory to every agent",
      "subreddit": "LLMDevs",
      "url": "https://github.com/zilliztech/memsearch",
      "author": "codingjaguar",
      "created_utc": "2026-02-16 18:39:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6hwr0/bring_openclawstyle_memory_to_every_agent/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r3hvkg",
      "title": "Observation: LLMs seem to have a \"Version 2.0\" bias when generating new UIs",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/rmkiv7qmh7jg1.jpeg",
      "author": "Routine_Connection8",
      "created_utc": "2026-02-13 06:29:02",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3hvkg/observation_llms_seem_to_have_a_version_20_bias/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    }
  ]
}