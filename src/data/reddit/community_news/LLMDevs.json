{
  "metadata": {
    "last_updated": "2026-01-29 17:09:57",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 115,
    "file_size_bytes": 171361
  },
  "items": [
    {
      "id": "1qoj60d",
      "title": "I relied on stateless retrieval for long-form agents. It failed after 50 turns. Here‚Äôs how I‚Äôm managing state now.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qoj60d/i_relied_on_stateless_retrieval_for_longform/",
      "author": "gt_roy_",
      "created_utc": "2026-01-27 16:48:00",
      "score": 31,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Full disclosure: I‚Äôm the dev behind this project.\n\nIn long-running agent sessions (\\~50‚Äì100 turns), I kept seeing the same failure mode: preferences established early would silently stop affecting generation, even though they were still retrievable. You build a cool agentic workflow, and it works great for the first few turns. By turn 60, it starts doing those statistical parlor tricks where it just ignores half your instructions or forgets a preference you established three sessions ago.\n\nThe problem is that stateless retrieval is, well, stateless. It‚Äôs fine for pulling static docs, but it doesn't actually 'learn' who the user is. You can try recursive summarization or sliding windows, but honestly, you‚Äôre just burning tokens to delay inevitable instruction drift.\n\nI spent the last few months building a layer to handle long-term state properly. I‚Äôm calling it MemOS (probably an overloaded term, but it manages the lifecycle). It‚Äôs an MIT-licensed layer that sits between the agent and the LLM.\n\nWhy stateless retrieval isn't enough:\n\nThe first thing people ask is why not just use a Vector DB. They are great for storage, but they don't have a logic layer for state. If a user says 'I hate Python' in turn 5 and 'actually I‚Äôm starting to like Python' in turn 50, a standard search returns both. It‚Äôs a mess.\n\nMemOS handles the lifecycle‚Äîit merges similar memories, moves old stuff to a 'MemVault' (cold storage), and resolves conflicts based on a freshness protocol.\n\nFacts vs. Preferences:\n\nI realized agents fail because they treat all context the same. I split them up:\n\n\\- Facts: Hard data (e.g., 'The project deadline is Friday')\n\n\\- Preferences: How the user wants things done (e.g., 'No unwraps in Rust, use safe error handling')\n\nWhen you hit addMessage, it extracts these into 'MemCubes' automatically so you don't have to manually tag everything.\n\nThe Implementation:\n\nI tried to keep the DX pretty simple, basically just a wrapper around your existing calls.\n\nfrom memos import MemClient\n\nclient = MemClient(api\\_key=\"your\\_key\") # or localhost\n\n\\# This extracts facts/prefs automatically in the background\n\nclient.add\\_message(\n\nuser\\_id=\"dev\\_123\",\n\nrole=\"user\",\n\ncontent=\"I'm on a Rust backend. Avoid unwraps, I want safe error handling.\"\n\n)\n\n\\# Retrieval prioritizes preferences and freshness\n\ncontext = client.search\\_memory(user\\_id=\"dev\\_123\", query=\"How to handle this Result?\")\n\nprint(context)\n\n\\# Output: \\[Preference: Avoids unwraps\\] \\[Fact: Working on Rust backend\\]\n\nLatency & 'Next-Scene Prediction':\n\nInjecting a massive history into every prompt is a great way to go broke and spike your latency. I added an async scheduling layer called Next-Scene Prediction. It basically predicts what memories the agent will need next based on the current convo trajectory and pre-loads them into the KV Cache.\n\nTech Stack:\n\nCore: Python / TypeScript\n\nInference: KV Cache acceleration + Async scheduling\n\nIntegrations: Claude MCP, Dify, Coze\n\nLicense: MIT (Self-hostable)\n\nSafety & Benchmarks:\n\nI‚Äôm using a 'Memory Safety Protocol' to check for source verification and attribution. Testing it against the LoCoMo dataset shows way better recall for preferences than standard top-k retrieval.\n\nIt‚Äôs still early and definitely has some rough edges. If you want to poke around, the GitHub is open and there‚Äôs a playground to test the extraction logic.\n\nRepo / Docs:\n\n\\- Github:  https://github.com/MemTensor/MemOS\n\n\\- Docs:  https://memos-docs.openmem.net/cn",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qoj60d/i_relied_on_stateless_retrieval_for_longform/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o21ybfn",
          "author": "transfire",
          "text": "‚ÄúThis extracts facts/prefs automatically in the background‚Äù   Is it running an second LLM in the background to do this?",
          "score": 1,
          "created_utc": "2026-01-27 17:40:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fvium",
          "author": "Sweet121",
          "text": "Glad to see more people admitting that standard RAG is failing for agents, i found it one year ago!",
          "score": 1,
          "created_utc": "2026-01-29 16:56:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoo1ho",
      "title": "Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qoo1ho/benchmark_of_qwen332b_reveals_12x_capacity_gain/",
      "author": "AIMultiple",
      "created_utc": "2026-01-27 19:35:07",
      "score": 26,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "We ran 12,000+ MMLU-Pro questions and 2,000 inference runs to settle the quantization debate. INT4 serves 12x more users than BF16 while keeping 98% accuracy.\n\nBenchmarked Qwen3-32B across BF16/FP8/INT8/INT4 on a single H100. The memory savings translate directly to concurrent user capacity. Went from 4 users (BF16) to 47 users (INT4) at 4k context. Full methodology and raw numbers here: (https://research.aimultiple.com/llm-quantization/).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qoo1ho/benchmark_of_qwen332b_reveals_12x_capacity_gain/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o25bhuy",
          "author": "fijuro",
          "text": "I'm considering switching to this model",
          "score": 1,
          "created_utc": "2026-01-28 03:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25co5c",
              "author": "frgal",
              "text": "the same",
              "score": 1,
              "created_utc": "2026-01-28 03:23:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cinjv",
          "author": "pbalIII",
          "text": "Worth separating capacity from speed. INT4 shrinks weights, so you mostly buy KV cache headroom and that becomes more concurrent contexts. But tokens per second and quality don't always move in lockstep, it depends on prompts and batching.\n\nIf you're switching, I'd run three quick checks.\n\n- Eval on your own prompt set, not just generic benchmarks\n- Latency at your target QPS and batch size\n- Quant recipe and calibration data, bad calibration can cause cliffs\n\nDo that and INT4 is usually the cleanest cost win.",
          "score": 1,
          "created_utc": "2026-01-29 03:34:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2cyi44",
          "author": "justron",
          "text": "Cool, nice writeup!  \n  \n\\- For the evaluation datasets, it isn't obvious to me whether the different quantizations generated different scores. You might consider putting the response quality, or benchmark scores, into their own chart.  \n  \n\\- The \"Evidence 1: BF16 Initialization\" and \"Evidence 2: GPTQ-Int4 Initialization\" sections in the article are identical--is that intentional?",
          "score": 1,
          "created_utc": "2026-01-29 05:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22xvte",
          "author": "Infamous_Knee3576",
          "text": "Nice work and white papers . How does one get job a firm like yours ??",
          "score": 1,
          "created_utc": "2026-01-27 20:14:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmf35d",
      "title": "How do LLMs ACTUALLY work?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qmf35d/how_do_llms_actually_work/",
      "author": "LordAntares",
      "created_utc": "2026-01-25 09:53:15",
      "score": 24,
      "num_comments": 46,
      "upvote_ratio": 0.78,
      "text": "I've heard the \"it just does autocomplete based on statistical analyses\" argument a million times. Everybody acts like it's self explanatory and obvious but I can't quite make the connection.\n\n  \nI understand if somebody asks \"what's Tokyo's population\", how it would get you an answer. However, sometimes it almost seems like understands questions and I know that's not the case. I'll give you a couple of examples:\n\n1.  The \"how many Rs in strawberry\" famous question. Though it used to fail that one, it seems like it attempts reasoning somehow. I don't understand how statistical data analysis would lead it to go back and forth with you trying to solve the riddle.  I'm sure nobody actually asked that question online and had conversations like that.\n2.  How does it do math? Again, the problems you ask it can get very specific with an untried combination of numbers. Clearly it does something more than predict the words, no?\n3.  I usually slam it on its coding abilities; specifically semantic understanding of what needs to be done. I can understand boiler plate code etc. but just sometimes when I ask it to debug what went wrong in my code, it actually provides a seemingly thoughtful answer, solving the problem on a \"thinking\" level. Did it just see that reply somewhere? But how could it have deduced that was the problem from the code, unless someone somewhere asked the same sentence before pasting the code?\n4.  I ask it to roleplay as a custom character for a video game or whatever. I give him a custom set of instructions and a background etc. It seems to reply in character, and when it tries to, for example, reference his home town, it's not just like \" `\"Been a while since I've been in \" + hometown + \".\"`. It kind of makes up lore about it or uses alternative ways to reference it. How does it do that?\n\n  \nI know it's not magic, but I don't understand how it works. The general \"it's just a glorified autocomplete\" doesn't satisfy my curiosity. Can somebody explain to me how it does seemingly semantic things?\n\nThanks.\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qmf35d/how_do_llms_actually_work/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1le2pp",
          "author": "UnbeliebteMeinung",
          "text": "[https://www.youtube.com/watch?v=D8GOeCFFby4](https://www.youtube.com/watch?v=D8GOeCFFby4)",
          "score": 26,
          "created_utc": "2026-01-25 10:00:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1llgrc",
              "author": "InTheEndEntropyWins",
              "text": "That's an amazing link, everyone should watch it.",
              "score": 3,
              "created_utc": "2026-01-25 11:05:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1llq33",
                  "author": "UnbeliebteMeinung",
                  "text": "I know. These questions come up all the time with these bad explanations \"its just the next token bro\".\n\nNow that you know the answer just copy the link when the next guy asks the same question :3",
                  "score": 3,
                  "created_utc": "2026-01-25 11:07:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mam60",
              "author": "LordAntares",
              "text": "This is fascinating, but essentially it says we haven't the slightest clue how they work.\n\nI also don't understand why they can decide to improve and pattern match what they haven't been coded to do.",
              "score": 2,
              "created_utc": "2026-01-25 14:04:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mcq87",
                  "author": "flextrek_whipsnake",
                  "text": "Yes, that is correct. We don't have a complete understanding of why this amount of scaling with these methods enables the models to achieve what they're able to achieve. That's the main reason why so many people, including myself, were skeptical of it for so long.\n\nAs always, the long answer is more complicated, but the short answer is we don't really know how they work.",
                  "score": 4,
                  "created_utc": "2026-01-25 14:16:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1n02ry",
                  "author": "roger_ducky",
                  "text": "The way I understood it is, it emulates the pattern matching part of the human brain. Not the same way, but close enough to work.\n\nWhat‚Äôs actually happening when generating is:\n\nRead full question. Think about first word to write and writes it.\n\nRead full question and first word, think about the second word to write.\n\nKeep repeating until it looks complete.",
                  "score": 1,
                  "created_utc": "2026-01-25 16:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1mx3kz",
              "author": "aletheus_compendium",
              "text": "thanks üôèüèª that's one i will save and share for sure. i've been giving this one out https://youtu.be/LPZh9BOjkQs \"Large Language Models explained briefly\" ü§ôüèª",
              "score": 1,
              "created_utc": "2026-01-25 15:56:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1n71ei",
          "author": "infamouslycrocodile",
          "text": "I can give you a more intuitive explanation: the model is ONLY trained to complete the next token given the current set of words preceding it - contextually it leads strongly to a very likely next word appearing. \n\nIf you focus on the next word completion / autocomplete you blind yourself to the preceding context. Have it complete the next word or sentence, then delete that sentence / play with the preceding context and see what other sentence comes out instead. \n\nDoing this enough and at different conversation lengths has the model learn what to pay attention to and how to inch closer to the correct result regardless. \n\nIt reconfigures its weights to achieve this, it's not learning the answers at that point, they're just a side effect of the main goal of learning how to be more likely to say the right thing.\n\n---\n\nBecause there's a finite set of weights to configure, the model has to come up with a good way to cram all that information in so it distills the knowledge and the techniques to get to the answers which happens to be similar to how we learn but less advanced.\n\nThis is why the models can get mixed up and hallucinate - \"The capital of Japan is Paris\" - the data is close together but not wired up correctly but it will get better with more training. \n\nInference time scaling is just a higher order autocomplete: perhaps there was another thing it learnt - \"The capital of France is Paris, but wait - I said Paris was the capital of Japan so that can't be right\" - it can use other things it has been trained on to connect concepts out loud, this might correlate highly to similar lines of reasoning that the model can use as a tool for the current line of thinking.",
          "score": 5,
          "created_utc": "2026-01-25 16:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lmtgs",
          "author": "consultant82",
          "text": "I think for the math part, reasoning + agentic tool combination is foundation. LLM reasons that ‚Äûmaybe I should use a calculator for given task‚Äú and a calc is invoked for given task. \n\nThe llm journey started all ‚Äûsimple‚Äú step by step with text2vec embeddings (semantic of tokens, enabling a meaning space), neural networks (‚Äûgive me input parameters given on known output, so I can learn predicting for given problem‚Äú), contextualized token prediction (transformer architecture) and this base foundation is now fed with lot of bells and whistles around it (tooling, more effective models, rag, ..).",
          "score": 4,
          "created_utc": "2026-01-25 11:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lo32i",
          "author": "GCoderDCoder",
          "text": "I stumbled across this the other day. She does a good job concisely explaining the different types of logic and how \"logic\" works with LLMs. I have peers who keep saying they're just pattern matching tools and I'm not for the AI hype but that's not a sufficient or fair description. \n\nI prefer describing them as complex text generators with emerging logic capabilities due to the science and art of how we use the text the model was trained on. Words have meaning so \"understanding\" or heavily connecting with relationships between words allows LLMs to generate value based on those relationships\n\nhttps://youtu.be/qXtNvfxBzlk",
          "score": 3,
          "created_utc": "2026-01-25 11:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oky6h",
          "author": "TheRealStepBot",
          "text": "To the degree that someone thinks that it‚Äôs fundamentally not what humans do, the explanation is wrong. But most ml people think that‚Äôs what humans do as well so when they use that phrase they are saying something different than most people hear. \n\nThe derisive it‚Äôs ‚Äújust‚Äù fancy autocomplete misunderstands almost every word in that sentence. It‚Äôs just fancy autocomplete in the same sense that aspects of what humans do is also just fancy autocomplete, and in that these models are almost certainly better at that aspect of cognition than at least the average person, if not most people.",
          "score": 3,
          "created_utc": "2026-01-25 20:12:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ldo3p",
          "author": "kubrador",
          "text": "it's still autocomplete, just autocomplete that's absurdly good at pattern matching across billions of examples. when you ask \"how many Rs in strawberry\" it's seen enough \"let me think through this letter by letter\" responses that it's learned the \\*pattern\\* of reasoning, not actual reasoning.",
          "score": 7,
          "created_utc": "2026-01-25 09:56:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26c3e4",
          "author": "r-3141592-pi",
          "text": "A few answers have offered good video explainers, but they often get lost in architectural details and miss the fundamental purpose of training deep neural networks.\n\nLet me try a different approach. Since early neural networks were inspired by the brain, let's start with how our brains process concepts.\n\nWhen you hear \"What is Tokyo's population?\", your brain recognizes the question format, identifies \"Tokyo\", and understands \"population\". You learned these concepts by forming neural pathways that encode information. \"Encoding\" simply means electrical signals flow through specific networks of neurons. As the signal passes through each connection, it gets modified, eventually forming a representation that can be associated with that concept. Each time you think \"Tokyo\", roughly the same group of neurons activates. The more you know about Tokyo, the more connections form, adding more semantic features to that signal. When you hear \"Tokyo\", parts of your brain light up pathways that encode \"Japan\", \"capital\", or \"Japanese food\". These pathways generate a combined signal representing everything you associate with the city. The same happens for syntax and the meaning of \"population\".\n\nThese encodings aren't precise. The same pathway might activate similarly for \"elephant\" and \"hippopotamus\". This process relies on competition in which the best selection wins. Your brain uses the surrounding words to suppress the irrelevant meaning and amplify the correct one. This competition between signals allows you to select the precise concept intended.\n\nHow do LLMs work? LLMs use a next-token prediction task to force the network to build internal representations that encode semantic information about each word and its relationships to other words. These representations are high-dimensional vectors that arise from the activations of artificial neurons, usually calculated as the nonlinear transformation of their weighted sum of inputs. After training, these vectors capture recognizable features. In this vector space, \"Tokyo\" sits close to other capital cities, near \"Japan\" and \"Japanese food\", but far from unrelated concepts.\n\nDeep architectures generalize features. As input like \"Tokyo\" moves through the network, early layers encode basic syntactic features. Later layers use those outputs to encode more complex conceptual meanings, until middle layers represent sophisticated concepts we associate with \"Tokyo\". For your question \"What is Tokyo's population?\", the network builds representations for (capital city) + (Japan) + (statistics) + (punctuation) and many more. These representations can be added together while preserving their meanings. This gives the final part of the network enough information to know that the most likely next-token prediction should look like a response to a question about Japan's capital city and should return a number.\n\nThe multiple semantic meanings are added to a residual stream in LLMs, which carries the meaning from the initial input. More contributions are added as different layers process it. Attention mechanisms look at the other words in the sentence (like \"population\"), calculating their importance and relationship to \"Tokyo\". Then the architecture takes that information and adds it to the original \"Tokyo\" vector. In real models, features are \"smeared\" across many dimensions, but let's assume they are clean for this example.\n\nSay dimensions encode specific features: [feat_1 = capital city, feat_2 = animal, feat_3 = punctuation, feat_4 = color, feat_5 = statistics]. \"Tokyo\" might generate [0.9, 0.1, 0, 0, 0], a question mark [0, 0, 1.2, 0.1, 0], and \"statistics\" [0.2, 0.1, 0, 0, 1]. Adding these gives [1.1, 0.2, 1.2, 0.1, 1], which includes high values in dimensions for identifying the most important features of your question.\n\nWhen internal representations match our understanding of concepts, you have a world model that can be used for prediction. This model enables reasoning through supervised training and reinforcement learning. During reinforcement learning with verified rewards, the model is encouraged to follow logical steps that lead to correct answers. Without human intervention, it begins to increase its use of strategies that help it solve problems. That's where the reasoning traces you may have seen come from: backtracking after spotting a mistake (\"wait...\"), recognizing a promising idea (\"aha!\"), breaking a difficult problem into smaller, more manageable parts, elaborating a plan to solve a problem, expressing uncertainty, and considering alternative approaches. Additional training allows them to use tools, search the web, and read relevant literature to understand the problem better before trying to solve it.\n\nThe better the world model in the base model, the better the network will be able to understand what you want, solve problems, and behave in ways that are compatible with the context provided.\n\nAs you can see, regardless of whether we are talking about biological brains or LLMs, the objective is identical: to create an internal representation accurate enough to predict what comes next. It's not magic, but the fact that it works so incredibly well is almost magical.\n\nI hope that helps.",
          "score": 2,
          "created_utc": "2026-01-28 07:28:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nbd36",
          "author": "insulaTropicalis",
          "text": "When a person says you that an LLM is just a sophisticate autocomplete, ask them what they know about linear transformations and non-convex optimization. If they can't answer the questions, they are just repeating concepts they don't understand that they read on social media.",
          "score": 3,
          "created_utc": "2026-01-25 16:57:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1m5o61",
          "author": "Ok-Lack-7216",
          "text": "The \"glorified autocomplete\" explanation is technically true but effectively useless because it ignores how the model decides what comes next.\n\nI actually just created a visual breakdown of this process that answers your specific examples:\n\n[https://youtu.be/x-XkExN6BkI](https://youtu.be/x-XkExN6BkI)\n\n1. The Strawberry Problem: This is a Tokenization issue. The AI doesn't see letters; it sees whole words (tokens) as single \"Lego bricks.\" It literally cannot \"see\" the letters inside the brick to count them.\n2. Roleplay & Coding: This works via the Attention Mechanism. The model doesn't just read left-to-right; it assigns a \"weight\" to previous instructions. When it generates a line of dialogue, it is mathematically \"attending\" to the character background you provided earlier, ensuring the prediction aligns with that context.\n\nIt‚Äôs not magic, but it is complex linear algebra. I traced a single prompt through the engine to show exactly how this works in the video.",
          "score": 2,
          "created_utc": "2026-01-25 13:36:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mb99s",
              "author": "LordAntares",
              "text": "I'm sorry, but this video is cringe.",
              "score": -2,
              "created_utc": "2026-01-25 14:08:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mibku",
                  "author": "Ok-Lack-7216",
                  "text": "Fair enough! I know the analogies (like the Grocery Store) aren't for everyone, but I wanted to try something different than the usual dry lectures. Thanks for giving it a shot anyway!",
                  "score": 3,
                  "created_utc": "2026-01-25 14:46:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1llcbt",
          "author": "InTheEndEntropyWins",
          "text": "The short answer is we don't know exactly how they work. We know the architecture, but how it actually works is based on its own learning and the networks are way too complex for us to understand what's it's learnt. But in some simple situations we have looked at the networks and understood what it's done.  \n\n\n>Sam Altman Says OpenAI Doesn‚Äôt Fully Understand How GPT Works Despite Rapid Progress\n‚ÄúWe certainly have not solved interpretability,‚Äù Altman said.\n[https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/](https://observer.com/2024/05/sam-altman-openai-gpt-ai-for-good-conference/)\n\n>During that training process, they learn their own strategies to solve problems. These strategies are encoded in the billions of computations a model performs for every word it writes. They arrive inscrutable to us, the model‚Äôs developers. **This means that we don‚Äôt understand how models do most of the things they do.**\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\nSo the Rs in strawberry is due to the fact it doesn't get each letter, the word strawberry is broken up into tokens like \"straw\" and \"berry\" and those turned into vectors. So all the LLM has is say two vectors and those vectors might not have anything about the letters in straw and berry. \n\n>How does it do math? \n\nThis is a really interesting question. Anthropic have done some studies on this exact question and for simple addition, they use a bespoke algorithm that has two parts an estimation part and an accuracy part. So it doesn't add up numbers like a human would normally do or how a human would program a computer would do. It's learnt this completely new method. \n\nIn terms of autocomplete, anthropic have demonstrated that it uses algorithms and multistep reasoning rather than just memorising data and looking things up. \n\n\n>Claude wasn't designed as a calculator‚Äîit was trained on text, not equipped with mathematical algorithms. Yet somehow, it can add numbers correctly \"in its head\". How does a system trained to predict the next word in a sequence learn to calculate, say, 36+59, without writing out each step?\n>\n>Maybe the answer is uninteresting: the model might have memorized massive addition tables and simply outputs the answer to any given sum because that answer is in its training data. Another possibility is that it follows the traditional longhand addition algorithms that we learn in school.\n>\n>Instead, we find that Claude employs multiple computational paths that work in parallel. One path computes a rough approximation of the answer and the other focuses on precisely determining the last digit of the sum. These paths interact and combine with one another to produce the final answer. Addition is a simple behavior, but understanding how it works at this level of detail, involving a mix of approximate and precise strategies, might teach us something about how Claude tackles more complex problems, too.\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\n\n>if asked \"What is the capital of the state where Dallas is located?\", a \"regurgitating\" model could just learn to output \"Austin\" without knowing the relationship between Dallas, Texas, and Austin. Perhaps, for example, it saw the exact same question and its answer during its training.\n>\nBut our research reveals something more sophisticated happening inside Claude. When we ask Claude a question requiring multi-step reasoning, we can identify intermediate conceptual steps in Claude's thinking process. In the Dallas example, we observe Claude first activating features representing \"Dallas is in Texas\" and then connecting this to a separate concept indicating that ‚Äúthe capital of Texas is Austin‚Äù. In other words, the model is¬†combining¬†independent facts to reach its answer rather than regurgitating a memorized response.\n[https://www.anthropic.com/news/tracing-thoughts-language-model](https://www.anthropic.com/news/tracing-thoughts-language-model)\n\nThat anthropic article is really good and has other examples, worth a read.\n\nSomeone else also pasted this link, so I'd just emphasise it's an amazing video worth watching.\n\n# The most complex model we actually understand\nhttps://www.youtube.com/watch?v=D8GOeCFFby4",
          "score": 3,
          "created_utc": "2026-01-25 11:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ln556",
              "author": "LordAntares",
              "text": "Thanks for the detailed reply. So it's not \"just a fancy autocomplete\". It might be a VERY fancy autocomplete tho.\n\nI will have to watch that video I guess.",
              "score": 1,
              "created_utc": "2026-01-25 11:20:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ltpyq",
                  "author": "InTheEndEntropyWins",
                  "text": ">Thanks for the detailed reply. So it's not \"just a fancy autocomplete\". It might be a VERY fancy autocomplete tho.\n\nIf you want to think about it in those terms, then when humans write and say stuff it's just \"VERY fancy autocomplete\".",
                  "score": 5,
                  "created_utc": "2026-01-25 12:14:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1lhnz8",
          "author": "guigouz",
          "text": "https://www.theguardian.com/technology/ng-interactive/2023/nov/01/how-ai-chatbots-like-chatgpt-or-bard-work-visual-explainer",
          "score": 1,
          "created_utc": "2026-01-25 10:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ng6dh",
          "author": "keebmat",
          "text": "[https://www.youtube.com/watch?v=xiqtrtKxUzY](https://www.youtube.com/watch?v=xiqtrtKxUzY)\n\n  \nyou dont want to know...",
          "score": 1,
          "created_utc": "2026-01-25 17:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nlsvk",
          "author": "crone66",
          "text": "For your second point.\n\n\nLLMs are actually really bad at math in terms of actually calculating. At the beginning they were completly useless. Then they added all simple math operations (add, subtract, devide, multiply) for two numbers of up to 4 digits to the training data. But obviously people quickly noticed that LLM can't deal with numbers with more than 5 digits. Therefore, they added a calculator tool that AI can use. Use any local ai model without a calculator tool and they will fail really quickly.",
          "score": 1,
          "created_utc": "2026-01-25 17:42:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nq0ua",
              "author": "LordAntares",
              "text": "So what is the top linked video about? It explained how llms do adding, and it wasn't hard coded as you say.",
              "score": 1,
              "created_utc": "2026-01-25 18:00:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1obauo",
                  "author": "crone66",
                  "text": "Yes it's not hard coded but it also doesn't understand how these math operations actually work otherwise it would be able to apply the rules to any given numbers but it can't. The reason is that the higher the number are the less likely they are even included in the trainingsset. S8nce LLM are a model that outputs the token with the highest probability the math equation (including the result) needs to be in the trainingset otherwise it essentialy outputs a random number. For \"Thinking models\" which is essentially the same base model but with a two step approach where you try to first break down the initial prompt by planning out a path to solution which provides useful context to shift the probability in the right direction. This context might include solving strategies such as removing zeros. Since 12+13 is in the training data it can solve it now the context or the final stage just needs to figure out that it has to add 3 zeros to the end which is most likely part of the training set too and even probability wise very likely. Therefore thinking models can even solve stuff thats not part of there trainings set.The fun thing is the llm actually don't know if the answer is correct but it will tell you it's correct with out a doubt. Since the thinking models will still fail for slightly more complex math questions calculator tool calls exist because LLMs are still really bad at calculating since they simply don't \"understand\" logic.\n\n\n(some parts are simplified to make easier to understand)",
                  "score": 2,
                  "created_utc": "2026-01-25 19:28:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1onpbv",
          "author": "TheRealStepBot",
          "text": "I‚Äôd say the way they work is related to information compression. You can‚Äôt complete the next word if you can‚Äôt internalize a semantic idea of what is being conveyed. The way to achieve this is by creating an information bottleneck where you force compression. \n\nGiven enough model complexity there really is no obvious limit to how complex an idea can be passed through this compression system. Consequently it‚Äôs auto complete yes but in order to complete every sentence ever written you must necessarily compress semantic understanding at some level.\n\nThe main reason people think these models ‚Äúdon‚Äôt understand‚Äù is actually not related to what they do but how we train them to do it. \n\nTheir internal representations are entangled and not perfectly able to be isolated. This in turn leads to ‚Äúhallucinations‚Äù and other artifacts that sometimes clouds what it is they are doing. \n\nThat we initialize and train them the way we do is mostly not because we can‚Äôt conceive of better ways but because of the pragmatic convenience of the way we do it and the quite good performance we do get. We are still in the infancy of having enough compute to apply these techniques as we do so convenience is still a significant driver.\n\nAs the tech matures however and the compute available continues to grow different implementations will be tried that will likely significantly improve performance without necessarily invalidating the ‚Äúit‚Äôs just fancy autocomplete‚Äù talking point.\n\nIn case you are interested I‚Äôm alluding to the ideas of continuous learning, and neuro evolutionary learning rather than direct gradient descent from random initialization. And that‚Äôs just assuming transformer architecture. There is so much more in the modern ml toolbox that just hast been brought to bear at the same scale as these models",
          "score": 1,
          "created_utc": "2026-01-25 20:24:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ovau1",
          "author": "Substantial_Sound272",
          "text": "A lot of folks don't know that the mathematical underpinnings of modern language models were discovered way back in 1948 in the absolutely incredible Shannon paper \"AMathematical Theory of Communication\". Section 6 titled \"Choice, Uncertainty, and Entropy\" is one of the most mind-blowing things I've ever read. He basically invents Information Theory and the concept of Entropy, which is directly used as a loss function for the training of modern Large Language Models, though he didn't know anything about neural networks or transformers or such.  Claude Shannon was an absolute genius. He is the person that Anthropic's Claude is named after, so if you really want to understand the math behind LLMs and you are at all mathematically inclined, check out section 6 of that paper!",
          "score": 1,
          "created_utc": "2026-01-25 20:57:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1p9rmu",
          "author": "justinhj",
          "text": "For 2. most models are not good at arithmetic. The ones you use on a website or app usually have built in function calling for that.",
          "score": 1,
          "created_utc": "2026-01-25 22:00:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pghst",
          "author": "wheres-my-swingline",
          "text": "This thread is enjoyable",
          "score": 1,
          "created_utc": "2026-01-25 22:29:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21lnuu",
          "author": "Available_Cream_752",
          "text": "Check out Andrej Karpathy's YT channel",
          "score": 1,
          "created_utc": "2026-01-27 16:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lkkix",
          "author": "Kiansjet",
          "text": "https://youtu.be/wjZofJX0v4M\n\n2. It has learned how math works via pattern recognition by seeing a ton of varied examples. Crucially, it ideally should NOT natively try to answer the question. It is a probabilistic prediction algorithm and not a precise calculator, so what it SHOULD do during one of its reasoning/tool call phases is to invoke a hard coded calculator tool or code executor to do the calculation for it. \n\nI'm only answering 2 because it's the only one I think I have a decent answer for.\n\nForgive me for what may come off as condescension but it really is the probabilistic behavior explanation you've heard. Similar to how you likely learned to speak your native language, it's given a ton of examples of what native coherent, valid text looks like and views it as a series of blocks of text called tokens, and learns what kinds of tokens show up around other kinds.\n\nIt does not need to have seen a exact example of a scenario you're hitting it with because during it's training phase it gains an understanding of how tokens relate to other tokens. It knows what the critical thinking chain of thought for debugging code generally looks like and for the language you're using, from there depending on the tools it's given it can try different solutions depending on what it thinks, probabilistically, whether you yourself can understand internally how it saw a similarity or not, is the solution to your problem.",
          "score": 1,
          "created_utc": "2026-01-25 10:57:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lnmz5",
          "author": "superawesomepandacat",
          "text": "Magnets how do they work",
          "score": 1,
          "created_utc": "2026-01-25 11:24:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkhuro",
      "title": "Adaptive execution control matters more than prompt or ReAct loop design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qkhuro/adaptive_execution_control_matters_more_than/",
      "author": "zennaxxarion",
      "created_utc": "2026-01-23 05:05:27",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I kept running into the same problem with agent systems whenever long multi-step tasks were involved. Issues with reliability kept showing up during agent evaluation, and then some runs were failing in ways it felt hard to predict. Plus the latency and cost variation just became hard to justify or control, especially when the tasks looked similar on paper.\n\nSo first I focused on prompt design and ReAct loop structure. I changed how the agent was told to reason and the freedom it had during each execution step. Some changes made steps in the process look more coherent and it did lead to fewer obvious mistakes earlier on.\n\nBut when the tasks became wider the failure modes kept appearing. The agent was drifting or looping. Or sometimes it would commit to an early assumption inside the ReAct loop and just keep executing even when later actions were signalling that reassessment was necessary.\n\nSo I basically concluded that refining the loop only changed surface behavior and there were still deeper issues with reliability.¬†\n\nInstead I shifted towards how execution decisions were handled over time at the orchestration layer. So because many agent systems lock their execution logic upfront and only evaluate outcomes after the run, you can‚Äôt intervene until afterwards, where the failure got baked in and you see wasted compute.\n\nIt made sense to intervene during execution instead of after the fact because then you can allocate TTC dynamically while the trajectories unfold. I basically felt like that had a much larger impact on the reliability. It shifted the question from why an agent failed to why the system was allowing an unproductive trajectory to continue unchecked for so long.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qkhuro/adaptive_execution_control_matters_more_than/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o17gmwm",
          "author": "PuzzleheadedTooth112",
          "text": "This feels like the same argument people had about monoliths years ago. You can keep making the monolith smarter or you can break the work up so failure doesn‚Äôt cascade. Most agent setups still look like monoliths to me.",
          "score": 1,
          "created_utc": "2026-01-23 08:50:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17h8fi",
          "author": "DrPickman",
          "text": "Definitely some food for thought. At some point I basically assumed long runs are too brittle for now and I should park it until the future. Just moved to shorter jobs with checkpoints and handoff between runs after finding the models weren‚Äôt impacting much.",
          "score": 1,
          "created_utc": "2026-01-23 08:55:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17hqqu",
          "author": "Intelligent_Front_37",
          "text": "TBH we stopped car‚Å§ing ab‚Å§out long tasks and decided it‚Äôs the task that‚Äôs the issue. If we can‚Äôt surf‚Å§ace prog‚Å§ress within a few minutes the task has to change shape. We got impr‚Å§oved reliab‚Å§ility after changing the problem instead of the agent.",
          "score": 1,
          "created_utc": "2026-01-23 09:00:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bko9f",
          "author": "pbalIII",
          "text": "Hit a similar wall last year. Prompt tweaks felt productive until tasks got wide enough that the agent started looping on stale assumptions.\n\nThe shift to runtime intervention changed what we measured. Instead of asking why did this fail, we started tracking how long did we let it keep going. Turns out most costly failures were obvious 3-4 steps before they cratered... the system just had no mechanism to reassess mid-run.\n\nOne pattern that helped: graduated containment. Monitor mode first, then restrict planning if risk scores climb, then pull tool access. Lets you calibrate intervention aggressiveness per task type instead of binary halt-or-continue.",
          "score": 1,
          "created_utc": "2026-01-23 22:17:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnpgcr",
      "title": "Stop manually iterating on agent prompts: I built an open-source offline analyzer based on Stanford's ACE that extracts prompt improvements from execution traces",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "author": "cheetguy",
      "created_utc": "2026-01-26 19:04:50",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "Some of you might have seen my [previous post](https://reddit.com/r/LLMDevs/comments/1obp91s/i_opensourced_stanfords_agentic_context/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: agentic system prompting from agent traces.\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's historical execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insight\n\n**How this compares to DSPy/GEPA:**\n\nWhile DSPy works best with structured data (input/output pairs), ACE is designed to work directly on execution traces (logs, conversations, markdown files) and keeps humans in the loop for review. Compared to GEPA, the ACE paper was able to show significant improvements on benchmarks.\n\n**Try it yourself:** [https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear your feedback if you do try it out",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o20ricf",
          "author": "JustViktorio",
          "text": "Why not to make a CLI tool from that so it could be callable in Claude Code / Codex / Open Code runtime?",
          "score": 2,
          "created_utc": "2026-01-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o217fmj",
              "author": "cheetguy",
              "text": "We're working on this. Hopefully we can release in the next couple of days. Join our Discord to stay updated: [https://discord.com/invite/mqCqH7sTyK](https://discord.com/invite/mqCqH7sTyK)",
              "score": 1,
              "created_utc": "2026-01-27 15:43:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qliszu",
      "title": "Enterprise data is messy, how do you make it work for AI?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qliszu/enterprise_data_is_messy_how_do_you_make_it_work/",
      "author": "chakratones",
      "created_utc": "2026-01-24 09:38:09",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.92,
      "text": "So pulling data from Salesforce, NetSuite, whatever enterprise systems you're stuck with that part's easy. It's what comes after that's a nightmare.\n\nYou extract everything and now you've got these giant tables, JSON files nested like Russian dolls, and absolutely zero context about what any of it means. Even the fancy LLMs just kinda... stare at it blankly. They can't reason over data when they don't know what \"field\\_7829\" actually represents or how it relates to anything else.\n\nCame across¬†[this article](https://thenewstack.io/how-precog-adds-business-context-to-make-enterprise-data-ai-ready/)¬†talking about adding business context early in the pipeline instead of trying to fix it later but I'm curious, what's actually working for you all?\n\nAre you building out semantic layers? Going heavy on NL to SQL? Experimenting with RAG setups? Or have you just accepted that AI answers on enterprise data are gonna be inconsistent at best?\n\nFeel like everyone's solving this differently and I'd love to hear what's actually holding up in production vs what sounds good in theory",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qliszu/enterprise_data_is_messy_how_do_you_make_it_work/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1eh7g1",
          "author": "Miclivs",
          "text": "Shameless plug, [clarityq.ai](https://www.clarityq.ai/), this is our specialization. tldr: RAG sucks, agentic search is way better, semantic layers suck, semantic catalogs (or the equivalent of property graphs) are way better. Data modeling is hard, ongoing maintenance is hard.",
          "score": 1,
          "created_utc": "2026-01-24 09:54:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ewxc0",
          "author": "siggywithit",
          "text": "Yup. Getting data out is the easy part.  Making it useful is where we spend the bulk of our time. Seen lots of chatter about this.  Anyone using it in production?",
          "score": 1,
          "created_utc": "2026-01-24 12:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gvbx9",
          "author": "Lost-Bathroom-2060",
          "text": "Interesting. I replied here to follow the thread",
          "score": 1,
          "created_utc": "2026-01-24 18:18:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1iz92j",
          "author": "tuffunny",
          "text": "We gave up on trying to make AI work directly with our raw enterprise data and just built a data warehouse with proper modeling first. I know that's not the sexy AI answer everyone wants to hear, but you can't skip the fundamentals.\n\nOnce we had clean dimensional models with proper naming conventions and documentation, THEN we pointed AI at it. Works way better. Turns out \"garbage in, garbage out\" applies to AI too, who knew?",
          "score": 1,
          "created_utc": "2026-01-25 00:18:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1izwyx",
          "author": "Zachariah1st",
          "text": "We use dbt to add descriptions/tags to everything, then feed that metadata to the LLM. Way better than raw table dumps. The descriptions are literally just this is ARR or this connects to customers via account\\_id",
          "score": 1,
          "created_utc": "2026-01-25 00:22:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j0v74",
          "author": "Ok_Spinach_4348",
          "text": "The real answer nobody wants to hear: you need a data catalog first. We use Atlan. Yeah it's more tooling and more $$ but AI without knowing your lineage/definitions is just expensive guessing.",
          "score": 1,
          "created_utc": "2026-01-25 00:26:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkzkn2",
      "title": "Mirascope: Typesafe, Pythonic, Composable LLM abstractions",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qkzkn2/mirascope_typesafe_pythonic_composable_llm/",
      "author": "teamdandelion",
      "created_utc": "2026-01-23 18:58:56",
      "score": 11,
      "num_comments": 16,
      "upvote_ratio": 0.92,
      "text": "Hi everyone! I'm an at Mirascope, a small startup shipping open-source LLM infra. We just shipped v2 of our open-source Python library for typesafe LLM abstractions, and I'd like to share it.\n\n*TL;DR: This is a Python library with solid typing and cross-provider support for streaming, tools, structured outputs, and async, but without the overhead or assumptions of being a framework. Fully open-source and MIT licensed.*\n\nAlso, advance note: All em-dashes in this post were written by hand. It's option+shift+dash on a Macbook keyboard ;)\n\nIf you've felt like LangChain is too heavy and LiteLLM is too thin, Mirascope might be what you're looking for. It's not an \"agent framework\"‚Äîit's a set of abstractions so composable that you don't actually need one. Agents are just tool calling in a while loop.\n\nAnd it's got 100% test coverage, including cross-provider end-to-end tests for every features that use VCR to replay real provider responses in CI.\n\nThe pitch: How about a low-level API that's typesafe, Pythonic, cross-provider, exhaustively tested, and intentionally designed?  \n\n\nMirascope's focus is on typesafe, composable abstractions. The core concepts is you have an `llm.Model` that generates `llm.Response`s, and if you want to add tools, structured outputs, async, streaming, or MCP, everything just clicks together nicely. Here are some examples:\n\n    from mirascope import llm\n    \n    model: llm.Model = llm.Model(\"anthropic/claude-sonnet-4-5\")\n    response: llm.Response = model.call(\"Please recommend a fantasy book\")\n    print(response.text())\n    # > I'd recommend The Name of the Wind by Patrick Rothfuss...\n\nOr, if you want streaming, you can use `model.stream(...)`  along with `llm.StreamResponse`:\n\n    from mirascope import llm\n    \n    model: llm.Model = llm.Model(\"anthropic/claude-sonnet-4-5\")\n    response: llm.StreamResponse = model.stream(\"Do you think Pat Rothfuss will ever publish Doors of Stone?\")\n    \n    for chunk in response.text_stream():\n      print(chunk, flush=True, end=\"\")\n\nEach response has the full message history, which means you can continue generation by calling \\`response.resume\\`:\n\n    from mirascope import llm\n    \n    response = llm.Model(\"openai/gpt-5-mini\").call(\"How can I make a basil mint mojito?\")\n    print(response.text())\n    \n    response = response.resume(\"Is adding cucumber a good idea?\")\n    print(response.text())\n\n`Response.resume` is a cornerstone of the library, since it abstracts state tracking in a very predictable way. It also makes tool calling a breeze. You define tools via the `@llm.tool` decorator, and invoke them directly via the response.\n\n    from mirascope import llm\n    \n    @llm.tool\n    def exp(a: float, b: float) -> float:\n        \"\"\"Compute an exponent\"\"\"\n        return a ** b \n    \n    model = llm.Model(\"anthropic/claude-haiku-4-5\")\n    response = model.call(\"What is (42 ** 3) ** 2?\", tools=[exp])\n    \n    while response.tool_calls:\n      print(f\"Calling tools: {response.tool_calls}\")\n      tool_outputs = response.execute_tools()\n      response = response.resume(tool_outputs)\n    \n    print(response.text())\n\nThe `llm.Response` class also allows handling structured outputs in a typesafe way, as it's generic on the structured output format. We support primitive types as well as Pydantic `BaseModel` out of the box:\n\n    from mirascope import llm \n    from pydantic import BaseModel\n    \n    class Book(BaseModel):\n        title: str\n        author: str\n        recommendation: str\n    \n    # nb. the @llm.call decorator is a convenient wrapper.\n    # Equivalent to model.call(f\"Recommend a {genre} book\", format=Book)\n    \n    @llm.call(\"anthropic/claude-sonnet-4-5\", format=Book)\n    def recommend_book(genre: str):\n      return f\"Recommend a {genre} book.\"\n    \n    response: llm.Response[Book] = recommend_book(\"fantasy\")\n    book: Book = response.parse()\n    print(book)\n    \n\nThe upshot is that if you want to do something sophisticated‚Äîlike a streaming tool calling agent‚Äîyou don't need a framework, you can just compose all these primitives.\n\n    from mirascope import llm\n    \n    @llm.tool\n    def exp(a: float, b: float) -> float:\n        \"\"\"Compute an exponent\"\"\"\n        return a ** b \n    \n    @llm.tool\n    def add(a: float, b: float) -> float:\n        \"\"\"Add two numbers\"\"\"\n        return a + b \n    \n    model = llm.Model(\"anthropic/claude-haiku-4-5\")\n    response = model.stream(\"What is 42 ** 4 + 37 ** 3?\", tools=[exp, add])\n    \n    while True:\n        for chunk in response.pretty_stream():\n            print(chunk, flush=True, end=\"\")\n        if response.tool_calls:\n          tool_output = response.execute_tools()\n          response = response.resume(tool_output) \n        else:\n            break # Agent is finished\n\nI believe that if you give it a spin, it will delight you, whether you're coming from the direction of wanting more portability and convenience than using raw provider SDKs, or wanting more hands-on control than the big agent frameworks. These examples are all runnable, you can run`uv add \"mirascope[all]\"`, and set API keys.\n\nYou can read more in the [docs](https://mirascope.com/docs/learn/llm/quickstart), see the source on [GitHub](https://github.com/Mirascope/mirascope/tree/main), or join our [Discord](https://mirascope.com/discord-invite). Would love any feedback and questions :)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qkzkn2/mirascope_typesafe_pythonic_composable_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1ahy89",
          "author": "MaticPecovnik",
          "text": "I like it at first glance. But why choose your framework if I can use pydantic-ai? It seems quite similar in design. What sets you apart?",
          "score": 5,
          "created_utc": "2026-01-23 19:14:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1an8c1",
              "author": "teamdandelion",
              "text": "PydanticAI shares a lot of values (e.g. type safety and structured output support), and both make it easy to write powerful agents. The difference is in philosophy of how you get there. Pydantic AI uses agents as the core primitive, whereas in Mirascope the primitives are one step \"more atomic\" at the level of LLM models, calls, and requests. Rather than providing a fixed agent as a primitive, we've just made it really easy to roll your own Agent.\n\nMirascope is one step \"closer to metal\" (ie. direct control over the LLM requests you're making, just with really nice APIs), whereas Pydantic gets convenience by going up one layer of abstraction.\n\nFor a lot of use cases both will serve well, but Mirascope takes more of an \"onion\" approach where it's easy to peel back a layer and get more direct control over LLM execution. The upshot is its easy to write Pydantic-style agents using Mirascope, but you also get flexibility over exactly what happens in a way that otherwise wouldn't exist.",
              "score": 5,
              "created_utc": "2026-01-23 19:39:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1aoz7v",
                  "author": "wbakst",
                  "text": "\\+1\n\nHi! I'm another author of Mirascope. I've spent a lot of time in the AI space developing open-source tools, and one thing that has been consistent throughout is that at some point I always want to break glass. I want to be able to seamlessly move up and down the interfaces of the tool as I need.\n\nWe didn't want an agent framework, we wanted the tools to easily build our own agent harnesses without abstraction hell. We built Mirascope because we felt no such tool existed yet. We wanted \"breaking glass\" to be top of mind in our development philosophy, and I think we've done a pretty good job of this.\n\nI think you'll feel the love and care we've put into everything we've built. I'm curious to hear what you think, and we always welcome any and all feedback with open arms!",
                  "score": 3,
                  "created_utc": "2026-01-23 19:48:06",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1dntoq",
                  "author": "MaticPecovnik",
                  "text": "Ok I get it and I agree.\n\nWhat I really like about pydantic ai is their focus od dependency injection and testing with their overrides. How do you do unit tests in Mirascope, assuming you don‚Äôt want to call LLMs durnt unit tests.",
                  "score": 2,
                  "created_utc": "2026-01-24 05:37:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1aidfb",
          "author": "langcuck",
          "text": "\"If you've felt like LangChain is too heavy\"\n\nThat's a nice way of putting it lol",
          "score": 3,
          "created_utc": "2026-01-23 19:16:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1axbjd",
              "author": "ChanceKale7861",
              "text": "Bahahahahhahaah‚Ä¶\n\nYou win. ü•á üòÇüôå",
              "score": 1,
              "created_utc": "2026-01-23 20:27:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1axr73",
                  "author": "langcuck",
                  "text": "just being honest ü§∑üèª‚Äç‚ôÇÔ∏è",
                  "score": 2,
                  "created_utc": "2026-01-23 20:29:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c597s",
          "author": "danigoncalves",
          "text": "You don't do that to somebody like me who was about to start a new AI project and thought about using PydanticAI. I really liked what I saw - it seems simple with a low learning curve, and the fact that you lay down the foundation with `llm.Model` and allow us to compose only the abstractions we need when we need them is very important when applications tend to become bigger. Optimizing and improving these parts atomically (one at a time) becomes crucial.",
          "score": 2,
          "created_utc": "2026-01-24 00:05:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c8xo9",
              "author": "wbakst",
              "text": "You're exactly who we built this for :)\n\nExcited for you to give it a try and let us know what you think!!",
              "score": 2,
              "created_utc": "2026-01-24 00:24:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ca1ii",
                  "author": "danigoncalves",
                  "text": "You bet :)",
                  "score": 2,
                  "created_utc": "2026-01-24 00:30:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1feynr",
                  "author": "danigoncalves",
                  "text": "I was messing around with a few examples and had two feelings (might be wrong). \n\n1. For me, the best frameworks are those where the source code itself serves as documentation. This is one of them.\n\n2. Is it me or instead of promoting this as an alternative to frameworks as pydanticAI, it can also be promoted as a sidecar for agents or other specific purpose frameworks? Part of the composable abstractions advantage is that if you feel that it serves you better have a higher abstract for agents, just plugin another framework and work with both. I think the flexibility is a huge win for the applications that start small and unscoped and grow into something more specific in the future.",
                  "score": 2,
                  "created_utc": "2026-01-24 14:12:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlqqcs",
      "title": "Self-contained npm installable WASM-based Alpine Linux VM for agents",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qlqqcs/selfcontained_npm_installable_wasmbased_alpine/",
      "author": "schmuhblaster_x45",
      "created_utc": "2026-01-24 16:02:12",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I've always thought that it would be great to have small Linux VM that could be integrated and deployed with minimal efforts and dependencies. So thanks to the container2wasm project (https://github.com/container2wasm/container2wasm) and Opus 4.5 I was able to build a small library that gives you just that.   \n  \nHere it is: [https://github.com/deepclause/agentvm](https://github.com/deepclause/agentvm)\n\nIt was quite fascinating to see Opus build an entire user mode network stack in Javascript, then also sobering to watch it try to fix the subtle bugs that it introduced, all while burning though my tokens....eventually it worked though :-)\n\nAnyways, I thought this might be useful, so I am sharing it here.\n\n",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qlqqcs/selfcontained_npm_installable_wasmbased_alpine/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1h4lu0",
          "author": "qa_anaaq",
          "text": "What would some use cases be for this, in terms of agent usage? I‚Äôm trying to understand why an agent would need this as a tool. Sandboxing code executions?",
          "score": 1,
          "created_utc": "2026-01-24 18:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j70dh",
              "author": "schmuhblaster_x45",
              "text": "Yes, sandboxing code executions if you don't have a container or other means available would be one reason. Also, not all systems or environments come with a bash and Python interpreter.",
              "score": 1,
              "created_utc": "2026-01-25 00:59:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1i3kpm",
          "author": "tisDDM",
          "text": "Both things - c2w and agentvm are great ideas !!!\n\nI directly starred the repo. Maybe it will be the solution to a future problem.",
          "score": 1,
          "created_utc": "2026-01-24 21:39:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1q80bl",
          "author": "promethe42",
          "text": "Outstanding work. I've been building WASM component for Python and other script runners. I desperately want a WASM interface/component to a containerized Linux. This is one of the key components to safe local and distributed agents.",
          "score": 1,
          "created_utc": "2026-01-26 00:39:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qsp50",
              "author": "schmuhblaster_x45",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-01-26 02:23:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1t5nxq",
                  "author": "promethe42",
                  "text": "You motivated me to do this: [https://github.com/container2wasm/container2wasm/pull/565](https://github.com/container2wasm/container2wasm/pull/565)\n\nI have a running Alpine WASM component running now!",
                  "score": 1,
                  "created_utc": "2026-01-26 12:50:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qm6811",
      "title": "Fine-tuning LLaMA 1.3B on insurance conversations failed badly - is this a model size limitation or am I doing something wrong?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qm6811/finetuning_llama_13b_on_insurance_conversations/",
      "author": "ZaRyU_AoI",
      "created_utc": "2026-01-25 02:09:23",
      "score": 10,
      "num_comments": 19,
      "upvote_ratio": 0.87,
      "text": "TL;DR:\nFine-tuned LLaMA 1.3B (and tested base 8B) on ~500k real insurance conversation messages using PEFT. Results are unusable, while OpenAI / OpenRouter large models work perfectly.\nIs this fundamentally a model size issue, or can sub-10B models realistically be made to work for structured insurance chat suggestions?\nLocal model preferred, due to sensitive PII.\n\n\n\nSo I‚Äôm working on an insurance AI project where the goal is to build a chat suggestion model for insurance agents.\nThe idea is that the model should assist agents during conversations with underwriters/customers, and its responses must follow some predefined enterprise formats (bind / reject / ask for documents / quote, etc.).\nBut we require an in-house hosted model (instead of 3rd party APIs) due to the senaitive nature of data we will be working with (contains PII, PHI) and to pass compliance tests later.\n\nI fine-tuned a LLaMA 1.3B model (from Huggingface) on a large internal dataset:\n- 5+ years of conversational insurance data\n- 500,000+ messages\n- Multi-turn conversations between agents and underwriters\n- Multiple insurance subdomains: car, home, fire safety, commercial vehicles, etc.\n- Includes flows for binding, rejecting, asking for more info, quoting, document collection\n- Data structure roughly like:\n{ case metadata + multi-turn agent/underwriter messages + final decision }\n- Training method: PEFT (LoRA)\n- Trained for more than 1 epoch, checkpointed after every epoch\n- Even after 5 epochs, results were extremely poor\n\nThe fine-tuned model couldn‚Äôt even generate coherent, contextual, complete sentences, let alone something usable for demo or production.\n\nTo sanity check, I also tested:\n- Out-of-the-box LLaMA 8B from Huggingface (no fine-tuning) - still not useful\n- OpenRouter API (default large model, I think 309B) - works good\n- OpenAI models - performs extremely well on the same tasks\n\nSo now I‚Äôm confused and would really appreciate some guidance.\n\nMy main questions:\n1. Is this purely a parameter scale issue?\nAm I just expecting too much from sub-10B models for structured enterprise chat suggestions?\n2. Is there realistically any way to make <10B models work for this use case?\n(With better formatting, instruction tuning, curriculum, synthetic data, continued pretraining, etc.)\n3. If small models are not suitable, what‚Äôs a practical lower bound?\n34B? 70B? 100B? 500B?\n4. Or am I likely doing something fundamentally wrong in data prep, training objective, or fine-tuning strategy?\n\nRight now, the gap between my fine-tuned 1.3B/8B models and large hosted models is massive, and I‚Äôm trying to understand whether this is an expected limitation or a fixable engineering problem.\n\nAny insights from people who‚Äôve built domain-specific assistants or agent copilots would be hugely appreciated.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qm6811/finetuning_llama_13b_on_insurance_conversations/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1l9ir5",
          "author": "AI_Data_Reporter",
          "text": "Sub-10B models require EMA stability and curriculum-based phasing to mitigate catastrophic forgetting in domain-specific tasks. 1.3B parameters lack the capacity for multi-turn reasoning without general-to-specific alignment. PEFT alone fails if the base model lacks instruction-following priors. Effective domain adaptation for insurance requires continued pre-training on cleaned corpora before LoRA application. Compute is not the bottleneck; it is the lack of stable weight updates.",
          "score": 8,
          "created_utc": "2026-01-25 09:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n2v21",
              "author": "kompania",
              "text": "Incredible post. A roadmap for this problem and others like it. Thank you for such substantive information.",
              "score": 1,
              "created_utc": "2026-01-25 16:21:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kpquj",
          "author": "No-Concentrate4531",
          "text": "Did you use the correct chat template? Also, there is also the case where you have to mitigate catastrophic forgetting. Finally, have you adjusted the rank of your LoRA matrices?\n\nAnother issue is that you are training on the base LLMs with no instruction following. You need to train that in as well. \n\nWhen it comes to finetuning LLMs, you need to be mindful of the experimental tricks to mitigate the limitations of LoRA.\n\nAlso, when running inferences, you need to ensure the model respects your system prompts if any.\n\n\nHere is a good article: https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms",
          "score": 7,
          "created_utc": "2026-01-25 06:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lmbph",
              "author": "ZaRyU_AoI",
              "text": "Yes, we have a custom chat template for our format.\nWe did have custom rank, alpha, dropout and target modules for our LoRA finetuning.\nThanks for the article. Might revisit our custom rank after going through this article.",
              "score": 1,
              "created_utc": "2026-01-25 11:13:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1kozc5",
          "author": "Lyuseefur",
          "text": "Follow unsloth guide. Try Qwen",
          "score": 6,
          "created_utc": "2026-01-25 06:25:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l5d2g",
          "author": "ampancha",
          "text": "The 1.3B parameter count is almost certainly insufficient for multi-turn insurance reasoning with structured output constraints; even with perfect training, small models struggle with long-context dependencies and format compliance. Your compliance concern is separate from model quality: regardless of which model you land on, PII/PHI in inference pipelines needs its own control architecture for audit trails, retrieval filtering, and data isolation. Sent you a DM with more detail.",
          "score": 5,
          "created_utc": "2026-01-25 08:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l1sxr",
          "author": "Imaginary-Ad-2308",
          "text": "A step back : You should explore all other methods that don't require training first. Fine-tuning is a headache; by the time you see improvements after six months, a new open-weight model will likely be released that outperforms your custom version anyway.",
          "score": 2,
          "created_utc": "2026-01-25 08:11:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rx9kx",
          "author": "burntoutdev8291",
          "text": "Did you try on the non fine tuned model to see how it performed? Since you're doing a constrained task, i think some forgetting is fine.",
          "score": 1,
          "created_utc": "2026-01-26 06:38:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t4l2d",
          "author": "bad_gambit",
          "text": "> Is this purely a parameter scale issue? Am I just expecting too much from sub-10B models for structured enterprise chat suggestions? \n\nNot really, I've helped deploy some customer service bot, and most of the time using Qwen3-8B + RAG ( + some relevant QA examples) works okay. Older, smaller models, such as Llama-2 1.3B, is *definitely* outdated and wont handle it.\n\nFrom what i see on Huggingface you're [referring](https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B) to a pruned Llama-2? Llama-2 is *ancient* in LLM timeframe!\n\nYou need to use a more recent model. Try Qwen3 4B Instruct or Ministral 3 3B or LFM 2.5 1.2B. That LFM has a slightly constrictive license though, and only permitted comercial use of <$10M revenue corporation.\n\nSmall model can work for simple Q&A, more recent ones are better at tool calling, and better in loading relevant information into context.\n\n> Is there realistically any way to make <10B models work for this use case? (With better formatting, instruction tuning, curriculum, synthetic data, continued pretraining, etc.) \n\nYes, small models can work fine on short messages, but it will need more wrangling and \"stiffer\" prompting (you need to be much more detailed on the system prompt), oftentimes even [double prompting](https://arxiv.org/abs/2512.14982).\n\n> If small models are not suitable, what‚Äôs a practical lower bound? 34B? 70B? 100B? 500B?\n\nNot sure about a lower bound, but i've used Devstral-Small-2 24B for local agent and it works fine, had also used ministral 14B for local rag and its slighly (noticably) worse at toolcall. So maybe around these parameter size (14B > X > 24B)?\n\n> Or am I likely doing something fundamentally wrong in data prep, training objective, or fine-tuning strategy? \n\nWell, you could\n1. Re-evaluate whether the cost of finetuning (and labour) are worthwhile compared to spending more on local hardware to host a larger model.\n2. Rethink whether a RAG system work better (it might)\n3. Find other avenues a, eg. ZDR provider (as an enterprise) and only host the data locally",
          "score": 1,
          "created_utc": "2026-01-26 12:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uyt2j",
              "author": "ZaRyU_AoI",
              "text": "Really informative, thanks.\n1. Management is looking into it.. could go either way atp honestly.\n2. A RAG module is in the development pipeline currently.. let's hope it works, fingers crossed\n3. Client is looking for a personalised, fine-tuned, locally hosted AI. Although we are outsourcing parts of it to ZDRs, we still need to show involvement of our fine-tuned model in the pipeline.",
              "score": 1,
              "created_utc": "2026-01-26 17:59:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1upsfi",
          "author": "Confident-Ad-3212",
          "text": "How many samples are you using? DM me if you want advice",
          "score": 1,
          "created_utc": "2026-01-26 17:19:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kycsl",
          "author": "Blahblahblakha",
          "text": "If you‚Äôre trying to get conversational ability, sub 8b models are not appropriate for a fine tune. I had to do the same for style transfer in the marketing domain. Sub 8b models fail at conversational ability and instruction following after Lora. Havent explored it in depth but i think it could be because low param models primarily end up learning output formats instead of the pattern in the fine tuning data. \n\nI experimented will almost all major sub 8b models. But only found acceptable conversational quality starting with qwen2.5-14b",
          "score": 1,
          "created_utc": "2026-01-25 07:42:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1li3ck",
          "author": "danish334",
          "text": "Start finetuning bigger models and if you encounter same issue then it's your setup that has problem (like your dataset).",
          "score": 0,
          "created_utc": "2026-01-25 10:35:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1lkp3q",
              "author": "ZaRyU_AoI",
              "text": "Not a cost effective way.. but I see your point.",
              "score": 1,
              "created_utc": "2026-01-25 10:58:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1llhio",
                  "author": "danish334",
                  "text": "Btw, which platform you used fot FT?",
                  "score": 1,
                  "created_utc": "2026-01-25 11:05:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1kk4o7",
          "author": "nmrk",
          "text": ">..\\[T\\]he boy began to delight in his daring flight, and abandoning his guide, drawn by desire for the heavens, soared higher. His nearness to the devouring sun softened the fragrant wax that held the wings: and the wax melted: he flailed with bare arms, but losing his oar-like wings, could not ride the air. Even as his mouth was crying his father‚Äôs name, it vanished into the dark blue sea, the Icarian Sea, called after him. The unhappy father, now no longer a father, shouted ‚ÄòIcarus, Icarus where are you? Which way should I be looking, to see you?‚Äô ‚ÄòIcarus‚Äô he called again. Then he caught sight of the feathers on the waves, and cursed his inventions. He laid the body to rest, in a tomb, and the island was named Icaria after his buried child.",
          "score": -8,
          "created_utc": "2026-01-25 05:49:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ql8b52",
      "title": "Reverse Engineering a $500M Mystery: From HashHop to Memory-Augmented Language Models",
      "subreddit": "LLMDevs",
      "url": "https://huggingface.co/blog/codelion/reverse-engineering-magic-hashhop",
      "author": "asankhs",
      "created_utc": "2026-01-24 00:41:32",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ql8b52/reverse_engineering_a_500m_mystery_from_hashhop/",
      "domain": "huggingface.co",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1ql6a1h",
      "title": "I Need help from actual ML Enginners",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1ql6a1h/i_need_help_from_actual_ml_enginners/",
      "author": "Dangerous_Young7704",
      "created_utc": "2026-01-23 23:16:57",
      "score": 9,
      "num_comments": 34,
      "upvote_ratio": 0.84,
      "text": "**Hey, I revised this post to clarify a few things and avoid confusion.**\n\nHi everyone. Not sure if this is the right place, but I‚Äôm posting here and in the ML subreddit for perspective.\n\n**Context**  \nI run a small AI and automation agency. Most of our work is building AI enabled systems, internal tools, and workflow automations. Our current stack is mainly Python and n8n, which has been more than enough for our typical clients.\n\nRecently, one of our clients referred us to a much larger enterprise organization. I‚Äôm under NDA so I can‚Äôt share the industry, but these are organizations and individuals operating at a 150M$ plus scale.\n\nThey want:\n\n* A private, offsite web application that functions as internal project and operations management software\n* A custom LLM powered system that is heavily tailored to a narrow and proprietary use case\n* Strong security, privacy, and access controls with everything kept private and controlled\n\nTo be clear upfront, we are not planning to build or train a foundation model from scratch. This would involve using existing models with fine tuning, retrieval, tooling, and system level design.\n\nThey also want us to take ownership of the technical direction of the project. This includes defining the architecture, selecting tooling and deployment models, and coordinating the right technical talent. We are also responsible for building the **core web application and frontend** that the LLM system will integrate into.\n\nThis is expected to be a multi year engagement. Early budget discussions are in the 500k to 2M plus range, with room to expand if it makes sense.\n\n**Our background**\n\n* I come from an IT and infrastructure background with USMC operational experience\n* We have experience operating in enterprise environments and leading projects at this scale, just not in this specific niche use case\n* Hardware, security constraints, and controlled environments are familiar territory\n* I have a strong backend and Python focused SWE co founder\n* We have worked alongside ML engineers before, just not in this exact type of deployment\n\nWhere I‚Äôm hoping to get perspective is mostly around **operational and architectural decisions**, not fundamentals.\n\n**What I‚Äôm hoping to get input on**\n\n1. **End to end planning at this scope** What roles and functions typically appear, common blind spots, and things people underestimate at this budget level\n2. **Private LLM strategy for niche enterprise use cases** Open source versus hosted versus hybrid approaches, and how people usually think about tradeoffs in highly controlled environments\n3. **Large internal data at the terabyte scale** How realistic this is for LLM workflows, what architectures work in practice, and what usually breaks first\n4. **GPU realities** Reasonable expectations for fine tuning versus inference Renting GPUs early versus longer term approaches When owning hardware actually makes sense, if ever\n\nThey have also asked us to help recruit and vet the right technical talent, which is another reason we want to set this up correctly from the start.\n\nIf you are an ML engineer based in South Florida, feel free to DM me. That said, I‚Äôm mainly here for advice and perspective rather than recruiting.\n\n**To preempt the obvious questions**\n\n* No, this is not a scam\n* They approached us through an existing client\n* Yes, this is a step up in terms of domain specificity, not project scale\n* We are not pretending to be experts at everything, which is why we are asking\n\nI‚Äôd rather get roasted here than make bad architectural decisions early.\n\nThanks in advance for any insight.  \n  \nEdit - P.S To clear up any confusion, we‚Äôre mainly building them a secure internal website with a frontend and backend to run their operations, and then layering a private LLM on top of that.\n\nThey basically didn‚Äôt want to spend months hiring people, talking to vendors, and figuring out who the fuck they actually needed, so they asked us to spearhead the whole thing instead. We own the architecture, find the right people, and drive the build from end to end.\n\nThat‚Äôs why from the outside it might look like, ‚Äúhow the fuck did these guys land an enterprise client that wants a private LLM,‚Äù when in reality the value is us taking full ownership of the technical and operational side, not just training a model.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ql6a1h/i_need_help_from_actual_ml_enginners/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1c0lcu",
          "author": "flonnil",
          "text": "not sure if you're trolling, so i'll open with the roasting-bit: terrabytes of internal data dont match terribly well with ML and a 1M$ budget.",
          "score": 13,
          "created_utc": "2026-01-23 23:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c222a",
              "author": "DistributionOk6412",
              "text": "I was about to day the same thing...we pay close to 3M yearly on ~5TB of data (as far as I remember, forgot how much data was exactly, but close to some TB)",
              "score": 2,
              "created_utc": "2026-01-23 23:47:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1c8oxs",
              "author": "Dangerous_Young7704",
              "text": "My client has a infinited budget, I kinda just threw numbers out there, but is $1 m not enough? I'm asking because they want me to estimate the cost",
              "score": -3,
              "created_utc": "2026-01-24 00:23:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ce3xn",
                  "author": "flonnil",
                  "text": "here's my advice: Do not under any circumstances say any more numbers to that guy before you have talked to multiple experts multiple times and now precisely what you are going to do.\n\ni'm sure it would be easier for people here to give you advice if we would know in broad terms and concepts what we're actually talking about.",
                  "score": 7,
                  "created_utc": "2026-01-24 00:52:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1c25zp",
          "author": "kubrador",
          "text": "hire a fractional ML/LLM architect first (like, this month) before making any other moves. you need someone who's done enterprise llm deployments to sense-check your decisions, and you clearly don't have that in-house. the $30-50k you spend on a good advisor now saves you $200k+ in wrong infrastructure choices later. companies like this don't care that you're figuring it out. they care that you have \\*someone\\* who knows what they're doing.",
          "score": 8,
          "created_utc": "2026-01-23 23:48:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c8ztc",
              "author": "Dangerous_Young7704",
              "text": "Alright, now I'm getting somewhere. Thank you for the response. Mind if I dm you? I got a whole lot of questions",
              "score": 1,
              "created_utc": "2026-01-24 00:25:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1c9frf",
                  "author": "kubrador",
                  "text": "sure",
                  "score": 1,
                  "created_utc": "2026-01-24 00:27:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d71sf",
          "author": "TheRealStepBot",
          "text": "If I‚Äôm being honest your client are idiots and picked the wrong people for this. Why hire people with zero ml experience and not even a clear path to even maybe having the sort of background to have a shot at this sort of thing? \n\nWhy do you think the people who know how to do this would want to deal you in?\n\nAnd people wonder why supposedly 90% of ai initiatives fail, it‚Äôs cause 90% of ai projects don‚Äôt even have ML engineers on them who have ever built anything interesting, never mind people who have specific experience with llm‚Äôs\n\nIt‚Äôs all a bunch of script kiddies jamming shit they don‚Äôt understand together with ai llm vibe coding and then people don‚Äôt understand why it doesn‚Äôt work. \n\nYou are getting ripped off by people with a lot less to lose than you.",
          "score": 5,
          "created_utc": "2026-01-24 03:43:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d8v1j",
              "author": "Dangerous_Young7704",
              "text": "I don't think you read the post, which sucks, but thanks for taking the time to respond",
              "score": 1,
              "created_utc": "2026-01-24 03:54:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1d9kv7",
                  "author": "TheRealStepBot",
                  "text": "Oh I read it all right.\n\nYou‚Äôre promising all kinds of shit to people that you can‚Äôt deliver on and now you‚Äôre panicking. \n\nAnd the people you promised this to will happily let the project fail and ruin your life and it won‚Äôt be any skin off their back, they will just do the project again with some other inept low bidder and hope they eventually strike gold for cheap.",
                  "score": 3,
                  "created_utc": "2026-01-24 03:59:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1djfxu",
          "author": "mdizak",
          "text": "I'd revisit the entire need to use a LLM as anything more than a text rendering engine, and find another avenue to organize, store, sort filter and search those terabytes of data..  Think RAG, but higher quality and mor deterministic.",
          "score": 3,
          "created_utc": "2026-01-24 05:05:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1d5oyx",
          "author": "cuba_guy",
          "text": "Congrats, great opportunity and if you're cofounder is at the lead+ eng level in the current market switch is possible. Ai with llms are much closer to soft eng than data/ml every was imo and I see a lot of good engineers transitioning. Learn a lot, Ai engineering book, agentic design patterns and hugging face free courses should get you started",
          "score": 2,
          "created_utc": "2026-01-24 03:34:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1d9dwn",
              "author": "Dangerous_Young7704",
              "text": "It is definitely a unicorn of a client willing to pay big bucks for this, but were not arrogant. My co-founder and I know we need to hire the actual ML/AI engineers on this one, but we're definitely going to use this as experience to dive deep into actual ML. Thanks for trecommendedded courses!",
              "score": 2,
              "created_utc": "2026-01-24 03:57:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3bup",
          "author": "Altruistic-Spend-896",
          "text": "Hmm...never ever invest in hardware first, do an MVP and ask if it's acceptable, on a reduced set. Fine-tuning a narrow LLM is doable, but the real strength lies in continuous monitoring , drift detection and retraining.-signed, your friendly senior MLOps eng",
          "score": 2,
          "created_utc": "2026-01-24 07:47:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ew3b1",
          "author": "itsmeknt",
          "text": "A lot depends on the specific requirements of the project. Real time chat application will have a very different architecture than offline batch doc processing. Docs in structured text files are very different than raw docs in PDFs or images.\n\nWithout understanding the project, I can only speak very generally:\n1. Requirements doc (including timeline) + budgeting comes first, which will determine hiring, architecture, hardware, milestones and schedule planning.\n2. Will depend on data security requirements, but the ideal case is to first try private hosted providers if the project allows it. You can stress test to find the actual demand curve and then make an educated guess on the hardware and its financial projections thereafter.\n3. At this scale I'm assuming offline batch doc processing. If self hosted, will need batch optimized inference servers like vllm, and it will be a trade off between speed, accuracy/intelligence, and $$$ but it can be doable. If hosted, then its a matter of negotiating with the provider.\n4. 4bit Qlora fine tune needs 2-4x more VRAM than small-cache inference, full fine tune needs 10-20x more VRAM. Yes you want to rent GPUs at first until you know your exact load and requirements, and if you end up determining that you can keep your own hardware GPUs under constant load then it will pay itself back in about 6 months.\n5. Architecture design roles as soon as possible, because the early planning stage can really make this 2x easier or 8x harder than it needs to be. And someone experienced in this field to accurately asses the hiring candidates, as its hard to tell who is competent vs just well practiced in interivews if you dont have the experience yourself.",
          "score": 2,
          "created_utc": "2026-01-24 12:06:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fmso3",
          "author": "damhack",
          "text": "Sounds like a Big Data job with some LLM interface sugar.  You need to architect the necessary private cloud infrastructure, get your Parquet (or other) pipeline in place, sort the datalake/warehouse, design the right APIs for the usecases and install a GPU cluster with VLLM, MLOps platform and a decent chat UI (e.g. LibreChat).  That‚Äôs about 7 different people.  Even $1m is too little for the team you‚Äôll need and I‚Äôm assuming they have another $1m for the infrastructure?",
          "score": 2,
          "created_utc": "2026-01-24 14:55:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gdetw",
          "author": "coloradical5280",
          "text": "Questions:\n\n1) yes\n2) unsloth, is where you start\n3) yes, and transformer architectures. That‚Äôs what LLMs are, terabytes if data, this is standard stuff for Enterprise \n4) Deoends on a lot of factors like how fast you want it done, but you‚Äôll need ~14 H100s just to run Kimi k2 1T, and potentially many, many more depending on active users and needed inference speed, and how much can be batched vs live. TO TUNE AND TRAIN, you don‚Äôt necessarily need more but I‚Äôd imagine they want to do this right and not have a months long running task, so you‚Äôre realistically looking at a full rack of B200s that would need to be rented, across multiple epochs. \n\nNone of this even considers what they have for SFT datasets, you‚Äôll have to outsource that, Enterprise ops like this will often just use the scale.ai level providers. Just for that custom dataset that you need for RL and really all post-training, they‚Äôre looking mid six-figures, potentially less or way more, depending on how messy or clean their data is and how multi modal and parsey things need to get. \n\n**Their current budget might get them a nice RAG.**\n\nThey have no idea what they want , which is typical , they need someone who really knows what they‚Äôre doing to sit down and probe the real problems and pain points and goals, and from there , map that to reality. \n\nDo not take this meeting , or if you did, don‚Äôt take the engagement, it‚Äôs not worth the potential reputation hit to your firm. You have what sounds like a nice little automation and workflow optimization consulting shop. That is not what they say they need. Like I said they don‚Äôt know what they need, but on scale alone, the inputs and outputs don‚Äôt match it‚Äôs just not a fit.",
          "score": 2,
          "created_utc": "2026-01-24 17:00:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gqy8g",
              "author": "Dangerous_Young7704",
              "text": "Thank you for the information and the kind words. ML engineers are truly nice dudes, but we're not taking on building the LLM by ourselves. I'm sorry if the post came off as such; it's more like we need to find the right people and qualify this project on the scale. We have done things similar, but never using or fine-tuning a private LLM of this scale",
              "score": 1,
              "created_utc": "2026-01-24 18:00:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gzu54",
                  "author": "coloradical5280",
                  "text": "Ohh gotcha. I think you just need the right people to ask the right questions, to get to the real problem or use case. My guess is they don‚Äôt need to actually fine tune a model, or at least, would agree that the ROI isn‚Äôt there, once they realize what that actually means. \n\nThey need an agentic RAG with knowledge graph, hybrid search obviously, all the works. The one model it could be useful for them to train is a cross encoder reranker thats constantly training on the data. Like SBERT deal like Marco, would be the easiest, lots of options. \n\nSounds like you some people reached out but happy to chat as well if you want to DM. Not sure I have the bandwidth currently to be your guy, but more than happy to help you vet other options.",
                  "score": 2,
                  "created_utc": "2026-01-24 18:37:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bysab",
          "author": "LingeringDildo",
          "text": "If you want a consulting situation, DM me. Sounds like defense-adjacent work, and that‚Äôs my area of expertise. \n\nFirst call is on me and happy to sign a multi party NDA with you and your potential customer. üôÇ",
          "score": 3,
          "created_utc": "2026-01-23 23:30:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c8epg",
              "author": "valuat",
              "text": "Well put. Pay-to-play is the name of the game.",
              "score": 2,
              "created_utc": "2026-01-24 00:22:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1caps8",
              "author": "Dangerous_Young7704",
              "text": "I sent you a DM",
              "score": 1,
              "created_utc": "2026-01-24 00:34:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gn02o",
          "author": "Sufficient_Ad_3495",
          "text": "Let it go buddy‚Ä¶ on so many issues‚Ä¶ you‚Äôre not going win such a project. Best introduce and partner as a finders fee.",
          "score": 1,
          "created_utc": "2026-01-24 17:43:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gq96s",
              "author": "Dangerous_Young7704",
              "text": "Thanks for trying to look out for me even though im some stranger on the internet, but we're not building the LLM and we have two main jobs, handle the web development and find the correct people for the project and manage it, so we have experience manageing projects of this level and worked with ML engineers just never Private LLM in this type of enviorment.",
              "score": 2,
              "created_utc": "2026-01-24 17:57:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gyqvu",
                  "author": "Sufficient_Ad_3495",
                  "text": "You want to find a company that can do it all on their behalf maybe two, and front those Companies with a  tight NDA and introduction and then for a period of time over which they are conducting for this particular project if it wins they pay you a fee undisclosed to the client. So you are managing the introduction to sale that‚Äôs the best you can hope for.\n\nYou are managing a one off introduction.. you will not be managing a project ongoing, that large company will not be managing the project through you because of risks, their purchasing wouldn‚Äôt allow it regardless of the enthusiasm and zeal through which your internal contact has instructed . I can tell your internal contact is senior but they‚Äôre not a decision maker, so caution with your time. I hope that helps.",
                  "score": 2,
                  "created_utc": "2026-01-24 18:33:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpmtms",
      "title": "LAD-A2A: How AI agents find each other on local networks",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qpmtms/lada2a_how_ai_agents_find_each_other_on_local/",
      "author": "franzvill",
      "created_utc": "2026-01-28 20:22:21",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "AI agents are getting really good at doing things, but they're completely blind to their physical surroundings.\n\nIf you walk into a hotel and you have an AI assistant (like the Chatgpt mobile app), it has no idea there may be a concierge agent on the network that could help you book a spa, check breakfast times, or request late checkout. Same thing at offices, hospitals, cruise ships. The agents are there, but there's no way to discover them.\n\nA2A (Google's agent-to-agent protocol) handles how agents talk to each other. MCP handles how agents use tools. But neither answers a basic question: how do you find agents in the first place?\n\nSo I built LAD-A2A, a simple discovery protocol. When you connect to a Wi-Fi, your agent can automatically find what's available using mDNS (like how AirDrop finds nearby devices) or a standard HTTP endpoint.\n\nThe spec is intentionally minimal. I didn't want to reinvent A2A or create another complex standard. LAD-A2A just handles discovery, then hands off to A2A for actual communication.\n\nOpen source, Apache 2.0. Includes a working Python implementation you can run to see it in action. Repo can be found at franzvill/lad.\n\nCurious what people think!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qpmtms/lada2a_how_ai_agents_find_each_other_on_local/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2fi7hj",
          "author": "ChanceKale7861",
          "text": "Goodness this is refreshing. Appreciate that you augment a separate protocol. Using MCP/A2A/ANP in tandem to address multi agent aspects is key. Thanks for this!",
          "score": 2,
          "created_utc": "2026-01-29 15:57:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fik6p",
              "author": "franzvill",
              "text": "Thanks! I'm posting the links here if you want to read more:\n\n[https://github.com/franzvill/lad](https://github.com/franzvill/lad)\n\n[https://lad-a2a.org/](https://lad-a2a.org/)",
              "score": 1,
              "created_utc": "2026-01-29 15:59:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a4pr9",
          "author": "franzvill",
          "text": "Links:   \n[https://github.com/franzvill/lad](https://github.com/franzvill/lad)  \n[https://lad-a2a.org/](https://lad-a2a.org/)",
          "score": 1,
          "created_utc": "2026-01-28 20:22:50",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2aon0h",
          "author": "Samrit_buildss",
          "text": "This is a nice framing of the discovery gap between agents vs tool usage. One question I had reading this: how do you think about *trust and scoping* once discovery is automatic? mDNS-style discovery works well for devices, but agents feel trickier e.g. avoiding accidental discovery across VLANs, or ensuring an agent only advertises capabilities to peers with the right trust context.\n\nCurious whether you see LAD-A2A as purely local / human-controlled environments (home, hotel, office), or something that could safely generalize beyond that with additional constraints.",
          "score": 1,
          "created_utc": "2026-01-28 21:50:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk6tpa",
      "title": "Still using real and expensive LLM tokens in development? Try mocking them! üê∂",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qk6tpa/still_using_real_and_expensive_llm_tokens_in/",
      "author": "kshantanu94",
      "created_utc": "2026-01-22 21:07:34",
      "score": 8,
      "num_comments": 16,
      "upvote_ratio": 0.78,
      "text": "Sick of burning $$$ on OpenAI/Claude API calls during development and testing? Say hello to **MockAPI Dog‚Äôs new** [Mock LLM API](http://mockapi.dog/llm-mock) \\- a free, no-signup required way to spin up LLM-compatible streaming endpoints in under 30 seconds.\n\n‚ú® **What it does:**  \n‚Ä¢ Instantly generate streaming endpoints that mimic **OpenAI**, **Anthropic Claude**, *or generic* LLM formats.  \n‚Ä¢ Choose content modes (generated, static, or hybrid).  \n‚Ä¢ Configure token output and stream speed for realistic UI testing.  \n‚Ä¢ Works with SSE streaming clients and common SDKs - just switch your baseURL!\n\nüí° **Why you‚Äôll love it:**  \n‚úî Zero cost - free mocks for development, testing & CI/CD.  \n‚úî No API keys or billing setup.  \n‚úî Perfect for prototyping chat UIs, test automation, demos, and more.\n\nGet started in seconds - [mockapi.dog/llm-mock](http://mockapi.dog/llm-mock) üê∂  \nDocs - [https://mockapi.dog/docs/mock-llm-api](https://mockapi.dog/docs/mock-llm-api)  \n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qk6tpa/still_using_real_and_expensive_llm_tokens_in/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o14ob2p",
          "author": "BrownOyster",
          "text": "Why not just spin up a <1B model locally? And if the tokens don't matter, might as well be a Q1",
          "score": 1,
          "created_utc": "2026-01-22 22:15:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14sphu",
              "author": "kshantanu94",
              "text": "Because then you‚Äôre not mocking anymore, you‚Äôre running an ML stack. üôÇ\n\nEven a <1B local model means weights, runtimes, hardware quirks, cold starts, and nondeterministic output (yes, even Q1). Mock LLM APIs are about deterministic responses, zero infra, full control over potential errors, and control over how fast or slow  LLM-streaming responses will be. If you‚Äôre testing integrations and failure modes, a fake endpoint is way more useful than a tiny ‚Äúreal‚Äù model that breaks in new and exciting ways.",
              "score": 1,
              "created_utc": "2026-01-22 22:39:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14t7l7",
                  "author": "BrownOyster",
                  "text": "Thanks for the answer",
                  "score": 1,
                  "created_utc": "2026-01-22 22:42:32",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o17us1z",
                  "author": "UnbeliebteMeinung",
                  "text": "More than half of your answer is complete wrong and shit.",
                  "score": -2,
                  "created_utc": "2026-01-23 10:59:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o15lej6",
          "author": "johnerp",
          "text": "So is the is open source? Can I self host it?\n\nI‚Äôm worried using it and then eventually getting a bill when you monetise it‚Ä¶. \n\nI‚Äôve already vibe coded a basic version.",
          "score": 1,
          "created_utc": "2026-01-23 01:11:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o171c3l",
              "author": "kshantanu94",
              "text": "It‚Äôs not open source :) but I don‚Äôt have intentions of monetizing it with money, might add a ‚Äòbuy a coffee button‚Äô or something. I don‚Äôt want to have sign up on it ever, so can‚Äôt really charge people.  Ah, nice (that you vibe coded a version)! :)",
              "score": 1,
              "created_utc": "2026-01-23 06:35:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17cgq8",
          "author": "Purple-Programmer-7",
          "text": "Love this idea, wouldn‚Äôt use an external service for it as I have to deal with compliance, but I was just thinking of mocking something since I already know the AI integration is working and the data returned is the same every time‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-23 08:11:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17d5e4",
              "author": "kshantanu94",
              "text": "As long as you don‚Äôt include any sensitive data when mocking, it should be alright üôÇ  \nBut I understand if it‚Äôs something that‚Äôs unusable for enterprise-related use cases.   \n  \nDo you think this might be useful of it was available in a more 'compliant way'?",
              "score": 1,
              "created_utc": "2026-01-23 08:18:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1c6k6x",
                  "author": "Purple-Programmer-7",
                  "text": "I‚Äôm skeptical of data going anywhere. Other devs may be less so.\n\nAnd no, I wouldn‚Äôt suggest you go down the compliance route (I.e. HIPAA) unless you want to target healthcare. It‚Äôs a big headache and unnecessary given your apparent scope and target audience.",
                  "score": 1,
                  "created_utc": "2026-01-24 00:12:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zpjt1",
              "author": "pro-cras-ti-nation",
              "text": "you should try the cloud mocking platforms like Beeceptor. it is quite mature in terms of features, customizations. we saved our AI costs during perf. test by mocking almost all external services.",
              "score": 1,
              "created_utc": "2026-01-27 10:16:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sdm82",
          "author": "FreeTinyBits",
          "text": "Good one, but I'm not sure if I'd need it. It could be useful in cases where you want to do testing that meets a baseline of your stuff integrating with LLMs.",
          "score": 1,
          "created_utc": "2026-01-26 08:58:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkec6n",
      "title": "This is kind of blowing my mind... Giving agents a \"Hypothesis-Driven Optimization\" skill",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qkec6n/this_is_kind_of_blowing_my_mind_giving_agents_a/",
      "author": "Floppy_Muppet",
      "created_utc": "2026-01-23 02:22:00",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been experimenting with recursive self-learning for the last few months, and I'm starting to see some really positive results (sry, internal data folks) by equipping my agents with what I guess I'd call a \"Hypothesis-Driven Optimization\" skill.\n\n\n\nBasically, it attempts to automate the scientific method through a perpetual 5-stage loop:\n\n1. **Group I/O's**: Organize I/O performance into three buckets within each problem space cluster (top, bottom, and average).\n2. **Hypothesize**: Use a FM to speculate on why the top and bottom groups diverged from the average.\n3. **Distill**: Use a SLM to turn each hypothesis into actionable hints.\n4. **A/B Test**: RAG those hints into your prompt to see if they outperform your control group.\n5. **Scale or Iterate**: Scale the winning hypothesis' \"Hint Pack\" or use the learnings from failed test to iterate on a new hypothesis.\n\n\n\nPreviously, my agents were setup to simply mimic top-performing I/O's without *traceability* or *testability* of the actual conjecture(s) it was making.\n\n\n\nNow I'm seeing my agents get incrementally better on their own (with stat sig proof), and I know why, and by how much... It's kind of insane rn.\n\n\n\nCurious who else has tried a similar approach yet?!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qkec6n/this_is_kind_of_blowing_my_mind_giving_agents_a/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o161dpp",
          "author": "qa_anaaq",
          "text": "You‚Äôd need a lot of data to approach the problem via this method though, right? What‚Äôs the volume of interactions you‚Äôre working with that‚Äôs showing promising results?",
          "score": 2,
          "created_utc": "2026-01-23 02:41:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o162w1y",
              "author": "Floppy_Muppet",
              "text": "Yes, you definitely need some amount of scale.\n\nI have hypotheses being generated on clusters when they have a minimum of 10 I/O's AND show a statistically significant difference between performance groups (so, in reality closer to \\~100 minimum I/O's).\n\nFor smaller scale agents, you could try broadening your problem spaces (to add more I/O's into each cluster) as well as the framing of your hypotheses as to try and discover more generally applicable hints.",
              "score": 1,
              "created_utc": "2026-01-23 02:49:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o174qwp",
                  "author": "tom-mart",
                  "text": ">I have hypotheses being generated on clusters when they have a minimum of 10 I/O's\n\n\nIs it 10 thousand or 10 millions I/O's?",
                  "score": 1,
                  "created_utc": "2026-01-23 07:03:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq4ted",
      "title": "Building opensource Zero Server Code Intelligence Engine",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/5o9xx8n8k9gg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-29 10:15:13",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq4ted/building_opensource_zero_server_code_intelligence/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ewgla",
          "author": "Key-Contact-6524",
          "text": "Gorgeous",
          "score": 1,
          "created_utc": "2026-01-29 14:14:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f2o22",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏èü´†",
              "score": 1,
              "created_utc": "2026-01-29 14:46:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ql0i6z",
      "title": "context management on long running agents is burning me out",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1ql0i6z/context_management_on_long_running_agents_is/",
      "author": "Main_Payment_6430",
      "created_utc": "2026-01-23 19:32:40",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 0.82,
      "text": "is it just me or does every agent start ignoring instructions after like 50-60 turns. i tell it dont do X without asking me first, 60 turns later it just does X anyway. not even hallucinating just straight up ignoring what i said earlier\n\ntried sliding window, summarization, rag, multiagent nothing really works. feels like the context just rots after a while\n\nhow are you guys handling this",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1ql0i6z/context_management_on_long_running_agents_is/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1axa59",
          "author": "Ok_Economics_9267",
          "text": "Keep context as short as possible. Manage memory manually. Add episodic and procedural memories. Search in memory and take only what matters, instead of adding whole memory to context.",
          "score": 3,
          "created_utc": "2026-01-23 20:27:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ao8jb",
          "author": "Arindam_200",
          "text": "I'm using byterover for it\n\nThey have context tree based approach. You can probably give it a shot",
          "score": 2,
          "created_utc": "2026-01-23 19:44:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1apqdq",
          "author": "taftastic",
          "text": "Langmem does it, beads helps a lot and makes shorter sessions way easier",
          "score": 2,
          "created_utc": "2026-01-23 19:51:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bw0zl",
          "author": "neoneye2",
          "text": "In the past I tried plain text responses, and my code was fragile.\n\nNowadays I'm using structured output, and is doing around 100 inference calls. Only asking for very narrow things, so the response stays below 4 kilobytes.\n\nThis is a document I have generated.  \n[https://neoneye.github.io/PlanExe-web/20260104\\_operation\\_falcon\\_report.html](https://neoneye.github.io/PlanExe-web/20260104_operation_falcon_report.html)\n\nAnd this is my code for orchestrating the agents  \n[https://github.com/neoneye/PlanExe/blob/main/worker\\_plan/worker\\_plan\\_internal/plan/run\\_plan\\_pipeline.py](https://github.com/neoneye/PlanExe/blob/main/worker_plan/worker_plan_internal/plan/run_plan_pipeline.py)",
          "score": 2,
          "created_utc": "2026-01-23 23:15:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b1y0o",
          "author": "one-wandering-mind",
          "text": "use a better model, reinject instructions to just prior to the current conversation turn, use separate models and tools as validators and guardrails for important behaviors to avoid, intentionally manage the context. you probably don't want a generic summary unless what you are building is generic. maintain just the important information for your task(s).",
          "score": 1,
          "created_utc": "2026-01-23 20:49:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bja3n",
          "author": "Fulgren09",
          "text": "Cache system prompt and send it each turn¬†",
          "score": 1,
          "created_utc": "2026-01-23 22:10:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1bkth1",
          "author": "johnerp",
          "text": "Yes this happens, there‚Äôs maths and reinforcement learning reasons.",
          "score": 1,
          "created_utc": "2026-01-23 22:17:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1c3jlq",
          "author": "Charming_Support726",
          "text": "Yes. It rots after a while, almost every model gets awkward after around 150-180k. Jump of early and start new. On opencode things like the DCP help - but you get hit by different issues",
          "score": 1,
          "created_utc": "2026-01-23 23:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e1swr",
          "author": "MajinAnix",
          "text": "Trying to solve this problem too, in my head I have solution with tasks (tasks have separate conversation history, structured output)",
          "score": 1,
          "created_utc": "2026-01-24 07:33:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e691z",
          "author": "DotPhysical1282",
          "text": "Run a parallel agent whose only job is to ensure your main agent is following instructions. After every x turns, ask it to verify the main agent is following instructions. If it gets it wrong, it‚Äôs time to remind it. Sending the prompt after each turn would be expensive and not necessary if it still has the context",
          "score": 1,
          "created_utc": "2026-01-24 08:13:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ffb9d",
              "author": "Main_Payment_6430",
              "text": "multiagent approach, i like it!",
              "score": 2,
              "created_utc": "2026-01-24 14:14:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ecsid",
          "author": "ggone20",
          "text": "What model are you using?\n\nEveryone likes to hate on OAI but since GPT 5.2, this is basically a non-issue. It truly does stay coherent though very complex workflows and literal day-long conversation sessions. Curious what other people‚Äôs mileage is here.\n\nBefore 5.2, my general rule of thumb was to never let context exceed 20ish percent of its claimed window. The data has shown since the beginning that anything past literally the first turn performance degraded dramatically.",
          "score": 1,
          "created_utc": "2026-01-24 09:13:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ff7sq",
              "author": "Main_Payment_6430",
              "text": "that's why i created one truth! [https://github.com/justin55afdfdsf5ds45f4ds5f45ds4/onetruth.git](https://github.com/justin55afdfdsf5ds45f4ds5f45ds4/onetruth.git) i build this today, i knew this issue was the same thing i was facing that we need to not let the context exceed, but i am not there to flush things up every second, and i open sourced it",
              "score": 1,
              "created_utc": "2026-01-24 14:13:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ueah2",
          "author": "Kong28",
          "text": "Why is an agent handling 50-60 turns of something?",
          "score": 1,
          "created_utc": "2026-01-26 16:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1anmiz",
          "author": "Altruistic-Spend-896",
          "text": "Langmem",
          "score": -1,
          "created_utc": "2026-01-23 19:41:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmho0s",
      "title": "Does anyone know of tools that let you branch off AI conversations without cluttering the main chat?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qmho0s/does_anyone_know_of_tools_that_let_you_branch_off/",
      "author": "Nkt_31",
      "created_utc": "2026-01-25 12:18:50",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I've been using AI for research and I keep running into this annoying workflow issue. I'll be in the middle of a good conversation, then the AI mentions something technical or uses a term I don't fully understand. When I ask for clarification in the same chat, it just keeps adding to this long scrolling mess and I lose track of the main thread.\n\nLike yesterday I was asking about data validation methods and wanted to quickly understand what it meant in that context. But if I ask in the same conversation, now my main research chat has this tangent stuck in the middle of it, and the AI's context window gets filled with stuff that's not really relevant to my main question.\n\nI know some apps have \"fork\" features or conversation branching, but I haven't found anything that actually works well for this. Ideally I'd want to:\n\n‚Ä¢‚Å†  ‚Å†Highlight a specific part of the AI's response \n\n‚Ä¢‚Å†  ‚Å†Branch off into a separate mini-conversation just about that\n\n‚Ä¢‚Å†  ‚Å†Keep that exploration isolated so it doesn't pollute the main chat\n\n‚Ä¢‚Å†  ‚Å†Maybe save the key insight and attach it back to the original point\n\nDoes anything like this exist? Or am I just supposed to open 10 different chat windows and copy-paste context around like a caveman?\n\nWould genuinely appreciate any suggestions. This is driving me nuts.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qmho0s/does_anyone_know_of_tools_that_let_you_branch_off/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1s2n1s",
          "author": "chatpatahu",
          "text": "wait, this is exactly what I needed last week when I was going down research rabbit holes  \n  \nI think the issue is most AI chat interfaces are designed for casual use, not actual deep research where you need to explore tangents. Like the UI assumes you're just having one linear conversation, but real research doesn't work that way at all  \n  \nhave you looked into any of the self-hosted options? I feel like some of the open source stuff might have better features for this kind of workflow since they're built by people who actually do research with AI",
          "score": 2,
          "created_utc": "2026-01-26 07:21:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1oozkh",
          "author": "throwaway490215",
          "text": "I'd like some improvement in this space as well, but I thought the 90% usecase was already available in most clients? \n\nClaude Code and OpenCode both allow you to --resume any chat, or jump back to a previous point and 'undo' parts of the chat.",
          "score": 1,
          "created_utc": "2026-01-25 20:29:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qm53f",
              "author": "The_Noble_Lie",
              "text": "And now /fork widely available.  I had my own fork / clone convo up until now",
              "score": 1,
              "created_utc": "2026-01-26 01:50:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1pxinq",
          "author": "Remarkable_Training9",
          "text": "hmm... this is literally what I built! My extension lets you organize conversations with folders and tags, so you can branch off tangents without cluttering the main chat. Plus it is cross-platform (chatgpt + claude) and all data stored locally. DM me if you want to try it out!",
          "score": 1,
          "created_utc": "2026-01-25 23:48:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1riqtx",
          "author": "elbiot",
          "text": "Claude definitely has that. You edit a message and it branches the conversation. Pretty sure ChatGPT does too",
          "score": 1,
          "created_utc": "2026-01-26 04:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s9m1s",
          "author": "Acceptable_Driver655",
          "text": "This is huge actually. I've been doing the \"10 different chat windows\" method and it's such a mess\n\nThe biggest problem I have is when I'm 20 messages deep into a conversation about one topic, then I need to understand one specific thing the AI mentioned, but asking about it in the same thread derails everything. And starting a fresh chat means I lose all the prior context\n\nIf this research layers thing lets you branch off while keeping the main conversation focused, that's basically how my brain actually works when doing research. I don't think linearly - I explore tangents, gather insights, then come back to the main path\n\nGoing to try this out tonight. The fact that it's self-hosted is actually a plus for me because I'm working with some proprietary data and don't want it going through too many external services\n\nThanks for the tip",
          "score": 1,
          "created_utc": "2026-01-26 08:22:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sfnaj",
          "author": "Ok-Lack-7216",
          "text": "On ChatGPT, there is an option. click 'more actions (three dots)' at the end of the response and branch out. \n\nhttps://preview.redd.it/tb7vjnp5vnfg1.png?width=307&format=png&auto=webp&s=6e196b69c5098f7dfd799e2f1a029ced8c7c4af8",
          "score": 1,
          "created_utc": "2026-01-26 09:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1u7a6x",
          "author": "Alternative_Nose_874",
          "text": "For what you‚Äôre asking about, there *are* some options beyond just opening new tabs and copy-pasting. A few open source tools let you create actual branches or isolated threads off a main LLM conversation so your main context stays clean, for example Delta, a local app that lets you rewind and branch chats into different directions without polluting the original thread, and Context Branching SDK which is literally built to isolate exploration branches from the main conversation context.\n\nThere‚Äôs also Multiversalchats on GitHub that treats messages as nodes and lets you branch/merge conversations like a flowchart, and some self-hosted research UIs (like KEA Research mentioned in the thread) that let you start ‚Äúresearch layers‚Äù off a highlighted point to explore without clutter.\n\nSo you‚Äôre not stuck with copypaste. but most good branching workflows right now live in opensource/selfhosted tools rather than in mainstream chat UIs.",
          "score": 1,
          "created_utc": "2026-01-26 16:00:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vu1dp",
          "author": "Strong_Worker4090",
          "text": "I usually just create a project and create different threads like: strategy, strategy questions, mvp, mvp questions, etc.\n\nThat way the project maintains its context but you can ask clarify question in a different thread with shared project context. \n\nAn example is \n\nThread 1: ‚Äúgive me my mvp roadmap over the next week‚Äù\n\nThread 2: used to answer questions from these one without losing context in thread 1",
          "score": 1,
          "created_utc": "2026-01-26 20:12:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qom1i3",
      "title": "Do you use Evals?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qom1i3/do_you_use_evals/",
      "author": "InvestigatorAlert832",
      "created_utc": "2026-01-27 18:26:02",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Do people currently run evaluations on your prompt/workflow/agent?\n\nI used to just test manually when iterating, but it's getting difficult/unsustainable. I'm looking into evals recently, but it seems to be a lot of effort to setup & maintain, while producing results that're not super trustworthy.  \n  \nI'm curious how others see evals, and if there're any tips?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qom1i3/do_you_use_evals/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o23xq84",
          "author": "kubrador",
          "text": "yeah evals are the \"we should probably do this\" that everyone avoids until their thing breaks in production. manual testing works great until you ship something that makes you want to delete your github account.\n\nthe annoying part is you're right. setting them up sucks and they're still kinda made up. i'd start stupid though: just pick like 5 test cases that would kill you if they broke, throw them in a txt file, and check them when you change stuff. beats maintaining a whole framework that makes you feel productive while being wrong.\n\nonce you have that baseline of \"oh this actually caught something real,\" then maybe think about scaling it. brute forcing lcm calls through test cases is way cheaper than debugging user complaints.",
          "score": 5,
          "created_utc": "2026-01-27 22:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25jxm4",
              "author": "InvestigatorAlert832",
              "text": "Thanks for the suggestion! So for test cases do  you mean I should put a bunch of messages arrays in there, run LLM calls and evaluate responses manually?",
              "score": 1,
              "created_utc": "2026-01-28 04:05:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o24gmd4",
          "author": "3j141592653589793238",
          "text": "Whether you use evals is what often separates successful and unsuccessful projects. Start with small sets, you can expand them later. Whether it's trustworthy depends on the type of eval & problem you're trying to solve. E.g. if you use LLMs to predict a number w/ structured outputs you can have a direct eval that's as trustworthy as your data is.\n\n[deeplearning.ai](http://deeplearning.ai) agentic AI course by Andrew Ng has a good introduction into evals for LLMs\n\nAlso, not mentioned there but I find running evals multiple times and averaging out results helps to stabilise some of the non-determinism in LLMs, just make sure you use a different seed each time (matters a lot for models like Gemini).",
          "score": 3,
          "created_utc": "2026-01-28 00:34:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25j0dn",
              "author": "cmndr_spanky",
              "text": "Or you could do like 15mins of reading and not pay for a dumb course",
              "score": 2,
              "created_utc": "2026-01-28 04:00:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27earm",
                  "author": "3j141592653589793238",
                  "text": "But the course is free... it's also written by someone with lots of credentials in the field e.g. he's a co-founder of Google Brain, adjunct professor at Stanford alongside many other things. It's likely to be better than some AI generated Medium article.\n\nWorth mentioning, I'm not affiliated with the course in anyway.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:48:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25im7w",
              "author": "InvestigatorAlert832",
              "text": "Thanks for the tips and the course, I'll definitely check it out!\nYou mentioned that trustworthiness depends on the type of problem, I wonder whether you have any tips on eval for chatbot, whose answer/decision can not be necessarily checked by simple code?",
              "score": 1,
              "created_utc": "2026-01-28 03:57:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27eosl",
                  "author": "3j141592653589793238",
                  "text": "Check out the course, it explores a few different approaches e.g. programmatically calculated metric, LLM-as-a-judge. It really depends, what is the purpose of your chatbot.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:50:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22h4if",
          "author": "demaraje",
          "text": "Test sets",
          "score": 1,
          "created_utc": "2026-01-27 19:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25mevh",
          "author": "Bonnie-Chamberlin",
          "text": "You can try LLM-as-Judge framework. Use listwise or pairwise comparison instead of one-shot.",
          "score": 1,
          "created_utc": "2026-01-28 04:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dq3d",
          "author": "PurpleWho",
          "text": "You're right, evals are a pain to set up.\n\nI generally use a testing playground embedded in my editor, like¬†[Mind Rig](https://mindrig.ai/)¬†or¬†[vscode-ai-toolkit](https://github.com/microsoft/vscode-ai-toolkit),¬†over a more formal Eval tool like PromptFoo, Braintrust, Arize, etc.   \n  \nUsing an editor extension makes the \"tweak prompt, run against dataset, review results\" loop much faster. I can run the prompt against a bunch of inputs, see all the outputs side-by-side, and catch regressions right away. Less setup hassle but more reliability than a mere vibe check.  \n   \nOnce your dataset grows past 20-30 scenarios, I just export the CSV of test scenarios to a more formal eval tool.",
          "score": 1,
          "created_utc": "2026-01-28 07:43:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnuyyj",
      "title": "Reducing token costs on autonomous LLM agents - how do you deal with it?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnuyyj/reducing_token_costs_on_autonomous_llm_agents_how/",
      "author": "PatateRonde",
      "created_utc": "2026-01-26 22:18:50",
      "score": 7,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hey,\n\nI'm working on a security testing tool that uses LLMs to autonomously analyze web apps. Basically the agent reasons, runs commands, analyzes responses, and adapts its approach as it goes.\n\nThe issue: It's stateless. Every API call needs the full conversation history so the model knows what's going on. After 20-30 turns, I'm easily hitting 50-100k tokens per request, and costs go through the roof  \n  \nWhat I've tried:\n\n  \\- Different models/providers (GPT-4o, GPT-5, GPT-5mini, GPT 5.2,  DeepSeek, DeepInfra with open-source models...)\n\n  \\- OpenAI's prompt caching (helps but cache expires)\n\n  \\- Context compression (summarizing old turns, truncating outputs, keeping only the last N messages)\n\n  \\- Periodic conversation summaries  \n\n\nThe problem is every approach has tradeoffs. Compress too much and the agent \"forgets\" what it already tried and goes in circles. Don't compress enough and it costs a fortune.  \n\n\nMy question:\n\nFor those working on autonomous agents or multi-turn LLM apps:\n\n  \\- How do you handle context growth on long sessions?\n\n  \\- Any clever tricks beyond basic compression?\n\n  \\- Have you found a good balance between keeping context and limiting costs?\n\n\n\nCurious to hear your experience if you've dealt with this kind of problem.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnuyyj/reducing_token_costs_on_autonomous_llm_agents_how/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1wohch",
          "author": "pmv143",
          "text": "You‚Äôve basically hit the core limitation of stateless LLM APIs. Once agents become long running and toolusing, the real cost isn‚Äôt actually tokens, it‚Äôs repeatedly reconstructing state. Compression can help but it‚Äôs lossy by definition. which is why agents loop or forget. One alternative pattern that work better is treating agent state as runtime state instead of prompt state . keep the model warm, preserve KV / execution context across turns, and only serialize when you truly need to suspend. That shifts the problem from prompt engineering to lifecycle management, but it avoids paying the full context price on every step.",
          "score": 5,
          "created_utc": "2026-01-26 22:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wueib",
              "author": "PatateRonde",
              "text": "That's a good point. I've been too focused on the prompt side and not enough on treating it as a runtime problem. I'm currently on commercial APIs OpenAI, DeepSeek... OpenAI's prompt caching now supports 24h retention which helps, but it still requires exact prefix matches and the cache can get invalidated easily when the conversation branches.\n\nI've been looking into self-hosted options. Looks like vLLM + LMCache is the go-to combo for this apparently it can give 3-10x improvements on multi-turn workloads by properly managing KV cache across turns. There's also llm-d for KV-cache aware routing if you're running multiple instances.  \n  \nHave you actually deployed something like this in production? My main concern is whether open-source models (Llama, Qwen, etc.) can match GPT-4o /GTP 5 quality for agentic tasks that require good reasoning and tool use. Trading 10x cost savings for an agent that hallucinates more doesn't seem worth it.\n\nTbh I'm not an expert, still figuring all this out as I go. But thanks for the insight, really helpful to shift my perspective on this.",
              "score": 1,
              "created_utc": "2026-01-26 22:56:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x1qb6",
                  "author": "pmv143",
                  "text": "We‚Äôve deployed this pattern in production. but with a slightly different framing. KV cache reuse (vLLM, LMCache, cache aware routing) absolutely helps, but it still assumes a mostly warm, long lived process. For agentic workloads with branching, tool waits, and spiky traffic, the bigger win for us was treating each step as a short lived execution and externalizing state entirely. That way you‚Äôre not paying tokens or GPUs to ‚Äúremember‚Äù things that the system already knows. On model quality, we‚Äôve seen that open source models can match GPT 4 class reasoning for many agent loops when tool use is explicit and scoped, but for the hardest reasoning steps we still mix in frontier models. In practice it ends up being a hybrid system, not an either or tradeoff between cost and correctness.",
                  "score": 2,
                  "created_utc": "2026-01-26 23:33:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1xd0sz",
          "author": "tech2biz",
          "text": "This is 100% the same background we had. Built n8n first, then own agents. Then went on-prem but quality was bad (in some cases). Then tried hybrid but static routing was either high cast or bad output, we couldn‚Äôt find the in between. And so developed dynamic cascading (cascadeflow), it‚Äôs open source and on github. Tries small model first, cascades to large one if needed. You can work it in any infra you already have. Hope it helps you with this. Lmk",
          "score": 4,
          "created_utc": "2026-01-27 00:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o234utb",
              "author": "PatateRonde",
              "text": "Oh nice, that's exactly the rabbit hole I'm in right now. Thanks for sharing the repo, I'll check it out and try to plug it into my setup. Will let you know if I manage to get it working!",
              "score": 2,
              "created_utc": "2026-01-27 20:46:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o235201",
                  "author": "tech2biz",
                  "text": "Awesome! Lmk if I can help in any way :)",
                  "score": 1,
                  "created_utc": "2026-01-27 20:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1wua0d",
          "author": "thisdude415",
          "text": "The key, I think, is to build a representation of the workflow, and \"fill it in\" with context as the agents work. \n\nThe goal is to keep your orchestration agent's context minimally littered with low-level work\n\nIf you can truly figure it out, you've got a high paying job waiting for you at your choice of AI lab",
          "score": 2,
          "created_utc": "2026-01-26 22:56:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wrajd",
          "author": "EpochRaine",
          "text": "Can you not use a local model, LoRa train it on your tools and content, merge into the core model and use RAG and minimal context inejction?",
          "score": 1,
          "created_utc": "2026-01-26 22:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wt081",
              "author": "PatateRonde",
              "text": "Interesting idea, but I'm not sure it's viable for my use case. I don't have the infra to run a large model locally, and from what I've tested, smaller models really struggle with the kind of multi-step reasoning and tool chaining I need for security testing. They tend to hallucinate findings or go in circles way more than GPT-5 class models. Fine-tuning could help with tool familiarity, but I'm not sure it would fix the core reasoning gap. Have you seen good results with LoRA-tuned models on complex agentic tasks?",
              "score": 2,
              "created_utc": "2026-01-26 22:50:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x2c4d",
                  "author": "Technical-History104",
                  "text": "When dealing with smaller models, you would need to deconstruct the workflow into even smaller pieces, and also rely more on regular SW code to do much of the orchestration of the work across each of the pieces.  Can your prompts and context be broken down to achieve the same goal?",
                  "score": 2,
                  "created_utc": "2026-01-26 23:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zjgdk",
          "author": "AI_Data_Reporter",
          "text": "TPT (Tokens Per Thought) efficiency remains the primary bottleneck for autonomous agents. Implementation of Observation Masking at M=10 reduces redundant state reconstruction costs by 40% in multi-hop reasoning tasks. AgentDiet protocols further prune trajectory bloat. The 'Unreliability Tax'‚Äîthe cost of model retries due to context fragmentation‚Äîis often overlooked but accounts for 15% of total spend in stateless architectures. Shifting to KV-cache aware routing is the only viable path.",
          "score": 1,
          "created_utc": "2026-01-27 09:19:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o203gn5",
          "author": "Zeikos",
          "text": "In general performance/cost in software is minimized by doing less work.  \n\nShift as much as possible of the workflow away from using the LLM. Use it only when strictly necessary.",
          "score": 1,
          "created_utc": "2026-01-27 12:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2352ay",
              "author": "PatateRonde",
              "text": "Yeah fair point. I've probably been over-relying on the LLM for stuff that could be handled with simpler logic. Gonna look into offloading more of the workflow to deterministic code and only hitting the model when I actually need reasoning.",
              "score": 1,
              "created_utc": "2026-01-27 20:47:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2155bn",
          "author": "Mole-Transistor4440",
          "text": "If we‚Äôre talking about LLM call costs suddenly spiking and wanting to rein that in - what kind of reduction are you actually aiming for? Like 5%, 10%, 20%, or something more aggressive?\n\nThere are a lot of smart folks here with real-world experience and plenty of optimization tricks, but without a target number it‚Äôs hard to tell which ideas are worth suggesting and which ones won‚Äôt really move the needle for you.",
          "score": 1,
          "created_utc": "2026-01-27 15:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o235hvd",
              "author": "PatateRonde",
              "text": "Honestly I don't have a hard target in mind. Right now a 30-40 turn session can easily burn through $1-3 depending on the model, and that adds up fast when you're iterating a lot during dev.\n\nI'd be happy with anything that cuts that in half without sacrificing too much output quality. But really I'm just trying to find something sustainable where I'm not scared to hit \"run\" because I know it's gonna cost me.",
              "score": 1,
              "created_utc": "2026-01-27 20:49:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o233i6z",
          "author": "yaks18",
          "text": "There‚Äôs a recent very interesting paper on Recursive Language Models (RLMs) that reframes the problem. Instead of stuffing everything into context, the model treats long text as external data, searches it, slices it, and only reasons over the parts it needs recursively calling itself on sub-chunks.\nSo the shift is from ‚Äúgive the model everything‚Äù to ‚Äúnavigate,  select, reason, compose.‚Äù\n\nhttps://arxiv.org/pdf/2512.24601",
          "score": 1,
          "created_utc": "2026-01-27 20:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26eele",
          "author": "Accurate-Ad-7944",
          "text": "yeah the context bloat is brutal, especially with web interactions where DOM snapshots eat tokens like crazy. I ended up using a hybrid approach - I keep the full last 3-4 turns in detail, then maintaining a *really* condensed running log off just actions taken and key outcomes (like \"tested login endpoint - got 403\"). \n\nthe real hack for me was caching selectors and page structures separately so the agent doesn't need the full HTML every time. I actually started using Actionbook for this recently - it kinda automates that manual caching/action manual creation I was doing, which cut my tokens usage on browser tasks by a stupid amount. not sure if it fits security testing directly, but for reducing repetitive context it helped me break the cycle of agents re-analyzing the same UI elements.\n\nstill have to tune the compression aggressively though, and sometimes the agent doesn't get lost. it's a constant trade-off.",
          "score": 1,
          "created_utc": "2026-01-28 07:48:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpbuhf",
      "title": "Local LLM deployment",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qpbuhf/local_llm_deployment/",
      "author": "Puzzleheaded-Ant1993",
      "created_utc": "2026-01-28 13:49:27",
      "score": 6,
      "num_comments": 11,
      "upvote_ratio": 0.75,
      "text": "Ok I have little to no understanding on the topic, only basic programming skills and experience with LLMs. What is up with this recent craze over locally run LLMs and is it worth the hype. How is it possible these complex systems run on a tiny computers CPU/GPU with no interference with the cloud and does it make a difference if your running it in a 5k set up, a regular Mac, or what. It seems Claude has also had a ‚Äòfew‚Äô security breaches with folks leaving back doors into their own APIs. While other systems are simply lesser known but I don‚Äôt have the knowledge, nor energy, to break down the safety of the code and these systems. If someone would be so kind to explain their thoughts on the topic, any basic info I‚Äôm missing or don‚Äôt understand, etc. Feel free to nerd out, express anger, interest, I‚Äôm here for it all I just simply wish to understand this new era we find ourselves entering. ",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qpbuhf/local_llm_deployment/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2811gw",
          "author": "Rand_o",
          "text": "I run a 30B sized LLM locally on a 128 GB iGPU setup via AMD and it is decent. Ive spent the last 2 weeks learning how to set it all up, how it works, etc. It‚Äôs slower than cloud. If you wanna match cloud performance right now its probably $10k or more worth of equipment and you still wont exactly match claude or chatgpt. But I do think things are going to keep improving and we will eventually get to the point where running locally is extremely good. Right now it‚Äôs almost there but not quite for the average person. Still impressive though¬†",
          "score": 1,
          "created_utc": "2026-01-28 14:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o281kdx",
              "author": "Rand_o",
              "text": "As far as what device to get, depends what you wanna do. I got a framework desktop for $2k (which has jumped to nearly $3k for the same machine) but now I kinda wish I just got a mac studio instead. It works but it doesnt really have the performance it should imo. Or if you want a beast you need to drop serious money - the cost of a car lol",
              "score": 2,
              "created_utc": "2026-01-28 14:52:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29jn4l",
              "author": "Danzaar",
              "text": "How much slower is it?",
              "score": 1,
              "created_utc": "2026-01-28 18:49:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a69xe",
                  "author": "Rand_o",
                  "text": "it has acceptable speed for about 3 requests when I am trying to code with it. Each request will take 10-15 min. Past that - 4+ requests it takes an hour or more to complete. So I am trying to come up with techniques to do everything in small steps",
                  "score": 1,
                  "created_utc": "2026-01-28 20:29:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29dafl",
          "author": "attn-transformer",
          "text": "Local models are smaller, and often trained for a narrow use case.   \nOllama is a good place to start.",
          "score": 1,
          "created_utc": "2026-01-28 18:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bslc1",
          "author": "Clay_Ferguson",
          "text": "The main reason to use a local LLM (or SLM) is when you need privacy because you have customer data or other sensitive information that you don't want to send out across the web, or if not for privacy reasons then because there's just a vast amount of actual 'inference' (prompting) that you need to do over hundreds or thousands of files, perhaps , where it would get expensive to do it on a cloud paid service. \n\nwhat you lose is significant IQ points when you run locally, so if you have an extremely difficult problem to solve , or if you just want to write the best code possible and that's when you want to try to definitely use a best in class type SOTA model from an online provider. however, I might be exaggerating the loss of IQ points , because I think the local models might be only like one year behind the best LLMs in terms of their capabilities, so for most use cases the loss of IQ points is probably negligible .",
          "score": 1,
          "created_utc": "2026-01-29 01:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28fyq7",
          "author": "cmndr_spanky",
          "text": "The real use case is enterprise companies who don‚Äôt want to send data over to a cloud hosted model like chatGPT / Claude. For simple agentic or document chat systems you can get nearly equiv performance out of smaller LLMs. So even running a much bigger local LLM that‚Äôs 100 to 200b sized might be worth it, but often 32b is even good enough. Secondarily, with high token usage the costs of using vendor hosted models is also going to sting (even a mid sized company), and running a local model on $10k+ hardware can still save money in the long run. A lot of money.",
          "score": 0,
          "created_utc": "2026-01-28 15:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o296fkq",
              "author": "DistributionOk6412",
              "text": "If you can run a good local model on a $10k hardware for a 20 ppl company i'm giving you $10k. The costs are extremely high and I'm sick of ppl with no experience making costs estimations",
              "score": 2,
              "created_utc": "2026-01-28 17:53:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28uj7h",
              "author": "czmax",
              "text": "Even this is sometimes overstated... sending data to a trusted cloud hosted model with appropriate legal/contractual/security protections to manage risk can work well. \n\nLike just general compute there are tons of reasons for local vs cloud. Most importantly, having the choice is a good thing for the industry. Although I've played with local models on my home systems I'm simply not investing enough in HW to make it as effective as a cloud solution for me. But I'm stoked to see the enthusiasm and work that made it possible for me to experiment with it. \n\nI hope the pendulum swings back. When local models are powerful enough, when HW is cheap enough, and when model architectures support true learning I'm hoping we see models that can develop over time to be good partners to individuals. When/if that happens I don't want to see vendor lock-in and prefer a more open ecosystem.",
              "score": 0,
              "created_utc": "2026-01-28 17:01:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o293kf2",
          "author": "burntoutdev8291",
          "text": "Mostly safety and data governance. The local models cannot beat the larger models, but for specific use cases they might be sufficient. A good RAG system doesn't really need strong models.\n\nAnother factor is cost but this needs analytics. Can you prove that your workload will save more from upfront hardware costs vs API? Because don't forget hardware is depreciating (without considering the RAM surges).",
          "score": 0,
          "created_utc": "2026-01-28 17:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2910y6",
          "author": "Western_Bread6931",
          "text": "i was just browsing through pottery barn minding my own biz when suddenly this ridiculous clown in a bright orange wig and neon shoes comes strolling in honking that absolute monster of a horn im not even kidding it was like he wanted to clear out the whole store\nand what does my body decide to do betrayal i pooped myself like hard im talking call the cleanup crew level everyones staring and im just standing there frozen praying for a hole to swallow me whole the worst part the clowns just laughing like its the funniest thing hes seen all day\npottery barn staff had to call my wife to come get me im never going back there again if anyone needs me ill be here rethinking life choices and burning these pants",
          "score": -6,
          "created_utc": "2026-01-28 17:29:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}