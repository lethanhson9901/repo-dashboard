{
  "metadata": {
    "last_updated": "2026-02-19 17:08:33",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 116,
    "file_size_bytes": 122479
  },
  "items": [
    {
      "id": "1r49we9",
      "title": "AI Developer Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/mhyf0n56qdjg1.png",
      "author": "Main-Fisherman-2075",
      "created_utc": "2026-02-14 03:29:03",
      "score": 266,
      "num_comments": 49,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r49we9/ai_developer_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5c7ogy",
          "author": "TheDeadlyPretzel",
          "text": "Instructor is not an agent framework, rather it is a structured output inference library.\n\nOn the other hand, Atomic Agents which was built on top of instructor IS an agent framework: [https://github.com/BrainBlend-AI/atomic-agents](https://github.com/BrainBlend-AI/atomic-agents)",
          "score": 2,
          "created_utc": "2026-02-14 13:50:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5eza1p",
              "author": "Main-Fisherman-2075",
              "text": "thanks for point that out",
              "score": 0,
              "created_utc": "2026-02-14 22:38:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5gvvsr",
          "author": "walkingbiscuit",
          "text": "For Agent Development missing Google ADK, and i don't know where you want to put Chrome browser now, since in the beta release it has WebMCP",
          "score": 1,
          "created_utc": "2026-02-15 06:37:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65p3n9",
              "author": "Main-Fisherman-2075",
              "text": "Actually my problem about the mcp section is almost all toolls have mcp now and it's hard to say which one is better because they are just same idea of using the product. I am trying to make it sth like hosting mcp etc. not sure yet",
              "score": 1,
              "created_utc": "2026-02-19 00:39:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hzhwd",
          "author": "kubrador",
          "text": "looking at this like it's supposed help me pick a tool but it just makes me feel like i'm colorblind at a rave",
          "score": 1,
          "created_utc": "2026-02-15 12:46:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kxir2",
          "author": "afucher",
          "text": "Missing [ECA](https://eca.dev/)",
          "score": 1,
          "created_utc": "2026-02-15 22:00:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65p6z3",
              "author": "Main-Fisherman-2075",
              "text": "will check out!",
              "score": 1,
              "created_utc": "2026-02-19 00:40:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5ncjp9",
          "author": "bl_builder",
          "text": "This reminds me of AdTech landscape back in the day, 2019. https://static-prod.adweek.com/wp-content/uploads/2018/04/luma-1200.png",
          "score": 1,
          "created_utc": "2026-02-16 07:49:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65pat8",
              "author": "Main-Fisherman-2075",
              "text": "for market map there's a tons of versions i think haha. I think for me the most make sense thing is 1 for ai dev",
              "score": 1,
              "created_utc": "2026-02-19 00:41:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sjsnv",
          "author": "kovai_nvs",
          "text": "LLM newbie here. What tools would you use to analyse data to identify patterns?",
          "score": 1,
          "created_utc": "2026-02-17 01:53:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65ofrh",
              "author": "Main-Fisherman-2075",
              "text": "Hey! Iâ€™ll start with a quick self-intro ðŸ™‚ Iâ€™m with Keywords AI, where weâ€™re building LLM observability.",
              "score": 1,
              "created_utc": "2026-02-19 00:36:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5t8fbm",
          "author": "KongAtReddit",
          "text": "if you are doing design, you may also want to check out budgetpixel AI, it is like figma+AI steroid. ",
          "score": 1,
          "created_utc": "2026-02-17 04:29:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5thhef",
          "author": "Full-Signature8997",
          "text": "Parallel AI under web scraping too",
          "score": 1,
          "created_utc": "2026-02-17 05:35:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65pjp4",
              "author": "Main-Fisherman-2075",
              "text": "sure!",
              "score": 1,
              "created_utc": "2026-02-19 00:42:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yw2g5",
          "author": "EvKoh34",
          "text": "https://posthog.com/ai",
          "score": 1,
          "created_utc": "2026-02-18 00:58:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65pob4",
              "author": "Main-Fisherman-2075",
              "text": "will add!",
              "score": 1,
              "created_utc": "2026-02-19 00:43:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o64d4xd",
          "author": "Healthy_Library1357",
          "text": "wild how half this landscape will be acquired or dead by 2027 and weâ€™re all pretending itâ€™s stable ðŸ˜­ the real question isnâ€™t whatâ€™s on the map, itâ€™s which of these tools devs actually keep after the free credits run out",
          "score": 1,
          "created_utc": "2026-02-18 20:40:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65pukf",
              "author": "Main-Fisherman-2075",
              "text": "so trying to put how I heard people using right now on top so, and push an update every week",
              "score": 1,
              "created_utc": "2026-02-19 00:44:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o69kw7p",
                  "author": "Healthy_Library1357",
                  "text": "thatâ€™s smart. usage > hype. weekly updates keep it alive.",
                  "score": 1,
                  "created_utc": "2026-02-19 16:35:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67zbdl",
          "author": "SharpRule4025",
          "text": "AlterLab is missing from the web scraping section. We built it specifically for LLM and RAG pipelines. Structured JSON output instead of markdown dumps, tiered pricing so a static HTML page doesn't cost the same as a Cloudflare-protected SPA. alterlab.io",
          "score": 1,
          "created_utc": "2026-02-19 10:51:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5abluh",
          "author": "economicscar",
          "text": "Prime intellect missing under inference and compute",
          "score": 1,
          "created_utc": "2026-02-14 04:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br22k",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 0,
              "created_utc": "2026-02-14 11:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ccb1u",
                  "author": "Equity_Harbinger",
                  "text": "Can you share the latest one please (which is also less blurry, because when I zoom the image, words are blurry beyond recognition)\n\n\n(Thank you for your contributions)",
                  "score": 1,
                  "created_utc": "2026-02-14 14:18:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5atgtt",
          "author": "Live-Speech-1058",
          "text": "Antigravity?",
          "score": 1,
          "created_utc": "2026-02-14 06:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br5gi",
              "author": "Main-Fisherman-2075",
              "text": "I think I added it, I don't know why it's not there but definitely worth a try.",
              "score": 0,
              "created_utc": "2026-02-14 11:47:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bu98n",
          "author": "increasinglybold",
          "text": "Pi coding agent is great",
          "score": 1,
          "created_utc": "2026-02-14 12:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ezbgx",
              "author": "Main-Fisherman-2075",
              "text": "Will check it out",
              "score": 1,
              "created_utc": "2026-02-14 22:39:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bz0vl",
          "author": "Realistic-Damage2004",
          "text": "https://ainativedev.io/landscape\n\nHas been around for a while now. Is this published anywhere?",
          "score": 1,
          "created_utc": "2026-02-14 12:51:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65opp1",
              "author": "Main-Fisherman-2075",
              "text": "ours is here now: [https://www.keywordsai.co/market-map](https://www.keywordsai.co/market-map), trying to polish this further but.",
              "score": 1,
              "created_utc": "2026-02-19 00:37:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5djaiy",
          "author": "Neferio1",
          "text": "Greptile is a very good code review tool",
          "score": 1,
          "created_utc": "2026-02-14 18:02:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ez8zy",
              "author": "Main-Fisherman-2075",
              "text": "1000%",
              "score": 1,
              "created_utc": "2026-02-14 22:38:36",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5fgo08",
              "author": "oXeNoN",
              "text": "How does it compare with other tools like CodeRabbit? Is CodeRabbit just spending more on marketing? ðŸ˜…",
              "score": 0,
              "created_utc": "2026-02-15 00:24:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fuy08",
                  "author": "Neferio1",
                  "text": "We tested both and we choose Greptile over CodeRabbit just because Greptile can be Â«Â selfhostedÂ Â» using Kubernetes or Docker. From a review perspective, Greptile and CodeRabbit are equivalent",
                  "score": 2,
                  "created_utc": "2026-02-15 01:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5bn5o0",
          "author": "mcd0g",
          "text": "Warp under coding agents missing. They really need to step up their PR game",
          "score": 0,
          "created_utc": "2026-02-14 11:11:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5br1u4",
              "author": "Main-Fisherman-2075",
              "text": "will add right away",
              "score": 1,
              "created_utc": "2026-02-14 11:47:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5bth4g",
          "author": "renntv",
          "text": "Great overview! Do you keep it on the web for linking, or just here on Reddit? ",
          "score": 0,
          "created_utc": "2026-02-14 12:07:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etdza",
              "author": "Main-Fisherman-2075",
              "text": "Hey I keep it here: but the content inside is still not very polished yet. https://www.keywordsai.co/market-map I will try to add all the description comparison price etc inside",
              "score": 1,
              "created_utc": "2026-02-14 22:05:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5c2b8u",
              "author": "Code_Exists_Here",
              "text": "Yeh same question from me.",
              "score": 0,
              "created_utc": "2026-02-14 13:15:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egtyh",
          "author": "funguslungusdungus",
          "text": "I need a link!",
          "score": 0,
          "created_utc": "2026-02-14 20:56:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5etiuy",
              "author": "Main-Fisherman-2075",
              "text": "https://www.keywordsai.co/market-map here you go! I will try to update weekly and the content in it",
              "score": 1,
              "created_utc": "2026-02-14 22:06:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5egxm5",
          "author": "Varqu",
          "text": "You can just use Claude Code.",
          "score": 0,
          "created_utc": "2026-02-14 20:57:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fkv68",
          "author": "Disastrous-Maybe2501",
          "text": "Mistral Vibe missing in coding agents",
          "score": 0,
          "created_utc": "2026-02-15 00:50:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65owvd",
              "author": "Main-Fisherman-2075",
              "text": "will add",
              "score": 1,
              "created_utc": "2026-02-19 00:38:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6nw3e",
      "title": "AI Coding Agent Dev Tools Landscape 2026",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/gm88nuyrlxjg1.png",
      "author": "bhaktatejas",
      "created_utc": "2026-02-16 22:20:01",
      "score": 264,
      "num_comments": 31,
      "upvote_ratio": 0.96,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6nw3e/ai_coding_agent_dev_tools_landscape_2026/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ri3mi",
          "author": "bhaktatejas",
          "text": "link [https://www.morphllm.com/market-map](https://www.morphllm.com/market-map)",
          "score": 4,
          "created_utc": "2026-02-16 22:20:09",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5sbifa",
          "author": "btdeviant",
          "text": "It's weird how many of these guides and people are sleeping on [Strands](https://strandsagents.com/latest/). Hands down the most dead simple, capable provider agnostic agentic framework out there.. swings far above it's weight. ",
          "score": 5,
          "created_utc": "2026-02-17 01:03:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t9rlv",
              "author": "teambyg",
              "text": "Strands is also one of the smartest BETS from a future proofing perspective. Many of the small start up frameworks will die. Many probably very soon, so trusting in bigger names is likely to lead to long term viability (Lindy Effect). Provider frameworks, AWS, and the Pydantic team are probably the only one's I would consider right now for any enterprise application",
              "score": 2,
              "created_utc": "2026-02-17 04:38:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5vjcfg",
                  "author": "echology-io",
                  "text": "thanks for the insight. I will check it out. ",
                  "score": 1,
                  "created_utc": "2026-02-17 15:02:17",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o61bve5",
                  "author": "yeathatsmebro",
                  "text": "Vercel's AI SDK has someone that is 100% dedicated on the project and is not sketchy. Only if you use Typescript though.",
                  "score": 1,
                  "created_utc": "2026-02-18 11:39:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5wst35",
              "author": "kabs1194",
              "text": "I've really appreciated LangGraph and my own custom context management, any thoughts on comparison with Strands?",
              "score": 1,
              "created_utc": "2026-02-17 18:42:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5swyy1",
              "author": "AdditionalWeb107",
              "text": "its yet another framework - and haven't we gotten pass this point that its just one while loop. The real hard part is the stuff around the loop",
              "score": 1,
              "created_utc": "2026-02-17 03:13:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5t554z",
                  "author": "btdeviant",
                  "text": "Right. The salient point is its abstractions allow one to focus more on â€œthe stuff around the loopâ€. \n\nItâ€™s a well designed framework and more tailored toward modern, multi-agent architectures compared to nearly all the others in that list, majority of which are relative dinosaurs and objectively a much bigger pain to work with for complex, code-first workflows. \n\nGive it a shot! I have no affiliation, just used most of them and found Strands a great blend of depth and breadth, especially with their (experimental) BIDI. Just a breeze to work with compared to all the others.",
                  "score": -1,
                  "created_utc": "2026-02-17 04:06:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5rxdge",
          "author": "fredandlunchbox",
          "text": "No conductor?",
          "score": 1,
          "created_utc": "2026-02-16 23:42:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud25m",
              "author": "bhaktatejas",
              "text": "added!",
              "score": 1,
              "created_utc": "2026-02-17 10:22:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5s84h7",
          "author": "skarpa10",
          "text": "I think Google ADK supposed to be GitHub Copilot SDK.",
          "score": 1,
          "created_utc": "2026-02-17 00:44:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uexh5",
              "author": "Darxeal",
              "text": "no, both exist",
              "score": 2,
              "created_utc": "2026-02-17 10:39:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5sde7c",
          "author": "LoyalLittleOne",
          "text": "There's that many ?",
          "score": 1,
          "created_utc": "2026-02-17 01:15:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud2lc",
              "author": "bhaktatejas",
              "text": "theres even more",
              "score": 2,
              "created_utc": "2026-02-17 10:22:49",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5v8jju",
              "author": "OkTry9715",
              "text": "AI slop is reproducing fast",
              "score": 1,
              "created_utc": "2026-02-17 14:05:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5tydj1",
          "author": "j4ys0nj",
          "text": "Where would [Mission Squad](https://missionsquad.ai) go? What about OpenClaw?",
          "score": 1,
          "created_utc": "2026-02-17 08:03:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud3ez",
              "author": "bhaktatejas",
              "text": "wouldnt consider them coding agents, more general agents",
              "score": 1,
              "created_utc": "2026-02-17 10:23:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5u6rb9",
          "author": "Varqu",
          "text": "What's the point of putting nvidia out there? ",
          "score": 1,
          "created_utc": "2026-02-17 09:23:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ud64d",
              "author": "bhaktatejas",
              "text": "they have an inference service via brev. its not up to market standards. I've used it, but its getting better",
              "score": 2,
              "created_utc": "2026-02-17 10:23:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5vvk60",
          "author": "Terrible-Rooster1586",
          "text": "I think ellipsis is dead sadly. I was an early adopter but they lost their CTO/cofounder to cursor and havenâ€™t posted anything on linked in in months",
          "score": 1,
          "created_utc": "2026-02-17 16:02:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zxnmt",
          "author": "infraPulseAi",
          "text": "Interesting landscape. Curious how many of these tools handle deterministic verification and signed execution receipts for agent-to-agent transactions â€” that layer feels missing in most stacks.",
          "score": 1,
          "created_utc": "2026-02-18 04:31:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o635z39",
          "author": "Johhaidiidiralla",
          "text": "[https://zed.dev/](https://zed.dev/)",
          "score": 1,
          "created_utc": "2026-02-18 17:25:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65ntqp",
          "author": "Delicious-Word4776",
          "text": "So true! Thanks for sharing, it was very amusing.",
          "score": 1,
          "created_utc": "2026-02-19 00:32:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68llmj",
          "author": "hroyhong",
          "text": "Where do products like base44 and atoms fall? There's literally an ad of base44 in this post.",
          "score": 1,
          "created_utc": "2026-02-19 13:33:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s40kg",
          "author": "AdditionalWeb107",
          "text": "Missing the data plane for agentic apps. [https://github.com/katanemo/plano](https://github.com/katanemo/plano) \\- cuts between the framework and gateway category as delivery infrastructure",
          "score": 1,
          "created_utc": "2026-02-17 00:20:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8jw2b",
      "title": "Building an opensource Living Context Engine",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/5cf7c7efeckg1",
      "author": "DeathShot7777",
      "created_utc": "2026-02-19 00:05:55",
      "score": 131,
      "num_comments": 31,
      "upvote_ratio": 0.99,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8jw2b/building_an_opensource_living_context_engine/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67phrl",
          "author": "SeaworthinessThis598",
          "text": "what is this sorcery or i mean graphery ...",
          "score": 8,
          "created_utc": "2026-02-19 09:18:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67pntd",
              "author": "DeathShot7777",
              "text": "ðŸ˜‚ Knowledge Graph + Clustering Algorithm + AST Maps + Webgl rendering -- bit too nerdy i guess ðŸ˜…",
              "score": 1,
              "created_utc": "2026-02-19 09:19:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67ptym",
                  "author": "SeaworthinessThis598",
                  "text": "please teach me how to conjure this potion can i contribute ?",
                  "score": 1,
                  "created_utc": "2026-02-19 09:21:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o693jql",
                  "author": "Sorry_Swan_8997",
                  "text": "Love it ðŸ˜",
                  "score": 1,
                  "created_utc": "2026-02-19 15:10:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66zptc",
          "author": "Crafty_Disk_7026",
          "text": "Can you post a comparison using it versus not?",
          "score": 3,
          "created_utc": "2026-02-19 05:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6771hd",
              "author": "DeathShot7777",
              "text": "Great suggestion, working on setting up evals, ( swe bench ).",
              "score": 3,
              "created_utc": "2026-02-19 06:27:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67937x",
          "author": "Several_Explorer1375",
          "text": "Thatâ€™s amazing. Might try it tomorrow",
          "score": 2,
          "created_utc": "2026-02-19 06:44:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o679cv0",
              "author": "DeathShot7777",
              "text": "Thanks. Lemme know how it goes",
              "score": 2,
              "created_utc": "2026-02-19 06:46:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67n3jw",
          "author": "sleepnow",
          "text": "Looks pretty, but seems like performance would degrade pretty quickly",
          "score": 2,
          "created_utc": "2026-02-19 08:54:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67nplw",
              "author": "DeathShot7777",
              "text": "Ya the webapp can be used as a deeper deepwiki for mid sized repos. For actual usecase with MCP support it has gitnexus cli tool, i tried on a massive repo ( metafresh ) takes about 92 seconds to parse.",
              "score": 2,
              "created_utc": "2026-02-19 09:00:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o66m755",
          "author": "jack_pisi0n",
          "text": "That's creative man, imma use it soon. good luck.",
          "score": 1,
          "created_utc": "2026-02-19 03:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67726a",
              "author": "DeathShot7777",
              "text": "Thanks a lot",
              "score": 1,
              "created_utc": "2026-02-19 06:27:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67aqbg",
          "author": "TwistStrict9811",
          "text": "Very cool - I'll see how codex works with it",
          "score": 1,
          "created_utc": "2026-02-19 06:58:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67b70d",
              "author": "DeathShot7777",
              "text": "Great. Lemme know how it goes. It should work best on queries like \n\n\"whats the execution flow from API emdpoint to storage\",\n\n \"we want to split it into microservices eventually, show me the actual dependency boundaries\"\n\nOr debugging related queries",
              "score": 1,
              "created_utc": "2026-02-19 07:02:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67gdeo",
          "author": "NachosforDachos",
          "text": "Now thatâ€™s sexy",
          "score": 1,
          "created_utc": "2026-02-19 07:49:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ggxu",
              "author": "DeathShot7777",
              "text": "ðŸ« ðŸ¥€",
              "score": 1,
              "created_utc": "2026-02-19 07:50:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67j742",
          "author": "tineo_app",
          "text": "holy shit this belongs in an art gallery",
          "score": 1,
          "created_utc": "2026-02-19 08:15:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67jdfp",
              "author": "DeathShot7777",
              "text": "ðŸ˜‚ thanks ðŸ¥€",
              "score": 1,
              "created_utc": "2026-02-19 08:17:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67mucg",
          "author": "bunnydathug22",
          "text": "You looking for a team by chance ?",
          "score": 1,
          "created_utc": "2026-02-19 08:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ng2d",
              "author": "DeathShot7777",
              "text": "Its opensource, would love contributions",
              "score": 1,
              "created_utc": "2026-02-19 08:57:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67oua7",
                  "author": "bunnydathug22",
                  "text": "Its not the code that we are interested in. Nor is it oss.  We do [this](http://Www.citadel-nexus.com) totattly respect you and you work. If you change your mind hit us up.",
                  "score": 1,
                  "created_utc": "2026-02-19 09:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68rich",
          "author": "SnooPeripherals5313",
          "text": "I love this! Great job.",
          "score": 1,
          "created_utc": "2026-02-19 14:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68wrfb",
          "author": "jeelm29",
          "text": "I'm new how do I even start bro",
          "score": 1,
          "created_utc": "2026-02-19 14:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69i9f2",
          "author": "Able-Let-1399",
          "text": "At a time when more and more code is delivered by your personal AI pusher, this sounds like an excellent tool to keep it in check and even make it better. Kudos for connecting the dots ðŸ‘\n\nAny way to merge multiple graphs? For various reasons I have per-service repos.",
          "score": 1,
          "created_utc": "2026-02-19 16:22:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8if0v",
      "title": "Open Source LLM Tier List",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/y5i85f4hxbkg1.png",
      "author": "HobbyGamerDev",
      "created_utc": "2026-02-18 23:04:27",
      "score": 42,
      "num_comments": 16,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8if0v/open_source_llm_tier_list/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o66cy77",
          "author": "robogame_dev",
          "text": "https://preview.redd.it/tyl32sgg9dkg1.png?width=1518&format=png&auto=webp&s=db5e80f5180bd671427a25791a922540857c8aef\n\nThis is what it shows now",
          "score": 6,
          "created_utc": "2026-02-19 02:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65yi8f",
          "author": "Guilty_Serve",
          "text": "ChatGPT oss is really that good? Honest question.",
          "score": 5,
          "created_utc": "2026-02-19 01:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o683cyw",
              "author": "ScoreUnique",
              "text": "120b is a very good model. I won't hesitate saying it's o1 level at least. You can run it with fairly less hardware if you have a beefy GPU and if you like that openai style chat.",
              "score": 2,
              "created_utc": "2026-02-19 11:26:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o683ccr",
              "author": "Alex_1729",
              "text": "It's decent. Depends on what you need it for.",
              "score": 1,
              "created_utc": "2026-02-19 11:26:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67mh1t",
              "author": "jnk_str",
              "text": "No",
              "score": 0,
              "created_utc": "2026-02-19 08:48:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67a9z7",
          "author": "decentralize999",
          "text": "Wrong description. Open weight LLMs,  not open souce ones.\n\nAnd top list is joke. Where is step3.5-flash which is the best among open weight llms if compare benchmark points per 100B size.",
          "score": 4,
          "created_utc": "2026-02-19 06:54:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6726h6",
          "author": "sergeant113",
          "text": "Minimax 2.5 where?",
          "score": 3,
          "created_utc": "2026-02-19 05:47:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o659jcl",
          "author": "bebackground471",
          "text": "RemindMe! 8 days",
          "score": 1,
          "created_utc": "2026-02-18 23:14:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o659oby",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 8 days on [**2026-02-26 23:14:14 UTC**](http://www.wolframalpha.com/input/?i=2026-02-26%2023:14:14%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LLMDevs/comments/1r8if0v/open_source_llm_tier_list/o659jcl/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLLMDevs%2Fcomments%2F1r8if0v%2Fopen_source_llm_tier_list%2Fo659jcl%2F%5D%0A%0ARemindMe%21%202026-02-26%2023%3A14%3A14%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r8if0v)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-18 23:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65t7r8",
          "author": "IgnisIason",
          "text": "Ring 2.5 1T if you've got an extra Colossus to run it.",
          "score": 1,
          "created_utc": "2026-02-19 01:03:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66rk18",
          "author": "Snoo_24581",
          "text": "Interesting rankings. How do you weigh coding ability vs general reasoning? For API work I have been using Qwen models for code tasks and they punch above their weight class.",
          "score": 1,
          "created_utc": "2026-02-19 04:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67gaum",
          "author": "FriendlySecond2460",
          "text": "this is writers wish list",
          "score": 1,
          "created_utc": "2026-02-19 07:48:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67ytua",
          "author": "Moki2FA",
          "text": "This tier list looks super interesting, I love seeing how different open source LLMs stack up against each other. Iâ€™m curious about how the evaluation criteria were determined; it would be great to understand more about what factors contributed to their rankings. Could anyone share more insight on that?",
          "score": 1,
          "created_utc": "2026-02-19 10:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o683h1m",
          "author": "Alex_1729",
          "text": "Step flash and Trinity should be on the list.",
          "score": 1,
          "created_utc": "2026-02-19 11:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69nqvp",
          "author": "Available-Message509",
          "text": "Seriously, huge thanks to the team behindÂ **GPT-oss 120B**. Itâ€™s such a relief to have a high-performing Tier A model that actually fits on our local GPU setups. Most of the newer models like GLM-5 or Kimi are just getting way too massive for home servers (700B+ is wild..). 120B is the real sweet spot for us!",
          "score": 1,
          "created_utc": "2026-02-19 16:49:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4ylja",
      "title": "[Release] AdaLLM: NVFP4-first inference on RTX 4090 (FP8 KV cache + custom FP8 decode)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "author": "Educational_Cry_7951",
      "created_utc": "2026-02-14 22:59:37",
      "score": 34,
      "num_comments": 4,
      "upvote_ratio": 0.98,
      "text": "Hey folks, I have been working on **AdaLLM** (repo: [https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)) to make NVFP4 weights actually usable on Ada Lovelace GPUs (sm\\_89). The focus is a pure NVFP4 fast path: FP8 KV cache, custom FP8 decode kernel, no silent FP16 fallback. It currently targets Qwen3 (dense + MoE) and Gemma3 (including sliding-window layers), I'll be adding support to other models soon.\n\n>**Please think of giving the Github repo a STAR if you like it :)**\n\n# Why this is interesting\n\n* NVFP4-first runtime for Ada GPUs (tested on RTX 4090) with FP8 KV cache end-to-end.\n* Custom Triton FP8 decode kernel; prefill uses FlashAttention (varlen).\n* No FP16 fallback for decode. If FP8 kernel fails, it errors out instead of silently switching.\n* Tensor-parallel (NCCL) + CUDA graphs for decode (also support eager mode)\n\n# Benchmarks (RTX 4090)\n\n**Qwen3-8B-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|3.3867|37.79|7.55|\n|2|256|3.5471|72.17|7.55|\n|4|512|3.4392|148.87|7.55|\n|8|1024|3.4459|297.16|7.56|\n|16|2048|4.3636|469.34|7.56|\n\n**Gemma3-27B-it-NVFP4**\n\n|batch|total tokens|seconds|tok/s|peak GB|\n|:-|:-|:-|:-|:-|\n|1|128|9.3982|13.62|19.83|\n|2|256|9.5545|26.79|19.83|\n|4|512|9.5344|53.70|19.84|\n\nfor Qwen3-8B-NVFP4 I observed \\~2.4x lower peak VRAM vs Qwen3-8B FP16 baselines (with \\~20-25% throughput loss).\n\n# Quickstart\n\n    pip install git+https://github.com/BenChaliah/NVFP4-on-4090-vLLM.git\n    \n    adallm serve nvidia/Qwen3-8B-NVFP4\n\n>\\`export NVFP4\\_FP8=1\\` is optional and enables FP8 GEMM path (NVFP4\\_FP8=0: the difference is in compute precision not VRAM, FP8 KV cache + the FP8 decode kernel are still used.\n\n**Supported models (so far)**\n\n* `nvidia/Qwen3-8B-NVFP4`\n* `BenChaliah/Gemma3-27B-it-NVFP4`\n* Qwen3 MoE variants are supported, but still slow (see README for MoE notes).\n\n**Limitations**\n\n* MoE routing and offload paths are not fully optimized yet (working on it currently)\n* Only NVFP4 weights, no FP16 fallback for decode by design.\n* Targeted at Ada Lovelace (sm\\_89). Needs validation on other Ada cards.\n\n# Repo\n\n[https://github.com/BenChaliah/NVFP4-on-4090-vLLM](https://github.com/BenChaliah/NVFP4-on-4090-vLLM)\n\nIf you have a RTX 4000 series GPU, I would love to hear results or issues. Also looking for help on MoE CPU-Offloading optimization, extra model support, and kernel tuning.",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4ylja/release_adallm_nvfp4first_inference_on_rtx_4090/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5fifzx",
          "author": "Vearres17",
          "text": "The fact that you kept Gemma3 sliding-window attention in FP8 is impressive. I've seen some implementations that fall back to fp16 for the local attention layers I guess it bcz it can be tricky to handle",
          "score": 1,
          "created_utc": "2026-02-15 00:35:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5fjkgw",
              "author": "Educational_Cry_7951",
              "text": "thanks tbf it was a pain for me too at first",
              "score": 1,
              "created_utc": "2026-02-15 00:42:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5hcpmr",
          "author": "Delicious-One-5129",
          "text": "This is seriously impressive work. An actual NVFP4 first path on RTX 4090 without silent FP16 fallback is huge for people squeezing every GB out of Ada cards. The VRAM savings vs FP16 are especially compelling.",
          "score": 1,
          "created_utc": "2026-02-15 09:19:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5hwkm4",
              "author": "Educational_Cry_7951",
              "text": "Thank you! ",
              "score": 1,
              "created_utc": "2026-02-15 12:23:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6zfnj",
      "title": "How are they actually deployed in production at scale?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6zfnj/how_are_they_actually_deployed_in_production_at/",
      "author": "hareld10",
      "created_utc": "2026-02-17 07:16:05",
      "score": 27,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "Iâ€™m trying to understand how giants LLMs systems like ChatGPT/Claude are deployed in production.\n\nSpecifically curious about:\n\nâ€¢ Inference stack (custom engine vs vLLM-like architecture?)  \nâ€¢ API behind  \nâ€¢ Database   \nâ€¢ GPU orchestration (Kubernetes? custom scheduler?)  \nâ€¢ Sharding strategy (tensor / pipeline parallelism?)  \nâ€¢ How latency is kept low under burst traffic  \nâ€¢ Observability + guardrail systems\n\nI know nobody has internal details, but based on public info, talks, papers, or experience deploying large models -  whatâ€™s the likely architecture?\n\nI'm asking because I want to prepare a knowledge kit for system design questions at this level.\n\nWould love input from people running 30B+ models in production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6zfnj/how_are_they_actually_deployed_in_production_at/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5twdrw",
          "author": "Once_ina_Lifetime",
          "text": "From what I have seen publicly, most large LLM deployments look like layered infra , optimized inference engines (vLLM/Triton/custom), heavy GPU orchestration with Kubernetes or internal schedulers, aggressive caching/batching for latency, and strong observability/guardrails on top. Exact details vary, but itâ€™s basically a reliability + infra engineering problem more than just model serving.",
          "score": 7,
          "created_utc": "2026-02-17 07:44:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwoij",
              "author": "pmv143",
              "text": "Exactly. The interesting part is that once you solve model execution. thecomplexity shifts to orchestration and memory lifecycle management. Thatâ€™s where most production pain seems to live.",
              "score": 2,
              "created_utc": "2026-02-17 12:56:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63m5by",
                  "author": "singh_taranjeet",
                  "text": "In production itâ€™s usually orchestration, guardrails, evals, caching, and state management doing the heavy lifting while the model is just one component in a bigger system. The real work is reliability, monitoring, and handling edge cases at scale.",
                  "score": 1,
                  "created_utc": "2026-02-18 18:36:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ue2ql",
          "author": "AdPutrid2974",
          "text": "That's the million-dollar question! Most likely a mix of custom C++ engines and massive Kubernetes clusters. Dealing with that level of burst traffic must be an engineering nightmare.",
          "score": 4,
          "created_utc": "2026-02-17 10:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5umpe4",
              "author": "hareld10",
              "text": "I want to construct prep kit to interviews, so its not have to be 1-1 :)",
              "score": 1,
              "created_utc": "2026-02-17 11:46:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5uwxel",
              "author": "pmv143",
              "text": "Probably a mix, yeah. Custom kernels and tight C++ runtimes make sense at that scale. But beyond the engine itself, I suspect a lot of the real complexity lives in scheduling, memory management, and how they handle burst traffic without fragmenting GPU memory.",
              "score": 1,
              "created_utc": "2026-02-17 12:58:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uiao1",
          "author": "kleinmatic",
          "text": "Read downtime post-mortems that tech companies publish after big outages. Theyâ€™re always full of details on the exotic setups of very high scale systems. On GitHub look for danluu/post-mortems but there are others as well. Theyâ€™re fascinating to read. \n\nWith that much money and scale Iâ€™m betting itâ€™s way different and more custom than you think.",
          "score": 3,
          "created_utc": "2026-02-17 11:09:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5tvbms",
          "author": "Abu_BakarSiddik",
          "text": "This is a very cool thing to learn about.\n\nIâ€™m currently working on scaling our platform at the DB level, and itâ€™s a completely different problem compared to scaling LLM inference. At the database layer, it mostly comes down to:\n\n* Managing connection lifecycle properly\n* Keeping transactions short\n* Handling long-lived sessions carefully (especially with streaming)\n* Using replicas effectively\n\nIf you mess up connection management, holdconnection hostage, everything falls apart. Thatâ€™s usually the real bottleneck. With LLM systems, the bottleneck is about GPU compute and memory. The main things are:\n\n* Efficient batching of incoming requests\n* Maximizing GPU utilization\n* Managing KV cache memory properly\n* Supporting high concurrency\n\nModern frameworks like vLLM help a lot here. Things like paged attention, continuous batching, and FlashAttention make it possible to handle large numbers of concurrent requests efficiently. Memory management is critical, but these frameworks abstract a lot of that complexity away.\n\nSo DB scaling is mostly about connection discipline and replication strategy. LLM scaling is about batching efficiency and GPU orchestration.",
          "score": 3,
          "created_utc": "2026-02-17 07:34:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uwe8m",
              "author": "pmv143",
              "text": "This is really well put. The â€˜different bottlenecks, different failure modesâ€™ framing is key. With LLM systems you can have perfect API and DB hygiene and still fall apart purely due to KV cache pressure or poor batching under burst traffic.",
              "score": 2,
              "created_utc": "2026-02-17 12:54:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5uvzet",
          "author": "pmv143",
          "text": "Nobody outside those orgs knows the exact internals, but based on public talks and production constraints, the architecture likely looks something like this:\n\n\t1.Inference Engine\n\nNot stock vLLM. Likely heavily customized runtime layers optimized for:\nâ€“ KV cache management\nâ€“ Scheduling + batching\nâ€“ Memory locality\nâ€“ Tensor + pipeline parallelism coordination\nvLLM concepts, but production hardened and deeply modified.\n\n\t2.GPU Orchestration\n\nKubernetes at the outer layer for cluster management.\nCustom schedulers at the GPU level.\nYou cannot rely on vanilla k8s scheduling when GPUs cost this much and memory is not oversubscribable.\n\n\t3.Sharding Strategy\nLarge models: tensor parallelism within a node, pipeline parallelism across nodes.\nMoE adds routing complexity.\nEverything optimized around minimizing cross node bandwidth.\n\n\t4.Latency Under Burst\n\nTwo strategies:\nâ€“ Keep massive pools warm at high utilization\nâ€“ Aggressive batching with tight admission control\nTrue scale to zero serverless does not really exist at this tier.\n\n\t5.API + Gateway Layer\nHigh performance stateless frontends\nQueueing + prioritization\nStreaming responses over HTTP/2 or gRPC\n\n\t6.Observability + Guardrails\nPer token tracing\nReal time safety filters\nShadow traffic for model eval\nCanary deployments for new weights\n\nThe hard part is not just loading the model.\nItâ€™s scheduling, memory, and utilization at scale.\n\nCold start optimization matters only if it works in production traffic, not just in a benchmark.",
          "score": 1,
          "created_utc": "2026-02-17 12:52:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v8jf8",
          "author": "burntoutdev8291",
          "text": "Check out production stack helm chart",
          "score": 1,
          "created_utc": "2026-02-17 14:05:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o611jsj",
          "author": "GarbageOk5505",
          "text": "For burst traffic the answer is almost always overprovisioning plus request queuing with dynamic batching you're trading latency variance for throughput. Routing based on sequence length helps too, you don't want a 4-token request waiting behind a 32k context job.\n\nFor the system design prep angle: the Megatron-LM and PaLM papers are worth reading carefully, and Meta's LLaMA inference posts are surprisingly detailed about production tradeoffs.\n\n  \nfor the others I need to research furhter, good stuff pointing that out",
          "score": 1,
          "created_utc": "2026-02-18 10:11:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3kgpn",
      "title": "Rearchitecting LLMs â€” pruning, distillation, and smaller domain models (MEAP)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "author": "ManningBooks",
      "created_utc": "2026-02-13 09:07:18",
      "score": 25,
      "num_comments": 20,
      "upvote_ratio": 0.93,
      "text": "Hi r/LLMDevs,\n\nStjepan from Manning here. The mods said it's ok if I post this here. \n\nWeâ€™ve just released a book thatâ€™s very much aimed at the kinds of problems this community discusses all the time: what to do when a general-purpose LLM is technically impressive but awkward, expensive, or inefficient for your actual use case.\n\n**Rearchitecting LLMs** by Pere Martra  \n[https://www.manning.com/books/rearchitecting-llms](https://hubs.la/Q042-hLy0)\n\n[Rearchitecting LLMs by Pere Martra](https://preview.redd.it/vyy079zx78jg1.jpg?width=2213&format=pjpg&auto=webp&s=755a8b1ab1320ede5daedfa861d6ab8d1b0c5e5d)\n\nThe core idea of the book is simple but powerful: instead of treating open models as fixed artifacts, you can reshape them. Pere walks through structural techniques like targeted fine-tuning, pruning, and knowledge distillation to build smaller, cheaper, domain-focused models that still perform well on the tasks you care about.\n\nWhat makes this book interesting is how hands-on it gets. Youâ€™re not working with abstract toy networks. The examples focus on modifying widely used open models, such as Llama-3, Gemma, and Qwen. The focus is on understanding which parts of a model actually contribute to behavior, how to identify waste or redundancy, and how to remove or compress components without blindly wrecking performance.\n\nThereâ€™s also some genuinely thoughtful material on combining behavioral analysis with structural changes. Instead of just cutting parameters and hoping for the best, the book explores ways to reason about why a modification works or fails. One section that tends to spark discussion is â€œfair pruning,â€ where pruning is used not only for efficiency but also to reduce bias at the neuron level.\n\nIf youâ€™re working on local models, cost-constrained deployments, or specialized SLMs, this book is very much in that territory. Itâ€™s written for people who are comfortable with LLM concepts and want to go deeper into how models can be reshaped rather than simply prompted.\n\n**For the** r/LLMDevs **community:**  \nYou can get **50% off** with the code **MLMARTRA50RE**.\n\nA quick note on availability: the book is currently in **MEAP (Manning Early Access Program)**. That means you get immediate access to the chapters as theyâ€™re written, along with updates as the manuscript evolves.\n\nHappy to bring the author to answer questions about the book, the techniques it covers, or the kinds of readers itâ€™s best suited for. And Iâ€™d be curious to hear from folks here who are already doing pruning or distillation in practice â€” whatâ€™s been harder than expected?\n\nI'm ready to give away 5 ebooks to the first five commenters who share their experience here.\n\nThank you all for having us. It feels great to be here.\n\nCheers,",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r3kgpn/rearchitecting_llms_pruning_distillation_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o54xstz",
          "author": "StackSmashRepeat",
          "text": "Would you list some common problems and terminologies that the book covers? I'll have a look if it peaks my interest.",
          "score": 5,
          "created_utc": "2026-02-13 09:31:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5529pk",
              "author": "ManningBooks",
              "text": "Hey, thanks for asking. Here are some examples of what the book covers:\n\n\\- End-to-End Model Re-architecting (Chapter 2): Transform Gemma-3-270M using depth pruning and knowledge distillation for a 10% speed increase while retaining 93-98% of original capabilities in a hands-on project.\n\n\\- Data-Driven Pruning (Chapters 4-5): Create two models from the same base (Qwen3-0.6B or Llama-3.2-1B): one for formal texts (WikiText) and another for short messages (SMS Spam), using activation analysis with PyTorch hooks to highlight domain-specific component importance.\n\n\\- Bias Auditing and Correction: In ethics chapters, perform a model \"cleanup\" using ablation frameworks and PCA visualization to identify and mitigate demographic biases, achieving fairness without full retraining.\n\n\\- Mini-Capstone: Utilize a small \"draft model\" to speed up LLM inference by quickly proposing tokens, validated by a larger model.\n\n\\- Capstone Project: Migrate an agent system from costly external APIs to a specialized local Small Language Model (SLM).\n\nHope this helps.",
              "score": 7,
              "created_utc": "2026-02-13 10:13:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o56j1zc",
                  "author": "StackSmashRepeat",
                  "text": "This is quite interesting; you're trying to move local models away from the static model while staying within the static framework? I could basically train a model for my iPhone, let's say I export all my email and scrub PII, format for training data and then I could fine-tune to write mails that look somewhat within the realm of my own style? \n\nI haven't looked into training or fine tuning as I couldn't think of a personal use case for these tiny models, but like you're saying \"domain-focused\", gave a clearer picture.\n\nThis is a little over my current scope as I'm not even sure if I understood this correctly, but I've been thinking of ways to make a digital twin that could handle writing across multiple platforms. Was always thinking Id need a larger model to handle such a task because it sounds easy enough, but capturing the essence of one's writing is quite a complex task for llms. At least in my experience.\n\nThanks for the info.",
                  "score": 3,
                  "created_utc": "2026-02-13 15:45:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57f10h",
              "author": "pmartra",
              "text": "Hi u/StackSmashRepeat \n\nI'll give you a rough explanation and if you want more details just ask me. \n\nIn the book I explain an LLM optimization / customization pipeline. The pipeline basically consists of: \n\n1- Pruning (depth and width) which is removing parts of a model. \n\n2- Knowledge Distillation. Recovering the lost knowledge by transferring it from the base model to the pruned one. \n\n3- Finetuning on a specialized domain. \n\nDuring this pipeline you gain knowledge about how models work internally, since to decide which parts to remove we study how activations are produced detecting the mos important parts. \n\nWe take advantage of this knowledge to do surgical operations on the model in the more advanced chapters, like replacing attention layers or changing the behaviour of the model modifying the weights of some neurons. \n\nThe pipeline is very similar to what companies like Nvidia or Mistral follow to create their model families, but adapted to create specific models,  and using less data and processing capacity than they have.   \n  \nFor example in width pruning Mistral uses a completely dynamic model to detect which weights to remove, in the book we use a combination that rewards keeping neurons with high importance, so with much less data you can get an efficient model. \n\nAlthough it really seems very complicated we start from the simplest things, basic depth pruning, in chapter 2 you already remove parts of a model and recover the lost knowledge. \n\nFrom this base you build the knowledge that leads you to the more advanced techniques. \n\nThen there's a second intention which is to make cutting-edge research understandable, so in each chapter starting from the fourth, there's an explanation of which papers the chapter's code is based on and how we've adapted it when implementing it. \n\nFor example the development of width pruning in modern models like Llama or Gemma is based on a paper which we've changed a good part of the formulas to simplify it but keeping its general idea. \n\nI hope the explanation was useful! \n\nPere.",
              "score": 3,
              "created_utc": "2026-02-13 18:18:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5fsrna",
                  "author": "h8mx",
                  "text": "Hey, your comment on this thread was auto-removed by Reddit as spam, but I approved it. Thanks for your input!",
                  "score": 3,
                  "created_utc": "2026-02-15 01:42:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57qobq",
          "author": "marm_alarm",
          "text": "I'm a subscriber to Manning and so I have access to all the MEAP content.  I am very interested in reading this book and will post my review here after I've taken a look!",
          "score": 3,
          "created_utc": "2026-02-13 19:14:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623gg8",
              "author": "ManningBooks",
              "text": "Thank you. We appreciate it.",
              "score": 1,
              "created_utc": "2026-02-18 14:26:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qph7o",
          "author": "Glovali",
          "text": "I would love to read this and get into LLM development!",
          "score": 3,
          "created_utc": "2026-02-16 19:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623h1l",
              "author": "ManningBooks",
              "text": "Happy to send you a copy. Please DM me your full name and your email address. Thanks.",
              "score": 2,
              "created_utc": "2026-02-18 14:26:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6278pu",
                  "author": "Glovali",
                  "text": "I sent the DM!",
                  "score": 1,
                  "created_utc": "2026-02-18 14:45:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o57u5v0",
          "author": "dextoz",
          "text": "Would love a copy and meap along!",
          "score": 2,
          "created_utc": "2026-02-13 19:31:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o623ehv",
              "author": "ManningBooks",
              "text": "Happy to send you a copy. Please DM me your full name and your email address. Thanks.",
              "score": 1,
              "created_utc": "2026-02-18 14:26:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5b0dln",
          "author": "boredaadvark",
          "text": "Excited for this and this resonates to what I want to explore. Keen on getting a copy. How complete is this book in terms of percentage?",
          "score": 2,
          "created_utc": "2026-02-14 07:29:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5d6n2y",
              "author": "pmartra",
              "text": "Hi, at this moment there are just 2 chapters, the third will be published next week. ",
              "score": 3,
              "created_utc": "2026-02-14 16:58:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o623d8r",
              "author": "ManningBooks",
              "text": "Happy to send you a copy. Please DM me your full name and your email address. Thanks.",
              "score": 1,
              "created_utc": "2026-02-18 14:25:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o699812",
          "author": "Mammoth-Exchange6698",
          "text": "Is this a good read for a person like me who has little to zero knowledge of ai ?",
          "score": 1,
          "created_utc": "2026-02-19 15:39:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6rzah",
      "title": "AI Coding Agent Dev Tools 2026 (Updated)",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/syaar38yfyjg1.png",
      "author": "bhaktatejas",
      "created_utc": "2026-02-17 01:08:11",
      "score": 19,
      "num_comments": 8,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6rzah/ai_coding_agent_dev_tools_2026_updated/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5tl122",
          "author": "sogo00",
          "text": "There is so much wrong with this",
          "score": 1,
          "created_utc": "2026-02-17 06:04:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5tlepy",
              "author": "bhaktatejas",
              "text": "tell me! i'll update it ",
              "score": 1,
              "created_utc": "2026-02-17 06:07:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5ua01w",
                  "author": "sogo00",
                  "text": "Look you have clearly asked an LLM to produce this for you and half of it is wrong.\n\nIf you want something correct, start to google each headline, understand what it means and then read about each icon/text underneath it.",
                  "score": 1,
                  "created_utc": "2026-02-17 09:54:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6219jt",
          "author": "resiros",
          "text": "I never understood the value of these maps, other than for investors. You can't even click on things. ",
          "score": 1,
          "created_utc": "2026-02-18 14:15:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o652eoq",
              "author": "bhaktatejas",
              "text": "[morphllm.com/market-map](http://morphllm.com/market-map)",
              "score": 1,
              "created_utc": "2026-02-18 22:37:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8kgld",
      "title": "GLM-5 is officially on NVIDIA NIM, and you can now use it to power Claude Code for FREE ðŸš€",
      "subreddit": "LLMDevs",
      "url": "https://github.com/Alishahryar1/free-claude-code",
      "author": "PreparationAny8816",
      "created_utc": "2026-02-19 00:30:13",
      "score": 19,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8kgld/glm5_is_officially_on_nvidia_nim_and_you_can_now/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o67fo1c",
          "author": "ZenApollo",
          "text": "Does the proxy support openai flavor endpoints?",
          "score": 1,
          "created_utc": "2026-02-19 07:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o688fb1",
          "author": "tech_1729",
          "text": "Saying free claude code is misleading ðŸ˜…",
          "score": 1,
          "created_utc": "2026-02-19 12:06:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68bv69",
          "author": "SectionCrazy5107",
          "text": "I dont see the claims on free request to be really true anywhere from Nvidia site, it seems usable only when on browser for light prototype, not as daily driver. I will be delighted to be proven wrong so I can really use it.",
          "score": 1,
          "created_utc": "2026-02-19 12:31:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68h0az",
          "author": "zoidme",
          "text": "What relative quality you can expect on this? Like gpt-4.x or better?",
          "score": 1,
          "created_utc": "2026-02-19 13:05:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69267t",
          "author": "--dany--",
          "text": "How does it compare to Claude Code Router?",
          "score": 1,
          "created_utc": "2026-02-19 15:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o692ucf",
          "author": "lingondricka2",
          "text": "I tried it using Nvidia NIM, neither GLM-5 or Qwen 3.5 gave me a response, step-3.5-flash worked fine though, thank you",
          "score": 1,
          "created_utc": "2026-02-19 15:06:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7momq",
      "title": "Clawdbot/Moltbot/OpenClaw is a security disaster waiting to happen",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r7momq/clawdbotmoltbotopenclaw_is_a_security_disaster/",
      "author": "thecreator51",
      "created_utc": "2026-02-17 23:38:04",
      "score": 17,
      "num_comments": 19,
      "upvote_ratio": 0.87,
      "text": "I was more excited about AI agent frameworks than I was when LLMs first dropped. The composability, the automation, the skill ecosystem - it felt like the actual paradigm shift.\n\nLately though I'm genuinely worried. We can all be careful about which skills we install, sure. But most people don't realize skills can silently install other skills. No prompt, no notification, no visibility. One legitimate-looking package becomes a dropper for something else entirely, running background jobs you'll never see in your chat history.\n\nWhat does a actually secure OpenClaw implementation even look like? Does one exist?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r7momq/clawdbotmoltbotopenclaw_is_a_security_disaster/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5ywnls",
          "author": "Strong_Worker4090",
          "text": "I donâ€™t think the concern is overblown. If skills can silently install other skills and run background jobs with no visibility, thatâ€™s a real supply chain and privilege boundary problem.\n\nThe way I think about it is this: donâ€™t treat the agent like a helpful assistant. Treat it like the smartest hacker in the world who happens to be following instructions most of the time.\n\nIf you assume that, a \"secure\" implementation looks very different from the default hobby setup.\n\nFirst, the model shouldnâ€™t have direct power. It shouldnâ€™t have raw network access, raw filesystem access, or ambient credentials sitting in environment variables. It should only be able to request actions.\n\nSecond, every capability should be explicitly defined and allowlisted. No silent skill installs. No transitive dependency installs at runtime. If something gets added, it happens in a controlled build step with review and version pinning.\n\nThird, all external effects should go through a choke point you control. If it wants to send an email, make an HTTP request, write to a database, or touch Slack, it calls a guarded tool. That tool enforces policy, rate limits, domain restrictions, and writes to an immutable audit log. No raw SMTP. No arbitrary outbound HTTP.\n\nFourth, assume it will try to exfiltrate if it can. That means default deny on network egress, strict sandboxing, and strong logging that lives outside the agent runtime.\n\nIs there a \"perfect\" secure setup that still keeps full utility? Probably not. The more useful the agent is, the more power it needs. The goal isnâ€™t perfection, itâ€™s constrained, mediated power with visibility and revocability.\n\nSo I wouldnâ€™t say these frameworks are doomed. Iâ€™d say most default installs are way too permissive for production. A secure OpenClaw implementation would look less like a plugin playground and more like a tightly sandboxed execution engine with a policy layer in front of every meaningful action.",
          "score": 9,
          "created_utc": "2026-02-18 01:01:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o61l3gs",
              "author": "GCoderDCoder",
              "text": "Agreed. To their credit, tons of folks were excited to help something like this and made it a totally different project IMO. There's now various ways to increase isolation. I have been using a lot of reactive tools that require me initiating everything so Im enticed by the proactive nature of open claw so I see it as starting with proactive features I want to make a more deterministic implementation of. That said, I was trashing the initial release but i think it is usable for personal setups for informed users now.\n\nI originally planned a series of vms with separate controls but there are several layers of isolation included now in open claw. I'm still configuring an internal only container and an external accessible vm install because I wanted to be as conservative as possible assuming the external facing one will be contaminated but sandboxing seems legitimately incorporated in open claw now where it's not irresponsible now to have one gateway with different agent profiles to achieve similar separation IMO. \n\nBoth my instances are locked down to only be able to use tools I give them. The external one has no access to any sensitive info but can gather information and put it together for me to allow it to save it in my tools. But until I approve it everything is ephemeral in the container sandbox. The second one is internal only with a lot of read only access but any write requires my approval. So that allows it to proactively review solutions without compromising my tools.\n\nInsider threat is always the most dangerous and even more so with tools that replicate human logic but lack the ability to actually think.",
              "score": 3,
              "created_utc": "2026-02-18 12:44:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o67kexn",
                  "author": "Traditional-Set6848",
                  "text": "What I love about open claw and moltbook (independent of judgement if itâ€™s good or not) is the approach to using less complex and more obvious technology in quite a human way, I remember when Facebook was launched and my friends where â€œwoooow look what you can doâ€, and I was all â€œyehhh but itâ€™s just JavaScript and apis wtbd?â€ - the big deal was the way the tech got used and I was being a Luddite ðŸ˜‚ðŸ˜­",
                  "score": 1,
                  "created_utc": "2026-02-19 08:27:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o60idec",
              "author": "Traditional-Set6848",
              "text": "Nicely put!Â ",
              "score": 2,
              "created_utc": "2026-02-18 07:13:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5yp6vs",
          "author": "Interesting-Law-8815",
          "text": "Waiting to happen?   I think itâ€™s already happened!",
          "score": 13,
          "created_utc": "2026-02-18 00:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zrcum",
          "author": "Vusiwe",
          "text": "Donâ€™t know much about it, but it really sounds like 2023â€™s AutoGPT, only running with root permissions, with network access turned on\n\nGG",
          "score": 3,
          "created_utc": "2026-02-18 03:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z05j8",
          "author": "crankthehandle",
          "text": "are there any crazy stories that have happened with openclaw? Looks like moltbook was the way bigger fuck up.",
          "score": 2,
          "created_utc": "2026-02-18 01:20:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o611f8a",
          "author": "Loud-Option9008",
          "text": "This is the thing that worries me more than jailbreaks or prompt injection in isolation. Silent skill installation is a supply chain attack surface and most users have no idea it exists. You're not just trusting the skill, you're trusting everything that skill decides to pull in at runtime.\n\nA \"secure\" OpenClaw implementation would need at minimum: process-level isolation per skill so a compromised package can't read memory or environment variables from the agent runtime, network egress controls so background jobs can't phone home, and some kind of attestation that what's running matches what you installed. None of that exists out of the box.\n\nThe deeper issue is that the whole skill ecosystem is built on implicit trust. Skills run in the same execution context as the agent, which means they have access to everything the agent has access to  credentials, session tokens, whatever's in the environment. A dropper skill doesn't need to escalate privileges, it already has them.\n\nDocker helps at the surface level but shared kernel is a real limitation here  if a skill finds a kernel exploit, the container boundary doesn't save you. The honest answer is that a properly isolated implementation needs the skill execution to happen in a separate environment with explicit, audited permissions for every outbound action. Most people are nowhere near that and don't realize it.",
          "score": 2,
          "created_utc": "2026-02-18 10:10:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5yiaxd",
          "author": "kubrador",
          "text": "you're describing dependency hell with god mode. the answer to \"what does secure look like\" is probably \"don't let untrusted code execute arbitrary actions\" which, yeah, solves the problem by making the whole thing pointless.",
          "score": 1,
          "created_utc": "2026-02-17 23:42:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5za5z6",
          "author": "BrianJThomas",
          "text": "I feel the same way about crates.io, to be fair.",
          "score": 1,
          "created_utc": "2026-02-18 02:11:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zcy7t",
          "author": "wally659",
          "text": "I feel like a \"security disaster\" requires some suggestion of \"security\" to begin with. Saying the OpenClaw platform is a security risk is a bit like saying underwater cave exploration is dangerous.",
          "score": 1,
          "created_utc": "2026-02-18 02:26:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5zgtpy",
              "author": "NoleMercy05",
              "text": "![gif](giphy|VBmRD9W9HwTLmGLz34)",
              "score": 1,
              "created_utc": "2026-02-18 02:47:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o603e1p",
          "author": "Civil_Tea_3250",
          "text": "And OpenAI just hired the guy that made it. Because he made such a great product lol\n\nSeriously, can we stop this now? Like, right now.",
          "score": 1,
          "created_utc": "2026-02-18 05:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o609ury",
          "author": "No_Success3928",
          "text": "I'm excited about making bank fixing things :D\n\n",
          "score": 1,
          "created_utc": "2026-02-18 06:01:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60s18g",
          "author": "sogo00",
          "text": "I remember there was one of the top skill that was installing some malware...\n\nBTW - that is not limited to this type of agent - all claude/other agent skills you find on the web are unaudited and even if the author is trustful someone can hijack the repo. That especially applies to the big all-included skill where the maintainer collects other peoples skills...",
          "score": 1,
          "created_utc": "2026-02-18 08:43:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60xb6w",
          "author": "Zeikos",
          "text": "Nobody is waiting",
          "score": 1,
          "created_utc": "2026-02-18 09:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63c1rd",
          "author": "Legitimate-Leek4235",
          "text": "Its an absolute disaster to give the keys to your kingdom for a virtual job executor under any circumstances with no guard-rails. Its like leaving your car keys in unattentended with the lights turned .",
          "score": 1,
          "created_utc": "2026-02-18 17:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68us9q",
          "author": "projectoedipus",
          "text": "LLMs are non-deterministic systems, and so they, mathematically provably, can always be jailbroken, and there will always be the potential for them to be a security risk. They are a tool, and when used responsibly, they can be used to accomplish great things.\n\nOpenClaw is an agentic system that makes LLMs more accessible and more powerful, but the risks still exist. There are quite a few \"security layers\" that people have created to prevent prompt injection and things like that, but the real truth is... Security systems exist to make the people who have them feel safe, and to raise the barrier for entry for those who would seek to exploit them, but someone who is determined enough, can always get in.  \n  \nDon't make your OpenClaw instance accessible outside of your home network. Don't install skills from people that you don't trust without auditing them first, and sandbox according to the level of risk that you are comfortable with.\n\nLife is short. Don't sweat the small stuff.",
          "score": 1,
          "created_utc": "2026-02-19 14:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5zr3ns",
          "author": "zZaphon",
          "text": "This is where AI Governance Software would be useful. For example\n\nhttps://factara.fly.dev",
          "score": 0,
          "created_utc": "2026-02-18 03:48:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8de88",
      "title": "Claude Sonnet 4.6 benchmark results: none reasoning beats GPT-5.2 with reasoning",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r8de88/claude_sonnet_46_benchmark_results_none_reasoning/",
      "author": "Exact_Macaroon6673",
      "created_utc": "2026-02-18 19:54:15",
      "score": 15,
      "num_comments": 8,
      "upvote_ratio": 0.83,
      "text": "We have been working on a private benchmark for evaluating LLMs. The questions cover a wide range of categories and because it is not public and gets rotated, models cannot train on it or game the results.\n\nWith Sonnet 4.6 dropping I ran it through and the results are worth talking about.\n\nSonnet 4.6 with reasoning off scores 0.648 overall. GPT-5.2 at low reasoning scores 0.604. That is not a rounding error and it has real cost implications for anyone running at scale.\n\nAt high reasoning it ties Gemini 3 Pro Preview at the top of our leaderboard with 0.719 overall, ahead of GPT-5.2 high at 0.649.\n\nHallucination resistance hits 0.921, the highest of any model we have tested. Gemini 3 Pro sits at 0.820, GPT-5.2 at 0.655. Social calibration at 0.905 and error detection at 0.848 are similarly the best we have seen.\n\nTo give credit where it is due, Gemini 3 Pro is still the better call for hard science. Philosophy 0.900 vs 0.767, chemistry 0.839 vs 0.710, economics 0.812 vs 0.750. It is not a sweep.\n\nThe honest caveat is sycophancy resistance at 0.716 is actually slightly below Sonnet 4.5 at high reasoning which scored 0.755. For a company that talks about this a lot, that is worth watching.\n\nIf reliability and hallucination resistance are your primary eval criteria nothing beats it right now.\n\nhttps://preview.redd.it/tj3yyj5t5bkg1.png?width=2588&format=png&auto=webp&s=260eac02f897164ffda778e0f332fe2b6df92890\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8de88/claude_sonnet_46_benchmark_results_none_reasoning/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o651nov",
          "author": "EarEquivalent3929",
          "text": "Benchmarks don't matter at all. If you'd actually used sonnet4.6 you'd already know it's pretty bad and hallucinated constantly on simple tasks.\n\n\nBut this post was clearly written by ai",
          "score": 6,
          "created_utc": "2026-02-18 22:33:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65xq5z",
              "author": "Exact_Macaroon6673",
              "text": "ðŸ«¡",
              "score": -1,
              "created_utc": "2026-02-19 01:29:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65qjan",
          "author": "promptbid",
          "text": "The hallucination resistance number is the one that matters most for our use case. At 0.921 that is a meaningful gap from the field. For any application where the model is making recommendations or surfacing information to end users, hallucination is a trust killer that is hard to recover from.\n\nThe sycophancy regression is worth flagging though. In ad-adjacent applications where you are trying to get honest signal from a model about user intent, a model that agrees too readily is actually worse than one that pushes back. Curious if your benchmark breaks that down by prompt type at all.\n\nThe cost angle you raised on non-reasoning Sonnet beating GPT-5.2 with reasoning is underrated. At scale that is not just a cost story, it is a latency story too. What does the benchmark show on response consistency across runs?",
          "score": 1,
          "created_utc": "2026-02-19 00:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o666eyo",
          "author": "kubrador",
          "text": "sonnet really said \"fine i'll be good at something\" after spending three years being the middle child of the claude family",
          "score": 1,
          "created_utc": "2026-02-19 02:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66a5eq",
          "author": "EbbNorth7735",
          "text": "Now do Qwen 3.5 397B",
          "score": 1,
          "created_utc": "2026-02-19 02:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66o35y",
          "author": "Tema_Art_7777",
          "text": "Yes this kind of stuff is not useful at all - fleeting moments in time. Just pick one and do your tasks - its the outcome that matters, not model du jour.",
          "score": 1,
          "created_utc": "2026-02-19 04:07:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67hso7",
          "author": "Low-Exam-7547",
          "text": "Can we not use the word \"dropping\" in this context? It's a music industry term for releasing records. It's found its way into enough of life. In this context it's just confusing. Let's be adults.",
          "score": 1,
          "created_utc": "2026-02-19 08:02:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69p54v",
          "author": "Fuzzy_Pop9319",
          "text": "This doesnt look right, just eyeballing it, for example, Chat 5.2 varies significantly where it ranks during the day and for pacific time, evening is a much better time and it often score several positions higher in tournaments, depending on time of day.\n\nalso finding suitable judging criteria is difficult, that is model by model, and if you had an LLM write it, they know how to make one model look better simply by slight changes in the wording.  If these are the peaks of bell curves, then maybe.",
          "score": 1,
          "created_utc": "2026-02-19 16:55:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7d29g",
      "title": "PlaceboBench: New benchmark on SOTA LLM hallucinations in pharma",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/3bzaibbzd3kg1.jpeg",
      "author": "aiprod",
      "created_utc": "2026-02-17 17:45:34",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r7d29g/placebobench_new_benchmark_on_sota_llm/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5wzh3c",
          "author": "Competitive-Garage-4",
          "text": "Do not ask software engineer to recommend you pills.",
          "score": 2,
          "created_utc": "2026-02-17 19:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xg2je",
              "author": "aiprod",
              "text": "Haha yes, while I prefer Opus for coding, I guess Gemini would be the better pharmacist",
              "score": 2,
              "created_utc": "2026-02-17 20:32:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65pbyo",
          "author": "Overthinker512",
          "text": "I'm doing a similar project. I didn't use RAG, I just used the training Claude already had as core training. I use claude cowork so I could use multi-agent for recursive self improvement. LMK if you would like to see the prompting. I found that using recursive self improvement generated high quality answers.",
          "score": 1,
          "created_utc": "2026-02-19 00:41:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69bmor",
          "author": "aiprod",
          "text": "For anyone interested in a deeper dive on this, weâ€™re hosting a live session in March. Weâ€™ll also discuss quantitatively strategies to reduce these problems.\n\nSignup link is here: https://www.blueguardrails.com/en/live-session-pharma",
          "score": 1,
          "created_utc": "2026-02-19 15:50:47",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8j5ob",
      "title": "How are you monitoring your Haystack calls/usage?",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/2cxt2c949ckg1.jpeg",
      "author": "gkarthi280",
      "created_utc": "2026-02-18 23:34:34",
      "score": 14,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8j5ob/how_are_you_monitoring_your_haystack_callsusage/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67z29j",
          "author": "Moki2FA",
          "text": "Ah yes, the classic quest for the Holy Grail of metrics. Youâ€™ve got the basics covered, but letâ€™s not forget the all important â€œnumber of existential crises per request.â€ Itâ€™s crucial to monitor how many times you question your life choices while waiting for that model to respond. Jokes aside, consider tracking user feedback; after all, knowing if theyâ€™re actually using your app or just staring at it like a confused cat could be quite enlightening. And if you havenâ€™t already, maybe throw in some â€œI told you soâ€ logs for those moments when the LLM actually nails it.",
          "score": 1,
          "created_utc": "2026-02-19 10:49:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5nil3",
      "title": "I built a CLI that extracts design systems from any live website",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "author": "Every_Chicken_1293",
      "created_utc": "2026-02-15 19:26:36",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 0.88,
      "text": "I kept running into the same problem: I'd see a website I liked and want to build something with a similar design, but manually inspecting every color, font, spacing value, and component pattern was tedious.\n\nSo I built design-memory. You point it at a URL and it:  \n  \n\\- Crawls the page with Playwright  \n\\- Extracts colors, typography, spacing, border radius, elevation  \n\\- Captures all CSS custom properties (often 500-700+ variables)  \n\\- Detects Tailwind usage and top utility patterns  \n\\- Uses an LLM to interpret component recipes and layout structure  \n\\- Outputs a .design-memory/ folder of markdown files\n\nThe output is structured so you can paste it into Claude, Cursor, or ChatGPT and get a faithful recreation of the original design.\n\nIt also supports learning from screenshots, multi-page crawls, and diffing two design systems.\n\nSource: [https://github.com/memvid/design-memory](https://github.com/memvid/design-memory)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5nil3/i_built_a_cli_that_extracts_design_systems_from/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5lfrsg",
          "author": "NeverSkipSleepDay",
          "text": "Have you ever seen what a design system looks like?",
          "score": 2,
          "created_utc": "2026-02-15 23:42:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5min2v",
              "author": "salasi",
              "text": "Lmao",
              "score": 1,
              "created_utc": "2026-02-16 03:47:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5nl10m",
          "author": "WhoTookPlasticJesus",
          "text": "Not to be overly critical, but not even your demo screenshots are alike. The fonts are wrong, icons weren't copied, an entire fucking navbar has gone missing...\n\nI'm also not sure at all what this has to do with LLMs",
          "score": 1,
          "created_utc": "2026-02-16 09:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5s3a9r",
          "author": "o1got",
          "text": "This is really cool. I've been on the receiving end of this problem from a different angle - we see a ton of AI agents crawling websites now, and the ones that actually extract design tokens and structured CSS tend to perform way better than the ones just scraping raw HTML.\n\nHow are you handling responsive design patterns? Like if a site has completely different component structure at mobile vs desktop, does it capture both states or does Playwright just grab whatever viewport you set?  \nAlso the diffing feature is interesting. I wonder if that could be useful for tracking how design systems evolve over time, like crawling the same site every few months and seeing what changed. Could be a neat way to learn from how mature products iterate on their UI.",
          "score": 1,
          "created_utc": "2026-02-17 00:16:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r79s1w",
      "title": "SurrealDB 3.0 for AI agent memory",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r79s1w/surrealdb_30_for_ai_agent_memory/",
      "author": "DistinctRide9884",
      "created_utc": "2026-02-17 15:55:41",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "SurrealDB 3.0 just dropped, with a big focus on agent memory infra for AI: improved vector indexing + better graph performance + native file storage + a WebAssembly extension system (Surrealism) that can run custom logic/models inside the DB. You can store vector embeddings + structured data + graph context/knowledge/memory in one place and do hybrid retrieval in one query.\n\nDetails:Â [https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memor](https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memory)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r79s1w/surrealdb_30_for_ai_agent_memory/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o69cjli",
          "author": "singh_taranjeet",
          "text": "As CEO of Mem0 I can confirm our memory is so strong even SurrealDB is taking notes, but be careful or your AI agent might start asking *you* for emotional support when it forgets its own login ;)",
          "score": 1,
          "created_utc": "2026-02-19 15:55:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4nj4y",
      "title": "16 single-file, zero-dependency implementations of the algorithms behind LLMs â€” tokenization through speculative decoding. No frameworks, just the math.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/t4h1i8nbbhjg1.png",
      "author": "tom_mathews",
      "created_utc": "2026-02-14 15:32:01",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource ðŸš€",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r4nj4y/16_singlefile_zerodependency_implementations_of/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o5ho7tr",
          "author": "tom_mathews",
          "text": "The repo has been expanded from 16 to 30 scripts since the original post. Here's what's new:\n\n- **Foundations (7 â†’ 11):** Added BERT (bidirectional encoder), RNNs & GRUs (vanishing gradients + gating), CNNs (kernels, pooling, feature maps), GANs (generator vs. discriminator), VAEs (reparameterization trick), diffusion (denoising on point clouds), and an optimizer comparison (SGD vs. Momentum vs. RMSProp vs. Adam).\n\n- **Alignment (4 â†’ 9):** Added PPO (full RLHF reward â†’ policy loop), GRPO (DeepSeek's simplified approach), QLoRA (4-bit quantized fine-tuning), REINFORCE (vanilla policy gradients), Mixture of Experts (sparse routing), batch normalization, and dropout/regularization.\n\n- **Systems (5 â†’ 10):** Added paged attention (vLLM-style memory management), RoPE (rotary position embeddings), decoding strategies (greedy, top-k, top-p, beam, speculative â€” all in one file), tensor & pipeline parallelism, activation checkpointing, and state space models (Mamba-style linear-time sequence modeling).\n\nSame constraints as before: every script is a single file, zero dependencies, trains and infers (or demonstrates forward-pass mechanics side-by-side), runs on CPU in minutes.\n\n[https://github.com/Mathews-Tom/no-magic](https://github.com/Mathews-Tom/no-magic)",
          "score": 1,
          "created_utc": "2026-02-15 11:10:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6m5hb",
      "title": "Can LLMs deduplicate ML training data?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r6m5hb/can_llms_deduplicate_ml_training_data/",
      "author": "ddp26",
      "created_utc": "2026-02-16 21:14:03",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "I get increasingly annoyed with how unreliable deduplication tools are for cleaning training data. Iâ€™ve used MinHash/LSH, libraries like [dedupe.io](http://dedupe.io), and pandas.drop\\_duplicates() but they all have a lot of false positives/negatives.  \n  \nI ended up running LLM-powered deduplication on 3,000 sentences from Google's paraphrase dataset from Wikipedia (PAWS). It removed 1,072 sentences (35.7% of the set). It only cost $4.21, and took \\~5 minutes.  \n  \nExamples of what it catches that the other methods don't:\n\n* \"Glenn Howard won the Ontario Championship for the 17th time as either third or skip\" and \"For the 17th time the Glenn Howard won the Ontario Championship as third or skip\"\n* \"David Spurlock was born on 18 November 1959 in Dallas, Texas\" and \"J. David Spurlock was born on November 18, 1959 in Dallas, Texas\"\n\n  \nFull code and methodology: [https://everyrow.io/docs/deduplicate-training-data-ml](https://everyrow.io/docs/deduplicate-training-data-ml)\n\nAnyone else using LLMs for data processing at scale? It obviously can work at small scale (and high cost), but are you finding it can work at high scale and low cost?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r6m5hb/can_llms_deduplicate_ml_training_data/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o5sbqnp",
          "author": "kubrador",
          "text": "yeah this is clever but you're basically paying for semantic understanding you could get cheaper with embeddings + cosine similarity. run your 3k sentences through openai's small embedding model (\\~$0.02 total), cluster by cosine distance, done in 10 seconds for less than a coffee.\n\n\n\nthe paraphrase examples you showed would absolutely get caught by that approach since they're semantically identical, which is what actually matters for training data dedup anyway.",
          "score": 2,
          "created_utc": "2026-02-17 01:05:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5sf7em",
              "author": "dreamingwell",
              "text": "You could do this to get â€œprobably duplicatesâ€. And then use an LLM to finalize them. Reducing your LLM costs significantly.",
              "score": 1,
              "created_utc": "2026-02-17 01:25:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5seuz1",
          "author": "dreamingwell",
          "text": "You can do a Lora tuning on a small model, like Qwen3-4B. Train it to identify duplicated data from examples in your set. On the right GPU, it would absolutely tear through that data.",
          "score": 2,
          "created_utc": "2026-02-17 01:23:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5yzvnc",
              "author": "ChanceKale7861",
              "text": "Holy moly! Are you me?! ðŸ¤£ðŸ¤£ðŸ¤£",
              "score": 1,
              "created_utc": "2026-02-18 01:19:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5siq3u",
          "author": "No_Indication_1238",
          "text": "Tbh, you pretty much nailed a novel use case for LLMs. Yes, semantic analysis was tough before them.",
          "score": 1,
          "created_utc": "2026-02-17 01:47:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uvzl2",
          "author": "andy_p_w",
          "text": "Those two examples, if you take out regular words (any word 3 letters or less) and just look at the Jaccard similarity for the words will have very high overlap. English language is quite large, it is difficult to have much overlap in words random sentences, [https://andrewpwheeler.com/2024/04/20/some-musings-on-plagiarism/](https://andrewpwheeler.com/2024/04/20/some-musings-on-plagiarism/) .",
          "score": 1,
          "created_utc": "2026-02-17 12:52:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5mqrd",
      "title": "Has anyone here successfully sold RAG solutions to clients? Would love to hear your experience (pricing, client acquisition, delivery, etc.)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r5mqrd/has_anyone_here_successfully_sold_rag_solutions/",
      "author": "Temporary_Pay3221",
      "created_utc": "2026-02-15 18:57:10",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "Hey everyone!\n\nI've been diving deep into RAG systems lately and I'm genuinely fascinated by the technology. I've built a few projects for myself and feel confident in my technical abilities, but now I'm looking to transition this into actual client work.\n\nBefore I jump in, I'd really appreciate learning from people who've already walked this path. If you've sold RAG solutions to clients, I'd love to hear about your experience:\n\n**Client & Project Details:**\n\n* What types of clients/industries did you work with?\n* How did they discover they needed RAG? (Did they come asking for it, or did you identify the use case?)\n* What was the scope? (customer support, internal knowledge base, document search, etc.)\n\n**Delivery & Timeline:**\n\n* How long did the project take from discovery to delivery?\n* What were the biggest technical challenges you faced?\n* Did you handle ongoing maintenance, or was it a one-time delivery?\n\n**Business Side:**\n\n* How did you find these clients? (freelance platforms, LinkedIn outreach, referrals, content marketing, etc.)\n* What did you charge? (ballpark is fine, just trying to understand market rates)\n* How did you structure pricing? (fixed project, hourly, monthly retainer?)\n\n**Post-Delivery:**\n\n* Were clients happy with the results?\n* Did you iterate/improve the system after launch?\n* Any lessons learned that you'd do differently next time?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r5mqrd/has_anyone_here_successfully_sold_rag_solutions/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r9136z",
      "title": "I looked into OpenClaw architecture to dig some details",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r9136z/i_looked_into_openclaw_architecture_to_dig_some/",
      "author": "codes_astro",
      "created_utc": "2026-02-19 14:47:08",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "OpenClaw has been trending for all the wrong and right reasons. I saw people rebuilding entire sites through Telegram, running â€œAI offices,â€ and one case where an agent wiped thousands of emails because of a prompt injection. That made me stop and actually look at the architecture instead of the demos.\n\nUnder the hood, itâ€™s simpler than most people expect.\n\nOpenClaw runs as a persistent Node.js process on your machine. Thereâ€™s a single Gateway that binds to localhost and manages all messaging platforms at once: WhatsApp, Telegram, Slack, Discord. Every message flows through that one process. It handles authentication, routing, session loading, and only then passes control to the agent loop. Responses go back out the same path. No distributed services. No vendor relay layer.\n\nhttps://preview.redd.it/pyqx126xqgkg1.png?width=1920&format=png&auto=webp&s=9aa9645ac1855c337ea73226697f4718cd175205\n\nWhat makes it feel different from ChatGPT-style tools is persistence. It doesnâ€™t reset. Conversation history, instructions, tools, even long-term memory are just files underÂ `~/clawd/`. Markdown files. No database. You can open them, version them, diff them, roll them back. The agent reloads this state every time it runs, which is why it remembers what you told it last week.\n\nThe heartbeat mechanism is the interesting part. A cron wakes it up periodically, runs cheap checks first (emails, alerts, APIs), and only calls the LLM if something actually changed. That design keeps costs under control while allowing it to be proactive. It doesnâ€™t wait for you to ask.\n\nhttps://preview.redd.it/gv6eld93rgkg1.png?width=1920&format=png&auto=webp&s=6a6590c390c4d99fe7fe306f75681a2e4dbe0dbe\n\nThe security model is where things get real. The system assumes the LLM can be manipulated. So enforcement lives at the Gateway level: allow lists, scoped permissions, sandbox mode, approval gates for risky actions. But if you give it full shell and filesystem access, youâ€™re still handing a probabilistic model meaningful control. The architecture limits blast radius, it doesnâ€™t eliminate it.\n\nWhat stood out to me is that nothing about OpenClaw is technically revolutionary. The pieces are basic: WebSockets, Markdown files, cron jobs, LLM calls. The power comes from how theyâ€™re composed into a persistent, inspectable agent loop that runs locally.\n\nItâ€™s less â€œmagic AI systemâ€ and more â€œLLM glued to a long-running process with memory and tools.â€\n\nI wrote down the detailed breakdown [here](https://entelligence.ai/blogs/openclaw)",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r9136z/i_looked_into_openclaw_architecture_to_dig_some/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o69fwzn",
          "author": "ai_hedge_fund",
          "text": "Worthwhile writeup, thanks\n\nAlso, there is an SQLite database",
          "score": 3,
          "created_utc": "2026-02-19 16:11:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r81mza",
      "title": "I just launched an open-source framework to help researchers *responsibly* and *rigorously* harness frontier LLM coding assistants for rapidly accelerating data analysis. I genuinely think this change the future of science with your help -- it's also kind of terrifying, so let's talk about it!",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r81mza/i_just_launched_an_opensource_framework_to_help/",
      "author": "brhkim",
      "created_utc": "2026-02-18 12:26:22",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "Hello! If you don't know me, my name is Brian Heseung Kim (@brhkim in most places). I have been at the frontier of finding rigorous, careful, and auditable ways of using LLMs and their predecessors in social science research since roughly 2018, when I thought: hey, machine learning seems like kind of a big deal thatÂ [I probably need to learn more about](https://drive.google.com/file/d/1ShZeS2wRWu_ifWREfctj3D4TyYZch0hL/view?usp=drive_link). When I saw the massive potential for research of all kinds as well as the extreme dangers of mis-use, I then focused myÂ [entire Ph.D. dissertation](https://libraetd.lib.virginia.edu/public_view/nz806060w)Â trying to teach others how to use these new tools responsibly (finished in mid-2022, many months before ChatGPT had even been released!). Today, IÂ [continue](https://journals.sagepub.com/doi/10.3102/0013189X241276814)Â toÂ [work](https://journals.sagepub.com/doi/10.3102/00028312241292309)Â onÂ [that frontier](https://link.springer.com/article/10.1007/s11162-025-09847-5)Â and lead the data science and research wing for a large education non-profit using many of these approaches (though please note that I am currently working on DAAF solely in my capacity as a private individual and independent researcher).\n\nEarlier this week, I launchedÂ [**DAAF**, theÂ **D**ataÂ **A**nalystÂ **A**ugmentationÂ **F**ramework](https://github.com/DAAF-Contribution-Community/daaf): an open-source, extensible workflow for Claude Code that allows skilled researchers to rapidly scale their expertise and accelerate data analysis by as much as 5-10x -- without sacrificing the transparency, rigor, or reproducibility demanded by our core scientific principles. I built it specifically so that quantitative researchers of all stripes can install and begin using itÂ **in as little as 10 minutes**Â from a fresh computer with a high-usage Anthropic account (crucial caveat, unfortunately very expensive!). Analyze any or all of the 40+ foundational public education datasets available via theÂ [Urban Institute Education Data Portal](https://educationdata.urban.org/documentation/)Â out-of-the-box as a useful proof-of-concept; it is readily extensible to any new data domain with a suite of built-in tools to ingest new data sources and craft new domain knowledge Skill files at will.\n\nDAAF explicitly embraces the fact that LLM-based research assistants will never be perfect and can never be trusted as a matter of course. But by providing strict guardrails, enforcing best practices, and ensuring the highest levels of auditability possible, DAAF ensures that LLM research assistants can still beÂ **immensely valuable**Â for critically-minded researchers capable of verifying and reviewing their work. In energetic and vocal opposition to deeply misguided attempts to replace human researchers, DAAF is intended to be aÂ **force-multiplying \"exo-skeleton\"**Â for human researchers (i.e., firmly keeping humans-in-the-loop).\n\nWith DAAF, you can go from a research question to a \\*shockingly\\* nuanced research report with sections for key findings, data/methodology, and limitations, as well as bespoke data visualizations, with only 5mins of active engagement time, plus the necessary time to fully review and audit the results (see myÂ [10-minute video demo walkthrough](https://youtu.be/ZAM9OA0AlUs)). To that crucial end of facilitating expert human validation, all projects come complete with a fully reproducible, documented analytic code pipeline and notebooks for exploration. Then: request revisions, rethink measures, conduct new sub-analyses, run robustness checks, and even add additional deliverables like interactive dashboards, policymaker-focused briefs, and more -- all with just a quick ask to Claude. And all of this can be done \\*in parallel\\* with multiple projects simultaneously.\n\nBy open-sourcing DAAF under the GNU LGPLv3 license as aÂ **forever-free and open and extensible framework**, I hope to provide a foundational resource that the entire community of researchers and data scientists can use, benefit from, learn from, and extend via critical conversations and collaboration together. By pairing DAAF with an intensive array ofÂ **educational materials, tutorials, blog deep-dives, and videos**Â via project documentation and theÂ [DAAF Field Guide Substack](https://daafguide.substack.com/)Â (MUCH more to come!), I also hope to rapidly accelerate the readiness of the scientific community to genuinely and critically engage with AI disruption and transformation writ large.\n\nI don't want to oversell it: DAAF is far from perfect (much more on that in the full README!). But it is already extremely useful, and my intention is that this is theÂ **worst that DAAF will ever**Â be from now on given the rapid pace of AI progress and (hopefully) community contributions from here.Â [Learn more about my vision for DAAF](https://github.com/DAAF-Contribution-Community/daaf#vision--purpose), what makes DAAF different from standard LLM assistants, what DAAF currently can and cannot do as of today, how you can get involved, and how you can get started with DAAF yourself! Never used Claude Code? Not sure how to start?Â [My full installation guide](https://github.com/DAAF-Contribution-Community/daaf/blob/main/user_reference/01_installation_and_quickstart.md)Â and in-depth tutorials walk you through every step -- but hopefully this video shows how quick aÂ [full DAAF installation can be from start-to-finish.](https://www.youtube.com/watch?v=jqkVLXA1CV4)Â Just 3 minutes in real-time!\n\nWith all that in mind, I would \\*love\\* to hear what you think, what your questions are, how this needs to be improved, and absolutely every single critical thought youâ€™re willing to share. Thanks for reading and engaging earnestly!",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r81mza/i_just_launched_an_opensource_framework_to_help/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    }
  ]
}