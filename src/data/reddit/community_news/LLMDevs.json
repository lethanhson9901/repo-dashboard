{
  "metadata": {
    "last_updated": "2025-12-31 06:39:02",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 33,
    "total_comments": 97,
    "file_size_bytes": 146501
  },
  "items": [
    {
      "id": "1pvghi8",
      "title": "Train a 4B model to beat Claude Sonnet 4.5 and Gemini Pro 2.5 at tool calling - for free (Colab included)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pvghi8/train_a_4b_model_to_beat_claude_sonnet_45_and/",
      "author": "DecodeBytes",
      "created_utc": "2025-12-25 16:08:58",
      "score": 33,
      "num_comments": 6,
      "upvote_ratio": 0.92,
      "text": "Using Open Source DeepFabric, a tool that lets you:\n\n1. Pick any MCP server or any given set of Tools\n2. A specific root topic (DevOps, Customer Care, Coding Agent)\n3. Auto-generate a tool calling / reasoning topic specific dataset, with real tool traces executed within isolated webassembly components.\n4. Fine-tune an SLM to become an expert at that specific MCP server using Unsloth's awesome training framework\n5. Evaluate against a training-blind subset of the dataset.\n\nWe trained Qwen3-4B to outperform Claude Sonnet 4.5 and Gemini Pro 2.5 against the more challenging to use Blender MCP server.\n\n|Model|Score|\n|:-|:-|\n|DeepFabric Fine Tuned|93.50%|\n|Claude Sonnet 4.5|80.50%|\n|Google Gemini Pro 2.5|47.00%|\n\n**The idea is simple:** frontier models are generalists, but a small model fine-tuned on domain-specific tool calling data can become a specialist that beats them at that specific task.\n\n[DeepFabric Pipeline](https://preview.redd.it/b5092mi7sd9g1.png?width=2816&format=png&auto=webp&s=a501f65f8dda88525d80bfa4dd4a83ab2cb962bd)\n\n  \n\n\n**Try it yourself on Google Colab using a Free T4:** [https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq](https://colab.research.google.com/drive/1EG1V40v5xkJKLf6Ra6W4378vYqlZNVWq)\n\n**GitHub:** [https://github.com/always-further/deepfabric](https://github.com/always-further/deepfabric)\n\nWould love feedback from the community, especially if you decide to generate your own dataset and model.",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pvghi8/train_a_4b_model_to_beat_claude_sonnet_45_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nw0im0n",
          "author": "Physical-Artist-6997",
          "text": "This idea of finetuning a very small LM in tool calling task seems really interesting to me, but I see one problem: the SLM needs to be good enough linguistically to properly articulate the outputs of the MCP server functions, right?",
          "score": 4,
          "created_utc": "2025-12-26 12:19:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1h4y2",
              "author": "Maleficent_Pair4920",
              "text": "Correct! Also the nuance is important that‚Äôs why large models are so good. If you think about MCP‚Äôs are mostly used during coding. So the context is so important",
              "score": 1,
              "created_utc": "2025-12-26 16:05:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvypg15",
          "author": "WhoTookPlasticJesus",
          "text": ">the more challenging to use Blender MCP server.\n\nThat's quite the understatement. \n\nThis is really exciting and germane to a \"specific root topic\" I'm currently researching. If ever gets past the exploratory phase I'll be sure to drop you a note.",
          "score": 3,
          "created_utc": "2025-12-26 02:39:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvzwvpp",
              "author": "DecodeBytes",
              "text": "That would be great, I welcome that. You can always find me in the discord server, just ping 'Luke'",
              "score": 1,
              "created_utc": "2025-12-26 08:41:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nw1eqca",
              "author": "Mikasa0xdev",
              "text": "Yo, 4B models are the new frontier! lol",
              "score": 1,
              "created_utc": "2025-12-26 15:52:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw2zzgj",
          "author": "makinggrace",
          "text": "Interesting! Did you run this with any other models?",
          "score": 1,
          "created_utc": "2025-12-26 20:57:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxeit5",
      "title": "How about this idea: Shipping raw Markdown instead of HTML for LLM-friendly sites?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1pxeit5",
      "author": "Superb_Strawberry828",
      "created_utc": "2025-12-28 01:31:24",
      "score": 17,
      "num_comments": 31,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pxeit5/how_about_this_idea_shipping_raw_markdown_instead/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwatqzt",
          "author": "0utkast_band",
          "text": "https://llmstxt.org/",
          "score": 18,
          "created_utc": "2025-12-28 02:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwb7u1u",
              "author": "DishSignal4871",
              "text": "Thank you. Consuming slop is bad enough. Willingly producing slop so it's palatable for AI is worse.¬†",
              "score": 4,
              "created_utc": "2025-12-28 04:22:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwemb9z",
                  "author": "Mikasa0xdev",
                  "text": "Markdown is the new HTML standard, lol.",
                  "score": 2,
                  "created_utc": "2025-12-28 18:42:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwas9rr",
          "author": "deltadeep",
          "text": "if your page can be represented as simple html pages that are the result of markdown-to-html converters, those pages are not hard at all for llms to parse. in fact, using a client side framework just makes this a whole lot more complex. just render clean, simple, semantic html and you're already done?\n\nthe reason we convert web pages to markdown for llms is because most pages are incredibly complex, bloated, and make no sense in \"view source\" as natural language. a minimal html page that's already just the result of a markdown->html generator, just headings, paragraphs, lists, links, etc, with no other trimmings does not have that problem and is ready to use as-is.",
          "score": 5,
          "created_utc": "2025-12-28 02:45:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhgxdm",
              "author": "Synyster328",
              "text": "Sure, but it wastes tokens with the verbosity",
              "score": 1,
              "created_utc": "2025-12-29 03:40:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwagwrv",
          "author": "LemmyUserOnReddit",
          "text": "Why the heck would you need react for this!?",
          "score": 4,
          "created_utc": "2025-12-28 01:38:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwalhyr",
              "author": "konmik-android",
              "text": "Shock!!! Browsers do not understand markdown!",
              "score": 2,
              "created_utc": "2025-12-28 02:05:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwarqee",
                  "author": "deltadeep",
                  "text": "what does that have to with react? the question is why not do this on the server. you don't need client-side javascript at all for this. on the server, render some markdown to html and include the original markdown in some part of the page for llms to scrape, like a hidden node, a script tag, whatever. then you have a normal web page that any user or client can make sense of, without needing a heavyweight headless browser to figure it out.",
                  "score": 3,
                  "created_utc": "2025-12-28 02:42:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwaru5o",
                  "author": "LemmyUserOnReddit",
                  "text": "Well obviously. But why react?",
                  "score": 1,
                  "created_utc": "2025-12-28 02:43:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwailge",
              "author": "ttkciar",
              "text": "You'll want *something* to render it client-side, and sure you could just write \"raw\" javascript, but there's nothing wrong with using a familiar old framework, especially one as ubiquitous as React.",
              "score": -1,
              "created_utc": "2025-12-28 01:48:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwarhcd",
                  "author": "deltadeep",
                  "text": "the question is really not about react it's: why does this need to be client-side?",
                  "score": 2,
                  "created_utc": "2025-12-28 02:41:08",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwasuq3",
                  "author": "LemmyUserOnReddit",
                  "text": "Having a page hydrate client-side is bad enough for loading UX, but requiring additional *unnecessary* dependencies to load before hydration makes this implementation dead on arrival IMO.",
                  "score": 2,
                  "created_utc": "2025-12-28 02:49:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwam42m",
          "author": "konmik-android",
          "text": "Sounds awesome! Also, some browsers will be able to add support to read this markdown to disable html/css/js and return to vanilla web experience. Actually, just having a reference to markdown file should be enough.",
          "score": 2,
          "created_utc": "2025-12-28 02:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwctd7v",
          "author": "New_Comfortable7240",
          "text": "What about in the api request add a header \"Content-Type\" and get the text version\n\n\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/Content-Type\n\n\nSo would be more like static site generator that besides html produces a markdown representation,¬† basically move your idea to the backend. You can take an active OSS static generator and add as a feature for this, for example¬†https://github.com/mdx-js/mdx",
          "score": 2,
          "created_utc": "2025-12-28 12:47:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwb2x95",
          "author": "Tema_Art_7777",
          "text": "See llms.txt",
          "score": 1,
          "created_utc": "2025-12-28 03:50:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwckvn7",
          "author": "promethe42",
          "text": "Claude Code uses an HTML to Markdown converter when fetching web pages. And implementing such a tool is trivial.¬†",
          "score": 1,
          "created_utc": "2025-12-28 11:34:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvpkui",
      "title": "I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/psxvruvmof9g1",
      "author": "CeFurkan",
      "created_utc": "2025-12-25 23:21:49",
      "score": 14,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pvpkui/i_wish_this_gpu_vram_upgrade_modification_became/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nw1f2ep",
          "author": "Mikasa0xdev",
          "text": "VRAM upgrades are trending now, great idea! lol",
          "score": 1,
          "created_utc": "2025-12-26 15:54:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pvk9fp",
      "title": "Langgraph, pydandicAI, dspy, or other ?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pvk9fp/langgraph_pydandicai_dspy_or_other/",
      "author": "one-wandering-mind",
      "created_utc": "2025-12-25 19:03:24",
      "score": 13,
      "num_comments": 9,
      "upvote_ratio": 0.94,
      "text": "For simple things, I don't use any of them, but I am wondering if some of these are mature enough to adopt. I have played around with a few, but probably not enough to hit the sharp edges that might still exist. \n\nI like the dspy approach of automated prompt optimization and could see using that in addition to other tooling depending on the task. If it wasn't for my dislike of langchain because of how poor their docs have been, bad abstractions, poor defaults and visibility, ect, I would probably go with langgraph.\n\nI assume that pydanticAI being from the pydandic folks are more thoughtful about their design choices, have better docs, will be better engineered, ect.\n\nLooking for something that is helpful for building workflows and have good hooks in for validators. Human in the loop and being able to resume, replay, and have support for escalation in the quality of model used, maybe multiple generations would be nice too. Ideally could be based on a model router and also potentially the results of validators.\n\nIn general, goals are also the same as choosing other non-AI frameworks. Good defaults and good abstractions to make the typical use a bit easier, but still allow for stepping outside of the default approaches when it makes sense to do so and it being clear of what the defaults are and how to configure and build outside of that.  ",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pvk9fp/langgraph_pydandicai_dspy_or_other/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvzizgb",
          "author": "Unique-Big-5691",
          "text": "yeah, your instincts here are pretty on point.\n\nmost of these frameworks look great until you actually try to live with them.\n\n* langgraph: the workflow idea is solid, but it carries a lot of langchain DNA. once things get weird, you‚Äôre debugging abstractions instead of your logic. fine if you fully buy in, annoying if you don‚Äôt.\n* dspy: super interesting, but i see it more as a tool you *add*, not something you build everything around. great for optimization loops, not really a full workflow backbone.\n* pydanticAI: feels more intentional. smaller, clearer, and validators aren‚Äôt an afterthought. it‚Äôs not as ‚Äúbatteries included,‚Äù but the mental model is clean: inputs, outputs, validation, retry/escalation. that makes human-in-the-loop, replay, and ‚Äútry a better model on failure‚Äù way easier to reason about.\n\nif you care about good defaults *and* being able to step outside them, pydanticAI + a bit of your own glue code is honestly a nice middle ground right now.\n\ntldr: dspy is a great add-on, langgraph is powerful but heavy, pydanticAI feels younger but better thought out if you value clarity over magic.",
          "score": 8,
          "created_utc": "2025-12-26 06:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw1ewai",
              "author": "Mikasa0xdev",
              "text": "Yo, pydanticAI is the clear winner. lol",
              "score": 1,
              "created_utc": "2025-12-26 15:53:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvx33mr",
          "author": "cmndr_spanky",
          "text": "I haven‚Äôt thoroughly tested them all, but I find pydantic ai to be a lot more clean and intuitive to use than langchain. I haven‚Äôt played as much with langraph though.",
          "score": 3,
          "created_utc": "2025-12-25 20:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw63zng",
              "author": "drugailie",
              "text": "Pydantic AI does have a nice vibe with its structure and clarity. If you're looking for good documentation and a clean API, it might be worth diving deeper into it. Langraph's kind of in the same boat, but yeah, their docs have been hit or miss.",
              "score": 1,
              "created_utc": "2025-12-27 10:05:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw09mjn",
          "author": "mamaBiskothu",
          "text": "Someone i trust vouched for strands. If you're ambitious vbuild the framework you want yourself",
          "score": 1,
          "created_utc": "2025-12-26 10:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw2gfgt",
          "author": "phicreative1997",
          "text": "I personally prefer dspy",
          "score": 1,
          "created_utc": "2025-12-26 19:10:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw62g88",
          "author": "Far_Statistician1479",
          "text": "Langgraph is great is you‚Äôre trying to build a workflow, which 90%+ of agents should be",
          "score": 1,
          "created_utc": "2025-12-27 09:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi8g26",
          "author": "PurpleWho",
          "text": "I use Mind Rig ([a VS Code extension I built](https://mindrig.ai/)) to run prompts in my editor against a CSV of test scenarios. This gives me the \"validator\" part you mentioned‚ÄîI can see exactly how prompts perform across edge cases before deploying. It's super lightweight, not as comprehensive as formal evaluation tools, but you can always export the CSV of your test scenarios to those tools when things get complex enough to justify the overhead.\n\nFor the workflow stuff (retries, escalation, HITL), I just write it as regular TypeScript code. No framework. When you separate evals from orchestration, you don't need LangGraph's complexity.",
          "score": 1,
          "created_utc": "2025-12-29 06:56:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzgm4v",
          "author": "omeraplak",
          "text": "If you‚Äôre looking for a TypeScript-first, code-first way to build agents, VoltAgent might be a good fit.(I'm maintainer)\n\nIt‚Äôs designed for building and debugging agent workflows with full LLM observability (traces, evals), and for automating agents via triggers and actions instead of hiding logic behind black boxes. You keep full code control while still being able to ship end-to-end, multi-agent systems.\n\nThere‚Äôs a concrete Slack agent example here: [https://voltagent.dev/recipes-and-guides/slack-agent/](https://voltagent.dev/recipes-and-guides/slack-agent/)\n\nRepo: [https://github.com/VoltAgent/voltagent](https://github.com/VoltAgent/voltagent)",
          "score": 0,
          "created_utc": "2025-12-26 06:04:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzxwb",
      "title": "If you had to choose ONE LLM API today (price/quality), what would it be?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pxzxwb/if_you_had_to_choose_one_llm_api_today/",
      "author": "SmaugJesus",
      "created_utc": "2025-12-28 19:23:51",
      "score": 10,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI‚Äôm currently building a small SaaS and I‚Äôm at the point where I need to choose an LLM API.\n\nThe use case is fairly standard:\n\n\t‚Ä¢\ttext understanding\n\n\t‚Ä¢\tclassification / light reasoning\n\n\t‚Ä¢\tgenerating structured outputs (not huge creative essays)\n\nI don‚Äôt need the absolute smartest model, but I do care a lot about:\n\n\t‚Ä¢\tprice / quality ratio\n\n\t‚Ä¢\tpredictability\n\n\t‚Ä¢\tgood performance in production (not just benchmarks)\n\nThere are so many options now (OpenAI, Anthropic, Mistral, etc.) and most comparisons online are either outdated or very benchmark-focused.\n\nSo I‚Äôm curious about real-world feedback:\n\n\t‚Ä¢\tWhich LLM API are you using in production?\n\n\t‚Ä¢\tWhy did you choose it over the others?\n\n\t‚Ä¢\tAny regrets or hidden costs I should know about?\n\nWould love to hear from people who‚Äôve actually shipped something.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pxzxwb/if_you_had_to_choose_one_llm_api_today/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwewvtw",
          "author": "tech2biz",
          "text": "IMO, trying to choose one model over all others is the wrong approach because you will always tend to go with a big one that can fulfill ALL requirements while only a small portion of your queries or tool calls really needs a big model and could easily be solved by a small or open source model. so ultimately just choosing one big model will always have a horrible price/quality ratio.",
          "score": 5,
          "created_utc": "2025-12-28 19:32:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjua2t",
              "author": "Mikasa0xdev",
              "text": "Model stacking is the real MVP.",
              "score": 1,
              "created_utc": "2025-12-29 14:39:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwf9dfo",
          "author": "aiprod",
          "text": "Tricky because model performance is constantly shifting. I‚Äòm a big fan of google‚Äòs offering lately. Flash and flash lite are both great for lower complexity workloads. They‚Äòre fairly cheap and fast. Google also has pretty good rate limits.",
          "score": 2,
          "created_utc": "2025-12-28 20:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwfyg9d",
          "author": "Comfortable-Sound944",
          "text": "To should probably create your own small evaluation set for your specific task to run against a potential model and try a few.\n\nAnother consideration you didn't mention I find important is speed. If it's online while a user waits or batch and how many calls do you need... \n\nClassification could be super simple for LLMs if you say look at this and choose one of 4 groups... This task can probably be done by the cheapest model from the last 2 years or so. Look at -nano's -lite ects",
          "score": 2,
          "created_utc": "2025-12-28 22:37:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgdup5",
          "author": "Lonely-Dragonfly-413",
          "text": "go with openai. Do not use google apis. their llm apis automatically retire each year. you have to update prompts when you switch to a new model. it is a nightmare.",
          "score": 2,
          "created_utc": "2025-12-28 23:59:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwkuchy",
              "author": "SmaugJesus",
              "text": "Ah damn, I was hesitating for taking Gemini.\nThank you for letting me know this downside",
              "score": 1,
              "created_utc": "2025-12-29 17:35:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwggfqp",
          "author": "FormalAd7367",
          "text": "i‚Äôm doing a lot of office works so i choose the cheap one like chinese model (qwen).",
          "score": 2,
          "created_utc": "2025-12-29 00:12:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwgry6j",
          "author": "Coollime17",
          "text": "I‚Äôve been using OpenAI APIs since ChatGPT launched and never really felt a reason to switch. \n\nRight now GPT 5.2, Reasoning=None is my go to starting point for most tasks. If it‚Äôs more complicated you can split it into multiple tasks or add reasoning. If it‚Äôs a well defined simple task or very token intensive you can use a mini model. Structured Outputs work really well and you can define a lot different constraints.\n\nCost is fairly well defined with being able to limit input/output tokens plus the amount of reasoning it can do. Usually they offer the best model at a discount to other ones to encourage people to switch so in general I‚Äôd suggest usually updating to the latest model if you care about cost/performance.",
          "score": 2,
          "created_utc": "2025-12-29 01:15:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwh1b5a",
          "author": "maher_bk",
          "text": "Gemini-2.5-flash-lite has been doing wonders for my scraping-with-ai at scale app.\nReally really underrated and price/rates/etc.. are so good.",
          "score": 2,
          "created_utc": "2025-12-29 02:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwktx4r",
              "author": "SmaugJesus",
              "text": "Yeah this one was on my list",
              "score": 1,
              "created_utc": "2025-12-29 17:33:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh2owx",
          "author": "khontolhu",
          "text": "Price / quality? Deepseek v3.2 (if speed is not a priority).\n\nGet 2 out of 3 price, quality, speed.",
          "score": 2,
          "created_utc": "2025-12-29 02:17:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhdy38",
          "author": "konmik-android",
          "text": "You need to run your real requests with all LLMs you are considering, and estimate the amount of tokens consumed and response speed. They might surprise you. Some LLMs are too slow, others consume more tokens than expected, some are good but on your specific task they might just drop the ball.",
          "score": 2,
          "created_utc": "2025-12-29 03:22:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhv02u",
          "author": "Unique-Big-5691",
          "text": "for that kind of use case, i‚Äôd optimize for boring and predictable, not ‚Äúbest model on twitter this week‚Äù.\n\nif i had to pick one right now, it‚Äôd probably be gpt-4o-mini (or that tier). it‚Äôs not the smartest model out there, but it‚Äôs been the least annoying in prod for me. it sticks to instructions, keeps its output shape, and i don‚Äôt spend time wondering why it suddenly went off format.\n\nclaude haiku is also solid, but i‚Äôve found you need to be more explicit to keep the structure consistent. totally usable, just a bit more prompt discipline.\n\nthe bigger thing though isn‚Äôt really the model, it‚Äôs how you deal with the output. once things are live, the pain comes from small format drift and edge cases. having something that enforces structure at the boundary (schemas, validation, etc. pydantic fits nicely here) makes life a lot calmer and lets you swap models later without rewriting everything.\n\nmy rule of thumb: pick the model that behaves most consistently with your prompts, then lock it down. boring and stable wins in prod.",
          "score": 2,
          "created_utc": "2025-12-29 05:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwi50eo",
          "author": "caffeine947",
          "text": "Cerebras with the latest glm series model.  Decent workhorse model with over 2000 tokens per second output and much less than 1s to first token.  Absolutely insane speeds, and not expensive either.",
          "score": 2,
          "created_utc": "2025-12-29 06:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwktq3y",
              "author": "SmaugJesus",
              "text": "Never heard, I will check this out. Thank you very much for your comment",
              "score": 1,
              "created_utc": "2025-12-29 17:32:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkj65n",
          "author": "Middle_Macaron1033",
          "text": "I‚Äôd do with Back Board IO, every time. It‚Äôs an LLM aggregator with a strong memory layer. It‚Äôd be dumb to choose only one LLM",
          "score": 2,
          "created_utc": "2025-12-29 16:42:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwktlag",
              "author": "SmaugJesus",
              "text": "Thank you for the tip, I‚Äôll check this out",
              "score": 1,
              "created_utc": "2025-12-29 17:31:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwf1n21",
          "author": "Purple-Programmer-7",
          "text": "Any of the big providers fit your description, but those are not the criteria I would be using.\n\n- Rate limits\n- Context windows\n- Scalability\n- Privacy\n- Security\n\nAnd these are highly dependent on use case.\n\nEdit: - Location",
          "score": 1,
          "created_utc": "2025-12-28 19:55:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf4s3k",
          "author": "PromptOutlaw",
          "text": "GPT 4.1/5.2. I‚Äôm busy releasing a paper on personality analysis and damn it‚Äôs so hard to tame LLMs with output and consistency. Whatever u do don‚Äôt consider Cohere, Deepaeek or Kimi. Nightmare.\n\nOpus is pretty tame with some adapters. Sonnet is unreliable with reasoning consistency",
          "score": 1,
          "created_utc": "2025-12-28 20:10:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwg4f9s",
              "author": "cathaysia",
              "text": "Can you elaborate more on why not to consider Cohere? I‚Äôm working on a project and got some credits from them, wanted to keep with something well maintained since I will be handing it over to a team with limited tech skills (no engineering team).",
              "score": 1,
              "created_utc": "2025-12-28 23:08:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgdzpo",
                  "author": "PromptOutlaw",
                  "text": "Quick clarification , I deployed Cohere-command-a on Azure AI and this can be different.\n\nMy current study has a JSON compliance acceptance criteria of 98%. Cohere and Kimi K2 had roughly 70% compliance. I spent days creating adapters and I was strict on not inferring json output. I‚Äôm ok stripping junk around it but the LLM had to provide a schema validated json in order for me to validate reasoning based on don numbers.\n\nHere is my prompt: https://github.com/Wahjid-Nasser/12-Angry-Tokens/blob/main/prompts/judge_prompt.md\n\nI don‚Äôt wanna shade Cohere, I just could not get the compliance right and direct api works could be diff. I‚Äôm just short on time and decided to exclude it",
                  "score": 1,
                  "created_utc": "2025-12-28 23:59:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf87dp",
              "author": "TastyWriting8360",
              "text": "Again hitonet.",
              "score": -2,
              "created_utc": "2025-12-28 20:27:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwf83d7",
          "author": "TastyWriting8360",
          "text": "Cheap, fast and smart? Hitonet.com try the free tier.",
          "score": -2,
          "created_utc": "2025-12-28 20:26:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw9pk0",
      "title": "[New Model] Genesis-152M-Instruct ‚Äî exploring hybrid attention + TTT at small scale",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pw9pk0/new_model_genesis152minstruct_exploring_hybrid/",
      "author": "Kassanar",
      "created_utc": "2025-12-26 17:25:54",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone üëã\n\nI‚Äôm sharing **Genesis-152M-Instruct**, an **experimental small language model** built to explore how *recent architectural ideas interact* when combined in a single model ‚Äî especially under **tight data constraints**.\n\n\n\nThis is **research-oriented**, not a production model or SOTA claim.\n\n\n\n\n\nüîç **Why this might be interesting**\n\n\n\nMost recent architectures (GLA, FoX, TTT, ¬µP, sparsity) are tested **in isolation** and usually at **large scale**.\n\nI wanted to answer a simpler question:\n\n\n\n*How much can architecture compensate for data at \\~150M parameters?*\n\n\n\nGenesis combines several **ICLR 2024‚Äì2025 ideas** into one model and evaluates the result.\n\n\n\n\n\n‚ö° **TL;DR**\n\n‚Ä¢ **152M parameters**\n\n‚Ä¢ Trained on **\\~2B tokens** (vs \\~2T for SmolLM2)\n\n‚Ä¢ Hybrid **GLA + FoX attention**\n\n‚Ä¢ **Test-Time Training (TTT)** during inference\n\n‚Ä¢ **Selective Activation (sparse FFN)**\n\n‚Ä¢ **¬µP-scaled training**\n\n‚Ä¢ Fully open-source (Apache 2.0)\n\n\n\nü§ó Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\nüì¶ pip install genesis-llm\n\n\n\n\n\nüìä **Benchmarks (LightEval, Apple MPS)**\n\n\n\nARC-Easy ¬† ¬† ‚Üí 44.0% ¬† (random: 25%)\n\nBoolQ¬† ¬† ¬† ¬† ‚Üí 56.3% ¬† (random: 50%)\n\nHellaSwag¬† ¬† ‚Üí 30.2% ¬† (random: 25%)\n\nSciQ ¬† ¬† ¬† ¬† ‚Üí 46.8% ¬† (random: 25%)\n\nWinogrande ¬† ‚Üí 49.1% ¬† (random: 50%)\n\n\n\n**Important context:**\n\nSmolLM2-135M was trained on **\\~2 trillion tokens**.\n\nGenesis uses **\\~2 billion tokens** ‚Äî so this is not a fair head-to-head, but an exploration of **architecture vs data scaling**.\n\n\n\n\n\nüß† **Architecture Overview**\n\n\n\n**Hybrid Attention (Qwen3-Next inspired)**\n\n\n\n**Layer** **%** **Complexity** **Role**\n\nGated DeltaNet (GLA) 75% O(n) Long-range efficiency\n\nFoX (Forgetting Attention) 25% O(n¬≤) Precise retrieval\n\n\n\nGLA uses:\n\n‚Ä¢ Delta rule memory updates\n\n‚Ä¢ Mamba-style gating\n\n‚Ä¢ L2-normalized Q/K\n\n‚Ä¢ Short convolutions\n\n\n\nFoX adds:\n\n‚Ä¢ Softmax attention\n\n‚Ä¢ Data-dependent forget gate\n\n‚Ä¢ Output gating\n\n\n\n\n\n**Test-Time Training (TTT)**\n\n\n\nInstead of frozen inference, Genesis can **adapt online**:\n\n‚Ä¢ Dual-form TTT (parallel gradients)\n\n‚Ä¢ Low-rank updates (rank=4)\n\n‚Ä¢ Learnable inner learning rate\n\n\n\nPaper: *Learning to (Learn at Test Time)* (MIT, ICML 2024)\n\n\n\n\n\n**Selective Activation (Sparse FFN)**\n\n\n\nSwiGLU FFNs with **top-k activation masking** (85% kept).\n\nCurrently acts as **regularization** ‚Äî real speedups need sparse kernels.\n\n\n\n\n\n**¬µP Scaling + Zero-Centered RMSNorm**\n\n‚Ä¢ Hyperparameters tuned on small proxy\n\n‚Ä¢ Transferred via ¬µP rules\n\n‚Ä¢ Zero-centered RMSNorm for stable scaling\n\n\n\n\n\n‚ö†Ô∏è **Limitations (honest)**\n\n‚Ä¢ Small training corpus (2B tokens)\n\n‚Ä¢ TTT adds \\~5‚Äì10% inference overhead\n\n‚Ä¢ No RLHF\n\n‚Ä¢ Experimental, not production-ready\n\n\n\n\n\nüìé **Links**\n\n‚Ä¢ ü§ó Model: [https://huggingface.co/guiferrarib/genesis-152m-instruct](https://huggingface.co/guiferrarib/genesis-152m-instruct)\n\n‚Ä¢ üì¶ PyPI: [https://pypi.org/project/genesis-llm/](https://pypi.org/project/genesis-llm/)\n\n\n\n\n\nI‚Äôd really appreciate feedback ‚Äî especially from folks working on **linear attention**, **hybrid architectures**, or **test-time adaptation**.\n\n\n\n*Built by Orch-Mind Team*",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pw9pk0/new_model_genesis152minstruct_exploring_hybrid/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pwaa1t",
      "title": "Transitioning from Full Stack Developer to AI / LLM Engineer ‚Äì Career Advice",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pwaa1t/transitioning_from_full_stack_developer_to_ai_llm/",
      "author": "Mazayaz",
      "created_utc": "2025-12-26 17:49:12",
      "score": 9,
      "num_comments": 16,
      "upvote_ratio": 0.85,
      "text": "Hi everyone,\n\nI‚Äôm a Full Stack Developer who recently started working with LLMs and really enjoyed it. I‚Äôd like to specialize further in this area, but I‚Äôm unsure what the next career steps should be.\n\nSo far, in projects and POCs, I have worked on:\n\n* Prompt optimization and prompt engineering\n* Using vector databases and embeddings\n* Implementing Retrieval-Augmented Generation (RAG) pipelines, including GraphRAG and LightRAG\n* Working with frameworks such as LangChain, LlamaIndex, CrewAI, etc.\n* Fine-tuning LLMs\n\nI‚Äôm looking for guidance from people who work as **AI Engineers, LLM Engineers, or AI Specialists**.\n\nWhat would you consider the **next steps** to grow in this career path?\n\nImportant context:  \nI‚Äôm **not interested in developing AI models from scratch or diving into traditional Machine Learning**. My goal is to keep building **around existing AI/LLMs**, focusing on engineering, system design, and using frameworks and industry patterns to overcome model limitations and build production-ready solutions.\n\nAny advice, learning paths, or real-world insights would be greatly appreciated.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pwaa1t/transitioning_from_full_stack_developer_to_ai_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nw2ma0s",
          "author": "coffee-praxis",
          "text": "I‚Äôve just made this transition, and let me tell you it‚Äôs a tough field. The pace and churn is higher than anything I‚Äôve worked on before because every engineer now thinks they are an AI expert and everyone is rushing to WOW everyone else with AI vaporware. First 6 months everything I worked on got cancelled or transferred to another team. I finally landed a hit product feature and saved my job essentially, but the whole job has been total chaos. \n \nStart thinking about how you can use all of those concepts you‚Äôve just outlined to unlock new capabilities for your company. This is way more important than any other learning path.",
          "score": 7,
          "created_utc": "2025-12-26 19:42:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw5wjqr",
              "author": "throwaway18249",
              "text": "Thanks for the honest input about this industry, makes me not want to go into it.",
              "score": 1,
              "created_utc": "2025-12-27 08:52:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nw8447f",
                  "author": "coffee-praxis",
                  "text": "I wouldn‚Äôt say don‚Äôt do it. Just that I was very comfortable with full stack dev. 2 week sprints, coherent product management, clear direction. Well defined goals and targets. \n\nand now I‚Äôm in a never ending hackathon where there‚Äôs literally none of that. It‚Äôs fun. But stressful and very very different.",
                  "score": 1,
                  "created_utc": "2025-12-27 17:57:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw26r1q",
          "author": "nore_se_kra",
          "text": "Whats the difference in the future? If LLMs are commodity then every sw engineer has to use them. And your list sounds like what everyone does who plays around with LLMs. I dont think that's where the value is unless you really want to work in research or an AI company.",
          "score": 1,
          "created_utc": "2025-12-26 18:20:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw27rmw",
              "author": "Mazayaz",
              "text": "I agree that's going to be something every sw engineer is going to use, like async, parallel and concurrent programming is now.  \nMy goal is to be ahead of the curve and become a reference in engineering systems around LLMs, not just calling an API.",
              "score": 1,
              "created_utc": "2025-12-26 18:25:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw28wjn",
                  "author": "nore_se_kra",
                  "text": "At least in my company these people are pretty annoying - every team wants to have their own rag chatbot and whatnot to show off. Why? Just use copilot or some azure integration.. You can stay ahead of the curve here too but its often not sexy.\n\nValue comes from bridging your domain specific knowledge to LLM usecases and especially bring them scalable into production. Most people dont even think about evals- thats something i didnt see in your list either. Probably a good starting point?",
                  "score": 1,
                  "created_utc": "2025-12-26 18:31:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nw65d2q",
          "author": "danigoncalves",
          "text": "My question is what is a AI Engineer? I mean operacionalization of self hosted models (alongside with monitoring and analytics) has its tricks but its one more software layer we have to provide like we did with all inovative parts black then in time. Prompt management its a topic for sure but also something we can tackle with time. The usage of agents framework is yet another kind of framework to learn. The only thing really new here is fine tuning models but even this with the latest RAG techniques and the growing context of the models (alongside with the different architecture that are trying to increase hugely this) might not make sense to bet in most of the use cases. I Will foresee most of software developers being \"AI engineers\" on the coming years, so it will not be a career choice but rather a evolution of the common software Engineer.",
          "score": 1,
          "created_utc": "2025-12-27 10:19:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6y67j",
              "author": "Mazayaz",
              "text": "I Agree 100% with you in the future years every developer is going to be working with this, like DevOps things are mandatory now.\nThat‚Äôs why I want to master myself being an developer focused on implementing things around AI, when the time comes I will be ahead and in a specialist position",
              "score": 1,
              "created_utc": "2025-12-27 14:13:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw6zvlp",
                  "author": "danigoncalves",
                  "text": "Experienced people will for sure be valued like they are right now. Trying to keep up the pace of the field is already a very good step.",
                  "score": 1,
                  "created_utc": "2025-12-27 14:24:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwdmqzq",
              "author": "throwaway18249",
              "text": "These two popular youtubers seem to agree on what an AIE is and what a MLE is. I think their definition is spot-on with what ive seen in job postings.\n\n  \n[https://youtu.be/NmBW49OBeBU?si=wiQjz-Cko3aqUhGo](https://youtu.be/NmBW49OBeBU?si=wiQjz-Cko3aqUhGo)\n\n[https://youtu.be/cqDQV5g7zHo?si=hcSq88-3O4DweBMb](https://youtu.be/cqDQV5g7zHo?si=hcSq88-3O4DweBMb)",
              "score": 1,
              "created_utc": "2025-12-28 15:48:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6fhrq",
          "author": "throwaway18249",
          "text": "Remind me! 1 day",
          "score": 1,
          "created_utc": "2025-12-27 11:56:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw6fjw3",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2025-12-28 11:56:19 UTC**](http://www.wolframalpha.com/input/?i=2025-12-28%2011:56:19%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LLMDevs/comments/1pwaa1t/transitioning_from_full_stack_developer_to_ai_llm/nw6fhrq/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLLMDevs%2Fcomments%2F1pwaa1t%2Ftransitioning_from_full_stack_developer_to_ai_llm%2Fnw6fhrq%2F%5D%0A%0ARemindMe%21%202025-12-28%2011%3A56%3A19%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201pwaa1t)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2025-12-27 11:56:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwcot05",
          "author": "Level_Ad4068",
          "text": "\nReality check: Can RAG specialization get me remote US contracts? [Non-US, career transition]\n\n\n\n\"I need honest assessment before investing 6 months.\n\nBackground:\n- 5 years legal (arbitration/conciliation)\n- 2.5 years ICT Business Analyst\n- Master's in ICT (Australia)\n- Based in India, targeting remote US contracts\n\nPlan\n- 6-month intensive RAG training\n- Focus: legal tech applications (contracts, compliance, discovery)\n- Goal: $300/day remote contracts with US companies\n- Timeline: Start earning within 9-12 months\n\nQuestions:\n1. Do US companies actually hire non-US remote contractors for RAG work?\n2. Is legal domain knowledge + BA background an advantage or irrelevant?\n3. What's realistic timeline from zero ML to paid RAG work?\n4. Am I better off targeting Australian market instead?\n\nNot looking for encouragement - need cold market reality. Is this viable or should I pivot?\"",
          "score": 1,
          "created_utc": "2025-12-28 12:09:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw5gfbi",
          "author": "j4ys0nj",
          "text": "I'd say keep it up. My opinion is that unless AI makes all engineering work obsolete within the few years, the more experience you can get the better. If it was easy, everyone would do it.\n\nOn a similar note, I'm also looking for help with my platform: [mission squad](https://missionsquad.ai). if you're interested, hit me up.",
          "score": 1,
          "created_utc": "2025-12-27 06:21:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pudy3c",
      "title": "Teaching LLMs to Remember: A Deep Dive into Ontology Memorization in Healthcare",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/uowm9szzs29g1.jpeg",
      "author": "lavishlyinspired",
      "created_utc": "2025-12-24 04:02:30",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.82,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pudy3c/teaching_llms_to_remember_a_deep_dive_into/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvpzr2f",
          "author": "SeaworthinessThis598",
          "text": "memory in my case is mostly information desiccation context management and abstraction through tool use .",
          "score": 1,
          "created_utc": "2025-12-24 14:30:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvq0exe",
              "author": "SeaworthinessThis598",
              "text": "i would not trust it in production of course , even in legal purposes . i have created a more robust system . that remembers 100% through additive processes , as inference is dilutive and a lossy process that results in information loss and hallucinations, and i think the solution is in maintaining a trace to root text .",
              "score": 1,
              "created_utc": "2025-12-24 14:34:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxxkdu",
      "title": "I learned basic llm libraried, some rag, and fine-tuning techniques, whats next?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pxxkdu/i_learned_basic_llm_libraried_some_rag_and/",
      "author": "Beyond_Birthday_13",
      "created_utc": "2025-12-28 17:51:42",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Some libs like openai api, and i use it for other urls too, some rag techniques with chroma faiss and qdrant, snd alittle finetuning.\n\nWhats next, should i learn agentic ai?, n8n? Should i go no /low code, or. Code heavy? Or is there another path i am not aware of?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pxxkdu/i_learned_basic_llm_libraried_some_rag_and/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwjundd",
          "author": "Mikasa0xdev",
          "text": "RAG is cool, but agents are the future.",
          "score": 2,
          "created_utc": "2025-12-29 14:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwhp965",
          "author": "Much-Researcher6135",
          "text": "Make something useful that people will buy then sell it",
          "score": 1,
          "created_utc": "2025-12-29 04:31:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pwl1n2",
      "title": "Holiday week AI dev news - NVIDIA/Groq deal, Copilot updates, GLM-4.7 release, and more",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pwl1n2/holiday_week_ai_dev_news_nvidiagroq_deal_copilot/",
      "author": "nerdswithattitude",
      "created_utc": "2025-12-27 01:31:52",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Put together a recap of what happened Dec 22-26. Holiday week but the news kept coming.\n\n**Highlights:**\n\n* NVIDIA announced a $20B licensing deal with Groq on Christmas Eve. Groq's LPU chips claim 10x better power efficiency for inference. Deal structure seems designed to avoid antitrust scrutiny.\n* GitHub shipped Copilot memory (learns your codebase over time), Agent Skills (custom workflows), and added Claude Opus 4.5 + GPT-5.2 to all paid tiers.\n* Zhipu AI released GLM-4.7 open-weight. Matches Claude Sonnet 4.5 on coding benchmarks, $3/month or free if you run it locally. Works with Claude Code, Cline, etc.\n* OpenAI posted about hardening ChatGPT Atlas against prompt injection. Used RL to find exploits‚Äîone tricked the browser agent into sending a resignation letter. Their take: \"may never be fully solved.\"\n* Vercel shipped AI SDK 6 with agent support and human-in-the-loop approvals.\n* Cursor is building their own model + acquired Graphite for code review.",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pwl1n2/holiday_week_ai_dev_news_nvidiagroq_deal_copilot/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pvi0em",
      "title": "Open-source project: deny-by-default runtime controls for MCP tool servers (MCPTrust)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pvi0em/opensource_project_denybydefault_runtime_controls/",
      "author": "bbbbbbb162",
      "created_utc": "2025-12-25 17:20:57",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "Hey Everyone, Merry Christmas!\n\nIf you‚Äôre building agentic workflows on top of MCP servers (Claude Code/Desktop, custom hosts, LangChain-style stacks), you‚Äôve probably hit the core problem: once you ‚Äúapprove‚Äù a tool server, it can drift ‚Äî new tools appear, schemas change, artifacts update, resources expand.\n\n**MCPTrust** is an open-source **runtime enforcement proxy** for MCP that makes this **fail-closed**: if it‚Äôs not in your reviewed lockfile, it‚Äôs blocked.\n\n# 2-minute usage\n\n    # Snapshot server capabilities into a v3 lockfile\n    mcptrust lock --v3 -- npx /server-filesystem /tmp\n    \n    # Run server behind deny-by-default enforcement\n    mcptrust proxy --lock mcp-lock.json -- npx u/modelcontextprotocol/server-filesystem /tmp\n    \n\n# What it enforces (practical production stuff)\n\n* **Runtime allowlisting** for tools/resources/prompts (lockfile v3)\n* **Drift detection** you can gate in CI (server changed vs lockfile)\n* **Supply-chain checks** (artifact hashing + provenance verification)\n* **Network hardening** (HTTPS-only downloads, SSRF defenses)\n* **Protocol hardening** (proxy-generated request IDs; drop unknown/duplicate responses)\n* **Policy hooks** (CEL) + signing (Sigstore keyless in CI / Ed25519 offline)\n\nRepo: [`https://github.com/mcptrust/mcptrust`](https://github.com/mcptrust/mcptrust)  \nLicense: Apache-2.0 (no tiers / no paid version)\n\nQuestion: in your agent/tooling stack, what‚Äôs the bigger pain, tool drift, resource expansion/data exfil, or artifact/provenance trust?",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pvi0em/opensource_project_denybydefault_runtime_controls/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvwl77s",
          "author": "DecodeBytes",
          "text": "awesome project!",
          "score": 2,
          "created_utc": "2025-12-25 18:37:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzz83e",
          "author": "lookyLool",
          "text": "Great stuff. Thank you for sharing.",
          "score": 2,
          "created_utc": "2025-12-26 09:06:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pww6zh",
      "title": "Failed Data Scientist trying to get into AI engineering",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pww6zh/failed_data_scientist_trying_to_get_into_ai/",
      "author": "throwaway18249",
      "created_utc": "2025-12-27 11:59:38",
      "score": 5,
      "num_comments": 18,
      "upvote_ratio": 0.78,
      "text": "I am not really sure how to write this post. My first job was a dead-end data scientist job where I worked in a fintech startup and used python/sql to do a mix of:\n\n1. Managing quantitative finance products\n2. Not-that-useful-for-business-value unsupervised machine learning models that were run manually on an AWS compute instance with no MLOps\n3. Data pipelines for tableau dashboards/daily email reports\n4. Ad-hoc business analysis in notebooks\n\nIn my next job (with about a year of unemployment after the last one) I worked as a data scientist, but mostly did data engineering work and left after 6 months:\n\n1. Postgres and Airflow backend development\n2. Simple statistical models for analytics with SQL that were calculated in a tumbling-window\n\nI have always wanted to get a proper job in machine learning engineering, and I have some of the skills required (LLMs, simple neural networks/traditional ml, infrastructure, working with data, data engineering, MLOps system design, CI/CD) but don't have the advanced skills required for this job (eg: reinforcement learning, computer vision, GPU infrastructure, recommendation, forecasting, robotics/embedded systems) and the market for MLE/DS jobs is incredibly competitive.\n\nI have come to realize that **my work experience/education is inadequate to compete with other candidates in the incredibly competitive and high-compensation DS/MLE job market**. So, now I am trying to pivot to a full-stack AI engineer role where there is a greater emphasis on front-end and back-end web application development while having the responsibilities of an AI engineer to use existing models (Eg: LLMs, Multimodal models, Hugging face, fine-tuning) to design and create AI features.\n\nMy definition of MLE/AIE being:\n\n* MLE: Engineers who build their own models, create algorithms/advanced ML strategies to address business problems, have a strong academic background\n* AIE: Engineers who use existing foundation models to set up AI workflows, do not use advanced ML strategies (RL, CV, etc...) or develop algorithms, do not have a strong academic background\n\nI am simply unable to compete with others to get a pure ML/AI role, so my plan is to become a full-stack AI engineer so as to utilize my existing engineering skills (while learning more front-end), while not entirely wasting my skills in ML/AI. The academic requirements for a full-stack web dev position are lower, and this job market has more positions than ML/AI (albeit lower salary, but I just want to continue my career), so I think this is the best course of action I can take right now.\n\nIn order to a job like that, I am trying to position myself as a full-stack engineer **who is willing to understand the product/business and knows how to use AI models to design features in to can create tangible value for the company**. This might be a tall order, but it's the best plan I have right now to revive my career which has been slowly dying, and I am open to any ideas/suggestions that may help. Thank you in advance.\n\nI am currently working on a project that will hopefully get me considered for AI/full-stack engineer jobs. It is a multi-agent system that integrates with a hypothetical CRM system to responds to customer support emails by understanding the content of the email, categorising it into an appropriate action category (e.g., escalate, flag, response, etc), and taking whatever actions are necessary (e.g., checking transactions/claims/statuses, etc...) to address the support request in that category. Then the agent prepares a response to the email with a list of actions taken and contextual data gathered from internal systems, for staff to manually review before sending it to the client. This interface for staff is accessible through an authenticated front-end which displays the details of the customer support case, the actions taken by the agent, and the email response that the agent has prepared.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pww6zh/failed_data_scientist_trying_to_get_into_ai/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nw6gm6h",
          "author": "dreamingwell",
          "text": "‚ÄúFailed‚Äù seems to imply somehow you stopped being a data scientist or whatever you wanted to be.\n\nYou‚Äôre putting too much focus on the title. Just do stuff and then you‚Äôre that thing. You don‚Äôt stop being that thing if you‚Äôre let go. Unless you stop doing it entirely.",
          "score": 14,
          "created_utc": "2025-12-27 12:06:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw7666v",
              "author": "throwaway18249",
              "text": "Thanks for the encouragement.",
              "score": 1,
              "created_utc": "2025-12-27 15:02:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw6lt32",
          "author": "makinggrace",
          "text": "No actual data here. Just a pep talk.\n\nKnowing how to do shit and proving that you do (go broad on github and showcase each skill group cleanly and clearly) is more compelling IMHO than a degree and work experience in a field that is moving as quickly as this one. You'll be happier working in the area you want to work in. It is always a benefit to be capable of understanding the business/business logic. That's what the data scientist brings the to ML/AI table. A lot of teams work in a black box from that perspective.\n\nUnderestimating yourself does no good. It's easy to lose confidence when you have a series of roles that just aren't it plus unemployment and oh yeah the economy. Been there (a different era) and it's a literal pit to crawl out of. Consider therapy. Even if you don't believe in it, building confidence = better interview outcomes.",
          "score": 2,
          "created_utc": "2025-12-27 12:49:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw75odw",
              "author": "throwaway18249",
              "text": "Hey, thank you for this. I am quite in a mental pit right now trying to crawl out, while being unemployed and trying to learn new skills. It is tough.",
              "score": 2,
              "created_utc": "2025-12-27 14:59:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nw7jwc6",
          "author": "wonker007",
          "text": "You are not the product of your education and work experience. Those only represent where you started and vaguely point where you may be headed, in what I am reading to be most of your career and life in front of you. You are much more than a resume and should value and evaluate yourself as such.\n\nNow for the advice (that ties in to how you should view yourself): You make no mention of learning, doing and experimenting new skills or fields on your own in your spare time - your emphasis/excuse on education/work background is telling. Nobody knew their skills at birth - it was all learned. What I look for in employees and collaborators is meta recognition (do you know what you don't know) and a strong self-learning/improvement ethic. This is because setting an always-higher bar for yourself means the personal standard of work also naturally becomes almost perfectionist. And as an employer and a project manager, I demand high output quality standards. Skills can be taught, attitude is your problem. And with the speed in which IT is moving, a passive attitude regarding your own skill set will leave you right back in unemployment land 5 years later - if you're lucky to hold on that long. Remember the prompt engineers at the height of COVID?\n\nI see that you are trying to reposition and pivot, and I applaud the attempt. But I believe you must also just dive in head-first and just develop the skills on your own time you already know will significantly elevate your chances of snagging that job you want. Gemini, Claude, Perplexity are your friends right now. Tap them for ideas and approaches to the most important skills and just do it. I have a child in his last year of college majoring in ComSci. I know how competitive and tough it is especially in the job market right now. It is palpable. But it is also true that chance favors the prepared mind. Don't blow any opportunity that may (and probably will) present itself in front of you in the future because your school and past jobs didn't give you the skills you needed to snag that opportunity. If you find yourself making that excuse, you deserve exactly what is coming.\n\nYou have incredible potential. Hell, I still feel that I still have potential left to develop and study every day. Don't be your greatest antagonist by believing that somehow you are simply a finished product of your past.",
          "score": 2,
          "created_utc": "2025-12-27 16:14:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw8nio3",
          "author": "robogame_dev",
          "text": "Case studies and portfolio beat degrees and certificates every time in the software biz.\n\nYou don‚Äôt want to go super deep with each demo project - the person evaluating it only wants to see a few screenshots tops, and is mostly scanning for keywords matching their tech stack or problem area. Optimal is that they can scan a list of 5-10 things you‚Äôve made, identify the 2-3 that lines up with their needs, and take a quick peek at those.\n\nI‚Äôve hired a fair number of developers in a past life as a startup guy. People who do projects on their own, determining what to make and building it, are a way safer hiring bet than anyone who‚Äôs portfolio is all things made at their jobs - anyone can portfolio by being in a team, there‚Äôs no way to know if they did any of that work, or if they were actually slowing said team down.\n\nOn the other hand someone who has self-built portfolio goes straight to the front of the line, you‚Äôre almost guaranteed that they can get stuff done. The very first question I want to know from people is ‚Äúso, do you ever code stuff for yourself, personal projects, for fun - anything like that?‚Äù An enthusiastic ‚Äúyes check this out!‚Äù Is the best possible evidence that they‚Äôre gonna be effective.\n\nSo step 1 is to bang out 5-10 1 week demos, put them on a public portfolio website with a screenshot and 5 paragraphs each, and distribute them across the techs and keywords for the jobs you want. Some humans will read it, many will just ask AI about you, so make sure both will be able to find it easily, drive them there from the top of any materials you submit etc.\n\nStep 2 is don‚Äôt bother applying to jobs. There are so many people going for jobs once they‚Äôre posted,  it‚Äôs effectively a lottery. You should identify companies you want to work at proactively, locate the people in related roles you want to work for or with on LinkedIn, and connect with them directly. For every job that gets posted, there is an unposted need that if you reach out, they can create the position for you.",
          "score": 2,
          "created_utc": "2025-12-27 19:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwb6qh2",
              "author": "throwaway18249",
              "text": "This is nothing short of fantastic advice. Thank you good sir.",
              "score": 3,
              "created_utc": "2025-12-28 04:14:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwi3fj2",
          "author": "Maleficent-Animal-57",
          "text": "I lead an AI Engineering team and have interviewed dozens of candidates for AIE roles this year and last. It's still a new role and we rarely find strong candidates, so I'd say the playing field is far more level than for DS or MLE (I was formerly both).\n\nHere are a few things candidates have done in interviews and on their resumes to stand out:\n\n* **Projects you can show.** If you can share your screen and walk through a project you've built, that puts you well ahead of most candidates.\n* **Product focus.** The interviews, despite seeming very technical on paper, are heavily product-focused. Our AIE team works closely with product and platform engineering.\n* **Scrappiness and breadth.** Many of us on the team are or were avid hackathon participants. A lot of our work starts as experimentation where we vibe code a product end-to-end, demo it to stakeholders, and if it gains traction, we put it into production (some of our AI products serve millions of users). We value people who can cobble things together quickly and also have the engineering discipline to take it to production safely.\n* **Genuine curiosity.** The field changes every week, and it's very evident when a candidate is genuinely excited about those developments and tinkers with the latest AI coding tools, APIs, and models versus a candidate who read five blog posts on RAG the day before the interview.\n\nThere's a lot more to say here. Feel free to dm. If you're US based, we're also hiring ;)",
          "score": 2,
          "created_utc": "2025-12-29 06:13:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk8a5x",
              "author": "throwaway18249",
              "text": "Thank you for sharing your perspective. Sure, I'll message you privately to connect. I've definitely got some questions because I want to find out what is going on in the industry.",
              "score": 1,
              "created_utc": "2025-12-29 15:50:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqd40m",
          "author": "djdjddhdhdh",
          "text": "Ai engineering is competitive as well. Most people are coming from years of experience. It sounds like you‚Äôre sadly a part of the whole junior hiring crisis. If I was you I‚Äôd get some projects under your belt, GitHub, huggingface. Get a mix of ml and se projects. Cuz an engineer who knows ml and can build a product around it and then run it in prod is incredibly valuable",
          "score": 2,
          "created_utc": "2025-12-30 14:07:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqyvkl",
              "author": "throwaway18249",
              "text": "That's what I am working on right now.\n\nknowing ml can mean many things. I thought it meant classification/regression/clustering, which I can do well and put into production, but there are very few uses for it in the industry and this didn't help me get a job.\n\nI'm hoping that adding some full stack to my skillset will help my career and provide a more stable direction. I am not a competitive person, and I don't want to compete with the MScs and PhDs.",
              "score": 1,
              "created_utc": "2025-12-30 16:00:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwrctby",
                  "author": "djdjddhdhdh",
                  "text": "Ye totally get it\n\nAnd the stuff you mentioned is still very relevant, it‚Äôs just that we are in a weird funk where companies think they can replace juniors with ai, so the work juniors would typically grind their teeth on just isn‚Äôt there in volumes it used to be. So 100% getting some software eng under your belt is good. \n\nI‚Äôd suggest integrating monitoring into your project with auto analysis/evals to detect and surface drift. Observability is the things most ppl forget",
                  "score": 1,
                  "created_utc": "2025-12-30 17:05:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwbv8h6",
          "author": "nmrk",
          "text": "There is a job for you on the Golgafrinchan Ark Fleet Ship B.",
          "score": 1,
          "created_utc": "2025-12-28 07:28:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwc8qov",
          "author": "queenjulien",
          "text": "May I ask, why did you leave your last job? Skills are important, but having a stable employment track record, even if it's not exactly for the role you want, is also a good signal for companies",
          "score": 1,
          "created_utc": "2025-12-28 09:38:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdmxo6",
              "author": "throwaway18249",
              "text": "I'll message you privately",
              "score": 1,
              "created_utc": "2025-12-28 15:49:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pwkjgi",
      "title": "Slashed My RAG Startup Costs 75% with Milvus RaBitQ + SQ8 Quantization!",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pwkjgi/slashed_my_rag_startup_costs_75_with_milvus/",
      "author": "Ok_Mirror7112",
      "created_utc": "2025-12-27 01:08:14",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "Hello everyone, I am building no code platform where users can build RAG agents in seconds.\n\nI am building it on AWS with S3, Lambda, RDS, and Zilliz (Milvus Cloud) for vectors. But holy crap, costs were creeping up FAST: storage bloating, memory hogging queries, and inference bills.\n\nStoring raw documents was fine but oh man storing uncompressed embeddings were eating memory in Milvus.\n\nThis is where I found the solution:\n\nWhile scrolling X, I found the solution and implemented immediately.\n\nSo 1 million vectors is roughly 3 GB uncompressed.\n\nI used Binary quantization with RABITQ (32x magic), (Milvus 2.6+ advanced 1-bit binary quantization)\n\nIt converts each float dimension to 1 bit (0 or 1) based on sign or advanced ranking.\n\nSize per vector: 768 dims √ó 1 bit = 96 bytes (768 / 8 = 96 bytes)\n\nCompression ratio: 3,072 bytes ‚Üí 96 bytes = \\~32x smaller.\n\nBut after implementing this, I saw a dip in recall quality, so I started brainstorming with grok and found the solution which was adding SQ8 refinement.\n\n* Overfetch top candidates from binary search (e.g., 3x more).\n* Rerank them using higher-precision SQ8 distances.\n* Result: Recall jumps to near original float precision with almost no loss.\n\nMy total storage dropped by 75%, my indexing and queries became faster.\n\nThis single change (RaBitQ + SQ8) was game changer. Shout out to the guy from X.\n\nLet me know what your thoughts are or if you know something better.\n\nP.S. Iam Launching Jan 1st ‚Äî waitlist open for early access:¬†[mindzyn.com](http://mindzyn.com/)\n\nThank you",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pwkjgi/slashed_my_rag_startup_costs_75_with_milvus/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1puuv0v",
      "title": "Eval setup was slowing us down more than model work",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1puuv0v/eval_setup_was_slowing_us_down_more_than_model/",
      "author": "coolandy00",
      "created_utc": "2025-12-24 19:08:15",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 0.73,
      "text": "We used to eval by spot-checking outputs and eyeballing logs. It always felt fine‚Ä¶ until something broke in prod and we couldn‚Äôt reproduce it. Then we‚Äôd lose hours because the outputs weren‚Äôt consistent (especially JSON) and we didn‚Äôt have a baseline to compare.\n\nNow we keep a small smoke eval that runs fast, validate JSON/schema first, and diff results against the last known good run.  \nIt‚Äôs not fancy, but it changed everything: we can tell in minutes if a change actually improved things or just shifted failures around.\n\nWhat‚Äôs the one part of eval that still wastes the most time for you?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1puuv0v/eval_setup_was_slowing_us_down_more_than_model/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvu11dc",
          "author": "Mikasa0xdev",
          "text": "Smoke tests save hours in LLM dev, true fact lol.",
          "score": 1,
          "created_utc": "2025-12-25 06:07:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvzbf26",
          "author": "dmart89",
          "text": "Same. I built this small cli for it which lets me run tests against different outputs i require\nhttps://github.com/davismartens/ev",
          "score": 1,
          "created_utc": "2025-12-26 05:20:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu52yw",
      "title": "Ingestion + chunking is where RAG pipelines break most often",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pu52yw/ingestion_chunking_is_where_rag_pipelines_break/",
      "author": "coolandy00",
      "created_utc": "2025-12-23 21:05:00",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I used to think chunking was just splitting text. It‚Äôs not. Small changes (lost headings, duplicates, inconsistent splits) make retrieval feel random, and then the whole system looks unreliable.\n\nWhat helped me most: keep structure, chunk with fixed rules, attach metadata to every chunk, and generate stable IDs so I can compare runs.\n\nWhat‚Äôs your biggest pain here: PDFs, duplicates, or chunk sizing?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pu52yw/ingestion_chunking_is_where_rag_pipelines_break/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvm0njw",
          "author": "natalyarockets",
          "text": "My biggest challenges are to do with ingesting PDFs of equipment manuals: connecting references to images/figures back to them, figuring out what to do with said figures (semantically summarize and embed that as a chunk and refer back to it?) and flow diagrams (convert to mermaid?), extract text like part numbers from images and figure out how/when to return it. Basically a lot of referencing and storage challenges and both ingestion and runtime.",
          "score": 3,
          "created_utc": "2025-12-23 21:19:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvw0xh",
              "author": "coolandy00",
              "text": "You mean you broke them into structured, addressable objects (text, figures, diagrams, entities) with explicit references, then generate derived representations (summaries, entities, Mermaid) that get embedded and linked. At runtime, you assembled the answers by resolving entities and references first..",
              "score": 1,
              "created_utc": "2025-12-25 16:08:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nvolzde",
          "author": "Main_Payment_6430",
          "text": "I wasted so much time trying to tune chunk sizes, but the problem is that code isn't just text, it is logic. If you split a function from its imports just to fit a token limit, that chunk becomes useless noise.\n\nThat is why I switched to using CMP. It doesn't chunk by size, it maps the actual AST (the code structure). It builds a skeleton of the signatures and types so the context is preserved exactly how the compiler sees it. It completely fixed that \"random\" retrieval issue for me because the AI isn't guessing based on text fragments anymore, it is following the real dependencies.",
          "score": 2,
          "created_utc": "2025-12-24 07:28:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvvwwbc",
              "author": "coolandy00",
              "text": "As long as it's structure-aware ingestion (ASTs, symbols, dependencies) so context is preserved the way a compiler sees it, which removes the randomness",
              "score": 1,
              "created_utc": "2025-12-25 16:13:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvw6fex",
                  "author": "Main_Payment_6430",
                  "text": "exactly, that's the goal to preserve the main things, AST, signature so the relation of files in the map is easier for claude and other ai to navigate than me having to paste files or they spending tokens just seeing the files structure each time. you should watch this video - [empusaai.com](http://empusaai.com)",
                  "score": 1,
                  "created_utc": "2025-12-25 17:10:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nvqj21j",
          "author": "patbhakta",
          "text": "This is 100% true.\n\nPDF ingestion is the worst, you constantly get problems as everyone stated.\nChunking issues\nOverlap issues\nRetrieval issues\nVectordb, graphdb, dB, issues\nPDF translation issues (fonts, formulas, tables, diagrams, links, foot notes, etc)\n\nMy work flow looks like this, get a random PDF, docx, xlsx, url, etc for processing. I check it out to see if the information is proprietary or not if not then I dump it into notebook llm to do a brief test on it. If it passes or proprietary then I dump it into open notebooklm with docling and pray. It's trashy but sometimes its better than nothing.\n\nI'm on the verge of giving up as it's too much work to scrub the data, with garbage data forget about inference, rag is hit or miss.\n\nDebating on a hybrid solution of using gemini file search for public PDFs, Chunking, embedding, and vector store. Then use another pipeline with a dedicated GPU running a hybrid OCR/VL LLM RAG.\n\nOpen source tools suck, 3rd party services suck, fortune 10 company tools suck... Lol seems like there isn't a solution unless you have manual HIL and/or heavy Ai cost.\n\nIf anyone is interested in a brainstorming session my DMs are open for a collab.",
          "score": 2,
          "created_utc": "2025-12-24 16:16:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvmla2u",
          "author": "CreepyValuable",
          "text": "I'm working on a whole other things but it has similarities. Contamination is a huge issue. It can completely send things off the rails. So badly that it can require a structural revision just to compensate for these cases.",
          "score": 1,
          "created_utc": "2025-12-23 23:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvpf8og",
              "author": "kingshekelz",
              "text": "To put it simple its alot of work to get things to work right  end to end.",
              "score": 1,
              "created_utc": "2025-12-24 12:09:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvp5f8h",
          "author": "0x-dawg",
          "text": "I wonder whether having an ontology serve as a \"contextualizer\" that forces chunking to be consistent according to the ontology would make sense",
          "score": 1,
          "created_utc": "2025-12-24 10:39:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvumkzn",
          "author": "Unique-Big-5691",
          "text": "yeah, this is one of those ‚Äúsounds simple, ruins everything if done sloppy‚Äù parts of RAG.\n\nchunking isn‚Äôt splitting text, it‚Äôs preserving meaning *under constraints*. once headings disappear or chunks shift between runs, retrieval starts feeling random even if the model is fine.\n\nagree a lot w/ what you said:\n\n* fixed rules > clever logic\n* metadata everywhere (section, source, order)\n* stable IDs so you can diff runs and debug instead of guessing\n\nthis is also where structure really helps. treating chunks like contracts instead of blobs makes the system feel predictable. that mindset is why stuff like pydantic fits so naturally here ‚Äî explicit schemas beat ‚Äúhope the text lines up‚Äù every time.\n\nbiggest pain for me has been PDFs tbh. inconsistent layouts + hidden structure are brutal. once that‚Äôs clean, sizing is way easier to reason about.",
          "score": 1,
          "created_utc": "2025-12-25 09:52:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvma7n9",
          "author": "OnyxProyectoUno",
          "text": "The issue is usually that you can't see what's happening between raw doc and final chunks. Most tools are black boxes where you dump files in and hope the chunking logic works, then you only find out chunks are broken when retrieval starts failing. By then you're debugging three layers deep instead of catching it at the source.\n\nChunk sizing hits me the worst because context windows keep changing and what worked for one document type completely breaks another. PDFs are brutal too since the parsing step can mess up before chunking even starts, but you don't know until way later. What document types are giving you the most trouble? Been working on something for this visibility problem, lmk if you want to check it out.",
          "score": 0,
          "created_utc": "2025-12-23 22:09:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu4347",
      "title": "Teaching AI Agents Like Students (Blog + Open source tool)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pu4347/teaching_ai_agents_like_students_blog_open_source/",
      "author": "Unable-Living-3506",
      "created_utc": "2025-12-23 20:22:55",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "**TL;DR:**  \nVertical AI agents often struggle because domain knowledge is tacit and hard to encode via static system prompts or raw document retrieval. What if we instead treat agents like students: human experts teach them through iterative, interactive chats, while the agent distills rules, definitions, and heuristics into a continuously improving knowledge base. I built an open-source prototype called [Socratic ](https://github.com/kevins981/Socratic)to test this idea and show concrete accuracy improvements.\n\nFull blog post: [https://kevins981.github.io/blogs/teachagent\\_part1.html](https://kevins981.github.io/blogs/teachagent_part1.html)\n\nGithub repo (Apache 2):¬†[https://github.com/kevins981/Socratic](https://github.com/kevins981/Socratic)\n\n3-min demo:¬†[https://youtu.be/XbFG7U0fpSU?si=6yuMu5a2TW1oToEQ](https://youtu.be/XbFG7U0fpSU?si=6yuMu5a2TW1oToEQ)\n\n  \nAny feedback is appreciated!\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pu4347/teaching_ai_agents_like_students_blog_open_source/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvmc21y",
          "author": "OnyxProyectoUno",
          "text": "The teaching approach makes a lot of sense, especially for domains where the real expertise lives in those messy edge cases that never make it into documentation. One thing I've noticed with similar systems is that the quality of your underlying document retrieval can make or break the whole feedback loop. If your agent is pulling in poorly chunked or irrelevant context during those teaching conversations, the human expert ends up correcting retrieval issues rather than actually teaching domain knowledge.\n\nHave you experimented much with how the document processing affects the teaching quality? I'm curious whether you've seen cases where the agent struggles to learn because the foundational documents feeding into those conversations aren't being parsed or chunked in a way that preserves the important contextual relationships.",
          "score": 2,
          "created_utc": "2025-12-23 22:19:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvmjsa4",
              "author": "Unable-Living-3506",
              "text": "Hello!\n\nTo directly answer your question: Socratic doesnt use RAG/embedding based retrieval, so there is no chunking involved. \nSocratic uses agentic search to navigate through the documents, same as coding agents. In the background, there is an agent that has access to a terminal. The agent uses bash commands like grep and sed to figure out which portion of the docs are relevant to the current task. \n\nThe downside of agentic search is that it uses more tokens than RAG. But the upside is that it allows the agent to find information more accurately. You no longer rely on chunking/small embedding model to find relevant info. Instead, you use the LLM to decide whats relevant, which is often more accurate.\n\nI am not saying RAG is no good. I used agentic search because it was easier to implement for me.\n\nRegarding your question about chunking hurting important context, there are a bunch of more advanced RAG techniques out there, eg agentic RAG, contextual RAG. My understanding is that they are made to address that problem, and should outperform default RAG (at the cost of additional tokens).\n\nHope this helps. Happy to discuss more!",
              "score": 1,
              "created_utc": "2025-12-23 23:01:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pvqspu",
      "title": "Local LLM concurrency question: ‚Äúsatellite orchestration‚Äù works, but LM Studio serializes requests and kills parallelism",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/h1ug8mjmzf9g1.png",
      "author": "marcosomma-OrKA",
      "created_utc": "2025-12-26 00:23:00",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pvqspu/local_llm_concurrency_question_satellite/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nvy9ryc",
          "author": "aaronr_90",
          "text": "Vllm, or even Llama.cpp can handle parallel requests.\n\nhttps://www.reddit.com/r/LocalLLaMA/s/eBaqUqIkzP",
          "score": 3,
          "created_utc": "2025-12-26 00:51:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvyabws",
              "author": "marcosomma-OrKA",
              "text": "Lovely!",
              "score": 1,
              "created_utc": "2025-12-26 00:55:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvyfp7o",
                  "author": "btdeviant",
                  "text": "Same with Ray",
                  "score": 1,
                  "created_utc": "2025-12-26 01:32:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nvzgvqm",
              "author": "Longjumping_Rule_163",
              "text": "Was here to say this. vLLM is vastly superior!",
              "score": 1,
              "created_utc": "2025-12-26 06:06:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nvzv3bz",
          "author": "Charming_Support726",
          "text": "llama.cpp might be the one, which you could setup easiest. vLLM could give some perfomance boost, but while researching not worth the hassle. \n\nOn linux I run llama.cpp on my AMD utilizing kyuz0 toolbox approach - which is just a specialized docker having access to your home / cache directory making things very easy. I dont use the model switching on llama.cpp. I start different toolboxes per modell on different ports",
          "score": 1,
          "created_utc": "2025-12-26 08:23:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nw0jnuo",
          "author": "Mythril_Zombie",
          "text": "Is there a functional difference between this architecture and multiple linear parallel paths with different LMMs at the end?    \nI'm trying to see the difference; does the 1:many schema give additional functionality, or is it a way to reduce necessary resources?   \nThe separate interactions with the LLM are encapsulated from each other, right? Like, they aren't adding to each other's context mid-processing.",
          "score": 1,
          "created_utc": "2025-12-26 12:29:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nw14fes",
              "author": "marcosomma-OrKA",
              "text": "Yes basically the the agent in the outer circle (satellite agents) are processing same input as the executor.  \nBut while the executor has the main role of keep the conversation active and engaging the satellite agents are generating context that will then be fees in to the executor. So executor can be a streaming agent and it only get updated context time to time. Main scope is to enforce behavior by real analysis during conversation and not using gigantic prompt.  \nMore details here:  \n[https://www.linkedin.com/pulse/orchestration-calling-agents-marco-somma-z2b8e/?trackingId=jFRwMfNfiqTOpaDpHq8WoA%3D%3D](https://www.linkedin.com/pulse/orchestration-calling-agents-marco-somma-z2b8e/?trackingId=jFRwMfNfiqTOpaDpHq8WoA%3D%3D)",
              "score": 1,
              "created_utc": "2025-12-26 14:53:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nw6ulj5",
                  "author": "dsartori",
                  "text": "It‚Äôs an interesting notion. You‚Äôre a lot further along than I am in exploring this space. Appreciate you sharing.",
                  "score": 1,
                  "created_utc": "2025-12-27 13:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1pzwp5t",
      "title": "LLM says it did an action‚Ä¶ but never actually used the tool ü§¶‚Äç‚ôÇÔ∏è",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pzwp5t/llm_says_it_did_an_action_but_never_actually_used/",
      "author": "marcocello",
      "created_utc": "2025-12-30 23:07:19",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm building an LLM agent with access to a fixed set of tools that perform real actions (create/update records, etc.).\n\nProblem: The model sometimes claims it did something (‚ÄúDone, I've done what you asked‚Äù) without ever calling the tool that would actually do it.\n\nSo:\n\n* If it can‚Äôt do something, I want it to say so\n* If no tool exists, I want a refusal\n* If no tool was called, it shouldn‚Äôt claim success\n\nStronger prompts help a bit, but don‚Äôt fully solve it.\n\n\n\nHow do you enforce *‚Äúno tool call = no claim of success‚Äù* in agent systems?\n\nPrompting? Execution contracts? Validation layers? Planning + verification loops?\n\nCurious what actually works in practice",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pzwp5t/llm_says_it_did_an_action_but_never_actually_used/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwtivvr",
          "author": "dreamingwell",
          "text": "Your context probably grew to be too big. Or just use a higher quality model. \n\nDon‚Äôt allow it to not call a tool (respond no tools called)\n\nSecondary LLM for validation",
          "score": 2,
          "created_utc": "2025-12-30 23:18:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtku40",
          "author": "xelnet",
          "text": "I can help out with this. CRUD is a specific animal. \n\nThe answer to your specific problem will depend on how your tools are designed and organized, as well as how the api‚Äôs are documented.\n\nDiagnostic will require detailed visibility into the logs.\n\nI‚Äôve spent the last year specializing in multiagent CRUD and built an observation tool for fine tuning to get to 100% tested crud environment",
          "score": 1,
          "created_utc": "2025-12-30 23:29:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwu3ah1",
          "author": "robogame_dev",
          "text": "99% of the time this is because the tools weren't actually presented to the model - log your raw payload before it goes to the model provider and verify the tools are present, that the model supports native tool calling or whatever. Or if you're using a framework with too much abstraction for that, chat with the model - ask it for details of the tools that it could only answer correctly if it had the real tool, for example \"Quote the exact description for tool \"my\\_tool\".",
          "score": 1,
          "created_utc": "2025-12-31 01:11:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pza816",
      "title": "Career advice regarding agentic ai engineer",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pza816/career_advice_regarding_agentic_ai_engineer/",
      "author": "EarthIntrepid7166",
      "created_utc": "2025-12-30 05:58:59",
      "score": 5,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Can any person who is been into the industry give me advice on is it worth it to go all in learning agentic ai. Like learning python , async programming , fast api , docker and databases management, tools, mcp. And make good projects around it. Like is their any opportunity for being an agentic ai engineer who is able to make good scalable agentic ai applications. Such roles are not floating around but I just want to know is their going to be or not. For a college student from Tier 1 college , that would be lot helpful.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pza816/career_advice_regarding_agentic_ai_engineer/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwp6m5j",
          "author": "metaphorm",
          "text": "agent development isn't really a specialization, it's just the state of the art in software development. it's worth learning some of the techniques. it's not a career path any more than MVC web framework is a career path.",
          "score": 3,
          "created_utc": "2025-12-30 08:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpfqcs",
          "author": "burntoutdev8291",
          "text": "No such career, just focus on basics. These tools are usually add on to current roles.",
          "score": 1,
          "created_utc": "2025-12-30 09:55:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpz8a5",
          "author": "Autwalk422",
          "text": "not really a career path or special role. Its fairly on easier side to master and implement then core MLE and datascientist roles and easier to replace relatively. recruiters are now itself hiring for AI engineers : full stack + RAG + agents. so its just more of a backend. role. if you have time, focus on core MLE stack if you want to purse career in ML. (mostly hiring across startups only)",
          "score": 1,
          "created_utc": "2025-12-30 12:40:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwv0sr8",
              "author": "FormalAd7367",
              "text": "but any ai can create the whole stack ?",
              "score": 1,
              "created_utc": "2025-12-31 04:33:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwrvy1r",
          "author": "CommodoreQuinli",
          "text": "AI engineering is just full stack engineering with pre trained llms in the stack, I wouldn‚Äôt focus solely on the llms and the only thing you listed that is ai specific is mcp everything else pertains to a standard full stack role",
          "score": 1,
          "created_utc": "2025-12-30 18:33:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwskmyd",
          "author": "airylizard",
          "text": "Learn IT Support systems. Automation typically falls under the umbrella of Data & IT, no so much engineering (at least in my experience).",
          "score": 1,
          "created_utc": "2025-12-30 20:30:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwotemj",
          "author": "CommercialComputer15",
          "text": "There is no career in agentic engineering. Focus on non digital roles and you‚Äôll be fine",
          "score": 1,
          "created_utc": "2025-12-30 06:31:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py3ax4",
      "title": "What‚Äôs your plan if a much better model drops (databases)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1py3ax4/whats_your_plan_if_a_much_better_model_drops/",
      "author": "BiggieCheeseFan88",
      "created_utc": "2025-12-28 21:38:54",
      "score": 4,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "You have 100 million items embedded with last year's model. A better model just dropped. What's your plan?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1py3ax4/whats_your_plan_if_a_much_better_model_drops/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwg5e0n",
          "author": "kkingsbe",
          "text": "Better get to reindexing lol",
          "score": 1,
          "created_utc": "2025-12-28 23:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjd6el",
          "author": "cangelis",
          "text": "It depends on how you structure them. Id store the name of the embedding model per collection (if my requirement is to query only one collection at a time) so that i could support multiple embedding models and query them based on the collection's model. I could start using the new model without migrating anything and also would give me a chance to see how better the new model is and assess if it is worth the migration. If the legacy collections could benefit from the new embedding model I'd consider reindexing in phases gradually.\n\nIf the requirement was to query on one global collection, I would create a new collection for the new embedding - > index the content - > do A/B testing - > remove the old collection.\n\nSo it really depends on your product and use-cases. I think the key here is to measure and see if there is any benefit of using the new model before you spend a fortune to reindex the whole content. Another thing is also about not making a change that can break production and planning a smooth transition.",
          "score": 1,
          "created_utc": "2025-12-29 12:55:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp9a7z",
              "author": "TheLexoPlexx",
              "text": "Yeah, tagging is the way to go but as of right now, an entire re-index takes me about 5 minutes max, so not even worth the effort.",
              "score": 1,
              "created_utc": "2025-12-30 08:54:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxcmh6",
      "title": "GitHub - pgedge-vectorizer: A PostgreSQL extension to create chunk tables for existing text data, and populate them with embeddings using your favourite LLM",
      "subreddit": "LLMDevs",
      "url": "https://github.com/pgEdge/pgedge-vectorizer",
      "author": "pgEdge_Postgres",
      "created_utc": "2025-12-28 00:03:19",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pxcmh6/github_pgedgevectorizer_a_postgresql_extension_to/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwemkxh",
          "author": "Mikasa0xdev",
          "text": "Vector databases are crucial for RAG, this is awesome lol.",
          "score": 1,
          "created_utc": "2025-12-28 18:44:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyme9l",
      "title": "Building 1 AI agent per day for the next 30 days - what should I build?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pyme9l/building_1_ai_agent_per_day_for_the_next_30_days/",
      "author": "anitakirkovska",
      "created_utc": "2025-12-29 13:33:21",
      "score": 4,
      "num_comments": 10,
      "upvote_ratio": 0.83,
      "text": "Hey everyone,\n\nI am starting a 30 day challenge to build 1 AI agent per day. So far I've built 3 agents, and need ideas for the next ones. I'm publishing all of my learnings and free access to all of them on agent yard (.) co \n\nWhat are some agents that you'd want to use? Please let me know in the comments and I'll try to build them ",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pyme9l/building_1_ai_agent_per_day_for_the_next_30_days/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwjkj3b",
          "author": "Hot_Substance_9432",
          "text": "Like this? [https://github.com/akshsgaur/30DaysofAIAgents](https://github.com/akshsgaur/30DaysofAIAgents)",
          "score": 2,
          "created_utc": "2025-12-29 13:43:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjn0v9",
              "author": "anitakirkovska",
              "text": "yes exactly, but I'm using vibe-coding tools. Should I also open a github page?",
              "score": 1,
              "created_utc": "2025-12-29 13:57:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwjn6zk",
                  "author": "Hot_Substance_9432",
                  "text": "Yes for sure it will give you visibility for  new jobs in future:)",
                  "score": 2,
                  "created_utc": "2025-12-29 13:58:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwjp2ii",
                  "author": "Hot_Substance_9432",
                  "text": "I apologize :) you are a big shot at vellum and do not need a  new job:)",
                  "score": 1,
                  "created_utc": "2025-12-29 14:09:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjpxr8",
          "author": "Hot_Substance_9432",
          "text": "What do you think of this [https://www.linkedin.com/posts/ashishpatel2604\\_35-agentic-ai-projects-hands-on-guide-for-activity-7389177729620979712-9tvr/](https://www.linkedin.com/posts/ashishpatel2604_35-agentic-ai-projects-hands-on-guide-for-activity-7389177729620979712-9tvr/)",
          "score": 2,
          "created_utc": "2025-12-29 14:14:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjxeok",
              "author": "anitakirkovska",
              "text": "helpful thank you!",
              "score": 1,
              "created_utc": "2025-12-29 14:56:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwnnvi8",
                  "author": "Hot_Substance_9432",
                  "text": "Even this [https://medium.com/@nocobase/top-18-open-source-ai-agent-projects-with-the-most-github-stars-f58c11c2bf6c](https://medium.com/@nocobase/top-18-open-source-ai-agent-projects-with-the-most-github-stars-f58c11c2bf6c)",
                  "score": 1,
                  "created_utc": "2025-12-30 02:08:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnb57p",
          "author": "baulperry",
          "text": "an ai agent that tells you what to build",
          "score": 2,
          "created_utc": "2025-12-30 00:57:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp8sgu",
          "author": "necati-ozmen",
          "text": "Here we‚Äôre collecting AI agent examples built with the VoltAgent AI agent framework.\n\nFull source code is included, so you can explore them and use them as inspiration.  \n[https://voltagent.dev/recipes-and-guides/](https://voltagent.dev/recipes-and-guides/)",
          "score": 2,
          "created_utc": "2025-12-30 08:50:13",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1py70qw",
      "title": "Built: OpenAI-compatible ‚Äúprompt injection firewall‚Äù proxy. I couldn‚Äôt find OSS that fit my needs. Wondering if anyone is feeling this pain and can help validate / review this project.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1py70qw/built_openaicompatible_prompt_injection_firewall/",
      "author": "jdpahl122",
      "created_utc": "2025-12-29 00:13:51",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 0.81,
      "text": "I‚Äôm sharing an early OSS project called Graedin Cline: a self-hosted LLM security proxy that sits between your app and your provider and tries to catch prompt injection / jailbreak / data exfil attempts before they hit the real model. \n\nRepo: [https://github.com/jdpahl122/graedin-cline](https://github.com/jdpahl122/graedin-cline)\n\nI work in MLOps and haven't found any great OSS solutions that solved this problem for me. I'm wondering if anyone else has this problem for personal projects where cloud provider or vendor specific solutions don't quite cut it. Please help guide me in advancing this project and adding quality features.\n\nSome things on my roadmap: 1) support small local models for classification to improve performance 2) Configuration UI 3) Possible rewrite in go / other more performant language for proxy ",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1py70qw/built_openaicompatible_prompt_injection_firewall/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwgjkm4",
          "author": "New_Comfortable7240",
          "text": "Wait, isn't that just a LLM guard/Input guardrails¬†layer, with another name?",
          "score": 1,
          "created_utc": "2025-12-29 00:29:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwgkk9w",
              "author": "jdpahl122",
              "text": "It is, but my intention is to have this as a completely OSS project, using a classifier and not regex, and not have sneaky enterprise pricing for basic features. Things like LiteLLM have a huge feature set and charge for basic things like logging. Outside of that, you're probably using something rolled by AWS, GCP, Azure, or OpenAI which is proprietary and specific to that ecosystem.",
              "score": 3,
              "created_utc": "2025-12-29 00:34:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwgn1io",
              "author": "staccodaterra101",
              "text": "Most of the projects advertised here and in all similar subs are just a different approach of providing the same functionality. And its perfecly fine like that because thats how sometimes a project pops out and become the standard. The only problem with this is the usually low mantainance and development of foss.",
              "score": 3,
              "created_utc": "2025-12-29 00:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwgoe65",
                  "author": "jdpahl122",
                  "text": "Yep, agreed. There‚Äôs definitely a lot of ‚Äúsame problem, different angle‚Äù in these subs, and that‚Äôs not a bad thing. The maintenance point is real.\n\nFor what it‚Äôs worth, I‚Äôm building this regardless because I personally keep needing a proxy-style guard I can drop in front of multiple projects (and I don‚Äôt want to depend on proprietary ecosystem-specific solutions). Sharing it is me basically saying: ‚ÄúI‚Äôm doing the work anyway. If anyone else feels this pain, I‚Äôd love feedback + test cases.‚Äù If nobody needs it, no harm done. If a few folks do, I‚Äôm happy to iterate in the open and keep it maintained since I‚Äôll be using it long-term.",
                  "score": 2,
                  "created_utc": "2025-12-29 00:54:56",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwjuvms",
              "author": "Mikasa0xdev",
              "text": "Guardrails need a good firewall.",
              "score": 1,
              "created_utc": "2025-12-29 14:42:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwh743d",
          "author": "FakeTunaFromSubway",
          "text": "Sorta cool but I think would be way more useful as a simple Python package rather than having to do all sorta devops to get this working. You know just like a \\`graedin.CheckForPromptInjection(my\\_prompt)\\`",
          "score": 1,
          "created_utc": "2025-12-29 02:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhw6em",
              "author": "AdditionalWeb107",
              "text": "eventually, you will be responsible for all \"middleware\" in your application layer - when that can be neatly buttoned up as an out-of-process proxy so you focus on core product logic, not the plumbing. Similar style project: [https://github.com/katanemo/plano](https://github.com/katanemo/plano)",
              "score": 1,
              "created_utc": "2025-12-29 05:17:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwi0g0h",
                  "author": "FakeTunaFromSubway",
                  "text": "And you're not responsible for a proxy layer? No the proxy layer has way more possible issues because it's routing requests directly through it, so now I have to wonder if it has zero-day exploits or other footguns. Plano at least has some reason to be a proxy layer because it's meant to be a front-line, and seems to have some traction. But would never trust a random vibe-coded project like OPs running as a proxy layer.",
                  "score": 1,
                  "created_utc": "2025-12-29 05:50:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwhvxyl",
          "author": "AdditionalWeb107",
          "text": "Check out Plano - https://github.com/katanemo/plano. A models-native proxy server for agentic traffic.",
          "score": 1,
          "created_utc": "2025-12-29 05:16:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pucg9h",
      "title": "Curious how GenAI teams (LLMOps/MLE‚Äôs) handle LLM fine tuning",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pucg9h/curious_how_genai_teams_llmopsmles_handle_llm/",
      "author": "Shreevenkr",
      "created_utc": "2025-12-24 02:45:07",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 0.81,
      "text": "Hey everyone,\n\nI‚Äôm an ML engineer and have been trying to better understand how GenAI teams at companies actually work day to day, especially around LLM fine tuning and running these systems in production.\n\nI recently joined a team that‚Äôs beginning to explore smaller models instead of relying entirely on large LLMs, and I wanted to learn how other teams are approaching this in the real world. I‚Äôm the only GenAI guy in the entire org.\n\nI‚Äôm curious how teams handle things like training and adapting models, running experiments, evaluating changes, and deploying updates safely. A lot of what‚Äôs written online feels either very high level or very polished, so I‚Äôm more interested in what it‚Äôs really like in practice.\n\nIf you‚Äôre working on GenAI or LLM systems in production, whether as an ML engineer, ML infra or platform engineer, or MLOps engineer, I‚Äôd love to learn from your experience on a quick 15 minute call.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pucg9h/curious_how_genai_teams_llmopsmles_handle_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvq2xy6",
          "author": "Impossible-Pea-9260",
          "text": "I‚Äôm trying to make friends and learn things and be better about it. Just a preface. When I began two months ago, looking into all this stuff cause I felt like finally the LLMs could do coding good enough and I wanted to see if I could do some crazy shit. I immediately identified that small models have power for the community and the user base and it‚Äôs particular the efficacy of small models compared to larger models is not large enough of a gap to not try to optimize smaller models and once I learned more about LLM‚Äòs, I decided that we needed to do geographical mapping because this really is a physical dimension of thought, or comparatively a physical realization of semantics. This is to map out - and the site currently has mock data but my idea branches out once you start trying to design an experiment. And I think it‚Äôs important to point out. This is PHI2 and PHI3 is just as relevant in different capacities. It‚Äôs built a little bit differently than PHI2 but the weights come from PHI2 so the experimental inference that is needed is large and massive, but the relevancy of the inference that we can attain eventually is going to be Paramount in my opinion and possibly even a paradigm shift.  Anyway - another idea, I have involves using models to train models, smaller than them by cross referencing outputs, but we can‚Äôt really do that until we understand how any model and I just chose PHI2, works on the inside. https://philab.technopoets.net/ && https://github.com/Everplay-Tech/PHILAB",
          "score": 1,
          "created_utc": "2025-12-24 14:48:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvuy2dq",
          "author": "Ok-LLama-Ok",
          "text": "There are quite a few options available now, and if you're training SLMs then a single GPU and [unsloth.ai](http://unsloth.ai) will cover your needs really well.The other consideration is data and more specifically data that will train what you want from the model. We need to do a lot of tool calling as we have internal tools that are quite parameter heavy and unique to our architecture, and DeepFabric has been really a game changer here. You outline your Tools / parameters, seed a root topic and it creates reasoning datasets with many examples of Tools being called by the model. This dataset then gets pushed to hugging face and we pull it down into a training run and do supervised fine tuning with unsloth. At the end we then evaluate how well the training went by using DeepFabric to perform an evaluation - for this we split off a small amount of the dataset to give the model a blind evaluation. Everything gets logged to wand , we then push the model to huggingface where we then deploy it to AWS. \n\n  \n[https://github.com/always-further/deepfabric](https://github.com/always-further/deepfabric) \n\n[https://docs.unsloth.ai/get-started/unsloth-notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks)",
          "score": 1,
          "created_utc": "2025-12-25 11:55:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvvm58a",
          "author": "East_Ad_5801",
          "text": "You need good logging/tracking/error reporting as a first class citizen, when you run the Ratchet i.e incremental LoRa, you pass those errors to your LLM and have them integrate changes, generate/download new training data.  I wouldn't do a full fine tune unless I'm adding 2k plus data points",
          "score": 1,
          "created_utc": "2025-12-25 15:06:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pu2m2v",
      "title": "Created a branched narrative with visual storytelling with OpenAI APIs",
      "subreddit": "LLMDevs",
      "url": "https://vinejam.app/",
      "author": "yashgarg_tech",
      "created_utc": "2025-12-23 19:22:23",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pu2m2v/created_a_branched_narrative_with_visual/",
      "domain": "vinejam.app",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pz2h8x",
      "title": "Evaluation Framework for LLM applications in Java",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pz2h8x/evaluation_framework_for_llm_applications_in_java/",
      "author": "Ok-Engineer9508",
      "created_utc": "2025-12-29 23:59:35",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I'm building Dokimos - a completely free and open-source LLM evaluation framework for Java that helps you validate LLM outputs of AI assistants and agents with structured assertions in a test-driven way.\n\nHow it works:  \n\\- Write assertions for LLM outputs with built-in or custom evaluators  \n\\- Run tests against any LLM implementation or provider  \n\\- View results in a web UI  \n\\- Tests are reusable\n\nMy Open-Source implementation:  \n\\- Multi-framework support / Framework-agnostic: JUnit 5, LangChain4j, Spring AI  \n\\-  Built-in evaluators or custom-evaluators   \n\\- Web UI for experiment results and history  \n\\- Works with local LLMs and proprietary models  \n\\- Docker deployment of server implementation  \n  \nGet started:  \n\\- GitHub:  [https://github.com/dokimos-dev/dokimos](https://github.com/dokimos-dev/dokimos)  \n\\- Documentation: [https://dokimos.dev/overview](https://dokimos.dev/overview)\n\nThe project is still new, and I'm actively working on it and improving it based on feedback. Star [the repo](https://github.com/dokimos-dev/dokimos) to stay updated! ‚≠êÔ∏è",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pz2h8x/evaluation_framework_for_llm_applications_in_java/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pym1kg",
      "title": "How would you build a RAG system over a large codebase",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pym1kg/how_would_you_build_a_rag_system_over_a_large/",
      "author": "Creepy_Page566",
      "created_utc": "2025-12-29 13:16:55",
      "score": 3,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I want to build a tool that helps automate IT support in companies by using a multi-agent system. The tool takes a ticket number related to an incident in a project, then multiple agents with different roles (backend developer, frontend developer, team lead, etc.) analyze the issue together and provide insights such as what needs to be done, how long it might take, and which technologies or tools are required.\n\nTo make this work, the system needs a RAG pipeline that can analyze the ticket and retrieve relevant information directly from the project‚Äôs codebase. While I have experience building RAG systems for PDF documents, I‚Äôm unsure how to adapt this approach to source code, especially in terms of code-specific chunking, embeddings, and intelligent file selection similar to how tools like GitHub Copilot determine which files are relevant.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pym1kg/how_would_you_build_a_rag_system_over_a_large/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwjr05f",
          "author": "OnyxProyectoUno",
          "text": "Code RAG is tricky because the chunking strategy completely changes what your agents can see. Functions get split mid-implementation, class definitions get separated from their methods, import statements lose context. Most people chunk by line count or file size and wonder why their retrieval misses obvious dependencies.\n\nThe file selection problem is harder than it looks. You need to understand call graphs, dependency trees, and which files actually relate to the error patterns in the ticket. If you're chunking a React component but missing the hook it depends on, your agents are working with incomplete information.\n\nWhat kills me is how many teams discover their code chunking is broken only after they've already embedded everything. You can't see what went wrong until you're deep into agent conversations getting weird responses about missing imports or incomplete function signatures. I ended up building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_c) to debug this stuff before it hits the vector store because debugging RAG without seeing your processed code chunks is like coding blindfolded.\n\nWhat's your current thinking on handling cross-file dependencies? Are you planning to chunk at the function level or preserve larger logical units?",
          "score": 2,
          "created_utc": "2025-12-29 14:20:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjv4ni",
              "author": "Creepy_Page566",
              "text": "Yes, that's a great idea that's why i posted this because I felt something is wrong üòÖ, I will look into the tool but the url doesn't seem to work, also is there other function than debugging the embeddings",
              "score": 1,
              "created_utc": "2025-12-29 14:43:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwut737",
                  "author": "hrishikamath",
                  "text": "It‚Äôs an ad mate, you aren‚Äôt going to get a response. Check out greptiles engineering blog. I remember them mentioning they turn codebase into natural language descriptions and do rag on that.",
                  "score": 1,
                  "created_utc": "2025-12-31 03:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwl0rzh",
          "author": "crustyeng",
          "text": "Why would you not just let the agent explore the code directly?  With regular file system tools?",
          "score": 1,
          "created_utc": "2025-12-29 18:04:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwovdt9",
          "author": "arakevonian",
          "text": "saw this over at r/mcp might be useful to ya [https://www.reddit.com/r/mcp/comments/1px5ty7/3\\_months\\_update\\_codegraphcontext\\_is\\_now\\_real/](https://www.reddit.com/r/mcp/comments/1px5ty7/3_months_update_codegraphcontext_is_now_real/)",
          "score": 1,
          "created_utc": "2025-12-30 06:48:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyesj4",
      "title": "Why does LLama 3.1 give long textbook style answer for simple definition questions?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pyesj4/why_does_llama_31_give_long_textbook_style_answer/",
      "author": "Dizzy-Watercress-744",
      "created_utc": "2025-12-29 06:21:48",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "I am using Llama3.1-8b-Instruct inferenced via vllm for my course assistant.  \nWhen I ask a question in simple language, for instance\n\n>what is sunrise and sunset?\n\nI get correct answer\n\nBut if I ask the same question in different format\n\n>what is sunrise, sunset?\n\nI get a huge para that has little relevance to the query.\n\nWhat can I do to rectify this",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pyesj4/why_does_llama_31_give_long_textbook_style_answer/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwq8cb9",
          "author": "justkickinit10210",
          "text": "What are you running it on? Llama.cpp you can cap the output with predict settings. Not sure about studio.\n\nOr train it giving shorter answers. Accidentally did this in a qwen3, but worked great.",
          "score": 1,
          "created_utc": "2025-12-30 13:39:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxchmt",
      "title": "How to improve my LLM / Rag skills for optimized chatbots?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pxchmt/how_to_improve_my_llm_rag_skills_for_optimized/",
      "author": "Spac3M0nk3yy",
      "created_utc": "2025-12-27 23:57:31",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hi,\n\nI'm an experienced developer with around 12 years professional experience in various businesses.\n\nCurrently I am working on a side project to learn more about RAG /LLms / AI development in general. Last time I did AI development was around 2018 when I was working with Dialogflow / categorization.\n\nI am a .NET developer, so by habit I went for Azure, and the OpenAI SDK with Azure Foundry.\n\nTo learn more about it, I went with an idea to build a \"recipe maker\" bot. The idea is that a user should be able to write \"I want to bake a chocolate cake\" / \"I want to make lassagna\" etc.\n\nI also created my vector database with various categories (diary/meat/pasta) and so on.\n\nCurrently what I got working is that my LLM takes the user input ,and returns generic ingredients required to cook whatever the user asks for. The LLM might return \"eggs / diary / flour\" etc as a strict JSON schema, and I can do a vector search towards my DB to get the required categories, and items.\n\nBut what I am struggeling with is if the user asks like \"It will be for 12 persons\", or \"I already have milk, please remove it\".\n\nWhat would be the best way to do some adjustments / calculation based on this kind of user input? Would that be in a system prompt, the JSON schema?\n\nCurrently I am saving a DTO with an initial \"RecipeState\" that I always feed as context to subsequent queries by the user, but I cannot really grasp how I should make this kind of logic, and where it belongs.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pxchmt/how_to_improve_my_llm_rag_skills_for_optimized/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nwb7tyq",
          "author": "robogame_dev",
          "text": "Just prompt the AI:\n‚ÄúHere‚Äôs the current ingredients json, and the user‚Äôs request: <user request> - apply the user‚Äôs request and output the ingredients json with any changes as are appropriate‚Äù",
          "score": 2,
          "created_utc": "2025-12-28 04:22:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwemrz3",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": -2,
              "created_utc": "2025-12-28 18:45:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwf69je",
                  "author": "robogame_dev",
                  "text": "Obvious bot u/Mikasa0xdev\n\nhttps://preview.redd.it/aq16vdfl60ag1.jpeg?width=1179&format=pjpg&auto=webp&s=f73b6a2ca225f2f29d067b2f032c56b344c68c5a",
                  "score": 1,
                  "created_utc": "2025-12-28 20:17:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwg5yhx",
          "author": "OnyxProyectoUno",
          "text": "The state management piece you're hitting is where most recipe bots get messy. Your RecipeState DTO approach is on the right track, but the logic placement matters.\n\nFor adjustments like \"for 12 persons\" or \"remove milk\", I'd handle this in your application layer, not the system prompt. The LLM should identify the intent and extract the parameters (serving size: 12, exclude: [\"milk\"]), then your code does the math and filtering. System prompts get unwieldy when you're trying to make them do calculations and state mutations.\n\nYour JSON schema should include fields for these modifiers. Something like `servingSize`, `excludeIngredients`, `includeIngredients`. Then your application logic scales quantities and filters the vector search results before returning to the user.\n\nThe RecipeState becomes your source of truth. Each user message updates it, and you always pass the current state when generating new responses. This keeps the conversation coherent without forcing the LLM to remember everything.\n\nWhere are you storing the RecipeState between messages? If it's just in memory, you'll lose context when the session ends. Most people end up persisting it to maintain longer conversations.",
          "score": 1,
          "created_utc": "2025-12-28 23:17:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pw7fkb",
      "title": "I let an AI agent run my customer gift campaign",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pw7fkb/i_let_an_ai_agent_run_my_customer_gift_campaign/",
      "author": "anitakirkovska",
      "created_utc": "2025-12-26 15:51:59",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I recently started an interesting holiday challenge: I'm going to be building AI agents for the next 30 days. The first agent I built was a¬†[secret santa gift generator](https://agentyard.co/secret-ai-santa), but today I want to share my next one:¬†[The Swag agent.](https://agentyard.co/customer-gifts)¬†üëöüéí\n\nThis one in particular was inspired from our team who was handing our customer holiday gift campaign. We were trying to build custom storefronts with our swag, and ways on how to make it personal for our customers. We realized that AI agents could be the perfect thing to use for this.\n\n(i) it's going to be a fun experiment to see how these agents will work for this task  \n(ii) it'll make for an interesting customer experience \n\nWatch my¬†[somewhat short demo](https://youtu.be/hWsIBFvxbco)¬†(\\~3min) for all the details on how I built this one.\n\nBut generally here's how it works:\n\n\\- Customer adds their hobbies/passion + shipping info  \n\\- Agent decides what to get them  \n\\- Agent designs a custom swag option  \n\\- Agent creates draft order and shipping details  \n\\- Human reviews the order\n\nThe agent workflow has access to 6 different tools, but it generally has a pretty simple architecture.\n\nPS: Not a promo. These are free demo agents I‚Äôm building to explore better patterns and techniques. You can run them yourself and learn from how they‚Äôre put together.",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pw7fkb/i_let_an_ai_agent_run_my_customer_gift_campaign/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pvb7tz",
      "title": "Ai avatar",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pvb7tz/ai_avatar/",
      "author": "agentic_coder7",
      "created_utc": "2025-12-25 11:09:59",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "For past 2 days ago, i started working on a project ai avatar lip-sync video creation, i cloned voice using TTS and for head and human like facial expression I  used SadTalker. But for hand gestures, I tried a lot of model sdx-turbo, it did not work. also I don't have enough gpu and storage so I can't run WAN2.2 /LongCat models they required atleast 24GB vram and I don't have that, I use kaggle and colab for my model related stuff. IF anyone knows how to make realistic ai avatar like heygen level quality then reply me. and if anyone knows small model which fits on my project then also reply me.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pvb7tz/ai_avatar/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvvmdvi",
          "author": "Avtrkrb",
          "text": "I've worked on something similar. DM me so we can connect",
          "score": 1,
          "created_utc": "2025-12-25 15:08:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nvyopw8",
          "author": "Lost-Bathroom-2060",
          "text": "I build avatar will nano banana pro‚Ä¶ and for video like cinematic you can consider Sora.. a lot of ai video testers are available too.. probably you can help test it out ..",
          "score": 1,
          "created_utc": "2025-12-26 02:34:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nweguyg",
          "author": "Kml777",
          "text": "Have you tried Tagshop AI, as this tool allows you to generate high-quality and realistic AI avatar lip-sync videos?  You can generate AI ads with text-to-speech, URL to video, and image to video. For quick, realistic avatar videos without dealing with heavy models or GPU limits, this tool works best.",
          "score": 1,
          "created_utc": "2025-12-28 18:17:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pv9s7p",
      "title": "I created interactive buttons for chatbots",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1pv9s7p",
      "author": "CrazyGeek7",
      "created_utc": "2025-12-25 09:31:45",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pv9s7p/i_created_interactive_buttons_for_chatbots/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pu523g",
      "title": "Try This if you are Interested in LLM Hacking",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1pu523g/try_this_if_you_are_interested_in_llm_hacking/",
      "author": "CIRRUS_IPFS",
      "created_utc": "2025-12-23 21:03:56",
      "score": 2,
      "num_comments": 5,
      "upvote_ratio": 0.67,
      "text": "There‚Äôs a **CTF-style app** where users can interact with and attempt to break **pre-built GenAI and agentic AI systems**.\n\nEach challenge is set up as a ‚Äúbox‚Äù that behaves like a realistic AI setup. The idea is to explore failure modes using techniques such as:\n\n* prompt injection\n* jailbreaks\n* manipulating agent logic\n\nUsers start with **35 credits**, and each message costs **1 credit**, which allows for controlled experimentation.\n\nAt the moment, most boxes focus on **prompt injection**, with additional challenges being developed to cover other GenAI attack patterns.\n\nIt‚Äôs essentially a hands-on way to understand how these systems behave under adversarial input.\n\nLink: [HackAI](https://hackai.lol)",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1pu523g/try_this_if_you_are_interested_in_llm_hacking/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "nvngauj",
          "author": "Lazer_7673",
          "text": "So what actually it does?",
          "score": 3,
          "created_utc": "2025-12-24 02:20:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nvni0l6",
              "author": "CIRRUS_IPFS",
              "text": "there are ways to attack LLMs system prompt and execute wrong function which will become malicious in real world. So, i have created a simulation where you can talk to these bots and try to crack the AI. Once you cracked you will get a FLAG{<secret>} and you need to submit it to collect rewards...",
              "score": 1,
              "created_utc": "2025-12-24 02:31:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nvnjwiv",
                  "author": "Lazer_7673",
                  "text": "Where to submit?",
                  "score": 1,
                  "created_utc": "2025-12-24 02:42:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}