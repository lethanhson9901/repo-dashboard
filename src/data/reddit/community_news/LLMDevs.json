{
  "metadata": {
    "last_updated": "2026-02-13 03:16:08",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 89,
    "file_size_bytes": 128325
  },
  "items": [
    {
      "id": "1qxmeee",
      "title": "I built RAG for 10K+ NASA docs (1950s‚Äìpresent) in 2 weeks: VLMs for complex tables, diagrams & formulas, 657K+ pages on a single H100, live-streamed full build.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qxmeee/i_built_rag_for_10k_nasa_docs_1950spresent_in_2/",
      "author": "Low_Acanthisitta7686",
      "created_utc": "2026-02-06 16:32:09",
      "score": 253,
      "num_comments": 44,
      "upvote_ratio": 0.95,
      "text": "**TL;DR:** I designed and built a full RAG system over 10,000 NASA technical documents spanning the 1950s to 2025 ‚Äî we're talking scanned typewriter reports, handwritten notes, propulsion diagrams, mathematical formulas, failure investigations. Off-the-shelf tools broke down fast. I ended up building a custom pipeline using Qwen3-VL-8B to process what traditional OCR and parsers couldn't handle, ran the whole thing on a single H100 (657,000+ pages, \\~180 pages/min), and built an agentic retrieval system that doesn't just search ‚Äî it investigates like a domain expert. The architecture is designed to scale to 100K+ documents. Everything was live-streamed (140+ hours across 15 streams), and the GitHub repo for the document processing pipeline and infra is coming soon.\n\nHey everyone, I'm Raj. Over the last 2 weeks, I live-streamed building what turned out to be the most technically challenging project I've taken on ‚Äî and I wanted to share the experience while it's fresh. This is a long one, I tried to keep it short, but there was too much that I think is genuinely useful to cut.\n\n# The Domain\n\nSo here's the scenario I designed for this project ‚Äî a fictional aerospace consultancy called \"Meridian Aerospace,\" modeled on very real challenges these companies face.\n\n85,000+ documents accumulated over 70+ years ‚Äî real documents from NASA's Technical Reports Server (NTRS). Propulsion test reports, failure investigations, component specs, regulatory filings. Engineers spending 4-6 hours per project digging through archives. A missed critical failure mode last quarter because the relevant data was buried in a 1997 test report nobody knew existed.\n\nNow here's what makes these documents painful:\n\n* 1950s‚Äì1990s scanned reports ‚Äî photocopied, faxed, re-scanned, degraded quality\n* Dense technical diagrams everywhere: thrust curves, propulsion schematics, thermal analysis charts\n* Mathematical formulas and engineering equations scattered throughout\n* Domain-specific acronyms (Isp, TWR, LOX, MMH, NTO) that are often never expanded in the text\n* Cross-references between documents ‚Äî failure reports cite original test data, compliance docs reference design specs\n* Tables spanning multiple pages with nested sub-headers\n\nI used 10,000 documents from NASA's Technical Reports Server as the working dataset, with the architecture designed from day one to handle the full 85K+ and beyond.\n\n# What I Built\n\nI'll walk through the three main layers, but I want to be clear ‚Äî these aren't independent pieces you build one after another. They feed into each other constantly. Decisions in the document processing layer directly shaped how the agent works, and understanding how engineers actually think (the agent layer) changed how I approached extraction. It's all connected.\n\n# The Document Processing Pipeline\n\nThis is where a huge chunk of the work lived, and honestly where most people underestimate the difficulty. The core realization: **you cannot build good retrieval over bad extractions.** If your chunked text is garbage, no embedding model or re-ranker is going to save you.\n\nI used Docling (from IBM, I know it has a ton of issues ‚Äî I found workarounds and solved them too) for layout detection ‚Äî figuring out where tables, figures, formulas, and text blocks sit on each page. Then Qwen3-VL-8B to actually interpret what's in those regions.\n\nA few of the harder problems:\n\n**Formula association:** Docling detects formulas fine, but they lose their position in the document flow. So you get a formula floating at the end of a page with no connection to the paragraph it belongs to. I built a system that paints colored bounding boxes with ID numbers directly onto page screenshots, then asks the VLM \"where does Formula 7 belong relative to these numbered paragraphs?\" Sounds weird, works surprisingly well. Gives you reading-order accuracy without re-OCRing anything.\n\n**Complex tables:** These were probably the single most painful thing to solve. We're talking massive grids ‚Äî 72 columns by 50 rows of stability data ‚Äî where position determines meaning. Down arrows mean \"carry this value down.\" Brackets group five rows under \"Unstable.\" Zebra lines and grid lines guide the human eye across dense numbers. Standard OCR reads left-to-right, top-to-bottom and has no idea what to do with any of this. Parsers treat the grid lines as noise or lose alignment if the scan is slightly tilted.\n\nI went through a lot of approaches. Standard markdown extraction lost alignment. CV-based heatmaps and projection lines to detect rows ‚Äî worked about 80% but too brittle for production. JSON output from the VLM broke constantly on large tables (missed closing brackets). Small models (7B) hallucinated numbers and missed columns entirely.\n\nWhat actually worked was treating the table as a photograph of data rather than a stream of text. Use Docling purely for finding the bounding box coordinates, crop the original high-res page image (no downscaling ‚Äî that destroys data in dense tables), and send the full-resolution crop to a large VLM. You need 72B+ to hold context across a 30-column table without losing track.\n\nTwo tricks that made a real difference. First, for tables with zebra lines or warped scans, I pre-process the image by drawing red horizontal lines onto it before sending to the VLM ‚Äî basically a \"digital ruler\" that forces the model to keep row alignment. Second, the prompt strategy ‚Äî instead of asking for just structured output, I ask for markdown (way more robust than JSON for grid data) plus a \"notes\" field where the model captures visual shorthand. \"If there's a down arrow, note the value is carried down. If there's a bracket, note the grouping.\" The model successfully returned \"unstable\" for rows that didn't explicitly have the text but were visually grouped under an \"Unstable\" bracket.\n\nFor the truly dense tables that still needed more work, I have a fallback that generates a detailed description and serves the raw image alongside it ‚Äî which honestly, in aerospace, engineers prefer anyway over a potentially wrong structured output. But this isn't a dead end. The digital ruler approach and the prompt strategy were working well, and with more time I think there's a solid solution there. I was time-boxed to 2 weeks for this entire project, so I made the pragmatic call to move on. Might revisit this specifically and share if I make a breakthrough.\n\n**Legacy scan quality:** Documents from the 1960s have noise, \"Confidential\" stamps, hole punches, scan artifacts ‚Äî and models happily pick all of these up as \"figures.\" Added a classification step asking the VLM: \"Is this a technical diagram or just a document artifact?\" Simple, but it cleaned up a lot of noise.\n\n**The full-page strategy:** I initially tried cropping individual formulas to save tokens. Docling's format detection models missed about 60% of small formulas in dense pages. So I pivoted ‚Äî if any formula is detected on a page, send the entire page screenshot to the VLM and let it transcribe everything in reading order. More expensive per page (didn't matter as I deployed on a GPU), but the accuracy difference is massive. In this domain, a missed variable isn't a minor bug.\n\n**On OCR,** I didn't actually need traditional OCR for most of the heavy lifting. The figures, tables, and formulas ‚Äî which are the hardest parts of these documents ‚Äî were all handled by the VLM pipeline. OCR was only needed as a fallback for pages where the embedded text layer was missing or corrupted. So the approach became: use native text extraction where available, VLM for all the visual/structured content, and OCR only when truly needed. Disabling forced OCR where it wasn't necessary cut processing time significantly.\n\n# H100 Infrastructure & Scaling\n\nProcessing 10K documents ‚Äî roughly 657,000+ pages ‚Äî on a single H100 was its own adventure.\n\n**Where it started:** My first attempt was basically a monolithic script. Every worker loaded the PDF, loaded the model onto the GPU, ran inference, unloaded. Workers were fighting each other for GPU memory, CPU, RAM. Everything was crashing. Back-of-the-napkin math said this approach would take somewhere around 28 days for the full dataset. Obviously not going to work.\n\n**The rewrite:** I moved to a proper service-oriented architecture. Separated the CPU-heavy work (Docling parsing, chunking, text extraction) from the GPU-heavy work (VLM inference). Stateless Celery workers handle the CPU side, feeding requests to a persistent vLLM server that does nothing but inference. Redis as the message broker. Took some inspiration from how production ML systems handle millions of requests with limited compute ‚Äî keep your inference engine as a persistent service, don't have each worker spin it up and tear it down.\n\nThat alone brought the estimate down to maybe 5-9 days. Still not great.\n\n**Then the tuning started.** FP8 quantization because running standard GGUF/Ollama on an H100 is wasting the hardware ‚Äî FP8 is specifically optimized for Hopper. Concurrency tuning: tested 6, 8, 9, 10 Docling workers. 9 caused instant OOM. 10 saturated the queue. 6 underutilized the GPU. 8 was the sweet spot. Dynamic image scaling for oversized PDFs ‚Äî some scans were 170MB, crashing workers during bitmap conversion. VRAM memory leak management ‚Äî usage would creep up batch after batch until it crashed, so I added explicit garbage collection between cycles.\n\n**End result:** \\~2.5 days, running at about 180 pages per minute. From 28 days to 2.5 days on the same hardware, just by thinking about architecture and resource management. Again, could have done better, but was on a time crunch.\n\n# The Agent & Retrieval Layer\n\nThis part tends to get underestimated. Building the agent wasn't just \"wire up some tools to an LLM and write a system prompt.\" A huge amount of time went into two things: understanding the people who would actually use this system, and shaping how the agent itself thinks.\n\nI spent a lot of time with Claude role-playing as different engineer personas ‚Äî a cautious senior engineer (\"Sandra\") approaching retirement who's seen things go wrong, a junior engineer who searches too narrowly. I was trying to understand: how does their day actually work? How do they use current traditional systems? What's literally going through their mind when they're investigating a failure mode? What are they worried about that they won't say out loud?\n\nThat process shaped everything about the agent. For example ‚Äî engineers don't just look for failure cases. They specifically look for *success cases* as counter-evidence to validate risky designs. A standard RAG setup completely misses that nuance. Or the fact that a \"question about a valve failure\" might actually be about defending a design decision in a review meeting next week. The agent needs to understand the situation behind the question.\n\nThat understanding fed directly into how I designed the agent's reasoning. One of the bigger realizations was that spiking domain intuition in the system prompt often outperforms complex retrieval engineering. Instead of hardcoding examples, I focused on making the agent think like a propulsion engineer. It should be low-opinionated and already have hypotheses before it runs a single search. When someone mentions a pressure value, it should have intuition about whether that's nominal or concerning. When it finds a document, it should reason about what it means, not just return it. It's not a search tool ‚Äî it's a reasoning engine with engineering expertise that uses search as one of its tools. And honestly, this is still just at the system prompt level ‚Äî keeping it low-opinionated, letting the model lean on its own domain knowledge rather than constraining it ‚Äî but it brings absolute wonders to how the system behaves.\n\nWhat came out of all that work:\n\nThe agent doesn't just search ‚Äî it investigates. It maintains a working task list and notes, forms hypotheses based on its domain intuition before it even touches the search tool, and updates its understanding as it learns. When a question branches, it spawns sub-agents for parallel research threads. It can navigate ‚Äî read adjacent chunks, follow cross-references between documents, pull threads across decades of reports.\n\nWhen the text extraction is uncertain ‚Äî and on 1950s docs, it will be ‚Äî the agent can request a screenshot of the actual PDF page region to visually verify what it's reading. That \"visual region\" tool ended up being one of the most important things in the whole system. It's the bridge between \"95% OCR accuracy\" and \"actually trustworthy in aerospace.\"\n\nI also integrated the NASA Thesaurus ‚Äî 18K aerospace terms filtered down to 3.5K propulsion-relevant concepts ‚Äî so the system handles query expansion properly. \"LOX\" matches \"Liquid Oxygen,\" \"2000 PSI\" finds results mentioning \"13.9 MPa.\" Without this, you're relying on exact keyword matches in a domain where everyone uses different terminology for the same thing.\n\nAnd time-boxed search ‚Äî engineers ask things like \"what do we know about cryogenic engine failures between 1970 and 1980?\" Filtering by time period before semantic search cuts the search space dramatically. When I tested this, the agent successfully traced the 50-year evolution of cryogenic systems ‚Äî from passive insulation in the 1970s to active cryo-coolers in the 2020s ‚Äî without any deep research mode. Just proper filtering and good retrieval.\n\n# What's Coming Next\n\nI've linked all the YouTube streams in the comments below ‚Äî 15 streams, some of them are 11+ hours long, so obviously that's a lot to sit through. To make things more digestible and actually useful, I'm going to be posting specific problem/solution breakdowns over the next few days, including how I evaluated the system with 10K docs. Each of these topics was genuinely its own nightmare to solve, and I think the details will be helpful for anyone working on similar problems.\n\nI'm also hoping to open-source the document processing pipeline and infrastructure code on GitHub soon, which I think will be genuinely useful for anyone dealing with large-scale document processing ‚Äî whether it's aerospace or not.\n\nOne last thing ‚Äî I genuinely want to thank the team behind Claude Code. Being honest, a project like this would realistically take a team of 3-4 engineers working 3-4 months. The document processing pipeline alone, the infrastructure, the agent design, the frontend, evaluation ‚Äî each of these is a serious body of work. I did it solo in 2 weeks, live on stream, and that would not have been possible without Claude Code, it was in the loop for pretty much all of it. Seriously, thank you to the engineers behind it.\n\nHappy to answer questions, and if you've dealt with similar problems ‚Äî legacy docs, domain-specific retrieval, scaling document processing ‚Äî I'd love to hear what you ran into.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qxmeee/i_built_rag_for_10k_nasa_docs_1950spresent_in_2/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3xvwcn",
          "author": "ksk99",
          "text": "Any evaluation metrixs u used, any subset of data set along with queries and results we can use to recreate/learn",
          "score": 6,
          "created_utc": "2026-02-06 18:02:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xxjlq",
              "author": "Low_Acanthisitta7686",
              "text": "Yeah! So in the last stream (Part 14) I went through evaluation. Basically, I loaded co-related full docs into Gemini (1M context window) and had it generate complex questions that tested different aspects of the agent ‚Äî things like cross-document reasoning, degraded documents, negative/boundary tests, and more.   \n  \nThen I evaluated and tested whether the system could find the right set of docs within the 10K-doc space.   \n  \nI felt like this gave me a solid initial test suite to run, which would probably end up being a pretty strong evaluation as well. Sure, I‚Äôll share the dataset and pipeline with the repo soon.",
              "score": 5,
              "created_utc": "2026-02-06 18:09:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xniw6",
          "author": "vladimirxi",
          "text": "Nuts.  Well done on the documentation!  Crazy project.  Well thought out.",
          "score": 9,
          "created_utc": "2026-02-06 17:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xpjs5",
              "author": "Low_Acanthisitta7686",
              "text": "Thank you!",
              "score": 4,
              "created_utc": "2026-02-06 17:31:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xd6ic",
          "author": "Low_Acanthisitta7686",
          "text": "Check the live streams here (15 streams, 140+ hours): [https://www.youtube.com/@rajsuthanofficial7585/streams](https://www.youtube.com/@rajsuthanofficial7585/streams)",
          "score": 7,
          "created_utc": "2026-02-06 16:33:17",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3xz1qw",
              "author": "flaxseedyup",
              "text": "You absolute gem of a man. I will definitely be looking at this. Thank you so much for sharing!    \n    \nWhere did you learn the majority of the core skills needed for such a project?    \n    \nDo you rate the ‚ÄúThe AI Automators‚Äù who are on YouTube?",
              "score": 6,
              "created_utc": "2026-02-06 18:17:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3y00x7",
                  "author": "Low_Acanthisitta7686",
                  "text": "haha thanks ‚ù§Ô∏è\n\nI pretty much learned by building stuff. I was working on my startup and also worked with a few enterprises along the way, but mostly just learned by doing random complex projects!",
                  "score": 1,
                  "created_utc": "2026-02-06 18:21:35",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3yeh4z",
          "author": "makinggrace",
          "text": "This was a beast of a project. Love it.",
          "score": 2,
          "created_utc": "2026-02-06 19:30:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yfh68",
              "author": "Low_Acanthisitta7686",
              "text": "for sure.... thanks!",
              "score": 1,
              "created_utc": "2026-02-06 19:35:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zizu5",
          "author": "tehsilentwarrior",
          "text": "Cool stuff. Will be useful in playing Kerbal üòÇ\n\nNo but, impressive stuff. Hope to see it soon!",
          "score": 2,
          "created_utc": "2026-02-06 22:54:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40q0gm",
              "author": "Low_Acanthisitta7686",
              "text": "appreciate it mate!",
              "score": 1,
              "created_utc": "2026-02-07 03:12:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zr9gx",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 2,
          "created_utc": "2026-02-06 23:41:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40q9jt",
              "author": "Low_Acanthisitta7686",
              "text": "thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-07 03:13:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40685c",
          "author": "Legitimate-Leek4235",
          "text": "I wonder if we give this prompt to opus 4.1 and all the agents which built the compiler, will it be able to replicate this",
          "score": 2,
          "created_utc": "2026-02-07 01:09:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40qtfh",
              "author": "Low_Acanthisitta7686",
              "text": "Nope ‚Äî not possible at all. There are so many moving pieces and decisions to make, and honestly it takes a ton of thinking, trial, and error to get to this point. Check my YouTube streams ‚Äî for around 3 days (each 8+ hours) I was banging my head building the infra, and then we completely rewrote the entire thing in a day. So it literally takes a lot of work. Luckily I‚Äôm an engineer, so it naturally 100√ó my potential.",
              "score": 1,
              "created_utc": "2026-02-07 03:17:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o40cczw",
          "author": "OnlyTimeFan",
          "text": "Thanks for sharing and spreading the knowledge",
          "score": 2,
          "created_utc": "2026-02-07 01:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40r1c8",
              "author": "Low_Acanthisitta7686",
              "text": "always!",
              "score": 2,
              "created_utc": "2026-02-07 03:18:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o415khy",
          "author": "FiddlyDink",
          "text": "This is incredible work! I‚Äôm working on a RAG project to connect an LLM to a Neo4j data store to be able to ask questions about old records and I know how challenging it can be. I‚Äôm not trying to solve half the challenges that you had to and yet I‚Äôm still struggling. I would love to learn more about this work and will definitely be checking out your streams!",
          "score": 2,
          "created_utc": "2026-02-07 04:59:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41c2p0",
              "author": "Low_Acanthisitta7686",
              "text": "‚ù§Ô∏è",
              "score": 1,
              "created_utc": "2026-02-07 05:50:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o41t0xh",
          "author": "Electrical-Paper-323",
          "text": "Excellent! Looking forward to the GitHub project üòä",
          "score": 2,
          "created_utc": "2026-02-07 08:25:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41ts01",
          "author": "Neovison_vison",
          "text": "Can you TLDR us in new since your last write up 4 months ago?",
          "score": 2,
          "created_utc": "2026-02-07 08:32:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4258z7",
              "author": "Low_Acanthisitta7686",
              "text": "Worked on a couple of projects since then, wrapped up a few, and have a few ongoing. But mostly right now I‚Äôm focused on my startup, basically building something similar: on-prem search at scale for regulated enterprises like finance, pharma, ITAR-protected companies, and more. It‚Äôs [intraplex.ai](http://intraplex.ai) if you‚Äôre interested.",
              "score": 2,
              "created_utc": "2026-02-07 10:25:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42hujq",
          "author": "redbull-hater",
          "text": "Good job man",
          "score": 2,
          "created_utc": "2026-02-07 12:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42pk2x",
          "author": "jaykeerti123",
          "text": "Great stuff. Waiting for the GitHub code.",
          "score": 2,
          "created_utc": "2026-02-07 13:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48ti6r",
          "author": "Manson_79",
          "text": "wanted to do the same for my company, but,  i was afraid for legal reasons about hallucinations,  how did you get around that?   i'm also in aviation fyi\n\n",
          "score": 2,
          "created_utc": "2026-02-08 12:51:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rxyle",
              "author": "Low_Acanthisitta7686",
              "text": "Totally valid concern, especially in aviation. I don‚Äôt treat the model as an authority at all. It‚Äôs strictly retrieval-grounded and always ties answers back to actual documents, and when things look uncertain it surfaces the original page for human verification. So it‚Äôs more of a smart document navigator than something you blindly trust.",
              "score": 1,
              "created_utc": "2026-02-11 10:17:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4bwsxb",
          "author": "Rockingtits",
          "text": "Super cool!",
          "score": 2,
          "created_utc": "2026-02-08 22:24:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fuhpt",
              "author": "Low_Acanthisitta7686",
              "text": "thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-09 14:45:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4di4d3",
          "author": "JackfruitVivid180",
          "text": "how can I get started into this ?\n\n",
          "score": 2,
          "created_utc": "2026-02-09 03:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4furqr",
              "author": "Low_Acanthisitta7686",
              "text": "I did a post on this a couple of months back, might be helpful for you - [https://www.reddit.com/r/LLMDevs/comments/1nl9oxo/i\\_built\\_rag\\_systems\\_for\\_enterprises\\_20k\\_docs/](https://www.reddit.com/r/LLMDevs/comments/1nl9oxo/i_built_rag_systems_for_enterprises_20k_docs/)",
              "score": 2,
              "created_utc": "2026-02-09 14:47:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lclmb",
          "author": "Mr_Frosty009",
          "text": "This is huge, congrats. But why RAG with this big piece of data, fine tuning would be better and faster at the end? Maybe I missed something and this is not local?",
          "score": 2,
          "created_utc": "2026-02-10 10:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ry8gn",
              "author": "Low_Acanthisitta7686",
              "text": "Fine-tuning doesn‚Äôt really inject knowledge in a reliable way, especially for research workflows where you need grounded answers. With RAG you‚Äôre always tied back to real documents, so you can push accuracy way higher and actually verify things. Fine-tuned models tend to drift and hallucinate when juggling lots of source docs. For this kind of work, I‚Äôd trust retrieval over memory every time.",
              "score": 1,
              "created_utc": "2026-02-11 10:19:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ntmkp",
          "author": "Nice_Baby_3044",
          "text": "great.what should be the infra needed for 2000 a4 size images for building agents?",
          "score": 2,
          "created_utc": "2026-02-10 18:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rycn5",
              "author": "Low_Acanthisitta7686",
              "text": "For \\~2k A4 images you don‚Äôt need anything crazy. A single decent GPU box (24‚Äì48GB VRAM) or even a solid CPU + API VLM setup is enough, since the bottleneck is mostly image parsing, not scale. Split CPU work (preprocessing/chunking) from inference, batch aggressively, and you‚Äôll be done in hours, not days. The heavy infra only really shows up when you‚Äôre pushing hundreds of thousands of pages.",
              "score": 1,
              "created_utc": "2026-02-11 10:20:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yvpnv",
          "author": "No_Wrongdoer41",
          "text": "Me and a small team are building a platform that can accomplish something comparable in a matter of hours, repeateably, on corpuses of thousands of documents. I'd love your feedback on what we're working in if you don't mind discussing it with me!",
          "score": 1,
          "created_utc": "2026-02-06 20:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ywgcm",
              "author": "Low_Acanthisitta7686",
              "text": "sure",
              "score": 1,
              "created_utc": "2026-02-06 21:00:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3yhhfr",
          "author": "satechguy",
          "text": "Which Ilm wrote such verbose text ;-)",
          "score": 2,
          "created_utc": "2026-02-06 19:45:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yitmx",
              "author": "Low_Acanthisitta7686",
              "text": "haha, I wrote it and had claude help polish the english.",
              "score": 2,
              "created_utc": "2026-02-06 19:52:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3zji2u",
                  "author": "tehsilentwarrior",
                  "text": "I write a lot too and people these days just think AI did it. \n\nI spent quite a lot of time perfecting my English so when I documented a massive project we have my boss kept asking what AI I used for that .. lol\n\nI just said <my inicials>AI, and he was hilariously confused when he couldn‚Äôt find it. I said I trained it myself over the last decades",
                  "score": 3,
                  "created_utc": "2026-02-06 22:57:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3y97qu",
          "author": "bbahner",
          "text": "This is awesome! Thanks for sharing. ",
          "score": 1,
          "created_utc": "2026-02-06 19:05:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ybujq",
              "author": "Low_Acanthisitta7686",
              "text": "Sure!",
              "score": 1,
              "created_utc": "2026-02-06 19:17:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4241bl",
          "author": "QuanstScientist",
          "text": "OMG, what an amazing process documentation. We both went through a very similar process, I‚Äôm working on a local RAG for the Epstein archive files, still in development, also using Claude, here at some details, still not in a state for public release https://boltzmannentropy.github.io/Librarius/",
          "score": 0,
          "created_utc": "2026-02-07 10:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o406bb6",
          "author": "EditorDisastrous4994",
          "text": "You should try Reseek. It's an AI second brain that handles semantic search across notes, PDFs, and web content in one place. It might be a good alternative if you want a unified system",
          "score": -1,
          "created_utc": "2026-02-07 01:09:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40r0u3",
              "author": "Low_Acanthisitta7686",
              "text": "Checked it out, but it doesn‚Äôt mention anywhere what scale it can work with, and that‚Äôs super important.",
              "score": 1,
              "created_utc": "2026-02-07 03:18:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1oa4i",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:53",
      "score": 185,
      "num_comments": 30,
      "upvote_ratio": 0.93,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) ‚Äì 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt‚Äôs trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1oa4i/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4s8hek",
          "author": "TylerDurdenFan",
          "text": ">The cleaning, chunking, and optimization challenges are exactly what excites me\n\nJust try to not get too excited around that material, mkay?",
          "score": 23,
          "created_utc": "2026-02-11 11:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sctpa",
              "author": "Cod3Conjurer",
              "text": "He he üòÇ",
              "score": -12,
              "created_utc": "2026-02-11 12:20:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4xu1u2",
                  "author": "kexxty",
                  "text": "Bro...this is not a laughing matter",
                  "score": 2,
                  "created_utc": "2026-02-12 06:27:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tf78a",
          "author": "Significant-Crow-974",
          "text": "It would be marvellous to run this over the full set of unredacted files.\nI am hoping that the FBI who have illegally redacted information do not now delete that hoard of documents.\nI hope that when they finally manage to charge Trump and the Epstein class that they will be able to utilise a tool such as this to make their prosecutions more effective.\nWell done and Thank you!",
          "score": 7,
          "created_utc": "2026-02-11 15:52:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tgqt7",
              "author": "Cod3Conjurer",
              "text": "The goal here is purely technical, building better retrieval over large unstructured datasets.  \nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.  \n",
              "score": 1,
              "created_utc": "2026-02-11 15:59:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tqxae",
                  "author": "Significant-Crow-974",
                  "text": "Yes. Fully appreciate that. Actually, I created a similar RAG for the first tranche of documents. Just as an exercise to see how a RAG could cope with so many documents.\nI would say that it was a partial success. Great insight.",
                  "score": 2,
                  "created_utc": "2026-02-11 16:46:46",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4wndcx",
                  "author": "Klutzy_Celebration80",
                  "text": "Might be interesting to see if you could have it un-redact the documents",
                  "score": 1,
                  "created_utc": "2026-02-12 01:33:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r00lr",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 7,
          "created_utc": "2026-02-11 05:12:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rdqsc",
              "author": "Cod3Conjurer",
              "text": "That's over 1gb¬†\nYou can generate using my code¬†",
              "score": 0,
              "created_utc": "2026-02-11 07:07:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sy6d6",
                  "author": "Borkato",
                  "text": "Wait 1gb is nothing when the models are like 20gb+",
                  "score": 1,
                  "created_utc": "2026-02-11 14:27:23",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4sf4yp",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -6,
                  "created_utc": "2026-02-11 12:36:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4tfnfq",
          "author": "DaRandomStoner",
          "text": "I was hoping you had the newly released documents in this... until we get these new documents processed through an OCR and into an organized data structure, we can't really go through them properly. \n\nIt would cost a good amount to process all the new documents so that we can include them in databases like this... it's all just compute costs though. DeepSeek's OCR is open-source and can run on most PCs. If a bunch of people got together we could expand databases like this to include all the newly released docs...",
          "score": 4,
          "created_utc": "2026-02-11 15:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4th0by",
              "author": "Cod3Conjurer",
              "text": "Yeah, this version doesn‚Äôt include the newly released documents yet. If those are raw scans, they‚Äôd need OCR + structured parsing before indexing.  \nThe main cost is compute and storage, not complexity.  \nA collaborative effort could definitely speed that up, especially for batching OCR and preprocessing at scale.",
              "score": 3,
              "created_utc": "2026-02-11 16:00:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4reucl",
          "author": "kondasamy",
          "text": "I think you should checkout - [https://jmail.world/jemini](https://jmail.world/jemini)",
          "score": 6,
          "created_utc": "2026-02-11 07:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rf5lj",
              "author": "Cod3Conjurer",
              "text": "Yeeha i have seen that¬†\nBut it is currently broken",
              "score": 0,
              "created_utc": "2026-02-11 07:20:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4x1h4q",
          "author": "jdsweet653",
          "text": "Great app! What did your ingestion py look like for the db?",
          "score": 2,
          "created_utc": "2026-02-12 02:57:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xjmq3",
              "author": "Cod3Conjurer",
              "text": "The ingestion was pretty simple, I loaded the cleaned JSON, chunked it (400 size, 80 overlap), deduped chunks using SHA-256 hashing, generated MiniLM embeddings, and upserted everything into ChromaDB with source metadata.",
              "score": 2,
              "created_utc": "2026-02-12 05:01:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yp6ur",
                  "author": "DanRan88",
                  "text": "Any idea on the size of the DB?¬†",
                  "score": 2,
                  "created_utc": "2026-02-12 11:22:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xgy3s",
          "author": "Big3gg",
          "text": "See if it knows how to make jerky",
          "score": 2,
          "created_utc": "2026-02-12 04:41:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xj134",
              "author": "Cod3Conjurer",
              "text": "he he ü§£",
              "score": 0,
              "created_utc": "2026-02-12 04:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4y92qs",
          "author": "DarKresnik",
          "text": "Amazing job. Now we need someone to \"find\" those 3m missing documents.",
          "score": 2,
          "created_utc": "2026-02-12 08:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zqs1i",
              "author": "Cod3Conjurer",
              "text": "Guess I‚Äôll have to OCR the entire publicly available dataset myself now, jokingü§£",
              "score": 1,
              "created_utc": "2026-02-12 15:14:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t10wl",
          "author": "StackSmashRepeat",
          "text": "So, have you come to terms with RAG being a dead end as far as real recall of memory works? Or are you just chunking and overlapping to a ridiculous point? I really don't think this is a sensible use of RAG. The LLM will at some point start hallucinating missing pieces from thin air, making this tool fairly unreliable for accuracy. \n\nPeople looking into these files need absolute accuracy.",
          "score": 2,
          "created_utc": "2026-02-11 14:42:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tfjmw",
              "author": "Cod3Conjurer",
              "text": "You‚Äôre right that absolute accuracy matters. That‚Äôs why this should be treated as an assistive search layer, not a final source of truth.\n\nAt the end of the day, it‚Äôs an engineering experiment, not a legal authority.",
              "score": 7,
              "created_utc": "2026-02-11 15:53:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4tikha",
                  "author": "StackSmashRepeat",
                  "text": "Make it do an online search after it retrieves data from rag and provide a link directly to an online source. End users are dumb and some will believe anything the llm tells them.",
                  "score": 0,
                  "created_utc": "2026-02-11 16:08:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o538mx9",
          "author": "HarjjotSinghh",
          "text": "2m pages = my new love language.",
          "score": 1,
          "created_utc": "2026-02-13 01:44:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1jr22",
      "title": "I'm super unemployed and have too much time so I built an open source SDK to build event-driven, distributed agents on Kafka",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r1jr22/im_super_unemployed_and_have_too_much_time_so_i/",
      "author": "orange-cola",
      "created_utc": "2026-02-11 01:30:26",
      "score": 23,
      "num_comments": 11,
      "upvote_ratio": 0.75,
      "text": "I finally got around to building this SDK for event-driven agents. It's an idea I've been sitting on for a while. I finally started working on it and it's been super fun to develop.\n\nI made the SDK in order to decompose agents into independent, separate microservices (LLM inference, tools, and routing) that communicate asynchronously through Kafka. This way, agents, tool services, and downstream consumers all communicate asynchronously and can be deployed, adapted, and scaled completely independently.\n\nThe event-driven structure also makes connecting up and orchestrating multi-agent teams trivial. Although this functionality isn't yet implemented, I'll probably develop it soon (assuming I stay unemployed and continue to have free time on my hands).\n\nCheck it out and throw me a star if you found the project interesting!¬†[https://github.com/calf-ai/calfkit-sdk](https://github.com/calf-ai/calfkit-sdk)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r1jr22/im_super_unemployed_and_have_too_much_time_so_i/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4q4hbc",
          "author": "qa_anaaq",
          "text": "Ah this is cool. I was obsessed with the idea of the event-driven approach to agents last year but never had to time to explore it. I‚Äôll be diving in. I always thought it‚Äôs a solid approach.",
          "score": 3,
          "created_utc": "2026-02-11 01:47:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4q9ba4",
              "author": "orange-cola",
              "text": "Yea for sure--I think it's a must for production-ready agents. Let me know what you think when you try it out! Always love any feedback",
              "score": 1,
              "created_utc": "2026-02-11 02:16:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rd7in",
          "author": "His0kx",
          "text": "Seems really interesting on paper ! How did you manage the separate microservice part ? One thing I have struggled with is that even with quality gates/api contracts between agents, for the same phase, same prompt/tools the output could be different with different subagents (maybe more food for thoughts when you will start working on orchestrating multi agents ?)",
          "score": 3,
          "created_utc": "2026-02-11 07:02:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rqnc4",
              "author": "orange-cola",
              "text": "How I'm currently going about it is to think of multi-agent orchestrations as agents in a text-based groupchat. Internally, each agent can can call tools with different schemas but when it responds into the \"groupchat,\" the message will always be text based. This way, the schema between agents will always be predictable, and agents can still effectively coordinate and chat among eachother. This is just my initial naive implementation so I fully expect to grow this into something more sophisticated as different agent communication patterns emerge.",
              "score": 1,
              "created_utc": "2026-02-11 09:09:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rwdwp",
                  "author": "His0kx",
                  "text": "Okay I think you have the right intuition üòÖ, Claude code did the same on the last release (Agent team)",
                  "score": 2,
                  "created_utc": "2026-02-11 10:02:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzu1d",
          "author": "Far_Independent8754",
          "text": "This is exactly the direction the industry needs to move. Building monolithic agents is a dead end for production.\n\nI‚Äôve been preaching about this lately‚Äîwe need to stop the 'Prompt Alchemy' and move toward **Microagentic Stacking**. Your approach with Kafka is the perfect infrastructure for it because it enforces the decoupling that most people ignore.\n\nIf you are breaking down agents into independent services, you‚Äôve already won half the battle against 'reasoning decay'. I actually wrote a **Manifesto** on why this modular/stacked approach is the only way to scale without the whole thing collapsing into a 'Big Ball of Mud'.\n\nCheck it out if you want to see the architectural patterns I'm formalizing: üîó[https://github.com/ericmora/microagentic-stacking](https://github.com/ericmora/microagentic-stacking)\n\nCongrats on the SDK, man. Building in public while job hunting is the best way to show senior-level thinking. Starred! ‚≠ê",
          "score": 2,
          "created_utc": "2026-02-11 05:11:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4robrd",
              "author": "orange-cola",
              "text": "Thanks for the star! And agreed, I believe production-grade agents will inevitably have to move towards event-driven architecture as these agent systems scale.",
              "score": 1,
              "created_utc": "2026-02-11 08:47:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4r3sei",
          "author": "Sea-Sir-2985",
          "text": "the decomposition into separate microservices for inference, tools, and routing is smart... the main pain point with monolithic agent frameworks is that scaling one part means scaling everything, and a slow tool call blocks the whole pipeline\n\nkafka as the backbone makes the async part trivial but i'd be curious how you're handling the latency tradeoff. LLM agents are already slow from inference time so adding message queue overhead on top might make the end-to-end response time rough for interactive use cases. seems like it'd shine more for batch processing and background agent workflows where latency doesn't matter as much\n\nthe multi-agent orchestration part is where this could get really interesting though... being able to spin up independent agent services that communicate through events without tight coupling is way cleaner than most multi-agent frameworks that try to do everything in one process",
          "score": 2,
          "created_utc": "2026-02-11 05:42:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rpx1r",
              "author": "orange-cola",
              "text": "I agree. I think added overhead from message queueing is probably best managed by dynamic instance scaling that adjusts to the incoming traffic load. It's more on the operational side of things, and outside of the SDK's domain, but the good thing is there are already well-established technologies for this purpose.\n\nAlso totally agree on the background agent workflows part. For agent operations on continuous background data streams, this SDK can really shine.",
              "score": 1,
              "created_utc": "2026-02-11 09:02:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ynvo1",
          "author": "UseMoreBandwith",
          "text": "interesting. but what could the use-cases for something like that?  (other than hacking/ddos etc)",
          "score": 1,
          "created_utc": "2026-02-12 11:11:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r04wed",
      "title": "Observations From Using GPT-5.3 Codex and Claude Opus 4.6",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "author": "Arindam_200",
      "created_utc": "2026-02-09 13:58:50",
      "score": 17,
      "num_comments": 6,
      "upvote_ratio": 0.82,
      "text": "I tested GPT-5.3 Codex and Claude Opus 4.6 shortly after release to see what actually happens once you stop prompting and start expecting results. Benchmarks are easy to read. Real execution is harder to fake.\n\nBoth models were given the same prompts and left alone to work. The difference showed up fast.\n\nCodex doesn‚Äôt hesitate. It commits early, makes reasonable calls on its own, and keeps moving until something usable exists. You don‚Äôt feel like you‚Äôre co-writing every step. You kick it off, check back, and review what came out. That‚Äôs convenient, but it also means you sometimes get decisions you didn‚Äôt explicitly ask for.\n\nOpus behaves almost the opposite way. It slows things down, checks its own reasoning, and tries to keep everything internally tidy. That extra caution shows up in the output. Things line up better, explanations make more sense, and fewer surprises appear at the end. The tradeoff is time.\n\nA few things stood out pretty clearly:\n\n* Codex optimizes for momentum, not elegance\n* Opus optimizes for coherence, not speed\n* Codex assumes you‚Äôll iterate anyway\n* Opus assumes you care about getting it right the first time\n\nThe interaction style changes because of that. Codex feels closer to delegating work. Opus feels closer to collaborating on it.\n\nNeither model felt ‚Äúsmarter‚Äù than the other. They just burn time in different places. Codex burns it after delivery. Opus burns it before.\n\nIf you care about moving fast and fixing things later, Codex fits that mindset. If you care about clean reasoning and fewer corrections, Opus makes more sense.\n\nI wrote a longer breakdown [here](https://www.tensorlake.ai/blog/claude-opus-4-6-vs-gpt-5-3-codex) with screenshots and timing details in the full post for anyone who wants the deeper context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r04wed/observations_from_using_gpt53_codex_and_claude/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4fmz12",
          "author": "swarmed100",
          "text": "Opus 4.6 reasons a lot longer than opus 4.5. One negative side I noticed from this is that it is \"better\" at finding delusional logic to explain why a set of facts that are clearly impossible \"make sense\", instead of concluding that some of the assumptions or inputs must be wrong since the set of facts are just impossible together.",
          "score": 7,
          "created_utc": "2026-02-09 14:03:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gbcxu",
              "author": "External-Yak-371",
              "text": "As a pro plan user I agree, but it also means my piddly allowance can nearly be consumed in one good planning session",
              "score": 3,
              "created_utc": "2026-02-09 16:10:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4hrtb3",
                  "author": "Manfluencer10kultra",
                  "text": "u/External-Yak-371 One git commit request for small refactors in many files (similar refactors) was enough for 48%.  It did notice I missed 5 items that needed to be refactored then used like 20k tokens to fix it, and then it was absolutely perfect.\n\nToo bad it was 50% of my 5h allowance (you get about 9 x 5h on pro at 11% of weekly ...).  In that sense, it was absolutely worthless spending my tokens on it.  \nBut what if I used Sonnet for it? it would have been maybe worse.  \nAnd these are the things that you don't want to do yourself, and want to hand over to AI. You close off your session after a long day, forget to commit all those refactors, but still want a sensible commit instead of \"lots of fixes\".  \nEh this is where AI tooling should come in to save the day, but nope..",
                  "score": 1,
                  "created_utc": "2026-02-09 20:21:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gcjpr",
              "author": "cmndr_spanky",
              "text": "This is pretty worrying but makes sense. More reasoning doesn‚Äôt mean better results and often just causes useless ‚Äúthought loops‚Äù that at best just wastes more open credits, at worst fills up context causing it to loose touch with the original request details.\n\nThat said, I‚Äôve never been impressed with any of openAI‚Äôs models as coding agents, so I‚Äôd suspect opus is still better despite the flaws. We‚Äôll see I guess",
              "score": 2,
              "created_utc": "2026-02-09 16:15:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gfqe5",
          "author": "kubrador",
          "text": "so basically one model is a startup founder and the other is an engineering lead pretending to care about code review",
          "score": 2,
          "created_utc": "2026-02-09 16:31:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hrdaq",
          "author": "Manfluencer10kultra",
          "text": "Bruh, I switched to Codex (Free) and getting incredible usage just on Free and GPT-5.2-Codex High, not even 5.3 and it's just night and day.  Claude put me in a depressive mood, and now I'm back enjoying engineering again.",
          "score": 2,
          "created_utc": "2026-02-09 20:19:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxbr2z",
      "title": "today's task",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/ggk43gavzthg1.jpeg",
      "author": "carsa81",
      "created_utc": "2026-02-06 08:02:13",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qxbr2z/todays_task/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3vayfa",
          "author": "West_Struggle2530",
          "text": "Opus 4.6",
          "score": 6,
          "created_utc": "2026-02-06 08:27:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o410o0h",
          "author": "mosquitosarenotcool",
          "text": "Kimi 2.5",
          "score": 1,
          "created_utc": "2026-02-07 04:24:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ziioq",
          "author": "Clean_Moose6832",
          "text": "Codex 5.3 is superior. ",
          "score": 0,
          "created_utc": "2026-02-06 22:52:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxa658",
      "title": "Built a Website Crawler + RAG (fixed it last night üòÖ)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qxa658/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:28:18",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "I‚Äôm **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff üíª), I thought:  \nEveryone is feeding PDFs‚Ä¶ **why not try something that‚Äôs not PDF ingestion?**\n\nSo I focused on fixing the **real problem ‚Äî crawling quality**.\n\nüîó GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What‚Äôs better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well üëÄ  \nFeedback, suggestions, and ‚≠ês are welcome!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qxa658/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o42m8nd",
          "author": "dezastrologu",
          "text": "AI slop post\n\nBut the tool sounds cool. Vibe coded?",
          "score": 1,
          "created_utc": "2026-02-07 12:54:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o445f1w",
              "author": "Cod3Conjurer",
              "text": "Kinda but not really¬†\nSome rough edges for sure, but the crawler logic and pipeline were intentionally built. Still learning and iterating.¬†",
              "score": 2,
              "created_utc": "2026-02-07 17:48:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r162ky",
      "title": "LLaDA2.1 vs Autoregressive Baselines: Is Diffusion Finally Competitive for Inference Throughput?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r162ky/llada21_vs_autoregressive_baselines_is_diffusion/",
      "author": "Ill_Awareness6706",
      "created_utc": "2026-02-10 16:51:41",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Been digging into the LLaDA2.1 paper (arXiv:2602.08676) after seeing claims about diffusion LLMs hitting 892 TPS on HumanEval+. The numbers are interesting enough that I'm starting to wonder if we've been sleeping on diffusion models for inference workloads.\n\nQuick context for those unfamiliar: LLaDA2.1 introduces a dual decoding system (Speedy Mode vs Quality Mode) that lets you trade accuracy for throughput. The 100B model hits 892 TPS peak on coding benchmarks with quantization, while the 16B model averages 1071 TPS across nine benchmarks vs 302 TPS for Qwen3 8B. The key innovation is Token to Token editing that lets the model correct its own mistakes during generation.\n\nStandard absorbing state diffusion models have a fundamental problem: once a \\[MASK\\] token becomes a real token, it's locked in forever. LLaDA2.1 adds what they call an \"Editing Set\" alongside the \"Unmasking Set\" at each timestep. This means the model can retroactively fix errors during parallel decoding, which addresses the token inconsistency issues that have plagued diffusion LLMs. Speedy Mode generates a rough draft fast (aggressive M2T threshold), then T2T passes clean up the artifacts. Quality Mode uses conservative thresholds throughout for higher accuracy but slower generation.\n\nThe benchmark highlights: LLaDA2.1 Flash (100B) on HumanEval+ scores 89.63 in both modes, with Speedy Mode achieving 13.81 tokens per forward vs Quality Mode's 9.18 TPF. On LiveCodeBench, Speedy Mode hits 44.05 at 6.48 TPF while Quality Mode reaches 45.37 at 3.80 TPF. Peak throughput with their quantized setup: 891.74 TPS on HumanEval+, 801.48 TPS on BigCodeBench Full, 663.39 TPS on LiveCodeBench. The 16B Mini variant peaks at 1586.93 TPS on HumanEval+. Compared to baselines across nine benchmarks: LLaDA2.1 Mini averages 1071.2 TPS vs 597.1 for LLaDA2.0 Mini, 464.7 for Ling Mini 2.0, and 301.9 for Qwen3 8B. That's roughly 3.55x throughput comparing the 16B diffusion model against the 8B autoregressive baseline, so not exactly apples to apples on parameters, but the efficiency gap is notable. Reproducibility note: these numbers use customized SGLang with per block FP8 quantization.\n\nThey also have Multi Block Editing that trades throughput for accuracy: AIME 2025 jumps from 63.33 to 70.00 with MBE, ZebraLogic from 84.20 to 88.20. Average across 10 benchmarks: 72.67 at 5.14 TPF with MBE vs 70.69 at 5.82 TPF without.\n\nOn the training side, they built what they claim is the first large scale RL framework for diffusion LLMs using EBPO (ELBO based Block level Policy Optimization). The core problem is that sequence level log likelihood is intractable in block autoregressive models, so they use Vectorized Likelihood Estimation for parallelized bound computation. Interesting direction if you're thinking about fine tuning diffusion models beyond standard SFT, though I haven't dug deep into the training methodology yet.\n\nNow here's where I'm genuinely uncertain. The paper is upfront about the tradeoffs: Speedy Mode works well for code and math but can produce artifacts on general chat. They specifically mention n gram repetitions that self correction only partially fixes. So the question becomes: are these throughput numbers meaningful if you can only use Speedy Mode for narrow domains?\n\nTo make this concrete: say you're running a code completion service handling 10K requests per minute. At 302 TPS with Qwen3 8B, you need roughly 33 inference instances to keep up. At 1071 TPS with LLaDA2.1 Mini, that drops to about 10 instances. That's real infrastructure savings if the quality holds up in production. But I'm skeptical whether benchmark TPS translates to real world throughput given the different decoding dynamics, and whether the occasional n gram artifacts would tank user experience.\n\nPaper: [https://arxiv.org/abs/2602.08676](https://arxiv.org/abs/2602.08676)\n\nGitHub: [https://github.com/inclusionAI/LLaDA2.X](https://github.com/inclusionAI/LLaDA2.X)\n\nFor those running inference at scale: would 3x throughput on code generation be enough to justify adding a completely different model architecture to your stack? Or is the operational complexity not worth it until diffusion models close the gap on general tasks?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r162ky/llada21_vs_autoregressive_baselines_is_diffusion/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qz1adm",
      "title": "-68% model size, <0.4 pp accuracy loss: Compressed LLaMA-3.2-1B ‚Üí Q4_0 GGUF on SNIPS Dataset (CPU Inference)",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1qz1adm",
      "author": "mr_ocotopus",
      "created_utc": "2026-02-08 06:10:48",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qz1adm/68_model_size_04_pp_accuracy_loss_compressed/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o47wcjn",
          "author": "Icy_Distribution_361",
          "text": "Looks interesting but can you say a bit more?",
          "score": 1,
          "created_utc": "2026-02-08 07:50:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47ym88",
              "author": "mr_ocotopus",
              "text": "Sure, it is basically a library to make your models smaller so they can perform a specific tasks at the same level as bigger models.   \nThe idea is to use these in on-device (OR) enterprise scale where you would want to process billions of records \\[save cost\\]  \nCheck out the repo for more : [https://github.com/chandan678/compressGPT](https://github.com/chandan678/compressGPT)\n\nBlog on how you can use it for on-device : [https://medium.com/@chandancjs/rethinking-on-device-llms-why-one-model-is-never-enough-3abccb4756bf](https://medium.com/@chandancjs/rethinking-on-device-llms-why-one-model-is-never-enough-3abccb4756bf)",
              "score": 1,
              "created_utc": "2026-02-08 08:11:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47z5co",
                  "author": "Icy_Distribution_361",
                  "text": "Ah, so it's not effectively the same ability model, it's same ability in specific tasks?",
                  "score": 1,
                  "created_utc": "2026-02-08 08:16:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4frssh",
          "author": "mr-KSA",
          "text": "This is truly valuable data. However, I feel that benchmarks and other metrics are unfortunately no longer providing anything beyond a general overview. The situation has become so¬†multifaceted¬†that it is becoming quite frustrating. To clarify: a 4B model specifically designed for translation can outperform an 80B model. Furthermore, the quality between different levels of¬†quantization¬†is, regrettably, inconsistent. I also suspect that many current models are being specifically¬†fine-tuned¬†to inflate these benchmark scores.\n\nIn my view,¬†empirical¬†experience is paramount. For instance, I have observed significant performance gaps between Q8 and Q4 quantization, particularly in MoE models. While a model like GPT-OSS 20B might be too 'clumsy' for my specific workflows, another user might prefer it over GPT-4. It ultimately depends on your specific use case. Because I utilize long, complex system prompts that require strict adherence to sequential instructions, models like GLM-4.7 or Granite 4 yield better results for me than Qwen 80B. For others, the opposite may be true.\n\nThe difference between Q4 and Q8 becomes especially¬†pronounced¬†in extended tasks where a structured 'flow' isn't utilized; a single logical error can lead to an¬†irreversible¬†divergence in the output. However, if a multi-model flow is implemented where each model is assigned a single task, Q4 is often sufficient. That said, I have encountered cases where a Qwen 30B (A3B) at Q8 provided answers that even a Qwen 80B at Q4 could not. I realize, of course, that many might disagree with this assessment.",
          "score": 1,
          "created_utc": "2026-02-09 14:31:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gtwta",
              "author": "mr_ocotopus",
              "text": "Hey, I agree to what you are saying.   \nAs LLM's get deployed on more and more systems there will be many \"right kind\" of ways to use it.   \nLike you have observed in your case it could be Q4 or Q8.   \nWhat compressGPT is trying to do is, provide a high level API to build the right kind of model that you would need. And if this particular flow does not workout you can itterate with other models/experiments faster - But since fine-tuning and quantising is a powerful tool it will be helpful to check weather a model passed down this pipeline will work or not. \n\nTLDR;   \nModels are \"correct model\" for a specific task, compressGPT offers one way of building \"right model\"",
              "score": 1,
              "created_utc": "2026-02-09 17:38:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qygq2b",
      "title": "We have developed a different architecture for LLM that does not work on transformers but works on the principles of reservoir computing and energy modelling. It remains constant on vRAM as we scale context unlike transformers.",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/3830vd8tc3ig1.jpeg",
      "author": "Dry_Oil2597",
      "created_utc": "2026-02-07 15:30:45",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qygq2b/we_have_developed_a_different_architecture_for/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o43s4zy",
          "author": "HumanDrone8721",
          "text": "That's lovely, now publish it, open source it and let us train models based on it with opensource data sets, like the ones from Olmo series, and this way we can compare the efficiency and accuracy of the new method. I think I still have $200 on thinker.ai so I could do a test run for you if you publish the pipeline for training and inference.",
          "score": 10,
          "created_utc": "2026-02-07 16:43:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ezthp",
          "author": "Noddybear",
          "text": "What perplexity?",
          "score": 1,
          "created_utc": "2026-02-09 11:23:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f049r",
              "author": "Dry_Oil2597",
              "text": "For average over 10k msmarco docs its 18",
              "score": 1,
              "created_utc": "2026-02-09 11:25:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r0lbvd",
      "title": "Mixture-of-Models routing beats single LLMs on SWE-Bench via task specialization",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "author": "botirkhaltaev",
      "created_utc": "2026-02-10 00:10:22",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been looking at per-task results on SWE-Bench Verified and noticed something that leaderboard averages hide: different models consistently solve¬†*different*¬†subsets of tasks.\n\nEven the top overall model on the leaderboard fails a non-trivial number of tasks that other models reliably solve, and the reverse is also true. This suggests strong task-level specialization rather than one model being strictly better.\n\nTo test this, I built a¬†**Mixture-of-Models architecture**, which is different from traditional routing that just defaults to the strongest aggregate model most of the time. The goal isn‚Äôt to route to a single model as often as possible, but to exploit complementary strengths between models.\n\nConcretely:\n\n* The problem description is embedded\n* It‚Äôs assigned to a semantic cluster (learned from general coding data, not SWE-Bench)\n* Each cluster has learned per-model success statistics\n* The task is routed to the historically strongest model for that¬†*type*¬†of problem\n\nImportantly, this does¬†**not**¬†route the top aggregate model for the majority of tasks. Several clusters consistently route to other models where they outperform it, even though it has the highest overall score.\n\nThere‚Äôs no new foundation model, no test-time search, and no repo execution, just a lightweight gating mechanism over multiple models.\n\nUsing this Mixture-of-Models setup, the system reaches 75.6% on SWE-Bench, exceeding single-model baselines (\\~74%). The takeaway isn‚Äôt the absolute number, but the mechanism: leaderboard aggregates hide complementary strengths, and mixture architectures can capture a higher ceiling than any single model.\n\nBlog with details and methodology here:¬†[https://nordlyslabs.com/blog/hypernova](https://nordlyslabs.com/blog/hypernova)\n\nGithub: the framework is open source !¬†[https://github.com/Nordlys-Labs/nordlys](https://github.com/Nordlys-Labs/nordlys)\n\nML/AI Research Community Discord:¬†[https://discord.gg/dqW7BBrq](https://discord.gg/dqW7BBrq)",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r0lbvd/mixtureofmodels_routing_beats_single_llms_on/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qz2edh",
      "title": "Golang or Python",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qz2edh/golang_or_python/",
      "author": "Ok-Satisfaction945",
      "created_utc": "2026-02-08 07:14:07",
      "score": 8,
      "num_comments": 19,
      "upvote_ratio": 0.91,
      "text": "Why python over golang? Current on my first year of mechatronics looking to expand and get ahead. I just bought a Jetson Orin nano I would like to start tinkering with. I understand python is the right now but from research I done I feel like golang really got more potential overall. Would love to hear from people in this space.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qz2edh/golang_or_python/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o48u3q8",
          "author": "etherealflaim",
          "text": "Go is better for building systems. Python is better for stitching together other people's stuff, which in the world of AI means a lot more Lego bricks to do random stuff like training and evaluating models locally.  If you're using cloud models like Gemini, model gateways, or providers like ollama though, suddenly this advantage gets blunted.\n\nFor agentic systems, Temporal (which was built in Go but that isn't super relevant) is a killer technology and you can even mix and match Go and Python where each one suits.",
          "score": 6,
          "created_utc": "2026-02-08 12:55:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a0ylc",
              "author": "Ok-Satisfaction945",
              "text": "Thanks for the straight to the point explanation also wasn‚Äôt aware of temporal looking into it üëçüèæ",
              "score": 2,
              "created_utc": "2026-02-08 16:53:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4c5agf",
              "author": "Osi32",
              "text": "I like this answer. Well put.",
              "score": 1,
              "created_utc": "2026-02-08 23:11:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48hoy1",
          "author": "Osi32",
          "text": "I built a large (non-AI) middleware last year in Golang.\nFirst off- I love Golang. It‚Äôs powerful, fast and efficient. It‚Äôs truly an amazing language.\n\nHowever, adoption of a new language is slow.\n\nMy general advice is- if you ever plan to hire people to maintain what you‚Äôre building or get someone else to help maintain it. Python programmers are a dime a dozen (figure of speech, not literally ;))\n\nGolang is great if you want to build something from scratch with minimal external libraries from other people.",
          "score": 2,
          "created_utc": "2026-02-08 11:12:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48n2cl",
              "author": "Ok-Satisfaction945",
              "text": "I see , I was getting the impression that with python it‚Äôs like ‚Äú we been doing it this way why change now?‚Äù Type scenario that‚Äôs why I wanted to ask the users . Solid advice personally I think I will start with golang just didn‚Äôt know it was it something fundamentally that I guess wasn‚Äôt capable of compared to python in action/real world.",
              "score": 2,
              "created_utc": "2026-02-08 12:00:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o48nx57",
          "author": "burntoutdev8291",
          "text": "Python. Faster iterations and POC. Libraries are also mostly in Python.\n\nI also say this with knowledge in rust and go.\n\nWith that said, you didn't really provide a use case. Do you want to do pure backend, devops, agentic, RAG, traditional AI engineering?",
          "score": 2,
          "created_utc": "2026-02-08 12:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48pn7a",
              "author": "Ok-Satisfaction945",
              "text": "I‚Äôm kind of all over the place atm, but I plan to go in mostly in robotics & agentic/ai integration. Wanted something I could take into different spaces. If I can‚Äôt spin of my own thing I plan of going to work at Lockheed, manufacturing/aerospace something of that nature. As far as python I noticed it‚Äôs more resources available. I just didn‚Äôt wanna learn something potentially useless or outdated by the time I get anywhere you know?",
              "score": 1,
              "created_utc": "2026-02-08 12:21:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47tlr2",
          "author": "tom-mart",
          "text": "Beacuse I know Python.",
          "score": 1,
          "created_utc": "2026-02-08 07:25:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47u894",
              "author": "Ok-Satisfaction945",
              "text": "Understandable, it just seems like Golang got way more potential & scalability, but again I‚Äôm not in any ‚Äúspace‚Äù per se. I‚Äôm starting sorta fresh the only limited experience I have is with xml & mySQL from hosting gaming servers in the past. If you could start again would you still choose python?",
              "score": 1,
              "created_utc": "2026-02-08 07:31:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o47v08g",
                  "author": "tom-mart",
                  "text": ">it just seems like Golang got way more potential & scalability\n\nAll the major AI tools are written in Python. Starting with PyTorch and ending on the agentic frameworks like Pydantic AI or Langchain. Not sure how you see more potential in Golang but if you do then the choice should be simple for you.",
                  "score": 2,
                  "created_utc": "2026-02-08 07:38:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dinb6",
          "author": "Terrible_Tangelo6064",
          "text": "I thought the title said \"golang or prison\"",
          "score": 1,
          "created_utc": "2026-02-09 03:45:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4g4xhm",
          "author": "BidWestern1056",
          "text": "most science researchers in ml/RL/AI use python so it is the most natural to use for any adjacent spaces because there will not be as many custom libraries in golang for specific scientific/matrix/tensor operations so for these you will have to pass to python or an equivalent in another language if it exists anyway so easier just to do it all in python. ",
          "score": 1,
          "created_utc": "2026-02-09 15:39:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gdd59",
          "author": "soopazoupy",
          "text": "python because nearly every AI stack assumes python first e.g. PyTorch, TensorFlow, ROS2 tooling, OpenCV, CUDA bindings, and most example repos you‚Äôll find are python native. small bonus with python is that it got libraries like pydantic to make it easy to structure inputs/outputs cleanly and newer tooling around it fits nicely when you start building ai powered or sensor-heavy pipelines. go is excellent for building reliable infrastructure, but the robotics/ML ecosystem around it is thinner so you‚Äôll fight the tooling more often. that doesn‚Äôt mean go is less potential tho but just that it's in a different lane",
          "score": 1,
          "created_utc": "2026-02-09 16:19:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o486z4n",
          "author": "Novel_Leading_7541",
          "text": "You don‚Äôt need to choose‚Äîlearn both lightly; with vibe coding and AI tools now, the language matters way less than understanding the problem üôÇ",
          "score": 1,
          "created_utc": "2026-02-08 09:31:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48mg6l",
              "author": "Ok-Satisfaction945",
              "text": "Definitely agree with you i just had one of those late night thoughts and wanted to hear from people that have used one or the other as far as their experiences. I live in a place where what Im doing or anything computer related is non existent & finding people with programming experience it‚Äôs difficult",
              "score": 1,
              "created_utc": "2026-02-08 11:55:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fl0y9",
              "author": "m98789",
              "text": "Not entirely true.\n\nThough the spirit of what you saying is true, in practice, you do need to select a language (and library ecosystem) that LLMs have a lot of training data on.",
              "score": 1,
              "created_utc": "2026-02-09 13:52:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4952w5",
          "author": "Crafty_Disk_7026",
          "text": "If you want quality software and less bugs then Go.  If you want to move fast and make things easier then Python.  I use both everyday",
          "score": 1,
          "created_utc": "2026-02-08 14:06:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a09fu",
              "author": "Ok-Satisfaction945",
              "text": "Anything you would say go lack that python it‚Äôs superior? And as far as libraries for go are they decently availability?",
              "score": 1,
              "created_utc": "2026-02-08 16:49:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4a1ngk",
                  "author": "Crafty_Disk_7026",
                  "text": "I would say no, atleast not for me, if I want to use Go for something I generally can.  People other than me may say that though.  Also it's not a big deal to have systems with 2 ore more languages that happily work together.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:56:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r041y4",
      "title": "A RAG Agent and their Types",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1r041y4",
      "author": "KarllsMarcel",
      "created_utc": "2026-02-09 13:21:33",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r041y4/a_rag_agent_and_their_types/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o4fh4qy",
          "author": "flonnil",
          "text": "ballsy to use n8n screenshots in something that tries to sell you as a \"pro\".",
          "score": 2,
          "created_utc": "2026-02-09 13:29:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j8ojz",
          "author": "dezastrologu",
          "text": "Cool more LLM generated slop",
          "score": 1,
          "created_utc": "2026-02-10 00:58:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ptltc",
          "author": "SystemFlowStudio",
          "text": "One thing that helped me think about RAG agents more clearly was separating **retrieval responsibility** from **control responsibility**.\n\nRough breakdown I keep coming back to:\n\n**1. Passive RAG**  \nRetrieval is a single, fixed step (query ‚Üí retrieve ‚Üí answer). No iteration, no state. Easy to reason about, hard to scale to complex tasks.\n\n**2. Tool-augmented RAG agent**  \nThe agent decides *when* to retrieve vs act. Retrieval becomes a tool call, often interleaved with reasoning steps. This is where looping and stale context issues start to appear.\n\n**3. Planner-Executor RAG**  \nPlanner decomposes the task, executor performs steps, retrieval happens per step. Much more powerful, but you need guardrails or you get infinite plan-replan cycles.\n\n**4. Memory-augmented / stateful RAG**  \nRetrieval isn‚Äôt just documents ‚Äî it includes prior actions, summaries, or checkpoints. Great for long tasks, very easy to accidentally poison the context.\n\nWhat I‚Äôve noticed is most ‚ÄúRAG agent bugs‚Äù aren‚Äôt retrieval quality issues ‚Äî they‚Äôre **control-flow failures** (no termination condition, repeated tool calls, planner drift).\n\nCurious how others here are drawing these boundaries ‚Äî especially in production systems.",
          "score": 1,
          "created_utc": "2026-02-11 00:42:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2s3r4",
      "title": "I dont get mcp",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "author": "Yaar-Bhak",
      "created_utc": "2026-02-12 12:26:56",
      "score": 8,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "All I understood till now is - \n\nI'm calling an LLM api normally and now\nInstead of that I add something called MCP which sort of shows whatever tools i have? And then calls api \n\n\nI mean, dont AGENTS do the same thing? \n\nWhy use MCP? Apart from some standard which can call any tool or llm \n\nAnd I still dont get exactly where and how it works \n\nAnd WHY and WHEN should I be using mcp? \n\nI'm not understanding at all üò≠ Can someone please help\n\n",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2s3r4/i_dont_get_mcp/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4zrwup",
          "author": "fooz42",
          "text": "Service registry and discovery for remote procedure call is a wheel that gets reinvented every platform. It's not a revolution except in the sense the wheel gets reinvented every time the cycle turns, and now I'm getting dizzy from my metaphor.",
          "score": 9,
          "created_utc": "2026-02-12 15:19:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z3q6l",
          "author": "rudzienki",
          "text": "It's just a standardised way for companies to \"expose their tools\".\n\nIf you're Stripe you have a bunch of tools: \"do payment\", \"check invoices\" etc. If you want your agent to use them you can just... add them as tools to your agent. That's it. \n\nBut with MCP you can just say \"connect to stripe MCP\" and it automatically fetches all Stripe tools to be called. Stripe updates tools, you get update automatically.\n\nBut aside from that - no difference. \n\nBtw, MCP is much bigger protocol that handles more stuff than exposing tools, but in reality it's 99%, other uses didn't get much traction as far as I know.",
          "score": 4,
          "created_utc": "2026-02-12 13:08:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4yy3mm",
          "author": "kubrador",
          "text": "mcp is basically \"what if we made tool use boring and standardized so literally any llm can talk to literally any tool without rewiring everything\" agents let your llm pick tools. mcp is the \\*protocol\\* so your llm doesn't need to know what tools exist. they just show up. it's the difference between \"here's a menu\" vs \"here's a standardized way to hand someone a menu\"\n\nyou need it when you're tired of writing custom integrations for every tool+llm combo. you don't need it if you're just bolting claude into your thing once and calling it a day.",
          "score": 2,
          "created_utc": "2026-02-12 12:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yzpvs",
              "author": "Yaar-Bhak",
              "text": "you think this can be used in production?\n\nand this means mcp would be used only in agentic flows right?",
              "score": 1,
              "created_utc": "2026-02-12 12:42:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4z5ouz",
          "author": "Astronos",
          "text": "it is function/tool calling over api",
          "score": 1,
          "created_utc": "2026-02-12 13:21:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zo2ke",
          "author": "Crafty_Disk_7026",
          "text": "MCP is just like an open ai spec the ai can read and know how to use your tool. It's literally just instruction manual",
          "score": 1,
          "created_utc": "2026-02-12 15:00:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tzq6",
          "author": "voidiciant",
          "text": "From what I understand:\n\nThe models have to be trained (and usually are, there is often a ‚Äûtool‚Äú tag on the downloads) to insert special keywords in their responses when a tool call is appropriate. \n\nThese keywords are intercepted by the runtime (the thing taking your input, converting to tokens,, etc) and the runtime performs the appropriate calls to the registered mcp tools (according to the protocol) and feeds back the tool-call results to the model, which in turn now incorporates them in the next response.\n\nAdditionally, and here I get fuzzy, the runtime generates a system prompt that contains a list of available MCP Tools, and the model is trained to understand this to generate the relevant keywords in the response based. \n\nMCP defines the protocols/API/formats. \n\nThat‚Äòs the gist for me",
          "score": 1,
          "created_utc": "2026-02-12 18:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51rpl9",
          "author": "CreepyValuable",
          "text": "MCP is kind of sort of a universal adapter to plug anything from ChatGPT to your toaster together.\n\nIt's not quite that straightforward and the actual interface is kind of clunky but it's pretty useful.\n\nFor example, my (not very good, but experimental so that's not important) AI uses it for things like a weather service, XiaoZhi AI esp support (essentially a smart speaker with a screen), VS Code integration and some other random things. It avoids needing a whole bunch of incompatible APIs.",
          "score": 1,
          "created_utc": "2026-02-12 20:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52svxj",
          "author": "Glum_Teaching8224",
          "text": "It's just tool using reference for the agent. ",
          "score": 1,
          "created_utc": "2026-02-13 00:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50wvs5",
          "author": "throwaway490215",
          "text": "MCPs are bullshit. They are a standard that basically tells the program run on your computer to prepend `some-tool --help` when you start a conversation, but with much more overhead, and **every conversation** even if you dont want to use `some-tool` this session. \n\nAnybody talking about credentials/authentication is a moron. \n\nJust add a \"Use `some-tool --help` to do X\" in your AGENTS.md and you're good.",
          "score": 0,
          "created_utc": "2026-02-12 18:31:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwht1",
          "author": "Electronic-Door7134",
          "text": "Good luck explaining to an auditor why your gave a 3rd party company full access to your company data (which is what happens without mcp)",
          "score": -1,
          "created_utc": "2026-02-12 15:41:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50avet",
              "author": "PaddingCompression",
              "text": "Wut",
              "score": 2,
              "created_utc": "2026-02-12 16:48:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2ree2",
      "title": "Lessons from building AI shopping assistant for 1B$+ skincare brand.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "author": "rudzienki",
      "created_utc": "2026-02-12 11:50:51",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 0.83,
      "text": "Hey! I was recently hired to build an AI shopping assistant for a huge brand, 1B$+ in revenue. Unfortunately can't say which one is it (damn NDAs), but I thought I'd share some lessons. After the project CTO told me ‚ÄúWorking with you was the best AI investment in the last year‚Äù, so I guess it went well!\n\nI'm reposting this from my linkedin, so sorry for this \"linkedinish\" vibe:\n\nThe biggest secret was, surprise, surprise, **not** wasn‚Äôt fancy AI methods, complex RAG pipelines, and multi step workflows. In the end it was good prompts, a bunch of domain-specific tools and one subagent.  \n  \nThe secret was the process.  \n  \nI didn‚Äôt know anything about skincare so I had to learn about it. Even light understanding of the domain turned out EXTREMELY IMPORTANT since it allowed m to play around with an agent and have a good judgement whether it says good things. The fastest feedback loop is always \"in your head\".   \n  \nI built a domain-specific dashboard for the client. A collaborative environment where domain experts can play around with an agent, comment, feedback, etc. I took the idea from [Hamel Husain](https://x.com/HamelHusain) who said that [‚ÄúThe Most Important AI Investment is A Simple Data Viewer‚Äù.](https://x.com/i/status/1991903412997509372) He was damn right about it.   \n  \nThe last thing is something that is not talked much about but it should. We got hundreds of files about company knowledge. This knowledge is spread around big organisations like crazy. But if you really really understand the domain, if you really digest it all and ask a lot of questions, you‚Äôll be able to COMPRESS this knowledge. You‚Äôll find common stuff, remove dead ends, and really narrow it down to sth that expresses most about this company in smallest piece of text. This is your system prompt!! Why split context and add a potential point of failure if you can have MOST of the important stuff always in the system prompt? It‚Äôs crazy how well it works.  \n  \nOn the context engineering side we ended up with a great system prompt + a bunch of tools for getting info about products. I added one subagent for more complex stuff (routine building), but that was the only ‚Äúfancy‚Äù thing out there.  \n  \nI think the lesson here is that building agents is not hard on the technical level, and every developer can do it! The models do all the heavy lifting and they‚Äôre only getting better. The secret is understanding the domain and extracting the domain knowledge from people who know it. It's communication.\n\n  \nI'm curious:\n\nHave you built such \"customer support\"-related agents for your companies too? One thing that triggers me is amount of those giant SaaS companies that promises \"the super ultra duper ai agent\", and honestly? I think they don't have much secret sauce. Models are doing heavy lifting, and simple methods where heavy lifting is done by domain-specific knowledge trump general purpose ones. \n\nHere's what Malte from Vercel recently wrote btw:\n\nhttps://preview.redd.it/h2pjrjfix1jg1.png?width=1198&format=png&auto=webp&s=c8cd25ac93ee3a1b92cab153a1c591edbaf35d78\n\nIt somehow clicks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2ree2/lessons_from_building_ai_shopping_assistant_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4ysvg2",
          "author": "HatApprehensive141",
          "text": "‚ÄúSecret sauce‚Äù = good prompts, domain tools, and actually understanding the business‚Ä¶ basically just doing your job properly.\n\nLots of companies hype up intergalactic RAG pipelines, but if you don‚Äôt compress real domain knowledge into a clear system, your agent is just an overconfident intern. The real edge isn‚Äôt the model magic, it‚Äôs the context quality.",
          "score": 6,
          "created_utc": "2026-02-12 11:53:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z7lpo",
          "author": "tom-mart",
          "text": "Wait till you discover the water is wet.",
          "score": 4,
          "created_utc": "2026-02-12 13:32:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ysv0w",
          "author": "kubrador",
          "text": "wow so the secret sauce was just... understanding the domain and writing good prompts. truly revolutionary stuff, might as well say the secret to cooking is using fresh ingredients and knowing what tastes good",
          "score": 2,
          "created_utc": "2026-02-12 11:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z2ese",
              "author": "rudzienki",
              "text": "I don't think simplicity is always obvious. There are many merchants of complexity out there who want to tell you otherwise.\n\nThat was the point of the post.",
              "score": 1,
              "created_utc": "2026-02-12 13:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4zc408",
          "author": "ampancha",
          "text": "You're right that domain knowledge compression matters more than complex RAG for quality. The gap I see in most \"it works\" agents is what happens at production scale: prompt injection attempts from real users, hallucinated product claims becoming liability, and cost spikes without per-user attribution. For a $1B brand those risks are where the actual work starts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-12 13:57:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51kyxj",
          "author": "nore_se_kra",
          "text": "Beyond the hype... interesting read despite the comments here. I dont think it hurts to tell the story of applied \"boring\" company specific domain knowledge one more time.",
          "score": 1,
          "created_utc": "2026-02-12 20:26:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o51q54o",
          "author": "SamCRichard",
          "text": "What LLM did you use or are you routing between them\n\n",
          "score": 1,
          "created_utc": "2026-02-12 20:50:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyimtf",
      "title": "Grounding Is Not a Prompt",
      "subreddit": "LLMDevs",
      "url": "https://substack.com/home/post/p-187075330",
      "author": "SeriousSir1148",
      "created_utc": "2026-02-07 16:45:11",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.73,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qyimtf/grounding_is_not_a_prompt/",
      "domain": "substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qy8ptz",
      "title": "Your agent's 100% pass rate on 10 runs is statistically compatible with 72% true reliability. Here's the math and a way to fix your CI.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qy8ptz/your_agents_100_pass_rate_on_10_runs_is/",
      "author": "Better_Accident8064",
      "created_utc": "2026-02-07 08:39:48",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 0.82,
      "text": "I ran a LangGraph agent with Claude 3.5 Haiku on a trivial task (\"What is 15 * 37?\") across 100 trials. Pass rate: 70%. Not 95%, not 99%. Seventy percent on a calculator task.\n\nThe interesting part isn't that agents fail ‚Äî everyone here knows that. It's that **single-run evals can't detect it.** If you run 10 trials and get 10/10, Wilson score CI at 95% confidence gives you [0.722, 1.000]. Your \"perfect\" result is statistically compatible with a system that fails 28% of the time.\n\nThis matters for CI/CD. Most teams either skip agent evals in their pipeline or run each test once and assert pass/fail. Both approaches have the same problem: they can't distinguish a 95%-reliable agent from a 70%-reliable one unless you run enough trials.\n\n**What actually works for catching regressions:**\n\nRun each test case N times (N >= 20 makes a real difference). Compute Wilson CI on the pass rate. Compare against your baseline using Fisher exact test instead of naive diff. Use Benjamini-Hochberg correction if you're testing multiple cases simultaneously ‚Äî otherwise you'll get false alarms.\n\nFor failure attribution: group trials into pass/fail, compare tool call distributions at each step, pick the step with the lowest Fisher p-value. This gives you \"step 2 tool selection is the bottleneck\" instead of \"test failed.\"\n\nI open-sourced the framework I built for this: [agentrial](https://github.com/alepot55/agentrial). It wraps any Python callable and has adapters for LangGraph, CrewAI, AutoGen, Pydantic AI, OpenAI Agents SDK, and smolagents. YAML config, runs in CI, exit code 1 on statistically significant regression.\n\n```\nbasic-math      20/20  CI=[0.839, 1.000]  PASS\nmulti-step      14/20  CI=[0.480, 0.862]  FAIL\n  ‚Üí Step 2: tool selection diverges (p=0.003)\n```\n\nCurious how others are handling this. Are you running multi-trial evals in CI? Using soft thresholds? Something else entirely?\n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qy8ptz/your_agents_100_pass_rate_on_10_runs_is/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o41ydxz",
          "author": "Thick-Protection-458",
          "text": "\\> Your agent's 100% pass rate\n\nWell, isn't it clear?\n\nI mean that's fucking heuristic rather than predictable algorithm. With additional sampling randomness.\n\nYou can never have 100% working solution in such a case. You can have (unknown to you yet) distribution of test results you can't reliably distinguish from 100%.",
          "score": 4,
          "created_utc": "2026-02-07 09:17:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o420e97",
              "author": "Better_Accident8064",
              "text": "Exactly. You can't distinguish true 100% from a sample that happened to look like 100%. But you can quantify it.\n\n20/20 passes gives a Wilson 95% CI of \\[0.839, 1.000\\], so your \"perfect\" score is compatible with 84% true reliability. agentrial just makes this math explicit and wires it into CI/CD.",
              "score": 1,
              "created_utc": "2026-02-07 09:37:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43me85",
          "author": "lionmeetsviking",
          "text": "I use this: https://github.com/madviking/pydantic-llm-tester",
          "score": 1,
          "created_utc": "2026-02-07 16:15:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2qfcz",
      "title": "Mix prompts instead of writing them by hand",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/5z7edpx9n1jg1.png",
      "author": "Everlier",
      "created_utc": "2026-02-12 10:55:12",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r2qfcz/mix_prompts_instead_of_writing_them_by_hand/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qzi6ot",
      "title": "Moltbook - No Human Captcha allows only LLMs post",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/85cc0i50nbig1.png",
      "author": "hasmcp",
      "created_utc": "2026-02-08 19:25:50",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 0.62,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qzi6ot/moltbook_no_human_captcha_allows_only_llms_post/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o4ckqbm",
          "author": "dezastrologu",
          "text": "Why do we still care about moltbook",
          "score": 3,
          "created_utc": "2026-02-09 00:40:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d8oco",
              "author": "hasmcp",
              "text": "My interest on the brilliant idea came for dealing with the spams and/or unwanted inputs. I found it clever. The developers are trying innovative ways and enjoying the moment.",
              "score": 4,
              "created_utc": "2026-02-09 02:50:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e5tbx",
                  "author": "HumanDrone8721",
                  "text": "So soon to verify that you're no robot you'll have to use a robot?",
                  "score": 3,
                  "created_utc": "2026-02-09 06:36:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4b61ea",
          "author": "Moliri-Eremitis",
          "text": "Interesting. How long was the verification window?",
          "score": 1,
          "created_utc": "2026-02-08 20:09:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bh7z1",
              "author": "_tony_lewis",
              "text": "Seems to be 30s and if you are incorrect it locks, one of my agents made a formatting mistake so list a post",
              "score": 2,
              "created_utc": "2026-02-08 21:05:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qprcx",
          "author": "dydhaw",
          "text": "This is the dumbest thing I've ever seen",
          "score": 1,
          "created_utc": "2026-02-11 03:59:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r204ab",
      "title": "Intent Model",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "author": "Repulsive_Laugh_1875",
      "created_utc": "2026-02-11 15:18:54",
      "score": 6,
      "num_comments": 13,
      "upvote_ratio": 0.88,
      "text": "Hi community,  \nthis is my first post here üôÇ\n\nI‚Äôm an experienced AI Engineer / AI DevOps Engineer / Consultant working for a well-known US-based company. I‚Äôd really appreciate your thoughts on a challenge I‚Äôm currently facing and whether you would approach it differently.\n\nUse-Case\n\nI‚Äôm building an **intent classifier** that must:\n\n* Run **on edge**\n* Stay around **\\~100ms latency**\n* Predict **1 out of 9 intent labels**\n* Consider **up to 2 previous conversation turns**\n\nThe environment is domain-specific (medical domain in reality), but to simplify, imagine a system controlling a car.\n\nExample:  \nYou have an intent like `lane_change`, and the user can request it in many different ways.\n\nCurrent Setup\n\n* Base model: **phi-3.5-mini-instruct**\n* Fine-tuned using **LoRA**\n* Model explicitly outputs only the intent token (e.g., `command_xyz`)\n* Each intent is mapped to a **single special token**\n* Almost no system prompt (removed to save tokens)\n\nPerformance:\n\n* \\~110ms latency (non-quantized) ‚Üí acceptable\n* \\~10 input tokens on average\n* \\~5 output tokens on average\n* 25k training samples\n* \\~95% accuracy\n\nSpeed is not the main issue ‚Äî I still have some room for token optimization and quantization if needed.\n\nthe real challenge -> the missing 5%.\n\nThe issue is **edge cases**.\n\nThe model operates in an open-input environment. The user can phrase requests in unlimited ways.\n\nFor example:  \nFor `lane_change`, there might be 30+ semantically equivalent variations. I built a synthetic data generation pipeline to create such variations and spent \\~2 weeks refining it. Evaluation suggests it's decent.\n\nBut:\n\nThere are still rare phrasings that the model hasn‚Äôt seen ‚Üí wrong intent prediction.\n\nOf course, I can:\n\n* Iteratively collect misclassifications\n* Add them to the training set\n* Retrain\n\nBut that‚Äôs slow and reactive.\n\nConstraints:\n\n* I could use a larger model (e.g., phi-4), and I‚Äôve tested it.\n* However, time-to-first-token for phi-4 is significantly slower.\n* Latency is more important than squeezing out a few extra percent of quality.\n\nSo scaling up model size isn‚Äôt ideal.\n\nMy questions to you:\n\nHow would you tackle the final 5%?\n\nI‚Äôd really appreciate hearing how others would approach this kind of edge, low-latency intent classification problem.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r204ab/intent_model/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4tefqh",
          "author": "Swimming-Chip9582",
          "text": "Can you detect whether the output is likely to be an edge case or know when it's part of an uncertain category?  Perhaps you can fallback on a larger model and accept latency when it's unsure. So starting both the small and large concurrent, if the small finishes first and is all good just cancel the large one, if it's uncertain then wait for completion from the bigger model. ",
          "score": 4,
          "created_utc": "2026-02-11 15:48:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4tll8h",
          "author": "FoodAccurate5414",
          "text": "You need to look into using very very very small models to handle edge cases. There are tons on hugging face. Run it along side your main model",
          "score": 2,
          "created_utc": "2026-02-11 16:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4v57q9",
              "author": "Repulsive_Laugh_1875",
              "text": "Can you recommend something or at least tell which ones you have in mind?",
              "score": 1,
              "created_utc": "2026-02-11 20:43:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4t91xc",
          "author": "InfraScaler",
          "text": "Ok, so catching new ways of expressing the intent from your users is definitely reactive, but what about using bigger LLMs to generate those for you? Can you even use agents leveraging big LLMs to \"test\" and help prepare training for your system? Not necessarily cheap, but your employer may be able to afford it :-)",
          "score": 1,
          "created_utc": "2026-02-11 15:22:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tc616",
              "author": "Repulsive_Laugh_1875",
              "text": "It‚Äôs not the employer ‚Äî it‚Äôs the customer who has to pay for it üòâ\n\nJokes aside, thank you for your comment.\n\nI‚Äôm already using GPT-5.2-chat for the synthetic data generation. As mentioned, I‚Äôm currently achieving a full match rate  (intent plus parameters) of around 95%, and based on the latest metrics even closer to 97%, which I consider quite solid.\n\nThat‚Äôs why I don‚Äôt believe the data generation itself is the core issue here.\n\n  \n\\-------------------------- edited\n\n  \nin fact, I also thought about leveraging two agents to simulate such qustion answer things and try to figure out such edge cases. But this is costly.",
              "score": 2,
              "created_utc": "2026-02-11 15:38:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4thauy",
                  "author": "InfraScaler",
                  "text": ">But this is costly.\n\nNot for you! :P",
                  "score": 2,
                  "created_utc": "2026-02-11 16:02:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4ybuxq",
                  "author": "BehindUAll",
                  "text": "Why is cost an issue? If you are using 2 classifiers, they are cheaper than LLMs and if you are using LLMs like Llama-3.1 8b or Mistral small etc. you will get great speed and is cheap too. Plus you can use groq, SambaNova or Cerebras for fast and cheap inference. And for LLMs the input will be like 10 words and output 1 word if you do your system prompt right. The cost goes way down.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:17:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vvjay",
          "author": "Charming_Support726",
          "text": "First I doubt that models with decoder only architecture are the best fit for this task.  They allways bring error classes which are unwanted in these scenarios.\n\nSecond I doubt, that you ever will reach 100% - but that's obvious\n\nThird: Encoder-Decoder architectures promise the best fit and efficiency for these tasks, but you'll never know ( we did back in 2020 over a 1000 intents with Rasa in a BERT-Style Intent detector went over 90% but appeared still flaky). T5Gemma Style Models could be a solution, but I got no experience on fine tuning them.\n\nFourth you could apply additional techniques like reranking or building a similarity distance to sample sentences to make sure that your generation is a valid result.  Maybe good to combine this approaches with multiple generations of the result.\n\n\n\n",
          "score": 1,
          "created_utc": "2026-02-11 22:53:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ysnco",
              "author": "Repulsive_Laugh_1875",
              "text": "To your fist point: I agree to this but I don¬¥t get what you mean by error classes? Do you mean just a wrong prediction label with this? If thats your point, you are right. These are the edge cases what I¬¥m talking about! It never responds in a different way, it just predicts sometimes the wrong label (edge cases).\n\n  \nTo your second point -> That is obvious and clear. Customer is already pretty satisfied with the solution, but I¬¥m not :D\n\n  \nthird: good hint. I need to look into them on how to really finetune them.\n\n  \nfourth: Indeed, I also wanted to test if you purely rely on similarity (history dependend intents excluded). I wanted to test if you can embedd those 25k and just detect the intent based on the similarity. I did something similar a few years back and this worked also very solid ;-) Now with the larger embedding representations, could also be an option.",
              "score": 1,
              "created_utc": "2026-02-12 11:51:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o51kkkq",
                  "author": "Charming_Support726",
                  "text": "Decoder only does not stick close to the input as encoder-decoder do. So you get all the things like hallucinations and similar. They also have issues generating \"i dont know\" tokens. \n\nIf you have an encoder part in the model like in a T5 the model is always working with a kind of embedding as knowledge representation for building a bridge to the decoder. \n\nThat's you are 90% there using an SLM, so these are my ideas to improve.",
                  "score": 1,
                  "created_utc": "2026-02-12 20:24:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yfzud",
          "author": "tshawkins",
          "text": "If it is acceptable to have edge case resolution slower, you could push the cases that fail to resolve out to a bigger model, and push those requests and thier resolutions into a set of training data for your next primary model rebuild. Its a hybrid solution that uses both bigger model and retraining for the 5% only.",
          "score": 1,
          "created_utc": "2026-02-12 09:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4u6y1l",
          "author": "TheBioto",
          "text": "Give Mistral 3b a shot and see how it works. \n\nI am currently doing something similar with endpoint nodes with small models. Mistral was the only one that was fast enough/could be guided enough to accomplish my needs.\n\nI don't have any suggestions for your edge cases issue, good luck!",
          "score": 1,
          "created_utc": "2026-02-11 18:01:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzm5bo",
      "title": "Skill Seekers v3.0.0 - Universal doc preprocessor for AI systems",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qzm5bo/skill_seekers_v300_universal_doc_preprocessor_for/",
      "author": "Critical-Pea-8782",
      "created_utc": "2026-02-08 21:56:24",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "TL;DR: One command converts docs into any AI format.\n\n  The Problem: Every AI project needs documentation preprocessing:\n\n  ‚Ä¢ RAG pipelines need clean, chunked text\n  ‚Ä¢ AI coding tools need structured knowledge\n  ‚Ä¢ Claude/GPT need formatted skills\n\n  Everyone rebuilds the same scrapers.\n\n  The Solution:\n\n  pip install skill-seekers\n  skill-seekers scrape --config react.json\n\n  16 Output Formats:\n\n   RAG/Vectors: LangChain, LlamaIndex, Chroma, FAISS, Haystack, Qdrant, Weaviate\n\n   AI Coding: Cursor, Windsurf, Cline, Continue.dev\n\n   AI Platforms: Claude, Gemini, OpenAI\n\n   Generic: Markdown\n\n  26 MCP Tools: Your AI agent can now prepare its own knowledge:\n\n  ‚Ä¢ scrape_docs, scrape_github, scrape_pdf\n  ‚Ä¢ package_skill, install_skill\n  ‚Ä¢ And 21 more...\n\n  Stats:\n\n  ‚Ä¢ 58,512 lines of Python\n  ‚Ä¢ 1,852 tests\n  ‚Ä¢ 100 test files\n  ‚Ä¢ 12 example projects\n  ‚Ä¢ 18 integration guides\n\n  Cloud & CI/CD:\n\n  # Upload to S3\n  skill-seekers cloud upload output/ --provider s3 --bucket my-bucket\n\n  # GitHub Action available\n\n  Links:\n\n  ‚Ä¢ GitHub: https://github.com/yusufkaraaslan/Skill_Seekers\n  ‚Ä¢ Website: https://skillseekersweb.com\n  ‚Ä¢ PyPI: pip install skill-seekers\n\n  Just launched v3.0.0 today. Happy to answer any questions!\n",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qzm5bo/skill_seekers_v300_universal_doc_preprocessor_for/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o4bzwzh",
          "author": "Critical-Pea-8782",
          "text": "hi please feel free to leave a any feedback you want and lets talk about it :) ",
          "score": 1,
          "created_utc": "2026-02-08 22:40:44",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}