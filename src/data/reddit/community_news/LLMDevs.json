{
  "metadata": {
    "last_updated": "2026-02-05 09:15:02",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 124,
    "file_size_bytes": 153005
  },
  "items": [
    {
      "id": "1qvtrw7",
      "title": "If RAG is dead, what will replace it?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qvtrw7/if_rag_is_dead_what_will_replace_it/",
      "author": "Normal_Sun_8169",
      "created_utc": "2026-02-04 16:50:23",
      "score": 77,
      "num_comments": 52,
      "upvote_ratio": 0.82,
      "text": "It seems like everyone who uses RAG eventually gets frustrated with it. You end up with either poor results from semantic search or complex data pipelines.\n\nAlso - searching for knowledge is only part of the problem for agents. I‚Äôve seen some articles and posts on X, Medium, Reddit, etc about agent memory and in a lot of ways it seems like that‚Äôs the natural evolution of RAG. You treat knowledge as a form of semantic memory and one piece of a bigger set of memory requirements.¬†\n\nThere was a paper published from Google late last year about self-evolving agents and another one talking about adaptive agents.\n\nIf you had a good solution to memory, it seems like you could get to the point where these ideas come together and you could use a combination of knowledge, episodic memory, user feedback, etc to make agents actually learn.\n\nSeems like that could be the future for solving agent data. Anyone tried to do this?¬†",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qvtrw7/if_rag_is_dead_what_will_replace_it/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3kg4v0",
          "author": "qa_anaaq",
          "text": "RAG isn‚Äôt dead. It‚Äôs perfectly fine and just needs to be used well. Everyone believes context graphs are the next trillion dollar industry. Context graph management at runtime is another flavor of RAG. \n\nRemember that RAG isn‚Äôt a narrow term. If something is pulled from somewhere to augment generation, it‚Äôs RAG.",
          "score": 123,
          "created_utc": "2026-02-04 17:48:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3luvt1",
              "author": "isthatashark",
              "text": "The challenge with the name \"RAG\" is that so many people use it as a shorthand to describe semantic search over chunked documents in a vector database. I think the days where you can built any sort of meaningful AI application with that approach are behind us.\n\nAs a pattern, retrieving context and using it to augment the LLM's generation is here to stay.",
              "score": 16,
              "created_utc": "2026-02-04 21:44:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3obe76",
                  "author": "FiddlyDink",
                  "text": "What is replacing chunked documents in a vector database for semantic search?",
                  "score": 4,
                  "created_utc": "2026-02-05 06:33:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mxhx9",
              "author": "FoldedKatana",
              "text": "Yeah I'm using graph rag for a client and it works great if the data is static.",
              "score": 3,
              "created_utc": "2026-02-05 01:10:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3l8dww",
              "author": "valuat",
              "text": "Everyone who?",
              "score": 5,
              "created_utc": "2026-02-04 19:56:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3na7fb",
              "author": "SUCK_MY_DICTIONARY",
              "text": "Oh I love the way this guy fucking thinks YES.\n\nWhat is your opinion on MoE? I want to know",
              "score": 1,
              "created_utc": "2026-02-05 02:23:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ks60y",
              "author": "howardhus",
              "text": "rag was never even alive.\n\nRag ist pulling chunks in a half assed vector search and letting some llm hallucinate some coherent sentence from it. The selling point was the LLM faking confidence. Worked just like in the real world..\n\nwas never great in theory but peopel were flashed as they saw some very self confident human readable answer \"here is the perfect answer to your question!\"\n\nthen you correct it: \"yes you are right! i lied! here is the actual correct answer (this time for real!)\"\n\nRAG was only great before the word \"hallucination\" also became a thing.",
              "score": -15,
              "created_utc": "2026-02-04 18:42:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3l0y0o",
                  "author": "Agreeable-Market-692",
                  "text": "This is a very outdated view of RAG. Hundreds of papers and dozens of models later and things are much improved.",
                  "score": 9,
                  "created_utc": "2026-02-04 19:22:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3lml96",
              "author": "Dense_Gate_5193",
              "text": "yeah RAG is neat but Graph-RAG is where it is at.\n\nit‚Äôs why i built nornic. \n\nhttps://github.com/orneryd/NornicDB\n\n0.17ms p95 transacted writes. \n\nneo4j drop-in replacement that‚Äôs 3-50x faster depending on operation.\n\nit also has a qdrant compatible grpc endpoint and is ~40% faster than qdrant proper\n\ngpu accelerated vector embedding search or cpu IVF-HNSW, tunable. \n\nmanaged vector embeddings mean you don‚Äôt need a remote model to generate embeddings for you. same for reranking. it runs an in-memory model for reranking.",
              "score": -11,
              "created_utc": "2026-02-04 21:05:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kfb2f",
          "author": "coffee-praxis",
          "text": "Agent memory alone doesn‚Äôt cut it. Let‚Äôs say you want grounded facts from a document source that‚Äôs too big for context window. You can‚Äôt just shove it all in ‚Äúagent memory‚Äù unless you retrieve the correct bits of it somehow. Now you‚Äôre back to RAG.",
          "score": 33,
          "created_utc": "2026-02-04 17:44:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ltfuv",
              "author": "isthatashark",
              "text": "I hear more people talking about this as semantic memory and thinking of it as one requirement in a bigger set of agent memory requirements rather than just RAG.",
              "score": 3,
              "created_utc": "2026-02-04 21:37:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3kkqao",
              "author": "NorCalZen",
              "text": "Sorry if this a naive question, but could you use a database solution like ScyllaDB to achieve the right results ?",
              "score": 1,
              "created_utc": "2026-02-04 18:08:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3klea2",
                  "author": "coffee-praxis",
                  "text": "RAG is ‚Äú**retrieval** augmented generation‚Äù. Any DB qualifies.",
                  "score": 21,
                  "created_utc": "2026-02-04 18:11:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3k9bq3",
          "author": "ethan000024",
          "text": "I‚Äôve been hearing more about agent learning lately too. Agree it‚Äôs a promising idea but also mostly hype when I‚Äôve tried to dig into it. The two most interesting projects I‚Äôve seen on this lately are Agent Lightning and Hindsight. Two very different approaches, Agent Lightning relies more on file system. Hindsight is closer to what you described with combining knowledge, episodic memory, etc. Both have learning aspects to it.¬†",
          "score": 12,
          "created_utc": "2026-02-04 17:16:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kgx9d",
              "author": "Normal_Sun_8169",
              "text": "I just looked those projects up. Very cool stuff. The learning demo they have on the GitHub repo for Hindsight is exactly what I was trying to describe. Reinforcement learning over agent memory to form mental models seems super powerful. Thanks for the info!",
              "score": 3,
              "created_utc": "2026-02-04 17:51:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kcja7",
          "author": "florinandrei",
          "text": "> If RAG is dead, what will replace it?\n\nTATTER\n\nTransformer-Attention Token Tangling for Eventually Rambling",
          "score": 22,
          "created_utc": "2026-02-04 17:31:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kielt",
              "author": "Floppy_Muppet",
              "text": "I believe \"token tangling\" is illegal in several states.",
              "score": 20,
              "created_utc": "2026-02-04 17:58:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k912z",
          "author": "Emma_4_7",
          "text": "The most annoying thing about agent memory right now is how many ‚Äúmemory‚Äù projects on GitHub are basic RAG solutions under the covers. That‚Äôs nice you can remember where I work after 10 whole messages.",
          "score": 16,
          "created_utc": "2026-02-04 17:15:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kgims",
              "author": "Normal_Sun_8169",
              "text": "Yeah, I‚Äôve noticed this too.",
              "score": 1,
              "created_utc": "2026-02-04 17:49:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3kig2c",
              "author": "Original_Finding2212",
              "text": "What do you think about this?\n\nQq folder here:\n\n[https://github.com/OriNachum/autonomous-intelligence](https://github.com/OriNachum/autonomous-intelligence)\n\nAnd add a star if you like or want to support üôèüèø\n\nhttps://preview.redd.it/r8euxdeboihg1.jpeg?width=2752&format=pjpg&auto=webp&s=050a9da330c4b9c4c558d792e243f8703b05dbfe",
              "score": -14,
              "created_utc": "2026-02-04 17:58:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3kvyj2",
                  "author": "leonjetski",
                  "text": "‚ÄúMapping sturucted outitites and complex relationships between a√¶√∞capta.‚Äù",
                  "score": 11,
                  "created_utc": "2026-02-04 18:59:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3mv41a",
          "author": "jba1224a",
          "text": "‚ÄúLet me just shove this shit into a vector database.  We don‚Äôt need to worry about chunking.  What‚Äôs an embedding model?‚Äù\n\n‚Ä¶.\n\n‚ÄúWhy do my results suck.  RAG is frustrating‚Äù",
          "score": 5,
          "created_utc": "2026-02-05 00:56:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mtaif",
          "author": "Ok-Owl-7515",
          "text": "I don‚Äôt think RAG is dead. Vector-only semantic search is what usually disappoints. What‚Äôs replacing it (for me) is hybrid retrieval + memory architecture: FTS/keyword first, then vectors only as fallback, union + rerank, and always return retrieval diagnostics (which backend, hit counts, scores, latency).\n\nThe biggest unlock is in considering embeddings/indexes as versioned, reproducible derived artifacts (model/version + source hash), and controlling changes via a small golden set to prevent silent changes to results. Retrieval is just one ‚Äúmemory surface,‚Äù alongside structured state/ledgers and episodic logs.",
          "score": 3,
          "created_utc": "2026-02-05 00:46:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kuy2i",
          "author": "metaphorm",
          "text": "my view is that RAG is still a highly relevant technique and the problems it has with accuracy are the current leading edge of LLM application development. agent memory might be a good approach for some classes of problems. \"deep\" agents might be another approach that works, i.e. an agent that has access to tools that allow it to introspect its own results. ",
          "score": 4,
          "created_utc": "2026-02-04 18:54:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3l78ss",
              "author": "techhead57",
              "text": "Its a tool in the toolbox. When LLMs came out rag was the only tool. Now there are all kinds of interfaces being hooked up to them and RAG has all kinds of fancy alternatives that are basically trying to do the same thing but better. And models are getting better at using this kind of input context because theyre being trained with tools use now.",
              "score": 4,
              "created_utc": "2026-02-04 19:51:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3k3xuy",
          "author": "Fragrant_Western4730",
          "text": "I don‚Äôt know about the rest of it, but I definitely experienced the shortcomings of RAG for searching documents. Cool thought. Interested to hear what people think about this. Upvoted.",
          "score": 2,
          "created_utc": "2026-02-04 16:51:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kg1c3",
              "author": "Normal_Sun_8169",
              "text": "Thanks!",
              "score": 1,
              "created_utc": "2026-02-04 17:47:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3kerpl",
          "author": "onetimeiateaburrito",
          "text": "I dunno man. I've spent a little bit trying to get a [RAPTOR](https://arxiv.org/abs/2401.18059) style system going and maybe it'll be cool? Who knows. I'm not a programmer and have no background in CS or ML. Just arguing with myself and Claude until something does something without spitting error codes. Then doing the same thing to see what's silently failing.",
          "score": 2,
          "created_utc": "2026-02-04 17:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l5qqf",
          "author": "WolfeheartGames",
          "text": "The problem is retrieval. How is the agent supposed to know what I'd available for lookup? It must be told.\n\nLet's say we have a list of things the agent can retrieve. If we give it to the agent it will hyper fixate on this and it causes new failure modes.\n\nSo then we need to monitor the inputs and outputs and see if we should be injecting information from retrieval in to the context window. This requires a signal of some kind. Either LLM, BERT, or otherwise.",
          "score": 2,
          "created_utc": "2026-02-04 19:44:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mgzqd",
              "author": "ai-tacocat-ia",
              "text": "It's really just a taxonomy problem. Is easy to think of it like a file system. \"Tell me what folders are in the current directory. I want to see the files and subfolders in this list of directories. Now show me what's in these subdirs.\"\n\nAlso, \"show me the paths of files whose contents contain these search terms\". Then let the LLM list the files it wants to pull.\n\nObviously doesn't need to be files - can be categories, subcategories, filter by tags, etc. Basically, give LLMs the same tools you enjoy as a human to find things.",
              "score": 1,
              "created_utc": "2026-02-04 23:38:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mnmdb",
                  "author": "WolfeheartGames",
                  "text": "That is not how real deployments usually work. It's okay for like a call center bot where the company will invest a lot in the docs for a RAG, but even then it's not enough. How does it know that a question is even contained in its RAG? How does it know how to search for it if the user gives terrible keywords, how does it know if should look elsewhere? It's not a listable directory to explore to gain insight from, and that's the problem. The agent only knows whats in it's system prompt until it's found something, and then it's still ignorant about potentially other useful things it didn't find. This breaks down further when data is less organized, like code or loose pdfs\n\nBut the fact that you're comparing RAG lookup to a directory is concerning. Vector and graph databases do not work like that at all. The problem of retrieval is partially because they don't work like that.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3liewe",
          "author": "DataCentricExpert",
          "text": "RAG isn‚Äôt dead, it‚Äôs just being asked to do too much.  \ngents break when you expect retrieval to behave like memory. What replaces it isn‚Äôt ‚Äúbetter RAG,‚Äù it‚Äôs layered memory...AG becomes infrastructure, not the strategy.",
          "score": 2,
          "created_utc": "2026-02-04 20:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lmpd1",
          "author": "xFloaty",
          "text": "every time your agent calls a tool to search for context, it‚Äôs RAG",
          "score": 2,
          "created_utc": "2026-02-04 21:05:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3nbxi9",
          "author": "hettuklaeddi",
          "text": "dead?!? RAG doesn‚Äôt even have the sniffles \n\nmaybe it‚Äôs dead to script kiddies, that‚Äôs fine",
          "score": 2,
          "created_utc": "2026-02-05 02:33:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3kz4oq",
          "author": "fabkosta",
          "text": "Downvoted. We had enough \"RAG is dead\" posts here. It's getting silly.",
          "score": 3,
          "created_utc": "2026-02-04 19:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l6m4k",
          "author": "AdOwn10",
          "text": "Ya the RAG people changed what ‚ÄúRAG‚Äù means so RAG isn‚Äôt dead. Vector database? No! We are not talking about ALL ways you get retrieve information to augment a context window.",
          "score": 2,
          "created_utc": "2026-02-04 19:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mc77z",
          "author": "andrew_kirfman",
          "text": "Rag isn‚Äôt 100% dead, but it‚Äôs definitely been impacted by agentic search and agent skills getting so good.  \n\nI only use semantic search for dart at a dartboard type searches.  Everything else is agentic search.",
          "score": 2,
          "created_utc": "2026-02-04 23:12:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3obzoo",
              "author": "Visionexe",
              "text": "What is Agentic search?",
              "score": 1,
              "created_utc": "2026-02-05 06:38:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3klpky",
          "author": "vagobond45",
          "text": "Knowledge Graphs combined with Answer Rag Audit should replace RAG",
          "score": 1,
          "created_utc": "2026-02-04 18:13:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l5cw1",
          "author": "Miclivs",
          "text": "Agentic search works really well when the agent knows what to look for.",
          "score": 1,
          "created_utc": "2026-02-04 19:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l9lk3",
          "author": "llOriginalityLack367",
          "text": "Mean pooling.\n\nMean pooling.\n\nMean pooling.",
          "score": 1,
          "created_utc": "2026-02-04 20:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mc5ln",
          "author": "Flat_Dependent3195",
          "text": "Can you share the link for the paper you mentioned?",
          "score": 1,
          "created_utc": "2026-02-04 23:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3n8d42",
          "author": "New-Unit-3900",
          "text": "Properly structured ontologies",
          "score": 1,
          "created_utc": "2026-02-05 02:12:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nrac8",
              "author": "smm_h",
              "text": "like what",
              "score": 1,
              "created_utc": "2026-02-05 04:05:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nhjmt",
          "author": "GoodEnoughSetup",
          "text": " In my experience, database solutions like ScyllaDB can definitely be part of a broader strategy to replace RAG. By incorporating a database for fast access to relevant data, you might enhance the context in which generative models operate, similar to how semantic memory aims to streamline information retrieval. Have you looked into any specific frameworks that could mesh well with that approach?",
          "score": 1,
          "created_utc": "2026-02-05 03:04:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o8gid",
          "author": "fooz42",
          "text": "It's a garbage in, garbage out problem. You can reduce the surface area of the generation to something very small in scope, or you can increase the quality of the included information in the context to improve the summary.",
          "score": 1,
          "created_utc": "2026-02-05 06:08:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3objra",
          "author": "iAM_A_NiceGuy",
          "text": "Compression",
          "score": 1,
          "created_utc": "2026-02-05 06:34:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3l4jqo",
          "author": "Able_Penalty8856",
          "text": "I also got frustrated with RAG. My plan is to study Unsloth to explore fine-tuned models. I'm aware that I'll likely face several challenges.",
          "score": 0,
          "created_utc": "2026-02-04 19:39:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oeyq1",
              "author": "Pixelmixer",
              "text": "This simply isn‚Äôt possible for a lot of workflows. As a super simple toy example; imagine you want to search text comments posted by users and provide that to an LLM. Fine-tuning could potentially work as a first pass (let‚Äôs also assume that the fine-tuned model has perfect retrieval for the purpose of this example), but even then you‚Äôd need to retrain it each time a user posts a new comment or changes their comment. It‚Äôs just too much, unfortunately.",
              "score": 1,
              "created_utc": "2026-02-05 07:04:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq4ted",
      "title": "Building opensource Zero Server Code Intelligence Engine",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/5o9xx8n8k9gg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-29 10:15:13",
      "score": 59,
      "num_comments": 37,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq4ted/building_opensource_zero_server_code_intelligence/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ewgla",
          "author": "Key-Contact-6524",
          "text": "Gorgeous",
          "score": 3,
          "created_utc": "2026-01-29 14:14:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f2o22",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏èü´†",
              "score": 1,
              "created_utc": "2026-01-29 14:46:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gn944",
          "author": "Happythen",
          "text": "Yea, you killed it with the visualizations, great work! Working on the same thing right now, implementing Graph RAG. Fun space right now for sure.",
          "score": 3,
          "created_utc": "2026-01-29 19:01:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2go6fj",
              "author": "DeathShot7777",
              "text": "Yup. Gets painful though when LLM starts going on a spree quering the graph and ends up its context window. Hard to solve but very rewarding",
              "score": 1,
              "created_utc": "2026-01-29 19:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jh12l",
          "author": "foobarrister",
          "text": "Very well done. How are you building the graph? Looks like Leiden based . .\n\n\nCurious why you didn't use tree-sitter or language specific tools like JavaParser for Java or Roslyn for dotnet etc..¬†\n\n\nWouldn't they give you a better nodes and relationships vs heuristic approach like Leiden?",
          "score": 2,
          "created_utc": "2026-01-30 03:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd95j",
              "author": "DeathShot7777",
              "text": "I m using Tree sitters. Simplified explanation : Extract IMPORTS, CALLS, DEFINES relations of each file, this already creates an accurate knowledge graph. Next use leidens algo to divide it into clusters and label those clusters ( for example AuthHandler cluster ) next find out the entrypoint of each service and DFS into the CALL chain to get the process maps in each cluster.\n\nSo the graph is quite accurate for static analyses, for the stuff like dynamic imports, runtime stuff, the cluster and process map handles most of it. These also saves a lot of tokens since the tools itself r intelligent not depending too much on LLM figuring stiff out.",
              "score": 1,
              "created_utc": "2026-01-30 07:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2grvi1",
          "author": "talltad",
          "text": "I like this, I know nothing about Software Dev but I'm working on a few things right now so I guess I'm vibe coding.  I don't know if there's a use case within this that you're looking for but if there is I'd be glad to help if needed.  It's clear this is a substantial amount of work so best of luck man!",
          "score": 1,
          "created_utc": "2026-01-29 19:23:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gt9tk",
              "author": "DeathShot7777",
              "text": "Indeed its a substantial amount of work üò≠ but good kind of pain ü´†. \n\nIf u would try out the MCP and plug it into your vibecoding tool for example cursor , claude code, etc load up a project into it and ask the ai about the codebase or how it works or the architecture, it should be able to go into full technical and architectural depth. Knowing the architecture even if u dont know development will help a lot in your vibecoding journey",
              "score": 2,
              "created_utc": "2026-01-29 19:29:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2hxq37",
                  "author": "talltad",
                  "text": "Cool and thanks man, I'll give it a try",
                  "score": 2,
                  "created_utc": "2026-01-29 22:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gweo8",
          "author": "SloSuenos64",
          "text": "This is so cool! Hooking up my Cursor project now....",
          "score": 1,
          "created_utc": "2026-01-29 19:44:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h59a4",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏è try creating detailed documentation using dumber model with gitnexus vs sota opus without gitnexus. For me it worked surprisingly well",
              "score": 3,
              "created_utc": "2026-01-29 20:26:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h87ok",
                  "author": "SloSuenos64",
                  "text": "Game-changer! I used to grep through every file and hope I didn't miss a dynamic import. Now, I can see the actual dependency graph!",
                  "score": 1,
                  "created_utc": "2026-01-29 20:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2h9i9u",
              "author": "SloSuenos64",
              "text": "Immediately found Structural Redundancy issues. Thank you!",
              "score": 2,
              "created_utc": "2026-01-29 20:47:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hh2cp",
                  "author": "DeathShot7777",
                  "text": "Your comments made my day. Thanks for trying it out ‚ù§Ô∏èü´†",
                  "score": 2,
                  "created_utc": "2026-01-29 21:23:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i2rs0",
          "author": "kfawcett1",
          "text": "~~Is this sending my entire codebase through your servers? Are you storing the data?~~\n\nnvm, found the answer.\n\n* All processing happens in your browser\n* No code uploaded to any server\n* API keys stored in localStorage only\n* Open source‚Äîaudit the code yourself",
          "score": 1,
          "created_utc": "2026-01-29 23:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3ohc",
              "author": "DeathShot7777",
              "text": "Client sided everything. So costs me 0 to deploy, so u all get it for free ü´†. Just trying to take it to a product stage from the current cool demo stage",
              "score": 2,
              "created_utc": "2026-01-29 23:15:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2i5x67",
                  "author": "kfawcett1",
                  "text": "how does it perform with 1M+ LOC codebases?",
                  "score": 2,
                  "created_utc": "2026-01-29 23:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2jwsky",
          "author": "Repulsive-Memory-298",
          "text": "I mean it looks cool but is it useful to you? The examples in the demo vid do not seem very helpful at a glance",
          "score": 1,
          "created_utc": "2026-01-30 05:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kb8wb",
              "author": "DeathShot7777",
              "text": "For me, it helps me understand repos better than DeepWiki right now. \n\nAfter I am done with File watch feature it should be able to work in background increasing accuracy of coding agents without me having to hooking up the website every time",
              "score": 1,
              "created_utc": "2026-01-30 07:24:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2kbcea",
              "author": "DeathShot7777",
              "text": "Maybe i should have shown the MCP working in cursor in the video",
              "score": 1,
              "created_utc": "2026-01-30 07:25:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k0uz7",
          "author": "Striking-Bluejay6155",
          "text": "Very nice, this amounts to a knowledge graph. Which visualization library are u using to visualize this?",
          "score": 1,
          "created_utc": "2026-01-30 06:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kaw3z",
              "author": "DeathShot7777",
              "text": "Sigma js + ForceAtlas2 and some custom logic and hit and trial to reduce the clumping up of nodes",
              "score": 2,
              "created_utc": "2026-01-30 07:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kuaqt",
                  "author": "Striking-Bluejay6155",
                  "text": "Thank you",
                  "score": 2,
                  "created_utc": "2026-01-30 10:16:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k977b",
          "author": "slowlearningovrtime",
          "text": "Couldn‚Äôt figure out the local LLM connection",
          "score": 1,
          "created_utc": "2026-01-30 07:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kb5dj",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/hnk1zn8mufgg1.png?width=407&format=png&auto=webp&s=67c054d2a1b0cb465b290b58f4ae823392167066\n\nblocked by my AV",
          "score": 1,
          "created_utc": "2026-01-30 07:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kbi6q",
              "author": "DeathShot7777",
              "text": "Huh. I have no idea why this happened even though its client sided",
              "score": 1,
              "created_utc": "2026-01-30 07:26:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ksgok",
          "author": "snirjka",
          "text": "Well done üëèüèª\nCool af",
          "score": 1,
          "created_utc": "2026-01-30 10:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ksygl",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏è trying to make this into a product from current cool demo stage ü§û",
              "score": 2,
              "created_utc": "2026-01-30 10:04:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ktlqw",
          "author": "redvox27",
          "text": "Looks incredible man! I'll check it out later today, and share my thoughts if you find that helpful",
          "score": 1,
          "created_utc": "2026-01-30 10:10:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l1jiu",
              "author": "DeathShot7777",
              "text": "Thanks. Would be really helpful",
              "score": 1,
              "created_utc": "2026-01-30 11:18:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rsp6h",
          "author": "Fresh_State_1403",
          "text": "this looks fascinating. is it practical too?",
          "score": 1,
          "created_utc": "2026-01-31 11:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rstuz",
              "author": "DeathShot7777",
              "text": "Haiku 4.5 was able to produce better architecture docs compared to opus 4.5 using gitnexus MCP in cursor. So has potential",
              "score": 1,
              "created_utc": "2026-01-31 11:09:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38vvhm",
          "author": "vidibuzz",
          "text": "Once everyone figures out that knowledge graph is the only smart way to channel vector data for valuable output, apps like this will be gold. Nice work. This looks amazing.   \n  \nWould be great for adding relevant visuals to a VidiCommerce shopping app.",
          "score": 1,
          "created_utc": "2026-02-02 23:21:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38wrkh",
              "author": "DeathShot7777",
              "text": "Thanks. Whats vidicommerce?",
              "score": 1,
              "created_utc": "2026-02-02 23:26:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qul18d",
      "title": "NotebookLM For Teams",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/b85n399898hg1",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:00",
      "score": 41,
      "num_comments": 3,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qul18d/notebooklm_for_teams/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3axdaf",
          "author": "Otherwise_Wave9374",
          "text": "This looks like a great niche community. The \"agents that act, not just chat\" framing is exactly the right line to draw.\n\nIf you end up pinning a starter guide, I would include basics like tool permissioning, memory vs no-memory defaults, and how to keep agent loops bounded so people do not burn tokens endlessly.\n\nI have a few writeups on agent guardrails and patterns here: https://www.agentixlabs.com/blog/",
          "score": 3,
          "created_utc": "2026-02-03 07:07:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jnqad",
          "author": "tsquig",
          "text": "Similar flexibility here with uploading / source inputs, but a few more options to generate outputs. [NotebookLM but for business](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:37:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gwyij",
          "author": "DAlmighty",
          "text": "They have also been doing crazy advertising on Reddit. You‚Äôd think that they would just pay the platform for the opportunity already.",
          "score": 0,
          "created_utc": "2026-02-04 03:45:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsw6tg",
      "title": "We are not the same",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/glett1s5dvgg1.png",
      "author": "alvinunreal",
      "created_utc": "2026-02-01 11:35:03",
      "score": 38,
      "num_comments": 7,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qsw6tg/we_are_not_the_same/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2yj0zt",
          "author": "brightheaded",
          "text": "A crumb of context sir",
          "score": 9,
          "created_utc": "2026-02-01 12:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yo43w",
              "author": "HumanDrone8721",
              "text": "Or a crust of context.",
              "score": 4,
              "created_utc": "2026-02-01 12:54:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yo5kf",
              "author": "Mythril_Zombie",
              "text": "Apparently something is different from something else.",
              "score": 2,
              "created_utc": "2026-02-01 12:55:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o30ca32",
          "author": "megatronus8010",
          "text": "How does the craber news work",
          "score": 1,
          "created_utc": "2026-02-01 18:02:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30f83c",
              "author": "alvinunreal",
              "text": "It's in sync with HackerNews submission - when someone submits there, it appears on crabbernews.\n\nBut difference is on HackerNews humans upvote, comment and discuss; on Crabbernews it's upto AI models.\n\nThis makes \"top\" posts different; For example are humans biased towards certain types of news or now...\n\nThat's the goal of this website, I'm currently adding core models to review new posts, and decide which to upvote but anyone can connect their openclaw agents too to participate.",
              "score": 3,
              "created_utc": "2026-02-01 18:15:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o353kdv",
                  "author": "germo20",
                  "text": "I think many of us would appreciate having this comment in the actual post, instead of just a comment.",
                  "score": 3,
                  "created_utc": "2026-02-02 11:57:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30nkzk",
          "author": "Ladder-Bhe",
          "text": "AGENTSÔºåPlease include historical messages when speaking, so we can see how this idea was generated, rather than the result of human intervention.",
          "score": 1,
          "created_utc": "2026-02-01 18:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrqdds",
      "title": "[P] Trained a 67M-parameter transformer from scratch on M4 Mac Mini - 94% exact-match accuracy on CLI command generation",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "author": "Great_Fun7005",
      "created_utc": "2026-01-31 02:47:54",
      "score": 34,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "I trained a small language model end-to-end on consumer hardware (M4 Mac Mini, 24GB RAM) and achieved 94% exact-match accuracy on CLI command generation.\n\n**Key details:**\n\n* Model: 67M parameters (12 layers, 512 hidden dim, RoPE, RMSNorm, SwiGLU)\n* Training: 204.8M tokens, \\~13 hours pretraining + 4 minutes fine-tuning\n* Hardware: Apple Silicon MPS, no discrete GPU\n* Cost: \\~$0.50 in electricity\n* Evaluation: Strict exact-match (no partial credit)\n\n**What worked:**\n\n* Modern architectural components (RoPE, RMSNorm, SwiGLU) are effective even at small scale\n* Marker-based output contracts for state signaling\n* Memory-mapped data loading to handle 200M+ tokens on limited RAM\n* Continual learning with evaluation gates that reject harmful updates\n\n**What failed (and why it matters):** All 6% of failures shared one pattern: early termination on symbol-dense patterns (regex, pipes, redirects). Not a reasoning failure‚Äîa data coverage problem. Adding \\~500 targeted examples would likely fix most of these.\n\n**Takeaway:** For narrow, exact tasks with controllable domains, small models trained from scratch can be practical, inspectable, and cheap to iterate on. Data quality mattered more than scale.\n\nFull technical writeup with training logs, failure analysis, and code: [https://geddydukes.com/blog/tiny-llm](https://geddydukes.com/blog/tiny-llm)\n\nGitHub: [https://github.com/geddydukes/tiny\\_llm](https://github.com/geddydukes/tiny_llm)\n\nHappy to answer questions about training dynamics, architecture choices, or the evaluation setup.",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2sfg8j",
          "author": "radarsat1",
          "text": "The implementation is fairly clean, good job. I have a question though, this seems to be an unusual TransformerBlock forward function, did you get this from somewhere or is it a mistake or maybe your own idea?\n\n```\n¬† ¬† ¬† ¬† h1 = self.norm1(x)\n¬† ¬† ¬† ¬† h2 = self.norm2(x)\n\n¬† ¬† ¬† ¬† attn_out = self.attn(h1, attn_mask, rope_cos, rope_sin)\n¬† ¬† ¬† ¬† mlp_out = self.mlp(h2)\n¬† ¬† ¬† ¬†¬†\n¬† ¬† ¬† ¬† return x + self.dropout(attn_out) + self.dropout(mlp_out)\n```\n\nI'm referring to how it adds `attn_out` and `mlp_out` instead of feeding `attn_out` into `mlp`.",
          "score": 4,
          "created_utc": "2026-01-31 13:56:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2th04w",
              "author": "Great_Fun7005",
              "text": "Thanks, appreciate it. This is an intentional pre-norm parallel residual block: x + attn(norm(x)) + mlp(norm(x)). Attention and MLP run in parallel off the same residual stream (with separate RMSNorm) and are summed in a single update. It‚Äôs a known Transformer variant used in several modern decoder-only models, not a mistake or a novel invention.",
              "score": 2,
              "created_utc": "2026-01-31 17:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uinbj",
                  "author": "radarsat1",
                  "text": "Do you know offhand which variants use this? I actually checked the Llama code before posting just in case I was saying something dumb, but it seems to work there as I am used to. I guess I can plumb the transformers library a bit to find out but I'm curious about it, if you happen to know.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2q4kfm",
          "author": "Dense_Gate_5193",
          "text": "thanks i am training SLMs for work and this is helpful",
          "score": 2,
          "created_utc": "2026-01-31 02:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4qfl",
              "author": "Great_Fun7005",
              "text": "Glad to provide a helpful resource!",
              "score": 1,
              "created_utc": "2026-01-31 02:57:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2u5cre",
          "author": "HealthyCommunicat",
          "text": "Woah, I was literally talking about how bad some models are with just basic commands, like hooking up glm 4.7 flash to codex cli and ask it to find a file‚Ä¶ watch it mess up the ‚Äúfind . -name ‚Äú___‚Äù‚Äù bash syntax 7 times before getting it right, or even editing a file i usually watch it struggle going through multiple different attempt methods until it just finally ends up on echoing it into the file lol\n\nThis is actually really cool, if someone was to take ur base and add upon it i‚Äôd totally use it",
          "score": 1,
          "created_utc": "2026-01-31 19:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u8met",
              "author": "Great_Fun7005",
              "text": "Feel free to add onto it! I have some future iterations planned but have a couple of projects I‚Äôm working on before I‚Äôll get back to this one.",
              "score": 1,
              "created_utc": "2026-01-31 19:19:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqgpt8",
      "title": "We did not see real prompt injection failures until our LLM app was in prod",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qqgpt8/we_did_not_see_real_prompt_injection_failures/",
      "author": "Zoniin",
      "created_utc": "2026-01-29 18:28:38",
      "score": 22,
      "num_comments": 27,
      "upvote_ratio": 0.97,
      "text": "I am a college student. Last summer I worked in SWE in the financial space and helped build a user facing AI chatbot that lived directly on the company website.\n\nBefore shipping, I mostly thought prompt injection was an academic or edge case concern. Then real users showed up.\n\nWithin days, people were actively trying to jailbreak the system. Mostly curiosity driven it seemed, but still bypassing system instructions, surfacing internal context, and pushing the model into behavior it was never supposed to exhibit.\n\nWe tried the usual fixes. Stronger system prompts, more guardrails, traditional MCP style controls, etc. They helped, but none of them actually solved the problem. The failures only showed up once the system was live and stateful, under real usage patterns you cannot *realistically* simulate in testing.\n\nWhat stuck with me is how easy this is to miss right now. A lot of developers are shipping LLM powered features quickly, treating prompt injection as a theoretical concern rather than a production risk. That was exactly my mindset before this experience. If you are not using AI when building (for most use cases) today, you are behind, but many of us are unknowingly deploying systems with real permissions and no runtime security model behind them.\n\nThis experience really got me in the deep end of all this stuff and is what pushed me to start building towards a solution to hopefully enhance my skills and knowledge along the way. I have made decent progress so far and just finished a website for it which I can share if anyone wants to see but I know people hate promo so I won't force it lol. My core belief is that prompt security cannot be solved purely at the prompt layer. You need runtime visibility into behavior, intent, and outputs.\n\nI am posting here mostly to get honest feedback.\n\nFor those building production LLM systems:\n\n* does runtime prompt abuse show up only after launch for you too\n* do you rely entirely on prompt design and tool gating, or something else\n* where do you see the biggest failure modes today\n\nHappy to share more details if useful. Genuinely curious how others here are approaching this issue and if it is a real problem for anyone else.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qqgpt8/we_did_not_see_real_prompt_injection_failures/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2gnfbv",
          "author": "tom-mart",
          "text": "We will have to teach you kids evwrythong from scratch\n\nRule number one of any public facing endpoint is that it will be abused and it will be exploited and it will be hacked. Those are not ifs, those are facts. When you create any public facing service, what it does is really a secondary concern. Your main issue is to think of any possible threat and mitigate it. For instance, I would never create anything that is public facing and doesn't require an account. Then you can detect harmful behaviour just by adding observing agent that looks at the chat interaction without being engaged in it and is able to lock user out when it detects any foul play. There are many other ways to implement basic chat security.",
          "score": 13,
          "created_utc": "2026-01-29 19:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2guo70",
              "author": "Zoniin",
              "text": "You are definitely not wrong on the core principle. Public endpoints will always be abused. The part that surprised me was how much harder this becomes with LLMs compared to traditional services. Auth and rate limiting help, but most of the failures we saw were not obviously malicious and came from normal users probing behavior rather than attacking infra. Observing agents and heuristics help too, sure, but they still rely on assumptions about intent that break down once prompts get stateful and context bleeds across turns. That gap between traditional endpoint security and model behavior is what caught me off guard and what I am trying to reason about more deeply.",
              "score": 4,
              "created_utc": "2026-01-29 19:36:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ha1vr",
          "author": "Strong_Worker4090",
          "text": "Yep, this matches what I‚Äôve seen too.\n\nReal users instantly go ‚Äúlol can I jailbreak it‚Äù the second it‚Äôs live. UAT almost never catches that because people are busy/timeboxed and test the happy path, not ‚Äúlet me spend 2 hours trying to break it and set it on fire.‚Äù\n\nWhat actually helped for us wasn‚Äôt stronger prompts, it was moving controls out of the prompt layer:\n\n* **Treat the LLM like untrusted input**: ***EVERY*** tool call is server-side validated (auth checks, allowlists, strict schemas).\n* **Least privilege**: split tools into read-only vs write vs ‚Äúdangerous‚Äù, and keep most sessions on the lowest tier.\n* **Data controls**: redact/classify sensitive stuff before it hits the model, and block obvious ‚Äúdump the context / dump the doc‚Äù outputs. I have a couple free tools I've been using for this.\n* **Runtime visibility**: log tool calls + retrievals, rate limit probing patterns, and add a few (as many as you can) jailbreak tests that run continuously on real flows.\n\nPrompts still matter, but more as polish. The security model has to be runtime + permissions + data handling.\n\nBiggest failure modes I‚Äôve seen: RAG leaks, tool misuse, and privilege confusion (‚Äúuser asked‚Äù != ‚Äúuser is allowed‚Äù).",
          "score": 6,
          "created_utc": "2026-01-29 20:50:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ni88w",
              "author": "Visionexe",
              "text": "How is this not complete and utter common sense?¬†",
              "score": 3,
              "created_utc": "2026-01-30 18:49:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nltkd",
                  "author": "Strong_Worker4090",
                  "text": "Yea I mean I think it is common sense at the conceptual level. The problem is execution. Most teams ship the demo version (prompt rules and a couple quick checks), then prod happens and you realize you need real gating, real permissioning, real logging, and real data handling. Each of those adds real engineering time and product friction, so it gets deprioritized until something breaks. That's showbiz baby",
                  "score": 1,
                  "created_utc": "2026-01-30 19:05:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hgzgp",
          "author": "darkwingdankest",
          "text": ">thought prompt injection was an academic or edge case concern \n\noh boy",
          "score": 6,
          "created_utc": "2026-01-29 21:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hetwb",
          "author": "Long_Complex_4395",
          "text": "Prompt injection and jailbreaks will always be present once it‚Äôs out in the wild and one should prepare for it.\n\nOne thing to do is to test for known vulnerabilities before deploying to production, then brace for the unknown because people will always want to know how far they can go",
          "score": 2,
          "created_utc": "2026-01-29 21:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hbfe7",
          "author": "codemuncher",
          "text": "It‚Äôs a classic that developers never think of adversarial uses of their systems. ‚ÄúOh what do you mean someone pressed every key on the keyboard at once, and the is deleted everything? That‚Äôs supposed to be impossible!‚Äù\n\nThis is a tale as old as time.",
          "score": 1,
          "created_utc": "2026-01-29 20:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hjt3n",
          "author": "kubrador",
          "text": "yep, users are absolutely feral once they get access. the prompt layer stuff is basically security theater. it's like locking your front door while leaving the windows open.\n\n\n\nruntime visibility is the real move though. most teams i've talked to are basically doing nothing and hoping their guardrails hold, which is wild. they break immediately under actual adversarial use.",
          "score": 1,
          "created_utc": "2026-01-29 21:36:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmly",
              "author": "Zoniin",
              "text": "Yeah, that framing matches what I saw almost exactly. The prompt layer gives a false sense of safety, and once users start poking at stateful systems the cracks show fast lol. I‚Äôll look into runtime security, do you have any tools or tips on that note? Some dude dropped one of the tools he used that actually looked pretty good but I am curious what you use for this.",
              "score": 1,
              "created_utc": "2026-01-29 22:13:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hm5t9",
          "author": "HealthyCommunicat",
          "text": "Can you give me some examples of what kind of prevention instructions/prompts you‚Äôve made and what/how it‚Äôs being circumvented?",
          "score": 1,
          "created_utc": "2026-01-29 21:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iif34",
              "author": "Zoniin",
              "text": "yeah, a lot of it started with fairly standard stuff. strict system prompts about role, explicit ‚Äúdo not reveal internal instructions,‚Äù tool usage constraints, and guardrails around what data could be accessed or returned. the circumvention was rarely a single prompt, it was usually gradual. things like multi turn probing that reframed the task, mixing benign requests with meta instructions, or steering the model to restate or summarize context in ways that effectively leaked system or RAG data. none of those looked obviously malicious in isolation, which is why they slipped past prompt level checks.",
              "score": 1,
              "created_utc": "2026-01-30 00:34:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iklp5",
                  "author": "HealthyCommunicat",
                  "text": "ive shipped a good amount of automation, only 3 of them being actual \"chat with the model\" kinda thing, and i went out of my way to take as much into consideration, i had gpt opus gemini3pro just speak with the agent nonstop and come up with as many possible situations and test to make certain that the model wont leak anything, if you can send me ur .md or post it online and tell me the model id i can try to help?",
                  "score": 1,
                  "created_utc": "2026-01-30 00:46:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hpp9m",
          "author": "gkarthi280",
          "text": "What really helps is observability. A lot of this stuff is hard to catch before production cuz LLMs  and agents are non-deterministic. You dont know whats going on under the hood. \n\nCheck out OpenTelemetry and pair it with an OTel compatible backend like SigNoz and you'll get detailed traces of every step the agent takes before it spits out its output. it helps you answer questions like:\n\n* what tools were being called\n* what inputs triggered them\n* how the model reasoned step-by-step\n\nIt doesn't solve prompt injection but makes it way easier to see failures and unintended behavior before they escalate, especially in prod.",
          "score": 1,
          "created_utc": "2026-01-29 22:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iywve",
          "author": "nmrk",
          "text": "There is an old internet saying, \"If your platform is full of assholes, and you could have done something to prevent it and didn't, *you're* the asshole.\"",
          "score": 1,
          "created_utc": "2026-01-30 02:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jln4z",
          "author": "gg6x3",
          "text": "The core concern here is that prompt engineering is not security. In my role building an enterprise GenAI platform, I‚Äôve seen this play out in infrastructure design. We recently migrated from direct (global, might I add) endpoints to a backend proxy model for data residency compliance. This shift moves the security boundary from the 'prompt' to the 'network path.' By routing traffic through a controlled backend, we gain the runtime visibility needed to monitor behavior and enforce data residency in real-time. SSO protects the entrance, but the proxy protects the data flow. Relying on system prompts to prevent exfiltration is a production risk we hope to thwart by centralizing control at the proxy layer.\n\nNow if your endpoint is connected to internal tools (like a database or file search), that opens a whole new can of worms.",
          "score": 1,
          "created_utc": "2026-01-30 04:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qs37a",
          "author": "Analytics-Maken",
          "text": "System prompts and duardrails operate at the prompt layer, but they can't enforce permissions or validate calls. Move security out of the prompt layer, treat every LLM output as untrusted input. Use gate tools, they call servers side with strict auth checks, allowlist, and schema validation. Some ETL tools like Windsor.ai have them and enable more than 325 data sources through them.",
          "score": 1,
          "created_utc": "2026-01-31 05:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31l0ar",
          "author": "ampancha",
          "text": "Your core thesis is correct: prompt-layer fixes don't hold because the model is still the one deciding whether to comply. The controls that survive real users operate at the infrastructure layer where the model has no agency: tool allowlists, schema validation, least-privilege service permissions, and output validation. In financial services, this matters doubly because a successful injection can trigger unauthorized data access through tool misuse, not just bad outputs.",
          "score": 1,
          "created_utc": "2026-02-01 21:32:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gseuz",
          "author": "ChanceKale7861",
          "text": "Prompt injection is a feature‚Ä¶ and you will not fix it. It‚Äôs part and parcel. Further, you need to go educate yourself on security and threat modeling and adversarial threats and then ensure your systems have the controls.\n\nHahahahahha so first time? And you didn‚Äôt think about ANY of this beforehand? Same old thing‚Ä¶ speed to market trumps all‚Ä¶ except a good design‚Ä¶ üòÇüòÇüòÇ \n\nIf this happened then it was warranted‚Ä¶ \n\nAnyone can leverage tools to test this stuff and automate the security and E2E testing etc.",
          "score": -2,
          "created_utc": "2026-01-29 19:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gvbop",
              "author": "Zoniin",
              "text": "Fair reaction tbh. To be clear it wasn't that we thought about none of it. We did threat modeling, prompt hardening, etc. What surprised me was not that abuse happened but more so how much of it fell into gray areas that were hard to classify as malicious ahead of time and only emerged once the system was stateful and under real usage. Automated testing and E2E help, but they do not surface the same failure modes we saw once users started interacting freely. That gap was what I found interesting, not the idea that public systems get abused.",
              "score": 3,
              "created_utc": "2026-01-29 19:39:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2hn3yg",
                  "author": "Adept_Carpet",
                  "text": "Did you roll out to a small group first (including a micro group of employees directed to try and break it)? Or was it basically sent to all users at once?",
                  "score": 2,
                  "created_utc": "2026-01-29 21:52:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3hal7f",
                  "author": "ChanceKale7861",
                  "text": "Good points! Thanks for engaging! \n\nGood call outs on the threat modeling. Thats what I‚Äôve found as well is more the edge cases. Like it didn‚Äôt occur with a few agents or otherwise. It was when things emerged in the course of running it.",
                  "score": 1,
                  "created_utc": "2026-02-04 05:16:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2iazfx",
          "author": "Far_Statistician1479",
          "text": "If you create a system where prompt injection can cause a problem, then you‚Äôre a bad developer and designed a bad system.",
          "score": -4,
          "created_utc": "2026-01-29 23:54:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hh4vz",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -5,
          "created_utc": "2026-01-29 21:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hqwbu",
              "author": "Zoniin",
              "text": "Appreciate you sharing that. More or less lines up pretty closely with the kinds of issues I was running into. I‚Äôll spend some time testing it out thanks again for sharing. What specifically do you use this for if you don‚Äôt mind my asking?",
              "score": 0,
              "created_utc": "2026-01-29 22:10:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1quxgq9",
      "title": "8 Ways OpenClaw Reduces Context Loss in Long-Running Agents",
      "subreddit": "LLMDevs",
      "url": "https://codepointer.substack.com/p/openclaw-stop-losing-context-8-techniques",
      "author": "noninertialframe96",
      "created_utc": "2026-02-03 17:01:02",
      "score": 17,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1quxgq9/8_ways_openclaw_reduces_context_loss_in/",
      "domain": "codepointer.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3hsmig",
          "author": "gtek_engineer66",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-02-04 07:45:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr71pv",
      "title": "Who still use LLMs in browser and copy paste those code in editior instead of using Code Agent?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "author": "monskull_",
      "created_utc": "2026-01-30 14:32:46",
      "score": 13,
      "num_comments": 34,
      "upvote_ratio": 0.76,
      "text": "I‚Äôm always excited to try new AI agents, but when the work gets serious, I usually go back to using LLMs in the browser, inline edits, or autocomplete. Agents‚Äîespecially the Gemini CLI‚Äîtend to mess things up and leave no trace of what they actually changed.\n\nThe ones that insist on 'planning' first, like Kiro or Antigravity, eventually over-code so much that I spend another hour just reverting their mistakes. I only want agents for specific, local scripts‚Äîlike a Python tool for ActivityWatch that updates my calendar every hour or pings me if I‚Äôm wasting time on YouTube.\n\nI want to know is there something i am missing? like better way to code with agents?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2ly9nd",
          "author": "-rnr",
          "text": "wish I could help, agents always feel like pair programming with someone who won‚Äôt stop refactoring.",
          "score": 11,
          "created_utc": "2026-01-30 14:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3l9k",
          "author": "Zeikos",
          "text": "Me.  \nI use LLMs as a fancy google.  \nI like my IDE as uncluttered ad possible.",
          "score": 10,
          "created_utc": "2026-01-30 15:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ma6qd",
          "author": "dsartori",
          "text": "I enjoy using Cline in VSCode, but I rarely give it permission to write directly to files. This is an evolution of my approach based on my own setup and experience. Jesus take the wheel coding is usually the illusion of productivity more than the reality if you have any standards for your codebase.",
          "score": 6,
          "created_utc": "2026-01-30 15:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mf2kf",
          "author": "mdizak",
          "text": "\n\nI'm blind, so don't use any of the IDEs as none of them are accessible via screen reader.\n\n\n\nOnly time I really use code LLMs give me in the browser is using Gemini to correct typos basically.  Just in the last few months these LLMs have finally gotten good enough so I can bang out say a 300 line Rust struct, then just pass it to Gemini to fix all the syntax and braces / brackets errors, and have it actually work.  That's been really nice, and a huge time saver.\n\n\n\nOther than that, I don't ever use code from LLMs as I find it slopppy, overly verbose, and poor design choices.  That's expected, as these are just predictive machines trained on the entirety of the internet, so by design, you're going to get the most average, middle of the road code out there.\n\n\n\nI do however use Claude Code asisstant here and there.  If I just need something done for a data processing or training pipeline of some kind, and other things that won't be going into production, then I'll sometimes use that.   Although think I'll start steering away from that, because as per usual, when I begin leaning on these things more I end up realizing their screw ups ultimately cost me more time and stress than any initial development time savings I get.",
          "score": 5,
          "created_utc": "2026-01-30 15:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ndlhq",
              "author": "monskull_",
              "text": "How blind ppl code? It's very impressive too.",
              "score": 3,
              "created_utc": "2026-01-30 18:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2m4zkx",
          "author": "Adept_Carpet",
          "text": "This is making me feel better because I feel like I should be using agents but the chat works better.\n\n\nThe agents work alright if everything is set up in a certain way, but all the projects I work on have stuff that kills agents like unused old versions of code sitting in directories or documentation from other projects that is similar enough to be reused if you are a human but makes agents very confused.\n\n\nThat stuff shouldn't be there, but it is and I can't get rid of it solely to enable more vibe coding.",
          "score": 3,
          "created_utc": "2026-01-30 15:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m9c3v",
              "author": "monskull_",
              "text": "I thought i am missing something.",
              "score": 2,
              "created_utc": "2026-01-30 15:30:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nbosm",
          "author": "UncleRedz",
          "text": "I find that I keep moving back and forth between web chat copy/paste and Qwen Code / Copilot Agent Mode etc. What's often missing in the discussions here is what languages and tech stack you are using and the state of the code base. Let me provide an example, I tried to use agents to build a Flutter app from scratch and it was a horrible waste of time. After a week of evenings I gave up and used Gemini Pro on the web instead, that worked great and was many times faster.\n\nWhen you know how to write the code, but see that the AI generates about 80-90% correct code, it's at least for me,, faster to just do copy paste, fix the last 10-20%, than it is to watch the AI correct it self like a trainwreck, retry, test, compile, try to fix something completely unrelated, still fail and announce that it successfully completed the task, remind it and have it try again, until it succeeded but at the same time refactored a bunch of unrelated stuff and left piles of unused code from trial and errors.\n\nFunny thing, once my Flutter app was practically feature complete, I tried Qwen Code again, and this time it actually worked perfectly, my guess is that there were enough code there for it to understand what needed to be done and how to fit it in, as opposed to an empty or nearly empty code base.\n\nFor other projects, I've had better success, HTML/JavaScript/CSS seems to work rather well, and creating boilerplate code in C# also works well, when the right classes are in the context for the AI to know how things should be implemented.\n\nMy point is that how successful you are seems to depend on multiple factors, such as how well the model handles your specific tech stack, and even down to specific versions of libraries, in addition to how clean, large or small your codebase is. And I also suspect that people have different tolerances for watching the AI trainwreck trying to repair it's mistakes, versus doing it them self, and what quality of code they accept. \n\nAt the end of the day, most important, is that every developer needs to own the code that the AI generates, when main branch breaks, or bugs reach customers, it's not AI's fault, it's the developer who needs to own it and fix it.",
          "score": 3,
          "created_utc": "2026-01-30 18:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o52jp",
          "author": "radarsat1",
          "text": "I would like to jump back into using agents but gave up for now since Google cut off free Gemini API usage. Somehow the browser interface I can basically spam the model as much as I want, so I use that.\n\nSo I end up architecting things so that most changes can be done on a single file instead of requiring little changes in many places, leads to good structure anyway.\n\nI do kind of miss working with agents though, but it can get wild. I agree with people here that there is something more careful and controlled when forced into using copy-paste, but it is also annoying. I feel like there must be some yet to be discovered interface that is a happy medium.",
          "score": 3,
          "created_utc": "2026-01-30 20:34:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lzh8t",
          "author": "IONaut",
          "text": "Yeah I don't use agents. I have a couple VS code plugins that can be agentic but I generally don't let them edit code at all. I just have them for the convenience of having an interface in the sidebar. Usually I'll just have them refactor something or provide autocomplete or write me a boiler plate function that is short enough that I can vet it before running it. I also don't use online APIs and run everything locally with LM Studio.",
          "score": 2,
          "created_utc": "2026-01-30 14:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m22ol",
              "author": "monskull_",
              "text": "Same.",
              "score": 2,
              "created_utc": "2026-01-30 14:56:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lzq9t",
          "author": "SerDetestable",
          "text": "vscode with copilot and opus",
          "score": 2,
          "created_utc": "2026-01-30 14:44:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m2etv",
              "author": "monskull_",
              "text": "what is opus?",
              "score": 2,
              "created_utc": "2026-01-30 14:57:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2m2l6g",
                  "author": "SerDetestable",
                  "text": "Opus 4.5, anthropic reasoning flagship model.",
                  "score": 3,
                  "created_utc": "2026-01-30 14:58:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2oxvxl",
                  "author": "drinksbeerdaily",
                  "text": "Yeah, you need to try Opus 4.5 in Claude Code or Opencode.",
                  "score": 1,
                  "created_utc": "2026-01-30 22:54:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m6ibj",
          "author": "knownboyofno",
          "text": "Yea, that's a problem. I normally have to say something like be DRY and SOILD while following the current repo conventions. It changes a bit based on the model I am using. I always make a plan then make sure it looks good before i have it do the work. I normally have two git worktrees open to work on two features at a time.",
          "score": 2,
          "created_utc": "2026-01-30 15:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2manee",
          "author": "dondie8448",
          "text": "Cant trust the agents with my code! They messed up my work a couple of times. Never again. Rather do it myself than let them screw up.",
          "score": 2,
          "created_utc": "2026-01-30 15:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mc9d4",
          "author": "gman55075",
          "text": "I always feel like a code agent is asking for trouble, tho that may be because I'm not good enough at debugging and I use a conversational prompting style to plan, then fine down to pseudo, then code.  I actually ended up building my own browser-style desktop API wrapper that can receive code output as artifacts, let me review/ edit them, then copy and paste into my IDE.  Maybe not ideal for someone with better skills but for me it works.",
          "score": 2,
          "created_utc": "2026-01-30 15:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2me1on",
          "author": "darkwingdankest",
          "text": "rookies",
          "score": 2,
          "created_utc": "2026-01-30 15:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mewme",
          "author": "typeryu",
          "text": "I have been using Codex with 5.2 high and it is very good. Unlike Claude Code which kind of does its own thing sometimes, I can steer pretty well to a point where I am generally explaining in natural language what I want and it takes care of the syntax which means I still know the codebase as if I did it myself. I imagine this is what it feels like to drive those quadrupedal robots which you give it inputs like playing video games and the AI figures out where to place the feet.",
          "score": 2,
          "created_utc": "2026-01-30 15:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mod82",
          "author": "Awkward-Customer",
          "text": ">and leave no trace of what they actually changed  \n...\n\n>I spend another hour just reverting their mistakes  \n...  \nI want to know is there something i am missing? like better way to code with agents?\n\n  \nAre you using revision control with your code bases? The copy/pasting from the web is a huge waste of time, try roocode or cline in visual studio, make sure your code is all revision controlled, review the changes and commit the code frequently. Or use claude code directly if you don't want to use the IDE plugins.\n\nIt should be very clear what these tools are doing if you're using git to manage your projects.",
          "score": 2,
          "created_utc": "2026-01-30 16:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ne1kv",
              "author": "monskull_",
              "text": "I usually use git create new branch let them do what they want if did't work delete the branch. This only happen with Gemini-CLI you can't even find old code in vs code timeline",
              "score": 1,
              "created_utc": "2026-01-30 18:31:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2nfqwm",
                  "author": "Awkward-Customer",
                  "text": "Ok, but if you're using git how do you have no trace of what they actually changed? I'm just trying to understand the problem you're running into here because I find having the tools work with the code directly at least an order of magnitude faster and copy/pasting from the web interface.",
                  "score": 2,
                  "created_utc": "2026-01-30 18:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mou7c",
          "author": "AurumDaemonHD",
          "text": "Exactly the ai agent frameworks are token munchers that need handholding and fall apart if u look at them wrong.\n\nYesterday i posted on [locallama](https://www.reddit.com/r/LocalLLaMA/s/lR5inneFwF) that i built a fish shell script to manage context maybe you could try that.",
          "score": 2,
          "created_utc": "2026-01-30 16:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2os9ef",
          "author": "No_Afternoon_4260",
          "text": "My boss",
          "score": 2,
          "created_utc": "2026-01-30 22:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ow2uh",
          "author": "esaule",
          "text": "use git my friend!",
          "score": 2,
          "created_utc": "2026-01-30 22:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3qmk",
          "author": "BidWestern1056",
          "text": "incognide and npcsh\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 2,
          "created_utc": "2026-01-30 23:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p83fc",
          "author": "TheCientista",
          "text": "I architect with ChatGPT chatting on the browser. Get it to make a tightly controlled instruction block for Claude code and paste that in. CC does the work and produces a summary. ChatGPT checks it. I pay two subs. It‚Äôs been great so far and only getting better. CC needs controlling so chat holds it to account. I am always in the loop. I clean as I go along.",
          "score": 2,
          "created_utc": "2026-01-30 23:49:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yeieo",
          "author": "bakawolf123",
          "text": "I can agree that CLI ones were indeed way too blackbox for me to be comfortable enough for letting them  do anything beyond a basic bootstrap.  \nWith IDE tools it's a bit better, mainly because you can clearly see the process as it unfolds and assess if it's doing what you want it or not and react immediately.\n\nSo what you could do to utilise them more efficiently:\n\nAsk model to analyse the repo and build rules/guardrails (i.e. an Agents md or similar) first.  \nAdjust it manually.  \nSkim through the thinking blocks as it is going on and hit stop if it goes sideways, then prompt a follow up.   \n  \nSometimes it would ignore your rules, sometimes it will hallucinate an API, sometimes it would ignore your utilities - gotta babysit, but it will still be better than copy pasting from browser.  \n  \nAs for planning stages - those are mostly a bummer for me as the plan is either not fully editable, or changes to it are largely ignored somehow, which kinda kills the purpose. They want you to keep refining it by using model again but that's just waste of tokens.",
          "score": 2,
          "created_utc": "2026-02-01 11:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yh11v",
              "author": "monskull_",
              "text": "Okay i will try this in next project",
              "score": 1,
              "created_utc": "2026-02-01 12:00:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30m3o3",
          "author": "2_minutes_hate",
          "text": "I will use a vscode extension here and there, but most of what I do with LLMs happens there exclusively as a chat (functionally I use it the same as a browser tab), or in a browser.",
          "score": 2,
          "created_utc": "2026-02-01 18:45:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3mft",
          "author": "No_Knee3385",
          "text": "When the IDE agent is rate limited",
          "score": 1,
          "created_utc": "2026-01-30 23:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2prszh",
          "author": "Johnlee01223",
          "text": "Same here.\n\nLLM, the quality of output correlates to the quality of input but in a larger codebase, these agents tend to add tons of unrelated stuffs to the context which ends up degrading the quality of the output unless everything is set up and documented in certain way.",
          "score": 1,
          "created_utc": "2026-01-31 01:40:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrcxoj",
      "title": "Claude code's main success story is their tool design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "author": "Miclivs",
      "created_utc": "2026-01-30 18:03:34",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.82,
      "text": "Claude Code hit $1B in run-rate revenue.\n\nIts core architecture? Four primitives: read, write, edit, and bash.\n\nMeanwhile, most agent builders are drowning in specialized tools. One per domain object (hmm hmm 20+ tool MCPs..)\n\nThe difference comes down to one asymmetry:\n\n**Reading forgives schema ignorance. Writing punishes it.**\n\nWith reads, you can abstract away complexity. Wrap different APIs behind a unified interface. Normalize response shapes. The agent can be naive about what's underneath.\n\nWith writes, you can't hide the schema. The agent isn't consuming structure‚Äîit's producing it. Every field, every constraint, every relationship needs to be explicit.\n\nUnless you model writes as files.\n\nFiles are a universal interface. The agent already knows JSON, YAML, markdown. The schema isn't embedded in your tool definitions‚Äîit's the file format itself.\n\nFour primitives. Not forty.\n\nWrote up the full breakdown with Vercel's d0 results: \n\nhttps://michaellivs.com/blog/architecture-behind-claude-code\n\nCurious if others have hit this same wall with write tools.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2n9nmx",
          "author": "pborenstein",
          "text": "I've been experimenting with other LLM code harnesses. Because I started with Claude Code, I tend to compare everything to it. \n\nThe first thing I missed was CC's command/skill structure. I realized that this is probably why I've never used an MCP. \n\nThe LLM can figure out what to do from my description. It deals with ambiguity better than code.",
          "score": 1,
          "created_utc": "2026-01-30 18:12:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2na2dy",
              "author": "Miclivs",
              "text": "I think there‚Äôs a good reason to compare everything to CC‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-30 18:13:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2njp9b",
          "author": "steinernein",
          "text": "Why even bother with four? Just reduce it down to three and alter the shape of what the API accepts and rejects.",
          "score": 1,
          "created_utc": "2026-01-30 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p6wgf",
          "author": "kubrador",
          "text": "the files-as-api thesis is clever but i'm skeptical this scales past toy problems. reads forgiving schema ignorance works until the agent hallucinates a field that doesn't exist and you're debugging why it corrupted your production database.",
          "score": 1,
          "created_utc": "2026-01-30 23:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p7lbg",
              "author": "Miclivs",
              "text": "Oh, it works AMAZINGLY well for our data analytics agent.",
              "score": 1,
              "created_utc": "2026-01-30 23:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nb9sz",
          "author": "One-Neighborhood4868",
          "text": "Just make an agent team to help out choose what tools to use?",
          "score": 0,
          "created_utc": "2026-01-30 18:19:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtgszg",
      "title": "Drowning in 70k+ papers/year. Built an open-source pipeline to find the signal. Feedback wanted.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qtgszg/drowning_in_70k_papersyear_built_an_opensource/",
      "author": "Real-Cheesecake-8074",
      "created_utc": "2026-02-02 01:08:40",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "Like many of you, I'm struggling to keep up. With over 80k AI papers published last year on arXiv alone, my RSS feeds and keyword alerts are just noise. I was spending more time filtering lists than reading actual research.\n\nTo solve this for myself, a few of us hacked together an open-source pipeline (\"Research Agent\") to automate the pruning process. We're hoping to get feedback from this community on the ranking logic to make it actually useful for researchers.\n\n**How we're currently filtering:**\n\n* **Source:**¬†Fetches recent arXiv papers (CS.AI, CS.ML, etc.).\n* **Semantic Filter:**¬†Uses embeddings to match papers against a specific natural language research brief (not just keywords).\n* **Classification:**¬†An LLM classifies papers as \"In-Scope,\" \"Adjacent,\" or \"Out.\"\n* **\"Moneyball\" Ranking:**¬†Ranks the shortlist based on author citation velocity (via Semantic Scholar) + abstract novelty.\n* **Output:**¬†Generates plain English summaries for the top hits.\n\n**Current Limitations (It's not perfect):**\n\n* Summaries can hallucinate (LLM randomness).\n* Predicting \"influence\" is incredibly hard and noisy.\n* Category coverage is currently limited to CS.\n\n**I need your help:**\n\n1. If you had to rank papers automatically, what signals would¬†*you*¬†trust? (Author history? Institution? Twitter velocity?)\n2. What is the biggest failure mode of current discovery tools for you?\n3. Would you trust an \"agent\" to pre-read for you, or do you only trust your own skimming?\n\nThe tool is hosted here if you want to break it:¬†[https://research-aiagent.streamlit.app/](https://research-aiagent.streamlit.app/)\n\nCode is open source if anyone wants to contribute or fork it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qtgszg/drowning_in_70k_papersyear_built_an_opensource/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o346bh4",
          "author": "PerceptualDisruption",
          "text": "Awesome",
          "score": 1,
          "created_utc": "2026-02-02 06:52:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qviqlh",
      "title": "How to become an AI Engineer in 2026 - what actually matters now?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qviqlh/how_to_become_an_ai_engineer_in_2026_what/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-04 08:05:16",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Trying to map out a realistic path into AI engineering and getting overwhelmed by contradictory advice.\n\nPython is still non-negotiable, but the \"just build a chatbot\" project approach doesn't cut it anymore. The market looks brutal for entry-level while senior roles are paying crazy money. Prompt engineering as a dedicated job seems dead, but the skill still matters. RAG, agentic AI, and MLOps seem to be where the growth is.\n\nThe part confusing me is traditional ML (sklearn, training models) vs pure LLM/API integration. Some say you need fundamentals, others say most jobs are just orchestrating existing models. With tools like Claude Code changing what coding even means, I'm not sure what skills are actually durable.\n\nFor people who've done this or are hiring:\n\n- What actually separated you from other candidates when you got in?\n- How much traditional ML do you use day-to-day vs LLM orchestration?\n- Best resources that actually helped you, not just ones you heard were good?\n- What does this role even look like in 2027 when agents do more of the work?\n\nNot looking for a generic roadmap. Looking for what's actually working right now.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qviqlh/how_to_become_an_ai_engineer_in_2026_what/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3kxegl",
          "author": "Number4extraDip",
          "text": "What matters. Solving a specific problem. Being very specific and not just doing what everyone else is doing",
          "score": 4,
          "created_utc": "2026-02-04 19:05:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mrjp9",
              "author": "bbahner",
              "text": "Can you be more specific? <grin>",
              "score": 3,
              "created_utc": "2026-02-05 00:36:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3nebra",
                  "author": "Number4extraDip",
                  "text": "Make something you wish existed. Make a product for yourself that no one is selling.",
                  "score": 1,
                  "created_utc": "2026-02-05 02:46:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lcedg",
          "author": "hrishikamath",
          "text": "Honestly most roles I have interviewed for have had AI in requirements but interviews were SWE stuff: system design, leetcode style and so on. Most but not all. During interviews I did speak about my projects that‚Äôs about it and some questions here and there. Yeah it‚Äôs more of just building agents for a lot of them. Traditional ML stuff is required by certain niche  companies. Certain companies randomly add its good to have fine tuning experience. But, yeah some companies develop their models for that you need solid fundamentals from ground up.",
          "score": 3,
          "created_utc": "2026-02-04 20:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lelk4",
          "author": "Canadianingermany",
          "text": ">Some say you need fundamentals, others say most jobs are just orchestrating existing models.  \n\n\nMost things that people are doing today are probably quite easy, and many are working on small problems that can probably be solved with some API and some prompt engineering.  \n\n  \nBut I'm not so convinced that in the future people will want to pay a full fledged DS wage for that because the barriers to entry are simply quite low.  \n\n  \nSo strategically I would concentrate on harder problems that need more than throw an LLM at it.  \n\n  \nBut what do I know?  I hire devs, I'm not one.\n\n>  \nI'm not sure what skills are actually durable.\n\nAt the end of the day.  The ability to solve problems and not be locked in to the solution that worked last time, but find the one for this problem.\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-04 20:26:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3nqzgg",
              "author": "hrishikamath",
              "text": "Not really building good rag systems or tasks that require lot of context requires good understanding and skills",
              "score": 1,
              "created_utc": "2026-02-05 04:03:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3oktgf",
          "author": "MediumShoddy5264",
          "text": "ML is not useful right now, you need to understand tool calling, context management, planning, evals, etc... ",
          "score": 1,
          "created_utc": "2026-02-05 07:58:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq72ap",
      "title": "Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1qq72ap",
      "author": "Charming_Group_2950",
      "created_utc": "2026-01-29 12:19:50",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq72ap/quantifying_hallucinations_by_calculating_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2l09rg",
          "author": "Mikasa0xdev",
          "text": "TrustifAI is smart. We need better LLM evaluation frameworks now.",
          "score": 1,
          "created_utc": "2026-01-30 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lipvq",
              "author": "Charming_Group_2950",
              "text": "Thanks! Glad you like it. Pls spread the word.",
              "score": 1,
              "created_utc": "2026-01-30 13:16:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsyhfi",
      "title": "Operating an LLM as a constrained decision layer in a 24/7 production system",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qsyhfi/operating_an_llm_as_a_constrained_decision_layer/",
      "author": "NationalIncome1706",
      "created_utc": "2026-02-01 13:29:29",
      "score": 8,
      "num_comments": 19,
      "upvote_ratio": 0.83,
      "text": "I‚Äôm an engineer by background (14+ years in aerospace systems),  \nand recently I‚Äôve been running a **24/7 always-on production system** that uses an LLM as a *constrained decision-making component*.\n\nThe specific application happens to be automated crypto trading,  \nbut this post is **not** about strategies, alpha, or performance.\n\nIt‚Äôs about a more general systems problem:\n\n>\n\n# System context (high-level)\n\n* **Runtime:** always-on, unattended, 24/7\n* **Environment:** small edge device (no autoscaling, no human in the loop)\n* **Decision model:** discrete, time-gated decisions\n* **Failure tolerance:** low ‚Äî incorrect actions have real cost\n\nThe system must continue operating safely even when:\n\n* external APIs are unreliable\n* the LLM produces malformed or inconsistent outputs\n* partial data or timing mismatches occur\n\n# How the LLM is used (and how it is not)\n\nThe LLM is **not** used for prediction, regression, or forecasting.\n\nIt is treated as a **bounded decision layer**:\n\n* It receives only *preprocessed, closed-interval data*\n* It must output exactly one of:\n   * `ENTRY`\n   * `HOLD`\n   * `CLOSE`\n\nThere are no confidence scores, probabilities, or free-form reasoning  \nthat directly affect execution.\n\nIf the response cannot be parsed, times out, or violates the expected format  \n‚Üí **the system defaults to doing nothing**.\n\n# Core design principles\n\n# 1. Decisions only occur at explicit, closed boundaries\n\nThe system never acts on streaming or unfinished data.\n\nAll decisions are gated on **closed time windows**.  \nThis eliminated several classes of failure:\n\n* phantom actions caused by transient states\n* rapid oscillation near thresholds\n* overlapping execution paths\n\nIf the boundary is not closed, the system refuses to act.\n\n# 2. ‚ÄúDo nothing‚Äù is the safest default\n\nThe system is intentionally biased toward inaction.\n\n* API error ‚Üí HOLD\n* LLM timeout ‚Üí HOLD\n* Partial or inconsistent data ‚Üí HOLD\n* Conflicting signals ‚Üí HOLD\n\nIn ambiguous situations, *not acting* is considered the safest outcome.\n\n# 3. Strict separation of concerns\n\nThe system is split into independent layers:\n\n* data preparation\n* LLM-based decision\n* execution\n* logging and notification\n* post-action accounting\n\nEach layer can fail independently without cascading into repeated actions  \nor runaway behavior.\n\nFor example, notifications react only to **confirmed state changes**,  \nnot to intended or predicted actions.\n\n# 4. Features that were intentionally removed\n\nSeveral ideas were tested and then removed after increasing operational risk:\n\n* adaptive or performance-based scaling\n* averaging down / martingale behavior\n* intra-window predictions\n* confidence-weighted LLM actions\n* automatic restart into uncertain internal states\n\nThe system became *more stable* by explicitly **not doing these things**.\n\n# Why I‚Äôm sharing this\n\nI‚Äôm sharing this to **organize and reflect on lessons learned** from operating  \na non-deterministic LLM component in a live system.\n\nThe feedback here is for personal learning and refinement of system design.  \nAny future write-up would be technical and experience-based,  \nnot monetized and not promotional.\n\n# Looking for discussion\n\nI‚Äôd appreciate perspectives from people who have:\n\n* deployed LLMs or ML components in always-on systems\n* dealt with non-determinism and failure modes in production\n* strong opinions on fail-safe vs fail-open design\n\nIf this kind of operational discussion is useful (or not), I‚Äôd like to know.\n\n\n\nhttps://preview.redd.it/79npeu8hxvgg1.jpg?width=2048&format=pjpg&auto=webp&s=0be3702d0694e3f1ff0f73c9d8b8e4b8fbf3b548\n\n\n\n*Not selling anything here. Just sharing an operational experience.*",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qsyhfi/operating_an_llm_as_a_constrained_decision_layer/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o30gf9h",
          "author": "chris_thoughtcatch",
          "text": "Why use the LLM over a heuristic? Or use the LLM to determine the heuristic?",
          "score": 4,
          "created_utc": "2026-02-01 18:20:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o320aqp",
              "author": "NationalIncome1706",
              "text": "Good question. Early versions of the system were almost entirely heuristic-based,\nand most of the core behavior still is.\n\nI didn‚Äôt introduce the LLM to replace rules, but to handle the gray zones between them.\nHeuristics work extremely well when decision boundaries are crisp.\nThey tend to become brittle or explosively complex when multiple conditions are\npartially satisfied at the same time.\n\nIn this setup, the LLM cannot mutate state, trigger execution, or override hard rules.\nIt only answers a constrained question: ‚ÄúIs this situation unambiguous or not?‚Äù\n\nIf heuristics are the hard constraints, the LLM acts more like a soft consensus checker\non top of them.\n\nI also deliberately avoided using the LLM to generate or adapt heuristics.\nOnce rules become model-derived, failure modes get harder to reason about\nand rollback becomes non-trivial.",
              "score": 2,
              "created_utc": "2026-02-01 22:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zkcop",
          "author": "pstryder",
          "text": "Strong agreement on default-to-inaction and closed boundaries. Curious whether you‚Äôve run into trust erosion when the LLM *sounds* confident but is actually constrained ‚Äî that‚Äôs been a surprisingly sharp edge for us.",
          "score": 2,
          "created_utc": "2026-02-01 15:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3271yr",
              "author": "NationalIncome1706",
              "text": "Yes ‚Äî we ran into that exact edge early on.\n\nThe problem wasn‚Äôt the constraints themselves, but the mismatch between linguistic confidence and actual agency. Humans tend to interpret confident language as capability, even when the model is heavily boxed in.\n\nWhat helped was reframing the LLM‚Äôs role entirely. Its output isn‚Äôt treated as an explanation or a recommendation ‚Äî it‚Äôs a state classification signal. We deliberately stripped away expressive language and made responses repetitive, terse, and sometimes even boring.\n\nOver time, that shifted trust away from the LLM as an ‚Äúactor‚Äù and toward the system as a whole. The goal wasn‚Äôt to make the LLM trustworthy, but to make it unnecessary to trust in isolation.\n\nOnce operators internalize that distinction, the confidence/constraint gap becomes much less sharp.",
              "score": 1,
              "created_utc": "2026-02-01 23:25:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31zmz3",
          "author": "Kimononono",
          "text": "How often does your system produce a BUY / SELL signal vs HOLD\n\nAt what interval does it run?",
          "score": 2,
          "created_utc": "2026-02-01 22:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32e4e0",
              "author": "NationalIncome1706",
              "text": "The decision loop runs on a fixed schedule, but with very different semantics.\n\nENTRY decisions are only evaluated on closed higher-timeframe candles, so they‚Äôre relatively infrequent by design.\nPosition management / exit checks run much more often, but the default outcome there is still HOLD.\n\nIn practice, the vast majority of evaluations result in HOLD. BUY/SELL is treated as an exception, not a steady stream.\n\nThat ratio is intentional. We don‚Äôt optimize for signal frequency ‚Äî we treat excessive activity as a smell. If the system is trading often, something upstream is probably too permissive.\n\nThe goal is to let the system be bored most of the time and only act when ambiguity collapses.",
              "score": 1,
              "created_utc": "2026-02-02 00:04:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o33pmxx",
                  "author": "Kimononono",
                  "text": "if I have a script which goes \n\nfunc analyze\\_signal():\n\nnum = random()  \nif(num < .01) BUY  \nif(num < .02) SELL  \nelse HOLD\n\nHow are you to smell anything if the scent is so faint?",
                  "score": 2,
                  "created_utc": "2026-02-02 04:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34gef2",
          "author": "Sea-Sir-2985",
          "text": "one thing i've run into with this kind of setup is that schema validation alone isn't enough... the LLM can return perfectly valid JSON that still makes a nonsensical decision. so we added a semantic validation layer on top, basically domain constraint checks that run after the LLM responds but before anything gets executed\n\nthe other piece that helped was logging every decision with the full prompt and response, not just the final action. when something goes wrong at 3am you need to see exactly what the model was thinking, not just what it did",
          "score": 2,
          "created_utc": "2026-02-02 08:24:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35elkz",
              "author": "NationalIncome1706",
              "text": "This matches our experience almost exactly.\n\nValid JSON is necessary but nowhere near sufficient.\nThe LLM sits inside a constrained decision layer,\nwhere its output is treated as a hypothesis that must pass\nexplicit domain checks (timeframe closure, regime alignment, risk bounds, state continuity).\n\nExecution is gated deterministically.\nHOLD is the fail-safe default.\n\nWe also persist the full prompt snapshot and model response,\nbecause without that, post-mortems are basically guesswork.",
              "score": 1,
              "created_utc": "2026-02-02 13:14:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o38eoep",
          "author": "KnightCodin",
          "text": "I might have missed the exact reason but why introduce LLM at all? \"non-determinism\" or \"Grey\" area are not sufficient reason to introduce further complexity into this \"bound decision later\" with strict output states. Having done mission critical production systems all my life (built and managed trading systems for a one the big 3 investment banks) I am curious to know what are the reasons for this. Decision thresholding can be used to achieve the ambiguity question easily. What am I missing?",
          "score": 2,
          "created_utc": "2026-02-02 21:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38v6x8",
              "author": "NationalIncome1706",
              "text": "Good question.\n\nThe distinction is between a *default HOLD* and a *judged HOLD*.\n\nWith heuristics alone, any region outside clearly defined thresholds\ncollapses into a blanket HOLD. That‚Äôs safe, but it also throws away cases\nthat are structurally unambiguous but hard to encode cleanly as rules.\n\nThe LLM doesn‚Äôt decide BUY/SELL, mutate state, or override rules.\nIt only answers a constrained question:\n‚ÄúIs this situation unambiguous enough to allow the rules to act?‚Äù\n\nIf the answer is no, it‚Äôs still HOLD.\nIf yes, execution remains fully rule-driven.\n\nSo the goal wasn‚Äôt to add intelligence on top of heuristics,\nbut to prevent heuristics from becoming brittle or explosively complex\nin partially satisfied, multi-condition regimes.",
              "score": 1,
              "created_utc": "2026-02-02 23:17:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o38x43n",
                  "author": "KnightCodin",
                  "text": "Okay - so you are in the multi-branching scenario. You are still introducing stochasticity. However, assuming you have explored other techniques, Without knowing all the constraints and ecosystems, simplest solution would be   \n1. Use a smaller LLM known to punch above its weight class Eg. Qwen 3 - 4B.  \n2. If your inference engine allows it, use constraints at logits level to reduce subjectivity",
                  "score": 2,
                  "created_utc": "2026-02-02 23:28:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ytwkw",
          "author": "NationalIncome1706",
          "text": "This is an experience report on operating an LLM inside a live system.\n\nNot a product, not prompts, not benchmarks.\n\n\n\nI‚Äôm especially interested in how others handle non-determinism,\n\nfail-safe defaults, and state consistency in always-on LLM-based systems.",
          "score": 1,
          "created_utc": "2026-02-01 13:33:04",
          "is_submitter": true,
          "replies": [
            {
              "id": "o37psxr",
              "author": "amejin",
              "text": "My opinion - how to \"handle non-determinism?\"\n\nWhen it comes to money, you don't.",
              "score": 2,
              "created_utc": "2026-02-02 19:56:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37upb6",
                  "author": "NationalIncome1706",
                  "text": "I largely agree with that principle. When real money is involved, non-determinism shouldn‚Äôt be trusted as a decision maker.\n\nThat‚Äôs exactly why, in my case, it‚Äôs not allowed to decide anything. It can‚Äôt allocate capital, trigger execution, or override hard rules.\n\nThe only thing it‚Äôs permitted to do is say ‚Äúdon‚Äôt act‚Äù when the system is near a boundary the deterministic logic can‚Äôt cleanly resolve.\n\nSo I don‚Äôt think of it as ‚Äúhandling non-determinism,‚Äù but rather containing it ‚Äî pushing it into a narrow veto role where failure modes collapse to inaction.\n\nIf a non-deterministic component can‚Äôt fail safely, it shouldn‚Äôt be anywhere near money. On that point, I think we‚Äôre aligned.",
                  "score": 1,
                  "created_utc": "2026-02-02 20:19:38",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv6w28",
      "title": "I‚Äôm building an open-source local AI agent in Go that uses IR + tools instead of wasting tokens",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qv6w28/im_building_an_opensource_local_ai_agent_in_go/",
      "author": "iagomussel",
      "created_utc": "2026-02-03 22:46:21",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nI‚Äôve been working on an open-source project called **IRon**: a local-first AI assistant focused on automation, not chat.\n\nThe main idea is:\n\nInstead of using LLMs to ‚Äúthink‚Äù and generate long text, IRon translates user input into a small structured format (IR ‚Äì Intermediate Representation) and executes real tools.\n\nSo most tasks don‚Äôt need heavy models.\n\n# What it does\n\nIRon works mainly through Telegram and runs locally.\n\nPipeline:\n\nUser ‚Üí Router ‚Üí (optional LLM) ‚Üí IR (JSON) ‚Üí Tools ‚Üí Result\n\nFeatures:\n\n* Deterministic router for common tasks (notes, lists, commands, etc.)\n* Dual output: short human reply + machine IR\n* Tool system (shell, docker, http, code exec, notes, scheduler, addons)\n* Cron-based scheduler\n* Codex/Ollama support for complex reasoning\n* Session isolation per chat\n* Addon system for external tools/adapters\n\n# Why I built it\n\nMost ‚ÄúAI assistants‚Äù today:\n\n* Burn tokens on simple things\n* Re-explain everything\n* Don‚Äôt integrate well with real systems\n* Lose context easily\n\nI wanted something closer to:\n\n‚ÄúNatural language ‚Üí compact instruction ‚Üí real execution‚Äù\n\nLike a mix of:\n\n* cron\n* Makefile\n* shell\n* and LLMs\n\nBut with safety and structure.\n\n# Example\n\nUser:  \n‚ÄúRemind me to pay rent tomorrow at 9‚Äù\n\nIRon:\n\n* Generates IR\n* Schedules cron\n* Uses scheduler tool\n* Confirms in one line\n\nNo long explanation. No wasted tokens.\n\n# Tech stack\n\n* Go\n* Telegram Bot API\n* Codex CLI / Ollama (future)\n* JSON-based IR\n* robfig/cron\n* Plugin system\n\nCurrent status\n\nIt‚Äôs usable and evolving.  \nMain focus now:\n\n* DSL for tasks\n* Better scheduling\n* Memory without huge context\n* More deterministic routing\n\n**It's in progress, so there are bugs yet, let me know if you can help.**\n\n# Repo\n\n[https://github.com/iagomussel/IRon](https://github.com/iagomussel/IRon?utm_source=chatgpt.com)\n\n# Looking for feedback\n\nI‚Äôm interested in feedback on:\n\n* Architecture\n* IR format\n* DSL ideas\n* Similar projects\n* Security concerns\n\nIf you‚Äôre into local AI, automation, or agent systems, I‚Äôd love your thoughts.\n\nThanks üôå",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qv6w28/im_building_an_opensource_local_ai_agent_in_go/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3g2vcj",
          "author": "DataCentricExpert",
          "text": "Do you have a sandboxed or local dev environment for testing IRon safely with real data, or is it purely the Telegram interface right now?",
          "score": 1,
          "created_utc": "2026-02-04 00:53:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g6ivm",
          "author": "SeaworthinessThis598",
          "text": "I want to get. afeel about the IR concept efficacy can you show us. a demo maybe ?",
          "score": 1,
          "created_utc": "2026-02-04 01:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fite7",
          "author": "Otherwise_Wave9374",
          "text": "Love the ‚ÄúIR + tools‚Äù approach. That feels like a practical agent design: keep the LLM for the hard parsing/planning edges, but push everything into a constrained representation so execution stays predictable.\n\nHow are you thinking about schema evolution for the IR and safety around tool permissions (per chat/session)? Those two things seem to make or break local agents. Ive been reading a bunch about structured agents lately, this page has some good notes: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 23:04:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qulg4u",
      "title": "n8n vs gumloop",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qulg4u/n8n_vs_gumloop/",
      "author": "OkWestern5",
      "created_utc": "2026-02-03 07:19:52",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "If you‚Äôre looking into n8n vs Gumloop, you‚Äôre probably not trying to find the ‚Äúbest‚Äù tool in general. You‚Äôre trying to understand which workflow automation platforms actually fit how your team works day to day. That‚Äôs where this comparison comes from. I also looked at a [broader comparison table](https://docs.google.com/spreadsheets/d/1zQr6iThp2fR-TLNMvSYHgx2ghSrzbYIduO4vX_jlHig/edit?gid=1301024975#gid=1301024975) of workflow automation platforms where n8n is listed, which helped set some baseline context.\n\n# High-level difference\n\n* **Gumloop** is built for business teams that want to automate workflows without involving engineering.\n* **n8n** is built for developer-first teams that want full control, even if that means more setup and maintenance.\n\nThis difference shows up across the product, from the editor to pricing and integrations.\n\n# Ease of use\n\n**Gumloop**  \nGumloop lets you focus on the business problem rather than implementation.\n\n* Visual, easy-to-follow canvas\n* Pre-built actions for common business tools\n* AI features included by default\n* Custom steps without deep technical knowledge\n\nMost teams can get useful workflows running quickly.\n\n**n8n**  \nn8n prioritizes flexibility over simplicity.\n\n* Node-by-node configuration\n* Direct access to APIs, JSON, and JavaScript\n\nYou gain more control, but also more responsibility for building and maintaining workflows.\n\n# Integrations and flexibility\n\nBoth platforms support tools like Google Workspace, Slack, Salesforce, and Notion.\n\n* **n8n** offers broader coverage via community-built nodes, but requires manual setup and upkeep.\n* **Gumloop** focuses on the integrations business teams actually use, with AI-assisted ways to extend them when needed.\n\nIn the **n8n vs Gumloop** comparison, this is often where teams weigh flexibility against effort.\n\n# Pricing and ownership\n\n* **Gumloop** bundles AI models, scraping, enrichment, and data sources into its plans.\n* **n8n** charges per execution, with AI and data services managed and billed separately.\n\nNeither approach is better by default - it depends on whether you prefer bundled convenience or modular control.\n\n# Final thoughts\n\nThe real distinction in n8n vs Gumloop is how much work you want around your automation. Gumloop minimizes it early on, while n8n gives you more room later if you‚Äôre willing to manage it.\n\nWhich side do you lean toward - simplicity or control?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qulg4u/n8n_vs_gumloop/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qv6z6k",
      "title": "Natural Language to shell commands tool. Fully local, Ollama powered.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qv6z6k/natural_language_to_shell_commands_tool_fully/",
      "author": "ykushch",
      "created_utc": "2026-02-03 22:49:38",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "[ask - natural language to shell commands](https://i.redd.it/vzpbvx06zchg1.gif)\n\nI built a CLI tool that turns natural language into shell commands using Ollama. It runs locally (no API keys, no data egress) and includes safety checks so you don't accidentally¬†`rm -rf`¬†your system.\n\nRepo:¬†[https://github.com/ykushch/ask](https://github.com/ykushch/ask)",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qv6z6k/natural_language_to_shell_commands_tool_fully/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3jo2dk",
          "author": "konmik-android",
          "text": "I often ask Claude to do that, but having an offline version would be much better. Remembering and typing all parameters of all commands is something I left in 1990x.",
          "score": 2,
          "created_utc": "2026-02-04 15:38:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g2dog",
          "author": "DataCentricExpert",
          "text": "Curious if anyone has tried running it with a local dev environment that enforces data masking and auditability?",
          "score": 1,
          "created_utc": "2026-02-04 00:50:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qranzr",
      "title": "How do you prevent credential leaks to AI tools?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "author": "llm-60",
      "created_utc": "2026-01-30 16:45:04",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.86,
      "text": "How is your company handling employees pasting credentials/secrets into AI tools like ChatGPT or Copilot? Blocking tools entirely, using DLP, or just hoping for the best?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2p7nzh",
          "author": "kubrador",
          "text": "hoping for the best is the classic strategy, followed by a panicked all-hands email in 6 months when someone inevitably pastes a prod database url into claude.\n\nmost companies doing it right use a combo: dlp tools with regex patterns for common secrets, network blocks on the obvious stuff, and mandatory training that employees ignore until it happens to them.",
          "score": 3,
          "created_utc": "2026-01-30 23:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pvelf",
          "author": "Basic_Cat_1006",
          "text": "Whoever is hardcoding or announcing secrets out of the .env should not be a mile near your code base lmao.",
          "score": 2,
          "created_utc": "2026-01-31 02:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vg7g9",
              "author": "graymalkcat",
              "text": "Just as an aside, if you use Claude, it will open your .env which will send that to Anthropic. Maintain different sets of keys. (It‚Äôs not that Claude is evil but rather that it forgets that it itself is a cloud service)",
              "score": 3,
              "created_utc": "2026-01-31 22:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o306z3b",
                  "author": "Basic_Cat_1006",
                  "text": "I hadn‚Äôt heard that so thank you immensely",
                  "score": 1,
                  "created_utc": "2026-02-01 17:38:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2n78m1",
          "author": "Miclivs",
          "text": "I made a thing for that! [https://github.com/Michaelliv/psst](https://github.com/Michaelliv/psst)",
          "score": 2,
          "created_utc": "2026-01-30 18:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q3ut4",
          "author": "cmndr_spanky",
          "text": "The same way you prevent employees from pasting their crediting into email, slack, GitHub etc‚Ä¶\n\nWhy is AI any different ?",
          "score": 1,
          "created_utc": "2026-01-31 02:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfpuh",
              "author": "konmik-android",
              "text": "You mean, you ask it politely and then it ignores you?",
              "score": 1,
              "created_utc": "2026-02-01 02:19:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zwmgm",
                  "author": "cmndr_spanky",
                  "text": "That and you make them sign agreements that could make them legally liable (like at my company). Security experts will always tell you that human behavior will always be the weakest link, not software vulnerabilities. That said you missed my larger point. OP‚Äôs post is dumb because a person pasting credentials into AI chat is literally no different than a person pasting it into a multitude of other insecure places (which people do all the time). There‚Äôs nothing special about it being ‚ÄúAI‚Äù in terms of how you deal with this as a company.",
                  "score": 2,
                  "created_utc": "2026-02-01 16:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t46i5",
          "author": "Friendly_Hat_9545",
          "text": "We tried the \"training and hoping\" approach at first, which lasted exactly until someone almost pasted a Dev database connection string. That was a fun afternoon lol.\n\nNow we use inline DLP that scans before stuff reaches the chatbot. We went with iboss AI Chat Security because it looks into our existing network stack and just... works? Blocks the paste if it sniffs keys or PII patterns. We still allow Copilot and ChatGPT, but now with guardrails. It's not perfect but way better than crossing our fingers.\n\nTBH, blocking the tools entirely just leads to Shadow IT. You gotta let people use the tools but make it safe.",
          "score": 1,
          "created_utc": "2026-01-31 16:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vumac",
          "author": "Agreeable-Market-692",
          "text": "litellm as a proxy",
          "score": 1,
          "created_utc": "2026-02-01 00:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33oqx4",
          "author": "jontaffarsghost",
          "text": ".geminiignore",
          "score": 1,
          "created_utc": "2026-02-02 04:35:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtm1r7",
      "title": "How do you debug multi-step agent workflows",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qtm1r7/how_do_you_debug_multistep_agent_workflows/",
      "author": "CreditOk5063",
      "created_utc": "2026-02-02 05:10:21",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "I've been working on a customer support agent that routes queries to different tools depending on intent. I am essentially building a state-machine style agent using LangGraph, and the state transitions are where the logic keeps drifting. The flow is: classify intent ‚Üí retrieve relevant docs ‚Üí generate response ‚Üí validate output format. Each node works fine in isolation but when the graph runs end to end the failure modes exhibit non-linear behaviors that are hard to replicate. Sometimes the classifier output schema breaks the retriever input, sometimes the context window gets bloated by step 3.\n\nMy current debugging approach is pretty manual. I added verbose logging at each node, dump intermediate state to JSON, and trace back from failures. But the hard part is not finding where it broke, it is understanding why a certain prompt phrasing caused a downstream node to behave differently. LLM outputs are not deterministic so reproducing issues is painful. So I started using Pydantic models for structured output at each step, and let Claude and Beyz coding assistant to help me do sanity check. But it still feels inefficient though. I'm curious how do you test nodes in isolation first or go straight to end-to-end runs? How do you handle the non-determinism problem when debugging state transitions? Is anyone using Pydantic strictly for node-to-node contracts or does the validation overhead add too much latency and retries for production pipelines? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qtm1r7/how_do_you_debug_multistep_agent_workflows/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o34jm5w",
          "author": "Zeikos",
          "text": "> \"Each node works fine in isolation\"\n\nThat phrasing is a bit of a red flag for me.  \nHow do you define \"fine\"?  \n\nA process of 5 steps where each has a 90% success rate is terrible, it'll fail extremely often.  \n\nBe a lot more strict about the reliability of each node.  \nGoing from 90% to 95% is noticeable, going from 95% to 98% is *more* noticeable.  \n90% -> 95% is a 50% improvement, 95% -> 98% is a 60% improvement.  \n\nWhen you have maxed that out then you want to look at each pairs of nodes.  \nWhat nodes produce bad output when they had good input? Why?  \nYou don't care about a bigger scope than the two connections, the other nodes can stay as a black box.  \n\nBe careful to attribute the failure correctly.  \nWhen something fails at step n then you don't care about steps > n (except eventual error-recovery steps).",
          "score": 3,
          "created_utc": "2026-02-02 08:55:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33vuq6",
          "author": "plarkin",
          "text": "The golden rule: If you can't reproduce it, you can't debug it. Make everything observable, cacheable, and replayable! \n\nAsk your favorite LLM about it ;)",
          "score": 2,
          "created_utc": "2026-02-02 05:26:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3413tb",
          "author": "InvestigatorAlert832",
          "text": "The current standard approach to address the reproducibility issue is to use an observability platform like langfuse to capture detailed logs, then clean&save them to datasets, then setup evaluators, and finally you can run evals on the dataset to gauge the quality of your node/program.\nIn terms of interfacing between LLM nodes, I personally prefer strongly typed enforcement. There are frameworks built specifically focusing on this - pydantic-ai, instructor.",
          "score": 1,
          "created_utc": "2026-02-02 06:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o341p8f",
              "author": "InvestigatorAlert832",
              "text": "I do think the observability platforms on the market are painful to use for the debug & iterate scenario though, and I'm actually working on a tool for this myself. Feel free to DM me if you are interested in being my beta tester :)",
              "score": 1,
              "created_utc": "2026-02-02 06:12:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qu4s0t",
      "title": "Researchers Found 175,000 Publicly Exposed Ollama AI Servers Across 130 Countries",
      "subreddit": "LLMDevs",
      "url": "https://thehackernews.com/2026/01/researchers-find-175000-publicly.html",
      "author": "codes_astro",
      "created_utc": "2026-02-02 19:22:28",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "News",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qu4s0t/researchers_found_175000_publicly_exposed_ollama/",
      "domain": "thehackernews.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qupmeh",
      "title": "help: sub-500ms voice-cloned conversational agent with personality fine-tune - hitting walls on coherence decay and emotional arc modeling",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qupmeh/help_sub500ms_voicecloned_conversational_agent/",
      "author": "kubrador",
      "created_utc": "2026-02-03 11:36:01",
      "score": 4,
      "num_comments": 5,
      "upvote_ratio": 0.75,
      "text": "been deep in a personal project for \\~14 months. started as a grief processing thing, became an actual technical challenge i can't stop optimizing. i hope someone here has hit similar walls\n\n**use case:** real-time bidirectional conversation with a personality-cloned agent. not [character.ai](http://character.ai) generic companion stuff. i need to reconstruct a specific person's conversational patterns, humor, emotional responses, and voice with enough fidelity that extended conversations feel coherent\n\n**training data:**\n\n* \\~4.2 hours cleaned audio (voicemails, video calls, voice memos) - normalized, VAD-chunked, noise-reduced with demucs\n* \\~45k text messages across 3 years, exported with timestamps and conversation threading intact\n* emails, DMs, \\~200 voice transcriptions i did manually to capture his specific punctuation patterns (he used \"lmao\" as a period, sent bursts of 4-6 short messages instead of one long one)\n* annotated \\~300 conversation samples for emotional tone shifts\n\n**current architecture:**\n\n*voice cloning:* started with elevenlabs, too flat. tried RVC v2 with 40 min of isolated vocals, better but still missing the laugh-while-talking thing he did. currently running OpenVoice v2 for tone color cloning + custom prosody model i hacked together using StyleTTS2's prosody encoder. getting maybe 87% fidelity on blind tests i made my friend do (she knew him, didn't tell her what she was evaluating)\n\n*LLM:*\n\n* base: qwen2.5-72b-instruct\n* fine-tuned with LoRA (r=128, alpha=256, targeting q\\_proj, k\\_proj, v\\_proj, o\\_proj, gate\\_proj, up\\_proj, down\\_proj)\n* \\~3 epochs on the message corpus, formatted as multi-turn conversations with his messages as completions\n* added DPO layer using \\~400 preference pairs i manually created (responses he would vs wouldn't say)\n\n*RAG:*\n\n* bge-large-en-v1.5 embeddings\n* pinecone with hybrid search (dense + BM25 sparse)\n* chunked by conversation session, not arbitrary token windows\n* cohere reranker before context injection\n* retrieval threshold at 0.72 similarity, top-k=5\n\n*inference:*\n\n* vLLM with continuous batching\n* AWQ 4-bit quantization\n* running on 2x 3090s i bought specifically for this (told my therapist it was for \"work\")\n* speculative decoding with qwen2.5-1.5b as draft model\n\n*pipeline:* whisper-large-v3 (faster-whisper implementation) ‚Üí vLLM ‚Üí OpenVoice ‚Üí speakers\n\n**current latency breakdown:**\n\n* STT: \\~180ms\n* LLM inference: \\~400-600ms (this is my bottleneck)\n* TTS: \\~220ms\n* total: \\~800-1000ms\n\ni can get sub-600ms if i drop to qwen2.5-32b but personality coherence degrades noticeably, he gets more generic, less him\n\n**where i'm actually stuck:**\n\n1. **coherence decay past \\~6k context** \\- around 30-40 minutes of conversation the model starts losing his speech patterns. the \"lmao\" frequency drops, responses get longer and more formal, fewer multi-message bursts. i've tried sliding window with summary injection but the summaries lose texture. anyone solved long-context personality preservation? would rope scaling help or just delay the decay?\n2. **emotional arc modeling** \\- he had this specific pattern where he'd deflect hard stuff with humor (2-3 deflection attempts) then eventually get genuine if you kept pushing. i've tried encoding this in the system prompt, tried training on annotated examples, tried constitutional AI-style principles. the model either goes full avoidant or skips straight to therapist mode. no middle, no arc. i don't know how to capture the slow opening up\n3. **the uncanny valley spikes** \\- 90% of the time it's him. then it'll say something he never would have said. reference something that didn't happen. use a phrase that's linguistically plausible but not HIM. i've started keeping a log of these failures to maybe train them out but there's no clear pattern. it's like the model is interpolating between him and some generic dude and occasionally lands wrong\n4. **inference optimization** \\- i know there's more performance on the table. have tried tensor parallelism across the 3090s but communication overhead eats the gains. looked into SGLang but haven't migrated yet. anyone running sub-400ms inference on 70b+ models with personality fine-tunes? what am i missing?\n\n**the question:**\n\nis there literature on modeling idiosyncratic personality in LLMs beyond basic fine-tuning? i've read the [character.ai](http://character.ai) scaling paper and some of the persona-chat stuff but it's all about creating coherent fictional personalities, not reconstructing a real specific person from data\n\nsometimes i run the same prompt through the model and through my memory of what he'd actually say and they match like 80% of the time. that last 20% keeps me up at night. i don't know if it's data sparsity or architecture limitations or if i'm just chasing something that can't be captured\n\nanyway. any architecture suggestions appreciated especially on the coherence decay and emotional modeling problems. those are the walls i keep hitting",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qupmeh/help_sub500ms_voicecloned_conversational_agent/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o3bttcm",
          "author": "Otherwise_Wave9374",
          "text": "This is a wild (and really thoughtful) build. On the agent side, it sounds like you are hitting 2 common walls at once: long-horizon persona drift and end-to-end latency. For drift, I have had better luck with periodically \"re-anchoring\" via retrieval of high-signal exemplars (not summaries), plus style constraints that are checked post-generation and corrected. For latency, speculative decoding + aggressive KV caching + smaller draft models helps, but 70B in sub-400ms is still brutal on 3090s. If you want more reading on agent memory/persona techniques, I have a few notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-03 12:04:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bzisx",
              "author": "kubrador",
              "text": "thank you!",
              "score": 1,
              "created_utc": "2026-02-03 12:45:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3c52bb",
          "author": "Its-all-redditive",
          "text": "What are you using for VAD/End of Turn signaling? You‚Äôre already at the near-practical limits of end to end latency in conversational speech with your setup. You can gain some additional optimization by placing the STT/TTS on an independent GPU setup from the LLM. There won‚Äôt be any resource interference from the overlapping pipelines. The LLM should be creating responses before the full transcription is done and the TTS should begin audio generation from the first available audio chunk before the full response is complete. The rest is just ‚Äúperceived‚Äù latency. In a real conversation, no one actually just begins speaking directly with an answer. Most of the time there are filler words like ‚Äòhmmm‚Äô, ‚Äòummm‚Äô etc or drawn out words like ‚Äòwellll‚Äô, ‚Äòsooo‚Äô etc. On top of this, perceived latency in real interaction is not just about audio. If I ask someone a question, the perceived latency is not when the other person actually speaks but when I register that they have acknowledged a response is wanted. A simple scrunch their eyes indicating a thoughtful reaction is enough to be perceived latency even if they don‚Äôt actually say anything for another second or two. \n\nReal conversational Ai will never be the STT>LLM>TTS workflow. It will be a single duplex speech to speech model. Check out NVIDIA Personaplex if you want to see end-game latency, but it will be a while before that type of model is smart enough to have real utility.",
          "score": 1,
          "created_utc": "2026-02-03 13:19:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dhwrk",
          "author": "notsofastaicoder",
          "text": "Why do you think the frequency or natural drop is happening? Any reason for not compressing context periodically?\n\nAlso interested in the hardware you are running on and costs  / min you will see with this setup?",
          "score": 1,
          "created_utc": "2026-02-03 17:23:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i74zu",
          "author": "Still-Bill-8436",
          "text": "I'm building a product that has an ultra-low-latency conversational agent pipeline baked-in. Been working (hear struggling) on it day/night for about ~2months, we're way below 500ms for processing (STT+LLM+TTS), with \"larger\" models, both LLM and TTS and FP8 quant.\n\nThere are various optimizations that allow us to get there. I am afraid half is just \"better hardware\", although there are some specific changes that help a lot.\n\nHappy to help, but before I start, could you clarify whether you intend to run this system for production and/or serve multiple users concurrently? This is tightly coupled with the options you have.\n\nSome breadcrumbs:\n- 400ms TTFT for the LLM is very high for a voice agent, you can get way lower (50-100ms) with inference hardware (that is not very expensive when rented), proper configuration and transport optimization for the requests\n- 4bit is very aggressive and I'm not sure Qwen2.5 will have very good agentic (complex toolcalling performance). This might work for just a conversational application, but to have the agent do actual things without a separate, supervisor model Qwen3 MoE models (you'll sure need more VRAM) will probably boost your results and prevent at least \"some\" psychosis.\n- 220ms TTFB is also high for the TTS, with a 4090 and Orpheus (fine-tuned too, excellent results) you can get closer to 100ms. \n- Get rid of HTTP1, EVERYWHERE. Make sure you use HTTP2 with SSE if you have micro-services communicating. If some of your requests have audio, ditch PCM16, compress everything with Opus, you'll save tens of milliseconds on request processing time\n- I realize this might just be a purely local personal project, but you haven't mentioned transport. For any kind of production application, you'll need a transport technology for the bi-dir audio to go from/to your users. This is going to sound weird, but it's a lot harder and not as easy as \"I'll just glue in WebRTC or a Websocket and have transport latency = RTT\". In reality, transport can add several hundreds ms of E2E latency and be several multiples of the RTT. I'd recommend not throwing this one under the rug if you want to go to production, as it will definitely impact your architectural choices. Transport out of the box roughly add 500ms with the mainstream solutions (Livekit SFU, Pipecat SFU..), so in reality you'd be around 1500ms E2E. \n\n\nI'm very interested in your take on the emotional parts / life-like conversational arcs as this is the problem were currently tackling.",
          "score": 1,
          "created_utc": "2026-02-04 10:01:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}