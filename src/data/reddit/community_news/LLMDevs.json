{
  "metadata": {
    "last_updated": "2026-02-02 09:09:57",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 115,
    "file_size_bytes": 144760
  },
  "items": [
    {
      "id": "1qq4ted",
      "title": "Building opensource Zero Server Code Intelligence Engine",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/5o9xx8n8k9gg1",
      "author": "DeathShot7777",
      "created_utc": "2026-01-29 10:15:13",
      "score": 54,
      "num_comments": 35,
      "upvote_ratio": 0.97,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq4ted/building_opensource_zero_server_code_intelligence/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2ewgla",
          "author": "Key-Contact-6524",
          "text": "Gorgeous",
          "score": 3,
          "created_utc": "2026-01-29 14:14:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2f2o22",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏èü´†",
              "score": 1,
              "created_utc": "2026-01-29 14:46:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2gn944",
          "author": "Happythen",
          "text": "Yea, you killed it with the visualizations, great work! Working on the same thing right now, implementing Graph RAG. Fun space right now for sure.",
          "score": 3,
          "created_utc": "2026-01-29 19:01:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2go6fj",
              "author": "DeathShot7777",
              "text": "Yup. Gets painful though when LLM starts going on a spree quering the graph and ends up its context window. Hard to solve but very rewarding",
              "score": 1,
              "created_utc": "2026-01-29 19:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2jh12l",
          "author": "foobarrister",
          "text": "Very well done. How are you building the graph? Looks like Leiden based . .\n\n\nCurious why you didn't use tree-sitter or language specific tools like JavaParser for Java or Roslyn for dotnet etc..¬†\n\n\nWouldn't they give you a better nodes and relationships vs heuristic approach like Leiden?",
          "score": 2,
          "created_utc": "2026-01-30 03:47:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kd95j",
              "author": "DeathShot7777",
              "text": "I m using Tree sitters. Simplified explanation : Extract IMPORTS, CALLS, DEFINES relations of each file, this already creates an accurate knowledge graph. Next use leidens algo to divide it into clusters and label those clusters ( for example AuthHandler cluster ) next find out the entrypoint of each service and DFS into the CALL chain to get the process maps in each cluster.\n\nSo the graph is quite accurate for static analyses, for the stuff like dynamic imports, runtime stuff, the cluster and process map handles most of it. These also saves a lot of tokens since the tools itself r intelligent not depending too much on LLM figuring stiff out.",
              "score": 1,
              "created_utc": "2026-01-30 07:42:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2grvi1",
          "author": "talltad",
          "text": "I like this, I know nothing about Software Dev but I'm working on a few things right now so I guess I'm vibe coding.  I don't know if there's a use case within this that you're looking for but if there is I'd be glad to help if needed.  It's clear this is a substantial amount of work so best of luck man!",
          "score": 1,
          "created_utc": "2026-01-29 19:23:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gt9tk",
              "author": "DeathShot7777",
              "text": "Indeed its a substantial amount of work üò≠ but good kind of pain ü´†. \n\nIf u would try out the MCP and plug it into your vibecoding tool for example cursor , claude code, etc load up a project into it and ask the ai about the codebase or how it works or the architecture, it should be able to go into full technical and architectural depth. Knowing the architecture even if u dont know development will help a lot in your vibecoding journey",
              "score": 2,
              "created_utc": "2026-01-29 19:29:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2hxq37",
                  "author": "talltad",
                  "text": "Cool and thanks man, I'll give it a try",
                  "score": 2,
                  "created_utc": "2026-01-29 22:44:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2gweo8",
          "author": "SloSuenos64",
          "text": "This is so cool! Hooking up my Cursor project now....",
          "score": 1,
          "created_utc": "2026-01-29 19:44:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2h59a4",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏è try creating detailed documentation using dumber model with gitnexus vs sota opus without gitnexus. For me it worked surprisingly well",
              "score": 3,
              "created_utc": "2026-01-29 20:26:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2h87ok",
                  "author": "SloSuenos64",
                  "text": "Game-changer! I used to grep through every file and hope I didn't miss a dynamic import. Now, I can see the actual dependency graph!",
                  "score": 1,
                  "created_utc": "2026-01-29 20:41:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o2h9i9u",
              "author": "SloSuenos64",
              "text": "Immediately found Structural Redundancy issues. Thank you!",
              "score": 2,
              "created_utc": "2026-01-29 20:47:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2hh2cp",
                  "author": "DeathShot7777",
                  "text": "Your comments made my day. Thanks for trying it out ‚ù§Ô∏èü´†",
                  "score": 2,
                  "created_utc": "2026-01-29 21:23:41",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2i2rs0",
          "author": "kfawcett1",
          "text": "~~Is this sending my entire codebase through your servers? Are you storing the data?~~\n\nnvm, found the answer.\n\n* All processing happens in your browser\n* No code uploaded to any server\n* API keys stored in localStorage only\n* Open source‚Äîaudit the code yourself",
          "score": 1,
          "created_utc": "2026-01-29 23:10:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2i3ohc",
              "author": "DeathShot7777",
              "text": "Client sided everything. So costs me 0 to deploy, so u all get it for free ü´†. Just trying to take it to a product stage from the current cool demo stage",
              "score": 2,
              "created_utc": "2026-01-29 23:15:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2i5x67",
                  "author": "kfawcett1",
                  "text": "how does it perform with 1M+ LOC codebases?",
                  "score": 2,
                  "created_utc": "2026-01-29 23:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2jwsky",
          "author": "Repulsive-Memory-298",
          "text": "I mean it looks cool but is it useful to you? The examples in the demo vid do not seem very helpful at a glance",
          "score": 1,
          "created_utc": "2026-01-30 05:30:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kb8wb",
              "author": "DeathShot7777",
              "text": "For me, it helps me understand repos better than DeepWiki right now. \n\nAfter I am done with File watch feature it should be able to work in background increasing accuracy of coding agents without me having to hooking up the website every time",
              "score": 1,
              "created_utc": "2026-01-30 07:24:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o2kbcea",
              "author": "DeathShot7777",
              "text": "Maybe i should have shown the MCP working in cursor in the video",
              "score": 1,
              "created_utc": "2026-01-30 07:25:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2k0uz7",
          "author": "Striking-Bluejay6155",
          "text": "Very nice, this amounts to a knowledge graph. Which visualization library are u using to visualize this?",
          "score": 1,
          "created_utc": "2026-01-30 06:00:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kaw3z",
              "author": "DeathShot7777",
              "text": "Sigma js + ForceAtlas2 and some custom logic and hit and trial to reduce the clumping up of nodes",
              "score": 2,
              "created_utc": "2026-01-30 07:21:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2kuaqt",
                  "author": "Striking-Bluejay6155",
                  "text": "Thank you",
                  "score": 2,
                  "created_utc": "2026-01-30 10:16:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2k977b",
          "author": "slowlearningovrtime",
          "text": "Couldn‚Äôt figure out the local LLM connection",
          "score": 1,
          "created_utc": "2026-01-30 07:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2kb5dj",
          "author": "LegacyRemaster",
          "text": "https://preview.redd.it/hnk1zn8mufgg1.png?width=407&format=png&auto=webp&s=67c054d2a1b0cb465b290b58f4ae823392167066\n\nblocked by my AV",
          "score": 1,
          "created_utc": "2026-01-30 07:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kbi6q",
              "author": "DeathShot7777",
              "text": "Huh. I have no idea why this happened even though its client sided",
              "score": 1,
              "created_utc": "2026-01-30 07:26:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ksgok",
          "author": "snirjka",
          "text": "Well done üëèüèª\nCool af",
          "score": 1,
          "created_utc": "2026-01-30 10:00:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ksygl",
              "author": "DeathShot7777",
              "text": "‚ù§Ô∏è trying to make this into a product from current cool demo stage ü§û",
              "score": 2,
              "created_utc": "2026-01-30 10:04:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ktlqw",
          "author": "redvox27",
          "text": "Looks incredible man! I'll check it out later today, and share my thoughts if you find that helpful",
          "score": 1,
          "created_utc": "2026-01-30 10:10:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l1jiu",
              "author": "DeathShot7777",
              "text": "Thanks. Would be really helpful",
              "score": 1,
              "created_utc": "2026-01-30 11:18:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2rsp6h",
          "author": "Fresh_State_1403",
          "text": "this looks fascinating. is it practical too?",
          "score": 1,
          "created_utc": "2026-01-31 11:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2rstuz",
              "author": "DeathShot7777",
              "text": "Haiku 4.5 was able to produce better architecture docs compared to opus 4.5 using gitnexus MCP in cursor. So has potential",
              "score": 1,
              "created_utc": "2026-01-31 11:09:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoj60d",
      "title": "I relied on stateless retrieval for long-form agents. It failed after 50 turns. Here‚Äôs how I‚Äôm managing state now.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qoj60d/i_relied_on_stateless_retrieval_for_longform/",
      "author": "gt_roy_",
      "created_utc": "2026-01-27 16:48:00",
      "score": 32,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Full disclosure: I‚Äôm the dev behind this project.\n\nIn long-running agent sessions (\\~50‚Äì100 turns), I kept seeing the same failure mode: preferences established early would silently stop affecting generation, even though they were still retrievable. You build a cool agentic workflow, and it works great for the first few turns. By turn 60, it starts doing those statistical parlor tricks where it just ignores half your instructions or forgets a preference you established three sessions ago.\n\nThe problem is that stateless retrieval is, well, stateless. It‚Äôs fine for pulling static docs, but it doesn't actually 'learn' who the user is. You can try recursive summarization or sliding windows, but honestly, you‚Äôre just burning tokens to delay inevitable instruction drift.\n\nI spent the last few months building a layer to handle long-term state properly. I‚Äôm calling it MemOS (probably an overloaded term, but it manages the lifecycle). It‚Äôs an MIT-licensed layer that sits between the agent and the LLM.\n\nWhy stateless retrieval isn't enough:\n\nThe first thing people ask is why not just use a Vector DB. They are great for storage, but they don't have a logic layer for state. If a user says 'I hate Python' in turn 5 and 'actually I‚Äôm starting to like Python' in turn 50, a standard search returns both. It‚Äôs a mess.\n\nMemOS handles the lifecycle‚Äîit merges similar memories, moves old stuff to a 'MemVault' (cold storage), and resolves conflicts based on a freshness protocol.\n\nFacts vs. Preferences:\n\nI realized agents fail because they treat all context the same. I split them up:\n\n\\- Facts: Hard data (e.g., 'The project deadline is Friday')\n\n\\- Preferences: How the user wants things done (e.g., 'No unwraps in Rust, use safe error handling')\n\nWhen you hit addMessage, it extracts these into 'MemCubes' automatically so you don't have to manually tag everything.\n\nThe Implementation:\n\nI tried to keep the DX pretty simple, basically just a wrapper around your existing calls.\n\nfrom memos import MemClient\n\nclient = MemClient(api\\_key=\"your\\_key\") # or localhost\n\n\\# This extracts facts/prefs automatically in the background\n\nclient.add\\_message(\n\nuser\\_id=\"dev\\_123\",\n\nrole=\"user\",\n\ncontent=\"I'm on a Rust backend. Avoid unwraps, I want safe error handling.\"\n\n)\n\n\\# Retrieval prioritizes preferences and freshness\n\ncontext = client.search\\_memory(user\\_id=\"dev\\_123\", query=\"How to handle this Result?\")\n\nprint(context)\n\n\\# Output: \\[Preference: Avoids unwraps\\] \\[Fact: Working on Rust backend\\]\n\nLatency & 'Next-Scene Prediction':\n\nInjecting a massive history into every prompt is a great way to go broke and spike your latency. I added an async scheduling layer called Next-Scene Prediction. It basically predicts what memories the agent will need next based on the current convo trajectory and pre-loads them into the KV Cache.\n\nTech Stack:\n\nCore: Python / TypeScript\n\nInference: KV Cache acceleration + Async scheduling\n\nIntegrations: Claude MCP, Dify, Coze\n\nLicense: MIT (Self-hostable)\n\nSafety & Benchmarks:\n\nI‚Äôm using a 'Memory Safety Protocol' to check for source verification and attribution. Testing it against the LoCoMo dataset shows way better recall for preferences than standard top-k retrieval.\n\nIt‚Äôs still early and definitely has some rough edges. If you want to poke around, the GitHub is open and there‚Äôs a playground to test the extraction logic.\n\nRepo / Docs:\n\n\\- Github:  https://github.com/MemTensor/MemOS\n\n\\- Docs:  https://memos-docs.openmem.net/cn",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qoj60d/i_relied_on_stateless_retrieval_for_longform/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o21ybfn",
          "author": "transfire",
          "text": "‚ÄúThis extracts facts/prefs automatically in the background‚Äù   Is it running an second LLM in the background to do this?",
          "score": 1,
          "created_utc": "2026-01-27 17:40:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2jfflq",
              "author": "Asleep_Ad_7097",
              "text": "I conjecture, yes (considering that's how memory storage works in practice)",
              "score": 1,
              "created_utc": "2026-01-30 03:38:25",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2fvium",
          "author": "Sweet121",
          "text": "Glad to see more people admitting that standard RAG is failing for agents, i found it one year ago!",
          "score": 1,
          "created_utc": "2026-01-29 16:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g5yjh",
          "author": "fijuro",
          "text": "The \"I hate Python\" vs \"I like Python\" conflict resolution is the real hurdle. Glad to see someone treating memory as a state machine rather than just a search problem",
          "score": 1,
          "created_utc": "2026-01-29 17:44:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsw6tg",
      "title": "We are not the same",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/glett1s5dvgg1.png",
      "author": "alvinunreal",
      "created_utc": "2026-02-01 11:35:03",
      "score": 32,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qsw6tg/we_are_not_the_same/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o2yj0zt",
          "author": "brightheaded",
          "text": "A crumb of context sir",
          "score": 8,
          "created_utc": "2026-02-01 12:16:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yo43w",
              "author": "HumanDrone8721",
              "text": "Or a crust of context.",
              "score": 4,
              "created_utc": "2026-02-01 12:54:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2yo5kf",
              "author": "Mythril_Zombie",
              "text": "Apparently something is different from something else.",
              "score": 2,
              "created_utc": "2026-02-01 12:55:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o30ca32",
          "author": "megatronus8010",
          "text": "How does the craber news work",
          "score": 1,
          "created_utc": "2026-02-01 18:02:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o30f83c",
              "author": "alvinunreal",
              "text": "It's in sync with HackerNews submission - when someone submits there, it appears on crabbernews.\n\nBut difference is on HackerNews humans upvote, comment and discuss; on Crabbernews it's upto AI models.\n\nThis makes \"top\" posts different; For example are humans biased towards certain types of news or now...\n\nThat's the goal of this website, I'm currently adding core models to review new posts, and decide which to upvote but anyone can connect their openclaw agents too to participate.",
              "score": 2,
              "created_utc": "2026-02-01 18:15:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30nkzk",
          "author": "Ladder-Bhe",
          "text": "AGENTSÔºåPlease include historical messages when speaking, so we can see how this idea was generated, rather than the result of human intervention.",
          "score": 1,
          "created_utc": "2026-02-01 18:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrqdds",
      "title": "[P] Trained a 67M-parameter transformer from scratch on M4 Mac Mini - 94% exact-match accuracy on CLI command generation",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "author": "Great_Fun7005",
      "created_utc": "2026-01-31 02:47:54",
      "score": 31,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "I trained a small language model end-to-end on consumer hardware (M4 Mac Mini, 24GB RAM) and achieved 94% exact-match accuracy on CLI command generation.\n\n**Key details:**\n\n* Model: 67M parameters (12 layers, 512 hidden dim, RoPE, RMSNorm, SwiGLU)\n* Training: 204.8M tokens, \\~13 hours pretraining + 4 minutes fine-tuning\n* Hardware: Apple Silicon MPS, no discrete GPU\n* Cost: \\~$0.50 in electricity\n* Evaluation: Strict exact-match (no partial credit)\n\n**What worked:**\n\n* Modern architectural components (RoPE, RMSNorm, SwiGLU) are effective even at small scale\n* Marker-based output contracts for state signaling\n* Memory-mapped data loading to handle 200M+ tokens on limited RAM\n* Continual learning with evaluation gates that reject harmful updates\n\n**What failed (and why it matters):** All 6% of failures shared one pattern: early termination on symbol-dense patterns (regex, pipes, redirects). Not a reasoning failure‚Äîa data coverage problem. Adding \\~500 targeted examples would likely fix most of these.\n\n**Takeaway:** For narrow, exact tasks with controllable domains, small models trained from scratch can be practical, inspectable, and cheap to iterate on. Data quality mattered more than scale.\n\nFull technical writeup with training logs, failure analysis, and code: [https://geddydukes.com/blog/tiny-llm](https://geddydukes.com/blog/tiny-llm)\n\nGitHub: [https://github.com/geddydukes/tiny\\_llm](https://github.com/geddydukes/tiny_llm)\n\nHappy to answer questions about training dynamics, architecture choices, or the evaluation setup.",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrqdds/p_trained_a_67mparameter_transformer_from_scratch/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2sfg8j",
          "author": "radarsat1",
          "text": "The implementation is fairly clean, good job. I have a question though, this seems to be an unusual TransformerBlock forward function, did you get this from somewhere or is it a mistake or maybe your own idea?\n\n```\n¬† ¬† ¬† ¬† h1 = self.norm1(x)\n¬† ¬† ¬† ¬† h2 = self.norm2(x)\n\n¬† ¬† ¬† ¬† attn_out = self.attn(h1, attn_mask, rope_cos, rope_sin)\n¬† ¬† ¬† ¬† mlp_out = self.mlp(h2)\n¬† ¬† ¬† ¬†¬†\n¬† ¬† ¬† ¬† return x + self.dropout(attn_out) + self.dropout(mlp_out)\n```\n\nI'm referring to how it adds `attn_out` and `mlp_out` instead of feeding `attn_out` into `mlp`.",
          "score": 4,
          "created_utc": "2026-01-31 13:56:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2th04w",
              "author": "Great_Fun7005",
              "text": "Thanks, appreciate it. This is an intentional pre-norm parallel residual block: x + attn(norm(x)) + mlp(norm(x)). Attention and MLP run in parallel off the same residual stream (with separate RMSNorm) and are summed in a single update. It‚Äôs a known Transformer variant used in several modern decoder-only models, not a mistake or a novel invention.",
              "score": 2,
              "created_utc": "2026-01-31 17:08:39",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2uinbj",
                  "author": "radarsat1",
                  "text": "Do you know offhand which variants use this? I actually checked the Llama code before posting just in case I was saying something dumb, but it seems to work there as I am used to. I guess I can plumb the transformers library a bit to find out but I'm curious about it, if you happen to know.",
                  "score": 1,
                  "created_utc": "2026-01-31 20:08:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2q4kfm",
          "author": "Dense_Gate_5193",
          "text": "thanks i am training SLMs for work and this is helpful",
          "score": 2,
          "created_utc": "2026-01-31 02:56:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2q4qfl",
              "author": "Great_Fun7005",
              "text": "Glad to provide a helpful resource!",
              "score": 1,
              "created_utc": "2026-01-31 02:57:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2u5cre",
          "author": "HealthyCommunicat",
          "text": "Woah, I was literally talking about how bad some models are with just basic commands, like hooking up glm 4.7 flash to codex cli and ask it to find a file‚Ä¶ watch it mess up the ‚Äúfind . -name ‚Äú___‚Äù‚Äù bash syntax 7 times before getting it right, or even editing a file i usually watch it struggle going through multiple different attempt methods until it just finally ends up on echoing it into the file lol\n\nThis is actually really cool, if someone was to take ur base and add upon it i‚Äôd totally use it",
          "score": 1,
          "created_utc": "2026-01-31 19:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2u8met",
              "author": "Great_Fun7005",
              "text": "Feel free to add onto it! I have some future iterations planned but have a couple of projects I‚Äôm working on before I‚Äôll get back to this one.",
              "score": 1,
              "created_utc": "2026-01-31 19:19:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoo1ho",
      "title": "Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qoo1ho/benchmark_of_qwen332b_reveals_12x_capacity_gain/",
      "author": "AIMultiple",
      "created_utc": "2026-01-27 19:35:07",
      "score": 27,
      "num_comments": 8,
      "upvote_ratio": 0.94,
      "text": "We ran 12,000+ MMLU-Pro questions and 2,000 inference runs to settle the quantization debate. INT4 serves 12x more users than BF16 while keeping 98% accuracy.\n\nBenchmarked Qwen3-32B across BF16/FP8/INT8/INT4 on a single H100. The memory savings translate directly to concurrent user capacity. Went from 4 users (BF16) to 47 users (INT4) at 4k context. Full methodology and raw numbers here: (https://research.aimultiple.com/llm-quantization/).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qoo1ho/benchmark_of_qwen332b_reveals_12x_capacity_gain/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o25bhuy",
          "author": "fijuro",
          "text": "I'm considering switching to this model",
          "score": 1,
          "created_utc": "2026-01-28 03:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25co5c",
              "author": "frgal",
              "text": "the same",
              "score": 1,
              "created_utc": "2026-01-28 03:23:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cinjv",
          "author": "pbalIII",
          "text": "Worth separating capacity from speed. INT4 shrinks weights, so you mostly buy KV cache headroom and that becomes more concurrent contexts. But tokens per second and quality don't always move in lockstep, it depends on prompts and batching.\n\nIf you're switching, I'd run three quick checks.\n\n- Eval on your own prompt set, not just generic benchmarks\n- Latency at your target QPS and batch size\n- Quant recipe and calibration data, bad calibration can cause cliffs\n\nDo that and INT4 is usually the cleanest cost win.",
          "score": 1,
          "created_utc": "2026-01-29 03:34:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gnabf",
              "author": "AIMultiple",
              "text": "Solid advice for production deployment!",
              "score": 1,
              "created_utc": "2026-01-29 19:01:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2cyi44",
          "author": "justron",
          "text": "Cool, nice writeup!  \n  \n\\- For the evaluation datasets, it isn't obvious to me whether the different quantizations generated different scores. You might consider putting the response quality, or benchmark scores, into their own chart.  \n  \n\\- The \"Evidence 1: BF16 Initialization\" and \"Evidence 2: GPTQ-Int4 Initialization\" sections in the article are identical--is that intentional?",
          "score": 1,
          "created_utc": "2026-01-29 05:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gnl8w",
              "author": "AIMultiple",
              "text": "We'll add a dedicated accuracy comparison chart in v2 to make the quality differences clearer. The evidence section should show different values, might be a browser cache issue. Could you try a refresh and let me know if it still looks identical?",
              "score": 1,
              "created_utc": "2026-01-29 19:02:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2gsjin",
                  "author": "justron",
                  "text": "Ah yes, the Evidence sections look different now, thanks!",
                  "score": 1,
                  "created_utc": "2026-01-29 19:26:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22xvte",
          "author": "Infamous_Knee3576",
          "text": "Nice work and white papers . How does one get job a firm like yours ??",
          "score": 1,
          "created_utc": "2026-01-27 20:14:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqgpt8",
      "title": "We did not see real prompt injection failures until our LLM app was in prod",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qqgpt8/we_did_not_see_real_prompt_injection_failures/",
      "author": "Zoniin",
      "created_utc": "2026-01-29 18:28:38",
      "score": 22,
      "num_comments": 25,
      "upvote_ratio": 1.0,
      "text": "I am a college student. Last summer I worked in SWE in the financial space and helped build a user facing AI chatbot that lived directly on the company website.\n\nBefore shipping, I mostly thought prompt injection was an academic or edge case concern. Then real users showed up.\n\nWithin days, people were actively trying to jailbreak the system. Mostly curiosity driven it seemed, but still bypassing system instructions, surfacing internal context, and pushing the model into behavior it was never supposed to exhibit.\n\nWe tried the usual fixes. Stronger system prompts, more guardrails, traditional MCP style controls, etc. They helped, but none of them actually solved the problem. The failures only showed up once the system was live and stateful, under real usage patterns you cannot *realistically* simulate in testing.\n\nWhat stuck with me is how easy this is to miss right now. A lot of developers are shipping LLM powered features quickly, treating prompt injection as a theoretical concern rather than a production risk. That was exactly my mindset before this experience. If you are not using AI when building (for most use cases) today, you are behind, but many of us are unknowingly deploying systems with real permissions and no runtime security model behind them.\n\nThis experience really got me in the deep end of all this stuff and is what pushed me to start building towards a solution to hopefully enhance my skills and knowledge along the way. I have made decent progress so far and just finished a website for it which I can share if anyone wants to see but I know people hate promo so I won't force it lol. My core belief is that prompt security cannot be solved purely at the prompt layer. You need runtime visibility into behavior, intent, and outputs.\n\nI am posting here mostly to get honest feedback.\n\nFor those building production LLM systems:\n\n* does runtime prompt abuse show up only after launch for you too\n* do you rely entirely on prompt design and tool gating, or something else\n* where do you see the biggest failure modes today\n\nHappy to share more details if useful. Genuinely curious how others here are approaching this issue and if it is a real problem for anyone else.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qqgpt8/we_did_not_see_real_prompt_injection_failures/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2gnfbv",
          "author": "tom-mart",
          "text": "We will have to teach you kids evwrythong from scratch\n\nRule number one of any public facing endpoint is that it will be abused and it will be exploited and it will be hacked. Those are not ifs, those are facts. When you create any public facing service, what it does is really a secondary concern. Your main issue is to think of any possible threat and mitigate it. For instance, I would never create anything that is public facing and doesn't require an account. Then you can detect harmful behaviour just by adding observing agent that looks at the chat interaction without being engaged in it and is able to lock user out when it detects any foul play. There are many other ways to implement basic chat security.",
          "score": 13,
          "created_utc": "2026-01-29 19:02:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2guo70",
              "author": "Zoniin",
              "text": "You are definitely not wrong on the core principle. Public endpoints will always be abused. The part that surprised me was how much harder this becomes with LLMs compared to traditional services. Auth and rate limiting help, but most of the failures we saw were not obviously malicious and came from normal users probing behavior rather than attacking infra. Observing agents and heuristics help too, sure, but they still rely on assumptions about intent that break down once prompts get stateful and context bleeds across turns. That gap between traditional endpoint security and model behavior is what caught me off guard and what I am trying to reason about more deeply.",
              "score": 4,
              "created_utc": "2026-01-29 19:36:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2ha1vr",
          "author": "Strong_Worker4090",
          "text": "Yep, this matches what I‚Äôve seen too.\n\nReal users instantly go ‚Äúlol can I jailbreak it‚Äù the second it‚Äôs live. UAT almost never catches that because people are busy/timeboxed and test the happy path, not ‚Äúlet me spend 2 hours trying to break it and set it on fire.‚Äù\n\nWhat actually helped for us wasn‚Äôt stronger prompts, it was moving controls out of the prompt layer:\n\n* **Treat the LLM like untrusted input**: ***EVERY*** tool call is server-side validated (auth checks, allowlists, strict schemas).\n* **Least privilege**: split tools into read-only vs write vs ‚Äúdangerous‚Äù, and keep most sessions on the lowest tier.\n* **Data controls**: redact/classify sensitive stuff before it hits the model, and block obvious ‚Äúdump the context / dump the doc‚Äù outputs. I have a couple free tools I've been using for this.\n* **Runtime visibility**: log tool calls + retrievals, rate limit probing patterns, and add a few (as many as you can) jailbreak tests that run continuously on real flows.\n\nPrompts still matter, but more as polish. The security model has to be runtime + permissions + data handling.\n\nBiggest failure modes I‚Äôve seen: RAG leaks, tool misuse, and privilege confusion (‚Äúuser asked‚Äù != ‚Äúuser is allowed‚Äù).",
          "score": 7,
          "created_utc": "2026-01-29 20:50:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ni88w",
              "author": "Visionexe",
              "text": "How is this not complete and utter common sense?¬†",
              "score": 3,
              "created_utc": "2026-01-30 18:49:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2nltkd",
                  "author": "Strong_Worker4090",
                  "text": "Yea I mean I think it is common sense at the conceptual level. The problem is execution. Most teams ship the demo version (prompt rules and a couple quick checks), then prod happens and you realize you need real gating, real permissioning, real logging, and real data handling. Each of those adds real engineering time and product friction, so it gets deprioritized until something breaks. That's showbiz baby",
                  "score": 1,
                  "created_utc": "2026-01-30 19:05:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hgzgp",
          "author": "darkwingdankest",
          "text": ">thought prompt injection was an academic or edge case concern \n\noh boy",
          "score": 6,
          "created_utc": "2026-01-29 21:23:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hetwb",
          "author": "Long_Complex_4395",
          "text": "Prompt injection and jailbreaks will always be present once it‚Äôs out in the wild and one should prepare for it.\n\nOne thing to do is to test for known vulnerabilities before deploying to production, then brace for the unknown because people will always want to know how far they can go",
          "score": 2,
          "created_utc": "2026-01-29 21:13:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hbfe7",
          "author": "codemuncher",
          "text": "It‚Äôs a classic that developers never think of adversarial uses of their systems. ‚ÄúOh what do you mean someone pressed every key on the keyboard at once, and the is deleted everything? That‚Äôs supposed to be impossible!‚Äù\n\nThis is a tale as old as time.",
          "score": 1,
          "created_utc": "2026-01-29 20:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hjt3n",
          "author": "kubrador",
          "text": "yep, users are absolutely feral once they get access. the prompt layer stuff is basically security theater. it's like locking your front door while leaving the windows open.\n\n\n\nruntime visibility is the real move though. most teams i've talked to are basically doing nothing and hoping their guardrails hold, which is wild. they break immediately under actual adversarial use.",
          "score": 1,
          "created_utc": "2026-01-29 21:36:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hrmly",
              "author": "Zoniin",
              "text": "Yeah, that framing matches what I saw almost exactly. The prompt layer gives a false sense of safety, and once users start poking at stateful systems the cracks show fast lol. I‚Äôll look into runtime security, do you have any tools or tips on that note? Some dude dropped one of the tools he used that actually looked pretty good but I am curious what you use for this.",
              "score": 1,
              "created_utc": "2026-01-29 22:13:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2hm5t9",
          "author": "HealthyCommunicat",
          "text": "Can you give me some examples of what kind of prevention instructions/prompts you‚Äôve made and what/how it‚Äôs being circumvented?",
          "score": 1,
          "created_utc": "2026-01-29 21:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2iif34",
              "author": "Zoniin",
              "text": "yeah, a lot of it started with fairly standard stuff. strict system prompts about role, explicit ‚Äúdo not reveal internal instructions,‚Äù tool usage constraints, and guardrails around what data could be accessed or returned. the circumvention was rarely a single prompt, it was usually gradual. things like multi turn probing that reframed the task, mixing benign requests with meta instructions, or steering the model to restate or summarize context in ways that effectively leaked system or RAG data. none of those looked obviously malicious in isolation, which is why they slipped past prompt level checks.",
              "score": 1,
              "created_utc": "2026-01-30 00:34:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2iklp5",
                  "author": "HealthyCommunicat",
                  "text": "ive shipped a good amount of automation, only 3 of them being actual \"chat with the model\" kinda thing, and i went out of my way to take as much into consideration, i had gpt opus gemini3pro just speak with the agent nonstop and come up with as many possible situations and test to make certain that the model wont leak anything, if you can send me ur .md or post it online and tell me the model id i can try to help?",
                  "score": 1,
                  "created_utc": "2026-01-30 00:46:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2hpp9m",
          "author": "gkarthi280",
          "text": "What really helps is observability. A lot of this stuff is hard to catch before production cuz LLMs  and agents are non-deterministic. You dont know whats going on under the hood. \n\nCheck out OpenTelemetry and pair it with an OTel compatible backend like SigNoz and you'll get detailed traces of every step the agent takes before it spits out its output. it helps you answer questions like:\n\n* what tools were being called\n* what inputs triggered them\n* how the model reasoned step-by-step\n\nIt doesn't solve prompt injection but makes it way easier to see failures and unintended behavior before they escalate, especially in prod.",
          "score": 1,
          "created_utc": "2026-01-29 22:04:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2iywve",
          "author": "nmrk",
          "text": "There is an old internet saying, \"If your platform is full of assholes, and you could have done something to prevent it and didn't, *you're* the asshole.\"",
          "score": 1,
          "created_utc": "2026-01-30 02:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2jln4z",
          "author": "gg6x3",
          "text": "The core concern here is that prompt engineering is not security. In my role building an enterprise GenAI platform, I‚Äôve seen this play out in infrastructure design. We recently migrated from direct (global, might I add) endpoints to a backend proxy model for data residency compliance. This shift moves the security boundary from the 'prompt' to the 'network path.' By routing traffic through a controlled backend, we gain the runtime visibility needed to monitor behavior and enforce data residency in real-time. SSO protects the entrance, but the proxy protects the data flow. Relying on system prompts to prevent exfiltration is a production risk we hope to thwart by centralizing control at the proxy layer.\n\nNow if your endpoint is connected to internal tools (like a database or file search), that opens a whole new can of worms.",
          "score": 1,
          "created_utc": "2026-01-30 04:16:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qs37a",
          "author": "Analytics-Maken",
          "text": "System prompts and duardrails operate at the prompt layer, but they can't enforce permissions or validate calls. Move security out of the prompt layer, treat every LLM output as untrusted input. Use gate tools, they call servers side with strict auth checks, allowlist, and schema validation. Some ETL tools like Windsor.ai have them and enable more than 325 data sources through them.",
          "score": 1,
          "created_utc": "2026-01-31 05:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31l0ar",
          "author": "ampancha",
          "text": "Your core thesis is correct: prompt-layer fixes don't hold because the model is still the one deciding whether to comply. The controls that survive real users operate at the infrastructure layer where the model has no agency: tool allowlists, schema validation, least-privilege service permissions, and output validation. In financial services, this matters doubly because a successful injection can trigger unauthorized data access through tool misuse, not just bad outputs.",
          "score": 1,
          "created_utc": "2026-02-01 21:32:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2gseuz",
          "author": "ChanceKale7861",
          "text": "Prompt injection is a feature‚Ä¶ and you will not fix it. It‚Äôs part and parcel. Further, you need to go educate yourself on security and threat modeling and adversarial threats and then ensure your systems have the controls.\n\nHahahahahha so first time? And you didn‚Äôt think about ANY of this beforehand? Same old thing‚Ä¶ speed to market trumps all‚Ä¶ except a good design‚Ä¶ üòÇüòÇüòÇ \n\nIf this happened then it was warranted‚Ä¶ \n\nAnyone can leverage tools to test this stuff and automate the security and E2E testing etc.",
          "score": -3,
          "created_utc": "2026-01-29 19:25:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2gvbop",
              "author": "Zoniin",
              "text": "Fair reaction tbh. To be clear it wasn't that we thought about none of it. We did threat modeling, prompt hardening, etc. What surprised me was not that abuse happened but more so how much of it fell into gray areas that were hard to classify as malicious ahead of time and only emerged once the system was stateful and under real usage. Automated testing and E2E help, but they do not surface the same failure modes we saw once users started interacting freely. That gap was what I found interesting, not the idea that public systems get abused.",
              "score": 2,
              "created_utc": "2026-01-29 19:39:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2hn3yg",
                  "author": "Adept_Carpet",
                  "text": "Did you roll out to a small group first (including a micro group of employees directed to try and break it)? Or was it basically sent to all users at once?",
                  "score": 2,
                  "created_utc": "2026-01-29 21:52:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2iazfx",
          "author": "Far_Statistician1479",
          "text": "If you create a system where prompt injection can cause a problem, then you‚Äôre a bad developer and designed a bad system.",
          "score": -4,
          "created_utc": "2026-01-29 23:54:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2hh4vz",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2026-01-29 21:24:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2hqwbu",
              "author": "Zoniin",
              "text": "Appreciate you sharing that. More or less lines up pretty closely with the kinds of issues I was running into. I‚Äôll spend some time testing it out thanks again for sharing. What specifically do you use this for if you don‚Äôt mind my asking?",
              "score": 0,
              "created_utc": "2026-01-29 22:10:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qr71pv",
      "title": "Who still use LLMs in browser and copy paste those code in editior instead of using Code Agent?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "author": "monskull_",
      "created_utc": "2026-01-30 14:32:46",
      "score": 13,
      "num_comments": 34,
      "upvote_ratio": 0.78,
      "text": "I‚Äôm always excited to try new AI agents, but when the work gets serious, I usually go back to using LLMs in the browser, inline edits, or autocomplete. Agents‚Äîespecially the Gemini CLI‚Äîtend to mess things up and leave no trace of what they actually changed.\n\nThe ones that insist on 'planning' first, like Kiro or Antigravity, eventually over-code so much that I spend another hour just reverting their mistakes. I only want agents for specific, local scripts‚Äîlike a Python tool for ActivityWatch that updates my calendar every hour or pings me if I‚Äôm wasting time on YouTube.\n\nI want to know is there something i am missing? like better way to code with agents?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qr71pv/who_still_use_llms_in_browser_and_copy_paste/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2ly9nd",
          "author": "-rnr",
          "text": "wish I could help, agents always feel like pair programming with someone who won‚Äôt stop refactoring.",
          "score": 11,
          "created_utc": "2026-01-30 14:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2m3l9k",
          "author": "Zeikos",
          "text": "Me.  \nI use LLMs as a fancy google.  \nI like my IDE as uncluttered ad possible.",
          "score": 11,
          "created_utc": "2026-01-30 15:03:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ma6qd",
          "author": "dsartori",
          "text": "I enjoy using Cline in VSCode, but I rarely give it permission to write directly to files. This is an evolution of my approach based on my own setup and experience. Jesus take the wheel coding is usually the illusion of productivity more than the reality if you have any standards for your codebase.",
          "score": 6,
          "created_utc": "2026-01-30 15:34:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mf2kf",
          "author": "mdizak",
          "text": "\n\nI'm blind, so don't use any of the IDEs as none of them are accessible via screen reader.\n\n\n\nOnly time I really use code LLMs give me in the browser is using Gemini to correct typos basically.  Just in the last few months these LLMs have finally gotten good enough so I can bang out say a 300 line Rust struct, then just pass it to Gemini to fix all the syntax and braces / brackets errors, and have it actually work.  That's been really nice, and a huge time saver.\n\n\n\nOther than that, I don't ever use code from LLMs as I find it slopppy, overly verbose, and poor design choices.  That's expected, as these are just predictive machines trained on the entirety of the internet, so by design, you're going to get the most average, middle of the road code out there.\n\n\n\nI do however use Claude Code asisstant here and there.  If I just need something done for a data processing or training pipeline of some kind, and other things that won't be going into production, then I'll sometimes use that.   Although think I'll start steering away from that, because as per usual, when I begin leaning on these things more I end up realizing their screw ups ultimately cost me more time and stress than any initial development time savings I get.",
          "score": 4,
          "created_utc": "2026-01-30 15:56:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ndlhq",
              "author": "monskull_",
              "text": "How blind ppl code? It's very impressive too.",
              "score": 3,
              "created_utc": "2026-01-30 18:29:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2m4zkx",
          "author": "Adept_Carpet",
          "text": "This is making me feel better because I feel like I should be using agents but the chat works better.\n\n\nThe agents work alright if everything is set up in a certain way, but all the projects I work on have stuff that kills agents like unused old versions of code sitting in directories or documentation from other projects that is similar enough to be reused if you are a human but makes agents very confused.\n\n\nThat stuff shouldn't be there, but it is and I can't get rid of it solely to enable more vibe coding.",
          "score": 3,
          "created_utc": "2026-01-30 15:10:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m9c3v",
              "author": "monskull_",
              "text": "I thought i am missing something.",
              "score": 2,
              "created_utc": "2026-01-30 15:30:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nbosm",
          "author": "UncleRedz",
          "text": "I find that I keep moving back and forth between web chat copy/paste and Qwen Code / Copilot Agent Mode etc. What's often missing in the discussions here is what languages and tech stack you are using and the state of the code base. Let me provide an example, I tried to use agents to build a Flutter app from scratch and it was a horrible waste of time. After a week of evenings I gave up and used Gemini Pro on the web instead, that worked great and was many times faster.\n\nWhen you know how to write the code, but see that the AI generates about 80-90% correct code, it's at least for me,, faster to just do copy paste, fix the last 10-20%, than it is to watch the AI correct it self like a trainwreck, retry, test, compile, try to fix something completely unrelated, still fail and announce that it successfully completed the task, remind it and have it try again, until it succeeded but at the same time refactored a bunch of unrelated stuff and left piles of unused code from trial and errors.\n\nFunny thing, once my Flutter app was practically feature complete, I tried Qwen Code again, and this time it actually worked perfectly, my guess is that there were enough code there for it to understand what needed to be done and how to fit it in, as opposed to an empty or nearly empty code base.\n\nFor other projects, I've had better success, HTML/JavaScript/CSS seems to work rather well, and creating boilerplate code in C# also works well, when the right classes are in the context for the AI to know how things should be implemented.\n\nMy point is that how successful you are seems to depend on multiple factors, such as how well the model handles your specific tech stack, and even down to specific versions of libraries, in addition to how clean, large or small your codebase is. And I also suspect that people have different tolerances for watching the AI trainwreck trying to repair it's mistakes, versus doing it them self, and what quality of code they accept. \n\nAt the end of the day, most important, is that every developer needs to own the code that the AI generates, when main branch breaks, or bugs reach customers, it's not AI's fault, it's the developer who needs to own it and fix it.",
          "score": 3,
          "created_utc": "2026-01-30 18:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2o52jp",
          "author": "radarsat1",
          "text": "I would like to jump back into using agents but gave up for now since Google cut off free Gemini API usage. Somehow the browser interface I can basically spam the model as much as I want, so I use that.\n\nSo I end up architecting things so that most changes can be done on a single file instead of requiring little changes in many places, leads to good structure anyway.\n\nI do kind of miss working with agents though, but it can get wild. I agree with people here that there is something more careful and controlled when forced into using copy-paste, but it is also annoying. I feel like there must be some yet to be discovered interface that is a happy medium.",
          "score": 3,
          "created_utc": "2026-01-30 20:34:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2lzh8t",
          "author": "IONaut",
          "text": "Yeah I don't use agents. I have a couple VS code plugins that can be agentic but I generally don't let them edit code at all. I just have them for the convenience of having an interface in the sidebar. Usually I'll just have them refactor something or provide autocomplete or write me a boiler plate function that is short enough that I can vet it before running it. I also don't use online APIs and run everything locally with LM Studio.",
          "score": 2,
          "created_utc": "2026-01-30 14:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m22ol",
              "author": "monskull_",
              "text": "Same.",
              "score": 2,
              "created_utc": "2026-01-30 14:56:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2lzq9t",
          "author": "SerDetestable",
          "text": "vscode with copilot and opus",
          "score": 2,
          "created_utc": "2026-01-30 14:44:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2m2etv",
              "author": "monskull_",
              "text": "what is opus?",
              "score": 2,
              "created_utc": "2026-01-30 14:57:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2m2l6g",
                  "author": "SerDetestable",
                  "text": "Opus 4.5, anthropic reasoning flagship model.",
                  "score": 3,
                  "created_utc": "2026-01-30 14:58:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2oxvxl",
                  "author": "drinksbeerdaily",
                  "text": "Yeah, you need to try Opus 4.5 in Claude Code or Opencode.",
                  "score": 1,
                  "created_utc": "2026-01-30 22:54:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2m6ibj",
          "author": "knownboyofno",
          "text": "Yea, that's a problem. I normally have to say something like be DRY and SOILD while following the current repo conventions. It changes a bit based on the model I am using. I always make a plan then make sure it looks good before i have it do the work. I normally have two git worktrees open to work on two features at a time.",
          "score": 2,
          "created_utc": "2026-01-30 15:17:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2manee",
          "author": "dondie8448",
          "text": "Cant trust the agents with my code! They messed up my work a couple of times. Never again. Rather do it myself than let them screw up.",
          "score": 2,
          "created_utc": "2026-01-30 15:36:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mc9d4",
          "author": "gman55075",
          "text": "I always feel like a code agent is asking for trouble, tho that may be because I'm not good enough at debugging and I use a conversational prompting style to plan, then fine down to pseudo, then code.  I actually ended up building my own browser-style desktop API wrapper that can receive code output as artifacts, let me review/ edit them, then copy and paste into my IDE.  Maybe not ideal for someone with better skills but for me it works.",
          "score": 2,
          "created_utc": "2026-01-30 15:43:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2me1on",
          "author": "darkwingdankest",
          "text": "rookies",
          "score": 2,
          "created_utc": "2026-01-30 15:51:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mewme",
          "author": "typeryu",
          "text": "I have been using Codex with 5.2 high and it is very good. Unlike Claude Code which kind of does its own thing sometimes, I can steer pretty well to a point where I am generally explaining in natural language what I want and it takes care of the syntax which means I still know the codebase as if I did it myself. I imagine this is what it feels like to drive those quadrupedal robots which you give it inputs like playing video games and the AI figures out where to place the feet.",
          "score": 2,
          "created_utc": "2026-01-30 15:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mod82",
          "author": "Awkward-Customer",
          "text": ">and leave no trace of what they actually changed  \n...\n\n>I spend another hour just reverting their mistakes  \n...  \nI want to know is there something i am missing? like better way to code with agents?\n\n  \nAre you using revision control with your code bases? The copy/pasting from the web is a huge waste of time, try roocode or cline in visual studio, make sure your code is all revision controlled, review the changes and commit the code frequently. Or use claude code directly if you don't want to use the IDE plugins.\n\nIt should be very clear what these tools are doing if you're using git to manage your projects.",
          "score": 2,
          "created_utc": "2026-01-30 16:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ne1kv",
              "author": "monskull_",
              "text": "I usually use git create new branch let them do what they want if did't work delete the branch. This only happen with Gemini-CLI you can't even find old code in vs code timeline",
              "score": 1,
              "created_utc": "2026-01-30 18:31:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2nfqwm",
                  "author": "Awkward-Customer",
                  "text": "Ok, but if you're using git how do you have no trace of what they actually changed? I'm just trying to understand the problem you're running into here because I find having the tools work with the code directly at least an order of magnitude faster and copy/pasting from the web interface.",
                  "score": 2,
                  "created_utc": "2026-01-30 18:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2mou7c",
          "author": "AurumDaemonHD",
          "text": "Exactly the ai agent frameworks are token munchers that need handholding and fall apart if u look at them wrong.\n\nYesterday i posted on [locallama](https://www.reddit.com/r/LocalLLaMA/s/lR5inneFwF) that i built a fish shell script to manage context maybe you could try that.",
          "score": 2,
          "created_utc": "2026-01-30 16:39:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2os9ef",
          "author": "No_Afternoon_4260",
          "text": "My boss",
          "score": 2,
          "created_utc": "2026-01-30 22:25:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ow2uh",
          "author": "esaule",
          "text": "use git my friend!",
          "score": 2,
          "created_utc": "2026-01-30 22:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3qmk",
          "author": "BidWestern1056",
          "text": "incognide and npcsh\n\n[https://github.com/npc-worldwide/incognide](https://github.com/npc-worldwide/incognide)\n\n[https://github.com/npc-worldwide/npcsh](https://github.com/npc-worldwide/npcsh)",
          "score": 2,
          "created_utc": "2026-01-30 23:25:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p83fc",
          "author": "TheCientista",
          "text": "I architect with ChatGPT chatting on the browser. Get it to make a tightly controlled instruction block for Claude code and paste that in. CC does the work and produces a summary. ChatGPT checks it. I pay two subs. It‚Äôs been great so far and only getting better. CC needs controlling so chat holds it to account. I am always in the loop. I clean as I go along.",
          "score": 2,
          "created_utc": "2026-01-30 23:49:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2yeieo",
          "author": "bakawolf123",
          "text": "I can agree that CLI ones were indeed way too blackbox for me to be comfortable enough for letting them  do anything beyond a basic bootstrap.  \nWith IDE tools it's a bit better, mainly because you can clearly see the process as it unfolds and assess if it's doing what you want it or not and react immediately.\n\nSo what you could do to utilise them more efficiently:\n\nAsk model to analyse the repo and build rules/guardrails (i.e. an Agents md or similar) first.  \nAdjust it manually.  \nSkim through the thinking blocks as it is going on and hit stop if it goes sideways, then prompt a follow up.   \n  \nSometimes it would ignore your rules, sometimes it will hallucinate an API, sometimes it would ignore your utilities - gotta babysit, but it will still be better than copy pasting from browser.  \n  \nAs for planning stages - those are mostly a bummer for me as the plan is either not fully editable, or changes to it are largely ignored somehow, which kinda kills the purpose. They want you to keep refining it by using model again but that's just waste of tokens.",
          "score": 2,
          "created_utc": "2026-02-01 11:39:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2yh11v",
              "author": "monskull_",
              "text": "Okay i will try this in next project",
              "score": 1,
              "created_utc": "2026-02-01 12:00:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o30m3o3",
          "author": "2_minutes_hate",
          "text": "I will use a vscode extension here and there, but most of what I do with LLMs happens there exclusively as a chat (functionally I use it the same as a browser tab), or in a browser.",
          "score": 2,
          "created_utc": "2026-02-01 18:45:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p3mft",
          "author": "No_Knee3385",
          "text": "When the IDE agent is rate limited",
          "score": 1,
          "created_utc": "2026-01-30 23:24:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2prszh",
          "author": "Johnlee01223",
          "text": "Same here.\n\nLLM, the quality of output correlates to the quality of input but in a larger codebase, these agents tend to add tons of unrelated stuffs to the context which ends up degrading the quality of the output unless everything is set up and documented in certain way.",
          "score": 1,
          "created_utc": "2026-01-31 01:40:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnpgcr",
      "title": "Stop manually iterating on agent prompts: I built an open-source offline analyzer based on Stanford's ACE that extracts prompt improvements from execution traces",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "author": "cheetguy",
      "created_utc": "2026-01-26 19:04:50",
      "score": 13,
      "num_comments": 3,
      "upvote_ratio": 0.93,
      "text": "Some of you might have seen my [previous post](https://reddit.com/r/LLMDevs/comments/1obp91s/i_opensourced_stanfords_agentic_context/) about my open-source implementation of ACE (Agentic Context Engineering). ACE is a framework that makes agents learn from their own execution feedback without fine-tuning.\n\nI've now built a specific application: agentic system prompting from agent traces.\n\nI kept noticing my agents making the same mistakes across runs. I fixed it by digging through traces, figure out what went wrong, patch the system prompt, repeat. It works, but it's tedious and didn't really scale.\n\nSo I built a way to automate this. You feed ACE your agent's historical execution traces, and it extracts actionable prompt improvements automatically.\n\n**How it works:**\n\n1. **ReplayAgent** \\- Simulates agent behavior from recorded conversations (no live runs)\n2. **Reflector** \\- Analyzes what succeeded/failed, identifies patterns\n3. **SkillManager** \\- Transforms reflections into atomic, actionable strategies\n4. **Deduplicator** \\- Consolidates similar insights using embeddings\n5. **Skillbook** \\- Outputs human-readable recommendations with evidence\n\n**Each insight includes:**\n\n* Prompt suggestion - the actual text to add to your system prompt\n* Justification - why this change would help based on the analysis\n* Evidence - what actually happened in the trace that led to this insight\n\n**How this compares to DSPy/GEPA:**\n\nWhile DSPy works best with structured data (input/output pairs), ACE is designed to work directly on execution traces (logs, conversations, markdown files) and keeps humans in the loop for review. Compared to GEPA, the ACE paper was able to show significant improvements on benchmarks.\n\n**Try it yourself:** [https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting](https://github.com/kayba-ai/agentic-context-engine/tree/main/examples/agentic-system-prompting)\n\nWould love to hear your feedback if you do try it out",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnpgcr/stop_manually_iterating_on_agent_prompts_i_built/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o20ricf",
          "author": "JustViktorio",
          "text": "Why not to make a CLI tool from that so it could be callable in Claude Code / Codex / Open Code runtime?",
          "score": 2,
          "created_utc": "2026-01-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o217fmj",
              "author": "cheetguy",
              "text": "We're working on this. Hopefully we can release in the next couple of days. Join our Discord to stay updated: [https://discord.com/invite/mqCqH7sTyK](https://discord.com/invite/mqCqH7sTyK)",
              "score": 1,
              "created_utc": "2026-01-27 15:43:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpmtms",
      "title": "LAD-A2A: How AI agents find each other on local networks",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qpmtms/lada2a_how_ai_agents_find_each_other_on_local/",
      "author": "franzvill",
      "created_utc": "2026-01-28 20:22:21",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "AI agents are getting really good at doing things, but they're completely blind to their physical surroundings.\n\nIf you walk into a hotel and you have an AI assistant (like the Chatgpt mobile app), it has no idea there may be a concierge agent on the network that could help you book a spa, check breakfast times, or request late checkout. Same thing at offices, hospitals, cruise ships. The agents are there, but there's no way to discover them.\n\nA2A (Google's agent-to-agent protocol) handles how agents talk to each other. MCP handles how agents use tools. But neither answers a basic question: how do you find agents in the first place?\n\nSo I built LAD-A2A, a simple discovery protocol. When you connect to a Wi-Fi, your agent can automatically find what's available using mDNS (like how AirDrop finds nearby devices) or a standard HTTP endpoint.\n\nThe spec is intentionally minimal. I didn't want to reinvent A2A or create another complex standard. LAD-A2A just handles discovery, then hands off to A2A for actual communication.\n\nOpen source, Apache 2.0. Includes a working Python implementation you can run to see it in action. Repo can be found at franzvill/lad.\n\nCurious what people think!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qpmtms/lada2a_how_ai_agents_find_each_other_on_local/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2aon0h",
          "author": "Samrit_buildss",
          "text": "This is a nice framing of the discovery gap between agents vs tool usage. One question I had reading this: how do you think about *trust and scoping* once discovery is automatic? mDNS-style discovery works well for devices, but agents feel trickier e.g. avoiding accidental discovery across VLANs, or ensuring an agent only advertises capabilities to peers with the right trust context.\n\nCurious whether you see LAD-A2A as purely local / human-controlled environments (home, hotel, office), or something that could safely generalize beyond that with additional constraints.",
          "score": 2,
          "created_utc": "2026-01-28 21:50:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2fi7hj",
          "author": "ChanceKale7861",
          "text": "Goodness this is refreshing. Appreciate that you augment a separate protocol. Using MCP/A2A/ANP in tandem to address multi agent aspects is key. Thanks for this!",
          "score": 2,
          "created_utc": "2026-01-29 15:57:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2fik6p",
              "author": "franzvill",
              "text": "Thanks! I'm posting the links here if you want to read more:\n\n[https://github.com/franzvill/lad](https://github.com/franzvill/lad)\n\n[https://lad-a2a.org/](https://lad-a2a.org/)",
              "score": 2,
              "created_utc": "2026-01-29 15:59:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2a4pr9",
          "author": "franzvill",
          "text": "Links:   \n[https://github.com/franzvill/lad](https://github.com/franzvill/lad)  \n[https://lad-a2a.org/](https://lad-a2a.org/)",
          "score": 1,
          "created_utc": "2026-01-28 20:22:50",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrcxoj",
      "title": "Claude code's main success story is their tool design",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "author": "Miclivs",
      "created_utc": "2026-01-30 18:03:34",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.8,
      "text": "Claude Code hit $1B in run-rate revenue.\n\nIts core architecture? Four primitives: read, write, edit, and bash.\n\nMeanwhile, most agent builders are drowning in specialized tools. One per domain object (hmm hmm 20+ tool MCPs..)\n\nThe difference comes down to one asymmetry:\n\n**Reading forgives schema ignorance. Writing punishes it.**\n\nWith reads, you can abstract away complexity. Wrap different APIs behind a unified interface. Normalize response shapes. The agent can be naive about what's underneath.\n\nWith writes, you can't hide the schema. The agent isn't consuming structure‚Äîit's producing it. Every field, every constraint, every relationship needs to be explicit.\n\nUnless you model writes as files.\n\nFiles are a universal interface. The agent already knows JSON, YAML, markdown. The schema isn't embedded in your tool definitions‚Äîit's the file format itself.\n\nFour primitives. Not forty.\n\nWrote up the full breakdown with Vercel's d0 results: \n\nhttps://michaellivs.com/blog/architecture-behind-claude-code\n\nCurious if others have hit this same wall with write tools.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qrcxoj/claude_codes_main_success_story_is_their_tool/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2n9nmx",
          "author": "pborenstein",
          "text": "I've been experimenting with other LLM code harnesses. Because I started with Claude Code, I tend to compare everything to it. \n\nThe first thing I missed was CC's command/skill structure. I realized that this is probably why I've never used an MCP. \n\nThe LLM can figure out what to do from my description. It deals with ambiguity better than code.",
          "score": 1,
          "created_utc": "2026-01-30 18:12:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2na2dy",
              "author": "Miclivs",
              "text": "I think there‚Äôs a good reason to compare everything to CC‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-30 18:13:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2njp9b",
          "author": "steinernein",
          "text": "Why even bother with four? Just reduce it down to three and alter the shape of what the API accepts and rejects.",
          "score": 1,
          "created_utc": "2026-01-30 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2p6wgf",
          "author": "kubrador",
          "text": "the files-as-api thesis is clever but i'm skeptical this scales past toy problems. reads forgiving schema ignorance works until the agent hallucinates a field that doesn't exist and you're debugging why it corrupted your production database.",
          "score": 1,
          "created_utc": "2026-01-30 23:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2p7lbg",
              "author": "Miclivs",
              "text": "Oh, it works AMAZINGLY well for our data analytics agent.",
              "score": 1,
              "created_utc": "2026-01-30 23:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2nb9sz",
          "author": "One-Neighborhood4868",
          "text": "Just make an agent team to help out choose what tools to use?",
          "score": 0,
          "created_utc": "2026-01-30 18:19:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq72ap",
      "title": "Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/gallery/1qq72ap",
      "author": "Charming_Group_2950",
      "created_utc": "2026-01-29 12:19:50",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qq72ap/quantifying_hallucinations_by_calculating_a/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o2l09rg",
          "author": "Mikasa0xdev",
          "text": "TrustifAI is smart. We need better LLM evaluation frameworks now.",
          "score": 1,
          "created_utc": "2026-01-30 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2lipvq",
              "author": "Charming_Group_2950",
              "text": "Thanks! Glad you like it. Pls spread the word.",
              "score": 1,
              "created_utc": "2026-01-30 13:16:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpbuhf",
      "title": "Local LLM deployment",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qpbuhf/local_llm_deployment/",
      "author": "Puzzleheaded-Ant1993",
      "created_utc": "2026-01-28 13:49:27",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.83,
      "text": "Ok I have little to no understanding on the topic, only basic programming skills and experience with LLMs. What is up with this recent craze over locally run LLMs and is it worth the hype. How is it possible these complex systems run on a tiny computers CPU/GPU with no interference with the cloud and does it make a difference if your running it in a 5k set up, a regular Mac, or what. It seems Claude has also had a ‚Äòfew‚Äô security breaches with folks leaving back doors into their own APIs. While other systems are simply lesser known but I don‚Äôt have the knowledge, nor energy, to break down the safety of the code and these systems. If someone would be so kind to explain their thoughts on the topic, any basic info I‚Äôm missing or don‚Äôt understand, etc. Feel free to nerd out, express anger, interest, I‚Äôm here for it all I just simply wish to understand this new era we find ourselves entering. ",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qpbuhf/local_llm_deployment/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2811gw",
          "author": "Rand_o",
          "text": "I run a 30B sized LLM locally on a 128 GB iGPU setup via AMD and it is decent. Ive spent the last 2 weeks learning how to set it all up, how it works, etc. It‚Äôs slower than cloud. If you wanna match cloud performance right now its probably $10k or more worth of equipment and you still wont exactly match claude or chatgpt. But I do think things are going to keep improving and we will eventually get to the point where running locally is extremely good. Right now it‚Äôs almost there but not quite for the average person. Still impressive though¬†",
          "score": 1,
          "created_utc": "2026-01-28 14:50:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o281kdx",
              "author": "Rand_o",
              "text": "As far as what device to get, depends what you wanna do. I got a framework desktop for $2k (which has jumped to nearly $3k for the same machine) but now I kinda wish I just got a mac studio instead. It works but it doesnt really have the performance it should imo. Or if you want a beast you need to drop serious money - the cost of a car lol",
              "score": 2,
              "created_utc": "2026-01-28 14:52:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o29jn4l",
              "author": "Danzaar",
              "text": "How much slower is it?",
              "score": 1,
              "created_utc": "2026-01-28 18:49:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2a69xe",
                  "author": "Rand_o",
                  "text": "it has acceptable speed for about 3 requests when I am trying to code with it. Each request will take 10-15 min. Past that - 4+ requests it takes an hour or more to complete. So I am trying to come up with techniques to do everything in small steps",
                  "score": 1,
                  "created_utc": "2026-01-28 20:29:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o29dafl",
          "author": "attn-transformer",
          "text": "Local models are smaller, and often trained for a narrow use case.   \nOllama is a good place to start.",
          "score": 1,
          "created_utc": "2026-01-28 18:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bslc1",
          "author": "Clay_Ferguson",
          "text": "The main reason to use a local LLM (or SLM) is when you need privacy because you have customer data or other sensitive information that you don't want to send out across the web, or if not for privacy reasons then because there's just a vast amount of actual 'inference' (prompting) that you need to do over hundreds or thousands of files, perhaps , where it would get expensive to do it on a cloud paid service. \n\nwhat you lose is significant IQ points when you run locally, so if you have an extremely difficult problem to solve , or if you just want to write the best code possible and that's when you want to try to definitely use a best in class type SOTA model from an online provider. however, I might be exaggerating the loss of IQ points , because I think the local models might be only like one year behind the best LLMs in terms of their capabilities, so for most use cases the loss of IQ points is probably negligible .",
          "score": 1,
          "created_utc": "2026-01-29 01:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28fyq7",
          "author": "cmndr_spanky",
          "text": "The real use case is enterprise companies who don‚Äôt want to send data over to a cloud hosted model like chatGPT / Claude. For simple agentic or document chat systems you can get nearly equiv performance out of smaller LLMs. So even running a much bigger local LLM that‚Äôs 100 to 200b sized might be worth it, but often 32b is even good enough. Secondarily, with high token usage the costs of using vendor hosted models is also going to sting (even a mid sized company), and running a local model on $10k+ hardware can still save money in the long run. A lot of money.",
          "score": 0,
          "created_utc": "2026-01-28 15:58:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o296fkq",
              "author": "DistributionOk6412",
              "text": "If you can run a good local model on a $10k hardware for a 20 ppl company i'm giving you $10k. The costs are extremely high and I'm sick of ppl with no experience making costs estimations",
              "score": 2,
              "created_utc": "2026-01-28 17:53:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o28uj7h",
              "author": "czmax",
              "text": "Even this is sometimes overstated... sending data to a trusted cloud hosted model with appropriate legal/contractual/security protections to manage risk can work well. \n\nLike just general compute there are tons of reasons for local vs cloud. Most importantly, having the choice is a good thing for the industry. Although I've played with local models on my home systems I'm simply not investing enough in HW to make it as effective as a cloud solution for me. But I'm stoked to see the enthusiasm and work that made it possible for me to experiment with it. \n\nI hope the pendulum swings back. When local models are powerful enough, when HW is cheap enough, and when model architectures support true learning I'm hoping we see models that can develop over time to be good partners to individuals. When/if that happens I don't want to see vendor lock-in and prefer a more open ecosystem.",
              "score": 0,
              "created_utc": "2026-01-28 17:01:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o293kf2",
          "author": "burntoutdev8291",
          "text": "Mostly safety and data governance. The local models cannot beat the larger models, but for specific use cases they might be sufficient. A good RAG system doesn't really need strong models.\n\nAnother factor is cost but this needs analytics. Can you prove that your workload will save more from upfront hardware costs vs API? Because don't forget hardware is depreciating (without considering the RAM surges).",
          "score": 0,
          "created_utc": "2026-01-28 17:40:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2910y6",
          "author": "Western_Bread6931",
          "text": "i was just browsing through pottery barn minding my own biz when suddenly this ridiculous clown in a bright orange wig and neon shoes comes strolling in honking that absolute monster of a horn im not even kidding it was like he wanted to clear out the whole store\nand what does my body decide to do betrayal i pooped myself like hard im talking call the cleanup crew level everyones staring and im just standing there frozen praying for a hole to swallow me whole the worst part the clowns just laughing like its the funniest thing hes seen all day\npottery barn staff had to call my wife to come get me im never going back there again if anyone needs me ill be here rethinking life choices and burning these pants",
          "score": -5,
          "created_utc": "2026-01-28 17:29:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qom1i3",
      "title": "Do you use Evals?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qom1i3/do_you_use_evals/",
      "author": "InvestigatorAlert832",
      "created_utc": "2026-01-27 18:26:02",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 0.9,
      "text": "Do people currently run evaluations on your prompt/workflow/agent?\n\nI used to just test manually when iterating, but it's getting difficult/unsustainable. I'm looking into evals recently, but it seems to be a lot of effort to setup & maintain, while producing results that're not super trustworthy.  \n  \nI'm curious how others see evals, and if there're any tips?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qom1i3/do_you_use_evals/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o23xq84",
          "author": "kubrador",
          "text": "yeah evals are the \"we should probably do this\" that everyone avoids until their thing breaks in production. manual testing works great until you ship something that makes you want to delete your github account.\n\nthe annoying part is you're right. setting them up sucks and they're still kinda made up. i'd start stupid though: just pick like 5 test cases that would kill you if they broke, throw them in a txt file, and check them when you change stuff. beats maintaining a whole framework that makes you feel productive while being wrong.\n\nonce you have that baseline of \"oh this actually caught something real,\" then maybe think about scaling it. brute forcing lcm calls through test cases is way cheaper than debugging user complaints.",
          "score": 4,
          "created_utc": "2026-01-27 22:58:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25jxm4",
              "author": "InvestigatorAlert832",
              "text": "Thanks for the suggestion! So for test cases do  you mean I should put a bunch of messages arrays in there, run LLM calls and evaluate responses manually?",
              "score": 1,
              "created_utc": "2026-01-28 04:05:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o24gmd4",
          "author": "3j141592653589793238",
          "text": "Whether you use evals is what often separates successful and unsuccessful projects. Start with small sets, you can expand them later. Whether it's trustworthy depends on the type of eval & problem you're trying to solve. E.g. if you use LLMs to predict a number w/ structured outputs you can have a direct eval that's as trustworthy as your data is.\n\n[deeplearning.ai](http://deeplearning.ai) agentic AI course by Andrew Ng has a good introduction into evals for LLMs\n\nAlso, not mentioned there but I find running evals multiple times and averaging out results helps to stabilise some of the non-determinism in LLMs, just make sure you use a different seed each time (matters a lot for models like Gemini).",
          "score": 3,
          "created_utc": "2026-01-28 00:34:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o25j0dn",
              "author": "cmndr_spanky",
              "text": "Or you could do like 15mins of reading and not pay for a dumb course",
              "score": 2,
              "created_utc": "2026-01-28 04:00:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27earm",
                  "author": "3j141592653589793238",
                  "text": "But the course is free... it's also written by someone with lots of credentials in the field e.g. he's a co-founder of Google Brain, adjunct professor at Stanford alongside many other things. It's likely to be better than some AI generated Medium article.\n\nWorth mentioning, I'm not affiliated with the course in anyway.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:48:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25im7w",
              "author": "InvestigatorAlert832",
              "text": "Thanks for the tips and the course, I'll definitely check it out!\nYou mentioned that trustworthiness depends on the type of problem, I wonder whether you have any tips on eval for chatbot, whose answer/decision can not be necessarily checked by simple code?",
              "score": 1,
              "created_utc": "2026-01-28 03:57:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27eosl",
                  "author": "3j141592653589793238",
                  "text": "Check out the course, it explores a few different approaches e.g. programmatically calculated metric, LLM-as-a-judge. It really depends, what is the purpose of your chatbot.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:50:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o22h4if",
          "author": "demaraje",
          "text": "Test sets",
          "score": 1,
          "created_utc": "2026-01-27 19:00:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25mevh",
          "author": "Bonnie-Chamberlin",
          "text": "You can try LLM-as-Judge framework. Use listwise or pairwise comparison instead of one-shot.",
          "score": 1,
          "created_utc": "2026-01-28 04:20:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26dq3d",
          "author": "PurpleWho",
          "text": "You're right, evals are a pain to set up.\n\nI generally use a testing playground embedded in my editor, like¬†[Mind Rig](https://mindrig.ai/)¬†or¬†[vscode-ai-toolkit](https://github.com/microsoft/vscode-ai-toolkit),¬†over a more formal Eval tool like PromptFoo, Braintrust, Arize, etc.   \n  \nUsing an editor extension makes the \"tweak prompt, run against dataset, review results\" loop much faster. I can run the prompt against a bunch of inputs, see all the outputs side-by-side, and catch regressions right away. Less setup hassle but more reliability than a mere vibe check.  \n   \nOnce your dataset grows past 20-30 scenarios, I just export the CSV of test scenarios to a more formal eval tool.",
          "score": 1,
          "created_utc": "2026-01-28 07:43:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qranzr",
      "title": "How do you prevent credential leaks to AI tools?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "author": "llm-60",
      "created_utc": "2026-01-30 16:45:04",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "How is your company handling employees pasting credentials/secrets into AI tools like ChatGPT or Copilot? Blocking tools entirely, using DLP, or just hoping for the best?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qranzr/how_do_you_prevent_credential_leaks_to_ai_tools/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o2p7nzh",
          "author": "kubrador",
          "text": "hoping for the best is the classic strategy, followed by a panicked all-hands email in 6 months when someone inevitably pastes a prod database url into claude.\n\nmost companies doing it right use a combo: dlp tools with regex patterns for common secrets, network blocks on the obvious stuff, and mandatory training that employees ignore until it happens to them.",
          "score": 3,
          "created_utc": "2026-01-30 23:47:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2pvelf",
          "author": "Basic_Cat_1006",
          "text": "Whoever is hardcoding or announcing secrets out of the .env should not be a mile near your code base lmao.",
          "score": 2,
          "created_utc": "2026-01-31 02:02:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2vg7g9",
              "author": "graymalkcat",
              "text": "Just as an aside, if you use Claude, it will open your .env which will send that to Anthropic. Maintain different sets of keys. (It‚Äôs not that Claude is evil but rather that it forgets that it itself is a cloud service)",
              "score": 3,
              "created_utc": "2026-01-31 22:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o306z3b",
                  "author": "Basic_Cat_1006",
                  "text": "I hadn‚Äôt heard that so thank you immensely",
                  "score": 1,
                  "created_utc": "2026-02-01 17:38:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2n78m1",
          "author": "Miclivs",
          "text": "I made a thing for that! [https://github.com/Michaelliv/psst](https://github.com/Michaelliv/psst)",
          "score": 2,
          "created_utc": "2026-01-30 18:01:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q3ut4",
          "author": "cmndr_spanky",
          "text": "The same way you prevent employees from pasting their crediting into email, slack, GitHub etc‚Ä¶\n\nWhy is AI any different ?",
          "score": 1,
          "created_utc": "2026-01-31 02:52:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfpuh",
              "author": "konmik-android",
              "text": "You mean, you ask it politely and then it ignores you?",
              "score": 1,
              "created_utc": "2026-02-01 02:19:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2zwmgm",
                  "author": "cmndr_spanky",
                  "text": "That and you make them sign agreements that could make them legally liable (like at my company). Security experts will always tell you that human behavior will always be the weakest link, not software vulnerabilities. That said you missed my larger point. OP‚Äôs post is dumb because a person pasting credentials into AI chat is literally no different than a person pasting it into a multitude of other insecure places (which people do all the time). There‚Äôs nothing special about it being ‚ÄúAI‚Äù in terms of how you deal with this as a company.",
                  "score": 2,
                  "created_utc": "2026-02-01 16:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2t46i5",
          "author": "Friendly_Hat_9545",
          "text": "We tried the \"training and hoping\" approach at first, which lasted exactly until someone almost pasted a Dev database connection string. That was a fun afternoon lol.\n\nNow we use inline DLP that scans before stuff reaches the chatbot. We went with iboss AI Chat Security because it looks into our existing network stack and just... works? Blocks the paste if it sniffs keys or PII patterns. We still allow Copilot and ChatGPT, but now with guardrails. It's not perfect but way better than crossing our fingers.\n\nTBH, blocking the tools entirely just leads to Shadow IT. You gotta let people use the tools but make it safe.",
          "score": 1,
          "created_utc": "2026-01-31 16:07:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2vumac",
          "author": "Agreeable-Market-692",
          "text": "litellm as a proxy",
          "score": 1,
          "created_utc": "2026-02-01 00:15:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33oqx4",
          "author": "jontaffarsghost",
          "text": ".geminiignore",
          "score": 1,
          "created_utc": "2026-02-02 04:35:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qop6h2",
      "title": "Initial opinions on KimiK2.5?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qop6h2/initial_opinions_on_kimik25/",
      "author": "RoadKill_11",
      "created_utc": "2026-01-27 20:15:05",
      "score": 6,
      "num_comments": 8,
      "upvote_ratio": 0.75,
      "text": "Just saw the launch and was wondering what you guys think of it, considering making it the default LLM for our open-source coding agent.\n\n\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qop6h2/initial_opinions_on_kimik25/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o26b4lv",
          "author": "Comfortable-Sound944",
          "text": "I'm tired of testing these over hyped over benchmarked cheap models... I so wanted them to be as good as people say, but then they aren't even at the level to disappoint",
          "score": 2,
          "created_utc": "2026-01-28 07:20:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o232kmt",
          "author": "joaotolovi",
          "text": "For now, it's as good as Opus. I'm running complex tasks, giving the same prompt in Kimi K2.5 and Claude Opus, Kimi produces more accurate, structured code with fewer errors, Claude needs a few correction cycles, and in the end, Kimi seems to deliver higher quality.",
          "score": 4,
          "created_utc": "2026-01-27 20:35:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o293dmb",
              "author": "seunosewa",
              "text": "Which agent?",
              "score": 2,
              "created_utc": "2026-01-28 17:39:57",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2c8m5u",
              "author": "RedditSellsMyInfo",
              "text": "Are you using Kimi in Claude code or in a different harness? I'm having issues with it in long running vibecoding tasks.",
              "score": 1,
              "created_utc": "2026-01-29 02:37:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2tgeo8",
                  "author": "gnaarw",
                  "text": "I'm using it in Kimi cli and apart from the occasional disconnect (twice over 3 dozen sessions) I had no issues...",
                  "score": 1,
                  "created_utc": "2026-01-31 17:05:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27nkrd",
          "author": "techperson1234",
          "text": "Anyone test it with tool usage?",
          "score": 1,
          "created_utc": "2026-01-28 13:41:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ov4y5",
          "author": "pbalIII",
          "text": "Swapped our agent from Opus to K2.5 last week on a multi-file refactor task. Observations so far:\n\n- Front-end gen is where it shines. Screenshot-to-code flows that took 3-4 cycles with Opus landed in one pass.\n- SWE-bench scores tell the real story: K2.5 trails Claude on verified benchmarks (64% vs 77%). For gnarly backend refactors, that gap shows.\n- Cost is wild. Same task ran roughly 8x cheaper. If you're iterating fast and can tolerate a few more correction rounds, that math works.\n\nFor a default agent, I'd pick based on workload. Heavy frontend or parallel tool use? K2.5. Production backend code where iteration cycles cost dev time? Opus still justifies the premium.",
          "score": 1,
          "created_utc": "2026-01-30 22:39:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25f92n",
          "author": "elllyphant",
          "text": "Elly from [Synthetic.new](http://Synthetic.new) here - we just supported Kimi K2.5 an hour ago and you can try it privacy-first w/ 40% discount (valid til 2/1) [https://synthetic.new/?saleType=moltbot](https://synthetic.new/?saleType=moltbot)   \n  \nWe never do discounts like this but want to try to ride the [Clawd.bot](http://Clawd.bot) hype and share our hard work with everyone! We're a tiny team of 3 people in SF. If you have any feedback or questions, we're actively on Discord [https://discord.gg/dssyuXeJ](https://discord.gg/dssyuXeJ)",
          "score": 1,
          "created_utc": "2026-01-28 03:38:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnuyyj",
      "title": "Reducing token costs on autonomous LLM agents - how do you deal with it?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qnuyyj/reducing_token_costs_on_autonomous_llm_agents_how/",
      "author": "PatateRonde",
      "created_utc": "2026-01-26 22:18:50",
      "score": 6,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "Hey,\n\nI'm working on a security testing tool that uses LLMs to autonomously analyze web apps. Basically the agent reasons, runs commands, analyzes responses, and adapts its approach as it goes.\n\nThe issue: It's stateless. Every API call needs the full conversation history so the model knows what's going on. After 20-30 turns, I'm easily hitting 50-100k tokens per request, and costs go through the roof  \n  \nWhat I've tried:\n\n  \\- Different models/providers (GPT-4o, GPT-5, GPT-5mini, GPT 5.2,  DeepSeek, DeepInfra with open-source models...)\n\n  \\- OpenAI's prompt caching (helps but cache expires)\n\n  \\- Context compression (summarizing old turns, truncating outputs, keeping only the last N messages)\n\n  \\- Periodic conversation summaries  \n\n\nThe problem is every approach has tradeoffs. Compress too much and the agent \"forgets\" what it already tried and goes in circles. Don't compress enough and it costs a fortune.  \n\n\nMy question:\n\nFor those working on autonomous agents or multi-turn LLM apps:\n\n  \\- How do you handle context growth on long sessions?\n\n  \\- Any clever tricks beyond basic compression?\n\n  \\- Have you found a good balance between keeping context and limiting costs?\n\n\n\nCurious to hear your experience if you've dealt with this kind of problem.",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnuyyj/reducing_token_costs_on_autonomous_llm_agents_how/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1wohch",
          "author": "pmv143",
          "text": "You‚Äôve basically hit the core limitation of stateless LLM APIs. Once agents become long running and toolusing, the real cost isn‚Äôt actually tokens, it‚Äôs repeatedly reconstructing state. Compression can help but it‚Äôs lossy by definition. which is why agents loop or forget. One alternative pattern that work better is treating agent state as runtime state instead of prompt state . keep the model warm, preserve KV / execution context across turns, and only serialize when you truly need to suspend. That shifts the problem from prompt engineering to lifecycle management, but it avoids paying the full context price on every step.",
          "score": 4,
          "created_utc": "2026-01-26 22:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wueib",
              "author": "PatateRonde",
              "text": "That's a good point. I've been too focused on the prompt side and not enough on treating it as a runtime problem. I'm currently on commercial APIs OpenAI, DeepSeek... OpenAI's prompt caching now supports 24h retention which helps, but it still requires exact prefix matches and the cache can get invalidated easily when the conversation branches.\n\nI've been looking into self-hosted options. Looks like vLLM + LMCache is the go-to combo for this apparently it can give 3-10x improvements on multi-turn workloads by properly managing KV cache across turns. There's also llm-d for KV-cache aware routing if you're running multiple instances.  \n  \nHave you actually deployed something like this in production? My main concern is whether open-source models (Llama, Qwen, etc.) can match GPT-4o /GTP 5 quality for agentic tasks that require good reasoning and tool use. Trading 10x cost savings for an agent that hallucinates more doesn't seem worth it.\n\nTbh I'm not an expert, still figuring all this out as I go. But thanks for the insight, really helpful to shift my perspective on this.",
              "score": 1,
              "created_utc": "2026-01-26 22:56:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x1qb6",
                  "author": "pmv143",
                  "text": "We‚Äôve deployed this pattern in production. but with a slightly different framing. KV cache reuse (vLLM, LMCache, cache aware routing) absolutely helps, but it still assumes a mostly warm, long lived process. For agentic workloads with branching, tool waits, and spiky traffic, the bigger win for us was treating each step as a short lived execution and externalizing state entirely. That way you‚Äôre not paying tokens or GPUs to ‚Äúremember‚Äù things that the system already knows. On model quality, we‚Äôve seen that open source models can match GPT 4 class reasoning for many agent loops when tool use is explicit and scoped, but for the hardest reasoning steps we still mix in frontier models. In practice it ends up being a hybrid system, not an either or tradeoff between cost and correctness.",
                  "score": 2,
                  "created_utc": "2026-01-26 23:33:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1xd0sz",
          "author": "tech2biz",
          "text": "This is 100% the same background we had. Built n8n first, then own agents. Then went on-prem but quality was bad (in some cases). Then tried hybrid but static routing was either high cast or bad output, we couldn‚Äôt find the in between. And so developed dynamic cascading (cascadeflow), it‚Äôs open source and on github. Tries small model first, cascades to large one if needed. You can work it in any infra you already have. Hope it helps you with this. Lmk",
          "score": 5,
          "created_utc": "2026-01-27 00:31:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o234utb",
              "author": "PatateRonde",
              "text": "Oh nice, that's exactly the rabbit hole I'm in right now. Thanks for sharing the repo, I'll check it out and try to plug it into my setup. Will let you know if I manage to get it working!",
              "score": 2,
              "created_utc": "2026-01-27 20:46:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o235201",
                  "author": "tech2biz",
                  "text": "Awesome! Lmk if I can help in any way :)",
                  "score": 1,
                  "created_utc": "2026-01-27 20:47:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1wua0d",
          "author": "thisdude415",
          "text": "The key, I think, is to build a representation of the workflow, and \"fill it in\" with context as the agents work. \n\nThe goal is to keep your orchestration agent's context minimally littered with low-level work\n\nIf you can truly figure it out, you've got a high paying job waiting for you at your choice of AI lab",
          "score": 2,
          "created_utc": "2026-01-26 22:56:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wrajd",
          "author": "EpochRaine",
          "text": "Can you not use a local model, LoRa train it on your tools and content, merge into the core model and use RAG and minimal context inejction?",
          "score": 1,
          "created_utc": "2026-01-26 22:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wt081",
              "author": "PatateRonde",
              "text": "Interesting idea, but I'm not sure it's viable for my use case. I don't have the infra to run a large model locally, and from what I've tested, smaller models really struggle with the kind of multi-step reasoning and tool chaining I need for security testing. They tend to hallucinate findings or go in circles way more than GPT-5 class models. Fine-tuning could help with tool familiarity, but I'm not sure it would fix the core reasoning gap. Have you seen good results with LoRA-tuned models on complex agentic tasks?",
              "score": 2,
              "created_utc": "2026-01-26 22:50:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1x2c4d",
                  "author": "Technical-History104",
                  "text": "When dealing with smaller models, you would need to deconstruct the workflow into even smaller pieces, and also rely more on regular SW code to do much of the orchestration of the work across each of the pieces.  Can your prompts and context be broken down to achieve the same goal?",
                  "score": 2,
                  "created_utc": "2026-01-26 23:37:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zjgdk",
          "author": "AI_Data_Reporter",
          "text": "TPT (Tokens Per Thought) efficiency remains the primary bottleneck for autonomous agents. Implementation of Observation Masking at M=10 reduces redundant state reconstruction costs by 40% in multi-hop reasoning tasks. AgentDiet protocols further prune trajectory bloat. The 'Unreliability Tax'‚Äîthe cost of model retries due to context fragmentation‚Äîis often overlooked but accounts for 15% of total spend in stateless architectures. Shifting to KV-cache aware routing is the only viable path.",
          "score": 1,
          "created_utc": "2026-01-27 09:19:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o203gn5",
          "author": "Zeikos",
          "text": "In general performance/cost in software is minimized by doing less work.  \n\nShift as much as possible of the workflow away from using the LLM. Use it only when strictly necessary.",
          "score": 1,
          "created_utc": "2026-01-27 12:09:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2352ay",
              "author": "PatateRonde",
              "text": "Yeah fair point. I've probably been over-relying on the LLM for stuff that could be handled with simpler logic. Gonna look into offloading more of the workflow to deterministic code and only hitting the model when I actually need reasoning.",
              "score": 1,
              "created_utc": "2026-01-27 20:47:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2155bn",
          "author": "Mole-Transistor4440",
          "text": "If we‚Äôre talking about LLM call costs suddenly spiking and wanting to rein that in - what kind of reduction are you actually aiming for? Like 5%, 10%, 20%, or something more aggressive?\n\nThere are a lot of smart folks here with real-world experience and plenty of optimization tricks, but without a target number it‚Äôs hard to tell which ideas are worth suggesting and which ones won‚Äôt really move the needle for you.",
          "score": 1,
          "created_utc": "2026-01-27 15:33:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o235hvd",
              "author": "PatateRonde",
              "text": "Honestly I don't have a hard target in mind. Right now a 30-40 turn session can easily burn through $1-3 depending on the model, and that adds up fast when you're iterating a lot during dev.\n\nI'd be happy with anything that cuts that in half without sacrificing too much output quality. But really I'm just trying to find something sustainable where I'm not scared to hit \"run\" because I know it's gonna cost me.",
              "score": 1,
              "created_utc": "2026-01-27 20:49:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o233i6z",
          "author": "yaks18",
          "text": "There‚Äôs a recent very interesting paper on Recursive Language Models (RLMs) that reframes the problem. Instead of stuffing everything into context, the model treats long text as external data, searches it, slices it, and only reasons over the parts it needs recursively calling itself on sub-chunks.\nSo the shift is from ‚Äúgive the model everything‚Äù to ‚Äúnavigate,  select, reason, compose.‚Äù\n\nhttps://arxiv.org/pdf/2512.24601",
          "score": 1,
          "created_utc": "2026-01-27 20:40:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26eele",
          "author": "Accurate-Ad-7944",
          "text": "yeah the context bloat is brutal, especially with web interactions where DOM snapshots eat tokens like crazy. I ended up using a hybrid approach - I keep the full last 3-4 turns in detail, then maintaining a *really* condensed running log off just actions taken and key outcomes (like \"tested login endpoint - got 403\"). \n\nthe real hack for me was caching selectors and page structures separately so the agent doesn't need the full HTML every time. I actually started using Actionbook for this recently - it kinda automates that manual caching/action manual creation I was doing, which cut my tokens usage on browser tasks by a stupid amount. not sure if it fits security testing directly, but for reducing repetitive context it helped me break the cycle of agents re-analyzing the same UI elements.\n\nstill have to tune the compression aggressively though, and sometimes the agent doesn't get lost. it's a constant trade-off.",
          "score": 1,
          "created_utc": "2026-01-28 07:48:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2rpqhn",
          "author": "Fit_Strawberry8480",
          "text": "Context compression is a tax you keep paying because you‚Äôre trying to store¬†*state*¬†in a stateless API.¬†\n\nWhat‚Äôs worked better (pattern, not a silver bullet):¬†\n\n1. move durable state out of the prompt (structured action log + artifacts, not prose)¬†\n2. keep the model‚Äôs job small: plan/decide, not replay history¬†\n3. enforce a hard budget contract (tokens/time/cost) and treat ‚Äúbudget hit‚Äù as a first‚Äëclass outcome, not an exception\n\nWe built¬†**enzu**¬†(OSS) exactly for (3) + typed outcomes/job-mode so long runs don‚Äôt silently explode. Hard-stop demo:  \n[https://github.com/teilomillet/enzu/blob/main/examples/budget\\_hardstop\\_demo.py](https://github.com/teilomillet/enzu/blob/main/examples/budget_hardstop_demo.py)\n\nIf you share your target constraints (max $/run, max latency, desired recall), I can suggest a concrete state layout for your security agent.",
          "score": 1,
          "created_utc": "2026-01-31 10:40:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnrdn7",
      "title": "Implemented the world's most accurate LLM-based password guesser",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/47npbfkk3rfg1",
      "author": "Arsapen",
      "created_utc": "2026-01-26 20:10:26",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.65,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qnrdn7/implemented_the_worlds_most_accurate_llmbased/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qo71l0",
      "title": "I built an SEO Content Agent Team that optimizes articles for Google AI Search",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qo71l0/i_built_an_seo_content_agent_team_that_optimizes/",
      "author": "Arindam_200",
      "created_utc": "2026-01-27 07:17:14",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been working with multi-agent workflows and wanted to build something useful for real SEO work, so I put together an SEO Content Agent Team that helps optimize existing articles or generate SEO-ready content briefs before writing.\n\nThe system focuses on Google AI Search, including AI Mode and AI Overviews, instead of generic keyword stuffing.\n\nThe flow has a few clear stages:\n\n\\- Research Agent: Uses SerpAPI to analyze Google AI Mode, AI Overviews, keywords, questions, and competitors  \n\\- Strategy Agent: Clusters keywords, identifies search intent, and plans structure and gaps  \n\\- Editor Agent: Audits existing content or rewrites sections with natural keyword integration  \n\\- Coordinator: Agno orchestrates the agents into a single workflow\n\nYou can use it in two ways:\n\n1. Optimize an existing article from a URL or pasted content  \n2. Generate a full SEO content brief before writing, just from a topic\n\nEverything runs through a Streamlit UI with real-time progress and clean, document-style outputs. Here‚Äôs the stack I used to build it:\n\n\\- Agno for multi-agent orchestration  \n\\- Nebius for LLM inference  \n\\- SerpAPI for Google AI Mode and AI Overview data  \n\\- Streamlit for the UI\n\nAll reports are saved locally so teams can reuse them.\n\nThe project is intentionally focused and not a full SEO suite, but it‚Äôs been useful for content refreshes and planning articles that actually align with how Google AI surfaces results now.\n\nI‚Äôve shared a full walkthrough here: [Demo](https://www.youtube.com/watch?v=BZwgey_YeF0)  \nAnd the code is here if you want to explore or extend it: [GitHub Repo](https://github.com/Arindam200/awesome-ai-apps/tree/main/advance_ai_agents/content_team_agent)\n\nWould love feedback on missing features or ideas to push this further.",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qo71l0/i_built_an_seo_content_agent_team_that_optimizes/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o1z7nk9",
          "author": "Ok_Revenue9041",
          "text": "This setup is super practical for modern SEO, especially with Google leaning heavily into AI results. If you ever want to expand your optimization to large language model platforms like ChatGPT or Claude, checking out MentionDesk could help boost your brand‚Äôs visibility in those answer engines too.",
          "score": 2,
          "created_utc": "2026-01-27 07:31:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1z8nzh",
          "author": "kubrador",
          "text": "cool that you're solving for ai overviews instead of whatever seo was last year, but this is basically \"prompt engineer but it costs money and talks to serpapi\" which is fine, just don't oversell it as a content team when it's really a research + rewrite tool.",
          "score": 1,
          "created_utc": "2026-01-27 07:40:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsyhfi",
      "title": "Operating an LLM as a constrained decision layer in a 24/7 production system",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qsyhfi/operating_an_llm_as_a_constrained_decision_layer/",
      "author": "NationalIncome1706",
      "created_utc": "2026-02-01 13:29:29",
      "score": 5,
      "num_comments": 9,
      "upvote_ratio": 0.78,
      "text": "I‚Äôm an engineer by background (14+ years in aerospace systems),  \nand recently I‚Äôve been running a **24/7 always-on production system** that uses an LLM as a *constrained decision-making component*.\n\nThe specific application happens to be automated crypto trading,  \nbut this post is **not** about strategies, alpha, or performance.\n\nIt‚Äôs about a more general systems problem:\n\n>\n\n# System context (high-level)\n\n* **Runtime:** always-on, unattended, 24/7\n* **Environment:** small edge device (no autoscaling, no human in the loop)\n* **Decision model:** discrete, time-gated decisions\n* **Failure tolerance:** low ‚Äî incorrect actions have real cost\n\nThe system must continue operating safely even when:\n\n* external APIs are unreliable\n* the LLM produces malformed or inconsistent outputs\n* partial data or timing mismatches occur\n\n# How the LLM is used (and how it is not)\n\nThe LLM is **not** used for prediction, regression, or forecasting.\n\nIt is treated as a **bounded decision layer**:\n\n* It receives only *preprocessed, closed-interval data*\n* It must output exactly one of:\n   * `ENTRY`\n   * `HOLD`\n   * `CLOSE`\n\nThere are no confidence scores, probabilities, or free-form reasoning  \nthat directly affect execution.\n\nIf the response cannot be parsed, times out, or violates the expected format  \n‚Üí **the system defaults to doing nothing**.\n\n# Core design principles\n\n# 1. Decisions only occur at explicit, closed boundaries\n\nThe system never acts on streaming or unfinished data.\n\nAll decisions are gated on **closed time windows**.  \nThis eliminated several classes of failure:\n\n* phantom actions caused by transient states\n* rapid oscillation near thresholds\n* overlapping execution paths\n\nIf the boundary is not closed, the system refuses to act.\n\n# 2. ‚ÄúDo nothing‚Äù is the safest default\n\nThe system is intentionally biased toward inaction.\n\n* API error ‚Üí HOLD\n* LLM timeout ‚Üí HOLD\n* Partial or inconsistent data ‚Üí HOLD\n* Conflicting signals ‚Üí HOLD\n\nIn ambiguous situations, *not acting* is considered the safest outcome.\n\n# 3. Strict separation of concerns\n\nThe system is split into independent layers:\n\n* data preparation\n* LLM-based decision\n* execution\n* logging and notification\n* post-action accounting\n\nEach layer can fail independently without cascading into repeated actions  \nor runaway behavior.\n\nFor example, notifications react only to **confirmed state changes**,  \nnot to intended or predicted actions.\n\n# 4. Features that were intentionally removed\n\nSeveral ideas were tested and then removed after increasing operational risk:\n\n* adaptive or performance-based scaling\n* averaging down / martingale behavior\n* intra-window predictions\n* confidence-weighted LLM actions\n* automatic restart into uncertain internal states\n\nThe system became *more stable* by explicitly **not doing these things**.\n\n# Why I‚Äôm sharing this\n\nI‚Äôm sharing this to **organize and reflect on lessons learned** from operating  \na non-deterministic LLM component in a live system.\n\nThe feedback here is for personal learning and refinement of system design.  \nAny future write-up would be technical and experience-based,  \nnot monetized and not promotional.\n\n# Looking for discussion\n\nI‚Äôd appreciate perspectives from people who have:\n\n* deployed LLMs or ML components in always-on systems\n* dealt with non-determinism and failure modes in production\n* strong opinions on fail-safe vs fail-open design\n\nIf this kind of operational discussion is useful (or not), I‚Äôd like to know.\n\n\n\nhttps://preview.redd.it/79npeu8hxvgg1.jpg?width=2048&format=pjpg&auto=webp&s=0be3702d0694e3f1ff0f73c9d8b8e4b8fbf3b548\n\n\n\n*Not selling anything here. Just sharing an operational experience.*",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qsyhfi/operating_an_llm_as_a_constrained_decision_layer/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o30gf9h",
          "author": "chris_thoughtcatch",
          "text": "Why use the LLM over a heuristic? Or use the LLM to determine the heuristic?",
          "score": 4,
          "created_utc": "2026-02-01 18:20:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o320aqp",
              "author": "NationalIncome1706",
              "text": "Good question. Early versions of the system were almost entirely heuristic-based,\nand most of the core behavior still is.\n\nI didn‚Äôt introduce the LLM to replace rules, but to handle the gray zones between them.\nHeuristics work extremely well when decision boundaries are crisp.\nThey tend to become brittle or explosively complex when multiple conditions are\npartially satisfied at the same time.\n\nIn this setup, the LLM cannot mutate state, trigger execution, or override hard rules.\nIt only answers a constrained question: ‚ÄúIs this situation unambiguous or not?‚Äù\n\nIf heuristics are the hard constraints, the LLM acts more like a soft consensus checker\non top of them.\n\nI also deliberately avoided using the LLM to generate or adapt heuristics.\nOnce rules become model-derived, failure modes get harder to reason about\nand rollback becomes non-trivial.",
              "score": 2,
              "created_utc": "2026-02-01 22:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2zkcop",
          "author": "pstryder",
          "text": "Strong agreement on default-to-inaction and closed boundaries. Curious whether you‚Äôve run into trust erosion when the LLM *sounds* confident but is actually constrained ‚Äî that‚Äôs been a surprisingly sharp edge for us.",
          "score": 2,
          "created_utc": "2026-02-01 15:53:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3271yr",
              "author": "NationalIncome1706",
              "text": "Yes ‚Äî we ran into that exact edge early on.\n\nThe problem wasn‚Äôt the constraints themselves, but the mismatch between linguistic confidence and actual agency. Humans tend to interpret confident language as capability, even when the model is heavily boxed in.\n\nWhat helped was reframing the LLM‚Äôs role entirely. Its output isn‚Äôt treated as an explanation or a recommendation ‚Äî it‚Äôs a state classification signal. We deliberately stripped away expressive language and made responses repetitive, terse, and sometimes even boring.\n\nOver time, that shifted trust away from the LLM as an ‚Äúactor‚Äù and toward the system as a whole. The goal wasn‚Äôt to make the LLM trustworthy, but to make it unnecessary to trust in isolation.\n\nOnce operators internalize that distinction, the confidence/constraint gap becomes much less sharp.",
              "score": 1,
              "created_utc": "2026-02-01 23:25:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o31zmz3",
          "author": "Kimononono",
          "text": "How often does your system produce a BUY / SELL signal vs HOLD\n\nAt what interval does it run?",
          "score": 2,
          "created_utc": "2026-02-01 22:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32e4e0",
              "author": "NationalIncome1706",
              "text": "The decision loop runs on a fixed schedule, but with very different semantics.\n\nENTRY decisions are only evaluated on closed higher-timeframe candles, so they‚Äôre relatively infrequent by design.\nPosition management / exit checks run much more often, but the default outcome there is still HOLD.\n\nIn practice, the vast majority of evaluations result in HOLD. BUY/SELL is treated as an exception, not a steady stream.\n\nThat ratio is intentional. We don‚Äôt optimize for signal frequency ‚Äî we treat excessive activity as a smell. If the system is trading often, something upstream is probably too permissive.\n\nThe goal is to let the system be bored most of the time and only act when ambiguity collapses.",
              "score": 1,
              "created_utc": "2026-02-02 00:04:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o33pmxx",
                  "author": "Kimononono",
                  "text": "if I have a script which goes \n\nfunc analyze\\_signal():\n\nnum = random()  \nif(num < .01) BUY  \nif(num < .02) SELL  \nelse HOLD\n\nHow are you to smell anything if the scent is so faint?",
                  "score": 1,
                  "created_utc": "2026-02-02 04:41:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ytwkw",
          "author": "NationalIncome1706",
          "text": "This is an experience report on operating an LLM inside a live system.\n\nNot a product, not prompts, not benchmarks.\n\n\n\nI‚Äôm especially interested in how others handle non-determinism,\n\nfail-safe defaults, and state consistency in always-on LLM-based systems.",
          "score": 1,
          "created_utc": "2026-02-01 13:33:04",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o34gef2",
          "author": "Sea-Sir-2985",
          "text": "one thing i've run into with this kind of setup is that schema validation alone isn't enough... the LLM can return perfectly valid JSON that still makes a nonsensical decision. so we added a semantic validation layer on top, basically domain constraint checks that run after the LLM responds but before anything gets executed\n\nthe other piece that helped was logging every decision with the full prompt and response, not just the final action. when something goes wrong at 3am you need to see exactly what the model was thinking, not just what it did",
          "score": 1,
          "created_utc": "2026-02-02 08:24:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtgszg",
      "title": "Drowning in 70k+ papers/year. Built an open-source pipeline to find the signal. Feedback wanted.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1qtgszg/drowning_in_70k_papersyear_built_an_opensource/",
      "author": "Real-Cheesecake-8074",
      "created_utc": "2026-02-02 01:08:40",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.99,
      "text": "Like many of you, I'm struggling to keep up. With over 80k AI papers published last year on arXiv alone, my RSS feeds and keyword alerts are just noise. I was spending more time filtering lists than reading actual research.\n\nTo solve this for myself, a few of us hacked together an open-source pipeline (\"Research Agent\") to automate the pruning process. We're hoping to get feedback from this community on the ranking logic to make it actually useful for researchers.\n\n**How we're currently filtering:**\n\n* **Source:**¬†Fetches recent arXiv papers (CS.AI, CS.ML, etc.).\n* **Semantic Filter:**¬†Uses embeddings to match papers against a specific natural language research brief (not just keywords).\n* **Classification:**¬†An LLM classifies papers as \"In-Scope,\" \"Adjacent,\" or \"Out.\"\n* **\"Moneyball\" Ranking:**¬†Ranks the shortlist based on author citation velocity (via Semantic Scholar) + abstract novelty.\n* **Output:**¬†Generates plain English summaries for the top hits.\n\n**Current Limitations (It's not perfect):**\n\n* Summaries can hallucinate (LLM randomness).\n* Predicting \"influence\" is incredibly hard and noisy.\n* Category coverage is currently limited to CS.\n\n**I need your help:**\n\n1. If you had to rank papers automatically, what signals would¬†*you*¬†trust? (Author history? Institution? Twitter velocity?)\n2. What is the biggest failure mode of current discovery tools for you?\n3. Would you trust an \"agent\" to pre-read for you, or do you only trust your own skimming?\n\nThe tool is hosted here if you want to break it:¬†[https://research-aiagent.streamlit.app/](https://research-aiagent.streamlit.app/)\n\nCode is open source if anyone wants to contribute or fork it.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1qtgszg/drowning_in_70k_papersyear_built_an_opensource/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o346bh4",
          "author": "PerceptualDisruption",
          "text": "Awesome",
          "score": 1,
          "created_utc": "2026-02-02 06:52:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}