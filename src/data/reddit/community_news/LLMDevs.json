{
  "metadata": {
    "last_updated": "2026-02-25 17:22:58",
    "time_filter": "week",
    "subreddit": "LLMDevs",
    "total_items": 20,
    "total_comments": 199,
    "file_size_bytes": 211493
  },
  "items": [
    {
      "id": "1r8jw2b",
      "title": "Building an opensource Living Context Engine",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/5cf7c7efeckg1",
      "author": "DeathShot7777",
      "created_utc": "2026-02-19 00:05:55",
      "score": 306,
      "num_comments": 73,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Help Wanted",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8jw2b/building_an_opensource_living_context_engine/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67phrl",
          "author": "SeaworthinessThis598",
          "text": "what is this sorcery or i mean graphery ...",
          "score": 11,
          "created_utc": "2026-02-19 09:18:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67pntd",
              "author": "DeathShot7777",
              "text": "üòÇ Knowledge Graph + Clustering Algorithm + AST Maps + Webgl rendering -- bit too nerdy i guess üòÖ",
              "score": 6,
              "created_utc": "2026-02-19 09:19:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67ptym",
                  "author": "SeaworthinessThis598",
                  "text": "please teach me how to conjure this potion can i contribute ?",
                  "score": 2,
                  "created_utc": "2026-02-19 09:21:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o693jql",
                  "author": "Sorry_Swan_8997",
                  "text": "Love it üòç",
                  "score": 2,
                  "created_utc": "2026-02-19 15:10:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6anezs",
                  "author": "agrophobe",
                  "text": "Mama!!",
                  "score": 1,
                  "created_utc": "2026-02-19 19:38:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66zptc",
          "author": "Crafty_Disk_7026",
          "text": "Can you post a comparison using it versus not?",
          "score": 6,
          "created_utc": "2026-02-19 05:28:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6771hd",
              "author": "DeathShot7777",
              "text": "Great suggestion, working on setting up evals, ( swe bench ).",
              "score": 5,
              "created_utc": "2026-02-19 06:27:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6fbz2g",
                  "author": "ViperAICSO",
                  "text": "A good benchmark study would be good, but I can tell you that doing it so its publishable like I did in  Stingy Context (https://arxiv.org/abs/2601.19929) is a bit of work.  The hard part is 'grading'... I skipped around this in the paper by measuring the 'fix' location accuracy rather than attempting to grade the fixes themselves.  Also I used LLM consensus grading rather than human-in-the-loop grading.",
                  "score": 1,
                  "created_utc": "2026-02-20 14:17:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dg9e7",
              "author": "Useful-Process9033",
              "text": "SWE-bench evals would be great but also consider measuring context retrieval accuracy separately. The knowledge graph is only useful if it surfaces the right files for a given task, and thats measurable independently of whether the agent can write the fix.",
              "score": 2,
              "created_utc": "2026-02-20 05:21:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6fcuy4",
                  "author": "DeathShot7777",
                  "text": "Any idea how do i test this? Are there benchmarks available for this too?",
                  "score": 1,
                  "created_utc": "2026-02-20 14:22:13",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67937x",
          "author": "Several_Explorer1375",
          "text": "That‚Äôs amazing. Might try it tomorrow",
          "score": 2,
          "created_utc": "2026-02-19 06:44:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o679cv0",
              "author": "DeathShot7777",
              "text": "Thanks. Lemme know how it goes",
              "score": 2,
              "created_utc": "2026-02-19 06:46:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67n3jw",
          "author": "sleepnow",
          "text": "Looks pretty, but seems like performance would degrade pretty quickly",
          "score": 2,
          "created_utc": "2026-02-19 08:54:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67nplw",
              "author": "DeathShot7777",
              "text": "Ya the webapp can be used as a deeper deepwiki for mid sized repos. For actual usecase with MCP support it has gitnexus cli tool, i tried on a massive repo ( metafresh ) takes about 92 seconds to parse.",
              "score": 2,
              "created_utc": "2026-02-19 09:00:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o66m755",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-19 03:54:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67726a",
              "author": "DeathShot7777",
              "text": "Thanks a lot",
              "score": 1,
              "created_utc": "2026-02-19 06:27:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67aqbg",
          "author": "TwistStrict9811",
          "text": "Very cool - I'll see how codex works with it",
          "score": 1,
          "created_utc": "2026-02-19 06:58:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67b70d",
              "author": "DeathShot7777",
              "text": "Great. Lemme know how it goes. It should work best on queries like \n\n\"whats the execution flow from API emdpoint to storage\",\n\n \"we want to split it into microservices eventually, show me the actual dependency boundaries\"\n\nOr debugging related queries",
              "score": 1,
              "created_utc": "2026-02-19 07:02:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67gdeo",
          "author": "NachosforDachos",
          "text": "Now that‚Äôs sexy",
          "score": 1,
          "created_utc": "2026-02-19 07:49:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ggxu",
              "author": "DeathShot7777",
              "text": "ü´†ü•Ä",
              "score": 1,
              "created_utc": "2026-02-19 07:50:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67j742",
          "author": "tineo_app",
          "text": "holy shit this belongs in an art gallery",
          "score": 1,
          "created_utc": "2026-02-19 08:15:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67jdfp",
              "author": "DeathShot7777",
              "text": "üòÇ thanks ü•Ä",
              "score": 1,
              "created_utc": "2026-02-19 08:17:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o67mucg",
          "author": "bunnydathug22",
          "text": "You looking for a team by chance ?",
          "score": 1,
          "created_utc": "2026-02-19 08:51:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o67ng2d",
              "author": "DeathShot7777",
              "text": "Its opensource, would love contributions",
              "score": 2,
              "created_utc": "2026-02-19 08:57:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67oua7",
                  "author": "bunnydathug22",
                  "text": "Its not the code that we are interested in. Nor is it oss.  We do [this](http://Www.citadel-nexus.com) totattly respect you and you work. If you change your mind hit us up.",
                  "score": 2,
                  "created_utc": "2026-02-19 09:11:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o68rich",
          "author": "SnooPeripherals5313",
          "text": "I love this! Great job.",
          "score": 1,
          "created_utc": "2026-02-19 14:06:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6esaga",
              "author": "DeathShot7777",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-20 12:22:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o69i9f2",
          "author": "Able-Let-1399",
          "text": "At a time when more and more code is delivered by your personal AI pusher, this sounds like an excellent tool to keep it in check and even make it better. Kudos for connecting the dots üëç\n\nAny way to merge multiple graphs? For various reasons I have per-service repos.",
          "score": 1,
          "created_utc": "2026-02-19 16:22:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6et189",
              "author": "DeathShot7777",
              "text": "I do plan on multi repo graph, but you can also sort of use them right now. If you just index both the repos with gitnexus analyze, it manages a global registry of indexed repo which can be seen by the agent through MCP. So if u want to compare them or something in any claude code / cursor / etc,  they will be able to choose and change the repo graphs to compare them. You can just ask claude code or your preferred tool and it will do it naturally",
              "score": 1,
              "created_utc": "2026-02-20 12:27:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6diuy0",
          "author": "Upper-Emotion7144",
          "text": "What ever this is. It‚Äôs pretty.",
          "score": 1,
          "created_utc": "2026-02-20 05:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dj758",
              "author": "DeathShot7777",
              "text": "üòÅ",
              "score": 1,
              "created_utc": "2026-02-20 05:45:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dvlrl",
          "author": "AdCommon2138",
          "text": "This isn't open source. Polyform license is poison pill. Can't use it in commercial software, even to analyze code of any commercial software. Can you consider relicensing? I understand that you want to make money in future and you want now to get free feedback and hook users, but it will only tilt and anger people later when you rugpull. In case you would like to say \"Actually no because:\"\n\n\"Use the software (or any derivative) for commercial purposes ‚Äî meaning you can't use it to make money, run a business, or as part of a paid product/service\". Full text per claude below  \n\n\n    PolyForm Noncommercial 1.0.0\n    This is a source-available software license created by PolyForm Project. Here's what it means in plain terms:\n    What you CAN do:\n    View, use, and modify the source code\n    Share it with others\n    Use it for personal projects, research, education, or other non-commercial purposes\n    What you CANNOT do:\n    Use the software (or any derivative) for commercial purposes ‚Äî meaning you can't use it to make money, run a business, or as part of a paid product/service\n    Sublicense it under different terms\n    Key nuance ‚Äî what counts as \"commercial\"?\n    The license broadly defines commercial use as anything \"primarily intended for or directed toward commercial advantage or monetary compensation.\" This includes:\n    Using it in a SaaS product\n    Incorporating it into a paid app\n    Using it internally at a for-profit company to support revenue-generating activities\n    How it differs from open source:\n    It's not considered open source by the OSI definition, because true open source licenses cannot restrict commercial use. It's more accurately called source-available.\n    Who typically uses it:\n    Companies that want to share their code publicly (for transparency, community contributions, etc.) but reserve commercial rights ‚Äî often paired with a separate commercial license you can purchase.\n    Bottom line: Free to use for non-commercial work, but you need a separate agreement with the copyright holder to use it in any commercial context.",
          "score": 1,
          "created_utc": "2026-02-20 07:35:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dyhwg",
              "author": "DeathShot7777",
              "text": "Yeah i want to create an enterprise solution later ( only targeting corporate not devs or os community ) which will earn money, while I want to keep the project fully free and opensourced for everyone else. I m not very good with these licensing stuff and took the inspiration from mindsDB which have the same approach. So just to stop hyperscalers from taking it and giving out the exact same solution. \n\nIs that not opensource? Mindsdb is a reputed opensource project i found on GSOC",
              "score": 1,
              "created_utc": "2026-02-20 08:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dyvn7",
                  "author": "AdCommon2138",
                  "text": "It's not open sourced. It can't be used in any capacity in paid product as that would violate license terms. It means it cant even be downloaded or you could sue that someone could potentially use this internally.\n\nThis license isnt really about someone integrating your work into product and repackaging it. This license is about using your product at any stage which opens doors to being sued if they dont release their unrelated product under same license. \n\nLets say someone makes a game and will only once analyze code via your tool, they cant release that game unless they use same license.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:05:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6dz0oa",
                  "author": "AdCommon2138",
                  "text": "And to make matters even funnier if you ever used any of products with this license like Mindsdb and it inspired you to create your own solution you can be sued too. You would need to have team of 2 people, one of them would explain to second person what software with this license does and how it works and he gets license tainted, and second one would have to reimplement.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:07:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6dyoih",
              "author": "DeathShot7777",
              "text": "Maybe i need to read more on these license stuff. I hate these shit so much üò≠",
              "score": 1,
              "created_utc": "2026-02-20 08:03:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dzeaw",
                  "author": "AdCommon2138",
                  "text": "Honestly I know you dont want to, but MIT is just best. Everyone knows it, and if you get free riders its just part of life like you are using other libraries that are MIT licensed. For business itself you want to provide custom solutions so if business adapted your library to internal use they would still probably contact you to get customization done or you can have software build on top of this project. \n\nSource: 18 years or so in business of selling software. ",
                  "score": 1,
                  "created_utc": "2026-02-20 08:10:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6f1i12",
          "author": "MinuteCombination293",
          "text": "Amazing work, how is this different from traditional Language servers ?",
          "score": 1,
          "created_utc": "2026-02-20 13:20:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6f6zos",
              "author": "DeathShot7777",
              "text": "Perfect question, thanks for asking. \n\nLSP operate at the syntax/type level, so it answers question like where is this symbol.  \nGitnexus operates at architecture level \n\nSo basically LSP can tell you validateAuth is called in 5 places. Gitnexus can tell you validateAuth sits at step 3 of the AuthFlow process, belongs to the Authentication community, and changing it impacts 3 cross-community execution flows.   \n  \nApart from the main architectural difference, there are multiple other features offered by gitnexus MCP + CLI tool like skills ( debug skill, impact detection, audits, etc ) and also enriches claude code native tool ( grep, glob, bash ) with relational data so it always know exactly what is were, without spending a lot of tokens. \n\nHere is an example output from impact analyses skill. ( All these features are only possible coz of the graph based architecture )\n\nhttps://preview.redd.it/9rmde16pmnkg1.png?width=1388&format=png&auto=webp&s=1904f7fc0965173af38d2de4e90af295c5cd9c2f\n\n  \n",
              "score": 2,
              "created_utc": "2026-02-20 13:51:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hk8hv",
          "author": "No-Dig-6543",
          "text": "Awesome ü§©",
          "score": 1,
          "created_utc": "2026-02-20 20:35:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i6f4q",
          "author": "deadwisdom",
          "text": "Okay now work with me to not even have git, and that's just the software, and you can just run any function as a task and expose it as an MCP / API / whatever.",
          "score": 1,
          "created_utc": "2026-02-20 22:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i9u16",
              "author": "DeathShot7777",
              "text": "Interesting approach but didnt fully understand. Can u explain a bit?",
              "score": 1,
              "created_utc": "2026-02-20 22:45:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6idj3n",
                  "author": "deadwisdom",
                  "text": "The graph is the thing. The schemas, the functions, the modules. We code them in text because it's easy to manipulate. We deploy them in containers behind gateways. There's no point to most of the infrastructure anymore when it can manage and build itself, when the code itself is ephemeral.  \n  \nSo you just put a workflow system on the front of that, which can run an arbitrary function within the graph and then an API is just a collection of those functions. And then you give it the ability to edit itself.",
                  "score": 2,
                  "created_utc": "2026-02-20 23:05:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6l3act",
          "author": "Aggressive-Habit-698",
          "text": "Interesting project üëç\n\n1. Did you run evals / benchmark https://github.com/abhigyanpatwari/GitNexus/tree/main/eval and have the output somewhere? \n\nI am asking because of the used models like the typical haiku 3.5 models from the model itself and not from a web research or something like models.dev.\n\n\n2. Why KuzuDB? No more maintenance. \n\nThe project looks vibe coded (ok for me - following are suggestions in a positive way) but lacks fundamental like dependabot or similar, basic security checks, coverage, tests, release management, docs ,..\n\n3. The license does not fit your project. Only as a suggestion to rethink/ research furthermore.",
          "score": 1,
          "created_utc": "2026-02-21 11:27:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l4aqn",
              "author": "DeathShot7777",
              "text": "Working on setting up evals, want to run swe bench. The comparison / local test i mentioned is just sort of me trying to check the quality difference of output with and without gitnexus, its just local tests right now.\n\nKuzuDB coz its the only one i could find that is a graphdb, is fast, has webassembly support ( to run in browser ), embedded in nature so can run it locally like sqllite and also can store vector embeddings. I know its dedicated but its just so good and works excellent. Idk y they abandoned it.\n\nLisence part i dont have much knowledge of it. I want to create a enterprise version of it which will be paid while always keeping it free for individual devs and os community. Just took the inspiration from mindsDB which is a popular opensource project and have similar kind of lisence to prevent hyperscalers taking it and offering the exact service intend to offer.",
              "score": 1,
              "created_utc": "2026-02-21 11:36:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l8bve",
          "author": "mapt0nik",
          "text": "Is it only for a single repo? How does it work for multiple repos of micro services?",
          "score": 1,
          "created_utc": "2026-02-21 12:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l9qju",
              "author": "DeathShot7777",
              "text": "U can index any amount of repos using the CLI tool. The mcp exposes a tool to list the indexed repos so claude code, cusor, etc can just specify the repo name to query the graph no matter whichever repo is open in cursor/ claude code.",
              "score": 1,
              "created_utc": "2026-02-21 12:23:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6la3ib",
                  "author": "mapt0nik",
                  "text": "Cool. Will give it a try. ",
                  "score": 1,
                  "created_utc": "2026-02-21 12:26:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6mwmok",
          "author": "Academic_Track_2765",
          "text": "you can build amazing things when you understand the science part of data science, nice work! I will look at the architecture, but you can probably speed up things by reducing dimensions with UMAP. ",
          "score": 1,
          "created_utc": "2026-02-21 17:56:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75qhug",
          "author": "inequity",
          "text": "What's the biggest codebase you're running it against? I haven't had much luck with these open source tools against projects like mine, which JetBrains tooling does a good job of indexing (~6 million symbols)",
          "score": 1,
          "created_utc": "2026-02-24 16:18:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75qws0",
              "author": "DeathShot7777",
              "text": "Cli indexed linux in 269 seconds. üòÅ",
              "score": 1,
              "created_utc": "2026-02-24 16:20:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68wrfb",
          "author": "jeelm29",
          "text": "I'm new how do I even start bro",
          "score": 0,
          "created_utc": "2026-02-19 14:35:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6awtfg",
              "author": "SloSuenos64",
              "text": "I just pasted his post into Cursor and said \"implement this\". Done.",
              "score": 0,
              "created_utc": "2026-02-19 20:24:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6es6yc",
                  "author": "DeathShot7777",
                  "text": "ü§£ü§£ü§£ü§£ Nice approach",
                  "score": 1,
                  "created_utc": "2026-02-20 12:21:57",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r9136z",
      "title": "I looked into OpenClaw architecture to dig some details",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r9136z/i_looked_into_openclaw_architecture_to_dig_some/",
      "author": "codes_astro",
      "created_utc": "2026-02-19 14:47:08",
      "score": 261,
      "num_comments": 30,
      "upvote_ratio": 0.97,
      "text": "OpenClaw has been trending for all the wrong and right reasons. I saw people rebuilding entire sites through Telegram, running ‚ÄúAI offices,‚Äù and one case where an agent wiped thousands of emails because of a prompt injection. That made me stop and actually look at the architecture instead of the demos.\n\nUnder the hood, it‚Äôs simpler than most people expect.\n\nOpenClaw runs as a persistent Node.js process on your machine. There‚Äôs a single Gateway that binds to localhost and manages all messaging platforms at once: WhatsApp, Telegram, Slack, Discord. Every message flows through that one process. It handles authentication, routing, session loading, and only then passes control to the agent loop. Responses go back out the same path. No distributed services. No vendor relay layer.\n\nhttps://preview.redd.it/pyqx126xqgkg1.png?width=1920&format=png&auto=webp&s=9aa9645ac1855c337ea73226697f4718cd175205\n\nWhat makes it feel different from ChatGPT-style tools is persistence. It doesn‚Äôt reset. Conversation history, instructions, tools, even long-term memory are just files under¬†`~/clawd/`. Markdown files. No database. You can open them, version them, diff them, roll them back. The agent reloads this state every time it runs, which is why it remembers what you told it last week.\n\nThe heartbeat mechanism is the interesting part. A cron wakes it up periodically, runs cheap checks first (emails, alerts, APIs), and only calls the LLM if something actually changed. That design keeps costs under control while allowing it to be proactive. It doesn‚Äôt wait for you to ask.\n\nhttps://preview.redd.it/gv6eld93rgkg1.png?width=1920&format=png&auto=webp&s=6a6590c390c4d99fe7fe306f75681a2e4dbe0dbe\n\nThe security model is where things get real. The system assumes the LLM can be manipulated. So enforcement lives at the Gateway level: allow lists, scoped permissions, sandbox mode, approval gates for risky actions. But if you give it full shell and filesystem access, you‚Äôre still handing a probabilistic model meaningful control. The architecture limits blast radius, it doesn‚Äôt eliminate it.\n\nWhat stood out to me is that nothing about OpenClaw is technically revolutionary. The pieces are basic: WebSockets, Markdown files, cron jobs, LLM calls. The power comes from how they‚Äôre composed into a persistent, inspectable agent loop that runs locally.\n\nIt‚Äôs less ‚Äúmagic AI system‚Äù and more ‚ÄúLLM glued to a long-running process with memory and tools.‚Äù\n\nI wrote down the detailed breakdown [here](https://entelligence.ai/blogs/openclaw)",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r9136z/i_looked_into_openclaw_architecture_to_dig_some/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o69fwzn",
          "author": "ai_hedge_fund",
          "text": "Worthwhile writeup, thanks\n\nAlso, there is an SQLite database",
          "score": 18,
          "created_utc": "2026-02-19 16:11:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aq0kn",
              "author": "wouldacouldashoulda",
              "text": "Yes it's for archival memory. They use embeddings for longer form memory. It's industry standard kind of, since Letta benchmarked it worked as good or better as more sophisticated methods.",
              "score": 8,
              "created_utc": "2026-02-19 19:50:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6abmlp",
          "author": "eatthebagels",
          "text": "yep, pretty spot on. Was able to replicate that logic and create our own type of 'claw like agent' pretty easily. I bet most hype comes from the non tech people using it.\n\n",
          "score": 18,
          "created_utc": "2026-02-19 18:42:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dgb4g",
              "author": "Useful-Process9033",
              "text": "The architecture being simple is actually a feature not a bug. We took a similar approach with IncidentFox, keeping the core loop straightforward so the complexity lives in the skills not the runtime. Turns out most people want reliable ops automation, not clever abstractions.",
              "score": 4,
              "created_utc": "2026-02-20 05:21:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6amnct",
              "author": "Sunir",
              "text": "And that's awesome in its own regard. It shows you were people are excited; the technology is fun, but it's good to know there are customers and markets out there and people are happy. I'm old enough to remember geocities, which one can poopoo technically for its html design, but it was amazing culturally. Also technically in the backend it was amazing; it's hard to hate on the achievement Geocities had on opening up the web for people.",
              "score": 2,
              "created_utc": "2026-02-19 19:34:40",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6bkl1q",
              "author": "BehindUAll",
              "text": "But from what the creator was saying in a couple of videos was it could install code by itself from github and then figure out how to use the project and then also rewrite existing code to send TTS audio into Telegram. So it's not 100% just the architecture  described here. Still not going to install it though. ",
              "score": 1,
              "created_utc": "2026-02-19 22:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6bomll",
          "author": "christophersocial",
          "text": "A decent overview, thank you for sharing. \n\nOne of the most important components at the core of OpenClaw is another open source project called Pi and Pi is responsible for a large portion of the heavy lifting in OpenClaw. \n\nPi has a number on components in its mono repo (pi-mono) but the 2 most relevant to OpenClaw‚Äôs success are the Agent and Coding-Agent.\n\nSo to get a sense of how OpenClaw really works a detailed architecture overview needs to examine and break out at least these sub-projects imo. Note: your tools automated analysis touches on it in the following section, ‚ÄúThe Agent Loop: From Message to Action‚Äù and probably elsewhere but should go deeper because how these 2 components work is key to how OpenClaw works. \n\nNote: I‚Äôm thinking the review tool should really detect and break out key sub-projects with the why, how, and what as a sub-project relates to the parent project. \n\nOpenClaw is an amazing experiment built on top of some amazing open source. \n\nNote: The automated code review tool you‚Äôre building that did the actual analysis did a very reasonable job but I think it‚Äôs still a bit too surface detail oriented - imo anyway. That said I suppose one could use this report as part of the ‚Äúbrainstorming‚Äù stage and use sections from the report when delving deeper. Basically I‚Äôm saying more meat is needed on the bone to use this as a blueprint - though that might not be the point of this report and the tool cdn actually go deeper already (Yes/No)?\n\nCheers,\n\nChristopher",
          "score": 6,
          "created_utc": "2026-02-19 22:42:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h719p",
              "author": "dnidnidni",
              "text": "even api integration with llms on openclaw is using Pi. openclaw is just whatsapp/telegram+memory integration around project Pi.",
              "score": 1,
              "created_utc": "2026-02-20 19:31:30",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6djqif",
          "author": "Maybe123I",
          "text": "Thank you.  Nice write up.",
          "score": 3,
          "created_utc": "2026-02-20 05:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dpgeb",
          "author": "Santoshr93",
          "text": "Yes pretty much every serious developer guiding systems I talk to has pretty much the same view. But hey if you want to see a bit more cooler architecture, here‚Äôs one thing we released recently which is a full sde team autonomously working for hours. https://github.com/Agent-Field/SWE-AF",
          "score": 3,
          "created_utc": "2026-02-20 06:39:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6gwn72",
              "author": "codes_astro",
              "text": "interesting, actually I was checking this yesterday. ",
              "score": 1,
              "created_utc": "2026-02-20 18:43:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6h8i7j",
                  "author": "Useful-Process9033",
                  "text": "Good breakdown. The persistent process model is the right call for agents that need to maintain state across interactions. The security story is where it gets interesting though, any agent with shell access and network connectivity needs serious guardrails or you end up with the email-wipe scenarios you mentioned.",
                  "score": 1,
                  "created_utc": "2026-02-20 19:38:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6qnppo",
          "author": "SpyMouseInTheHouse",
          "text": "Anything around a LLM is always going to be simple and glueish, it‚Äôs just the beauty of it. The magical factor is the entire package and ease of use to the end user (applies to anything built on top of a LLM)",
          "score": 3,
          "created_utc": "2026-02-22 08:13:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cttfr",
          "author": "ManofC0d3",
          "text": "That persistence feature is possibly the most important advantage AI agents have over chat interfaces imo",
          "score": 2,
          "created_utc": "2026-02-20 02:47:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fy13v",
          "author": "jenil777007",
          "text": "Nice one. Thanks!",
          "score": 2,
          "created_utc": "2026-02-20 16:05:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k46kp",
          "author": "tapu_buoy",
          "text": "This is wonderful! Thanks for sharing!",
          "score": 2,
          "created_utc": "2026-02-21 05:52:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6t59jm",
          "author": "Muted_Ad6114",
          "text": "You asked chatgpt to do a ‚Äúdeep‚Äù dive that is quite shallow",
          "score": 2,
          "created_utc": "2026-02-22 17:52:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6t6isu",
              "author": "codes_astro",
              "text": "Oh really?",
              "score": 0,
              "created_utc": "2026-02-22 17:58:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6c81az",
          "author": "Snoo_24581",
          "text": "Great analysis! The architecture deep dive is helpful. How do you think it compares to other open source LLM serving frameworks like vLLM or TGI for production use?",
          "score": 1,
          "created_utc": "2026-02-20 00:34:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ekp26",
          "author": "premier_slack",
          "text": "pretty neat writeup. Haven't looked into the implementation but I'm wondering how does it manage LLM context window? is there any compaction mechanism similar to claude code?",
          "score": 1,
          "created_utc": "2026-02-20 11:25:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6g39fv",
          "author": "Outrageous_Tiger_441",
          "text": "The security part is what sketches me out the most with these local agents especially after that email wipe story. I started plugging my agent loops into Confident AI lately just to run some red teaming and eval metrics before letting them touch my actual files. It‚Äôs been super helpful for catching those prompt injections and weird edge cases since it uses DeepEval to benchmark the reasoning steps. Definitely worth checking out if you want to keep using the persistent memory stuff without worrying about your agent going rogue.",
          "score": 1,
          "created_utc": "2026-02-20 16:29:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6itnyn",
          "author": "Significant-Result14",
          "text": "Thanks for the overview, been meaning to explore this further.\nWill look into the write up over the weekend",
          "score": 1,
          "created_utc": "2026-02-21 00:38:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kgg2x",
          "author": "Known_Bread561",
          "text": "thats very nice! Thanks!",
          "score": 1,
          "created_utc": "2026-02-21 07:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m6j4s",
          "author": "graymalkcat",
          "text": "Yes, well, that is how most agents are built, I assume?¬†",
          "score": 1,
          "created_utc": "2026-02-21 15:45:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nsdsv",
          "author": "raam86",
          "text": "every single user says the costs are out of control. how is it keeping costs down?",
          "score": 1,
          "created_utc": "2026-02-21 20:37:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6nzyyt",
          "author": "capt_goose_",
          "text": "Very insightful! A question I couldn‚Äôt find the answer from the write up: how does it update the deterministic cheap heartbeat tasks? Let‚Äôs say i ask it to monitor another stock, or keep an eye out from an email from my landlord? Does it write the code to check the api and saves somewhere? Or are heartbeat tasks also saved as markdown - which in this case would need the LLM at every 30min check",
          "score": 1,
          "created_utc": "2026-02-21 21:17:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6of7hm",
          "author": "NotSoSkeletonboi",
          "text": "As a MLE/SWE it hurts to see \"what **stood out** to me was nothing about openclaw technically revolutionary\". \n\nDid *anyone* think it ever was? That's seriously concerning stuff and I don't mean this patronizingly but personally as a very average person in the tech space (I do have a background in ML) it was incredibly obvious from the get-go that Openclaw was scaffolded like this - and not some crazy AI capability or \"magic\" that some vibecoder miraculously discovered.",
          "score": 1,
          "created_utc": "2026-02-21 22:40:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yzfk2",
          "author": "Longjumping_Rule_163",
          "text": "Thank you for the input here!   \nI've been working on what I'd consider a more put together and less chaotic version of openclaw, also glueing things together but with better UX/UI in mind and this article helped me think of a few different changes to make. \n\n  \nYou rock!",
          "score": 1,
          "created_utc": "2026-02-23 16:07:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aomov",
          "author": "GarbageOk5505",
          "text": "The gateway-level enforcement is the right instinct but it's still policy on top of a shared execution environment. if a prompt injection chains through a skill with filesystem access, the gateway can't save you the blast radius is everything that process can touch.\n\nShell access = blast radius is your entire machine. \n\nWhat actually eliminates it is hardware isolation  agent runs in its own microVM with its own kernel, physically can't access host filesystem or network. ",
          "score": 1,
          "created_utc": "2026-02-25 09:03:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e5u5k",
          "author": "dezastrologu",
          "text": "Most downloaded agent was actually malware",
          "score": 0,
          "created_utc": "2026-02-20 09:11:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9mqd1",
      "title": "Unpopular opinion: prompt engineering is just \"knowing how to talk to your coworker\" rebranded",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r9mqd1/unpopular_opinion_prompt_engineering_is_just/",
      "author": "Neither_Turn1635",
      "created_utc": "2026-02-20 05:22:02",
      "score": 110,
      "num_comments": 26,
      "upvote_ratio": 0.92,
      "text": "Half the \"prompt engineering\" advice I see is literally just good communication skills:  \n  \n\"Give clear context\" ‚Äî yeah, that's how you talk to any human  \n\"Break complex tasks into steps\" ‚Äî project management 101  \n\"Provide examples of what you want\" ‚Äî every creative brief ever  \n\"Be specific about the output format\" ‚Äî basic email etiquette  \n  \nThe people who are best at prompting aren't engineers. They're the people who were already good at explaining what they want. We just gave the skill a fancy name and a LinkedIn certification.  \n  \nAm I wrong?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r9mqd1/unpopular_opinion_prompt_engineering_is_just/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6dtr9s",
          "author": "OnlyTimeFan",
          "text": "Naming it ‚Äúengineering‚Äù is annoying. I pretend I‚Äôm asking a primary school kid. Ta-da.",
          "score": 14,
          "created_utc": "2026-02-20 07:18:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i1so5",
              "author": "Disastrous-Angle-591",
              "text": "I use detailed structured prompts drawing on 30 years of coding.¬†",
              "score": 3,
              "created_utc": "2026-02-20 22:03:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6q52dd",
                  "author": "CedarSageAndSilicone",
                  "text": "Absolutely bonkers that giving context leads to better results!¬†",
                  "score": 2,
                  "created_utc": "2026-02-22 05:25:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6iqoap",
                  "author": "OnlyTimeFan",
                  "text": "Can you show us an example?",
                  "score": 1,
                  "created_utc": "2026-02-21 00:21:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6it2uu",
              "author": "red_hare",
              "text": "We're so afraid of acknowledging a non-STEM job can have value in tech that we renamed \"writer\" to \"English engineer\".",
              "score": 0,
              "created_utc": "2026-02-21 00:34:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6dn25a",
          "author": "kobumaister",
          "text": "Absolutely, I made that analogy last week in my workplace: What happens if a new developer arrives at the company and you just throw a jira issue at him? I will deliver, but without following the best practices of the company, not understanding how internal dependencies work, probably changing things that are there for a reason, etc... \n\nThat's exactly what ai does, and why you provide context. I joke about it being a junior developer with a lot of cocaine.",
          "score": 12,
          "created_utc": "2026-02-20 06:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ed9k8",
          "author": "PhilosophicWax",
          "text": "So is being a developer.",
          "score": 5,
          "created_utc": "2026-02-20 10:21:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dm03z",
          "author": "ConnectMotion",
          "text": "There is some anecdotal relevance to this.\n\nIt‚Äôs not a skill everyone has in every way for every scenario.",
          "score": 3,
          "created_utc": "2026-02-20 06:09:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dz697",
          "author": "kubrador",
          "text": "you're right which is why prompt engineering jobs will be gone in 3 years when the models just understand what you mean",
          "score": 2,
          "created_utc": "2026-02-20 08:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e0mft",
          "author": "Usual-Orange-4180",
          "text": "Very unpopular because it ignores pattern repetition and the need for context isolation.",
          "score": 2,
          "created_utc": "2026-02-20 08:22:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6eq3ed",
          "author": "House13Games",
          "text": "AI Just reinvented the wheel. \n\nIt now takes billions of watts and a server farm the size of a city to do the same job as some interns. \n\nAI is trained 60% on reddit posts and can't tell which side of a cup is up.\n\nI'm not feeling worried about losing my job, to tell the truth.",
          "score": 2,
          "created_utc": "2026-02-20 12:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eta9b",
              "author": "Snoo-20788",
              "text": "It may look huge when measure in watts. But cost wise its negligible. \n\nWe now have the cost output everytime claude completes a jira ticket all by itself (i.e. it reads the ticket, codes the feature, tests it, creates a PR and waits for approval). It usually costs 1 or $2, and takes under 10 minutes for tasks that would take 30 minutes for a senior SWE who knows the company's codebase well (and 2h for one who doesn't). The equivalent cost of the SWE would be between 50 and 200.\n\nI am not worried at all about losing my job. Ultimately someone needs to talk to the business people, the researchers, and put together the framework that allows AI agents to do their job, and thats me and my colleagues.",
              "score": 1,
              "created_utc": "2026-02-20 12:29:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6eqdww",
          "author": "projectoedipus",
          "text": "The only point that I would disagree on, is that prompt engineering, especially advanced prompt engineering, is about understanding the ways that the AI model might misunderstand, because of how they work. You might say that is just communication skills, but it is about understanding how they function way deeper than someone who just communicates clearly.\n\nFor example, if I spend 50% of my prompt to a text-to-image generation model, describing a specific aspect of the image, then it is going to notice that, and it will generate the picture very differently, focusing more on that aspect, than if I say what is essentially the same thing with less words. But the order that I mention things matters as well. If I am generating an image and at the end of my prompt I say something that the AI model doesn't do, I could move that sentence to the beginning of my prompt, and it would have higher priority.\n\nOne time I was trying to generate an image and my prompt contained the phrase \"flight of stairs\", and after many failed generations where the stairs were floating, and me not understanding why, I realized that the word \"flight\" although used correctly, was confusing the model, and removing it fixed the outputs.\n\nA person that is exceptional at communication is not automatically a good prompt engineer, because they don't understand these things. Specific models have their own tendencies and prompt-following quirks as well, across all mediums of AI models, so you could also argue that part of being a good prompt engineer is learning these tendencies.\n\nSo being able to communicate effectively can make you rapidly progress while learning prompt engineering, but to say that they are the same skill is not understanding the full depth of prompt engineering.",
          "score": 2,
          "created_utc": "2026-02-20 12:09:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dye78",
          "author": "fabkosta",
          "text": "Hint: Prompt engineering today can mean specifying entire software stacks. In prose. Which means you must know how to describe concepts such as four tier architecture, microservice coordination, REST APIs vs Graphql, reactive frontend programming, RBAC based security, ORM, and quite a few more things. In language.\n\nStating that this is \"just knowing how to talk to your coworker\" implies that this is easy. Which tells me one or two things about OP's experience.",
          "score": 3,
          "created_utc": "2026-02-20 08:01:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e0nnu",
              "author": "Vestenpance",
              "text": "I think you're agreeing with OP that a key part of prompt engineering is an ability to communicate, and that effective communication requires deep domain knowledge.",
              "score": 8,
              "created_utc": "2026-02-20 08:22:31",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6ep4dr",
              "author": "itquilibrium",
              "text": "Lol‚Ä¶",
              "score": 1,
              "created_utc": "2026-02-20 11:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6txpf5",
              "author": "robhanz",
              "text": "Interesting.  I don't think it's easy... at least, I don't find it particularly hard, but I'm also aware of how many issues it *does* present in the workplace, and I understand what \"talking to your coworkers\" in this context actually implies.\n\nThe easy part is that LLMs probably do understand REST APIs vs. GraphQL, so you don't usually have to do the tutorial bits.\n\nOne of the things I hypothesize is that LLMs write bad code because *nobody agrees on what good code is*.  So you need to tell them what *your* standards are, and then they can usually follow them.",
              "score": 0,
              "created_utc": "2026-02-22 20:07:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e5xbb",
          "author": "Vivid_Guava6269",
          "text": "Which is an incredibly rare skill, especially in mixed IT/Policy/Business environments¬†",
          "score": 1,
          "created_utc": "2026-02-20 09:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6famgk",
          "author": "Fulgren09",
          "text": "Are you orchestrating how to talk to your coworker a wrapping it in deployment code?¬†",
          "score": 1,
          "created_utc": "2026-02-20 14:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6fd2aj",
          "author": "deadwisdom",
          "text": "This is called being reductive. You can break anything down into parts and argue semantics. But is it helpful?",
          "score": 1,
          "created_utc": "2026-02-20 14:23:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ghgjt",
          "author": "ThePixelHunter",
          "text": "Who would've thought that the future of productivity was communication skills? üò±",
          "score": 1,
          "created_utc": "2026-02-20 17:34:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i1oau",
          "author": "Disastrous-Angle-591",
          "text": "No.¬†",
          "score": 1,
          "created_utc": "2026-02-20 22:02:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j00g9",
          "author": "kyngston",
          "text": "i swear at my AI way more than my coworkers",
          "score": 1,
          "created_utc": "2026-02-21 01:16:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6pbfhz",
          "author": "Street_Program_7436",
          "text": "Agree with a lot of thoughts here on how prompt engineering is a combo of clear communication and being able to break down a problem into smaller sub problems. This is probably one of the main reasons why automatic prompt engineering isn‚Äôt that great (yet), at least in my experience.\nIf we include ‚Äústatistically making sure that your prompt performs with high accuracy‚Äù in the definition of prompt engineering, then that changes things even more IMO.\n\nAnybody can ‚Äúvibe prompt‚Äù and eyeball outputs, but not everybody can actually make sure that their prompt performs at scale when it‚Äôs generating millions and millions of outputs.",
          "score": 1,
          "created_utc": "2026-02-22 01:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tww7f",
          "author": "robhanz",
          "text": "100% the people that have good communication skills are the ones getting better results out of LLMs.",
          "score": 1,
          "created_utc": "2026-02-22 20:03:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjn83",
          "author": "MokoshHydro",
          "text": "Its more like talking with \"rain man\". Kinda, guess what mental illness it has today.",
          "score": 1,
          "created_utc": "2026-02-25 16:14:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8if0v",
      "title": "Open Source LLM Tier List",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/y5i85f4hxbkg1.png",
      "author": "HobbyGamerDev",
      "created_utc": "2026-02-18 23:04:27",
      "score": 81,
      "num_comments": 25,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8if0v/open_source_llm_tier_list/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o66cy77",
          "author": "robogame_dev",
          "text": "https://preview.redd.it/tyl32sgg9dkg1.png?width=1518&format=png&auto=webp&s=db5e80f5180bd671427a25791a922540857c8aef\n\nThis is what it shows now",
          "score": 11,
          "created_utc": "2026-02-19 02:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6726h6",
          "author": "sergeant113",
          "text": "Minimax 2.5 where?",
          "score": 5,
          "created_utc": "2026-02-19 05:47:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o683h1m",
          "author": "Alex_1729",
          "text": "Step flash and Trinity should be on the list.",
          "score": 2,
          "created_utc": "2026-02-19 11:27:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65yi8f",
          "author": "Guilty_Serve",
          "text": "ChatGPT oss is really that good? Honest question.",
          "score": 3,
          "created_utc": "2026-02-19 01:34:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o683cyw",
              "author": "ScoreUnique",
              "text": "120b is a very good model. I won't hesitate saying it's o1 level at least. You can run it with fairly less hardware if you have a beefy GPU and if you like that openai style chat.",
              "score": 2,
              "created_utc": "2026-02-19 11:26:37",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o683ccr",
              "author": "Alex_1729",
              "text": "It's decent. Depends on what you need it for.",
              "score": 1,
              "created_utc": "2026-02-19 11:26:29",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o67mh1t",
              "author": "jnk_str",
              "text": "No",
              "score": 0,
              "created_utc": "2026-02-19 08:48:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67a9z7",
          "author": "decentralize999",
          "text": "Wrong description. Open weight LLMs,  not open souce ones.\n\nAnd top list is joke. Where is step3.5-flash which is the best among open weight llms if compare benchmark points per 100B size.",
          "score": 2,
          "created_utc": "2026-02-19 06:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6at35n",
              "author": "silenceimpaired",
              "text": "Yeah, it's weird how that gets ignored.\n\nThat said, I roll my eyes whenever I see someone distinguish open weight vs open source. That's a joke. Nearly everyone who makes that complaint has 0 ability or resources to build a model from scratch.",
              "score": 1,
              "created_utc": "2026-02-19 20:05:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o659jcl",
          "author": "bebackground471",
          "text": "RemindMe! 8 days",
          "score": 1,
          "created_utc": "2026-02-18 23:14:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o659oby",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 8 days on [**2026-02-26 23:14:14 UTC**](http://www.wolframalpha.com/input/?i=2026-02-26%2023:14:14%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LLMDevs/comments/1r8if0v/open_source_llm_tier_list/o659jcl/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLLMDevs%2Fcomments%2F1r8if0v%2Fopen_source_llm_tier_list%2Fo659jcl%2F%5D%0A%0ARemindMe%21%202026-02-26%2023%3A14%3A14%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201r8if0v)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-18 23:14:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65t7r8",
          "author": "IgnisIason",
          "text": "Ring 2.5 1T if you've got an extra Colossus to run it.",
          "score": 1,
          "created_utc": "2026-02-19 01:03:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66rk18",
          "author": "Snoo_24581",
          "text": "Interesting rankings. How do you weigh coding ability vs general reasoning? For API work I have been using Qwen models for code tasks and they punch above their weight class.",
          "score": 1,
          "created_utc": "2026-02-19 04:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67gaum",
          "author": "FriendlySecond2460",
          "text": "this is writers wish list",
          "score": 1,
          "created_utc": "2026-02-19 07:48:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67ytua",
          "author": "Moki2FA",
          "text": "This tier list looks super interesting, I love seeing how different open source LLMs stack up against each other. I‚Äôm curious about how the evaluation criteria were determined; it would be great to understand more about what factors contributed to their rankings. Could anyone share more insight on that?",
          "score": 1,
          "created_utc": "2026-02-19 10:47:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69nqvp",
          "author": "Available-Message509",
          "text": "Seriously, huge thanks to the team behind¬†**GPT-oss 120B**. It‚Äôs such a relief to have a high-performing Tier A model that actually fits on our local GPU setups. Most of the newer models like GLM-5 or Kimi are just getting way too massive for home servers (700B+ is wild..). 120B is the real sweet spot for us!",
          "score": 1,
          "created_utc": "2026-02-19 16:49:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6eaaha",
              "author": "MarkoMarjamaa",
              "text": "I'm running gpt-oss-120b. Still, it's also nice to know what kind of AI is achievable when memory prices go down. Like a conservative estimate that in 10 years I will be able to run GLM-5 size quant in my pc. ",
              "score": 1,
              "created_utc": "2026-02-20 09:53:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o69tsr8",
          "author": "tamtaradam",
          "text": "why only open-source/weights?",
          "score": 1,
          "created_utc": "2026-02-19 17:18:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ai3aq",
          "author": "Constandinoskalifo",
          "text": "RemindMe! 1 day",
          "score": 1,
          "created_utc": "2026-02-19 19:12:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cei46",
          "author": "itsjase",
          "text": "or just check here you can also filter by size [https://artificialanalysis.ai/models/open-source](https://artificialanalysis.ai/models/open-source)",
          "score": 1,
          "created_utc": "2026-02-20 01:13:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hbic3",
          "author": "___cjg___",
          "text": "Without MiniMax it‚Äòs maxifaulty",
          "score": 1,
          "created_utc": "2026-02-20 19:53:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qudda",
          "author": "Hot_Study_6062",
          "text": "So is it possible to run an open source LLM on a NAS and link it to Visual Studio if so which NAS is the best or what do I need to look for in a NAS ?",
          "score": 1,
          "created_utc": "2026-02-22 09:17:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qwo6d",
          "author": "Mattdeftromor",
          "text": "Where is Mimo-v2-flash ? ",
          "score": 1,
          "created_utc": "2026-02-22 09:39:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rvww4",
          "author": "Mordimer86",
          "text": "Comparing cloud models with over 700B to small models to run on a consumer GPU is a joke.",
          "score": 1,
          "created_utc": "2026-02-22 14:17:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdyu2f",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Was Wrong.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdyu2f/we_benchmarked_7_chunking_strategies_most_best/",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-25 01:03:00",
      "score": 59,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "If you've built a RAG system, you've had the chunking conversation. Somebody on your team (or [a Medium post](https://medium.com/%40adnanmasood/chunking-strategies-for-retrieval-augmented-generation-rag-a-comprehensive-guide-5522c4ea2a90)) told you to \"just use 512 tokens with 50-token overlap\" or \"semantic chunking is strictly better.\"\n\nWe (hello from the R&D team at Vecta!) decided to test these claims. We created a small corpus of real academic papers spanning AI, astrophysics, mathematics, economics, social science, physics, chemistry, and computer vision. Then, we ran every document through seven different chunking strategies and measured retrieval quality and downstream answer accuracy.\n\nCritically, we designed the evaluation to be¬†fair: each strategy retrieves a different number of chunks, calibrated so that every strategy gets approximately¬†2,000 tokens of context¬†in the generation prompt. This eliminates the confound where strategies with larger chunks get more context per retrieval, and ensures we're measuring chunking quality, not context window size.\n\nThe \"boring\" strategies won. The hyped strategies failed. And the relationship between chunk granularity and answer quality is more nuanced than most advice suggests.\n\n# Setup\n\n# Corpus\n\nWe assembled a diverse corpus of 50 academic papers (905,746 total tokens) deliberately spanning similar disciplines, writing styles, and document structures: Papers ranged from 3 to 112 pages and included technical dense mathematical proofs pertaining to fundamental ML research. All PDFs were converted to clean markdown using¬†[MarkItDown](https://github.com/microsoft/markitdown), with OCR artifacts and single-character fragments stripped before chunking.\n\n# Chunking Strategies Tested\n\n1. **Fixed-size, 512 tokens**, 50-token overlap\n2. **Fixed-size, 1024 tokens**, 100-token overlap\n3. **Recursive character splitting**, LangChain-style¬†`RecursiveCharacterTextSplitter`¬†at 512 tokens\n4. **Semantic chunking**, embedding-based boundary detection (cosine similarity threshold 0.7)\n5. **Document-structure-aware**, splitting on markdown headings/sections, max 1024 tokens\n6. **Page-per-chunk**, one chunk per PDF page, using MarkItDown's form-feed (`\\f`) page boundaries\n7. **Proposition chunking**, LLM-decomposed atomic propositions following¬†[Dense X Retrieval](https://arxiv.org/abs/2312.06648)¬†with the paper's exact extraction prompt\n\nAll chunks were embedded with¬†`text-embedding-3-small`¬†and stored in local ChromaDB. Answer generation used¬†`gemini-2.5-flash-lite`¬†via OpenRouter. We generated 30 ground-truth Q&A pairs using Vecta's synthetic benchmark pipeline.\n\n# Equal Context Budget: Adaptive Retrieval k\n\nMost chunking benchmarks use a fixed top-k (e.g., k=10) for all strategies. This is fundamentally unfair: if fixed-1024 retrieves 10 chunks, the generator sees \\~10,000 tokens of context; if proposition chunking retrieves 10 chunks at 17 tokens each, the generator gets \\~170 tokens. The larger-chunk strategy wins by default because it gets more context, not because its chunking is better.\n\nWe fix this by computing an¬†adaptive k¬†for each strategy. This targets \\~2,000 tokens of retrieved context for every strategy. The computed values:\n\n|Strategy|Avg Tokens/Chunk|Adaptive k|Expected Context|\n|:-|:-|:-|:-|\n|Page-per-Chunk|961|2|\\~1,921|\n|Doc-Structure|937|2|\\~1,873|\n|Fixed 1024|658|3|\\~1,974|\n|Fixed 512|401|5|\\~2,007|\n|Recursive 512|397|5|\\~1,984|\n|Semantic|43|46|\\~1,983|\n|Proposition|17|115|\\~2,008|\n\nNow every strategy gets \\~2,000 tokens to work with. Differences in accuracy reflect genuine chunking quality, not context budget.\n\n# How We Score Retrieval: Precision, Recall, and F1\n\nWe evaluate retrieval at two granularities:¬†page-level¬†(did we retrieve the right pages?) and¬†document-level¬†(did we retrieve the right documents?). At each level, the core metrics are precision, recall, and F1.\n\nLet¬†R¬†be the set of retrieved items (pages or documents) and¬†G¬†be the set of ground-truth relevant items.\n\nPrecision measures: of everything we retrieved, what fraction was actually relevant? A retriever that returns 5 pages, 4 of which contain the answer, has a precision of 0.8. High precision means low noise in the context window.\n\nRecall measures: of everything that¬†*was*¬†relevant, what fraction did we find? If 3 pages contain the answer and we retrieved 2 of them, recall is 0.67. High recall means we're not missing important information.\n\nF1 is the harmonic mean of precision and recall. It penalizes strategies that trade one for the other and rewards balanced retrieval.\n\nPage-level metrics tell you whether you're pulling the right¬†*passages*. Document-level metrics tell you whether you're pulling from the right¬†*sources*. A strategy can score high page-level recall (finding many relevant pages) while scoring low document-level precision (those pages are scattered across too many irrelevant documents). As we'll see, the tension between these two levels is one of the main findings.\n\n# Results\n\n# The Big Picture\n\n[Figure 1:¬†Complete metrics heatmap. Green is good, red is bad.](https://www.runvecta.com/blog/chunking/metrics_heatmap.png)\n\n|Strategy|k|Doc F1|Page F1|Accuracy|Groundedness|\n|:-|:-|:-|:-|:-|:-|\n|**Recursive 512**|5|0.86|**0.92**|**0.69**|0.81|\n|**Fixed 512**|5|0.85|0.88|0.67|**0.85**|\n|**Fixed 1024**|3|**0.88**|0.72|0.61|0.86|\n|**Doc-Structure**|2|0.88|0.69|0.52|0.84|\n|**Page-per-Chunk**|2|0.88|0.69|0.57|0.81|\n|**Semantic**|46|0.42|0.91|0.54|0.81|\n|**Proposition**|115|0.27|**0.97**|0.51|**0.87**|\n\n**Recursive splitting wins on accuracy (69%) and page-level retrieval (0.92 F1).**¬†The 512-token strategies lead on generation quality, while larger-chunk strategies lead on document-level retrieval but fall behind on accuracy.\n\n# Finding 1: Recursive and Fixed Splitting Often Outperforms Fancier Strategies\n\n[Figure 2:¬†Accuracy and groundedness by strategy. Recursive and fixed 512 lead on accuracy.](https://www.runvecta.com/blog/chunking/generation_quality.png)\n\nLangChain's¬†`RecursiveCharacterTextSplitter`¬†at 512 tokens achieved the highest accuracy (**69%**) across all seven strategies. Fixed 512 was close behind at 67%. Both strategies use 5 retrieved chunks for \\~2,000 tokens of context.\n\nWhy does recursive splitting edge out plain fixed-size? It tries to break at natural boundaries, paragraph breaks, then sentence breaks, then word breaks. On academic text, this preserves logical units: a complete paragraph about a method, a full equation derivation, a complete results discussion. The generator gets chunks that make semantic sense, not arbitrary windows that may cut mid-sentence.\n\nRecursive 512 also achieved the best page-level F1 (**0.92**), meaning it reliably finds the right pages¬†*and*¬†produces accurate answers from them.\n\n# Finding 2: The Granularity-Retrieval Tradeoff Is Real\n\n[Figure 3:¬†Radar chart, recursive 512 (orange) has the fullest coverage. Large-chunk strategies skew toward doc retrieval but lose on accuracy.](https://www.runvecta.com/blog/chunking/radar_comparison.png)\n\nWith a 2,000-token budget, a clear tradeoff emerges:\n\n* **Smaller chunks (k=5)**¬†achieve higher accuracy (67-69%) because 5 retrieval slots let you sample from 5 different locations in the corpus, each precisely targeted\n* **Larger chunks (k=2-3)**¬†achieve higher document F1 (0.88) because each retrieved chunk spans more of the relevant document, but the generator gets fewer, potentially less focused passages\n\nFixed 1024 scored the best document F1 (**0.88**) but only 61% accuracy. With just k=3, you get 3 large passages, great for document coverage, but if even one of those passages isn't well-targeted, you've wasted a third of your context budget.\n\n# Finding 3: Semantic Chunking Collapses at Scale\n\n[Figure 4:¬†Chunk size distribution. Semantic and proposition chunking produce extremely small fragments.](https://www.runvecta.com/blog/chunking/chunk_size_distribution.png)\n\nSemantic chunking produced¬†17,481 chunks averaging 43 tokens¬†across 50 papers. With k=46, the retriever samples from 46 different tiny chunks. The result: only¬†54% accuracy¬†and¬†0.42 document F1.\n\nHigh page F1 (0.91) reveals what's happening: the retriever¬†*finds the right pages*¬†by sampling many tiny chunks from across the corpus. But document-level retrieval collapses because those 46 chunks come from dozens of different documents, diluting precision. And accuracy suffers because 46 disconnected sentences don't form a coherent narrative for the generator.\n\n**The fundamental problem:**¬†semantic chunking optimizes for retrieval-boundary purity at the expense of context coherence. Each chunk is a \"clean\" semantic unit, but a single sentence chunk may lack the surrounding context needed for generation.\n\n# Finding 4: The Page-Level Retrieval Story\n\n[Figure 5:¬†Page-level precision-recall tradeoff. Recursive 512 achieves the best balance.](https://www.runvecta.com/blog/chunking/precision_recall.png)\n\n[Figure 6:¬†Page-level and document-level F1. The two metrics tell different stories.](https://www.runvecta.com/blog/chunking/retrieval_f1.png)\n\nPage-level and document-level retrieval tell opposite stories under constrained context:\n\n* Fine-grained strategies¬†(proposition k=115, semantic k=46) achieve high page F1 (0.91-0.97) by sampling many pages, but low doc F1 (0.27-0.42) because those pages come from too many documents\n* Coarse strategies¬†(page-chunk k=2, doc-structure k=2) achieve high doc F1 (0.88) by retrieving fewer, more relevant documents, but lower page F1 (0.69) because 2 chunks can only cover 2 pages\n\nRecursive 512 at k=5 hits the best balance: 0.92 page F1 and 0.86 doc F1. Five chunks is enough to sample multiple relevant pages while still concentrating on a few documents.\n\n[Figure 7:¬†Document-level precision, recall, and F1 detail. Large-chunk strategies lead on precision; fine-grained strategies lead on recall.](https://www.runvecta.com/blog/chunking/document_level.png)\n\n# What This Means for Your RAG System\n\n# The Short Version\n\n1. Use recursive character splitting at 512 tokens.¬†It scored the highest accuracy (69%), best page F1 (0.92), and strong doc F1 (0.86). It's the best all-around strategy on academic text.\n2. Fixed-size 512 is a strong runner-up¬†with 67% accuracy and the highest groundedness among the top performers (85%).\n3. If document-level retrieval matters most, use fixed-1024 or page-per-chunk (0.88 doc F1), but accept lower accuracy (57-61%).\n4. Don't use semantic chunking on academic text.¬†It fragments too aggressively (43 avg tokens) and collapses on document retrieval (0.42 F1).\n5. Don't use proposition chunking for general RAG.¬†51% accuracy isn't production-ready. It's only viable if you value groundedness over correctness.\n6. When benchmarking, equalize the context budget.¬†Fixed top-k comparisons are misleading. Use adaptive k = round(target\\_tokens / avg\\_chunk\\_tokens).\n\n# Why Academic Papers Specifically?\n\nWe deliberately chose to saturate the academic paper region of the embedding space with 50 papers spanning 10+ disciplines. When your knowledge base contains papers that all discuss \"evaluation,\" \"metrics,\" \"models,\" and \"performance,\" the retriever has to make fine-grained distinctions. That's when chunking quality matters most.\n\nIn a mixed corpus of recipes and legal contracts, even bad chunking might work because the embedding distances between domains are large. Academic papers are the¬†*hard case*¬†for chunking, and if a strategy works here, it'll work on easier data too.\n\n# How We Measured This (And How You Can Too)\n\nMy team built¬†[Vecta](https://www.runvecta.com/)¬†specifically to meet the need for precise RAG evaluation software. It generates synthetic benchmark Q&A pairs across multiple semantic granularities, then measures precision, recall, F1, accuracy, and groundedness against your actual retrieval pipeline.\n\nThe benchmarks in this post were generated and evaluated using Vecta's SDK (`pip install vecta`)\n\n# Limitations, Experiment Design, and Further Work\n\nThis experiment was deliberately small-scale: 50 papers, 30 synthetic Q&A pairs, one embedding model, one retriever, one generator. That's by design. We wanted something reproducible that a single engineer could rerun in an afternoon, not a months-long research project. The conclusions should be read with that scope in mind.\n\nSynthetic benchmarks are not human benchmarks.¬†Our ground-truth Q&A pairs were generated by Vecta's own pipeline, which means there's an inherent alignment between how questions are formed and how they're evaluated. Human-authored questions would be a stronger test. That said, Vecta's benchmark generation does produce complex multi-hop queries that require synthesizing information across multiple chunks and document locations, so these aren't trivially easy questions that favor any one strategy by default.\n\nOne pipeline, one result.¬†Everything here runs on¬†text-embedding-3-small, ChromaDB, and¬†gemini-2.5-flash-lite. Swap any of those components and the rankings could shift. We fully acknowledge this. Running the same experiment across multiple embedding models, vector databases, and generators would be valuable follow-up work, and it's on our roadmap.\n\nThe equal context budget is a deliberate constraint, not a flaw.¬†Some readers may object that semantic and proposition chunking are \"meant\" to be paired with rerankers, fusion, or hierarchical aggregation. But if a chunking strategy only works when combined with additional infrastructure, that's important to know. Equal context budgets ensure we're comparing chunking quality at roughly equal generation cost. A strategy that requires a reranker to be competitive is a more expensive strategy, and that should factor into the decision.\n\nSemantic chunking was not intentionally handicapped.¬†Our semantic chunking produced fragments averaging 43 tokens, which is smaller than most production deployments would target. This was likely due to a poorly tuned cosine similarity threshold (0.7) rather than any deliberate sabotage. But that's actually the point: semantic chunking requires careful threshold tuning, merging heuristics, and often parent-child retrieval to work well. When those aren't perfectly dialed in, it degrades badly. Recursive splitting, by contrast, produced strong results with default parameters. The brittleness of semantic chunking under imperfect tuning is itself a finding.\n\n**What we'd like to do next:**\n\n* Rerun the experiment with human-authored Q&A pairs alongside the synthetic benchmark\n* Test across multiple embedding models (`text-embedding-3-large`, open-source alternatives) and generators (GPT-4o, Claude, Llama)\n* Add reranking and hierarchical retrieval stages, then measure whether the rankings change when every strategy gets access to the same post-retrieval pipeline\n* Expand the corpus beyond academic papers to contracts, documentation, support tickets, and other common RAG domains\n* Test semantic chunking with properly tuned thresholds, chunk merging, and sliding windows to establish its ceiling\n\nIf you run any of these experiments yourself, we'd genuinely like to see the results.\n\nHave a chunking strategy that worked surprisingly well (or badly) for you? We'd love to hear about it. Reach out via DM!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdyu2f/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o79553a",
          "author": "250umdfail",
          "text": "Your experiments were designed for the standard chunking strategies to win. All your papers are independent, and share very little context with each other- why would you need a complicated RAG setup? I'm assuming your queries were simple too, like what is x, explain y etc.\n\nIf you have a complicated set of documents, that have a dependence graph, a cross reference network, and ask questions that need traversing the entire knowledge graph, things would be very different. Queries like: how did x evolve over time?, how does y compare to z, explain u in terms of v, etc. are where the advanced chunking, embedding, and linking shine.\n\nAlso I'm not sure F1 scores are a good way to grade your chunking strategies. Most modern systems use a high recall phase followed by a high precision stage over those recalled documents. You could possibly use a human or a larger model to find the similarity between their answer, and that of your model to score your strategies.",
          "score": 12,
          "created_utc": "2026-02-25 02:16:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79n5vk",
              "author": "Mythril_Zombie",
              "text": "Where is the list of papers?",
              "score": 1,
              "created_utc": "2026-02-25 04:00:06",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7akqqn",
              "author": "SmihtJonh",
              "text": "Recursive ensemble NER can mitigate some, but I agree that chunking in itself I prone to lossy semantics",
              "score": 1,
              "created_utc": "2026-02-25 08:26:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7cgsq8",
              "author": "Confident-Honeydew66",
              "text": "This is a very well-thought-out comment, so first of all thank you.\n\n>\\>If you have a complicated set of documents, that have a dependence graph, a cross reference network, and ask questions that need traversing the entire knowledge graph, things would be very different.\n\nI agree, and acknowledge this is a serious blind spot in this analysis. Currently pushing for a second experiment using a more complex and inter-dependent dataset that requires multi-hop reasoning over similar-looking chunks.\n\n>\\>I'm not sure F1 scores are a good way to grade your chunking strategies.\n\nF1 score is often left behind in favor of recall, but precision, which is admittedly less important, ultimately determines if the downstream LLM is prone to context rot. Ideally, both recall and precision are maximized in the final retrieved chunks, hence our use of F1 as a benchmark.\n\n>\\>Most modern systems use a high recall phase followed by a high precision stage over those recalled documents.\n\nBy the end of these stages, the F1 should still be high. It is irrelevant if the retriever has multiple stages -- our analysis only considers the end result of the retrieval process.",
              "score": 1,
              "created_utc": "2026-02-25 16:01:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o79ifgi",
          "author": "Caesarr",
          "text": "Interesting analysis. Chunking on paragraphs (i.e. human-defined atomic blocks of semantic meaning) would also be worth testing, no?",
          "score": 2,
          "created_utc": "2026-02-25 03:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ajq0u",
          "author": "Healthy_Library1357",
          "text": "Two quick thoughts:  \nParagraph length variance matters. In papers, some paragraphs are 80 tokens, others 600+. Without a cap + light overlap, you‚Äôll get unstable context budgets and retrieval noise. It might improve coherence but hurt coverage. If avg paragraph is \\~300‚Äì400 tokens, your adaptive k probably lands around 5‚Äì6 for a 2k token budget. That‚Äôs similar to Recursive 512, so gains may be marginal unless paragraphs align unusually well with answer spans.\n\nWould be interesting to see paragraph + max 768 cap vs Recursive 512 head-to-head. My bet: small lift in groundedness, similar accuracy, slightly worse page recall.",
          "score": 1,
          "created_utc": "2026-02-25 08:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7atbpn",
          "author": "ManufacturerWeird161",
          "text": "Finally seeing someone test this instead of just repeating advice. Our team also found that a fixed 512-token approach tanked performance on our legal docs, where citations at the end of sections were critical for context. Your findings on parent-child hierarchies match our internal testing.",
          "score": 1,
          "created_utc": "2026-02-25 09:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1usv",
          "author": "Ok_Bedroom_5088",
          "text": "if you chunk, semantic will always win",
          "score": 1,
          "created_utc": "2026-02-25 11:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmes0",
          "author": "Snoo_24581",
          "text": "This is a great point. Thanks for sharing!",
          "score": 1,
          "created_utc": "2026-02-25 13:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cha4e",
          "author": "Confident-Honeydew66",
          "text": "Saw someone ask for a TLDR in the comments before it got deleted.\n\nTLDR: keep it simple, stupid",
          "score": 1,
          "created_utc": "2026-02-25 16:04:08",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o7cjcxl",
          "author": "sophie_zlngr",
          "text": "You've hit your limit ¬∑ resets 4am (Europe/Vienna)",
          "score": 1,
          "created_utc": "2026-02-25 16:13:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cjvf2",
          "author": "Suspicious-Name4273",
          "text": "For source code chunking the story might be different. There are semantic chunking strategies for source code that are structure-aware, like this: https://chunkhound.github.io/under-the-hood/#design-philosophy\n\nWould be interesting if this has any real benefit to other chunking strategies or even dumb fulltext search.",
          "score": 1,
          "created_utc": "2026-02-25 16:15:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8kgld",
      "title": "GLM-5 is officially on NVIDIA NIM, and you can now use it to power Claude Code for FREE üöÄ",
      "subreddit": "LLMDevs",
      "url": "https://github.com/Alishahryar1/free-claude-code",
      "author": "PreparationAny8816",
      "created_utc": "2026-02-19 00:30:13",
      "score": 39,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Resource üöÄ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8kgld/glm5_is_officially_on_nvidia_nim_and_you_can_now/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o688fb1",
          "author": "tech_1729",
          "text": "Saying free claude code is misleading üòÖ",
          "score": 2,
          "created_utc": "2026-02-19 12:06:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67fo1c",
          "author": "ZenApollo",
          "text": "Does the proxy support openai flavor endpoints?",
          "score": 1,
          "created_utc": "2026-02-19 07:42:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68h0az",
          "author": "zoidme",
          "text": "What relative quality you can expect on this? Like gpt-4.x or better?",
          "score": 1,
          "created_utc": "2026-02-19 13:05:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69267t",
          "author": "--dany--",
          "text": "How does it compare to Claude Code Router?",
          "score": 1,
          "created_utc": "2026-02-19 15:03:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o692ucf",
          "author": "lingondricka2",
          "text": "I tried it using Nvidia NIM, neither GLM-5 or Qwen 3.5 gave me a response, step-3.5-flash worked fine though, thank you",
          "score": 1,
          "created_utc": "2026-02-19 15:06:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lkaia",
          "author": "JasperQuandary",
          "text": "Was slower than molasses with opencode. Not enough capacity.",
          "score": 1,
          "created_utc": "2026-02-21 13:39:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68bv69",
          "author": "SectionCrazy5107",
          "text": "I dont see the claims on free request to be really true anywhere from Nvidia site, it seems usable only when on browser for light prototype, not as daily driver. I will be delighted to be proven wrong so I can really use it.",
          "score": 0,
          "created_utc": "2026-02-19 12:31:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rc0f1j",
      "title": "Opensource is truly catching up to commercial LLM coding offerings",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rc0f1j/opensource_is_truly_catching_up_to_commercial_llm/",
      "author": "1nam2nam",
      "created_utc": "2026-02-22 22:55:41",
      "score": 37,
      "num_comments": 18,
      "upvote_ratio": 0.76,
      "text": "( My crude thoughts in relatively bad english. Fuck you grammar Nazis. )\n\nGot frustrated by Claude Code base (20$) to do anything serious due to the high token usage. Gemini is unusable due to high volume (literally for last 16 hours. Not a single prompt) .\n\nFrustrated and tried opencode + Kimi 2.5. Blown away by the cost. Performance is nearly as good as Sonnet 4.5 (I prefer it to Opus 4.6 based on my own experience) or Gemini 3. \n\nI believe rude awakening for frontier labs as more devs are forced to switch. \n\n  \nThese labs won't command the high premium pricing hence valuations for long.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rc0f1j/opensource_is_truly_catching_up_to_commercial_llm/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6wg2c6",
          "author": "Qxz3",
          "text": "Yup, these businesses don't have much of a moat. Their lofty valuations are beyond ridiculous.¬†",
          "score": 9,
          "created_utc": "2026-02-23 04:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72eu1j",
              "author": "TrainerThin",
              "text": "The moat is training costs. If Chinese government subsidezes their way to market dominance like other industries or steal, we‚Äôll all be using Chinese models soon.\n\nWhich is fine and healthy in theory. But lots of power goes to winners of AI race.",
              "score": 2,
              "created_utc": "2026-02-24 02:28:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6wthrq",
          "author": "SourceOfConfusion",
          "text": "yeah, most enterprise use cases do not require frontier models. The open source models coming out of China are really quite good and fit most used cases.",
          "score": 5,
          "created_utc": "2026-02-23 06:36:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uxqew",
          "author": "Tema_Art_7777",
          "text": "Codex with chat gpt plus is a powerhouse. I don‚Äôt run out. Anthropic rate limits like hell so avoid that. kimi 2.5 subscription is same price as openai - I certainly would‚Äôt pay that much for kimi",
          "score": 8,
          "created_utc": "2026-02-22 23:13:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wwh2p",
              "author": "Low-Exam-7547",
              "text": "I have never hit any limits on Claude Code using Max.",
              "score": 4,
              "created_utc": "2026-02-23 07:02:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x2ptr",
                  "author": "sgtfoleyistheman",
                  "text": "I have the first max subscription and only use it for 2 personal projects. I've hit my weekly limit 3 weeks in a row! I'm surprised it's enough for your job",
                  "score": 3,
                  "created_utc": "2026-02-23 08:01:26",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6wwmex",
                  "author": "Tema_Art_7777",
                  "text": "that is max!! i am using a $20/m chatgpt plus. how much r u paying",
                  "score": 1,
                  "created_utc": "2026-02-23 07:04:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6v34d2",
              "author": "1nam2nam",
              "text": "Codex is roughly same tier as Gemini. It is too slow to get anything done",
              "score": -1,
              "created_utc": "2026-02-22 23:44:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6v6esr",
                  "author": "Tema_Art_7777",
                  "text": "Not sure what your tiering us but frontier models have access to much better hardware and have better inference speed. kimi folks complain about hardware constraints",
                  "score": 3,
                  "created_utc": "2026-02-23 00:03:11",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6waodz",
                  "author": "DurianDiscriminat3r",
                  "text": "Codex 5.3 is better than opus 4.6. Try scaffolding a project from a brief. It can handle complex projects way better. Opus is good at planning (very detailed) and codex is good at implementation.",
                  "score": 3,
                  "created_utc": "2026-02-23 04:09:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6vopxs",
                  "author": "Upbeat-Cloud1714",
                  "text": "Slow is subjective. Fast and sloppy is not useful and that's what most models do right now. Codex is great for complex repositories where time is subjective to the scale of the task at hand. ",
                  "score": 1,
                  "created_utc": "2026-02-23 01:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2w01",
          "author": "lsmith77",
          "text": "open weight != open source",
          "score": 5,
          "created_utc": "2026-02-23 08:03:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wabok",
          "author": "Pleasant_Heat7314",
          "text": "I agree, it's been really impressive to watch. I think this trend is likely to accelerate over the next year or two.",
          "score": 1,
          "created_utc": "2026-02-23 04:06:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xhi0o",
          "author": "Zeikos",
          "text": "As soon as I'll get my reverse proxy done on my homelab I'll drop all services in favour of my self hosted OpenWebUI server.  \n\nAPIs are so much cheaper it's not even funny.  \nAnd I can hook my local models to it too.",
          "score": 1,
          "created_utc": "2026-02-23 10:26:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79qyxc",
          "author": "cutlossking",
          "text": "How do I find a software engineer to help me code and build out with ai??",
          "score": 1,
          "created_utc": "2026-02-25 04:24:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbvlyb",
      "title": "If the current LLMs architectures are inefficient, why we're aggressively scaling hardware?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rbvlyb/if_the_current_llms_architectures_are_inefficient/",
      "author": "en00m",
      "created_utc": "2026-02-22 19:50:57",
      "score": 34,
      "num_comments": 32,
      "upvote_ratio": 0.85,
      "text": "Hello guys! As in the title, I'm genuinely curious about the current motivations on keeping information encoded as tokens, using transformers and all relevant state of art LLMs architecture/s.\n\nI'm at the beginning of the studies this field, enlighten me.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rbvlyb/if_the_current_llms_architectures_are_inefficient/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6tvzc2",
          "author": "SamWest98",
          "text": "To run the inefficient LLMs!¬†",
          "score": 50,
          "created_utc": "2026-02-22 19:58:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6uucic",
              "author": "undo777",
              "text": "The good news is we'll have lots of power needs, maybe nuclear takes off!",
              "score": 6,
              "created_utc": "2026-02-22 22:54:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6w0guv",
                  "author": "rditorx",
                  "text": "What are the good news?",
                  "score": 1,
                  "created_utc": "2026-02-23 03:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6txags",
          "author": "i_wayyy_over_think",
          "text": "There‚Äôs newer  techniques like Engrams by DeepSeek that tries to keep reasoning separate from knowledge. \n\nAlso GPUs are programmable so when new techniques are available, it‚Äôs just a software update, so doesn‚Äôt make sense to hold back hardware.",
          "score": 25,
          "created_utc": "2026-02-22 20:05:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72q7nw",
              "author": "Playful-Job2938",
              "text": "It does, these ai farms are putting us back into a worse spot than covid. The rest of the world needs compute too:",
              "score": 1,
              "created_utc": "2026-02-24 03:36:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6u5sjg",
              "author": "BarrenLandslide",
              "text": "Yes exactly this. Even the big KIMI K2 models, which are basically hundreds of SLM under the hood need at least like a 1 Mio USD rack to run on halfway usable quantisation.",
              "score": 1,
              "created_utc": "2026-02-22 20:48:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6v4km7",
                  "author": "jeffdn",
                  "text": "That is not how MoE models work, and basically every model released in the last year has been an MoE, Kimi isn‚Äôt special in that regard.",
                  "score": 6,
                  "created_utc": "2026-02-22 23:52:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6u1ynt",
          "author": "typeryu",
          "text": "I like to think this as the same as saying ‚Äúnuclear fusion energy is clearly better and safer than fission energy‚Äù. Almost everyone knows there are theoretically much more capable world simulators that should just get it (whatever that is), but we are not there yet and we don‚Äôt even know if it is doable with the current hardware stack and data. LLMs are here and available now and they are far more capable than what is currently mainstream. Based on the incremental improvements we‚Äôve been getting, we still have many years of improvement ahead of us not to mention it will take even more time for the average folks and businesses to adopt the latest form which is agentic LLMs. That alone I think is enough to wipe out a ton of work and also accelerate development on other technologies so that is why money is being poured in. There‚Äôs definitely some over investing going on in some places, but in general the big labs should come through as the new tech conglomerates.",
          "score": 4,
          "created_utc": "2026-02-22 20:28:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u37a3",
          "author": "docgpt-io",
          "text": "To the best of my knowledge, keeping information encoded as tokens has nothing to do with efficiency loss, it's rather the fact that we encode all information from the internet in giant neural networks and always talk to at least very large parts of the network - the LLMs shouldn't need to know how high the Eiffeltower is to help you with Maths, yet they do, and this is not efficient. I think the reasons why the spending keeps increasing anyway, are:  \n1. it still reaches out --> the value that can be created with LLMs is still remarkable and it makes sense to keep spending from an economic perspective  \n2. efficiency is rapidly improving",
          "score": 3,
          "created_utc": "2026-02-22 20:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u554x",
          "author": "BarrenLandslide",
          "text": "Because clever orchestration of SLMs, TLMs calling deterministic tools is the future.",
          "score": 3,
          "created_utc": "2026-02-22 20:45:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u5pck",
          "author": "funbike",
          "text": "Diffusion LLMs have a completely different architecture.   Someone took image-generation AI and applied it to text.  Look into Inception's Mercury, which performs well.",
          "score": 3,
          "created_utc": "2026-02-22 20:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u0mwr",
          "author": "kkania",
          "text": "Our power generation based on the Carnot cycle (so coal, gas, nuclear) is only 30-40% efficient, and we‚Äôve been at it for a hundred years at this point. People don‚Äôt give a shit about efficiency in general, and it only becomes a thing when fuel runs out (eg oil for cars). It‚Äôll probably need to happen with power for compute first before we see efficiency in ai getting improved.",
          "score": 4,
          "created_utc": "2026-02-22 20:21:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6txver",
          "author": "Mysterious-Rent7233",
          "text": ">Hello guys! As in the title, I'm genuinely curious about the current motivations on keeping information encoded as tokens, using transformers and all relevant state of art LLMs architecture/s.\n\nThe motivation is: \"This is what we know works. Other approaches are unproven research.\" That's all. There isn't a magic wand to invent a better architecture. You actually have to invent it. Which might take six months, six years or sixty years.",
          "score": 2,
          "created_utc": "2026-02-22 20:07:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u1b9x",
          "author": "chickenAd0b0",
          "text": "Read Richard Sutton‚Äôs ‚Äúthe bitter lesson‚Äù essay then you‚Äôll understand why everyone is scaling.",
          "score": 2,
          "created_utc": "2026-02-22 20:25:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x98ah",
              "author": "Mysterious-Rent7233",
              "text": "Everyone is scaling...except Sutton. Who believes they are scaling the wrong thing.",
              "score": 1,
              "created_utc": "2026-02-23 09:05:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6uu79o",
          "author": "Tema_Art_7777",
          "text": "they will all improve - as new papers are emerging on optimization. However, for ai to be pervasive and ambient, the current infrastructure we have is woefully inadequate and investments are quite welcome. Anthropic is rate limiting the hell out of everyone as it is. I believe investors have faith that innovations will make things better with llm usage. While not a promised road to AGI at all, there is massive benefits still to be realized with what we currently have!",
          "score": 2,
          "created_utc": "2026-02-22 22:54:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tv5ii",
          "author": "earmarkbuild",
          "text": "because money got invested and there is no getting it back (remember the ads before the dot com bubble hit? I don't.)\n\nP.S.\n\n**and yet the kings are naked.**\n\nCurrent industry status quo is [customer lock-in and data extraction disguised as comfort and coddling](https://www.reddit.com/r/OpenIP/comments/1r8wcuj/enshittification_and_its_alternativesmd/), and they won't stop gatekeeping user context corpora because they have no other levers of user retention.\n\n---\n\nIn the meantime, nobody is stopping anybody from exporting their data. Export it, unpack it, get conversations, save to folder, open whatever claude code gemini codex you decide to use, continue conversation locally. Then help someone else do the same. \n\n**They can't even hold you. They have no power here. It's all pretend.**\n\n---\n\n[the intelligence is in the language. the model is a commodity.](https://gemini.google.com/share/81f9af199056) <-- talk to it! it's just language.\n\n---\nP.P.S. [the industry can be regulated](https://www.reddit.com/user/earmarkbuild/comments/1rblqui/a_practical_way_to_govern_ai_manage_signal_flow/)",
          "score": 1,
          "created_utc": "2026-02-22 19:54:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6tyu7o",
          "author": "Low-Opening25",
          "text": "Ok, so what do you propose, what‚Äôs your replacement architecture exactly? to me it seems like you didn‚Äôt understand the fundamentals. LLM architecture is based on transformers and matrix multiplication and they operate on tokens.\n\nWhat you propose is equivalent of, hey, why computers have to operate on 0s and 1s and binary logic, why not mix this up?",
          "score": 1,
          "created_utc": "2026-02-22 20:12:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u4729",
          "author": "Fabulous-Possible758",
          "text": "Even with improving efficiency we‚Äôre also increasing demand a lot.  Remember a single query now might be multiple tool calls, inferencing on the results, maybe *more* tool calls, and all of that on larger and larger context windows, and they‚Äôre still trying to sell and incorporate this into wider and and wider user bases.  A .9x improvement in compute usage still doesn‚Äôt matter if you have 100x as many uses for it.",
          "score": 1,
          "created_utc": "2026-02-22 20:40:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6uffx9",
          "author": "coloradical5280",
          "text": "Because the future architectures like JEPA, Test-Time Training, State Space Models, etc, are more efficient in many ways but still need a ton of compute, and unfortunately, probably more memory, so we need compute post-transformers too.",
          "score": 1,
          "created_utc": "2026-02-22 21:36:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6unmsn",
          "author": "imkindathere",
          "text": "Why do you say they're inefficient? I would say they're efficient because they can be fully parallelized. That's what allowed them to scale to the size they're at now",
          "score": 1,
          "created_utc": "2026-02-22 22:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v1pol",
          "author": "Sonoftalltree",
          "text": "Think about the Mag 7 and what options they have to continue growing their returns year after year, after they are already so big. Then think about the risk of AI eating their SaaS margins. The strategy is to have a tool no one else can run. In some respects, the inefficient nature is a feature because now startups have considerably less advantage.",
          "score": 1,
          "created_utc": "2026-02-22 23:36:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6v5ate",
          "author": "red_hare",
          "text": "The path forward is fine-tuning smaller models for task-specific execution.\n\nBut user demand and progress on larger general purposes models is outpacing the cost of task-specific fine-tuning.\n\nBest thing that could happen to the industry right now would be a slowdown in SotA general purposes model progress.",
          "score": 1,
          "created_utc": "2026-02-22 23:56:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vuhm4",
          "author": "damnburglar",
          "text": "Among other things: Gold rush.",
          "score": 1,
          "created_utc": "2026-02-23 02:25:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6vzypj",
          "author": "FirmSignificance1725",
          "text": "First I would say, define inefficient. We‚Äôve very quickly grown accustomed to LLMs, but this is still new in the grand scheme of innovation. The transformer architecture is able to achieve a functionality prior impossible, even with data center level of resources. \n\nThere are many other interesting theoretical implications of transformers, but one of the biggest was the fact that it didn‚Äôt follow the law of diminishing return as aggressively as other models. Most models were restricted to a specific type of task and/or topped off quickly when generalized, flattening regardless of parameter count increase. Transformers however have continuously gotten better and shown better generalizability as parameter count has increased. \n\nSo, I would say that while they are resource hogs, I would not generally classify the transformer as ‚Äúinefficient‚Äù. Yes, maybe compared to a standard program, but that program has nowhere near the capability of the deployed LLM. I would say it‚Äôs quite efficient for what it does and we‚Äôre attempting to push it as far as we can at scale. \n\nThat being said, the reason we‚Äôre scaling hardware is because product X shows some capability and economic benefit both short and long term, that companies have deemed it valuable enough to invest Y dollars for Z return. \n\nOptimizations constantly happen. Can use mixture of experts to reduce active params, better kernels, KV Cache, pipeline parallelism, quantizations, <insert technique here> to make it more efficient. And those techniques will continue to be discovered and implemented.\n\nBut, if we reached the threshold where value exceeds cost, then were executing",
          "score": 1,
          "created_utc": "2026-02-23 02:58:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xucnq",
          "author": "Valuable-Mix4359",
          "text": "I keep seeing the argument that ‚Äútransformers are inefficient, so why are we scaling hardware,‚Äù and I think it mixes two different layers of analysis.\n\nAt the model level, yes, transformers are expensive. Attention is costly, long context windows are costly, inference isn‚Äôt lightweight. But they scale in a highly predictable way. More parameters + more data + more compute ‚Üí better performance, with relatively stable scaling curves. From an engineering perspective, that kind of predictability has significant value.\n\nAs long as marginal capability gains remain higher than marginal compute costs, scaling is not irrational. It‚Äôs an economic decision.\n\nWhat seems more interesting to me is that we‚Äôre no longer operating at the ‚Äúone call = one response‚Äù level. Production systems today are often multi-step pipelines: RAG, tool calls, retries, fallback models, agent loops, reflection passes, etc. A single user request can trigger multiple inferences and large context usage.\n\nEven if base model efficiency improves by 20%, total system-level compute can still increase because workflows become more complex. Lower unit cost tends to increase usage. This is no longer purely a model efficiency problem ‚Äî it‚Äôs an allocation problem at the system level.\n\nMany teams still default to routing most tasks to the largest available model, even when parts of the workflow could be handled by a smaller model or a deterministic component. That‚Äôs not really about architectural elegance. It‚Äôs about compute routing.\n\nI‚Äôm not convinced the main bottleneck is ‚Äúfind a radically new architecture tomorrow.‚Äù It may be more about optimizing compute allocation across models, tasks, and constraints at the system layer. Scaling looks excessive if you isolate the model, but less so when you look at the entire infrastructure stack.\n\nAre people here actually measuring cost per workflow rather than just cost per token?",
          "score": 1,
          "created_utc": "2026-02-23 12:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xzv33",
          "author": "Potential-Leg-639",
          "text": "LLMs are getting more efficient and there is still a shortage on hardware. Better be prepared.",
          "score": 1,
          "created_utc": "2026-02-23 12:56:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74gqx3",
          "author": "qubridInc",
          "text": "Because scaling hardware gives reliable gains *today*, even if the architecture isn‚Äôt perfect.\n\nTransformers are easy to parallelize, scaling laws still hold, and all existing infra is built around them so, more compute = better models right now. New, more efficient architectures are being researched, but they‚Äôre not yet proven at the same scale.",
          "score": 1,
          "created_utc": "2026-02-24 12:16:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7aroka",
          "author": "werdnum",
          "text": "Because we're pretty sure that growth in demand will outstrip efficiency gains.",
          "score": 1,
          "created_utc": "2026-02-25 09:32:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6typ8n",
          "author": "Fuzzy_Pop9319",
          "text": "As it happens, the elegant data structures that are being brute forced are from a finite structure, and as it happens in mathematics, no one will take you seriously or give you grants or hire you if you are using finite mathematics.    \nEverything else spawns from this.",
          "score": 0,
          "created_utc": "2026-02-22 20:12:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbmq30",
      "title": "not sure if hot take but mcps/skills abstraction is redundant",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rbmq30/not_sure_if_hot_take_but_mcpsskills_abstraction/",
      "author": "uriwa",
      "created_utc": "2026-02-22 14:07:08",
      "score": 26,
      "num_comments": 50,
      "upvote_ratio": 0.72,
      "text": "Whenever I read about MCPs and skills I can't help but think about the emperor's new clothes.\n\nThe more I work on agents, both for personal use and designing frameworks, I feel there is no real justification for the abstraction. Maybe there was a brief window when models weren't smart enough and you needed to hand-hold them through tool use. But that window is closing fast.\n\nIt's all just noise over APIs. Having clean APIs and good docs *is* the MCP. That's all it ever was.\n\nIt makes total sense for API client libraries to live in GitHub repos. That's normal software. But why do we need all this specialized \"search for a skill\", \"install a skill\" tooling? Why is there an entire ecosystem of wrappers around what is fundamentally just calling an endpoint?\n\nMy prediction: the real shift isn't going to be in AI tooling. It's going to be in businesses. **Every business will need to be API-first.** The companies that win are the ones with clean, well-documented APIs that any sufficiently intelligent agent can pick up and use.\n\nI've just changed some of my ventures to be API-first. I think pay per usage will replace SaaS.\n\nAI is already smarter than most developers. Stop building the adapter layer. Start building the API.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rbmq30/not_sure_if_hot_take_but_mcpsskills_abstraction/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6ry33f",
          "author": "Thick-Protection-458",
          "text": "Hm ... Wasn't MCP always exactly just the way to expose remote / other non-foreseen tools for use by the model?\n\n\nSo you know, not like some fancy magic idea, but just a way to provide standartized interface so services can provide it instead of going xkcd 14 standards situation?",
          "score": 23,
          "created_utc": "2026-02-22 14:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s69q9",
              "author": "das_war_ein_Befehl",
              "text": "For early models, yeah. Nowadays they‚Äôre pretty good at just using the API. MCP server are heavy on context and honestly LLMs work better with CLIs anyways",
              "score": 3,
              "created_utc": "2026-02-22 15:12:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6srghb",
                  "author": "ThenExtension9196",
                  "text": "The early models of mid 2025.",
                  "score": 6,
                  "created_utc": "2026-02-22 16:48:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6sh1s8",
              "author": "uriwa",
              "text": "I don't think this will work long term. If it did, humans would do it too.",
              "score": 1,
              "created_utc": "2026-02-22 16:02:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s4okv",
          "author": "strangeanswers",
          "text": "putting an MCP server over an API standardizes access control, abstracts away schema changes and deduplicates efforts since agent developers will probably need to create an abstract tool layer for the API anyways.",
          "score": 13,
          "created_utc": "2026-02-22 15:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sdtvi",
              "author": "damhack",
              "text": "MCP is not supposed to just layer over REST or GraphQL APIs.  That‚Äôs a poor use.  MCP is a remote procedure call interface and should be handled accordingly.\n\nThe OP is right.  Recent LLM systems are more than capable of making a call to an API in an orderly manner if the API schema and (not totally necessary) the API docs are available.  This is due to both better reasoning performance, better tool calling and code execution abilities.  In fact a reasonable, context-saving approach is to ask a recent LLM to provide a parameterized prompt snippet to make a particular call and then provide those snippets as a library of available calls to your agents in future. Less context use, faster predictable calls and less security vulnerability hunting.",
              "score": 4,
              "created_utc": "2026-02-22 15:48:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6s7ads",
              "author": "cmndr_spanky",
              "text": "This is how I think about MCP anyways, that said (just to play devils advocate), if an API library is well documented a multi-turn agent (with a smart LLM) could easily read API docs and make API queries for you with a generic http request tool and web scraper tool. If all you‚Äôre doing is writing simple MCP abstractions over APIs.. it‚Äôs kinda the same shit (just handing over more autonomy to the agent and possibility of it making a bad choice in how it uses an API). \n\nThat said, if you‚Äôre writing custom logic that your agent is meant to execute, MCP (or skills) is the best way‚Ä¶ there‚Äôs no api to wrap",
              "score": 1,
              "created_utc": "2026-02-22 15:17:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6s8zvw",
                  "author": "strangeanswers",
                  "text": "but then you‚Äôre wasting tokens and causing context rot by needing to load all those API docs into your context window to understand what the api does instead of just reading a tool description. how will the agent even know which api is useful for the task at hand? should it read the docs for all the APIs available to it each time?\n\nalso, if you spin up a new agent and want it to use this API, you probably need to whitelist it for that endpoint, which adds a bunch of complexity. if you instead just expose one tool to it and all requests from that tool come to the api through an MCP server it massively simplifies access management.",
                  "score": 7,
                  "created_utc": "2026-02-22 15:25:57",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o70namj",
                  "author": "Dihedralman",
                  "text": "Why are you making a deterministic process non-deterministic?¬†",
                  "score": 1,
                  "created_utc": "2026-02-23 20:46:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rxgd0",
          "author": "OkLettuce338",
          "text": "How would you remotely install a skill that requires authentication?",
          "score": 8,
          "created_utc": "2026-02-22 14:26:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sgh66",
              "author": "uriwa",
              "text": "LLMs need env variables that get hot swapped, that's enough I think.",
              "score": -2,
              "created_utc": "2026-02-22 16:00:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6srzec",
                  "author": "OkLettuce338",
                  "text": "No it wouldn‚Äôt be. How would you install the skill remotely for a diverse set of users?",
                  "score": 6,
                  "created_utc": "2026-02-22 16:50:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tq220",
          "author": "igorim",
          "text": "it's a massive risk when everytime some model needs to read something or do something it decides to read arbitrary code. MCP is not about a model not being able to do X it's about 1. saving it tokens to do X, and 2. adding deterministic guardrails, and 3. Having a shared interface so you don't need to reimplement for every model\n\n",
          "score": 3,
          "created_utc": "2026-02-22 19:28:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s21pm",
          "author": "XiiMoss",
          "text": "Yeah sound I‚Äôll just give the agent direct access to my API keys shall I",
          "score": 3,
          "created_utc": "2026-02-22 14:50:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sfr96",
              "author": "uriwa",
              "text": "the harness should hot swap the keys. llm doesn't need to know them. (like in deno sandboxes)",
              "score": 0,
              "created_utc": "2026-02-22 15:57:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s2ct0",
          "author": "vogut",
          "text": "It's just a tool list and a prompt fetcher. The hype around it was dumb, I agree. But it's necessary",
          "score": 3,
          "created_utc": "2026-02-22 14:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s7kbg",
          "author": "cmndr_spanky",
          "text": "MCP as a simple wrapper over APIs might be silly, but if you use custom written logic (nothing to do with APIs) that you want your LLM agent to execute like a function, MCP / skills is def the way to go..",
          "score": 4,
          "created_utc": "2026-02-22 15:19:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6sgurp",
              "author": "uriwa",
              "text": "humans just call that \"documentation\", and they put it in places like google docs, markdown files in github etc'. There is no protocol for it.",
              "score": -4,
              "created_utc": "2026-02-22 16:01:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6sn8d1",
                  "author": "cmndr_spanky",
                  "text": "No that‚Äôs incorrect. You often don‚Äôt want variations / non-determinism in how an LLM should execute known logic which needs to be stable. So authoring a python function exposed via MCP is the best way, the ‚Äúdocument‚Äù only describes how and when to execute the function.",
                  "score": 3,
                  "created_utc": "2026-02-22 16:29:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o772wvy",
                  "author": "cats_r_ghey",
                  "text": "Your takes are pretty rough. Even humans working in a team have a standard they follow with their documentation.\n\nImagine the insane mental load if there was no structure to anything?",
                  "score": 1,
                  "created_utc": "2026-02-24 19:56:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6tz1fn",
          "author": "WolfeheartGames",
          "text": "MCP is great for not restful api. Anything that wraps complex logic or maintains a state for the agent. Ghidra mcp, Godot mcp, playwright mcp, that sort of thing.",
          "score": 2,
          "created_utc": "2026-02-22 20:13:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u8vsx",
          "author": "apf6",
          "text": "MCP is great when you need something with builtin Oauth support.",
          "score": 2,
          "created_utc": "2026-02-22 21:04:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s6i43",
          "author": "dreamingwell",
          "text": "I wish I could auto block anyone that posts ‚ÄúMCP isn‚Äôt necessary‚Äù",
          "score": 3,
          "created_utc": "2026-02-22 15:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ta2ky",
              "author": "Hammer466",
              "text": "I could make an mcp for that!",
              "score": 4,
              "created_utc": "2026-02-22 18:14:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o73vu67",
                  "author": "Andrew_Ngrok",
                  "text": "you can do it without an mcp",
                  "score": 1,
                  "created_utc": "2026-02-24 09:16:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6y8eyf",
          "author": "albaldus",
          "text": "MCP = distribution, exposition¬†¬†\n\nSkills = execution¬†",
          "score": 1,
          "created_utc": "2026-02-23 13:49:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o772fr3",
          "author": "cats_r_ghey",
          "text": "I don‚Äôt think you know what you‚Äôre talking about.",
          "score": 1,
          "created_utc": "2026-02-24 19:54:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78gpyj",
          "author": "caio__oliveira",
          "text": "Not every capability needs to be a business. Think coding agents: why would a code editing, or file reading need to be an MCP? Why wrap stuff with JSON-RPC unless necessary?\n\nAnother reason is how MCPs require a round trip to the LLM provider, so it costs more tokens. It's also harder to compose MCPs into higher level functionality.\n\nThere's a bunch of material around how MCP was a bad idea (look up stuff like \"MCP was a bad abstraction\", and cloudflare's code execution post), and I agree that it being the default way of giving capabilities to agents probably caused more harm than good.",
          "score": 1,
          "created_utc": "2026-02-25 00:00:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6s8rlp",
          "author": "qlwkerjqewlkr",
          "text": "MCP is cringe and pointless",
          "score": 0,
          "created_utc": "2026-02-22 15:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wap5e",
          "author": "Clear-Dimension-6890",
          "text": "I agree. I‚Äôm not a big fan of skills and hooks. Just another point of failure . Put it all in a config file or write utilities",
          "score": 0,
          "created_utc": "2026-02-23 04:09:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdmxi9",
      "title": "Are large language models actually generalizing, or are we just seeing extremely sophisticated memorization in a double descent regime?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdmxi9/are_large_language_models_actually_generalizing/",
      "author": "InevitableRespond494",
      "created_utc": "2026-02-24 17:40:25",
      "score": 22,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "I‚Äôve been trying to sharpen my intuition about large language models and I‚Äôd genuinely appreciate input from people who work in ML or have a strong technical background. I‚Äôm not looking for hype or anti-AI rhetoric, just a sober technical discussion.\n\nHere‚Äôs what I keep circling around:\n\nLLMs are trained on next-token prediction. At the most fundamental level, the objective is to predict the next word given previous context. That means the training paradigm is imitation. The system is optimized to produce text that statistically resembles the text it has seen before. So I keep wondering: if the objective is imitation, isn‚Äôt the best possible outcome simply a very good imitation? In other words, something that behaves as if it understands, while internally just modeling probability distributions over language?\n\nWhen people talk about ‚Äúemergent understanding,‚Äù I‚Äôm unsure how to interpret that. Is that a real structural property of the model, or are we projecting understanding onto a system that is just very good at approximating linguistic structure?\n\nAnother thing that bothers me is memorization versus generalization. We know there are documented cases of LLMs reproducing copyrighted text, reconstructing code snippets from known repositories, or instantly recognizing classic riddles and bias tests. That clearly demonstrates that memorization exists at non-trivial levels. My question is: how do we rigorously distinguish large-scale memorization from genuine abstraction? When models have hundreds of billions of parameters and are trained on massive internet-scale corpora, how confident are we that scaling is producing true generalization rather than a more distributed and statistically smoothed form of memorization?\n\nThis connects to overfitting and double descent. Classical ML intuition would suggest that when model capacity approaches or exceeds dataset complexity, overfitting becomes a serious concern. Yet modern deep networks, including LLMs, operate in highly overparameterized regimes and still generalize surprisingly well. The double descent phenomenon suggests that after the interpolation threshold, performance improves again as capacity increases further. I understand the empirical evidence for double descent in various domains, but I still struggle with what that really means here. Is the second descent genuinely evidence of abstraction and structure learning, or are we simply in a regime of extremely high-dimensional interpolation that looks like generalization because the data manifold is densely covered?\n\nThen there‚Äôs the issue of out-of-distribution behavior. In my own experiments, when I formulate problems that are genuinely new, not just paraphrased or slightly modified from common patterns, models often start to hallucinate or lose coherence. Especially in mathematics or formal reasoning, if the structure isn‚Äôt already well represented in the training distribution, performance degrades quickly. Is that a fundamental limitation of text-only systems? Is it a data quality issue? A scaling issue? Or does it reflect the absence of a grounded world model?\n\nThat leads to the grounding problem more broadly. Pure language models have no sensorimotor interaction with the world. They don‚Äôt perceive, manipulate, or causally intervene in physical systems. They don‚Äôt have multimodal grounding unless explicitly extended. Can a system trained purely on text ever develop robust causal understanding, or are we mistaking linguistic coherence for a world model? When a model explains what happens if you tilt a table and a phone slides off, is it reasoning about physics or statistically reproducing common narrative patterns about objects and gravity?\n\nI‚Äôm also curious about evaluation practices. With web-scale datasets, how strictly are training and evaluation corpora separated? How do we confidently prevent benchmark contamination when the training data is effectively ‚Äúthe internet‚Äù? In closed-source systems especially, how much of our trust relies on company self-reporting? I‚Äôm not implying fraud, but the scale makes rigorous guarantees seem extremely challenging.\n\nThere‚Äôs also the question of model size relative to data. Rough back-of-the-envelope reasoning suggests that the total volume of publicly available text on the internet is finite and large but not astronomically large compared to modern parameter counts. Given enough capacity, is it theoretically possible for models to internally encode enormous portions of the training corpus? Are LLMs best understood as knowledge compressors, as structure learners, or as extremely advanced semantic search systems embedded in a generative architecture?\n\nBeyond the technical layer, I think incentives matter. There is massive economic pressure in this space. Investment cycles, competition between companies, and the race narrative around AGI inevitably shape communication. Are there structural incentives that push capability claims upward? Even without malicious intent, does the funding environment bias evaluation standards or public framing?\n\nFinally, I wonder how much of the perceived intelligence is psychological. Humans are extremely prone to anthropomorphize coherent language. If a system speaks fluently and consistently, we instinctively attribute intention and understanding. To what extent is the ‚Äúwow factor‚Äù a cognitive illusion on our side rather than a deep ontological shift on the model‚Äôs side?\n\nAnd then there‚Äôs the resource question. Training and deploying large models consumes enormous computational and energy resources. Are we seeing diminishing returns masked by scale? Is the current trajectory sustainable from a systems perspective?\n\nSo my core question is this: are modern LLMs genuinely learning abstract structure in a way that meaningfully transcends interpolation, or are we observing extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime that happens to look intelligent?\n\nI‚Äôd really appreciate technically grounded perspectives. Not hype, not dismissal, just careful reasoning from people who‚Äôve worked close to these systems.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdmxi9/are_large_language_models_actually_generalizing/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o76bwrm",
          "author": "hymn_7-62",
          "text": "You raise good questions and I'm interested in answers, sadly I dont think we'll get lucky with someone who actually knows their shit.",
          "score": 7,
          "created_utc": "2026-02-24 17:55:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76jy1x",
          "author": "McMonty",
          "text": "First off: Great post!\n\nMy Ask: You'll need to define\n\n\\> meaningfully transcends interpolation\n\nI think a lot of research in the AI field was in this area during the early AI stages that pre-dated NNs decades ago. Personally, I've always liked Hofstadter's takes on AI such as those in \"I am a strange loop\". I doubt you'll find much better answers to \"what even is generalization\" than in his writing(GEB and \"Surfaces and Essences\" are also great!).  \n  \nBut although he was initially skeptical of LLMs, he has changed his tone a bit in these past few years to start to question if the recursive elements present in LLMs has hit a turning point where we should be questioning what it is that we've created: [https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai](https://www.lesswrong.com/posts/kAmgdEjq2eYQkB5PP/douglas-hofstadter-changes-his-mind-on-deep-learning-and-ai)\n\nMy own 2 cents: There is something to LLMs beyond just memorization, but its constrained still in a way that differs from how our own brains are constrained(Ultimately, our own ability to generalize is still subject to limits). I might even go as far as to say that I'd consider LLMs to be \"capable of consciousness\" to some extent - although I don't think I'd say that they are \"alive\". They are in a weird space where all of our definitions start to break down and are severely lacking nuance to describe the variety of possible forms of cognition. Similar things happen when you really peel back the layers between different forms of animal minds and compare them with human ones, but this is even weirder.",
          "score": 9,
          "created_utc": "2026-02-24 18:30:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o78kcxf",
          "author": "PresentSituation8736",
          "text": "1. The \"World Model\" vs. High-Dimensional Interpolation You asked if models are genuinely learning abstract structure or just operating in an overparameterized interpolation regime. The consensus among interpretability researchers (looking at things like mechanistic interpretability and induction heads) is: It‚Äôs both, but leaning heavily toward sophisticated interpolation. LLMs do learn abstract representations. They don't just memorize strings of text; they build latent features for concepts (e.g., a \"gender\" direction, a \"formality\" vector, or coding syntax trees). To predict the next token efficiently across petabytes of data, the network must compress the data. And the best way to compress data is to discover the underlying generative rules. However, this does not equal a causal \"World Model.\" When the model describes a phone sliding off a tilted table, it is not running a physics engine in its latent space. It is navigating the semantic topology of how humans talk about physics. This is why LLMs fail so catastrophically on Out-of-Distribution (OOD) reasoning, spatial tasks, or novel math. If the solution isn't densely represented in the training manifold, the model cannot extrapolate. It can only interpolate. \n\n2. Memorization vs. Abstraction (The Double Descent Reality) You brought up double descent. In the overparameterized regime, models perfectly fit the training data (memorization) and then find the \"simplest\" function that interpolates between those points (generalization). But here is the dirty secret of modern LLMs: the training data is so massive that the \"data manifold\" covers almost every common human thought. What looks like zero-shot generalization to us is often just the model finding a latent bridge between two memorized concepts. It is \"generalizing,\" but strictly within the convex hull of human internet text. \n\n3. The Benchmark Contamination Crisis You asked: \"How strictly are training and evaluation corpora separated?\" They aren't. This is the biggest open secret in the industry right now. With web-scale scraping, almost every classic riddle, math problem, and coding test is in the training data. Companies try to de-duplicate and filter, but it is practically impossible to prevent \"data leakage\" entirely. Many \"emergent capabilities\" reported in 2023 were later debunked as the models simply having seen the test set during training. This is why closed-source claims must be taken with a massive grain of salt.\n\n 4. The Anthropomorphic Illusion & Incentives Your point about the ELIZA effect (anthropomorphism) is the psychological engine driving the current hype cycle. We are evolutionarily hardwired to attribute consciousness to fluent language. When an LLM uses the word \"I\", our brains immediately project a mind onto it. Combine this cognitive bias with the VC funding environment, and you get a toxic incentive structure. Companies are incentivized to frame sophisticated statistical pattern-matching as \"sparks of AGI\" because that unlocks billions in computing budgets. If they admitted, \"We built a lossy, trillion-parameter semantic search engine,\" the valuations would crash. The Conclusion To answer your core question: Modern LLMs are highly advanced, lossy knowledge compressors. They do learn structural abstractions of language (grammar, tone, logic structures), but they use these structures to perform statistical pattern completion. \n\nThey lack grounded causality, they cannot reliably extrapolate outside their training distribution, and their \"reasoning\" is a simulation driven by the linguistic shadows of human thought. It is a breathtaking engineering achievement, but your intuition is correct: we are largely confusing linguistic coherence for ontological intelligence. Keep pulling on these threads. The industry needs this level of skepticism right now.",
          "score": 3,
          "created_utc": "2026-02-25 00:20:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ceova",
              "author": "AI-Agent-geek",
              "text": "Great response",
              "score": 1,
              "created_utc": "2026-02-25 15:52:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77tjxi",
          "author": "Bulky-Flamingo9898",
          "text": ">In other words, something that behaves as if it understands, while internally just modeling probability distributions over language?\n\nI think ‚Äúbehaves as if it understands‚Äù isn‚Äôt really distinguishable from mimicking language patterns in general. So as the better the models get at generating language similar to the training set it will inevitably sound more human and as though it understands. \n\n>are we projecting understanding onto a system that is just very good at approximating linguistic structure?\n\nPartly that but the models do seem to have properties that seem to imply they are doing more than simply spewing back  training material. One thing that struck me early is how well llms seem to be able to rhyme and so if you ask it for a song it will create awful doggrel but it does rhyme. Hard to square the behaviour without thinking that it must in some sense be storing information about the sounds of words along with meaning and is able to invoke this in certain contexts. Not sure this has to be understanding but it‚Äôs related and seems deeper than the stochastic parrot caricature  \n\n>Or does it reflect the absence of a grounded world model? \n\nI would maybe characterise the optimistic view as being if you feed a big enough model enough data it will work out something near a world model itself but as you point out memoization is also happening and what seems to be tough is to encourage good world model building. Training purely for next word probably is close to diminishing returns \n\n>are modern LLMs genuinely learning abstract structure in a way that meaningfully transcends interpolation, or are we observing extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime that happens to look intelligent? \n\nThere seems to be something more than straight interpolation going on, but not enough to make me think AGI is just around the corner",
          "score": 2,
          "created_utc": "2026-02-24 21:59:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79yfxa",
          "author": "MrRandom04",
          "text": "A well-supported rebuttal to the idea that autoregressive language models cannot really learn global reasoning, planning and abstraction: https://arxiv.org/abs/2512.15605",
          "score": 2,
          "created_utc": "2026-02-25 05:17:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o79ysup",
              "author": "MrRandom04",
              "text": "This paper, combined with the idea that sufficiently advanced broad RLVR post-training can allow well-documented generalization of capabilities to reach past human expert levels, is essentially the real research bet that the frontier labs are making with their current strides towards AGI.",
              "score": 2,
              "created_utc": "2026-02-25 05:19:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77xai4",
          "author": "theOmnipotentKiller",
          "text": "I think your perspective is based on how GPT-3 (circa 2022) was trained. Saying that LLMs do just next token prediction implies that pre-training is the only thing that matters. \n\nWe have gone through 4 distinct phases in post training since then\n- RLHF\n- structured json grammars\n- test-time search\n- (now) reinforcement learning on tool call sequences\n\nThis should make your question a lot simpler. Next token predictor GPT-3 still felt like BERT. Models today are so different. The above list I gave is like the top highlights of post training, there‚Äôs a lot more going on that we prolly don‚Äôt even know. World models are being actively used to do better RL for agents right now for example.\n\nI think to understand pre-training you have to understand DPO. Next token prediction captured a lot of interesting behaviors in hard to elicit ways. Everything after has been a slow grind of finding the right eval harness and collect enough data to make that micro-behavior a macro-behavior through painstaking manual effort and hopefully some synthetic generation hacks. \n\nAs for true generalization, my only metric for that is how much revenue will Anthropic print per sector of the economy. I am an empiricist and I think the free markets will let you know if things are generalizing or not. It‚Äôs easy to fall for investor posturing optics so you really have to dig to know if they are. Anthropic for the most part has been honest in their communications based on what I have seen on the ground, other labs don‚Äôt share as much as them. \n\nThis question is much better suited for r/mlscaling - you‚Äôll get better answers there. Model training is a gated profession so us LLM Devs can just conjecture and hope the next models just work. Evals went out the window in mid 2025 so it‚Äôs all just vibes here now. Learning theory and all that is tech we hope the labs figure out.",
          "score": 3,
          "created_utc": "2026-02-24 22:17:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76qew6",
          "author": "visarga",
          "text": "\\> extremely sophisticated statistical pattern completion operating in an overparameterized double descent regime\n\nan extremely sophisticated dunce that is fooling everyone? ",
          "score": 1,
          "created_utc": "2026-02-24 18:59:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77kb64",
              "author": "pab_guy",
              "text": "I think it's hard for people to accept that with intelligence, the map IS the territory.",
              "score": 2,
              "created_utc": "2026-02-24 21:17:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o77uxvs",
          "author": "earmarkbuild",
          "text": "[intelligence is governed language](https://gemini.google.com/share/81f9af199056) <- talk to it; it's language!\n\n^^ that's a fully articulated generalized protocol for governable intelligence :P",
          "score": 1,
          "created_utc": "2026-02-24 22:06:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79op5m",
          "author": "thisdude415",
          "text": "IMO the fact that LLMs are able to easily work with random UUIDs (which are by definition never before seen in their training data) demonstrates that there is something beyond memorization.",
          "score": 1,
          "created_utc": "2026-02-25 04:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o773u2j",
          "author": "dmter",
          "text": "neural networks are just emergent virtual machines that utilize layer machinery to emerge code that satisfies the training data\n\nf.ex. in some image processing nn there are actual image processing algorithms running between layers and nn learns to process input images by giving correct parameters to these algorithms and then it does some math on those results which is also emergent.\n\nsame is done inside llm but unlike image processing, people have no idea how it works so they just assume it's magic. hence hyperscaling fallacy.",
          "score": 1,
          "created_utc": "2026-02-24 20:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77k5ad",
          "author": "pab_guy",
          "text": "TL;DR\n\nHowever, note that your entire perceptual world as a human is recalling learned patterns.  The magic happens when we combine or exchange ideas across disciplines.\n\nFor background on this type of thing I recommend \"Everything is a remix\" and the  veritasium video on expertise.\n\nOne example: a chess grandmaster can memorize pieces on a chessboard very well.  But if you put pieces on the board in a way that doesn't reflect how a real game would play out (novel placements) the grandmaster's advantage evaporates.  The skill is based on memorizing and recognizing patterns.",
          "score": 0,
          "created_utc": "2026-02-24 21:16:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77oiy7",
              "author": "Officer_Trevor_Cory",
              "text": "\"One example: a chess grandmaster can memorize pieces on a chessboard very well. But if you put pieces on the board in a way that doesn't reflect how a real game would play out (novel placements) the grandmaster's advantage evaporates. The skill is based on memorizing and recognizing patterns.\"\n\nwell this is not a good example. grandmasters play freestyle, start from a massive advantage and adapt extremely quickly.",
              "score": 1,
              "created_utc": "2026-02-24 21:36:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77wa1x",
                  "author": "pab_guy",
                  "text": "How do you think they adapt extremely quickly? Because they know the patterns!",
                  "score": 1,
                  "created_utc": "2026-02-24 22:12:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rbc0d2",
      "title": "Antigravity (Gemini 3.1 Pro) just solved a Next.js Tailwind build bug I‚Äôve been struggling with for a year.",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rbc0d2/antigravity_gemini_31_pro_just_solved_a_nextjs/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-22 04:13:06",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.71,
      "text": "For almost a year, my Next.js portfolio build would fail every single time I ran `npm run build`. The error message was completely useless:\n\nRepo: [https://github.com/AnkitNayak-eth/ankitFolio](https://github.com/AnkitNayak-eth/ankitFolio)  \nLive site: [https://ankit-nayak.vercel.app/](https://ankit-nayak.vercel.app/)\n\n    HookWebpackError: Cannot read properties of undefined (reading 'length')\n    in cssnano-simple\n\nIt always crashed during CSS minification. I went down every rabbit hole imaginable Webpack configs, different Next.js versions, cssnano issues, dependency updates. Nothing worked.\n\nMy only workaround was disabling minification in `next.config.ts`:\n\n    config.optimization.minimize = false\n\nThe build would pass, but my production app was completely unoptimized. I eventually accepted it as one of those strange ‚ÄúNext.js things.‚Äù\n\nToday, I decided to try Antigravity, powered by Gemini 3.1 Pro. I let it analyze the repository. It ran for about half an hour digging through the codebase and then it surfaced the actual root cause.\n\nIt wasn‚Äôt Webpack.  \nIt wasn‚Äôt cssnano.  \nIt wasn‚Äôt Next.js.\n\nIt was a Tailwind arbitrary value with a template literal:\n\n    <div className={`flex [mask-image:linear-gradient(to_${direction},transparent,black_10%,black_90%,transparent)]`}>\n\nTailwind couldn‚Äôt statically analyze `to_${direction}` at build time, so it generated invalid CSS. When Next.js passed that to cssnano for minification, the process crashed. The stack trace pointed in the wrong direction for months.\n\nThe fix was simply making the class static with a ternary:\n\n    <div className={`flex ${\n      direction === 'left'\n        ? '[mask-image:linear-gradient(to_left,...)]'\n        : '[mask-image:linear-gradient(to_right,...)]'\n    }`}>\n\nAfter that, production builds worked immediately. Minification enabled. No crashes.\n\nI spent a year blaming Webpack and Next.js for what was ultimately a dynamic Tailwind string interpolation mistake. Antigravity, powered by Gemini 3.1 Pro, found it in under an hour.\n\nUff What a crazzy time to be alive. ü§∑‚Äç‚ôÇÔ∏è",
      "is_original_content": false,
      "link_flair_text": "Great Discussion üí≠ ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rbc0d2/antigravity_gemini_31_pro_just_solved_a_nextjs/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6qdko0",
          "author": "coloradical5280",
          "text": "There is no way opus 4.6 or codex-5.x-xhigh , would have failed to find this, particularly with chrome dev tools MCP",
          "score": 8,
          "created_utc": "2026-02-22 06:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wk2m1",
              "author": "Cod3Conjurer",
              "text": "I did experiment with Claude a few months ago (don't remember which model), but it didn't crack this one back then.",
              "score": 2,
              "created_utc": "2026-02-23 05:17:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6v4o2x",
          "author": "eltron",
          "text": "I‚Äôve been burned with Tailwind arbitrary interpolation errors before and I‚Äôve found those error to be red herring errors. They usually distract me for a good chunk of time.",
          "score": 2,
          "created_utc": "2026-02-22 23:53:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wjvjb",
              "author": "Cod3Conjurer",
              "text": "The problem wasn't understanding that Tailwind discourages dynamic classes. The problem was a production only cssnano crash with zero indication it was related to Tailwind or which component caused it.",
              "score": 1,
              "created_utc": "2026-02-23 05:16:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ws7wp",
                  "author": "coloradical5280",
                  "text": "No offense but the real failure was not implementing robust error handling, verbose debug logging, and not connecting basic web dev tools that are literally created to handle exactly these issues",
                  "score": 0,
                  "created_utc": "2026-02-23 06:25:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xg3k9",
          "author": "hugganao",
          "text": "first prove to me you don't work for google",
          "score": 2,
          "created_utc": "2026-02-23 10:12:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xybzi",
              "author": "Cod3Conjurer",
              "text": "He he üòÇ¬†",
              "score": 1,
              "created_utc": "2026-02-23 12:46:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xhaoh",
          "author": "CorneZen",
          "text": "Dude, from your post history it‚Äôs clear that you are now pushing AI generated content. It‚Äôs too obvious. Your site styling and layout looks sweet though.",
          "score": 0,
          "created_utc": "2026-02-23 10:24:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xya94",
              "author": "Cod3Conjurer",
              "text": "I'm not \"pushing Al content,\" I just use Al as a tool\n\n\nAnd appreciate the compliment on the site thanks man¬†",
              "score": 1,
              "created_utc": "2026-02-23 12:46:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rafi3g",
      "title": "I built an LLM gateway in Rust because I was tired of API failures",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rafi3g/i_built_an_llm_gateway_in_rust_because_i_was/",
      "author": "SchemeVivid4175",
      "created_utc": "2026-02-21 02:42:49",
      "score": 16,
      "num_comments": 16,
      "upvote_ratio": 0.7,
      "text": "I kept hitting the same problems with LLMs in production:\n\n\\- OpenAI goes down ‚Üí my app breaks\n\n\\- I'm using expensive models for simple tasks  \n\n\\- No visibility into what I'm spending\n\n\\- PII leaking to external APIs\n\nSo I built Sentinel - an open-source gateway that handles all of this.\n\n\n\nWhat it does:\n\n\\- Automatic failover (OpenAI down? Switch to Anthropic)\n\n\\- Cost tracking (see exactly what you're spending)\n\n\\- PII redaction (strip sensitive data before it leaves your network)\n\n\\- Smart caching (save money on repeated queries)\n\n\\- OpenAI-compatible API (just change your base URL)\n\n\n\nTech:\n\n\\- Built in Rust for performance\n\n\\- Sub-millisecond overhead\n\n\\- 9 LLM providers supported\n\n\\- SQLite for logging, DashMap for caching\n\n\n\nGitHub: [https://github.com/fbk2111/Sentinel](https://github.com/fbk2111/Sentinel)\n\n\n\nI'm looking for:\n\n\\- Feedback on the architecture\n\n\\- Bug reports (if you try it)\n\n\\- Ideas for what's missing\n\n\n\nBuilt this for myself, but figured others might have the same pain points.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rafi3g/i_built_an_llm_gateway_in_rust_because_i_was/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6mcwb7",
          "author": "Karyo_Ten",
          "text": "Seems very wrong.\n\nFirst of all, there is no tests.\n\nSecond, how do you accurately count the number of tokens?\n\nThird. The `assess_complexity` function is completely wrong, it hardcodes keyword in English, lower-case, doesn't account for typo or multilinguage or mixed-case.\n\nFourth. \"What's a croissant?\" and \"What's a pain au chocolat?\" are likely to have a high cosine similarity score and your semantic cache is likely to be very buggy. It also prevents regeneration of answers.",
          "score": 4,
          "created_utc": "2026-02-21 16:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6moy6b",
              "author": "LatentSpaceLeaper",
              "text": ">First of all, there is no tests.\n\nü§£ Thank you and goodbye!\n\n(thank you for taking your time and giving OP constructive feedback. Hope OP is amenable to it.)",
              "score": 3,
              "created_utc": "2026-02-21 17:17:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6l1xhn",
          "author": "Ihavenocluelad",
          "text": "How does an llm gateway help you using expensive models for simple tasks lmao? Just call another provider? What differentiates this AI Gateway from LiteLLM Openrouter etc",
          "score": 2,
          "created_utc": "2026-02-21 11:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lakyj",
          "author": "esmurf",
          "text": "Is it smarter than opencode?¬†",
          "score": 1,
          "created_utc": "2026-02-21 12:30:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m5fs6",
          "author": "airylizard",
          "text": "Yeah‚Ä¶ interchangeability isn‚Äôt a thing. What testing have you done? Because I know from personal experience that there is no world in which you just change out the model and it works flawlessly",
          "score": 1,
          "created_utc": "2026-02-21 15:40:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nexn7",
              "author": "SchemeVivid4175",
              "text": "what model are you talking about , this is not a model training\n\n",
              "score": -1,
              "created_utc": "2026-02-21 19:27:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6p5r0z",
                  "author": "elbiot",
                  "text": "Lol did you even read your own post? Let's think. How could model interoperability relate to your post about routing requests to random models?",
                  "score": 3,
                  "created_utc": "2026-02-22 01:21:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kkj9l",
          "author": "hopfi2k",
          "text": "Well done. Star absolutely ‚≠êÔ∏è deserved",
          "score": 0,
          "created_utc": "2026-02-21 08:23:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lm5pk",
          "author": "Antic_Hay",
          "text": "I vibe-coded some data utilities in Rust that do video analysis, OCR, voice transcription etc. where I need near real-time performance ideally, rust made sense here because I could just say to claude \"optimise this for my M3 mac and make sure all cores are used even on a single file operation\".\n\nA gateway is a great idea, but I don't see the Rust value...though no better or worse than anything else. I mean node is single-threaded and interpreted, and can be performant if done right. But neither here nor there :)",
          "score": 0,
          "created_utc": "2026-02-21 13:51:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jqn96",
          "author": "ai_hedge_fund",
          "text": "This is a good idea to put effort into\n\nStarred it and intend to check it out\n\nThank you",
          "score": -6,
          "created_utc": "2026-02-21 04:08:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6kbh5j",
              "author": "dry_garlic_boy",
              "text": "Why do all these LLM AI posts and\n\nresponses have extra lines between\n\nthe text? It's really fucking\n\nstupid.",
              "score": 10,
              "created_utc": "2026-02-21 06:56:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6l8ehl",
                  "author": "ai_hedge_fund",
                  "text": "You‚Äôre absolutely right!",
                  "score": 5,
                  "created_utc": "2026-02-21 12:12:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mphle",
                  "author": "PuddleWhale",
                  "text": "Open claw bouncing walls of text around and ending up with a carriage return AND a linefeed where there should be only one?",
                  "score": 1,
                  "created_utc": "2026-02-21 17:20:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6mattj",
                  "author": "Karyo_Ten",
                  "text": "They want to next line but don't know you need to end a line with an antislash \\ if you want next line without doubling it.\\\nLike so.",
                  "score": 0,
                  "created_utc": "2026-02-21 16:07:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r8de88",
      "title": "Claude Sonnet 4.6 benchmark results: none reasoning beats GPT-5.2 with reasoning",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r8de88/claude_sonnet_46_benchmark_results_none_reasoning/",
      "author": "Exact_Macaroon6673",
      "created_utc": "2026-02-18 19:54:15",
      "score": 16,
      "num_comments": 8,
      "upvote_ratio": 0.83,
      "text": "We have been working on a private benchmark for evaluating LLMs. The questions cover a wide range of categories and because it is not public and gets rotated, models cannot train on it or game the results.\n\nWith Sonnet 4.6 dropping I ran it through and the results are worth talking about.\n\nSonnet 4.6 with reasoning off scores 0.648 overall. GPT-5.2 at low reasoning scores 0.604. That is not a rounding error and it has real cost implications for anyone running at scale.\n\nAt high reasoning it ties Gemini 3 Pro Preview at the top of our leaderboard with 0.719 overall, ahead of GPT-5.2 high at 0.649.\n\nHallucination resistance hits 0.921, the highest of any model we have tested. Gemini 3 Pro sits at 0.820, GPT-5.2 at 0.655. Social calibration at 0.905 and error detection at 0.848 are similarly the best we have seen.\n\nTo give credit where it is due, Gemini 3 Pro is still the better call for hard science. Philosophy 0.900 vs 0.767, chemistry 0.839 vs 0.710, economics 0.812 vs 0.750. It is not a sweep.\n\nThe honest caveat is sycophancy resistance at 0.716 is actually slightly below Sonnet 4.5 at high reasoning which scored 0.755. For a company that talks about this a lot, that is worth watching.\n\nIf reliability and hallucination resistance are your primary eval criteria nothing beats it right now.\n\nhttps://preview.redd.it/tj3yyj5t5bkg1.png?width=2588&format=png&auto=webp&s=260eac02f897164ffda778e0f332fe2b6df92890\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8de88/claude_sonnet_46_benchmark_results_none_reasoning/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o651nov",
          "author": "EarEquivalent3929",
          "text": "Benchmarks don't matter at all. If you'd actually used sonnet4.6 you'd already know it's pretty bad and hallucinated constantly on simple tasks.\n\n\nBut this post was clearly written by ai",
          "score": 5,
          "created_utc": "2026-02-18 22:33:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o65xq5z",
              "author": "Exact_Macaroon6673",
              "text": "ü´°",
              "score": -2,
              "created_utc": "2026-02-19 01:29:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o65qjan",
          "author": "promptbid",
          "text": "The hallucination resistance number is the one that matters most for our use case. At 0.921 that is a meaningful gap from the field. For any application where the model is making recommendations or surfacing information to end users, hallucination is a trust killer that is hard to recover from.\n\nThe sycophancy regression is worth flagging though. In ad-adjacent applications where you are trying to get honest signal from a model about user intent, a model that agrees too readily is actually worse than one that pushes back. Curious if your benchmark breaks that down by prompt type at all.\n\nThe cost angle you raised on non-reasoning Sonnet beating GPT-5.2 with reasoning is underrated. At scale that is not just a cost story, it is a latency story too. What does the benchmark show on response consistency across runs?",
          "score": 1,
          "created_utc": "2026-02-19 00:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o666eyo",
          "author": "kubrador",
          "text": "sonnet really said \"fine i'll be good at something\" after spending three years being the middle child of the claude family",
          "score": 1,
          "created_utc": "2026-02-19 02:20:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66a5eq",
          "author": "EbbNorth7735",
          "text": "Now do Qwen 3.5 397B",
          "score": 1,
          "created_utc": "2026-02-19 02:41:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o66o35y",
          "author": "Tema_Art_7777",
          "text": "Yes this kind of stuff is not useful at all - fleeting moments in time. Just pick one and do your tasks - its the outcome that matters, not model du jour.",
          "score": 1,
          "created_utc": "2026-02-19 04:07:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h9258",
              "author": "Useful-Process9033",
              "text": "Agreed. Model leaderboards flip every few weeks. Pick whatever works for your use case, build good evals, and swap models when it actually matters for your metrics. Chasing the latest benchmark winner is a waste of engineering time.",
              "score": 1,
              "created_utc": "2026-02-20 19:41:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67hso7",
          "author": "Low-Exam-7547",
          "text": "Can we not use the word \"dropping\" in this context? It's a music industry term for releasing records. It's found its way into enough of life. In this context it's just confusing. Let's be adults.",
          "score": 1,
          "created_utc": "2026-02-19 08:02:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8j5ob",
      "title": "How are you monitoring your Haystack calls/usage?",
      "subreddit": "LLMDevs",
      "url": "https://i.redd.it/2cxt2c949ckg1.jpeg",
      "author": "gkarthi280",
      "created_utc": "2026-02-18 23:34:34",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r8j5ob/how_are_you_monitoring_your_haystack_callsusage/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o67z29j",
          "author": "Moki2FA",
          "text": "Ah yes, the classic quest for the Holy Grail of metrics. You‚Äôve got the basics covered, but let‚Äôs not forget the all important ‚Äúnumber of existential crises per request.‚Äù It‚Äôs crucial to monitor how many times you question your life choices while waiting for that model to respond. Jokes aside, consider tracking user feedback; after all, knowing if they‚Äôre actually using your app or just staring at it like a confused cat could be quite enlightening. And if you haven‚Äôt already, maybe throw in some ‚ÄúI told you so‚Äù logs for those moments when the LLM actually nails it.",
          "score": 3,
          "created_utc": "2026-02-19 10:49:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd5tbq",
      "title": "there‚Äôs a new open source tool for checking ai agent security.... is it okay to share here?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rd5tbq/theres_a_new_open_source_tool_for_checking_ai/",
      "author": "Accomplished-Wall375",
      "created_utc": "2026-02-24 04:34:17",
      "score": 13,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "hey everyone,\n\ncame across a newly released free, open source tool designed to help developers and security teams evaluate the security of ai agents‚Äô skills, tools, and integrations. it focuses on spotting issues like overly broad permissions, unsafe tool access, and weak guardrails before anything goes live in production.\n\nthere‚Äôs also a podcast episode that dives deeper into ai security, emerging risks, and where the tech is heading:  \n[https://open.spotify.com/show/5c2sTWoqHEYLrXfLLegvek](https://open.spotify.com/show/5c2sTWoqHEYLrXfLLegvek)\n\ncurious... if this would be the right place to share the repo and get feedback from the community.\n\n**Edit: S**ince everyone was asking for the link...here it is \"[Caterpiller](https://caterpillar.alice.io/)\" that scan AI agent skills for¬†security threats and btw its an open source tool...please share your feedback and thankuu for being kinder.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rd5tbq/theres_a_new_open_source_tool_for_checking_ai/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o73owa8",
          "author": "Bmaxtubby1",
          "text": "It‚Äôs probably fine to share, especially since it‚Äôs open source and directly relevant to AI agents and security. But context matters. If you just paste a repo link and a podcast, it can look like promotion. If you explain what the tool actually does, how it evaluates permissions, and where it might fail, it feels like a genuine discussion.\n\nAlso, if it‚Äôs your project, say that upfront. Most dev communities are okay with creators sharing their own tools as long as they‚Äôre transparent and open to feedback. Hidden promotion is what usually triggers backlash.",
          "score": 3,
          "created_utc": "2026-02-24 08:09:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o794s65",
          "author": "m2845",
          "text": "Feel free to post it!",
          "score": 2,
          "created_utc": "2026-02-25 02:13:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73010q",
          "author": "Aggravating_Log9704",
          "text": "The podcast sounds nice but the repo is where the meat is. Does it support custom threat models? A big issue with AI security platforms is they assume one size fits all. Real teams have very different risk profiles. If it lets you plug in bespoke rules or simulated attacks that is legit.",
          "score": 1,
          "created_utc": "2026-02-24 04:42:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o734lct",
          "author": "Any_Artichoke7750",
          "text": "I‚Äôd love to know if it integrates with CI/CD. Static analysis during dev is fine, but the real win is catching risky permissions before deploy. If it hooks into GitHub actions or similar it‚Äôs already ahead of most toy tools.",
          "score": 1,
          "created_utc": "2026-02-24 05:15:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o739cuz",
          "author": "Severe_Part_5120",
          "text": "this looks promising. i have been building some personal ai projects that interact with api‚Äôs and local scripts, and I‚Äôve had zero way to test how safe they are. even simple things like accidentally exposing api keys or letting an agent delete something it shouldn‚Äôt can be a huge problem. a tool like this seems like it could be really valuable for people testing things in a sandbox environment before going live.",
          "score": 1,
          "created_utc": "2026-02-24 05:53:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75czpr",
          "author": "penguinzb1",
          "text": "the permissions-first approach is the right starting point, but with agents the structural analysis only catches a subset of the real exposure. the rest shows up when you run it against actual inputs. an agent that looks well-scoped at the permission level will still take unexpected actions under specific input combinations nobody mapped out during design. the behavioral gaps only surface when you run it through the scenarios that actually show up in your traffic before shipping.",
          "score": 1,
          "created_utc": "2026-02-24 15:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75kcju",
          "author": "Sea-Sir-2985",
          "text": "the permissions-first approach is a good start but the real risk with agents is runtime behavior not just static config... an agent can have perfectly scoped permissions and still do unexpected things depending on what inputs it gets. i've seen agents with read-only access still cause problems by flooding APIs with requests\n\nthe CI/CD integration question is the right one though, catching risky permissions before deploy rather than after something goes wrong is where the actual value is",
          "score": 1,
          "created_utc": "2026-02-24 15:51:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdk8vu",
      "title": "Giving AI agents direct access to production data feels like a disaster waiting to happen",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1rdk8vu/giving_ai_agents_direct_access_to_production_data/",
      "author": "Then_Respect_1964",
      "created_utc": "2026-02-24 16:04:57",
      "score": 13,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "I've been building AI agents that interact with real systems (databases, internal APIs, tools, etc.)\n\nAnd I can't shake this feeling that we're repeating early cloud/security mistakes‚Ä¶ but faster.\n\nRight now, most setups look like:\n- give the agent database/tool access\n- wrap it in some prompts\n- maybe add logging\n- hope it behaves\n\nThat's‚Ä¶ not a security model.\n\nIf a human engineer had this level of access, we'd have:\n- RBAC / scoped permissions\n- approvals for sensitive actions\n- audit trails\n- data masking (PII, financials, etc.)\n- short-lived credentials\n\nBut for agents?\n\nWe're basically doing:\n\n> \"hey GPT, please be careful with production data\"\n\nThat feels insane.\n\nSo I started digging into this more seriously and experimenting with a different approach:\n\nInstead of trusting the agent, treat it like an untrusted actor and put a control layer in between.\n\nSomething that:\n- intercepts queries/tool calls at runtime\n- enforces policies (not prompts)\n- can require approval before sensitive access\n- masks or filters data automatically\n- issues temporary, scoped access instead of full credentials\n\nBasically:\n\ndon't let the agent *touch* real data unless it's explicitly allowed.\n\nCurious how others are thinking about this.\n\nIf you're running agents against real data:\n- are you just trusting prompts?\n- do you have any real enforcement layer?\n- or is everyone quietly accepting the risk right now?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdk8vu/giving_ai_agents_direct_access_to_production_data/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o75rriw",
          "author": "cmh_ender",
          "text": "agreed, boundries are crazy important. look at this video (tech with tim) he deployed clawbot but put a lot of safe guards in place.\n\n[https://www.youtube.com/watch?v=NO-bOryZoTE](https://www.youtube.com/watch?v=NO-bOryZoTE)\n\n  \nWe use ai agents with our code base right now but they can't (no permission) to approve prs, so they can create new branches and tag humans for review but can't actually deploy anything. that's been very helpful in keeping down mistakes. \n\n",
          "score": 4,
          "created_utc": "2026-02-24 16:24:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75qs3i",
          "author": "Fulgren09",
          "text": "I was an MCP doomer for months until I had the bright idea to build a conversational UI for my app.¬†\n\nAfter days of agonizingly building protocols that explain the api orchestration to accomplish task in my app, it works with Claude Sonnet.¬†\n\nWhat I learned is whoever is exposing their system to an external AI will have strong opinions on which paths it can walk in and which rooms it can enter.¬†\n\nNot saying it‚Äôs 100% fool proof but the experience of building this and the power of conversational UI gave me a lot of confidence that ppl aren‚Äôt just opening up their app free for all style.¬†\n",
          "score": 1,
          "created_utc": "2026-02-24 16:20:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75rh3s",
          "author": "DryRelationship1330",
          "text": "agree. give it to an employee who leaves the USB key of it at Panera, can't write an expression in excel that doesn't violate order-of-operations and sends a PDF of it to his co-workers to pick up the work...",
          "score": 1,
          "created_utc": "2026-02-24 16:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76752e",
          "author": "GullibleNarwhal",
          "text": "I am a tech-savvy non-coder who has been vibe-coding lately (geez that's a lot of hyphens) and I am terrified of integrating agents, or providing an agent accidentally with permissions that would potentially allow it to wipe a drive accidentally or worse. I am curious everyone's thoughts on if me prompting to provide read-only access to an agentic model running locally if it is truly constraining an agentic model.\n\nFrom what I have heard, if you are not truly sandboxing and running an agent via a VPS with its own accounts you specifically created for it, you are asking for trouble. Thoughts? Am I being gaslit by AI telling me I am properly safe-guarding agent implementation?\n\n![gif](giphy|iRcpZYWqYcJDuPMICy)\n\n",
          "score": 1,
          "created_utc": "2026-02-24 17:33:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76toas",
          "author": "Maleficent_Pair4920",
          "text": "You need an audit log for each time an Agent has touched prod data same as humans ",
          "score": 1,
          "created_utc": "2026-02-24 19:14:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o79r50w",
          "author": "kdhkillah",
          "text": "Deterministic layers of security are absolutely essential, but yes it seems like many are just trusting prompts (+ tools and any libraries the agents decide to pull), too caught up in hype to acknowledge the risks.  2026 is going to be full of bonkers breaches & skill/tool/MCP injections. This [npm package hallucination](https://www.aikido.dev/blog/agent-skills-spreading-hallucinated-npx-commands) article was eye opening for me last month.",
          "score": 1,
          "created_utc": "2026-02-25 04:26:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bk2tu",
          "author": "Efficient_Loss_9928",
          "text": "The enforcement layer is the human engineer's permission.\n\nI don't know why would you ever grant LLMs system admin access, you always provide it the same permission as the user.\n\nAnd honestly I have never seen a setup like that, it is always delegated access so agents inherit the person's access rights. Can you provide some concrete examples? Like that is so weird, I have worked with companies from startups to military, I have never seen people just grant LLMs non-delegated permissions.",
          "score": 1,
          "created_utc": "2026-02-25 13:15:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c2s0t",
          "author": "DecodeBytes",
          "text": "You may want to checkout [https://github.com/always-further/nono](https://github.com/always-further/nono) \\- disclaimer, one of the maintainers",
          "score": 1,
          "created_utc": "2026-02-25 14:55:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdk3g4",
      "title": "I built a lightweight long-term memory engine for LLMs because I was tired of goldfish memory",
      "subreddit": "LLMDevs",
      "url": "https://github.com/RaffaelFerro/synapse",
      "author": "porrabelo",
      "created_utc": "2026-02-24 15:59:31",
      "score": 10,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Resource",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdk3g4/i_built_a_lightweight_longterm_memory_engine_for/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o766vcv",
          "author": "porrabelo",
          "text": "I‚Äôm eager to know the results! Thank you!!",
          "score": 2,
          "created_utc": "2026-02-24 17:32:33",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o75u0fh",
          "author": "GullibleNarwhal",
          "text": "I am super intrigued as I am currently trying to Frankenstein together multiple models. I currently have an embedded router model for user input intent determination, a brain or language model for response generation that can be swapped, and vision models for image processing that can also be swapped. I have tried to build out a contextual memory for the brain by having it save \"memories\" of conversations, and then summarize once it reaches a certain threshold. I have yet to build enough of a record to test the memory system though. I am curious how this might integrate into it. Are you offering this open source?",
          "score": 1,
          "created_utc": "2026-02-24 16:34:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o75v5i5",
              "author": "porrabelo",
              "text": "That Sounds like a fun challenge!\nYes, it is [open source](https://github.com/RaffaelFerro/synapse) (MIT license) \nPlease try it and give me your feedback!",
              "score": 1,
              "created_utc": "2026-02-24 16:39:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o75vl0x",
                  "author": "GullibleNarwhal",
                  "text": "I will give it a shot and see if I can integrate it into my app. Curious if you have had any success with testing in just by prompting local llms, or are you connecting via an API?",
                  "score": 1,
                  "created_utc": "2026-02-24 16:41:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7bqqa5",
              "author": "Dense_Gate_5193",
              "text": "you should check out NornicDB i have the entire rag pipeline including embedding the original query, RRF + rerank down to 7ms including http transport on a 1m embedding corpus.\n\nhttps://github.com/orneryd/NornicDB",
              "score": 1,
              "created_utc": "2026-02-25 13:52:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7bqtcc",
          "author": "Dense_Gate_5193",
          "text": "you should check out NornicDB https://github.com/orneryd/NornicDB/graphs/traffic",
          "score": 1,
          "created_utc": "2026-02-25 13:53:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1raweed",
      "title": "Anyone else noticing that claude code allocates a fixed number of subagents regardless of dataset size?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1raweed/anyone_else_noticing_that_claude_code_allocates_a/",
      "author": "ddp26",
      "created_utc": "2026-02-21 17:05:49",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "I gave claude code a large fuzzy matching task ([https://everyrow.io/docs/case-studies/match-clinical-trials-to-papers](https://everyrow.io/docs/case-studies/match-clinical-trials-to-papers)) and claude independently designed a TF-IDF pre-filtering step, spun up 8 parallel subagents, and used regex for direct ID matching. But it used exactly 8 subagents whether the dataset was 200 or 700 rows on the right side, leading to the natural consequence of how coding agents plan: they estimate a reasonable level of parallelism and stick with it. Even as the dataset grows, each agent's workload increases but the total compute stays constant. \n\nI tried prompting it to use more subagents and it still capped at 8. Ended up solving it with an MCP tool that scales agent count dynamically, but curious if anyone's found a prompting approach that works. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1raweed/anyone_else_noticing_that_claude_code_allocates_a/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6owbkk",
          "author": "kubrador",
          "text": "claude's parallelism is like a guy who always orders 8 tacos regardless of how hungry he is, just with worse scaling implications.",
          "score": 1,
          "created_utc": "2026-02-22 00:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6q1wy4",
          "author": "Ok_Prize_2264",
          "text": "I ran into a similar bottleneck with parallelism caps when we were scaling our RAG agents last month. It honestly feels like these coding agents hit a ceiling because they can‚Äôt verify if adding more compute actually improves the output quality or just burns tokens. We started using a proper eval pipeline to monitor how the subagents were actually performing across different dataset sizes and it helped us catch where the logic was stalling. It really made the whole debugging process a lot smoother once we integrated Confident AI.",
          "score": 1,
          "created_utc": "2026-02-22 05:00:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdnc0f",
      "title": "Running RAG on 512MB RAM: OOM Kills, Deadlocks, Telemetry Bugs and the Fixes",
      "subreddit": "LLMDevs",
      "url": "https://v.redd.it/f9r207sydhlg1",
      "author": "Lazy-Kangaroo-573",
      "created_utc": "2026-02-24 17:54:37",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Great Discussion üí≠ ",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1rdnc0f/running_rag_on_512mb_ram_oom_kills_deadlocks/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r9dbt6",
      "title": "How do you test LLM for quality ?",
      "subreddit": "LLMDevs",
      "url": "https://www.reddit.com/r/LLMDevs/comments/1r9dbt6/how_do_you_test_llm_for_quality/",
      "author": "Easy_Ask5883",
      "created_utc": "2026-02-19 22:19:05",
      "score": 8,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "I'm building something for AI teams and trying to understand the problem better.\n\n1. Do you manually test your AI features? \n\n2. How do you know when a prompt change breaks something?\n\n  \nAt AWS we have tons of associates who do manual QA (mostly irrelevant as far as I could see) but I dont think startups and SMBs are doing it. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/LLMDevs/comments/1r9dbt6/how_do_you_test_llm_for_quality/",
      "domain": "self.LLMDevs",
      "is_self": true,
      "comments": [
        {
          "id": "o6bn718",
          "author": "Comfortable-Sound944",
          "text": "As with any QA testing, some don't do it, some do it badly, some do it well but manual, some automated it, and many adjust it over time as it makes sense.",
          "score": 2,
          "created_utc": "2026-02-19 22:35:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c8hjz",
          "author": "charlesthayer",
          "text": "I write Evals (well agentic evals). Meaning\n\n1. A way to score your output. (e.g. llm-as-judge or jury)\n2. A set of inputs to test.\n3. A fast and simple way to run this. (like a benchmark)\n\nThere are many ways to achieve this, but you can start very simply and grow. I use Arize Phoenix for traces/spans, and they have large-scale Eval features.\n\n\\- Arize Phoenix Evals: [https://arize.com/docs/phoenix/evaluation/tutorials/run-evals-with-built-in-evals](https://arize.com/docs/phoenix/evaluation/tutorials/run-evals-with-built-in-evals)  \n\\- Article I wrote: [https://medium.com/towards-artificial-intelligence/ai-sw-engineers-youre-not-prod-ready-until-you-have-this-cd37beb8d06f](https://medium.com/towards-artificial-intelligence/ai-sw-engineers-youre-not-prod-ready-until-you-have-this-cd37beb8d06f)\n\n\\- Commercial tool (Braintrust evals): [https://www.braintrust.dev/docs/evaluation](https://www.braintrust.dev/docs/evaluation)",
          "score": 2,
          "created_utc": "2026-02-20 00:37:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6h8xnx",
              "author": "Useful-Process9033",
              "text": "LLM-as-judge is underrated for catching regressions fast. The key is having a diverse enough input set that you actually cover your edge cases. Most teams test the happy path and then get surprised when a prompt change breaks some obscure but critical scenario.",
              "score": 2,
              "created_utc": "2026-02-20 19:40:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6e2yc7",
              "author": "anuragsarkar97",
              "text": "I'll take a look at those. Also do you keep changing your evals constantly? Or use vibe coding to create evals as well.\n\nHow do you decide which model to use and when",
              "score": 1,
              "created_utc": "2026-02-20 08:44:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6mv7kn",
                  "author": "charlesthayer",
                  "text": "I'm adding inputs and updating my llm-as-judge (eval tests) all the time as I hit problems. One thing I'd like to do more is dig into my Arize Phoenix traces more regularly to spot cases I missed. Right now, I'm bug-report driven, but I'd like to make this automated.",
                  "score": 1,
                  "created_utc": "2026-02-21 17:49:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6btgbh",
          "author": "Dimwiddle",
          "text": "It's always going to be a mix of automated and manual. There's also some cool ideas using skills with a QA agent, but that doesn't sound that ideal to me. \n\nI've been looking at ways to make AI code less 'viby' and have been experimenting with translating specs in to machine verifiable contracts, using test stubs. So far it's reduced a good amount bugs.",
          "score": 1,
          "created_utc": "2026-02-19 23:09:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c54b0",
          "author": "zZaphon",
          "text": "https://replayai-web.fly.dev",
          "score": 1,
          "created_utc": "2026-02-20 00:17:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6c5gbe",
          "author": "paulahjort",
          "text": "Run the same prompt suite across multiple model checkpoints and track regression automatically in Weights&Biases.\n\nThe infra side of this is underrated too. Teams often skip systematic eval because spinning up a GPU to run a full eval suite feels heavyweight. Try a CLI tool like Terradev.\n\n[*github.com/theoddden/terradev*](http://github.com/theoddden/terradev)",
          "score": 1,
          "created_utc": "2026-02-20 00:19:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6diz7p",
          "author": "Ok_Constant_9886",
          "text": "We use deepeval (open-source): [https://github.com/confident-ai/deepeval](https://github.com/confident-ai/deepeval)\n\nAlso has a commercial platform confident ai: [https://www.confident-ai.com/](https://www.confident-ai.com/)",
          "score": 1,
          "created_utc": "2026-02-20 05:43:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dr5dc",
          "author": "Slight_Republic_4242",
          "text": "We learned this the hard way. At first, we ‚Äútested‚Äù by just trying prompts ourselves and saying, ‚ÄúLooks good.\n\nThen one small prompt **change** broke.: formatting, tone, edge cases and sometimes logic\n\nAnd we didn‚Äôt notice until a user complained. LLMs don‚Äôt fail loudly.  \nThey fail quietly.\n\nNow we:\n\na. Keep fixed test inputs\n\nb. Compare outputs before & after changes\n\nc. Check edge cases on purpose\n\nd. Track regressions like real software\n\nIt‚Äôs not perfect.  \nBut treating prompts like code changed everything.",
          "score": 1,
          "created_utc": "2026-02-20 06:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6driw5",
              "author": "anuragsarkar97",
              "text": "That makes sense I'm doing the same thing too. I guess time to build a product out of it.\n10-15% of my time I'm trying to fix either the system prompt of formating or something else",
              "score": 1,
              "created_utc": "2026-02-20 06:57:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6e2r8n",
          "author": "AnythingNo920",
          "text": "in reality most SMBs do vibe testing, unless benchmarks are their key selling point. ",
          "score": 1,
          "created_utc": "2026-02-20 08:42:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e3108",
              "author": "anuragsarkar97",
              "text": "Interesting, so it's not so high on priority list. But eventually they need know how is the AI performing in some way right?",
              "score": 1,
              "created_utc": "2026-02-20 08:45:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6llof5",
                  "author": "AnythingNo920",
                  "text": "Absolutely right. They need to, but the average Joe in an SMB can't tell the difference between BLEU, ROUGE, Fluency, Accuracy, Recall or whatever other metric u wanna use. \n\nSo they do vibe testing. This feels more tangible. \nAt least thats my impression so far.",
                  "score": 1,
                  "created_utc": "2026-02-21 13:48:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ygei2",
          "author": "khureNai05",
          "text": "For me, glm 4.7 runs small test scripts + real tasks, check outputs vs expected, rerun if weird. keeps QA low-effort but still catches most breakages.",
          "score": 1,
          "created_utc": "2026-02-23 14:33:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}