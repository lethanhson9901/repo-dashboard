{
  "metadata": {
    "last_updated": "2026-01-15 16:59:59",
    "time_filter": "week",
    "subreddit": "softwarearchitecture",
    "total_items": 50,
    "total_comments": 332,
    "file_size_bytes": 409241
  },
  "items": [
    {
      "id": "1pz9v5b",
      "title": "Is it still true that 30 percent of the workforce runs 100 percent of the project?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1pz9v5b/is_it_still_true_that_30_percent_of_the_workforce/",
      "author": "StillUnkownProfile",
      "created_utc": "2025-12-30 05:39:34",
      "score": 89,
      "num_comments": 71,
      "upvote_ratio": 0.89,
      "text": "I recently hit a point of total burnout and frustration. I finally went to my manager to complain that I was doing all the work, that others werenâ€™t contributing much, and their unfinished tasks were constantly being pushed onto my plate. His response was pretty blunt: he said thatâ€™s just the reality of corporate life, especially in IT, where only about 30% of the team actually contributes to the project. Iâ€™m wondering if this is still a common, accepted truth in the industry?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pz9v5b/is_it_still_true_that_30_percent_of_the_workforce/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nwp6xo1",
          "author": "Oliceh",
          "text": "I have worked for 20 years now in various large organizations. 80% of the people is absolutely useless. Of that 80% there is a group of 25%-ish that is actively harming progress with their incompetence. They are even worse than useless. This isn't intentional incompetence, this is them doing their best. \n\nWhat I learned is... stick to the 20% that actually do stuff and learn and work with them.",
          "score": 19,
          "created_utc": "2025-12-30 08:32:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwu27ip",
              "author": "Extreme_Elderberry16",
              "text": "The 25% that actively harm the process with their incompetence is so true (at least at my company). I usually realize this when for a few weeks you are not constantly in firefighting mode. This is usually during yearly vacation periods when a large portion of said people are ooo",
              "score": 3,
              "created_utc": "2025-12-31 01:05:20",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nx1393o",
              "author": "vikings-gg",
              "text": "The quote I was told by a CVP at top technology company was this â€œ80% of the workers do 20% of the work while the other 20% of the workers do the remaining 80% of the workâ€ itâ€™s up to you what group youâ€™re going end up in",
              "score": 2,
              "created_utc": "2026-01-01 04:01:16",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwpcrdz",
              "author": "StillUnkownProfile",
              "text": "Appreciate you sharing that. I will do my best to stay in the 20% that learns and gets things done.",
              "score": 2,
              "created_utc": "2025-12-30 09:27:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwz0sxr",
              "author": "MaverickGuardian",
              "text": "Weirdest part is that incompetency doesn't prevent people ending up as ctos, architects or tech leads. Causing even more problems.",
              "score": 1,
              "created_utc": "2025-12-31 20:26:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx1rfie",
                  "author": "Oliceh",
                  "text": "Because the people promoting them are part of the 80%",
                  "score": 1,
                  "created_utc": "2026-01-01 07:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx5h6qr",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 1,
              "created_utc": "2026-01-01 22:18:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7m19d",
                  "author": "Oliceh",
                  "text": "Guess you were the 80%",
                  "score": 1,
                  "created_utc": "2026-01-02 06:12:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwp0kus",
          "author": "mistyharsh",
          "text": "As much as it may sound like Pareto Principle but it is not. Pareto Principle is about systems and their interconnections. Your manager saying this is wrong assumption on human capability.\n\nThe situation you describe is a case of bad management and bad assimilation of knowledge. The bad management because they have failed to build proper performance culture and haven't been able to incentivize teams/people to finish the projects or tasks. The bad assimilation of knowledge in team because likely the case that people are not possessing enough knowledge about the system to get the projects to the finish line.\n\nYour manager is just normalizing bad management and cloaking it with some misunderstood principle. Remember that Pareto is about explaining the natural uneven distribution within the system and not about the uneven distribution of work within the group of people. One classic example is \"Google Search and Ads\" is 70% of Google revenue (uneven earning compared to other services but can be explained by Pareto Principles) but that doesn't mean their Android or Cloud teams are useless.\n\nConsider this situation as a first red flag. There are ways to measure organization's health. If you see more red flags, it is time that you leave that place immediately.",
          "score": 38,
          "created_utc": "2025-12-30 07:34:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwph7rf",
              "author": "OkTrade8132",
              "text": "got any pointers with regards to measuring an organizations health?",
              "score": 4,
              "created_utc": "2025-12-30 10:08:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwyj8oy",
                  "author": "ericmutta",
                  "text": "The way it affects *your* health is the best measure I've found.",
                  "score": 1,
                  "created_utc": "2025-12-31 18:54:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwpb4o2",
              "author": "StillUnkownProfile",
              "text": "It clarified a lot of what I have been sensing. Thank you.",
              "score": 2,
              "created_utc": "2025-12-30 09:11:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwoo9i6",
          "author": "OneHumanBill",
          "text": "It's the good old 80/20, aka the Pareto Principle.  Twenty percent of the people contribute roughly eighty percent of the value.  It's true in any industry.  Always has been.  In the old days it was the 20% that you'd be looking to get into leadership roles as they got into their forties and fifties.\n\nWhat's going to be interesting in the next few years is if AI is going to be effectively replacing the eighty percent of the workforce that only contributes the twenty percent.  If you're frustrated just remember which side of that divide you want to be on, and how you build your reputation for value productivity.",
          "score": 43,
          "created_utc": "2025-12-30 05:50:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwor9be",
              "author": "StillUnkownProfile",
              "text": "I understand that dynamic, and Iâ€™m already on the value side. My question is whether being on that side now also means accepting burnout as normal, or if thereâ€™s still an expectation that itâ€™s managed better.",
              "score": 11,
              "created_utc": "2025-12-30 06:14:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwpuaco",
                  "author": "bezik7124",
                  "text": "Take it easy man, I was like that doing extreme overtime on top of it for a few years. It's unmanageable in the long run, and destroys your health. Do what you're comfortable with, take things slower when you're not feeling like it, find time to relax and take frequent breaks at work. And clock out exactly when your shift ends. You're still going to be more productive than the 80%, and while this might be counter-intuitive, it's probably not going to hinder your career in any way - people generally respect you more when you value your time and don't overwork yourself.",
                  "score": 5,
                  "created_utc": "2025-12-30 12:03:22",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpwd2b",
                  "author": "OneHumanBill",
                  "text": "I've been at this a long long time now.  I'm on what I hope will be my final software project as I'm approaching 50.\n\nYeah you have to accept this to some extent.  Frequent breaks and clocking out exactly on time blah blah blah will rather quickly put you on the non-value side because people are looking to you to set the pace.  If you slow down, they'll get worse. \n\nSo instead what you do is look to improve team performance via process.  Look at metrics.  Invent them if necessary.  Hey involved in methodological comparisons.  Give XP a close look.  Look to improve team accountability.  If you can improve the teams' productivity even a little bit you can then try to evade burnout.  You won't always be able to, especially when you're personal life hands you emergencies right around crunch times.\n\nOn your side, look into mental disciplines like stoicism.  Don't laugh, it's helped me.  My next trick is getting into yoga.  Software creation the highest levels does take a toll on your body and mine doesn't quite handle the physical stress like it did thirty years ago when I was getting started.\n\nThe more you try to embrace process improvements the more you'll be pulled into leadership, sales, staffing, and non-technical type of roles.  I've pushed back on this for years but I'm finally at the age where I feel like I have nothing left to prove in the technical arena.  Like I said I'm on my final project as an architect/tech lead, and starting to make that transition.  I'll be a hobbyist again, and I think I'll be able to really enjoy writing software just for fun again like I did as a kid. \n\nThis isn't the answer you're looking for probably but I've had a pretty fulfilling career, including the burnout parts.  I hope it helps.",
                  "score": 3,
                  "created_utc": "2025-12-30 12:19:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwp7i92",
              "author": "IndividualShape2468",
              "text": "And the 20% are over in the overemployed subredditÂ ",
              "score": 3,
              "created_utc": "2025-12-30 08:38:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwpwz1f",
                  "author": "OneHumanBill",
                  "text": "No.  The overemployed are screwing us all over and kidding themselves with how productive they think they are.  I've fired maybe a dozen of these assholes who try to overclock meetings and think that nobody notices.  Somebody like me will because their productivity is in the bottom 4%, Pareto squared territory, and investigate.  And if nobody notices then they're still not in the top 20%.",
                  "score": 1,
                  "created_utc": "2025-12-30 12:24:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwppozw",
              "author": "Ok-Craft4844",
              "text": "Judging by my evaluations, im perceived to be be on the 20% doing 80% side, but tbh, that won't help in the pictured scenario.\n\nIf AI really replaces 80% of the workforce, there is no one left for the 20% to sell the shit we build to, and you don't need most of the 20% either.",
              "score": 1,
              "created_utc": "2025-12-30 11:25:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwptvct",
                  "author": "ALAS_POOR_YORICK_LOL",
                  "text": "You cant really know that with any kind of confidence. There are so many variables here it's hard to confidently project how it will change the workforce.",
                  "score": 1,
                  "created_utc": "2025-12-30 12:00:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwpwjgs",
                  "author": "OneHumanBill",
                  "text": "I'm aware.  It's one of those things that the powers that be haven't thought through very well.  I don't have any good solutions though.  Do you?",
                  "score": 1,
                  "created_utc": "2025-12-30 12:20:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwoxl5i",
          "author": "Qinistral",
          "text": "Iâ€™m surprised no one is commenting on your managers response. Thatâ€™s pretty crap IMO. His job is to balance out the work and make sure people are pulling their weight. This should be HIS problem, not yours.\n\nThere are a couple main levers that lead to inequality. 1. Ability. 2. Effort. Not everyone will be high ability and high effort. Some will be medium ability and medium effort. But your manager needs to be culling low ability and/or low effort. Work is a team sport and low performers kill morale.",
          "score": 13,
          "created_utc": "2025-12-30 07:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoy8sn",
              "author": "StillUnkownProfile",
              "text": "Couldnâ€™t agree more. The manager should be managing the workload and addressing low performers, itâ€™s not really the teamâ€™s responsibility to carry that.",
              "score": 1,
              "created_utc": "2025-12-30 07:13:14",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwp5y1y",
              "author": "Rincho",
              "text": "Well, not really. Manager's needs to make sure they deliver in time, costs etc. How it is done may differ",
              "score": 1,
              "created_utc": "2025-12-30 08:23:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwopx1a",
          "author": "GrauDiamand",
          "text": "Just dont do much either lol",
          "score": 7,
          "created_utc": "2025-12-30 06:03:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoqmn9",
              "author": "StillUnkownProfile",
              "text": "Doing the same now :)",
              "score": 4,
              "created_utc": "2025-12-30 06:09:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwormx0",
          "author": "larowin",
          "text": "Do what you can, be communicative, and document everything. Youâ€™ll float a level up, or youâ€™ll survive the next cull. Itâ€™s easy to float in environments like this, but itâ€™s easy to get cut as well.",
          "score": 5,
          "created_utc": "2025-12-30 06:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoty3k",
              "author": "StillUnkownProfile",
              "text": "Iâ€™m working as a consultant so my company has no idea of what I will do, and client doesnâ€™t care how I will do.",
              "score": 2,
              "created_utc": "2025-12-30 06:36:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwoxw4z",
                  "author": "Qinistral",
                  "text": "Sounds like your best strategy is to tune for learning as much as you can so you can get a job elsewhere with better coworkers.",
                  "score": 3,
                  "created_utc": "2025-12-30 07:10:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwounzs",
          "author": "justUseAnSvm",
          "text": "I'd put concerns about your co-workers out of your mind, that stuff all averages out in the end, burnout is a far greater concern and amplifies all those anxieties about co-workers.\n\nI hope your manager helped you develop a plan, but for me, the combination of deciding to let certain things go, and being consistent in my schedule (have to walk the dog), and always tempering my effort and expectations has worked really well.\n\nJust hang in there! Burnout happens when you put work in and don't get the reward. Our brain can't help but \"learn\" that activity isn't something worth doing anymore, so celebrate your wins when you can!",
          "score": 4,
          "created_utc": "2025-12-30 06:42:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwow3la",
              "author": "StillUnkownProfile",
              "text": "I get what youâ€™re saying, but being in the same cubicle and seeing others not contribute makes it tough to just let go. How do you keep your focus in that kind of environment?",
              "score": 1,
              "created_utc": "2025-12-30 06:54:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwoyahf",
                  "author": "justUseAnSvm",
                  "text": "Be very deliberate about letting go of things you can't control. This thesis is the idea behind Stoicism, and although that belief system is largely replaced, it's survived so long because at its core is a powerful idea.\n\nIt isn't always easy, but accepting your coworkers failings make your work harder, is a far more effective belief to hold because it centers your concern wholly on what you can do.",
                  "score": 3,
                  "created_utc": "2025-12-30 07:13:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx20l8c",
          "author": "matrium0",
          "text": "My experience  after 16+ years as Software Developer, developing on some 20 to 30ish projects over the years, I would say there is SOME truth to that.\n\nTo be really productive I think there are two binary variables:\n\n\\- highly motivated\n\n\\- highly skilled\n\n  \nAn developer with both TRUE is very productive. Though in my personal experience these are about equally distributed, so you could say  that only 1/4 of devs is really great, the others either lack in-depth understanding or are a bit unmotivated or (absolute worst case) both.\n\nJust my personal experience ofc.",
          "score": 3,
          "created_utc": "2026-01-01 09:05:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwonvqd",
          "author": "Rygel_XV",
          "text": "The pareto principle. 80% of the work is being done by 20% of the people. It's a truth of life. But it is sad that your manager is ok with putting all work on you. This does not sound like a healthy place.",
          "score": 7,
          "created_utc": "2025-12-30 05:47:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwoobkn",
              "author": "StillUnkownProfile",
              "text": "Yeah, given the current market situation, making a shift is nearly impossible, so Iâ€™ll have to put up with this environment or project for now.",
              "score": 2,
              "created_utc": "2025-12-30 05:51:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpp7q5",
          "author": "Ok-Craft4844",
          "text": "Tbh, \"70% not contributing\" is optimistic, it's more like 30% not contributing, 20% trying to contribute at a net loss (but maybe will be there some day) and 20% actively working against the project, be it with forcing discussions over fictional or already solved concerns, introducing bullshit compliance or \"best practices\".",
          "score": 2,
          "created_utc": "2025-12-30 11:21:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqi5g2",
          "author": "dmangd",
          "text": "My response would be: ok, the Fire the non-contributing 70% and give me their salary",
          "score": 2,
          "created_utc": "2025-12-30 14:35:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv82wu",
          "author": "Crafty-Pool7864",
          "text": "Start saying no. Your manager has clearly told you thereâ€™s no repercussions for bad performance.",
          "score": 2,
          "created_utc": "2025-12-31 05:24:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww0spe",
          "author": "SpecificNo8047",
          "text": "Excuse me, but this is just a bad manager, using \"it has always been like this everywhere\" as an excuse for his inability to fix the work process. If he knows there is a problem, but not fixing it, though it is literally his job, he is enabling and creating this problem.\n\nDon't listen to this bs about \"everywhere is like this\", I worked in several companies and half of them had normal work life balance, and quite an equal team contributions (not possible to have 100% equal as people are different, but it was at the level acceptable for everyone). And you know what? This normal work life balance and equal contributions were the things that were organized by a team lead and a manager, it is literally their job to organize processes in a team, pick people capable for equal consistent performance and handle these issues. So you are just working in a mismanaged place with delusional management.\n\nTrust me, as soon as you obtain health issues due to overwork and stress, and will be unable to perform as good, you will be replaced in this company in a heartbeat. Place YOUR needs first, company second. Sadly most if us learn it the hard way, as well as I did.",
          "score": 2,
          "created_utc": "2025-12-31 09:35:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwoodrb",
          "author": "FreshPrinceOfRivia",
          "text": "Somebody at my org complained that 20% of people do 80% of the work and mentioned the Pareto principle. I'm not sure it's actually that bad, but a few people do take a disproportionate amount of tasks consistently.",
          "score": 2,
          "created_utc": "2025-12-30 05:51:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwor47q",
              "author": "StillUnkownProfile",
              "text": "Itâ€™s not that they will take more tasks, the other people are not able to deliver on time and Iâ€™m being pulled into it.",
              "score": 1,
              "created_utc": "2025-12-30 06:12:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwpbtjp",
                  "author": "FreshPrinceOfRivia",
                  "text": "I've experienced that this year also. At least I'm getting promoted soon, but it sucks being pulled into projects because the \"owners\" cannot handle it on their own.",
                  "score": 2,
                  "created_utc": "2025-12-30 09:18:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwovz10",
          "author": "gbrennon",
          "text": "I think i underrstood ur \"pain\"... But thats life ðŸ˜¢\n\nTry to motivate other contributors so u don't carry the whole company.... \n\nIve experienced this several times and it proved that the company doesnt even isnt autonomous....\n\nIn this scenario if 1 persona got involved in any accident the businness willl be impacted",
          "score": 1,
          "created_utc": "2025-12-30 06:53:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwozmey",
              "author": "StillUnkownProfile",
              "text": "True, theyâ€™re senior, so feedback might not go anywhere. But I guess itâ€™s worth a try since there arenâ€™t many other options.",
              "score": 1,
              "created_utc": "2025-12-30 07:25:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx54oqb",
                  "author": "salva922",
                  "text": "Always been on the 20%. I have no degree and lead a team of eth grads.\n\nI used this as leverage for getting higher salary as all others. Like this i bought a gls amg 63, RR Cullinan and a house.\n\nI make visible what I do...presentstions, internal blog posts, etc. And ofc the metrics of your ticketing system etc.",
                  "score": 1,
                  "created_utc": "2026-01-01 21:14:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwpbaif",
          "author": "wonder_grove",
          "text": "Are we sure the 20(30, whatever)% do a good job with the 80(70)% of the work? In my recent experience, there are devs who start a project/feature, they leave to the next sexy thing and the others are left with their bugs/leftovers.",
          "score": 1,
          "created_utc": "2025-12-30 09:13:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwprpj7",
              "author": "StillUnkownProfile",
              "text": "Yeah, that happens too. I think it really varies by organization and culture. In my case, the frustration is that I end up doing both: building new features and cleaning up bugs left behind by others.",
              "score": 1,
              "created_utc": "2025-12-30 11:42:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpjy0m",
          "author": "quincycs",
          "text": "Hi, been there.  Itâ€™s alluring to become part of the problem by having unfinished work too. \n\nTry to verify some accusations you may be having and make a friend with another manager or someone above your manager. \n\n1. Is it really true that the manager is doing nothing nor sees it as a problem to solve?  Or was it just a way of the manager giving you validation that it is a problem.  I canâ€™t imagine the 80/20 rule being enough to explain away the situation to a higher up.  Everyone should be contributing to the teamâ€™s performance appropriate to their level.  If not a contributorâ€¦ boom. \n\n2. Gather data so itâ€™s not just your own feelings of burn out talking.  Are there metrics that are tracked today?  Find a way to explain that the organization can be significantly more efficient and craft a measurement system.  The measurement could be as simple as tickets completed per sprint.  Then the managers job should be to identify individuals who are consistently underperforming to the teamâ€™s avg.   Consider level (junior/senior) as another factor whether someone should be contributing more or less.",
          "score": 1,
          "created_utc": "2025-12-30 10:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwpl4cy",
          "author": "quincycs",
          "text": "Consider also you may just be experiencing growing pains of seniority.  Generally the more good you get, the more lonely it can feel.  This â€œlonelinessâ€ is a way of framing the pain you feel of â€œwhy am I the onlyâ€¦â€.   Sometimes the answer is simply because you care more or worked harder or had better parents.  Try and pick a lost sheep who has potential in being trained to be more like you â€” mentor someone & find someone to mentor you.",
          "score": 1,
          "created_utc": "2025-12-30 10:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq6zjf",
          "author": "gormami",
          "text": "I would say in terms of contribution, that may be correct, depending on how you define the project team.  As a security practitioner, I need to be aware of projects, and have my own items to check off, but don't contribute to what most people think of as the project.  The same is true of operations and support roles, depending on exactly how the definitions fall.  They are there to receive information, and perhaps steer things a bit, but don't contribute to the execution of the project directly, they just make sure the exit criteria are met so that they can do their jobs once it is launched.\n\nThat is very different than having tasks assigned to others moved to you when they are not completed by the original assignee.  That is a management problem.  If those persons work for your manager, they should be held directly accountable for delivery.  If they work for other teams, it could be performance that your manager should address with theirs, or it could be poor project planning/management.  If that person's supervisor was not made aware of the workload of the project, and agreed to it before it got started, then it is not surprising that other tasks for their team might be preventing them from completing the ones for this project.  This is one of the biggest failures I've seen in project management, grabbing someone, particularly SMEs, for a project, but never getting their boss's agreement, so they don't have the time to do what the project wants. \"Matrix organizations\" are famous for that, in the end, you can't really serve more than one master.  Some**one** has to have the final say on your time and production.",
          "score": 1,
          "created_utc": "2025-12-30 13:31:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6rkgk",
          "author": "Pale_Height_1251",
          "text": "The 30% will be variable, probably depending on the size of the company/team, smaller teams tend to be better, in my experience. \n\nBut as a concept, probably true, it's surprising how many people in a team aren't really adding much value. Even people who are *good* at what they do, when they leave sometimes you realise that they were good at what they did, but didn't actually *do* that much of it.",
          "score": 1,
          "created_utc": "2026-01-02 02:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx77al5",
          "author": "Main-Eagle-26",
          "text": "Always been true. Every workplace Iâ€™ve ever been in there is dead weight and then the actual workers.",
          "score": 1,
          "created_utc": "2026-01-02 04:26:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx77uma",
              "author": "StillUnkownProfile",
              "text": "Even the startups?",
              "score": 1,
              "created_utc": "2026-01-02 04:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx81t4k",
          "author": "Slow-Bodybuilder-972",
          "text": "Iâ€™d be surprised if it was as high as 30%.\n\nMy current job, Iâ€™d say itâ€™s about 50%, previous job, 10%.",
          "score": 1,
          "created_utc": "2026-01-02 08:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqf5ha",
          "author": "dashingThroughSnow12",
          "text": "It varies over time and part of your managerâ€™s job is to fix this.\n\nYou can do a task in an hour that Jim would take two days and a lengthy review to do. It is obvious why you get the task. But Jim is a perfectly smart guy. He needs coaching and opportunities to get better. Given enough experience in your stack, Jim can do that task in half a day and an easy review. Debora likewise. She works on a bunch of small tickets because her confidence sucks. Your manager needs to get her out of her comfort zone and have her do larger, more impactful tasks. To be the primary contributor to a small project instead of only ever a helper.\n\nI was in the scenario you describe yourself in earlier this year. My manager fixed it by upskilling many of the team mates.",
          "score": 0,
          "created_utc": "2025-12-30 14:19:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8ziok",
      "title": "Finally convinced leadership to let us rewrite the legacy app. Now everyone is terrified to start",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q8ziok/finally_convinced_leadership_to_let_us_rewrite/",
      "author": "No-Possibility6866",
      "created_utc": "2026-01-10 09:08:22",
      "score": 70,
      "num_comments": 53,
      "upvote_ratio": 0.92,
      "text": "Fought for two years to get approval for this rewrite. Legacy Rails monolith that's been limping along since 2014. Spaghetti code everywhere. Zero tests. Half the team refuses to touch certain files.\n\nNow we have the green light and everyone is frozen. Including me honestly. The risk of breaking something critical during migration is real. This app processes actual money.\n\nBeen reading about different approaches. Some teams write characterization tests against the old system first. Others run both systems in parallel with feature flags. Some just go for it and fix bugs as they surface.\n\nNo clue which path makes sense for us. Would help to hear what actually worked for teams in similar situations.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q8ziok/finally_convinced_leadership_to_let_us_rewrite/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nyrfy2d",
          "author": "Salketer",
          "text": "And now you understand why it took two years to convince...",
          "score": 89,
          "created_utc": "2026-01-10 09:49:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrd68r",
          "author": "Confident_Pepper1023",
          "text": "Cover the flows with e2e and integration tests, then use strangler fig pattern to migrate it piece by piece.\n\n\nI would advise **against** running two systems in parallel as that's a slippery slope towards introducing two systems you have to maintain for an unplanned amount of time.\n\nEdit: missed an important word there",
          "score": 101,
          "created_utc": "2026-01-10 09:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyre7v5",
              "author": "breek727",
              "text": "Just make sure to delete from the old once youâ€™ve happily flipped to the new, then itâ€™s not duplicated functionality running in parallel",
              "score": 22,
              "created_utc": "2026-01-10 09:32:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyreaqj",
              "author": "funkyfly",
              "text": "Adding to that, apply the Modern Software Engineering practices right from the start for the rewrite (testing, CI/CD, TBD, etc.)",
              "score": 18,
              "created_utc": "2026-01-10 09:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nys1add",
              "author": "edgmnt_net",
              "text": "I think two systems is actually better if you can secure agreement that this is a new application and a fresh start. That means management won't require everything to match one-to-one and will plan a decent feature set and a sunsetting timeline for the old application. At the other extreme, if you need to work with an existing application, refactor it piecewise and even preserve bug-for-bug compatibility (including undocumented stuff), things aren't looking good at all (and it's pretty hard to make better choices).\n\nHowever, it doesn't sound like OP actually managed to get a green light for that, not in explicit terms at least. I would advise further negotiation and/or presenting some plan, even if they go with a refactoring like you said.",
              "score": 8,
              "created_utc": "2026-01-10 12:51:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nyxa8i8",
              "author": "justaguy1020",
              "text": "How do you migrate without running in parallel to some extent?",
              "score": 2,
              "created_utc": "2026-01-11 05:26:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcbjss",
                  "author": "Wiszcz",
                  "text": "strangler fig pattern",
                  "score": 2,
                  "created_utc": "2026-01-13 12:33:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyrs842",
          "author": "waterkip",
          "text": "Why not start writing tests and untangle the monolith? How did you get the green light without a plan? Im confused.",
          "score": 15,
          "created_utc": "2026-01-10 11:40:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrmy7c",
          "author": "Isogash",
          "text": "Done this for replacing an external system we didn't control, and we actually approached it by first using an incremental migration.\n\n\nEvery day, a batch process would run through the data from the old system and turn it into a bunch of events, and then we would apply any new events to the new system (along with checking that no previously applied events had changed.)\n\n\nThis migration gave use a system in a useful state where we could start delivering immediate value in some cases e.g. integrating with a different Direct Debit provider, replacing semi-manual payment workflows. All of these initiatives allowed us to gradually grow and replace functionality that the old system used to do for us.\n\n\nEventually, the new system was just doing everything, so we started writing some of our new business straight to it and also making it the source of truth for others. Once the migration was complete, the old system was just out of the loop entirely and we'd managed to do the whole thing with only a couple of brief outages (which was an improvement over the old system's reliability.)\n\n\nI think the approach that works will be unique to your situation, for us we had a lot of data in the old system that was timestamped fairly well, and we had a very clear design for how the new system was going to model this data. I don't hear about people doing incremental migrations like this much, but it turned out to be the entire backbone of our entire approach.",
          "score": 11,
          "created_utc": "2026-01-10 10:53:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyro81t",
              "author": "Few_Wallaby_9128",
              "text": "This is a very i teresrimf approach, i can see rhe events processing beinf built and tested incrementally and endimg up in quite a solid solurion, i guess so long as business logic is clear and there are not endless edge cases.",
              "score": 1,
              "created_utc": "2026-01-10 11:04:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrs5hc",
                  "author": "Isogash",
                  "text": "There were certainly near endless edge cases, and one of the important calls we made was that we did not need to make all of the data perfect, we had \"unkown\" event types where a change was made that we didn't really know what to do with, and we would just copy down the new data.\n\nEspecially when you have long-running processes and some of them are semi-manual, you can't expect the processes to have been consistent, or even correct, you just have to treat the data as truth and accept it even if you know some of it is probably wrong. (Still, it's worth reporting/marking that some of the data is clearly wrong or in a very poor state.)\n\nAt a large enough size and with enough exposure to the real world, all systems accumulate incorrect data, and whether or not it's worth the effort to find and correct it is a business decision based on cost-benefit, not just an engineering one.",
                  "score": 1,
                  "created_utc": "2026-01-10 11:39:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyrek6h",
          "author": "More-Ad-7243",
          "text": "First point: pushing for a rewrite without a plan or options of approaches is a bit risky; how can success be reasonably measured and whatever level of management have confidence and trust that you and your team will deliver positive business impact and value?\n\nDetail, with the team, what the current pain points are with this system.  Are these technical, business  process, time to deliver a feature, some other dimension unique to your organisation?\nAre the processes well understood?  Take the opportunity to reach out to relevant business units to ensure domain logic and rules are captured. \nCharacterisation tests will help with that.  \nI've not read it, but I think 'Working Effectively with Legacy Code' maybe a suitable resource.\n\nI would aim to minimise disruption to production operations and as such want to replace in-situ with the ability to switch things on/off (release toggles).\nHow does the system behave in, and interact with, the wider system of software - is it still reasonable to remain a monolith?  Is a different style more appropriate and why so?  I'm not advocating  microservices!\n\nBest of luck!  Rather I should say planning and executing.",
          "score": 15,
          "created_utc": "2026-01-10 09:36:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysvt8v",
          "author": "PmMeCuteDogsThanks",
          "text": "Why did you fight for a rewrite without having idea of how should be done? Any problem will be a blame on your part from here on.\n\nAnd let me guess. The new solution should be micro services! Event driven! Nosql database! Cloud! Ai! Async! Distributed teams!",
          "score": 9,
          "created_utc": "2026-01-10 15:47:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrw4g3",
          "author": "yopla",
          "text": "Do not. Write tests first. Map out the current architecture and the target architecture. Identify modules, identify what can be isolated or externalized then slowly refactor the current application into the new target progressively keeping the system in a working state.",
          "score": 4,
          "created_utc": "2026-01-10 12:12:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nys8kiz",
          "author": "UnreasonableEconomy",
          "text": ">  Spaghetti code everywhere. Zero tests. Half the team refuses to touch certain files.\n\nWhat are the objectives for the new system? \n\n- \"no spaghetti\"? \n- \"more tests\"? \n- \"files must not be yucky\"?\n\nHow do you track these NFRs? How would you even articulate them?\n\nIn the VC world we would call this a tarpit proposition...\n\nNote, I'm not saying a rewrite is never warranted - but typically you'd lead with the warranted objective if you have it...",
          "score": 3,
          "created_utc": "2026-01-10 13:39:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyroora",
          "author": "BaronOfTheVoid",
          "text": "Total rewrites tend to be just as broken as the original.\n\nIf you find some feature that could be isolated try to isolate it and maybe arrive at a bunch of services that work together. Not microservices, just \"normal\" sized ones.",
          "score": 3,
          "created_utc": "2026-01-10 11:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxpr9x",
              "author": "holkerveen",
              "text": "Sound advice! Gradually migrate. Baby steps.\n\nAlso, do you know what got the codebase to this point and what you implement to prevent a repeat? What are the lessons learned?",
              "score": 1,
              "created_utc": "2026-01-11 07:31:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nywtylg",
          "author": "BillBumface",
          "text": "Ah, things you should never do, part one:\n\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/",
          "score": 3,
          "created_utc": "2026-01-11 03:46:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzc14wp",
              "author": "Wesd1n",
              "text": "If the underlying requirements have changed enough then some level of rewrite may be warranted. If the migration itself warrant changing almost everything anyway.\n\n\nBut then I would argue you have a plan you set out to evolve and solve the new system requirements that the old system could never live up to with reasonable changes.Â \n\n\nBut I do find it intimidating to think of any rewrite in the context the article presents and makes me question the thesis of my idea of what a rewrite is.\n\n\n\n\nIn the case of op it seems like they should, as the article states just rework the system.Â \nMaking it not yucky. So people will touch it again.Â \nSince the underlying requirements haven't changed.",
              "score": 1,
              "created_utc": "2026-01-13 11:12:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nyrhq7z",
          "author": "flavius-as",
          "text": "Done this in the past to the point that the rest of the organization didn't know what we're doing the whole time.\n\nOf course, nothings was ever broken, no glitches, as we were taking apart parts of legacy and routing the calls (in the load balancer) to the new system.\n\nCherry on top: we didn't have any of original or previous developers of the code, all of us were for less than a year there.\n\nTechnology from 20 years ago.\n\nI could write a book about this, but here are the key points:\n\n* structured logging, it's easier to roll your own than to simply dump into existing systems\n* Identify inputs and outputs of use cases, map dependencies in the data layer (output of one use case serves as input to another use case)\n* potentially remove unused code\n* have a tight monitoring focused on outcomes at the business level; it doesn't matter how the DTO looks, the rate of orders placed matters\n* use tools tactically. Examples: apache nifi to route and potentially duplicate data; SQL COALESCE to select from new system vs old system; views to map the legacy amd new type of data - be ready to remove these once they're not used any more; CDC to feed legacy data into the new database during the transition (per use case) phase - while still having a single source of truth - so employ COALESCE tactically \n\nThat last point is key: you can juggle with confidence the steps while planning ahead, and removing the helper infrastructure or code you don't use any more\n\nThis is largely what I call \"front door Strangler pattern\".\n\nThere is also a \"back door Strangler\" in which you extract code in legacy, which is stateless (for example some calculations, business processes) and design it in a way fit for the new application, but use it for confirmation in the legacy first. This is best for business critical code.\n\nGenerally: plan to write code and glue things together, just to throw them away in 1-2 years.\n\nMake sure that stakeholders know it can take a couple number of years. Was it developed over 20 years? It will take 5-8 years to fully replace.\n\nTechnically at a very high level: it's all about inputs and outputs.\n\nThat is: shape of data (name of fields etc) and flow of data.",
          "score": 5,
          "created_utc": "2026-01-10 10:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrt2zp",
          "author": "Wild-Nail4873",
          "text": "agree with characterization tests. we did a similar migration last year\n\nwrote e2e tests against production behavior using momentic since we could describe flows in english without understanding the spaghetti underneath. gave us a safety net for the rewrite\n\nalso run old and new in parallel with feature flags before cutting over completely. caught so many edge cases that way\n\ngood luck. its stressful but worth it once youre out",
          "score": 2,
          "created_utc": "2026-01-10 11:47:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyskyjs",
          "author": "SeniorIdiot",
          "text": "This is going to hurt. I hope everyone understands the risk and opportunity cost that comes with a rewrite like this.\n\nWhat not to do: Spend a year where stakeholders and architects keep adding more and more new features and technology (second-system effect), and architects writes 200 pages of documentation on how to implement logging. Or worse, chief architect loudly proclaims that they got a $10M budget - again - and blows through it. Then the organization throws all the work away and repeats the same mistake three times. True story, FML.\n\nAnyway, I uploaded a little snippet from an **old** Uncle Bob video about big rewrites: [https://vimeo.com/1086538049](https://vimeo.com/1086538049)",
          "score": 2,
          "created_utc": "2026-01-10 14:50:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz45qkl",
          "author": "wiedereiner",
          "text": "Take the lead then. Choose a strategy, create the plan, help with the execution.\n\n\nMaybe you can split up your existing solution in components. If so choose the simpler ones upfront, create e2e tests and then do a rewrite.\n\nAnd learn from that. Next time think about all this BEFORE convincing someone.",
          "score": 2,
          "created_utc": "2026-01-12 05:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrylvc",
          "author": "vbilopav89",
          "text": "Yes, yes, yeees, do the microservices thing, join the dark side, what could possibly go wrong, am I right lol.Â \n\n\nI feel your pain. Been there. Here is my honest advice: it works, don't touch it. If you want build a completely new in parallel with that one.Â ",
          "score": 2,
          "created_utc": "2026-01-10 12:32:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrh30z",
          "author": "alien3d",
          "text": "What make sense - if back end you can still hold it .If front end , focus this first.",
          "score": 1,
          "created_utc": "2026-01-10 09:59:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrm5pb",
          "author": "CzyDePL",
          "text": "Read Fearless Rails Refactoring",
          "score": 1,
          "created_utc": "2026-01-10 10:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyrsz8v",
          "author": "IIMiCum",
          "text": "been through this twice. do not rewrite without characterization tests first\n\nyou dont need to understand the code to write them. just document current behavior from the outside. input x gives output y. click this get that. you need something to verify the new system matches the old one\n\nits tedious but its the only way to migrate with confidence",
          "score": 1,
          "created_utc": "2026-01-10 11:46:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysmie9",
          "author": "AvoidSpirit",
          "text": "I wonder what leadership gets convinced and ready to invest without even seeing a concept of a plan",
          "score": 1,
          "created_utc": "2026-01-10 14:59:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysos9h",
          "author": "not-halsey",
          "text": "Do you have a good CI/CD pipeline in place, with a staging area for manual testing? Iâ€™d start there first, and get some standard procedures in place if there isnâ€™t already",
          "score": 1,
          "created_utc": "2026-01-10 15:11:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysw260",
          "author": "_Trio13_",
          "text": "Two years planning the rewrite might have been more practical. You really need understanding and sign-off on the process, not just the end point.\n\nYou can still do that, though. Get the complete migration plan figured out. Test approaches on throw-away second systems. Argue about it with your team until everyone is convinced. **Then** start working on the code. Spending the time planning and then not going ahead is so much better than getting half way done and realizing it's impossible.",
          "score": 1,
          "created_utc": "2026-01-10 15:48:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyt690n",
          "author": "No-Project-3002",
          "text": "Migration become more difficult when you don't have documentation in hand, we have worked in this sitution where we don't had any documentation no old employee who fill in what was the issues, and before me team had no idea how to migrate which cause new application was performing worst then old application with more spaghetti code with simplified UX.\n\nIt took 4 years to complete migration from old vb6 application to C# .net 8, it is possible to migrate but process can depend multiple factors",
          "score": 1,
          "created_utc": "2026-01-10 16:37:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytvhh5",
          "author": "midasgoldentouch",
          "text": "You spent two years trying to get approval for a rewrite and didnâ€™t decide on a plan as part of that?",
          "score": 1,
          "created_utc": "2026-01-10 18:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyu0nku",
          "author": "LaSweetmia",
          "text": "Whatever you and your team does: now after green light you need one lead for the assessment and this person should get free analysis reign for a considerable amount of time while everyone else stays focused on the current Roadmap.\n\nOtherwise you have too many people interfering with an already complex situation. Strategies should emerge naturally after heads cool off and the excitement vanes. Then comes a lot of Analysis that you verify with a test pilot that must have to be small enough that one person alone can do it.\n\nThe ln eventually the team can evaluate and probably throw the pilot away. Then you try again and then you will do the second slightly larger pilot and then rinse and repeat.. Eventually you will learn what you have to do. And this process needs to be decoupled from Roadmap development. Godspeed.",
          "score": 1,
          "created_utc": "2026-01-10 19:00:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuckvd",
          "author": "serverhorror",
          "text": "Star creating end-2-end/integration tests.\n\nIf you want to change the language or framework create them with the target language.\n\nBetter yet: Just don't rewrite, start refactoring what you already have and do it in small steps.",
          "score": 1,
          "created_utc": "2026-01-10 19:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyuo8x9",
          "author": "virtualstaticvoid",
          "text": "You definitely need a plan, which you can iterate on, because it's impossible to \"big bang\" a replacement - Galls Law. \n\nI've found the strangler fig pattern is a viable approach. \n\nhttps://en.wikipedia.org/wiki/John_Gall_(author)#Gall's_law\n\nhttps://martinfowler.com/bliki/StranglerFigApplication.html",
          "score": 1,
          "created_utc": "2026-01-10 20:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyvjn4b",
          "author": "zp-87",
          "text": "Step 1: define list of all features\nStep 2: write safety net - tests that test all features\nStep 3: start greenfield as usual, but always test with the safety net",
          "score": 1,
          "created_utc": "2026-01-10 23:36:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxym2c",
          "author": "syscall_cart",
          "text": "it depends on your architecture, how \"extensible\" and how big the legacy system is. If possible, have both systems run in parallel and refactor pieces one by one starting with the most problematic areas. Leave bug free, stable areas last as these aren't the deciding factor for whether the delivery is a success or not. Point is to avoid second-system-syndrom at all cost. [https://medium.com/holes/second-system-effect-1a4a1cf7683](https://medium.com/holes/second-system-effect-1a4a1cf7683)",
          "score": 1,
          "created_utc": "2026-01-11 08:52:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyzrrgc",
          "author": "cto_resources",
          "text": "The answer is in the usage. Look carefully at the list of use cases. You want to peel away a community of users, one at a time. \n\nI had a legacy system written entirely in PHP that I wanted to modernize. We peeled away one user community at a time, starting with the most tolerant: the admins. \n\nThat let us build the basic architecture out. The services. The interfaces. The dependent technologies (in our case, Auth-Auth). With each step, we added as few new features as we could, just adding parallel features then moving that community over to the new UX and components. \n\nBoth *could* run side by side but rarely did. Usually weâ€™d launch a set of features for a community and then move that community over. They could go back to using the legacy interface if they wanted to, but we would ask â€œwhyâ€ because that showed some gap in our delivery.\n\nMake sense?",
          "score": 1,
          "created_utc": "2026-01-11 16:22:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0sbv2",
          "author": "wjrasmussen",
          "text": "This story seems hard to believe.",
          "score": 1,
          "created_utc": "2026-01-11 19:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4y2eb",
          "author": "BigOk3353",
          "text": "Get enterprise version of 'Antigravity' you alone can get it done at less than the budget approved. \n\n\nStart with writing tests using its AI and then move to incremental changes and validation using the same test.",
          "score": 1,
          "created_utc": "2026-01-12 09:54:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz73h38",
          "author": "Critical-Copy-5490",
          "text": "This might be useful - [https://martinfowler.com/articles/patterns-legacy-displacement/](https://martinfowler.com/articles/patterns-legacy-displacement/)",
          "score": 1,
          "created_utc": "2026-01-12 17:33:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7bqrx",
          "author": "LoveThemMegaSeeds",
          "text": "Do it piecemeal. Thereâ€™s a reason why rewrites are such a terrible idea. Donâ€™t risk breaking everything at once. To replace you must first bring up to date or at least have the test harnesses in place to know what your breaking",
          "score": 1,
          "created_utc": "2026-01-12 18:11:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzb4dg8",
          "author": "lisnter",
          "text": "I architected and ran a project that replaced a 30 year old mainframe application with a Java infrastructure in the early 2000â€™s.  The project had been attempted twice before and failed but many of the original programmers were still at the company and retiring so it needed to be replaced. \n\nI designed a system that took input from a new web UI and fed them into both the new business logic and the old green-screen front-end to drive it as though a user had keyed in the commands. Since the web-UI could encapsulate several steps the Java code sent multiple command sequences to the old command processor. Fortunately these were well defined and there was an â€œescape nowâ€ sequence that we could use to start over if things got confused. \n\nThe company was very risk averse so we ran with the old system as the system-of-record for several months checking the output against the new system until we switched and had the new system be be system-of-record for several more months. Eventually the customer was happy with the output and we turned off the feed to the old command processor and they could retire the mainframe. The switch was just a configuration update - no new code was deployed - to minimize change control.",
          "score": 1,
          "created_utc": "2026-01-13 06:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbv4n3",
          "author": "ryan_the_dev",
          "text": "I have seen it fail so many times. I have come onto teams that are 10 years into a 2 year modernization project.",
          "score": 1,
          "created_utc": "2026-01-13 10:18:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdj3o1",
          "author": "czlowiek4888",
          "text": "I did something similar once.\nThe strategy was like this:\n1. Documenting all features that clients use and that must work.\n2. Write integration tests that cover all business paths\nIntegration tests were enclosing application as a blackbox, I mock all apis and integrations, databases states.\nYou need to be able to track everything how your app interacts with anything that is external to the process it runs in.\nEvery single if statement was tested, algorithms were unit tested to avoid testing them using integration tests.\n3. Now you can actually refactor parts of code reliably. Or you write new app and test it using this test suite during development. (You run only few tests, adjust code for it and run next tests etc)",
          "score": 1,
          "created_utc": "2026-01-13 16:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg712x",
          "author": "m39583",
          "text": "How on earth did you get permission to rewrite a core application without any sort or plan or idea of what you were going to do?!Â  And why did you even start pushing for this without any plan!\n\n\nWhat is the scope of the replacement? Must it do exactly everything the old application does? But it sounds like you don't even know what the old application does.\n\n\nOr do you have time and resources commited from your business teams to design the functionality for a greenfield replacement?\n\n\nSuggest reading this: https://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/\n\n\n\nIt's old but still very relevant.\n\n\nWhat language are you writing the new one in?Â  I think the only real argument for a full rewrite is if you need to change platforms because the old one is dead.Â \n\n\nRails seems to be mostly dead, so that might be a chance to rewrite it into something else like Java, Python or C#\n\n\nIf you aren't confident improving your current application, what makes you think your new one will be any better?!",
          "score": 1,
          "created_utc": "2026-01-14 00:07:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkld0h",
              "author": "ahgreen3",
              "text": ">Rails seems to be mostly dead, so that might be a chance to rewrite it into something else like Java, Python or C#\n\nRails isn't dead. I thoroughly dislike Ruby and Rails, but it is no way dead.\n\nI have worked on/lead about a dozen web app modernization efforts over the last 15 years and the 1 thing that will doom the best planned modernization is unnecessarily changing the language. If the language is active and capable, continue with it. If your legacy app was built in COBOL or Fortran then changing to something less specialized, and more web app appropriate MIGHT be a good idea.\n\nYour engineering team currently has a good chunk of understanding of Ruby and will intuitively know horrendous practices/situations by just glancing at a block of source code. Changing languages means your team will now need to learn all these oddities of a new language, taking more time and causing more issues with the modernization effort.\n\nI cannot emphasis this enough because the only 2 modernization efforts I've personally witness fail were fundamentally due to an unnecessary change in programming language (PHP->Rails and Python->Node).",
              "score": 1,
              "created_utc": "2026-01-14 17:18:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzlo60i",
                  "author": "m39583",
                  "text": "My main point was that the only good reason for a full rewrite is if your on a dead end platform, otherwise your much better off incrementally refactoring and improving what you've got.\n\n\n\n\nAt some point if your platform is dead you *are* going to have to change.Â  So maybe not rails but where is the line.Â  What about Perl, Visual Basic, Pascal...?Â  How long do you keep going with an application written on a platform that isn't going anywhere?\n\n\nI would say that PHP to Rails and Python to Node are both pointless and also going in the wrong direction.Â  Especially the latter!",
                  "score": 1,
                  "created_utc": "2026-01-14 20:12:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzkpgdx",
          "author": "ahgreen3",
          "text": "I agree with all the recommendations about tests and understanding inputs/outputs, etc but there is one thing overlooked: coding standards.\n\nYou mentioned no one wants to touch certain files and there's lots of spaghetti code. In my experience, this indicates there was no coding standing (or a very different one to what is currently used). Determining the coding standing for the project and just implementing the basics (ie what your IDE will do automatically) will be a huge help initially. Once all the files just use the same style, you will be able to surgically make changes and, probably more importantly, track the logic changes in github without being distracted about the 400 lines were you switched tabs to spaces.",
          "score": 1,
          "created_utc": "2026-01-14 17:37:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzl4vzc",
          "author": "ImpossibleClub4045",
          "text": "Replicate data, migrate functionality, rinse, repeat until itâ€™s all off",
          "score": 1,
          "created_utc": "2026-01-14 18:45:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmbgw9",
          "author": "Remarkable_Permit304",
          "text": "Read this: https://www.goodreads.com/book/show/54716655-kill-it-with-fire\n\nNot perfect, but itâ€™s a good start.",
          "score": 1,
          "created_utc": "2026-01-14 21:58:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nys3hx7",
          "author": "Jolly-Lie4269",
          "text": "Well thatâ€™s not going to be a complete disaster",
          "score": 1,
          "created_utc": "2026-01-10 13:07:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa83h4",
      "title": "Anyone actually keep initial architecture docs up to date and not abandoned after few months? Ours always rot",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qa83h4/anyone_actually_keep_initial_architecture_docs_up/",
      "author": "Independent-Run-4364",
      "created_utc": "2026-01-11 19:12:46",
      "score": 38,
      "num_comments": 31,
      "upvote_ratio": 0.98,
      "text": "At my current team, we started out with decent arch docs â€œhow the system worksâ€ pages. Then we shipped for a few weeks, priorities changed, a couple of us made small exceptions and now suddenly we don't use the them anymore and they r lost in time.\n\nIf youâ€™ve found a way to keep this from rotting, whatâ€™s the trick? like ADRs that people would actually read ? some sort of PR gate and checklist? or do you just accept it and rely on code review + tribal knowledge?\n\nWould love to hear whatâ€™s worked ! (or what you tried that was a total waste of time)\n\nEDIT: Thanks everyone for your advice !!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qa83h4/anyone_actually_keep_initial_architecture_docs_up/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz0uvpf",
          "author": "sfboots",
          "text": "I update the top level architecture documentation around when we hire a new person.   Then I get the new developer to review and bookmark it\n\nWe are growing slowly so that is every other year",
          "score": 21,
          "created_utc": "2026-01-11 19:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz12d0i",
          "author": "ThigleBeagleMingle",
          "text": "_Do you keep arch docs up day?_ lol thatâ€™s a good one. \n\nAnything not automated or blocking ci/cd ainâ€™t happening.",
          "score": 11,
          "created_utc": "2026-01-11 19:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0x47v",
          "author": "Saki-Sun",
          "text": "1. Replace architectural documentation with simplified on boarding style docs.\n\n\n2. Stick them as close to the code as possible.\n\n\n\n3. Do your jobs and keep them up to date you lazy bastards.",
          "score": 12,
          "created_utc": "2026-01-11 19:31:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1aloc",
          "author": "RGBrewskies",
          "text": "docs are out of date more or less instantly\n\nit's one of the reasons why agile manifesto pushes working code over comprehensive documentation \n\nbut try explaining that to middle management",
          "score": 6,
          "created_utc": "2026-01-11 20:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0wvcn",
          "author": "DeathByWater",
          "text": "The rot hasn't set in at my current posting. Potential reasons:\n\n\n- Modeling in C4 (simple), and only down two top levels of hierarchy; leaving the details out because:\n- Details change frequently and are hard to maintain, but the source of truth for the details is the IaC, not the diagrams\n- Getting the team involved in planning frequently means they have to be up to date as an effective piece of communicationÂ ",
          "score": 8,
          "created_utc": "2026-01-11 19:30:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz149oy",
              "author": "Rednavoguh",
              "text": "So even the top level docs are modeled in C4? Does that serve your needs? Here we are comtemplating building up a new capability model but I'm very curious to hear from you if C4 can provide the same information",
              "score": 1,
              "created_utc": "2026-01-11 20:03:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz1by5m",
                  "author": "DeathByWater",
                  "text": "Yeah, the highest level arch doc is the C4 one. It's fine. Maybe a couple of caveats:\n\n\n- It's a startup and we've only been going about 18 months, so not a lot of time for cruft to build up\n- Very limited number of services yet; nothing like the sprawl you get at an enterprise after 20 years\n- I'm deliberately sticking to pretty simple architecture; REST APIs, pubsub, and queues from which event messages are consumed.\n\n\nThat said, I've worked with a very large media company that worked with more much more complex sets of services and architecture, and it seemed to work for them too.\n\n\nI just use a miro board with links to different frames, but they used LeanIX to keep on top of it.\n\n\nThere are of course confluence docs and READMEs etc that describe more of the detail; but pictures have higher information density than words for the broad strokes.",
                  "score": 1,
                  "created_utc": "2026-01-11 20:39:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz0v981",
          "author": "caprica71",
          "text": "They usually rot \n\nArchitect KPIs are usually about how many projects they worked on. Not about keeping things up to date after the project is over.\n\nI have lost track of how many current states I have had to rebuild from scratch.   Sometimes you get lucky and a operational team will have great docs, but mostly they are a shambles",
          "score": 3,
          "created_utc": "2026-01-11 19:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0vurq",
          "author": "Masked_Solopreneur",
          "text": "When you say initial, it sounds like you are going from greenfield to continious dev/ops. I find the need for suchs docs lower in continious dev/ops than in the beginning of a project.Â \nThink about what value you want from the docs or if you really just seek to have them properly maintained.Â \nIf the value is clear for you, you need to communicate it to your team so you can engage in maintenance. Principles, like Did, can be helpfull here.Â \nKeeping docs close to code (mermaid etc.) I find helpfull for software architecture. A central codebase for broader context can also work.\nI have also had success with using tracing as the source of truth for integrations.Â ",
          "score": 2,
          "created_utc": "2026-01-11 19:26:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1b1d3",
          "author": "virtualstaticvoid",
          "text": "The \"trick\" I think is in the team culture and values - if the team wants to keep documentation up to date, then it will be so.\n\nIn my experience, that's rarely the case, unless their is a business driver or the project is open source. \n\nI've found that including documentation _together_ with each ADR, not only provides an event log of the evolution of the system, but also defines the context for the documentation, so it remains relevant and thus doesn't need to be updated as such. If the architecture changes, you add a new ADR and new documentation.",
          "score": 2,
          "created_utc": "2026-01-11 20:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1b3yh",
          "author": "anotherchrisbaker",
          "text": "Someone needs to own them. Maybe lead or EM?",
          "score": 2,
          "created_utc": "2026-01-11 20:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vw37",
          "author": "felipasset",
          "text": "Automate architecture documention generation and automate architecture rules checking.",
          "score": 2,
          "created_utc": "2026-01-11 22:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3h3aj",
          "author": "arihoenig",
          "text": "Ours are petrified at the moment of sign-off. They are nothing more than a mechanism to get buy in from stakeholders and once that Rubicon is crossed, they become fossils.\n\nThey do serve a secondary purpose in that the process of creating the document helps clarify the design in the architects mind.\n\nIt isn't a big deal because we can just get copilot to document the architecture from the current source and that is way better anyway.",
          "score": 2,
          "created_utc": "2026-01-12 03:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4ue5x",
          "author": "JosephineRoberts_",
          "text": "The only way Iâ€™ve seen docs not rot is making them part of the change, not homework after. We keep a tiny set of â€œlivingâ€ docs in the repo (high-level diagram, invariants, contracts) and any PR that breaks one has to update it, same as tests. Everything else is allowed to be historical junk and we donâ€™t pretend itâ€™s current.",
          "score": 2,
          "created_utc": "2026-01-12 09:18:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz67oup",
          "author": "symbiat0",
          "text": "You could make the doc updates mandatory to pass a PR... ðŸ˜",
          "score": 2,
          "created_utc": "2026-01-12 15:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz12zwl",
          "author": "ziksy9",
          "text": "This is one place where Agentic workflows really shine.  I have a plethora of decision docs, feature specs, planned features, architecture designs, analyzses, security reviews, etc.\n\nI'm trying to get better at every major milestone to update these docs by telling AI to identify any misconceptions, architectural alignment, missing features, and missing information. Believe it or not, it works quite well to ensure things are aligned, the readme is up to date, any make file changes are documented. Once files haven't been modified for a long while they are moved to docs/decisions, docs/archive, etc. anything in the top docs/ layer is a live or continually changing document.\n\nThis also is a force multiplier because you can refer to these docs for context when prompt engineering and since it's all in version control, you can see the history as needed.\n\nFor every epic, arch decision, or proposed feature plan, a document is added along with local backlog.md tasks also stored in the repo for reference. This has been a game changer since everything you need to know, work on, and what the current code status is, is contained right in the local repo along with detailed work history with document references.",
          "score": 3,
          "created_utc": "2026-01-11 19:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1vss7",
              "author": "fartnugges",
              "text": "Could you share a bit more about how you do this? How does AI identify missing information, features, etc? Like, do you have a back and forth chat with it, or have it review your code base?",
              "score": 2,
              "created_utc": "2026-01-11 22:11:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz2a07g",
                  "author": "ziksy9",
                  "text": "Plan: I give the agent the readme, where the docs are located, what my intentions are, and what personas it should use (golang architect etc which I have agents set up for). I have it build context for the task at the top level without deep diving.\n\nCollect: I ask the AI to run multiple agents with this context and identify features that are not documented, documentation that is out of date with the code base, etc.  I have a backlog.md MCP running, and have it create tickets to update documentation with code and doc references.\n\nExecute: I review and approve all the tickets created and have a specific agent use that context to do a deep dive and either accept the results, or have a conversation about the focused task until I'm happy with the result. Having local tickets helps if the context or session is lost/corrupt.\n\nReview: for each task I have it complete, I manually review, close the ticket and commit, then move to the next.\n\nThis works even better when you have swagger docs built in to your APIs, unit tests, and acceptance tests that can be run and referenced for even more context.\n\nThe biggest thing is providing enough context around a specific task without having the world in a single session to provide focus.\n\nFor new features:\nI generally use claude Opus for the harder things, and  Sonnet for information gathering.  I will also tell Claude to use multiple personas to review architecture decisions, provide alternatives, and come to a consensus between multiple personas. Some are architecture experts, some are business, some security, some are cloud experts.  Then I converse to fill any gaps or other alternatives and have it write out an implementation plan and once happy, create backlog tickets for the new feature. Rinse, repeat.\n\nAt checkpoints I will do a documentation scub, a security audit, test coverage check, lint, etc to keep project health acceptable and limit any cruft.\n\nFor existing applications. I have had Claude generate all the documentation, mermaid charts (all markdown) for application design and workflows, codebase layout with descriptions, and provide explanations of the purpose of modules and a glossary. I found it's the fastest way to get up to date and a mostly correct understanding of a system before I start deep diving in the codebase.",
                  "score": 4,
                  "created_utc": "2026-01-11 23:22:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1wc95",
          "author": "PaulPhxAz",
          "text": "I find having \"Documents\" is the issue.\n\nYou should have a wiki with living pages that the team has signed-on to steward the knowledgebase.  And they have to be useful and concise.\n\nThat is a big ask, and a big cultural shift for a lot of people.",
          "score": 1,
          "created_utc": "2026-01-11 22:14:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2037x",
          "author": "failsafe-author",
          "text": "I thinks this is hard to do well, and actually would be one of the best use cases for AI. If every PR went through an AI check to ensure the architecture docs are still accurate, that would make it so much easier to keep them in sync.",
          "score": 1,
          "created_utc": "2026-01-11 22:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2x18n",
          "author": "tarwn",
          "text": "Split your docs into 3 types of docs:\n\n1. How to setup the system, run it, run tests, etc.\n2. The map: start at the highest possible level, explain how it works and why, then go one level deeper in every area that matters, do it again\n3. The change: an RFC or ADR when a change is made that describes why the change is made\n\nUpdate #1 when you make major changes, every time someone sets up the system then update it if it's out of date.\n\nUpdate #2 when you make major changes to the system as part of the definition of done. Review it every 4 months for the things that got forgotten. Kill detail levels that never get updated.\n\nUpdate #3 never. They are point in time documents, \"this is why we did that thing 3 years ago\". Now you know why you thought you were doing it and if you're assumptions were wrong so it's easier to rip it out and try the simpler thing you should have done instead.\n\n  \nI would not use AI Agents to write these docs. Generally there isn't enough data made available to them to understand what maters most in your environment. People will already read only about half of what you document, so keep it as focused and as contextual to what matters in your environment as possible. Anything else just waters down the percentage of attention people will actually spend on it. Do consider using AI Agents to review #1 and #2 for gaps or drift as a faster way to detect that updates have been missed.",
          "score": 1,
          "created_utc": "2026-01-12 01:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2xe9b",
          "author": "Admirable_Swim_6856",
          "text": "Every new hire should be updating the docs as they learn the system. otherwise, AI is actually very good at documentation, an automation to update docs on a regular cycle would work.",
          "score": 1,
          "created_utc": "2026-01-12 01:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3089s",
          "author": "alien3d",
          "text": "When we start using jquery - When people start learn react. When we start react , when people bragging vue /selvte.  Your arc doc , can't update as the framework keep updating each time.  But you can update the data flow diagram / business requirement document (brd) or some call as product requirement document(prd)",
          "score": 1,
          "created_utc": "2026-01-12 01:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4hc7d",
          "author": "atika",
          "text": "If you include your docs in the repo, you can make sure in the pull request that they are updated. Otherwise the PR doesnâ€™t get approved.",
          "score": 1,
          "created_utc": "2026-01-12 07:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5trl2",
          "author": "gororuns",
          "text": "IMO it's better to use RFCs and ADRs to record major architectural decisions that were made at that point in time, these don't necessarily need to be updated.",
          "score": 1,
          "created_utc": "2026-01-12 13:52:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz943hx",
          "author": "robogame_dev",
          "text": "Generate your docs using an automatic process that reads them from the code.",
          "score": 1,
          "created_utc": "2026-01-12 23:12:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzaxqtq",
          "author": "themessymiddle",
          "text": "It gets risky when code is changing and the behavior is different than what the rest of the org is referencing. But docs arenâ€™t fun to make, automating the architecture docs as code changes definitely helps",
          "score": 1,
          "created_utc": "2026-01-13 05:20:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0buq1",
      "title": "Did i do any good? Trying to graph and understand DDD using Clean Architecture book as source.",
      "subreddit": "softwarearchitecture",
      "url": "https://i.redd.it/8vjt3bgz6jag1.jpeg",
      "author": "Smileynator",
      "created_utc": "2025-12-31 12:26:12",
      "score": 37,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q0buq1/did_i_do_any_good_trying_to_graph_and_understand/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "nwwmsej",
          "author": "heavy-minium",
          "text": "I think this mostly addresses the later part that comes after DDD. First you start with the domain(s) and not the architecture. Then you create bounded contexts, make up the core business rules, define an ubiquitous language, think about how to integrate different bounded contexts (e.g. events, ACLs) and once you start diving into what you've been looking at here, that's where the DDD design phase actually already ended and you are basically \"implementing\" it with a specific software architecture. And that's where ports and adapters (hexagonal architecture) often come in, because you want the domain model independent from frameworks and storage. The often used SAGA pattern is also as a concern of implementation that fullfills DDD constraints, but isn't part of the DDD itself.",
          "score": 16,
          "created_utc": "2025-12-31 12:48:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwsb3i",
              "author": "Smileynator",
              "text": "i might not have all my terminology and segregation thereof down yet then. are there any good sources to recommend for Hexagonal Architecture?",
              "score": 2,
              "created_utc": "2025-12-31 13:26:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwu9sv",
                  "author": "heavy-minium",
                  "text": "Isn't that coming up in the Clean Architecture book? Hmm, there are so many things named like this and similar. The terms are actually quite ambiguous, I just remembered a page where this kind of stuff was elaborated on this way:\n\n>Applications that follow the Dependency Inversion Principle as well as the Domain-Driven Design (DDD) principles tend to arrive at a similar architecture. This architecture has gone by many names over the years. One of the first names was Hexagonal Architecture, followed by Ports-and-Adapters. More recently, it's been cited as theÂ [Onion Architecture](https://jeffreypalermo.com/blog/the-onion-architecture-part-1/)Â orÂ [Clean Architecture](https://blog.cleancoder.com/uncle-bob/2012/08/13/the-clean-architecture.html). The latter name, Clean Architecture, is used as the name for this architecture in this e-book.\n\n>\\[...\\]\n\n>Clean architecture puts the business logic and application model at the center of the application. Instead of having business logic depend on data access or other infrastructure concerns, this dependency is inverted: infrastructure and implementation details depend on the Application Core. This functionality is achieved by defining abstractions, or interfaces, in the Application Core, which are then implemented by types defined in the Infrastructure layer. A common way of visualizing this architecture is to use a series of concentric circles, similar to an onion. Figure 5-7 shows an example of this style of architectural representation.",
                  "score": 3,
                  "created_utc": "2025-12-31 13:38:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwx2zr",
          "author": "RipProfessional3375",
          "text": "Honestly, with things like Clean Architecture and DDD, read them, try to get a feeling for the general principles, and then try to apply them to some real cases and see how it feels.\n\nI am very positive that you cannot grasp a larger architectural principle by reading about it. The reading helps introduce you to the ideas so you can color your real world experience with a new perspective. Anyone trying to memorize the rules of an architectural style without understanding the principles is going to make some pretty terrible stuff.\n\nAlways focus on making things that work, while doing so, learn how some the principles of the architectures you read about could apply here, could have saved you some trouble, etc. Don't try to let a ruleset do the thinking for you, it never works.",
          "score": 9,
          "created_utc": "2025-12-31 13:55:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxequ7",
              "author": "Smileynator",
              "text": "i don't think i am trying too. the whole point of this diagram was to make some sort of mold. not a strict rule set.\n\nthe issue i personally had with reasoning through an idea in any idea of an architecure was \"ok but i don't see how or where i would then resolve X or Y, or if i do, i think the end result is a mess, or poorly maintainable\"\n\nIn my effort to get other team members on board with DDD in general, and clean architecture that is somewhat standardized across the growing company, i wanted to try and use the diagram to reason through use-cases, to reason why X or Y should be resolved at step Z or W.\n\n  \ni am trying to figure out a way to get people along which doesn't boil down to \"read this 300 page book and practice it unquestionably for a few months\" that is never going to fly anyway. then again maybe i am too ambitious for wanting to even try this.",
              "score": 2,
              "created_utc": "2025-12-31 15:32:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx7sq2p",
                  "author": "gbrennon",
                  "text": "I think u can do some practice sessions with ur team members and slowly apply some concepts so they understand it without noticing! \n\nAnd then u just tell them something like \"u just applied the foo principles from the book bar\" ðŸ¤“",
                  "score": 1,
                  "created_utc": "2026-01-02 07:09:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5th3k",
          "author": "Gorgo4",
          "text": "If you want to do it with clean architecture, than I think you are looking for this kind of book\n\nhttps://preview.redd.it/w2xclgdzjtag1.jpeg?width=3024&format=pjpg&auto=webp&s=522dbeeed34bf5a9e2c7376a5d09907f021a4b2c\n\nI get the feeling, that you might have mixed DDD with clean architecture a bit. When I tried something similar (convincing a customer that is), I have benefited from turning this around. There was no Point for me convincing anyone. It was more beneficial to explore why DDD might be helpful with what we suffer with (measurable). So first, I had to explore what I struggle with and if DDD might be worth it over other solutions that are faster, easier, cheaper or more accepted by my fellow engineers. Like writing an ADR and challenge the options by your Team. This was a more open aproach than trying to convince someone (which was my first Impuls als well).\n\nIt also helped me to understand the difference between strategical and tactical DDD to see why clean architecture is just ohne way  to do domain centric implemtation. The DDD-crew repo might be a good Point to picture this. See: [https://github.com/ddd-crew/ddd-starter-modelling-process](https://github.com/ddd-crew/ddd-starter-modelling-process)\n\nWhile clean architecture is good to close the architecture Code gap, it is not always beneficial. I guess you know most of this, as you have mentioned with the silver bullet. But this Point did only driven into my brain once I really started working with it. But I still prefer clean architecture whenever it is reasonable. I simply have a new measure for what is reasonable.\n\nI recoment \"learing Domain driven Design\" from vlad khononov to learn DDD itself. Or rather, to visit a good course to discuss this with others who try to lear this and a teacher who is stuck with you.",
          "score": 2,
          "created_utc": "2026-01-01 23:25:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7sw2u",
              "author": "gbrennon",
              "text": "I love this book because its about how to do and what to do!",
              "score": 2,
              "created_utc": "2026-01-02 07:10:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6ramq",
          "author": "gbrennon",
          "text": "domain shouldnt call any other layer.\n\nit should be like the kernel of ur software.\n\napplication layer should call domain layer and delegate logic to infrastructure layer through ports.\n\npresentation layer should manage the calls through presenters.\n\nthis layer can contains a composition root that wires the implementations",
          "score": 1,
          "created_utc": "2026-01-02 02:44:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7q82f",
              "author": "bigkahuna1uk",
              "text": "Iâ€™m donâ€™t quite follow. The domain can be driven or or it can drive other domains through the use of ports  that are part of its domain. The implementation of the ports, the adapters, allow the domain to call the other layers through ports. So the domain layer can interact indirectly with other layers. \n\nYou describe a driver scenario which invokes the domain. But the converse can also be true. The domain can also drive. For instance the domain wishing to persist data to a repository or sending out a notification is an action driven from the domain. Itâ€™s instigated from the domain itself.",
              "score": 1,
              "created_utc": "2026-01-02 06:47:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7rrc8",
                  "author": "gbrennon",
                  "text": "I undestood u bro ðŸ¤“\n\nI love ports and adapters from alistair cockburn!\n\nremember that if the domain is driving or been driven  it defines the ports so it can be driven, right?\n\nIf he uses some port for composing an implementatio it is doing this so that who is outside interact with it, right? \n\nPorts are a way to delegate logic to someone else and to focus on the business or application logic!",
                  "score": 1,
                  "created_utc": "2026-01-02 07:00:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q19kce",
      "title": "How do large hotel metasearch platforms (like Booking or Expedia) handle sorting, filtering, and pricing caches at scale?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q19kce/how_do_large_hotel_metasearch_platforms_like/",
      "author": "Sweaty_Ingenuity_824",
      "created_utc": "2026-01-01 17:26:33",
      "score": 36,
      "num_comments": 34,
      "upvote_ratio": 0.89,
      "text": "Iâ€™m building a unified hotel search API that aggregates inventory from multiple suppliers (TBO, Hotelbeds, etc.). Users search by city, dates, and room configuration, and I return a list of hotels with prices, similar to Google Hotels or Booking.\n\nI currently have around 3 million hotels stored in PostgreSQL with full static metadata (name, city, star rating, facilities, coordinates, and so on). Pricing, however, is fully dynamic and only comes from external supplier APIs. I canâ€™t know the price until I call the supplier with specific dates and occupancy.\n\n**Goal**\n\n* Expose a fast, stateless, paginated `/search` endpoint.\n* Support sorting (price, rating) and filtering (stars, facilities).\n* Minimize real-time supplier calls, since they are slow, rate-limited, and expensive.\n\n**Core problem**  \nIf I only fetch real-time prices for, say, 20 hotels per page, how do I accurately sort or filter the full result set? For example, â€œshow the cheapest hotel among 10,000 hotels in Dubai.â€  \nCalling suppliers for all hotels on every search is not feasible due to cost, latency, and reliability.\n\n**Current ideas**\n\n1. Cache prices per hotel, date, and occupancy in Redis with a TTL of around 30â€“60 minutes. Use cached or estimated prices in search results, and only call suppliers in real time on the hotel detail page.\n2. Pre-warm caches for popular routes and date ranges (for example, Dubai or Paris for the next month) using background jobs.\n3. Restrict search-time sorting and filtering to whatâ€™s possible with cached or static data:\n   * Sort by cached price.\n   * Filter by stars and facilities.\n   * Avoid filters that require real-time data, such as free cancellation.\n\n**Questions**\n\n1. How do large platforms like Booking or Expedia actually approach this? Do they rely on cached or estimated prices in search results and only fetch real rates on the detail page?\n2. Whatâ€™s a reasonable caching strategy for highly dynamic pricing?\n   * Typical TTLs?\n   * How do you handle volatility or last-minute price changes?\n   * Is ML-based price prediction commonly used when the cache is stale?\n3. How is sorting implemented without pricing every hotel? Is it common to price a larger subset (for example, the top 500â€“1,000 hotels) and sort only within that set?\n4. Any advice on data modeling? Should cached prices live in Redis only, PostgreSQL, or a dedicated pricing service?\n5. What common pitfalls should I watch out for, especially around stale prices and user trust?\n\n**Stack**\n\n* NestJS with TypeScript\n* PostgreSQL (PostGIS for location queries)\n* Redis for caching\n* Multiple external supplier APIs, called asynchronously\n\nIâ€™ve read a lot about metasearch architectures at a high level, but I havenâ€™t found concrete details on how large systems handle pricing and sorting together at scale. Insights from anyone who has worked on travel or large-scale e-commerce search would be really appreciated.\n\nThanks.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q19kce/how_do_large_hotel_metasearch_platforms_like/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nx3ynjw",
          "author": "Slow-Entertainment20",
          "text": "My assumption is that when people are searching for prices is based on a city, so geographic location with a specific geospatial DB matters. There might be 10k hotels in Dubai but in a specific city thereâ€™s probably 200 or less. Which breaks the problem down drastically.",
          "score": 9,
          "created_utc": "2026-01-01 17:42:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4117m",
              "author": "Sweaty_Ingenuity_824",
              "text": "Youâ€™re right that search is geo-based, but in practice the number can still be very large. Many major cities have thousands of properties once you include hotels, serviced apartments, and other types of stays. For example, Dubai has 8,000+ properties and Paris has 5,000+, so even within a single city the search space is often much bigger than a few hundred hotels. Thatâ€™s why pricing, caching, and ranking still need to work at large scale even with geographic filtering.",
              "score": 1,
              "created_utc": "2026-01-01 17:54:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5h6mg",
                  "author": "mnp",
                  "text": "Sorting a collection of 1e3 items should only take milliseconds. Is that really the problem here?",
                  "score": 7,
                  "created_utc": "2026-01-01 22:18:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx6ejnl",
                  "author": "BlackHolesAreHungry",
                  "text": "You can sort an array of a million integers in memory easily. You need a big db only if you have more than a million rows.",
                  "score": 2,
                  "created_utc": "2026-01-02 01:26:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx66n5o",
          "author": "lighthouserecipes",
          "text": "I was technical co-founder of an accomodations metasearch that included both hotels and Airbnb.Â  It is now defunct but we solved this by a similar stack to what you are thinking of.\n\n\nNodejs, postgresql with postgis, redis.\n\n\nOur data sources were a mix of real time APIs, scraped calendars off Airbnb, real-time scraping of some sites, ftp of daily inventory and price feeds, etcÂ \n\n\nWe went for max accuracy by hitting every API every time, with a little caching here and there. We would start rendering results instantly and send the prices to the browser as they arrived. The server handled pagination and filter range calculation and we only needed to send the browser a couple pages, but during the first 10 seconds those pages would churn a lot. The full result set was several MB for a big city, but that would just sit in memory for some minutes after the session and then get dumped.",
          "score": 6,
          "created_utc": "2026-01-02 00:39:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx48amm",
          "author": "Qinistral",
          "text": "Why wouldnâ€™t you price every hotel? You canâ€™t sort by price if you donâ€™t price them all.\n\nFor architecture You need to use a search engine like ElasticSearch.\n\nYou keep your source of truth in Postgres, which you update at a regular cadence, then you update your search engine from that. \n\nIf you need to update your prices from 3P more frequently then you can do something like query on demand when a user queries and gets a page of results you can trigger an async query to refresh that data if itâ€™s not been refreshed in say an hour. (So most active hotels have most fresh data).\n\n1. The secret here is that customers are NOT always seeing the best most accurate globally true answer to their query. This is just a secret reality. Product design has to make hard decisions like this and account for it.\n\n2. I donâ€™t know what the landscape looks like in this industry but there are probably change FEEDs that brokers connect to. So instead of repeated polling and caching, they just listen to events that notify of price changes. This gives them near real time data.\n\n3. Brokers set their own prices. Brokers have a lot of data, so they can analyze trends and triage the market. They can buy hotel rooms with bulk discounts and then mark them up themselves, since they are setting the price, there is no consistency concerns.",
          "score": 9,
          "created_utc": "2026-01-01 18:29:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6ea7l",
              "author": "BlackHolesAreHungry",
              "text": "Why elasticsearch? Postgres table per city or a Redis cache will do the trick",
              "score": 3,
              "created_utc": "2026-01-02 01:24:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx7jwrv",
                  "author": "ccb621",
                  "text": "Why would you create one table per city? There are anywhere from 500 to 4 million cities in the world depending on your definition of the term.Â ",
                  "score": 5,
                  "created_utc": "2026-01-02 05:55:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx7pajq",
                  "author": "Sweaty_Ingenuity_824",
                  "text": "I agree with the separation-of-concerns argument. Postgres is my source of truth, but once you mix geo, filters, sorting, scoring, and pagination at scale, a search engine becomes more practical. Postgres *can* do a lot of this (especially with PostGIS), but once ranking and re-ranking logic grows, pushing that workload into a search index feels safer long-term.",
                  "score": 3,
                  "created_utc": "2026-01-02 06:39:39",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nx6ighm",
                  "author": "Qinistral",
                  "text": "1. **Features**: I Have heard that PG has gotten more and more 'search' extensions, but I don't know how complete it is, whereas ES is basically functionally complete. Full text search, filtering on any combination of fields, sorting on many combinations of fields, relevant scoring, etc; these cannot use normal btree style indexes. Search engines are build from the ground up to support these queries, along with being very easy to scale horizontally.\n\n2. **Separation of concerns**:  Good architecture is about risk mitigation, organizational alignment, and enablement for future features.  Having a separate search engine supports all of these values.  Granted OP sounds like they're building alone in their basement so they may want to optimize for 'fast to build' over 'good' and that's fine. But for public guidance I'm going going to optimize my comments for 'good' :).",
                  "score": 4,
                  "created_utc": "2026-01-02 01:50:33",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny0y14d",
                  "author": "saravanasai1412",
                  "text": "Elasticsearch / Alogila / Typesense is build for these searching capabilities. Postgres can't handle user typo & perform semantic search in sub ms latency at very large dataset. Its come with cost but worth than implementing our own search algorithms.",
                  "score": 1,
                  "created_utc": "2026-01-06 15:57:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx7p4ud",
              "author": "Sweaty_Ingenuity_824",
              "text": "Pricing every hotel sounds correct in theory, but at 3M+ properties it isnâ€™t feasible in practice due to supplier rate limits, latency, and cost. Even within one city, pricing thousands of hotels for every date/occupancy combination explodes quickly. Thatâ€™s why Iâ€™m exploring partial pricing + cached/derived prices for ranking, with real pricing happening on detail or pre-booking. I agree with your point though: users are not always seeing a globally perfect answer, and product has to accept that trade-off.",
              "score": 1,
              "created_utc": "2026-01-02 06:38:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx88j7t",
          "author": "newleafturned2024",
          "text": "I used to work in this space. A couple of things in no particular order:-\n\n1. Booking and Expedia are not metasearch, they're OTAs. Metasearch refers to services like Google Hotels and Kayak where they aggregate OTA results.\n\n2. Typically what happens is between search and reservation, there's an extra step that does live search. Some call  it availability check, it has all sorts of names - there's no standard here. This allows you to have slightly more control over the TTL while balancing between supplier rate limits and info accuracy.\n\n3. Back to caching search results: you pretty much figured it out already but unfortunately I don't believe there's a perfect solution. You'll have to find a sweet spot that works for you in terms of supplier rate limits, cost, and accuracy. \n\n4. My suggestion is specialize in a niche and limit the number of hotels. For TTL, I think we used to set it to a constant number like x minutes. In general, prices for stays far in the future tend to fluctuate less. But it's not a law! They also get less searches. So if you set a higher TTL for these searches, you might get higher hit but sacrifice cache utilization - which is important if you want to optimize for cost - you don't want Redis to store data no one will access!\n\n5. Pre-warming cache - only works if your feed is small and your search traffic has low entropy (all of them search for the same hotel/location). Otherwise you'll be wasting supplier API rate limits on artificial queries.\n\nI no longer work in this space but I'm still following travel tech from time to time. Hit me up if you want some free advice that may or may not be relevant.",
          "score": 2,
          "created_utc": "2026-01-02 09:38:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx51z7l",
          "author": "maxufimo",
          "text": "If I remember correctly, the large providers like Expedia and Booking are maintaining their own inventory, most channel managers will be pushing the current price to them.\n\n\nOne pattern I see with smaller aggregators is to pull the pricing data periodically, since bulk operations are usually cheaper, and caching this for searching and sorting. Then they refresh the pricing data for specific booking dates and occupancy when displaying the details.",
          "score": 1,
          "created_utc": "2026-01-01 20:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7puc2",
              "author": "Sweaty_Ingenuity_824",
              "text": "Yes, that matches what Iâ€™ve seen as well. Large players get pushed updates or bulk feeds and can maintain their own inventory, while smaller aggregators usually rely on periodic bulk pulls and caching for search and sorting. Then they re-fetch or refresh prices for the exact dates and occupancy on the detail or booking step. My challenge is figuring out how to model and cache those bulk prices in a way that still works for sorting when search parameters vary a lot.",
              "score": 1,
              "created_utc": "2026-01-02 06:44:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5w4ev",
          "author": "sansp00",
          "text": "Used to work in the field a couple of years ago. At that time, companies like Expedia would get a daily dump of the inventory+price to build their catalog. They used to have only  a subset of the pricing (1/2/4 pax) which are the most common. At the time , the look to book ratio was around 3%, so when customers would move forward, the real call to the tour operator would be done and the price would be updated to reflect the 'true' value.",
          "score": 1,
          "created_utc": "2026-01-01 23:40:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7q6pm",
              "author": "Sweaty_Ingenuity_824",
              "text": "Yes, thatâ€™s exactly the direction I want to take as well avoid calling providers until the user is actually moving toward booking. Periodic dumps for a subset (like 1/2/4 pax) make sense and I can do that for top 200â€“500 hotels in a city.\n\nThe open question for me is the long tail. In a city like Dubai, that still leaves 6â€“7k less popular hotels. I canâ€™t realistically dump or keep prices updated for all of them, but I still need some price signal for search and sorting. Thatâ€™s the part Iâ€™m trying to figure out: how to represent pricing for those long-tail properties without polling every hotel or overloading suppliers.",
              "score": 1,
              "created_utc": "2026-01-02 06:47:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx9jnm6",
                  "author": "sansp00",
                  "text": "Do you have 'deals' brokered with them ? Back in the day, we needed deals to resell and most of them included the B2B API's and inventory catalog (not all sites get the same inventory). The block of inventory you are allowed to sell can in some cases not even be hosted with the provider. For long tail stuff, the information is sometimes entered manually within the system and you don't have any API and or dump. Happened a couple of times on previous trips where I had a confirmed booking with an online platform only to realize at the hotel that the room was already booked (due to no real time B2B integration)",
                  "score": 1,
                  "created_utc": "2026-01-02 15:17:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6vnwm",
          "author": "bert1589",
          "text": "I worked in this space years ago. Most of those suppliers just have negotiated rates and allotments that they knew the pricing for based on all the variables.",
          "score": 1,
          "created_utc": "2026-01-02 03:11:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7qhg6",
              "author": "Sweaty_Ingenuity_824",
              "text": "That makes sense for large suppliers or OTAs that directly negotiate rates and control allotments. In my case, the product is a unified hotel API for smaller OTAs. They donâ€™t want to integrate 3â€“4 different suppliers, manage separate integrations, handle hotel mapping, or maintain static hotel data themselves. The goal is to abstract all of that behind a single API, so they can access inventory and pricing from multiple providers without dealing with individual supplier integrations or maintaining their own hotel database.",
              "score": 1,
              "created_utc": "2026-01-02 06:49:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxdj7b0",
          "author": "Happyman501",
          "text": "Have couple of questions ,\n1 . Do  hotel prices vary daily .\n2. Do they have base prices set . \n\nDepending on the answers it would be combination of elastic search and redis . Redis would be the main guy here, since it would hold prices with a expiry date etc and fast lookups . Plus event driven architecture either a queue and lamda or  function updating redis keys  and other things like notifyjng customers etc.\n\nAlso redis has geo search and geo columns , i gues postgress and comos also have them but redis is faster . You can narrow your searvh and have a radius to search",
          "score": 1,
          "created_utc": "2026-01-03 03:29:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdy578",
          "author": "Blakeacheson",
          "text": "What if you sampled each hotel (continually) to establish a general cost profile â€¦. And then used this data to perform a full fetch in realtime of the top 10 cheapest hotels?",
          "score": 1,
          "created_utc": "2026-01-03 05:06:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx44zoe",
          "author": "mark_tyler",
          "text": "Look at ElasticSearch or something similar",
          "score": 2,
          "created_utc": "2026-01-01 18:13:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7lfns",
          "author": "No_Flan4401",
          "text": "I don't see the problem with only 3 million rows? Slap good index on it and run the queries?Â \n\n\nI admit I only skimmed your post but what problem do you experience at the moment, what are we trying to solve here?",
          "score": 0,
          "created_utc": "2026-01-02 06:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7r0ra",
              "author": "Sweaty_Ingenuity_824",
              "text": "The issue isnâ€™t the 3 million rows or querying them static data is easy to handle with proper indexing. The real problem is pricing.\n\nPrices donâ€™t live in my database; they come from supplier APIs, and fetching them requires external calls. I donâ€™t want to call suppliers for every hotel on every search. Thatâ€™s why Iâ€™m looking to design a pricing cache/pool that is refreshed periodically.\n\nThe difficulty is that a single hotel doesnâ€™t have one price. Pricing depends on multiple variables like check-in date, length of stay, rooms, and occupancy. So the question isnâ€™t â€œhow to query 3M hotels,â€ but â€œhow to model and cache pricing signalsâ€ in a way that:\n\n* Avoids calling suppliers for every search\n* Handles many pricing permutations reasonably\n* Allows approximate pricing for low-visibility hotels (without supplier calls) so sorting still works\n* Uses real supplier calls only for popular properties or when the user moves toward booking\n\nThatâ€™s the core problem Iâ€™m trying to solve.",
              "score": 1,
              "created_utc": "2026-01-02 06:54:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx87wuf",
                  "author": "No_Flan4401",
                  "text": "Thanks for clarifying.Â \n\n\nSo the problem is you don't have access to the data or am I still misreading? Is this a personal project or a commercial?Â \nI don't know the hotel and pricing space, but I'm guessing the other providers use either a common pricing api/brooker where hotels deliver informations or they make individual agreements with hotels and calculate price based on some different rules?\nIf you need this data for commercial use you need to consider the legal stuff, if it's ok to show the data on your site. I would investigate how the other providers get their data.\n\n\nI'm not sure if it's possible but perhaps you need to limit the scope to a start. So you users can only get prices for x variables u support? Perhaps you can combine the cache with some scraping of the individual sites, but again it depends on how non static the pricing data is.Â \n\n\nI totally get you think this is hard but perhaps a possibility is your application don't make sense when you don't own the pricing data?",
                  "score": 1,
                  "created_utc": "2026-01-02 09:32:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx46bmb",
          "author": "GrogRedLub4242",
          "text": "I've worked for 2 of the major travel ecommerce shops that did exactly that as a senior engineer/architect. If you have budget for professional consultation you're welcome to DM me.",
          "score": -13,
          "created_utc": "2026-01-01 18:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4s64j",
          "author": "dashingThroughSnow12",
          "text": "By not spamming a bunch of subreddits with this question.",
          "score": -6,
          "created_utc": "2026-01-01 20:08:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q23osh",
      "title": "How do you keep architecture decisions lightweight without losing context over time?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q23osh/how_do_you_keep_architecture_decisions/",
      "author": "manwhoos",
      "created_utc": "2026-01-02 16:50:20",
      "score": 36,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m trying to find a balance between keeping architecture decisions flexible and still documenting enough context so things donâ€™t get lost after a few months. Full-blown documentation tends to age quickly, but when nothing is written down, future developers have no idea why certain choices were made. ADRs seem useful, but in many teams they either feel too formal or eventually get ignored.  \n  \nIâ€™ve seen this general trade-off mentioned in some high-level materials published by [Sumatoâ¤Soft for mobile app development](https://sumatosoft.com/blog/ai-powered-iot-overview), but Iâ€™m more interested in what actually worâ¤ks in real projects. How do you document decisions just enough without letting the process become a chore?",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q23osh/how_do_you_keep_architecture_decisions/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxa5vn5",
          "author": "joelparkerhenderson",
          "text": "Start with a lightweight architect decision record, not a heavyweight one. See the ADR example templates and teamwork documentation here: [https://github.com/joelparkerhenderson/architecture-decision-record](https://github.com/joelparkerhenderson/architecture-decision-record)\n\nFor SumatoSoft specifically, their process documentation recommends doing ADRs with their vendors. That's a really good idea IMHO because it shows how the vendor thinks about stacks and tradeoffs.",
          "score": 8,
          "created_utc": "2026-01-02 17:03:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxah2s0",
          "author": "Qinistral",
          "text": "Iâ€™ve almost never felt the need for a decision tuned framework. We have 1:3:1 docs, design docs, tickets, pull requests, all with context.\n\nI think understanding how the system works now is more important than why it was decided. If you know how it works now you should be able to determine if it fits the needs going forward.\n\nI donâ€™t spend my time wondering â€œwhy did they pick ActoveMQ 5 years ago,â€ instead I wonder â€œgiven our current and future needs is it worth switching to Kafkaâ€, etc.",
          "score": 6,
          "created_utc": "2026-01-02 17:55:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxb30fq",
              "author": "serverhorror",
              "text": "What's \"1:3:1 docs\"?",
              "score": 4,
              "created_utc": "2026-01-02 19:37:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbegns",
                  "author": "Qinistral",
                  "text": "\"One problem, three options, one recommendation\", and for each option you briefly describe it and list pros/cons. Then you review it with your team. (You can google \"1-3-1 decision making\" and see a lot of articles about it.)\n\nIn the context of OP, these do provide some history of decision making, but that is not their purpose. Their purpose is to make good decisions now, and they are not done for every decision, so it's not a true ledger of decisions over time.",
                  "score": 7,
                  "created_utc": "2026-01-02 20:33:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxewjqq",
              "author": "ArtSpeaker",
              "text": "\\> I think understanding how the system works now is more important than why it was decided.  \n\\> I donâ€™t spend my time wondering â€œwhy did they pick ActoveMQ 5 years ago,â€ instead I wonder â€œgiven our current and future needs is it worth switching to Kafkaâ€, etc.  \n  \nThe whys are important too, just not for everyone. Cause it's going to be a list of GOTCHAS.  Whys like \"this is currently incompatible with the tech we have (and are waiting on the right update)\", and \"our customer's  use case forcing legacy support\", and \"all teams are willing to make the change but there isn't a way to prioritize it this year\"  are all nice to have somewhere to save the future team days/weeks/months of pain rediscovering what we already know now. Not critical, but nice.  \n  \nEstablishing a set of assumptions as the foundation for the API + service choices make BIG cross team changes much easier. But not many folks are in that boat.",
              "score": 2,
              "created_utc": "2026-01-03 09:52:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxaqd2a",
          "author": "SolarNachoes",
          "text": "Wiki meeting transcripts and notes. Promote key decisions to ADRs.\n\nAnything that involves R&D gets its own wiki pages. Final take away goes to ADRs.\n\nAI can summarize a whole bunch of ADRs in one go.",
          "score": 3,
          "created_utc": "2026-01-02 18:38:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxb0wfi",
          "author": "No_Flan4401",
          "text": "As a developer what I need when on boarding a new project is to understand the system at high level. A combination of writing and diagram.works best.\nThen some docs about how the code base is structured, then some docs on design and code conventions. Often some docs about data modeling is helpful.\nFrom here on it will differ what I need, and often the above is enough and I can go read the code.Â ",
          "score": 3,
          "created_utc": "2026-01-02 19:27:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbjyfs",
          "author": "heraldev",
          "text": "My suggestion - religiously keep all meeting notes, photos of the whiteboard, then use AI to create docs based on that. At Meta we just simply used google docs for all projects, and as long as you don't duplicate information, and keep log of everything, the documentation won't go too stale. Meeting notes get more important during the late stages of the project - when you need to update assumptions that didn't work, and this context gets lost, or more like buried in the codebase.\n\nI'm currently building a tool on top of Claude Code to generate diagrams from meeting notes and keep track of the PRDs that come after, lmk if you want to try.",
          "score": 3,
          "created_utc": "2026-01-02 21:00:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxeo1yh",
          "author": "ducki666",
          "text": "I do a lot brown field reviews.\n\nBack then it was a lot work. (And money)\n\nNow, with decent code base and iac, throw an AI on it and let explain. What I have done in weeks in the past takes now 1 day plus review and fine tuning.",
          "score": 2,
          "created_utc": "2026-01-03 08:37:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbdjzd",
          "author": "da8BitKid",
          "text": "AIis good at summarizing. You can't just vibe it, you need to edit, but it's a good place to start.",
          "score": 1,
          "created_utc": "2026-01-02 20:29:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxc69gh",
          "author": "Happy_Breakfast7965",
          "text": "It is a chore, nothing wrong with chores.\n\nSoftware engineering requires discipline. Analysis, thinking, design, implementation, testing, releasing, etc. All require accountability and professional approach. \n\nADR can be pretty lightweight.\n\nHowever, if you want lightweight consuption of the documentation, you need to put efforts into production. If you are trying to make documentation production lightweight, consumption will be heavy.",
          "score": 1,
          "created_utc": "2026-01-02 22:51:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxbsbvv",
          "author": "Lekrii",
          "text": "Separate architecture from engineering, and don't let engineers dominate architecture conversations.",
          "score": -2,
          "created_utc": "2026-01-02 21:40:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxby80u",
              "author": "SpaceGerbil",
              "text": "What?",
              "score": 2,
              "created_utc": "2026-01-02 22:09:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxc5kxv",
                  "author": "Lekrii",
                  "text": "Architecture is high level, and strategic.  Engineering is more detailed.  If you let architecture discussions devolve into engineering, you'll spend all your time talking about details instead of the high level design and strategy.",
                  "score": 0,
                  "created_utc": "2026-01-02 22:47:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q20ike",
      "title": "A lot of edge cases donâ€™t live in code , they live between teams",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q20ike/a_lot_of_edge_cases_dont_live_in_code_they_live/",
      "author": "Suspicious-Case1667",
      "created_utc": "2026-01-02 14:49:53",
      "score": 35,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "Something Iâ€™ve noticed working with complex SaaS products: many of the hardest edge cases arenâ€™t caused by missing validations or bad logic. They come from how different teams interpret and own the system.\n\nProduct defines a rule one way.\nEngineering implements a reasonable version of it.\nBilling assumes something slightly different.\nSupport adds exceptions to keep customers happy.\nFinance looks at outcomes months later.\nEach piece is â€œcorrectâ€ in isolation.\n\nBut when those interpretations stack over time, you end up with workflows that technically work, yet produce unintended long-lived states financial drift, entitlement confusion, or accounts that donâ€™t match policy anymore.\nNo single line of code is wrong.\nNo single team â€œbrokeâ€ anything.\nAnd because nothing crashes or alerts, the issue survives quietly.\n\nThatâ€™s why these edge cases are so hard to fix:\n\nNo clear owner across the full lifecycle\nFixing it might hurt legitimate users\nSupport already has a manual workaround\nThe cost shows up slowly, not catastrophically\nFrom the outside it looks like a weird edge case.\nFrom inside the org, itâ€™s often just organizational gravity.\nThis is also why many of these issues are discoverable purely through the frontend. The UI reflects what the company allows culturally and operationally, not just what the backend enforces.\n\nHave you run into edge cases that werenâ€™t â€œbugsâ€ but also werenâ€™t really intentional?\nHow do your teams decide when something is acceptable behavior vs something to close?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q20ike/a_lot_of_edge_cases_dont_live_in_code_they_live/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nx9vuwx",
          "author": "dashingThroughSnow12",
          "text": "AI generated?",
          "score": 17,
          "created_utc": "2026-01-02 16:16:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9x2pr",
          "author": "Sad_Amphibian_2311",
          "text": "These are IMO the most interesting things to find and be aware of in a system. I found them in every job I had, and they are often a missing piece to many puzzles. Not that any the puzzles could easily be solved, in most cases things were too cemented and the perceived business value of the edge cases was too low to address the problem at a fundamental level. By now my mantra is \"there are no edge cases\" because too often it's not an isolated edge case but a variation which - if properly understood - would lead to a whole new understanding of the non-edge cases as well.",
          "score": 3,
          "created_utc": "2026-01-02 16:22:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxapo5f",
          "author": "TehLittleOne",
          "text": "For me it's always a matter of how big of a problem it is. There's always going to be something but the guiding principle is always based on what the outcome is. If it's a five alarm fire where it will block users from logging in, great, I'm fixing it now, consequences be damned for any other team. If it causes some field to show up wrong in the support portal that they can live with, probably won't be fixed any time soon. So essentially my acceptability aligns directly with severity. Maybe once upon a time I cared but at some point things scale and you kind of just have to accept no harm no foul.\n\nAlso one way I try to counteract this is have engineers write documentation that is full-featured. I want to see what the admin portal looks like with the data, I want to know how the data flows between different parts or what the database is supposed to look like. I basically tackle every problem of documentation as anyone in the org should be capable of reading it to some degree. Obviously not everyone is going to understand everything, but I want something for everyone in there, organized well enough they can quickly skim to the relevant parts. I quite routinely tell people to pretend someone else is reading it and to see if they can make sense of it. For example, I'll tell them to pretend the CTO is reading it. Is the documentation lacking a high level technical summary? What if it's the CFO instead? One of them I can be much more technical with but neither of them want to understand the schemas.\n\nAnother way we counteract this is by cross team reviews. If you are working on something that touches code another team owns you need to reach out to that team (typically their engineering lead) and get them to review. I've caught a lot of issues like this by reviewing code for other teams. Better to spend 15 minutes now preventing an issue than 15 hours fixing it later.",
          "score": 3,
          "created_utc": "2026-01-02 18:35:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrkslj",
          "author": "couchwarmer",
          "text": "All the time. Slurping data from Epic is always an adventure. The HL7 v2 feed and FHIR API result are generally easy to match up. But when you need to pull from the crazy that is the Clarity extract, you eventually laugh, cry, or drink in frustration. Someone somewhere knows how all three correspond exactly for a given FHIR resource, but in however many years I've yet to find one of those someones.",
          "score": 3,
          "created_utc": "2026-01-05 04:59:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxde2j0",
          "author": "Quiet-Arm-641",
          "text": "Conwayâ€™s Law",
          "score": 5,
          "created_utc": "2026-01-03 02:58:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9jn6m",
          "author": "halfxdeveloper",
          "text": "Thereâ€™s a company that started on the mainframe. Users were comfortable with it and had learned its quirks. The development teams would build a product with loose restrictions and the users would find ways to make it work for their specific use case. One in particular was a field called â€œstatusâ€ that was just an open field with two characters. The uses for that field were so wild that I canâ€™t even name 10% of them. But each team had its own use for that field and Iâ€™m fairly certain every combination of two characters was used. The business wanted to modernize eventually and the cobol programs were converted to jsp pages with spring backend. And of course, a two character â€œstatusâ€ field. One day a director asked the development team to have a certain two character combination (a period and question mark) trigger a workflow in the backend when a user saved that status. Once other directors found out that was a thing you could do, they requested their own triggers for combinations and of course that switch statement grew like wildfire. Of course, any smart individual would see where this is headed. Some teams were having processes kick off because they typed a combination that meant one thing to them but something else for a different team. Was this a bug? No. But it highlights your point precisely. As the number of stakeholders grows and the number of development teams grows, this is just how things go in large corporations. Funny enough, the next modernization effort was led by an IT VP who was a very close minded individual that insisted the â€œstatusâ€ field would not live to see another day. It was (unsurprisingly) one of the top listed reasons why users refused to use the new web interface and several went back to using the mainframe.",
          "score": 5,
          "created_utc": "2026-01-02 15:17:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxscbae",
          "author": "elch78",
          "text": "The quote \"all models are wrong, some models are useful\" comes to my mind when I read this.",
          "score": 2,
          "created_utc": "2026-01-05 08:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxscwb8",
              "author": "Suspicious-Case1667",
              "text": "Exactly. The model isn't \"wrong\" in isolation it's just incomplete once it meets real people, incentives, and time. Most of the issues I've seen come from where multiple \"useful\" models overlap and quietly contradict each other.",
              "score": 1,
              "created_utc": "2026-01-05 08:52:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q59bu2",
      "title": "ProtoBuf Question",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q59bu2/protobuf_question/",
      "author": "Landmark-Sloth",
      "created_utc": "2026-01-06 04:54:23",
      "score": 32,
      "num_comments": 38,
      "upvote_ratio": 0.87,
      "text": "This is probably a stupid question but I only just started looking into ProtoBuf and buffer serialization within the last week and I cannot find a solid answer to this online. \n\n  \nQ: Let's say I have a client - server setup. The server feeds many messages (of different types) to the client. At some point, the client will need to take in the byte streams and deserialize them to \"do work\". Protobuf or whatever other serialization library has methods for this but all the examples I've seen already know the end result datatype. What happens when I just receive generic messages but don't know end datatype? \n\n  \nOnline search shows possible addition of some header data that could be used to map to a datatype. Idk. Curious to hear the best way to do it, not in love with this extra info when not completely necessary. ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q59bu2/protobuf_question/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxyevj3",
          "author": "_Trio13_",
          "text": "I think that's one of the major issues with protobuf - the byte streams aren't self-identifying. You need to wrap or prefix each object with some type specifier. The protobuf.Any type might be a workable container. Or you could create a single top-level protobuf message and use a oneof field as a union of all your message types.",
          "score": 26,
          "created_utc": "2026-01-06 04:59:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxygfcx",
              "author": "larowin",
              "text": "This is the whole point of gRPC - you get protobufs with metadata",
              "score": 25,
              "created_utc": "2026-01-06 05:10:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyfg6c",
          "author": "AvailableFalconn",
          "text": "Most situations Iâ€™ve used it, like in GRPC or in data warehouses, you know what data type youâ€™re expecting so this isnâ€™t an issue. Â Depending on why your data type is ambiguous, there might be solutions like using unions to wrap the relevant options. Â But in general itâ€™s not a self-documenting format.",
          "score": 16,
          "created_utc": "2026-01-06 05:03:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4qiyp",
              "author": "Landmark-Sloth",
              "text": "Thanks for the comment. I will give an example to feed the \"depending on why your data type is ambiguous\".  \n\n  \nLet's say I have a logger module. It connects to server or broker or whatever to get messages. Log messages aren't guaranteed to be the same (unless special consideration is taken). Let's say I have an embedded system. I might log data for a device, say a few voltage values, current etc. Now I also want to log system info, maybe a state machine transition etc.",
              "score": 1,
              "created_utc": "2026-01-07 02:55:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxyydjo",
          "author": "st4reater",
          "text": "Why don't you know what you're receiving?",
          "score": 8,
          "created_utc": "2026-01-06 07:37:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4qpl7",
              "author": "Landmark-Sloth",
              "text": "I responded to another message above with this example but copy - pasting here: Let's say I have a logger module. It connects to server or broker or whatever to get messages. Log messages aren't guaranteed to be the same (unless special consideration is taken). Let's say I have an embedded system. I might log data for a device, say a few voltage values, current etc. Now I also want to log system info, maybe a state machine transition etc.\n\n  \nLet me know what you think. Thanks for the comment. Below looks like bot behavior.",
              "score": 1,
              "created_utc": "2026-01-07 02:56:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nys29f7",
                  "author": "RipProfessional3375",
                  "text": "The main problem you are running into is that the marschalled binary is not human readable. This is just a fact of protobuff. Everything else is generally solvable or a misunderstanding of protobuff.",
                  "score": 1,
                  "created_utc": "2026-01-10 12:58:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nygjn7d",
              "author": "GregsWorld",
              "text": "E.g. A video game where you don't know if the player will use an ability or move next",
              "score": 1,
              "created_utc": "2026-01-08 19:52:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxz5h38",
              "author": "black_at_heart",
              "text": "Protocol Buffers (protobuf) are designed for **maximum efficiency**, which means they strip away almost all metadata that humans find helpfulâ€”like field names and data typesâ€”to save space.\n\nWhen you receive a raw protobuf byte stream, it is effectively a \"nameless\" string of numbers. Here is exactly why you can't decode it without a schema or a header.\n\n1. It Uses \"Tags\" instead of Names\n\nIn JSON, you see `\"user_id\": 123`. In protobuf, the name \"user\\_id\" is never sent. Instead, it only sends a **field number** (the \"tag\") that was defined in your `.proto` file.\n\n* **The Problem:** If you receive a message starting with `Field #5`, you have no idea if #5 stands for `price`, `age`, or `zip_code` unless you have the original schema to look it up.\n\n2. The \"Wire Type\" is Ambiguous\n\nProtobuf groups all data types into just a few \"wire types\" (categories of encoding). For example, `int32`, `int64`, `uint32`, `bool`, and `enum` are all encoded using the **Varint** wire type.\n\n* **The Problem:** If the decoder sees a Varint with the value `1`, it doesn't know if that means `true` (bool), the number `1` (int), or the first entry in a list (enum). It needs the schema to know how to \"cast\" that number into the correct programmatic type.\n\n3. There Is No \"Outer\" Message Type\n\nIf you send a `LoginRequest` and a `LogoutRequest`, the binary payloads might look very similar. Unlike a self-describing format (like a JSON object that might have a `\"type\": \"Login\"` field), a raw protobuf message **does not identify itself**.\n\n* **The Problem:** The receiver just gets bytes. Without a **header** (like an ID in the TCP packet or a gRPC metadata field) or a pre-defined sequence, the receiver won't even know which message class to use for decoding.\n\n4. It Is Not \"Self-Delimiting\"\n\nProtobuf does not have \"start\" or \"end\" markers (like `{ }` in JSON). It is just a stream of fields.\n\n* **The Problem:** If you are reading from a stream (like a network socket), you don't know where one message ends and the next begins. This is why most implementations use a \"Length-Prefixed\" headerâ€”essentially a small number at the very beginning that tells you \"the next 150 bytes are one message.\"\n\n#",
              "score": -28,
              "created_utc": "2026-01-06 08:43:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz5xvt",
                  "author": "st4reater",
                  "text": "I'm not reading that slop, sorry",
                  "score": 12,
                  "created_utc": "2026-01-06 08:48:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxz5kzm",
                  "author": "black_at_heart",
                  "text": "How do we fix this?\n\nIn practice, we handle this in two ways:\n\n1. **The Schema:** Both the sender and receiver have a compiled version of the `.proto` file. This acts as the \"decoder ring.\"\n2. **The Envelope (Header):** Most developers wrap their protobuf messages in a \"Wrapper\" or \"Envelope\" message that includes a field for the message type and the length of the payload.",
                  "score": -24,
                  "created_utc": "2026-01-06 08:44:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxyeyq8",
          "author": "no1SomeGuy",
          "text": "Just use a standard wrapper on it like cloudevents...the point of protobuf is just efficient serialization of the main payload.  \n[https://github.com/cloudevents/spec/blob/main/cloudevents/formats/protobuf-format.md](https://github.com/cloudevents/spec/blob/main/cloudevents/formats/protobuf-format.md)",
          "score": 5,
          "created_utc": "2026-01-06 05:00:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz2mts",
          "author": "jeffbell",
          "text": "You are correct. You have to know what data is coming.Â \n\nTo make them self defining (like json) adds overhead to the message. At the scale of Google if you add a few percent to the processing required you have to build another datacenter or lay more fiber and thatâ€™s real money.Â \n\nItâ€™s an intermediate design between specifying all the bits like a C struct, and accepting everything like a string key-value store. Itâ€™s more of an integer key-value.",
          "score": 5,
          "created_utc": "2026-01-06 08:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyoj1u",
          "author": "Foreign_Clue9403",
          "text": "Technically is this not an issue with all protocols and interfaces?  \n\nFrom a ZTA view you could always argue that â€œthis header says this is type X of size Y but I donâ€™t know if I can just accept thatâ€\n\nBut to this end it encourages simplifying and limiting the number of types you ought to allow. \n\nAnd from a design standpoint, metadata is necessary info in order for the system to behave to requirement, even if itâ€™s quite redundant in many individual application use cases.",
          "score": 3,
          "created_utc": "2026-01-06 06:11:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny039kj",
          "author": "asdfdelta",
          "text": "You're seeing protobufs as REST APIs that blindly call endpoints with mystery packages. It's different in a big way.\n\nThe whole point of RPC is that the server and client know more about each other, like what procedure to call and what data types to expect. This in turn increases performance because less compute cycles are spent figuring out what to do with the payload, interfaces, and abstractions.",
          "score": 3,
          "created_utc": "2026-01-06 13:19:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4nxye",
              "author": "Landmark-Sloth",
              "text": "This is a good answer, thank you. I understand a lot of other comments talking about this being the issue with protobufs. But something tells me google knew better and didn't incorporate. Its not a limitation but rather its just not intended usage and I don't completely understand intended usage. \n\n  \nI am still fuzzy on exactly how multiple data types can be handled. I understand the intention of client / server knowing more about each other than just being blind. But I still think having multiple datatype between the two does not signal poor design. Let me know your thoughts here. Thanks again for the comment.",
              "score": 2,
              "created_utc": "2026-01-07 02:41:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny4so3l",
                  "author": "asdfdelta",
                  "text": "In strongly typed paradigms, you wouldn't pass multiple data types internally to a service unless you overloaded a method signature.\n\nRPC (where protobufs get their main use) stands for Remote Procedure Call. Procedure, in this case, implies a *single method*. Yes, this is essentially evoking a method call in an entirely separate runtime over the network.\n\nWith that mindset, a different data type would simply be an overloaded signature and ergo a different method or RPC call. It's not 'restrictive' in the same sense that OOP isn't 'rescrictive' of how objects can be used. It's that way because that's OOP. \n\nYou **could** pass them all with some blank and others populated, then have the receiving method decipher what the hell it just got... But at that point you've lost the plot. It's why infinitely dynamic signatures don't really exist (we're ignoring loosely typed languages here).\n\nRPC is glorious because it bypasses all of the interpretation layers of compute just trying to figure out what to do with what data was given to it. All you get is exactly what you expect then you immediately get to using it. Clean, to the point, and extremely performant. Obviously not meant for general-purpose stuff as this tightly couples both services together so proceed with care.",
                  "score": 1,
                  "created_utc": "2026-01-07 03:07:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxz7sd6",
          "author": "afops",
          "text": "You can mark classes and subclassses. E.g if you have an abstract class animal and messages is an array of animal which are Cat or Dog then the subtypes will be indexed e.g 1=Cat 2=Dog followed by the common Animal data and then the specific data.\n\nBut of course the possible subtypes must be known (a closed set) at both transmitter and receiver.",
          "score": 2,
          "created_utc": "2026-01-06 09:05:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxz8mj3",
          "author": "sessamekesh",
          "text": "You _can_ haggle together a vaguely JSON-ish protobuf type to hold arbitrary data with a big ol' union type that optionally holds another instance of itself. I've seen it done successfully, and if you're really concerned about size over the wire and are sending a bunch of numeric data it might even be worth it for you.\n\n\nGenerally though, protos hold structured data. If your server and client both understand the possible data types (or the client is only responsible for ferrying it to downstream consumers that do) you can also encode an arbitrary proto as a blob or string and pass that as a field. I've done that for proxy layers that don't actually understand the data they are forwarding, just the header proto data.",
          "score": 1,
          "created_utc": "2026-01-06 09:14:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzxlk4",
          "author": "Wh00ster",
          "text": "Me of the benefits is that you know what to expect. Itâ€™s an RPC. A remote function. Statically compiled. If you want to accept generic data where the client can send whatever, then thatâ€™s just a POST request with JSON. Going even further you could ask â€œwhat if I donâ€™t know if itâ€™s a POST or GET?â€ This sounds like an XY problem",
          "score": 1,
          "created_utc": "2026-01-06 12:43:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny0im45",
          "author": "fogchaser35",
          "text": "This is only a problem if you are using protobufs as a mechanism to only serialize/deserialize data, and using your own custom code to read/write from a socket. When I first used protobufs back in 2010-11, I did it the same way. I used the first 8 bytes for a code to indicate the object type, the next 8 bytes for the length of the protobuf blob, and then the protobuf blob itself.\nYou donâ€™t have to do this anymore. You should be using GRPC now to handle all of this.",
          "score": 1,
          "created_utc": "2026-01-06 14:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4pu0s",
              "author": "Landmark-Sloth",
              "text": "I am exactly in your situation. I am thinking of using ZMQ (or some other similar like socket API messaging library) and protobuf as the mechanism to get to and from serialized data. gRPC doesn't really make much sense for me, but then again I haven't dug too deep on gRPC. Let me know your thoughts. Appreciate the comment.",
              "score": 1,
              "created_utc": "2026-01-07 02:51:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny2yi6y",
          "author": "sukaibontaru",
          "text": "Maybe json-rpc",
          "score": 1,
          "created_utc": "2026-01-06 21:27:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3ygag",
          "author": "_Trio13_",
          "text": "gRPC depends on the client and server having agreed about the type information ahead of time. I don't know of any way to derive a type name from a gPRC stream and dynamically create an instance based on the data. The endpoints are strongly typed.",
          "score": 1,
          "created_utc": "2026-01-07 00:24:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4hm6f",
          "author": "kbeta",
          "text": "Protobuf can also use a set of Descriptors (https://protobuf.com/docs/descriptors)  to dynamically marshal/unmarshal messages without having the generated proto code for the message compiled in. These descriptors are themselves protobufs (and always compiled in), so can be sent over the wire / included as a header in a data file, etc.",
          "score": 1,
          "created_utc": "2026-01-07 02:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4s9hy",
              "author": "Landmark-Sloth",
              "text": "Yes thank you for this. I went down a rabbit hole and came to the same ending page (more or less):  [https://protobuf.dev/programming-guides/techniques/#union](https://protobuf.dev/programming-guides/techniques/#union)",
              "score": 1,
              "created_utc": "2026-01-07 03:04:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyga8xn",
          "author": "jrinehart-buf",
          "text": "Sorry I'm late to the party, I've been catching up from a long holiday break.\n\nAre all of your events of known types/structures, and you're basically looking for an envelope to wrap them in that says \"the contents of this envelope are an event of type com.foo.TypeA\"? If so, Protobuf's Any type is likely what you're after.",
          "score": 1,
          "created_utc": "2026-01-08 19:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxym3mu",
          "author": "HRApprovedUsername",
          "text": "try deserialize expected type a; catch try deserialize expected type b; etc",
          "score": -12,
          "created_utc": "2026-01-06 05:52:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxz60cs",
              "author": "st4reater",
              "text": "Wtf",
              "score": 3,
              "created_utc": "2026-01-06 08:48:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny4qkiq",
              "author": "Landmark-Sloth",
              "text": "yea i know enough to know that aint it",
              "score": 1,
              "created_utc": "2026-01-07 02:55:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyimcw",
      "title": "How do you work with AI as a long-term architect (docs + decisions + staying up-to-date)?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1pyimcw/how_do_you_work_with_ai_as_a_longterm_architect/",
      "author": "Acceptable-End-4636",
      "created_utc": "2025-12-29 10:08:31",
      "score": 32,
      "num_comments": 8,
      "upvote_ratio": 0.85,
      "text": "Iâ€™m looking for practical setups people use when working as an architect / platform owner on long-running projects (1â€“3 years).\n\nWhat Iâ€™m trying to optimize for:\n\n* one main workspace\n* chat with an AI assistant who can do research (also in web)\n* assistant should uses project documentation as context\n* can edit docs (e.g. Markdown or integrate with Conflu), not just answer questions\n* minimal context switching - ideally one tool\n* collaboration (let others do the same in the workspace)\n\nAdditionally, could be a separate tool for:\n\n* staying up-to-date with latest tech changes (OpenAI, Anthropic, Microsoft, Google, etc.), key aspect of new versions of frameworks etc - with ability to edit list of my interest.\n\n  \nAt the moment I have some licenses in my organization - MS Copilot license (and entire MS 365 ecosystem), Github copilot license, Confluence, and privately ChatGPT Go. But I am open for any toolsets. ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pyimcw/how_do_you_work_with_ai_as_a_longterm_architect/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nwj1yoz",
          "author": "arikisfruits",
          "text": "It kind of depends, but you should look into:\n* A workflow that keeps your documentation up to date, and ensures that each system and process have clear information on the scope, constraints, data models, contracts, etc. If this drifts, youâ€™re always starting from a losing position if you want to maximise correctness of the AI outputs. This can be AI assisted. \n* Governance of architectural decisions (clear why, options, decided approach, etc). Similar to the above, it ensures that you, your peers and AI have correct context.\n\nOnce you have the above, and the governance around it, you can centralise it in e.g. a git repo, and teach your LLM of choice about it. You can use pretty much any provider, but Anthropic models, especially Opus 4.5 (even Sonnet) work great with it.\n\nThis also very much depends on the size, scope and the goals youâ€™re trying to achieve. For more complex setups and tasks, Iâ€™d advise getting deep into the ecosystem of skills, MCPs, sub agents, etc.",
          "score": 9,
          "created_utc": "2025-12-29 11:27:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwj3y99",
              "author": "Acceptable-End-4636",
              "text": "Regarding centralizing in a repo - I was thinking about the same (e.g. VS Code + Github Copilot). By teaching our LLM, do you think of creating a custom system prompt that would let the agent work in more 'architect' role ? Since it is not coding, do you think Sonnet could act also great when it comes to architectural thinking, decision making, research (including tools from different players).",
              "score": 1,
              "created_utc": "2025-12-29 11:44:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwj69w3",
                  "author": "arikisfruits",
                  "text": "In essence, what you want is that the LLM always has enough quality context dependent on what goal you're trying to achieve. You do this by ensuring:  \n  \n1. You have machine-readable context (e.g. md files)  \n2. That the model is instructed to use that context, and that you steer it to behave in a certain way (boundaries, reasoning, examples of output) by giving it instructions in e.g. [CLAUDE.md](http://CLAUDE.md) file.\n\nRE: architect role, it very much depends on the quality of context you give it toâ€”that's what you want to focus on. Saying \"please write architectural documentation for an order management system\" is going to give you *something*, but most likely not what you want. On the other hand, steering the model in the right way can give you absolutely great output.\n\nRE: research capabilitiesâ€”not sure, someone else can chime in.",
                  "score": 1,
                  "created_utc": "2025-12-29 12:02:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwnky2j",
          "author": "Lekrii",
          "text": "I'm an architect for everything in three lines of business (shortest projects are a few months long, the longest running project I have has been going for 2 1/2 years so far).\n\nI occasionally ask AI questions the same as I would search on Google, but other than that I don't really use AI.  My most valuable asset is my personal knowledge.  This is just my opinion, but I'd rather put in more time and be able to do the work myself.\n\nI know a lot of people will disagree, but I'm of the mindset that AI causes as many problems as it solves.  I use it very hesitantly.",
          "score": 2,
          "created_utc": "2025-12-30 01:52:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpi3sd",
              "author": "Acceptable-End-4636",
              "text": "Here's the confusion - I do use my knowledge, but creating a detailed System Design document with a lot of things (architecture, infrastructure, security, business logic and more) - it takes a lot of time. My idea is like - talking to chat gpt within a Project, he gets my points but produces a clean, well-structured white paper, that it can keep updating when something comes up to my mind. Also, as you might know we could give it a specific role and behavior, which may result in challenging my thoughts and sometimes bringing up topics I forgot to put into white paper. Based on such documentation, you could later easily define tasks for developers or share it with higher management.",
              "score": 1,
              "created_utc": "2025-12-30 10:16:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwu01ey",
          "author": "ElectronicFrame5726",
          "text": "You might be interested in [Where Architects Sit in the Era of AI](https://www.infoq.com/articles/architects-ai-era/) which is more of a meta perspective than a practical setup for an architect's workstation. As far as tooling is concerned, take a look at [Ask the Software Architect](https://www.exploravention.com/products/askarch/) which helps architects formulate plans for large initiatives such as major feature enhancements or paying down tech debt. In the context of the article, the tool would be an example of an AOTL model.",
          "score": 2,
          "created_utc": "2025-12-31 00:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmp34l",
          "author": "gmosalazar",
          "text": "Depending on how far down the rabbit hole you want to go, I might also suggest using a local version of an LLM model instead of the cloud provided ones. Thatâ€™d be one or two iterations deep.\n\nOverall Iâ€™m interested about this too, I just have a dedicated project with a major LLM provider that tracks .md files that I provide.",
          "score": 1,
          "created_utc": "2025-12-29 22:57:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwmy59y",
          "author": "SirOk748",
          "text": "At some point, the decisions you make may not make sense to others without context. Collaboration across teams or individuals will be easier when everyone can easily find answers or access to previous decisions. I highly recommend this short read : [https://decisionrecords.org/blog/how-should-teams-document-important-decisions](https://decisionrecords.org/blog/how-should-teams-document-important-decisions) \\- A tool I can recommend on this topic is [decisionrecords.org](http://decisionrecords.org)",
          "score": 1,
          "created_utc": "2025-12-29 23:46:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3ufzi",
      "title": "Was Kevin Mitnick actually right about security?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q3ufzi/was_kevin_mitnick_actually_right_about_security/",
      "author": "Suspicious-Case1667",
      "created_utc": "2026-01-04 16:36:29",
      "score": 31,
      "num_comments": 12,
      "upvote_ratio": 0.79,
      "text": "Kevin Mitnick spent decades repeating one idea that still makes people uncomfortable:\n\nâ€œPeople are the weakest link.â€\nAt the time, it sounded like a hackerâ€™s oversimplification. But looking at modern breaches, itâ€™s hard not to see his point.\nMost failures donâ€™t start with zero-days or broken crypto.\n\nThey start with:\nsomeone trusting context instead of verifying\nsomeone acting under urgency or authority\nsomeone following a workflow that technically allows a bad outcome\nMitnick believed hacking was less about breaking systems and more about understanding how humans behave inside them. \n\nSocial engineering worked not because systems were weak, but because people had to make decisions with incomplete information.\nWhatâ€™s interesting is that even today, many incidents labeled as â€œtechnicalâ€ are really human edge cases: valid actions, taken in the wrong sequence, under the wrong assumptions.\n\nSo I want to know how people here see it now:\nWas Mitnick right, and we still havenâ€™t fully designed for human failure?\nOr have modern systems (MFA, zero trust, guardrails) finally reduced the human factor enough?\n\nIf people are the weakest link, is that a security failure or just reality we need to accept and design around?\n\nGenuinely interested in how practitioners think about this today",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q3ufzi/was_kevin_mitnick_actually_right_about_security/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxnjd68",
          "author": "Iryanus",
          "text": "I am curious: Was that ever really in doubt?",
          "score": 40,
          "created_utc": "2026-01-04 17:02:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxovhvq",
              "author": "mackfactor",
              "text": "This. It never was in doubt. That's why everyone that works at a company of even a relatively small size takes info sec training every year. Even for software vulnerabilities, those are still human weaknesses (though not the type that Mitnick was talking about). Software will act the same way when exposed to the same stressors - meanwhile you have companies that have hundreds of people that work for them that have a high variation in the way they might respond. There was never any question that humans are the weak point.",
              "score": 9,
              "created_utc": "2026-01-04 20:38:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxo026h",
              "author": "dashingThroughSnow12",
              "text": "The crypto folks systematically seem to ignore that humans are the weakest link and that a finance system has to protect that. They seem to only be concerned with things being algorithmically secure as opposed to holistically secure.\n\nI digress.\n\nWhile most will not disagree out loud, most will silently ignore the human factor. Throw up their hands and say there is nothing they can do in such and such a circumstance when it involves flesh and blood humans being the root cause.\n\nOften Iâ€™ll hear devs have some half-baked definition of authorization that conveniently leaves out tricky elements.",
              "score": 7,
              "created_utc": "2026-01-04 18:18:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxqqutz",
                  "author": "UnreasonableEconomy",
                  "text": "> Often Iâ€™ll hear devs have some half-baked definition of authorization that conveniently leaves out tricky elements.\n\nWe're already getting rid of passwords altogether. There's unfortunately only so many hours in a day...",
                  "score": 1,
                  "created_utc": "2026-01-05 02:10:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxnfn86",
          "author": "gambit_kory",
          "text": "He was 100% correct.",
          "score": 24,
          "created_utc": "2026-01-04 16:45:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrr4br",
          "author": "faultydesign",
          "text": "https://xkcd.com/538/",
          "score": 3,
          "created_utc": "2026-01-05 05:43:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxnoxqd",
          "author": "Bodine12",
          "text": "\"Genuinely interested in...\" = Not really interested because this is AI-slop.",
          "score": 9,
          "created_utc": "2026-01-04 17:28:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxp2wx5",
          "author": "justUseAnSvm",
          "text": "The *reduction ad absurdum* is that with a rubber hose, or your family on the line, you'll do anything an attacker wants.",
          "score": 2,
          "created_utc": "2026-01-04 21:13:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxpg3u7",
          "author": "serverhorror",
          "text": "Why would we assume it's anything but humans? And what makes you think it's an oversimplification?",
          "score": 2,
          "created_utc": "2026-01-04 22:14:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsgmnz",
          "author": "ERP_Architect",
          "text": "From what Iâ€™ve seen, he was pointing at an uncomfortable reality more than blaming people.\n\nMost failures Iâ€™ve been close to werenâ€™t caused by someone doing something obviously wrong. They were caused by someone making a reasonable decision under pressure, with incomplete context, inside a system that technically allowed it.\n\nModern guardrails help, but they donâ€™t remove the human factor. They just shift where it shows up. Instead of clicking the wrong thing, it becomes approving the wrong request or working around friction to get work done.\n\nWhere systems still break is when they assume perfect behavior. In practice, good security designs accept human judgment as a constant and focus on limiting impact and recovering quickly, not pretending people can be engineered out of the loop.",
          "score": 2,
          "created_utc": "2026-01-05 09:28:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxswln7",
          "author": "darkwyrm42",
          "text": "He was 100% correct, and before you think you can solve the problem, nothing is foolproof because people are geniuses at being stupid.",
          "score": 2,
          "created_utc": "2026-01-05 11:48:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxolycw",
          "author": "Aggressive_Ad_5454",
          "text": "Bit pointless to try to identify the â€œweakestâ€ link when youâ€™re defending against bad actors. They only need to find one weak link. You need to reinforce all the weak links.",
          "score": 1,
          "created_utc": "2026-01-04 19:55:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbkxof",
      "title": "How to setup Architecture Governance | Learnings and Practices",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qbkxof/how_to_setup_architecture_governance_learnings/",
      "author": "sahil000005",
      "created_utc": "2026-01-13 07:18:53",
      "score": 31,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "Iâ€™m an architect working on establishing an **Architecture Governance** process in my organization.\n\nI understand this is a subjective topic, but Iâ€™d love to learn how others have approached itâ€”what has worked well and what hasnâ€™t.\n\nMy primary focus is on defining **guardrails and architecture guidelines** that enable teams to work independently, with minimal involvement from architects, while still avoiding significant architectural deviations.\n\nLooking forward to hearing real-world experiences and lessons learned.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qbkxof/how_to_setup_architecture_governance_learnings/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzbssxk",
          "author": "HandsOnArch",
          "text": "Just my 2 cents based on what worked (and failed) for us:\n\n**Priority 1: Make architecture decisions explicit and visible.**  \nWe learned that if itâ€™s not documented, it effectively doesnâ€™t exist.  \nWe require ADR/ADL-style docs, very pragmatically in Confluence (beter would be probably Git or Jira).\n\nFor us, governance worked best when we focused first on everything that is visible from the outside:\n\n* APIs & contracts\n* Logging & monitoring standards\n* Third-party components\n* Reused / managed platform services\n* ... \n\n**Priority 2: Keep internal architecture deliberately flexible.**  \nThis was an important learning for us. Governance easily overreaches here.  \nWe had to learn to â€œpick our battlesâ€ and leave teams freedom internally to keep speed and ownership high.\n\nIf I were to start again, I would probably look into Architecture as Code earlier. I personally donâ€™t have strong tool recommendations yet, so for us it always came down to a cost/benefit trade-off: documentation in Confluence vs. technical enforcement via tooling.\n\nWhat helped us a lot in the beginning were guild / community structures. They gave us a natural place to grow standards and patterns together with the people actually doing the work. Our experience was that governance has to be built with the teams, otherwise commitment stays shallow.\n\nOne more thing we learned:  \nSomeone needs to clearly own moderation and final decisions. Without a visible decision owner, we never really converged on a shared strategy.",
          "score": 12,
          "created_utc": "2026-01-13 09:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbxx5x",
              "author": "sahil000005",
              "text": "Great to hear about your experience.\n\nWe already have a fairly mature product, and weâ€™re now looking to introduce architecture governance. Do you think itâ€™s practical to start capturing ADRs (or other architectural artifacts) for an existing system at this stage?\n\nIâ€™m struggling to figure out how to move forward because the backlog is quite large, and itâ€™s not clear whether weâ€™ll ever reach a point where the documentation becomes truly beneficialâ€”given that a significant portion of the system would need to be captured retrospectively.\n\nWould love to hear how you approached this, or what youâ€™d recommend in a similar situation.",
              "score": 3,
              "created_utc": "2026-01-13 10:44:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzcli2e",
                  "author": "HandsOnArch",
                  "text": "The good news is: governance introduction does not have to turn into a refactoring program.\n\nI would start with ADR- and policy-based guardrails that explicitly shape future decisions (e.g. \"for new or significantly changed components, we do X / use Y\").\n\nI would treat technical debt and distance-to-target as a separate, incremental investment topic with explicit cost/benefit decisions.\n\nFor me, those are two different problems â€” and I would handle them separately.\n\nJust my personal take:\n\nI wouldnâ€™t wait for the perfect framework, the perfect tooling or the perfect org setup. I would start with a small, accepted core of architects in the middle, give them a clear mandate to moderate and decide, and grow from there.\n\nEven a lightweight governance setup already helps:\n\n\\- to avoid building new technical debt\n\n\\- to even make technical debt visible and understandable in the first place\n\n\\- to create clarity in decisions\n\n\\- in the long run to make everyone who touches architecture stronger\n\n  \nGood luck!",
                  "score": 1,
                  "created_utc": "2026-01-13 13:36:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcjuv6",
          "author": "violentlymickey",
          "text": "You could write some \"golden path\" documentation in a central location for system design patterns and agreed upon common approaches. The problem is enforcement, but this can be slightly mitigated with static analysis tools if you're willing to put some effort into getting those written. Best approach would be to automate as much as you can, with for instance CI jobs.\n\nYou don't want to go too heavy handed on this though. It's better to have guidelines and have teams implement them the way they see fit rather than enforce a singular style, especially if your teams work on different stacks.",
          "score": 3,
          "created_utc": "2026-01-13 13:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd3pln",
          "author": "GrogRedLub4242",
          "text": "write the core architecture constraints or interfaces down in a doc (possibly with diagrams.) make them avail",
          "score": 2,
          "created_utc": "2026-01-13 15:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf8ruq",
          "author": "Ok-Scientist9904",
          "text": "In addition to ADRs, I would also set up a decision matrix for teams on items that MUST come to an architect for review. Like- anytime using a new technology or open source is proposed, Integrating with third party systems etc",
          "score": 2,
          "created_utc": "2026-01-13 21:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze2kd1",
          "author": "themessymiddle",
          "text": "ADRs like others have mentioned, I also like to document principles, constraints, basically anything that would help teams make decisions. Different things work for different companies, so Iâ€™d say start lightweight with some guidance that helps in the specific areas where teams arenâ€™t yet able to work as independently as youâ€™d like. Good governance is easier when itâ€™s built up over time",
          "score": 1,
          "created_utc": "2026-01-13 18:03:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzenvug",
          "author": "dudeaciously",
          "text": "We have a standard set of diagrams, UML based.  Any architecture can not be explained by a single perspective.  Some diagrams are optional.  \n\nOnce everyone is working in similar visual language, it becomes easier for the peer group to suggest clarifications and fine grained problems.\n\nArchitects don't create erroneous things.  But we sometimes don't go down good paths in favor of certain concerns.  Ignoring CI/CD, not sufficient data architecture, not sufficiently scalable, secure, etc.",
          "score": 1,
          "created_utc": "2026-01-13 19:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgcna0",
          "author": "lexseasson",
          "text": "https://medium.com/@eugeniojuanvaras You can take a walk there are some solutions",
          "score": 1,
          "created_utc": "2026-01-14 00:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhzh6c",
          "author": "aphillippe",
          "text": "The technical side of things should be a secondary concern. Primary concern should be the people and process. If peers arenâ€™t bought in to the benefits and pay attention to decisions, if you only have carrot and no stick, if senior leadership doesnâ€™t back you up, it is all performative",
          "score": 1,
          "created_utc": "2026-01-14 06:57:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q848wy",
      "title": "How do you actually understand a codebase you didnâ€™t write?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q848wy/how_do_you_actually_understand_a_codebase_you/",
      "author": "Bioseamaster",
      "created_utc": "2026-01-09 09:52:36",
      "score": 29,
      "num_comments": 35,
      "upvote_ratio": 0.88,
      "text": "Iâ€™m running into this more and more and Iâ€™m curious how other teams handle it.\n\nBetween AI-generated code, contractors, and fast-moving startups, it feels like a lot of us are shipping systems that nobody fully â€œownsâ€ anymore. When you inherit a codebase you didnâ€™t write (or havenâ€™t touched in months), reading the code line by line doesnâ€™t really answer the questions you care about.\n\n* What does this system *actually* do end-to-end? \n* What assumptions does it rely on? \n* Which parts are fragile vs safe to change? \n* Did this PR just refactor, or did it subtly change behavior?\n\nDocs are often outdated, tests donâ€™t explain intent, and PR reviews tend to focus on style or correctness, not whether the change still makes sense in context.\n\n  \nHow do you personally approach understanding an unfamiliar or AI-written codebase before you trust it or approve changes? Any tools, workflows, or mental models that actually work in practice?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q848wy/how_do_you_actually_understand_a_codebase_you/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nykku8i",
          "author": "GrogRedLub4242",
          "text": "strawman there\n\nI read the code, docs, diagrams. I peek under rocks. study logs, etc. build up a model in my mind",
          "score": 35,
          "created_utc": "2026-01-09 10:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylwjcp",
              "author": "MasterA96",
              "text": "Same",
              "score": 3,
              "created_utc": "2026-01-09 15:12:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nymjg6e",
              "author": "hurley_chisholm",
              "text": "This.\n\nIâ€™ll also add reading commit history. If something really confuses me, Iâ€™ll even track the progression of specific functionality over several commits.\n\nETA: If you use Jetbrains IDEs, Iâ€™m also a big believer in the â€œCall Hierarchyâ€ / â€œCaller Hierarchyâ€ action for especially complex code. It shows everywhere a class/type/method is called or all the classes/types/methods that are called and their hierarchies within a particular declaration, respectively.",
              "score": 3,
              "created_utc": "2026-01-09 16:56:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymppep",
                  "author": "GrogRedLub4242",
                  "text": "grep/find and \"thinking\" can do that too. no IDE needed, def not JetBrains",
                  "score": -1,
                  "created_utc": "2026-01-09 17:24:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nym4o7n",
              "author": "LordWecker",
              "text": "When you say strawman here, are you just saying that it's a non-issue (or maybe a skill issue)?",
              "score": 1,
              "created_utc": "2026-01-09 15:50:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymoyr3",
                  "author": "GrogRedLub4242",
                  "text": "meant strawman argument",
                  "score": 2,
                  "created_utc": "2026-01-09 17:21:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyqy9gc",
              "author": "_TheShadowRealm",
              "text": "I wish my team had docs and diagrams. These guys just pass on information through oral tradition like a Stone Age tribe ðŸ˜­ they would much rather waste time explaining things 10 times to 10 different people than write it down - itâ€™s actually insane",
              "score": 1,
              "created_utc": "2026-01-10 07:05:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrcsup",
                  "author": "Confident_Pepper1023",
                  "text": "Did you consider starting to document everything and lead by example?",
                  "score": 1,
                  "created_utc": "2026-01-10 09:19:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykjo0f",
          "author": "Sad_Amphibian_2311",
          "text": "Unfamiliar codebase: it's possible to extract the reasoning from change history.   \nAI codebase has no reasoning.",
          "score": 10,
          "created_utc": "2026-01-09 09:58:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykm4mz",
          "author": "da_supreme_patriarch",
          "text": "Usually, looking at integration tests, API endpoints, message consumers and scheduled jobs is a good place to start in order to map out a system's capabilities. The next step is to look at the DB schema, if there is one, and look for indexes to figure out common access patterns and then try to find corresponding queries in the codebase.\n\nNot really sure how one would map out AI generated codebases, I haven't really encountered a fully generated one yet, but I would assume that the same technique would still work, albeit less effectively",
          "score": 9,
          "created_utc": "2026-01-09 10:20:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykkp2r",
          "author": "PabloZissou",
          "text": "Fire the debugger up and good luck, AI Slop is a disaster but debugger should also help",
          "score": 6,
          "created_utc": "2026-01-09 10:07:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykltld",
          "author": "jac4941",
          "text": "Show me the binary I can run and break. Or show me the Helm chart that spins it up on a cluster, so I can then break it. Subscribe to PRs for the parts I care about, subscribe to changelogs. Push linters and commit hooks and SCA tools in there and review the results. Profiling.\n\nAI hasn't changed any of that, just the importance of understanding how to jump in and effectively review the unknown.",
          "score": 4,
          "created_utc": "2026-01-09 10:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykzw9f",
          "author": "Aggressive_Ad_5454",
          "text": "Use a good IDE. VS or something from JetBrains. They have features like â€œnavigate to the definition of this symbolâ€ that can work across large codebases.\nThey also use the Javadoc / JSDoc / Doxygen / whatever standard class and method header comments. Theyâ€™ll show you popups with those comments as you code.\n\nThey have â€œsearch everywhereâ€ features. \n\nAnd, as you learn the code base you can add header comments where theyâ€™re missing.",
          "score": 4,
          "created_utc": "2026-01-09 12:12:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyks3qm",
          "author": "geeky_traveller",
          "text": "I have built an internal tool for this use case, where I have my own prompts and chatbot where I can ask questions and it will provide me context based on the code, monitoring dashboards, data and current docs within the system.\n\nI can ask any questions over the whole codebase across different repositories and it responds with the answers.\n\nIf it's just single repo, then I use cursor/claude code within that repo, but I have seen many a times it find difficulty to get the cross service context and dependencies\n\nThis becomes my starting point, and then I look into the code for deep dives",
          "score": 3,
          "created_utc": "2026-01-09 11:11:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nylp4i5",
              "author": "Equivalent_Affect734",
              "text": "That sounds really useful. Do you have any more info about how the tool can keep context across multiple repositories?",
              "score": 1,
              "created_utc": "2026-01-09 14:37:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nykz4u9",
          "author": "WhiskyStandard",
          "text": "The least obvious advice I have is to profile its most important workflows. People often reach for profiling for performance purposes, but itâ€™s very valuable for understanding what really calls what and how often. Patterns will emerge that may not be evident from reading the code or stepping through it linearly with a debugger.",
          "score": 2,
          "created_utc": "2026-01-09 12:06:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nylc6rr",
          "author": "asdfdelta",
          "text": "> it feels like a lot of us don't fully \"own\" the code base anymore.\n\nThis has been true with every abstraction put on top of the bare metal.\n\nBut it's okay. We now get to think less about individual lines of code and more about patterns, so consume and create really good conceptual documentation. If none exists, then you have the fun task of creating it. That's also true since the dawn of computing as well. AI is just another permutation on a solved problemset.",
          "score": 2,
          "created_utc": "2026-01-09 13:29:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyme9h5",
              "author": "LordWecker",
              "text": "I agree with your statement of where this leads us, but I think it's a fundamentally different step in the evolution of abstractions.  Whether or not I have a clue what the compiler is doing with it, I should still know why I wrote a piece of code, and I can take that code and run it on other machines and share it with other people.\n\nWith ai: the code is still the operational instructions set.  Prompts don't become a higher level language (and can't ever be, because they're not deterministic).\n\nAi is really just a supercharged evolution of IDE utilities; which still absolutely changes where cognitive load exists, so I still agree with this statement:\n\n>We now get to think less about individual lines of code and more about patterns, so consume and create really good conceptual documentation.\n\nBut I don't think it absolves engineers from owning their code.",
              "score": 2,
              "created_utc": "2026-01-09 16:33:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nymijkw",
                  "author": "asdfdelta",
                  "text": "Yeah, I can understand your perspective there.\n\nThis is different in that we know less, but I see this as a relinquishment of control in the same way as using a compiler relinquished control. Ultimately, you need to own not just the code, but the behavior and outcome of the application. If there is a bug, we fix it even if it's a quirk of the compiler.\n\nSo I agree with you, and also recognizing the similarities.",
                  "score": 1,
                  "created_utc": "2026-01-09 16:52:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nymndjm",
          "author": "professor_jeffjeff",
          "text": "Learn to read code.  Unlike the docs and the humans who wrote it, source code doesn't lie and doesn't forget about certain parts of itself that don't get worked on very often.  It always tells you exactly what it's going to do.  That doesn't mean you're always going to read it correctly, but nothing tells you how code works better than the code itself.\n\nThe one thing that source code can't tell you is the business domain around that code which is the reason the code was written.  I can read some code and determine that a Customer is a Most Valuable VIP Customer if they've bought more than 8 widgets in the last 38 days and that they will then receive a 6.3% discount on their next two sales.  However, I will never be able to figure out where that logic came from or why it's significant to the business (at least not solely from the source code itself, unless the code has the domain knowledge also coded into it e.g. DDD or somesuch)",
          "score": 2,
          "created_utc": "2026-01-09 17:13:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykl56h",
          "author": "markojov78",
          "text": "I once refused job when they explained to me that I am to inherit code written by one guy and there is no documentation because apparently he saw no reason to document for himself, and during the interview process I never had chance to even speak with the guy...",
          "score": 1,
          "created_utc": "2026-01-09 10:11:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykypbo",
              "author": "WhiskyStandard",
              "text": "Was the code creating value for the business? Because of it was, the challenge of taming that and making it something that others can work on and extend can be rewarding.\n\nOf course, it can also be a nightmare, so not judging you for turning it down.",
              "score": 2,
              "created_utc": "2026-01-09 12:03:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyld8rc",
                  "author": "markojov78",
                  "text": "According to them It was important part of logic for an insurance company.\n\nMain reason why I ended up on that interview is my previous experience in refactoring of big legacy system so I assumed that they wanted the same, but...\n\nThey were very clear that they do not see any need for refactoring and improvements because according to them it works fine and all they needed was maintenance (as in chasing and fixing eventual problems) and occasional implementation of new features.\n\nBecause the main guy was not available to answer some very important questions I had, I politely declined",
                  "score": 1,
                  "created_utc": "2026-01-09 13:35:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykpxdn",
          "author": "SkatoFtiaro",
          "text": "Will talk mostly for \"user interaction\" systems, not \"pure systemic\" (e.g transaction processing system in a bank).\n\nIn order for you to understand the code base, first thing you DONT have to understand is the codebase itself.\n\nFirst thing you need to understand is the user. What does a user does in his daily life. What is the user's goals? What is his emotions using the system? You need to understand first and foremost WHY the system exists in the first place.\n\nThen, that's where the \"good architecture\" fits. A \"good architecture\" is not about \"event sourcing\", Kafka, DBMS-es and Kubernetes.\n\nA good architecture, the most important thing it has to do, is to reveal User's goals. After you understand the user, the architecture should easily give you where things happen, why they happen. Then, again - If the architecture is good, you do not have to worry about all at once. You just become familiar with some highly related use cases. You  just open a \"package/module/project/microservice\" and all the code around these usecases are there without any kind of \"context violation\" to bombardize you and the system itself.\n\nAs the time passes, these two go hand in hand and you simply DO NOT HAVE to understand the \"codebase\" as a whole....\n\n  \nWe devs & architects use plenty words, but we forget that is all about \"COUPLING AND COHESION\". A lot of folks I have seen around also forget that coupling & cohesion has its roots to the user and his goals as first priority. To put simply, \"Things that are likely to change together, they go together - in the same neighborhood\". All you have to do then is to go \"neighborhood\" by \"neighboorhood\".",
          "score": 1,
          "created_utc": "2026-01-09 10:53:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyks7os",
          "author": "ALAS_POOR_YORICK_LOL",
          "text": "I found an entry point and start reading. I try to build up the mental model of the codebase in my mind.",
          "score": 1,
          "created_utc": "2026-01-09 11:12:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyprek7",
          "author": "zenware",
          "text": "TLDR; You actually achieve something by doing the work to achieve that thing.\n\nIMO your first two questions should be answered by Docs & Diagrams, and if they donâ€™t exist yet you slog through making them. You should have a single document that describes at the very least the expected/intended end-to-end of a system, and it should also highlight details like what services it depends on (among other assumptions.)\n\nThe other two are answered by source control/team typically. I can look at change history and frequency in Git and understand areas of code with high change frequency are resilient to change, and areas with low change frequency (think once a year or once every never) are maybe not fragile but a lot more care should be exercised when making changes there. As for â€œWhat did this PR actually do?â€ well ideally thatâ€™s answered by reading the code, talking to the person who wrote it, and running your test suite.",
          "score": 1,
          "created_utc": "2026-01-10 02:18:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqodp8",
          "author": "Admirable_Swim_6856",
          "text": "AI is very good at presenting you with a high level view of a codebase. Complete with diagrams. I use that as a start and then dig in, asking for more detail on sections as I go, validating areas myself with reading the code.",
          "score": 1,
          "created_utc": "2026-01-10 05:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqpejo",
          "author": "narrow-adventure",
          "text": "Youâ€™ve gotta read the code, it is a complete documentation of itself as it will never be outdated - it does exactly what it says it does.\n\nI personally like to start by analyzing which frameworks are used and how the project runs locally. After that I like to take one feature to get a sense of how things are built, so Iâ€™ll click a button and then trace through all the code that is executed, for web apps this would be creating an entity and then tracing the frontend code through the network tab all the way to the backend and to the actual db - at that point you have a good understanding of how the project is setup and running. The final step is to learn about the business logic, find someone using the product to show you how it works and explain why it works that way.\n\nUnfortunately there are no shortcuts on this one :/",
          "score": 1,
          "created_utc": "2026-01-10 05:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nysfq4l",
          "author": "Silent_Coast2864",
          "text": "This is one of the areas where AI really shines and is a game changer.",
          "score": 1,
          "created_utc": "2026-01-10 14:21:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdhlca",
      "title": "The U.S. Gov once threw a hacker in solitary because they thought he could WHISTLE nuclear launch codes. I wish I was joking.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qdhlca/the_us_gov_once_threw_a_hacker_in_solitary/",
      "author": "Suspicious-Case1667",
      "created_utc": "2026-01-15 11:57:07",
      "score": 26,
      "num_comments": 10,
      "upvote_ratio": 0.81,
      "text": "Okay Reddit, gather around, because this is one of those stories where reality is so stupid it loops back into entertainment.\n\nSo, in the 90s, hacker Kevin Mitnick gets arrested. Fine. He did hack stuff. Cool.\nBut hereâ€™s where everything goes full WTF levels unknown to mankind:\n\nA federal judge was convinced genuinely, unironically convinced that Kevin could â€œstart a nuclear war by whistling into a phone.â€\n\nLet me repeat that:\nA man was thrown in solitary confinement because someone thought he could blow up the world using dial-up noises.\nThis wasnâ€™t satire.\nThis was the United States justice system.\nThey literally banned him from:\nUsing a phone\nTouching a computer\nBeing near anything with â€œtonesâ€\nAnd kept him in solitary like he was a human rootkit about to self-replicate\nAll because they believed he was some kind of mythical techno-wizard who could whistle binary like a Final Boss NPC.\n\nMeanwhile, actual cybersecurity experts were like:\n\nâ€œUhâ€¦ thatâ€™s notâ€¦ how anything works.â€\nAnd the court was like:\nâ€œShhhhh. Heâ€™s dangerous. He knowsâ€¦ computers.â€\n\nThe whole thing became one of the biggest controversies in cybercrime history because it showed just how hilariously clueless the system was about technology.\n\nImagine going to prison because a judge thinks you might be able to hack NORAD with your mouth.\n",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qdhlca/the_us_gov_once_threw_a_hacker_in_solitary/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzppld9",
          "author": "jonathon8903",
          "text": "From my understanding of his story it was an unfortunate mix of an overzealous prosecutor and a judge who didn't understand technology to have any doubt. I'd like to hope that a similar situation couldn't happen today.",
          "score": 8,
          "created_utc": "2026-01-15 11:59:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpw4hy",
              "author": "Isogash",
              "text": "The system generally assumes that the defense is responsible for defending against stupid prosecutions, so this guy's lawyers were clearly not good enough. I don't necessarily agree but that's still how it works.\n\nI'd like to think that we've come a long way since and you are more likely to get lawyers who are tech-savvy enough to cut through the bullshit.",
              "score": 1,
              "created_utc": "2026-01-15 12:45:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzq3pbv",
                  "author": "caboosetp",
                  "text": "The lawyer doesn't even need to be tech savvy. They need to be smart enough to listen to their client and find an expert who can testify.Â ",
                  "score": 2,
                  "created_utc": "2026-01-15 13:31:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzpukx2",
          "author": "eduanlenine",
          "text": "And now we all have to watch boring videos from his security training company",
          "score": 4,
          "created_utc": "2026-01-15 12:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpq3ig",
          "author": "Comprehensive-Art207",
          "text": "â€Everything is computer.â€ /DJT",
          "score": 3,
          "created_utc": "2026-01-15 12:03:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzq4b6o",
          "author": "Fine-Ad9168",
          "text": "Whistling into phones to hack was very much a thing.  I don't think think you could launch missiles, I think you could just get free long distance calls.",
          "score": 5,
          "created_utc": "2026-01-15 13:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqf0tk",
          "author": "pandershrek",
          "text": "You can indeed hack an air gapped machine with tone injection. It was shown with a PoC less than 20 years ago, so this actually isn't far fetched because most scada systems are hilariously insecure.\n\nOn the tail of Stuxxnet they did a ton of work at kapersky to understand how so many exploits went into one attack. \n\nThough this might have been from a presentation at black hat before that",
          "score": 3,
          "created_utc": "2026-01-15 14:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpuyks",
          "author": "GForce1975",
          "text": "Mitnick was a cocky guy and didn't do himself any favors..but that was pretty ridiculous.\n\nRemember all the \"free mitnick\" merch?",
          "score": 2,
          "created_utc": "2026-01-15 12:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqk2zj",
          "author": "Exotic_eminence",
          "text": "ðŸ†“ KEVIN\n\nTo be fair he did do a lot of hacking with his mouth \n\nðŸ‘„ \n\nItâ€™s called social engineering",
          "score": 1,
          "created_utc": "2026-01-15 14:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr4u8p",
          "author": "arnedh",
          "text": "There was both the possibility of changing the mode of a telephone call with 2600Hz and of submitting numbers as DTMF tones. https://en.wikipedia.org/wiki/Phreaking.\n\nWith DTMF tones, he might even have been capable of controlling equipment and hitting higher privileges with ABCD codes. https://en.wikipedia.org/wiki/DTMF_signaling, https://en.wikipedia.org/wiki/Precise_tone_plan\n\nWhat you could possibly achieve vs a government computer with those two techniques (and clicking/tapping the right rhythm) is not clear to me, but if I were the judge, I think I would err on the side of caution - the government wouldn't be likely to be forthcoming with specifics on what one could or couldn't hack, especially if security was below par  - imagine the headlines.",
          "score": 1,
          "created_utc": "2026-01-15 16:32:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q76gol",
      "title": "Rebuilding Event-Driven Read Models in a safe and resilient way",
      "subreddit": "softwarearchitecture",
      "url": "https://event-driven.io/en/rebuilding_event_driven_read_models/",
      "author": "Adventurous-Salt8514",
      "created_utc": "2026-01-08 08:27:46",
      "score": 25,
      "num_comments": 1,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q76gol/rebuilding_eventdriven_read_models_in_a_safe_and/",
      "domain": "event-driven.io",
      "is_self": false,
      "comments": [
        {
          "id": "nyh3vvl",
          "author": "tarwn",
          "text": "My guidance is to do \"inline projection\" until you can't, and then and only then let the reason you can't push the architecture into more async directions. Don't prematurely optimize the entire system to async event sourcing, which tends to be unexpected complexity for everyone consuming your API, working on the system, etc, just to solve a theoretical constraint or one constraint out of NN aggregates.",
          "score": 2,
          "created_utc": "2026-01-08 21:22:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6dsdw",
      "title": "How do you keep a mental model of a complex system as it evolves?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q6dsdw/how_do_you_keep_a_mental_model_of_a_complex/",
      "author": "Greedy_Engineering_1",
      "created_utc": "2026-01-07 12:13:13",
      "score": 25,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "Hi all,\n\nIm an engineering student taking a Lean Startup course, and I'm trying to learn how teams handle large, evolving codebases in practice. \n\nEspecially interested in how people build and maintain an understanding of system structure, flows and dependencies as things change over time.\n\nI'm doing short 10 min conversations to learn from real experiences. Noting to sell, no demos - just wanting to understand. \n\nWould love to hear how you approach this!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q6dsdw/how_do_you_keep_a_mental_model_of_a_complex/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "ny6tg5k",
          "author": "titpetric",
          "text": "Systems design. Keeping a system in your head may be 1% of the code. You can update it as you go, with documentation. It's systems all the way down...",
          "score": 9,
          "created_utc": "2026-01-07 12:32:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6rj0h",
          "author": "UnreasonableEconomy",
          "text": "Archimate",
          "score": 4,
          "created_utc": "2026-01-07 12:19:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny6tv8n",
              "author": "Greedy_Engineering_1",
              "text": "Interesting will look into, thanks!   \nIs this widely adopted in large organizations? Does it make sense for smaller teams as well?",
              "score": 2,
              "created_utc": "2026-01-07 12:35:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny70sdl",
                  "author": "UnreasonableEconomy",
                  "text": "It depends - I tend to promulgate it. I would say all large orgs use it to some extent, but it's rarely ubiquitous. You'll know what I mean as soon as you work in one. \n\nThere's a free modeler: https://www.archimatetool.com/\n\nWhat makes it powerful is that it shapes how you think about the organization and your systems, and forces you to answer questions you might not have thought of, in terms of how things relate to each other.\n\nand it broadly speaking fits into the TOGAF ecosystem. I would recommend studying the metamodel though\n\nUnfortunately the open group has become increasingly uppity in recent times with the resources it makes available, but they *should* still be free with a free account. If you can't access it, ask one of your professors, they should be able to help you out. Here's a public link I found for the 3.1 spec: https://university.sk/wp-content/uploads/2020/01/ARCHIMATE_v3_1_specifikacia.pdf?utm_source=chatgpt.com",
                  "score": 1,
                  "created_utc": "2026-01-07 13:18:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nygs8xd",
              "author": "SirOk748",
              "text": "I haven't used the tool, but I've learned a lot from some of their publications from a while back. Valuable resource and a good answer for the OP.",
              "score": 2,
              "created_utc": "2026-01-08 20:30:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny7ravy",
          "author": "YakRepresentative336",
          "text": "The most spreading diagram actually is C4 model that have  4 levels of Abstractions,\n\neach level of abstractions correspond to the level of interest of stakeholders (level of details) but anyone can see whatever they want,\n\nExample : \n- components & code level for technical team\n- containers & components for any higher level like Architect\n- context for board",
          "score": 4,
          "created_utc": "2026-01-07 15:36:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny9tknf",
          "author": "Doctuh",
          "text": "Fred Brooks had it right 50 years ago: [Conceptual Integrity](https://wiki.c2.com/?ConceptualIntegrity).",
          "score": 6,
          "created_utc": "2026-01-07 21:04:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyafwqc",
          "author": "Comfortable_Ask_102",
          "text": "My 2c on this:\n\n1. I think the mental model of a system is what [Martin Fowler](https://martinfowler.com/ieeeSoftware/whoNeedsArchitect.pdf) describes as the \"system architecture\": the significant components, their interfaces with other components and systems. The tricky part is to identify the \"significant\" components, it's something only the developers in the project can tell. It's fuzzy, but some indicators are: parts of system that receive constant updates, the *critical* part of the system (i.e. if this component breaks we no longer have a *working* system).\n2. Linus Torvalds has a cool [quote](https://softwareengineering.stackexchange.com/questions/163185/torvalds-quote-about-good-programmer) that can give some insight: *\"Bad programmers worry about the code. Good programmers worry about data structures and their relationships.\"*\n\nI try to keep a good understanding of:\n\n* The components in the system e.g. web apps, internal and external APIs, data sources, orchestrators, etc. More so at a high-level: \"this is a web app that is used by X persona for Y thing\" or \"we use service X to process payments\".\n* How these components interact with each other and when, e.g. \"we have a payment processor that consumes some 3rd party providers, and the logic varies depending on the country or running promotional campaigns.\n* The high level data structures e.g. a Product, an Order, a Payment or Shipment. I think it makes it easier to reason as \"an Order can have several Products, and once the Payment succeeds we create a Shipment for the Order. It's very abstract and high level, and each part will map to a different part of the system.\n\nNow, all of that usually lives in the minds of the developers, the hard part is to communicate this understanding to others. There's really no silver bullet for that, and I see good recommendations in the other comments. I think the foundation for most documentation is to actually learn *how to write*. I mean, anyone can write a bunch of words and text that makes sense to them, and you can generate a lot of text with modern LLMs, the difficult part is to write something that is *useful* to people so they actually want to read it. Not too high-level, not too low-level, targeting the correct audience, writing succinctly while providing explanations or references when necessary. It's really an art.",
          "score": 1,
          "created_utc": "2026-01-07 22:42:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyconbi",
          "author": "JosephineRoberts_",
          "text": "I donâ€™t actually try to hold the whole system in my head I keep a few â€œjumping-off pointsâ€ and re-learn the rest on demand.\n\nFor me thatâ€™s up-to-date API/event contracts, one or two simple architecture diagrams that show flows and boundaries, and a service catalog or tracing tool that tells me â€œwho calls whatâ€ from real traffic. When I need to work on something, I walk a real user journey end-to-end, follow the contracts, and only update docs where behavior truly changed. Over time you remember the invariants and where to look things up, not every component.",
          "score": 1,
          "created_utc": "2026-01-08 06:10:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd15p7",
          "author": "MathematicianSome289",
          "text": "Take a business and break it down into subdomains. Then, map each capability of your system into these subdomains. Start by showing the inputs and outputs of each subdomain. Just this alone creates a conveyer belt level description of how value flows through the business. Then, show the inputs and the outputs of the capabilities within the subdomain. Altogether this creates a  distilled model that maps technology to the business. It allows stakeholder to ask questions such as â€œwhich subdomains are core, supporting and generic to the business?â€ â€œWhat capabilities do we have in X subdomain?â€. In my practical experience as an architect for large enterprises this works far better than C4 because it is low effort to maintain and reason about and more stakeholders can understand it. It also brings focus and clarity to the parts of the system that essential to the business. You donâ€™t need any fancy tools, nonsense subscriptions, just pure conceptual work.",
          "score": 1,
          "created_utc": "2026-01-08 07:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny81bcb",
          "author": "umlcat",
          "text": "You don't. I have designed several ERP/CRM systems several timers and what I do is I make several UML designs that serve as a map of the entire system ...",
          "score": 0,
          "created_utc": "2026-01-07 16:22:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q6c88a",
      "title": "How are you documenting large event-driven architectures?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q6c88a/how_are_you_documenting_large_eventdriven/",
      "author": "boyneyy123",
      "created_utc": "2026-01-07 10:46:33",
      "score": 24,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "Hi, \n\nMy name is Dave Boyne and I'm the creator of **EventCatalog**, an open-source tool for documenting event-driven architectures (domains, services, events, commands, schemas, etc.).\n\nI've spent the past 4 years (on/off) building this open source project to help people catalog their events (document them).... over the past few years the project has grown, more people are using it and now it does much more (e.g DDD, automated diagrams, integration with schema registries, AsyncAPI, OpenAPI etc....)\n\nIâ€™m posting here because this community regularly discusses the *hard parts* of software architecture, and documentation is one of those things that often starts simple and quietly becomes a real problem as systems grow.\n\nI'm always looking for ways to improve the project, so I'm just curious to learn from you all.\n\nIâ€™d love feedback on things like:\n\n* How do you currently document event-driven architectures?\n* What breaks first as your documentation grows?\n* What *hasnâ€™t* worked for you, even if it sounded good in theory?\n\nIf people are interested, Iâ€™m happy to share links or go deeper in the comments, but mainly Iâ€™d value honest feedback, criticism, or alternative approaches.\n\nThanks in advance ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q6c88a/how_are_you_documenting_large_eventdriven/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "ny79lrm",
          "author": "joelparkerhenderson",
          "text": "Great work, thank you for sharing. I'm your target buyer.   \n  \nHow I currently do it: using architecture-driven documentation tooling such as PlantUML, C4, ArchiMate, TOGAF, trying to shoehorn event specs; the event aspects don't work well enough IMHO.\n\nWhat breaks first: every vendor that tries to claim \"single source of truth\" then can't deliver on the claim well enough to be pragmatic in real world heterogenous environments. What breaks first tends to be in two different major areas: specific-technology (e.g. exact capabilities, necessary integrations) and general-administrative (e.g. year-long procurement processes, regulated-industry compliance).   \n  \nHonest feedback: your website hits many of the problems that I encounter often, such as claiming \"single source of truth\", and doing a 14-day evaluation period which is far too short to be pragmatic-- I much prefer federated approaches and 1-year evaluation period for non-production work. See the book Data Mesh for an excellent approach to the enterprise federation aspects-- I would love to buy a comparable book for Event Mesh.",
          "score": 6,
          "created_utc": "2026-01-07 14:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykzosv",
              "author": "boyneyy123",
              "text": "Thanks for the feedback, yeah I dont really like single source of truth either tbh... I actually removed that. The 14 day trial is interesting and 1 year evaluation.. would think about that.\n\nWill check out the book too!",
              "score": 1,
              "created_utc": "2026-01-09 12:10:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyh8p9p",
          "author": "SirOk748",
          "text": "Whether one understands Event Driven Architecture or not, your solution does address a key issue in large complex systems, which is keeping track of the complexity, particularly for the entire team. For Example, personally, in my projects, I have a good grasp of the infrastructure architecture, but mentally I lose track of the control and data plane.",
          "score": 3,
          "created_utc": "2026-01-08 21:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nykzi8p",
              "author": "boyneyy123",
              "text": "Yeah it's an interesting one. I started focusing on EDA about 4 years ago with this, as a catalog for events... as there wasnt many solutions out there, but its turning into a tool for complex systems... allow people to use DDD etc to document them, seems to be working so far.... Im curious where AI plays a part in this picture going forward too... any ideas?",
              "score": 1,
              "created_utc": "2026-01-09 12:09:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz4iu2q",
          "author": "davidslv",
          "text": "Joining the conversation, EventCatalog looks promising, Iâ€™m going to have a proper read on the website",
          "score": 2,
          "created_utc": "2026-01-12 07:29:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz5iolj",
              "author": "boyneyy123",
              "text": "Great, let me know if you have any questions happy to help.",
              "score": 1,
              "created_utc": "2026-01-12 12:45:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyn8rws",
          "author": "RipProfessional3375",
          "text": "I only read the title and came here to recommend event catalog, but I guess that may not be useful for you.  \n\n\nThe thing most lacking in our documentation is reliable actual usage. The documentation can't keep up with consumers changing or stopping consumption. We need to integrate runtime statistics. Its the most reliable documentation there is.",
          "score": 1,
          "created_utc": "2026-01-09 18:49:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxpop7",
      "title": "Microservices as an Architecture vs. a Management Pattern",
      "subreddit": "softwarearchitecture",
      "url": "https://youtu.be/eEiKM8njGvk",
      "author": "parsaeisa",
      "created_utc": "2025-12-28 11:57:35",
      "score": 21,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pxpop7/microservices_as_an_architecture_vs_a_management/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": [
        {
          "id": "nwe1u8c",
          "author": "SeniorIdiot",
          "text": "That was always the point. Conway's Law and DDD bounded contexts making a baby.",
          "score": 7,
          "created_utc": "2025-12-28 17:04:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4fvri",
      "title": "How much software design is a junior expected to know?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q4fvri/how_much_software_design_is_a_junior_expected_to/",
      "author": "megacrops",
      "created_utc": "2026-01-05 08:01:12",
      "score": 20,
      "num_comments": 16,
      "upvote_ratio": 0.92,
      "text": "Hello all,\n\nI'm going to graduate college in a few months, and join a team at a big bank as a new grad. In big corpos, how much software design is a junior expected to know? I'm talking about OOD, System design, and ability to understand large, complex codebases.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q4fvri/how_much_software_design_is_a_junior_expected_to/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxshjni",
          "author": "mistyharsh",
          "text": "Absolutely none. The most important I feel that a junior needs to understand is SOLID principles and an ability to recognise the code that doesn't follow one. But, that's it. Learning to make it an intuition is the only step that matters in the beginning.",
          "score": 25,
          "created_utc": "2026-01-05 09:37:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvtr3u",
              "author": "iMac_Hunt",
              "text": "Maybe my standards are low but even a basic understanding of SOLID for a junior puts you better than most in my books. \n\nI expect juniors to be able to read/write code somewhat fluently - knowing some design patterns or specific frameworks are bonuses.",
              "score": 9,
              "created_utc": "2026-01-05 20:44:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxwqcf8",
              "author": "megacrops",
              "text": "I see. I feel that I can do at least that much in that case. Do you have any recommendations for wat I can do to be better prepared?",
              "score": 1,
              "created_utc": "2026-01-05 23:21:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxu9vsc",
          "author": "iceburg47",
          "text": "When starting my first entry level position my manager semi-jokingly told me that on day-1 I was just expected to be able to find my way back from the bathroom but to start learning certain things immediately (mostly relevant to our specific domain.)\n\nJust be prepared for expectations to grow quickly but organically.",
          "score": 4,
          "created_utc": "2026-01-05 16:28:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsc3fb",
          "author": "RipProfessional3375",
          "text": "There is 'know' and 'understand' and these two are very different.\n\nI would expect a junior to know about these concepts, know their most important terminology, know enough about them to begin the long process of actually understanding them.",
          "score": 5,
          "created_utc": "2026-01-05 08:44:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybo7pa",
              "author": "ICantLearnForYou",
              "text": "THIS. Modern interviews are testing these concepts. Do a system design prep course like Design Gurus to get started.",
              "score": 1,
              "created_utc": "2026-01-08 02:28:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxva4mt",
          "author": "sennalen",
          "text": "In an interview? All of it. On the job? None of it.",
          "score": 2,
          "created_utc": "2026-01-05 19:13:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvcyhp",
          "author": "christianhelps",
          "text": "When I get new grads I expect them to know basically zero. You should be capable of researching issues on your own and asking for help afterwards. The only time I've ever had issue giving a junior my time and knowledge is when they haven't made a solid effort themselves first.Â \n\n\nAs long as you're willing to learn, seniors will be more than happy to lend you a hand and mentor you. My best advice is to listen hard, people don't like having to repeat themselves and you build report by showing that you follow feedback.",
          "score": 2,
          "created_utc": "2026-01-05 19:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxxdpr",
          "author": "VictorBaird_",
          "text": "Way less than youâ€™re probably worried about. They expect you to write clean code, follow the existing patterns, ask for help early, and not get lost in the repo forever. Basic OOP and being able to reason about trade offs is enough, real system design youâ€™ll learn alongside seniors on the job.",
          "score": 2,
          "created_utc": "2026-01-06 03:11:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxw4j6q",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-05 21:34:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxwpxq9",
              "author": "megacrops",
              "text": "The bank I'm joining is more invested in Tech than most others, I heard that spring boot and python are the two main langs there.",
              "score": 2,
              "created_utc": "2026-01-05 23:19:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxkqpc",
          "author": "tr14l",
          "text": "For loops, conditionals, classes... A few other things. \n\n\nI would expect a solid foundation in CS, whether self taught or not. However, many do not.",
          "score": 1,
          "created_utc": "2026-01-06 02:02:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxychlb",
          "author": "Separate_Earth3725",
          "text": "Probably zero. I know in the US, CS coursework involves at least one software engineering course but for me there is no expectation that you come in knowing what youâ€™re doing.\n\nAt the very least, just be aware of how object orientation works in terms of classes vs objects and very basic inheritance like whatâ€™s a base class and a child class and how do they interact.",
          "score": 1,
          "created_utc": "2026-01-06 04:43:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybmy79",
          "author": "dash_bro",
          "text": "So long they get LLD it's a win. \n\nLLD is very learnable on the job, HLD is through experience once they learn what needs to be learnt in the first place \n\nIn my opinion, very similar to how there are teaching schools for dentistry, surgery, etc. - you learn the theory and then an experienced professional shows you the actual thing on the job. \n\nThis is what I'd recommend, if I had enough time and mental space for coaching juniors actively:\n\nHave juniors pick up the basics (theory, ideas about HLD from a simple 2 week bootcamp etc) -> look at company's existing arch -> sit in design meetings and make notes/questions -> research and answer questions, run by the seniors -> the seniors start involving the juniors for their opinions -> seniors start letting juniors design with just course corrections -> juniors are now mid level or senior",
          "score": 1,
          "created_utc": "2026-01-08 02:21:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyjujhp",
          "author": "Calm__Koala",
          "text": "Fresh outta college? Zero. Takes a handful of years on the job to fully grasp system design.",
          "score": 1,
          "created_utc": "2026-01-09 06:16:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyqt7yt",
          "author": "Admirable_Swim_6856",
          "text": "Not much, I would focus on being able to translate product requirements to good code. System design and the like is the domain of a senior imo.",
          "score": 1,
          "created_utc": "2026-01-10 06:23:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsy1d1",
          "author": "Fippy-Darkpaw",
          "text": "As a senior software engineer, you should know at least what is in the design docs and the training docs.\n\nAny decent workplace would train you up on both. ðŸ‘",
          "score": 1,
          "created_utc": "2026-01-05 11:59:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaqpg2",
      "title": "I keep learning this in system design: one pattern alone rarely gives you a full solution.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qaqpg2/i_keep_learning_this_in_system_design_one_pattern/",
      "author": "Icy_Screen3576",
      "created_utc": "2026-01-12 09:43:00",
      "score": 19,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "I hit this again while working on a **flight search system**.\n\n[Initial State](https://preview.redd.it/p6p8jeei1wcg1.png?width=1205&format=png&auto=webp&s=54838216ce2e0a682f11b5db63475f4af3ce96cb)\n\n# The problem\n\n* Call multiple flight providers\n* Each responds at a different speed\n* Some fail\n* Users expect immediate results\n\nNo single pattern covered all of that.\n\n# What didnâ€™t work\n\n* **Synchronous calls** â†’ blocked by the slowest provider\n* **Async +** `Task.WhenAll` â†’ still waits for everyone\n* **Background threads + polling** â†’ fragile under restarts and scale\n\nEach approach solved part of the problem.\n\n# What worked\n\nThe solution was possible when **combining patterns**, each covering a different concern:\n\n* **Scatterâ€“Gather** â†’ parallel provider calls\n* **Publishâ€“Subscribe** â†’ decouple dispatch from providers\n* **Correlation ID** â†’ track one search across async boundaries\n* **Aggregator** â†’ merge partial responses safely\n* **Async Reply over HTTP** â†’ return immediately\n* **Hexagonal Architecture** â†’ the code structure discipline\n\nTogether, they formed a stable flow.\n\n[Request Flow](https://preview.redd.it/oyqrv50l1wcg1.png?width=4011&format=png&auto=webp&s=d3cee75dbae048339bf7d6e14fa9cea410b9f968)\n\n# User Interface\n\n[Progressive Results](https://preview.redd.it/y1ex8s1n1wcg1.png?width=1201&format=png&auto=webp&s=31a11c964a9907e717c543179aa77f056cfb4fa8)\n\nI uploaded the [code](https://github.com/justifiedcode/flight-search-system) to github for those who want to explore.\n\nâ€” HH",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qaqpg2/i_keep_learning_this_in_system_design_one_pattern/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz9vzq7",
          "author": "halfxdeveloper",
          "text": "I wouldnâ€™t say users want immediate results. Users want immediate feedback. Their frustration comes from not knowing if the website crashed or if there arenâ€™t any results. Iâ€™ve found skeletons arenâ€™t enough. And lazy loading also isnâ€™t enough when the results shuffle with new data intermittently.",
          "score": 7,
          "created_utc": "2026-01-13 01:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbtuui",
              "author": "Icy_Screen3576",
              "text": "Client polling for status with correlation id worked well.",
              "score": 3,
              "created_utc": "2026-01-13 10:07:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbnspu",
          "author": "saravanasai1412",
          "text": "In paper it looks good. Start engineering this for scale. What happens if 10k concurrent users accessing this system. \n\nDoes this works.",
          "score": 1,
          "created_utc": "2026-01-13 09:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbu7sv",
              "author": "Icy_Screen3576",
              "text": "Good idea to test that. It must be a more instances thing.",
              "score": 2,
              "created_utc": "2026-01-13 10:10:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pz3vkb",
      "title": "This is a detailed breakdown of a FinTech project from my consulting career.",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/this-is-a-detailed-breakdown-of-a-fintech-project-from-my-consulting-career-9ec61603709c",
      "author": "trolleid",
      "created_utc": "2025-12-30 00:59:09",
      "score": 18,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pz3vkb/this_is_a_detailed_breakdown_of_a_fintech_project/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nwzv0e9",
          "author": "Mehazawa",
          "text": "Thanks for sharing. What is the advantage of having events in db, over than using the state + cdc to dump them to mq and after save somewhere for auditability?",
          "score": 3,
          "created_utc": "2025-12-31 23:16:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwz62u",
          "author": "4nh7i3m",
          "text": "Thank you for sharing. It helps me to have a better understanding of how the other real system works.",
          "score": 1,
          "created_utc": "2025-12-31 14:07:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q18j30",
      "title": "Where does software architecture fit into backend design process?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q18j30/where_does_software_architecture_fit_into_backend/",
      "author": "tejveeer",
      "created_utc": "2026-01-01 16:44:39",
      "score": 17,
      "num_comments": 12,
      "upvote_ratio": 0.9,
      "text": "Hey, I'm a junior aspiring to be a backend engineer.\n\nI'm currently trying to understand database and api design in greater depth, and now I've encountered software architecture.\n\nHow do these three fit into the product design process?\n\nMy current understanding of the product design process is as follows:\n\n1. Determine product functionality\n2. Translate into requirements and constraints\n3. Design the API (the specifics of which I'm learning through *The Design of Web APIs* by Lauret)\n4. Design the database based on the resources required for the API\n\nWhere does software architecture fit into this? What about system design? What is the relationship of software architecture and system design? When does system design appear in the design process?\n\nSorry for question spamming, would appreciate any pointers on this subject.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q18j30/where_does_software_architecture_fit_into_backend/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nx3saiv",
          "author": "Glove_Witty",
          "text": "What about your non functional requirements and constraints? Performance, availability, scalability, security, maintainability, dev velocity, cost.",
          "score": 10,
          "created_utc": "2026-01-01 17:09:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx43mxj",
          "author": "bobaduk",
          "text": "What you're describing _is_ software architecture and _is_ system design.\n\nThe architecture of a system is the set of decisions we make that are hard to change and constrain future decisions. For example, we might decide that we're building a monolithic golang app with a postgres database, or we might decide that we're building a set of typescript microservices that will run as cloudflare workers. Those decisions will lead to very different constraints in future.\n\nSoftware architecture as a _process_ is the practices we employ to make those hard-to-change decisions. We ideally make those decisions by gathering the functional and non-functional requirements, understanding the constraints, and then making choices that satisfy all our constraints while providing maximum optionality for the future, ie, the ability to change our minds when we realise we got it wrong.\n\nSystem design is ... designing a system... I guess it's possible to design a system without considering the constraints or the ability to change our minds, but I would still call that \"doing architecture\", just doing it badly.\n\nA software architect is someone who leads that process of making decisions and builds consensus and understanding with teams so that everyone is on the same page about the decisions they're making, and the reasons for choosing those decisions.\n\nI think you're looking for clear definitions for things, but that's not how human endeavours work. We're describing processes by which people create a thing together, and that's necessarily messy and without hard edges.",
          "score": 6,
          "created_utc": "2026-01-01 18:07:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3uqwr",
          "author": "Classic_Chemical_237",
          "text": "Itâ€™s not about the current product requirements. Database migration sucks (downtime, chance of bugs, and occasional migration done wrong can completely disable the business for days) so you want to minimize as much as possible. So itâ€™s always good to challenge PM on whatâ€™s the potential direction in the future, and design the DB to be flexible, even if itâ€™s not needed right now.\n\nBe aware of pager duty. Thatâ€™s always a BE responsibility to respond to emergency DB/API downtime. You need proper monitoring with automation to alert you when this happens, and you need a troubleshooting playbook to follow at 3AM when your mind is half asleep.",
          "score": 7,
          "created_utc": "2026-01-01 17:22:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx433ur",
          "author": "gaelfr38",
          "text": "Software architecture and system design are similar in the way I see it.\n\nIt happens when an application grows in features and/or need to communicate with other applications.\n\nIf you start small on a single feature, I think that's why software architecture is unclear to you for now.\n\nSome things you may want to look at: DDD, hexagonal architecture, event-based architecture, micro services, monolith, modulith as we call it now (intermediate between micro services and monolith), ...",
          "score": 3,
          "created_utc": "2026-01-01 18:04:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7sw88",
          "author": "ERP_Architect",
          "text": "A simple way to think about it is that architecture sets the boundaries, and API and database design happen inside those boundaries.\n\nAPIs and schemas are concrete. Architecture is where you decide things like what owns what data, what needs to scale, what can fail, and what should stay simple. Those choices quietly shape how your APIs look and how your database ends up structured.\n\nSystem design usually sits in between. Itâ€™s the step where you sketch the big pieces and their responsibilities before you worry about endpoints or tables. In real projects this isnâ€™t linear. You bounce back and forth as constraints show up.",
          "score": 2,
          "created_utc": "2026-01-02 07:10:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6cnm1",
          "author": "systemic-engineer",
          "text": "None of the answers are wrong.\n\nAll talk tech.  \nWhile the answer is humans. \n\nThis question is one you best ask in your org.  \nFind a trustworthy senior.\n\nNot the 10x engineer.  \nFind the engineer who maintains the team calendar.  \nAnd doesn't groan.  \nBecause someone needs to do it. \n\nAnd then let them explain it to you,\nin the context of the org.\n\nTheory is great.  \nUntil you sit in a meeting room.  \nAnd need to explain it to real humans.",
          "score": 1,
          "created_utc": "2026-01-02 01:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6i52h",
          "author": "who_am_i_to_say_so",
          "text": "Architecture super-high-level is describing how the arrows connect the data together, how the layers interact with each other.\n\nHereâ€™s one little pattern that can carry you for almost forever: Model, View, Controller. MVC.\n\nThereâ€™s your start. Enjoy!",
          "score": 1,
          "created_utc": "2026-01-02 01:48:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6ouki",
          "author": "Purple-Control8336",
          "text": "Solution Architecture should define standards, principles, guardrails(constraints). \n\nSoftware Architecture should translate Solution Architecture (logical) to Technology Architecture(Physical ) level which is softwares we need to build web/mobile/ AI Agentic for Front end, integration, backend, database, cross cutting concerns like login, auditing, logging ,monitoring, unit testing, DevSecOps, infra environment needs, non functional all itties. \n\nIf Solution and Technical Architects roles do not exists and all done by engineers in squad team, then its all over the place.",
          "score": 1,
          "created_utc": "2026-01-02 02:29:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6qamb",
          "author": "tr14l",
          "text": "No, sam, we aren't asking for AGI. We're literally just asking for a definition of it so you stop using it as a blank check marketing term.",
          "score": 1,
          "created_utc": "2026-01-02 02:38:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7pdpn",
          "author": "No_Flan4401",
          "text": "So when building and starting a new project, we need to understand the domain (e.g. shipping), what problems the application should handle and hereafter get requirements and scope the process.Â \nFrom here on you should have a good idea on what you are building. Based on this you start to make choices, everything from programming languages and database to how the application should communicate (there are other options than http protocol).Â \nYou also draw some boxes on a whiteboard to try to slice it up, and decides on how the data models should look.Â \nLastly you decide on if it makes more sense to do a monolith or microservices architecture and what this even means for your application.\n\n\nThen you start building th MVP and keep in mind your probably misunderstood a lot so you make adjustments and the way. Hopefully I did a good enough job so none of the big decisions needs to be changed.Â ",
          "score": 1,
          "created_utc": "2026-01-02 06:40:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsnmqz",
          "author": "mother_fkr",
          "text": "between 2 and 3",
          "score": 1,
          "created_utc": "2026-01-05 10:32:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny67uzp",
          "author": "Borster_91",
          "text": "software architecture, and more general, solution architecture cover 2 full and 3/4 partially until define which kind of datastore your solution needs (sql, no-sql, object store, ...) and the context (where entities goes into)  \n  \nSolution architecture output should be the components and the technologies that can grant the requirements of your system, not the implementation. So for example API can be an outcome but then you can design it as HATEOAS priciple or not.",
          "score": 1,
          "created_utc": "2026-01-07 09:35:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q08rl2",
      "title": "My side project ArchUnitTS reached 200 stars on GitHub",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/my-side-project-archunitts-reached-200-stars-on-github-bb9d47f9b442",
      "author": "trolleid",
      "created_utc": "2025-12-31 09:15:12",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q08rl2/my_side_project_archunitts_reached_200_stars_on/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx0xqld",
          "author": "wampey",
          "text": "Hey cool! Was just reading about such tests in fundamentals of software architecture!",
          "score": 1,
          "created_utc": "2026-01-01 03:24:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9519c",
      "title": "Understanding the Decorator Design Pattern in Go: A Practical Guide",
      "subreddit": "softwarearchitecture",
      "url": "https://medium.com/design-bootcamp/understanding-the-decorator-design-pattern-in-go-a-practical-guide-493b4048f953",
      "author": "priyankchheda15",
      "created_utc": "2026-01-10 14:11:18",
      "score": 14,
      "num_comments": 8,
      "upvote_ratio": 0.7,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q9519c/understanding_the_decorator_design_pattern_in_go/",
      "domain": "medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nyslars",
          "author": "OneCalligrapher7695",
          "text": "Iâ€™m sorry, but this go code is not idiomatic and I wouldnâ€™t advise anyone to implement this pattern.  If you want to become a better go developer, the first thing you should do is *forget design patterns from other languages* and opt for straightforward, *readable* go code using common idioms.\n\n1. The inheritance argument is a straw man.   There is no exploding subclasses problem in go because go does not have inheritance.  You are solving a problem that does not exist in the language.\n\n2. The idiomatic way to decorate single method interfaces in go is through middleware/functional decorators which the article relegates to a footnote.\n\n3. Your toy example looks good in the article, but will break down in reality.  If you need to decorate a larger interface you will have to manually implement and forward every single method to the wrapped object.\n\n4. It makes the program hard to read and understand",
          "score": 16,
          "created_utc": "2026-01-10 14:52:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyuqok8",
              "author": "BaronOfTheVoid",
              "text": "> forget design patterns from other languages and opt for straightforward, readable go code\n\nPfft.\n\nThe best hing about the net/http Handler stack is that it is predestined to for the decorator pattern and if you actually look at community middleware examples they are built like decorators and it is _good_.\n\nPragmaticism wins over ideology.",
              "score": 3,
              "created_utc": "2026-01-10 21:09:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzhe3ey",
              "author": "Dontdoitagain69",
              "text": "Forget GO as well and GO home. Design patterns donâ€™t belong to languages btw",
              "score": 1,
              "created_utc": "2026-01-14 04:14:39",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nysuwjp",
              "author": "Mortale",
              "text": "â€œforget design patterns from other languagesâ€ is so dangerous phrase, Iâ€™d rather not recommend to repeat it. Golang is one of best OOP languages and all OOP patterns are valid and important. \n\nDecorator pattern is great, but given examples areâ€¦ poor? Imagine that you have ChatGPT SDK and two use cases:\n1. Random stupid question where do you wait for stupid response\n2. Asking if the contract / agreement is okay. \n\nInstead of doing two methods: Send() and SendSecure(), you can create decorator for Send() and use it in second scenario only. That way you follow OCP principle and donâ€™t break anything in running code.",
              "score": 1,
              "created_utc": "2026-01-10 15:43:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nytdxmf",
                  "author": "OneCalligrapher7695",
                  "text": "But you see all OO design patterns are not valid or important in the context of go.  In fact many are harmful or unnecessary.  \n\nYou are making the incorrect assumption that because go can model some classical OO features, that go is equivalent to those classical OO languages and therefore the patterns will translate well.  This is a trap. \n\nGo is fundamentally different from those languages.  For example, go does not have classes with constructors, it does not support inheritance, it does not have class hierarchies or relationships via extends or overrides, and polymorphism is implemented through implicitly satisfied interfaces rather than by overriding in derived classes.  \n\nGo does not have these language design defects (or choices) and so the design patterns that were developed to deal with the complexity that arise from those defects simply do not apply.",
                  "score": 5,
                  "created_utc": "2026-01-10 17:14:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzheir9",
          "author": "Dontdoitagain69",
          "text": "https://codingplainenglish.medium.com/10-design-patterns-in-go-that-separate-juniors-from-seniors-ed2bdf70d1bc",
          "score": 1,
          "created_utc": "2026-01-14 04:17:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5m1ms",
      "title": "Iâ€™m evaluating a write-behind caching pattern for a latency-sensitive API.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q5m1ms/im_evaluating_a_writebehind_caching_pattern_for_a/",
      "author": "saravanasai1412",
      "created_utc": "2026-01-06 15:44:10",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Flow  \n\n*   Write to Redis first (authoritative for reads) \n* Return response immediately to reduce latency \n*  Persist to DB asynchronously as a fallback (used only during Redis failure)\n\n  \nThe open question  \n  \nWould you persist to DB using **in-process background tasks** (simpler, fewer moving parts)  \nor use a **durable queue (Celery / Redis Streams / etc.)** for isolation, retries, and crash safety?\n\nAt what scale or failure risk does the extra infra become â€œworth itâ€ in your experience?  \nCurious how other solution architects think about this trade-off.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q5m1ms/im_evaluating_a_writebehind_caching_pattern_for_a/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "ny14i74",
          "author": "YakRepresentative336",
          "text": "IMO the key questions between in-process backgrouns tasks and durable queue will be to determine if data lost, inconsistency and lost write are acceptable, if not go for durable queue",
          "score": 11,
          "created_utc": "2026-01-06 16:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2h2sy",
          "author": "Few_Wallaby_9128",
          "text": "If you tolerate failures (data loss), you could write to a memory ring cache and return immediarely; then asynchronously from the ring cache you would publish to something like a kafka stream and from there finally to redis and/or db. If you dont want data loss, you can drop the ring cache and write to kafka stream with the appropriate configuration. With the ring cache you probably dont need kafka and a less performant durable queue would do too.",
          "score": 3,
          "created_utc": "2026-01-06 20:07:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny172ij",
          "author": "asdfdelta",
          "text": "Caches for APIs that need low latency carries with it an acceptance that *some* calls will pay the cost to fetch the data at some point. In the event of a Redis failure where the entire cache goes down, reloading the entire cache from a db might already be stale depending on how long the manual load takes.\n\nA secondary fallback to a cache is redundant with your fallback to the source of truth, since it will always be more up-to-date than a secondary db.\n\nIf you must have one, then keep the load compute separate to not interfere with the performance.",
          "score": 2,
          "created_utc": "2026-01-06 16:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny3cd34",
          "author": "FrostingLong4107",
          "text": "I am a complete noob and have recently subscribed to this sub. So please pardon my basic questions here. Only intention is to learn here for myself. \n\nOP, Curious to why data is being written to redis first and not to db? Is it only because this data is not needed long term and any failure to insert to db is not an issue? And also more importantly writing to db first and returning response is slower than writing to redis for this API use case?\n\nThe usual pattern I have seen, people write to db and then on a fetch, add a copy to redis cache and on any subsequent query for the same key return the cached copy instead of fetching from db.",
          "score": 2,
          "created_utc": "2026-01-06 22:32:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybryuz",
          "author": "ThigleBeagleMingle",
          "text": "You need a WAL and reconciliation process. \n\nThereâ€™s no mention of SLA, scope/impact, or volume so itâ€™s speculative to suggest implementation \n\nEssentially you persist payload to durable store (eg file system or stream). You donâ€™t want for processing only confirmation storage has replicated value.\n\nhttps://en.wikipedia.org/wiki/Write-ahead_logging",
          "score": 2,
          "created_utc": "2026-01-08 02:48:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nybaeb5",
          "author": "rkaw92",
          "text": "It does not become worth it at all, in most scenarios. Instead, write it directly into Redis only. Redis is now your single source of truth. Set up durability to meet your SLA. Done.\n\n\nRedis not meeting your SLA (e.g. several seconds of data loss not acceptable)? Get a fast but durable DB like ScyllaDB or Cassandra, or use a managed one like DynamoDB. Or maybe consider Volt Active Data. There's a lot of interesting infra to choose from.\n\n\nIf your entities are fairly few but mutating fast, consider an in-memory architecture where there's no read-modify-write loop, only appends. Event Sourcing dovetails with this, but is far from the only choice. In any case, invest early in concurrency control (mutual exclusion) so that a split brain can't ruin your consistency.\n\n\nIn some very rare write-only scenarios, it makes sense to persist to a stream only, like Kafka, NATS JetStream, or Apache Pulsar. This can be great for maintaining a stable latency, but has one major downside: there is no way to know if the operation eventually succeeds or fails. Therefore, it is only useful in fire-and-forget requests, like Web telemetry where the client doesn't really care about getting data back.",
          "score": 1,
          "created_utc": "2026-01-08 01:15:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q7uhuh",
      "title": "Polling vs Websocket Opinion",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q7uhuh/polling_vs_websocket_opinion/",
      "author": "PrivateMattersHelp",
      "created_utc": "2026-01-09 01:23:04",
      "score": 12,
      "num_comments": 19,
      "upvote_ratio": 0.83,
      "text": "Iâ€™m building the UI for an internal app with \\~5 users. There are about two request a day, and concurrent usage is rare (more than one user in UI at same time). The backend is fully serverless (Lambdas) making it more difficult to implement websockets, and a DynamoDB table tracks job status until completion (max \\~5 minutes). Given the low volume, Iâ€™m leaning toward simple polling the dynamodb table for status instead of WebSockets, but the architect in my team wants to go with WebSockets. Any thoughts or gotchas I should consider?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q7uhuh/polling_vs_websocket_opinion/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nyjggwx",
          "author": "markoNako",
          "text": "Maybe you can also take a look at server side events as an 3rd option.",
          "score": 11,
          "created_utc": "2026-01-09 04:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyji68i",
              "author": "PrivateMattersHelp",
              "text": "I'll check it out, thanks!",
              "score": 0,
              "created_utc": "2026-01-09 04:48:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyjhnm2",
          "author": "Leonobrien",
          "text": "5 users, limited concurrency, requirement to update status, presuming a web UI. Unless the design has some greater scale (100's - 1000's users) and distribution (multi-region, multi-service) requirement, just go with the simplest implementation. I'd argue you're best to implement simple, and fast, rather than looking for 'cool' design inclusions - sounds like your architects are over complicating this (though the req's you describe are limited)\n\nPolling will have less additional implementation complexity initially IMO. Hell, a page refresh every x seconds requires the most basic effort.",
          "score": 11,
          "created_utc": "2026-01-09 04:45:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyjhym9",
              "author": "PrivateMattersHelp",
              "text": "Yep, that's what I'm thinking as well. I already implemented the polling and it's working great. But the architect started yelling that thats not a good solution. So I said I would look into it more (I'm new to the team)... These are literally the requirements",
              "score": 2,
              "created_utc": "2026-01-09 04:46:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyjvfsa",
                  "author": "Leonobrien",
                  "text": "I would be asking them to explain why aspect of the solution is not good - you can't just say it's not a good solution. If it satisfies the use case and user experience then against what  criteria are they saying it's not good. Is there an approved standard or pattern, what non-functionals does it not satisfy. Asking them to explain the basis for why will expose over engineering and personal biases.",
                  "score": 4,
                  "created_utc": "2026-01-09 06:24:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykgmcr",
          "author": "Quest4theUnknown",
          "text": "We used to serve hundreds of users using short polling. The use case was fetching progress for an ongoing process. As our requirements and user base grew, we gradually moved to ws but until then, a few API calls per second really werenâ€™t an issue for our database.",
          "score": 3,
          "created_utc": "2026-01-09 09:30:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym7m6v",
          "author": "InfraScaler",
          "text": "You have ample margin to poll fairly often and fail+retry if concurrency is not allowed. Is there some sort or requirement for very low latency that would prevent you from let's say polling every 10 seconds?",
          "score": 3,
          "created_utc": "2026-01-09 16:03:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nypspeh",
              "author": "PrivateMattersHelp",
              "text": "Nope",
              "score": 1,
              "created_utc": "2026-01-10 02:25:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyku1oy",
          "author": "paradroid78",
          "text": "Polling is usually simpler.",
          "score": 2,
          "created_utc": "2026-01-09 11:27:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym8og8",
          "author": "justinhunt1223",
          "text": "Long polling is incredibly simple and easy to implement and works very well. Websockets are great, but obviously require some infrastructure. With such a small user base, I wouldn't hesitate to use long polling.",
          "score": 2,
          "created_utc": "2026-01-09 16:08:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymgab6",
          "author": "TrickyInformation604",
          "text": "ask him 'why'\n\nis it real time? polling has higher latency (depends on the interval. latency and resource efficiency are tradeoff) compared to WS.  \nfor very low requests, polling seems perfect over maintaining WS connections and the over-engineering came from that solution.",
          "score": 1,
          "created_utc": "2026-01-09 16:42:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymvhd2",
          "author": "Qinistral",
          "text": "Are you long polling or short polling? Iâ€™ve worked on systems that do short polling with MILLIONS of users. I donâ€™t remember exact poll volume, but many more than 2 a day lmao. \n\nThere is zero reason to overthink this.",
          "score": 1,
          "created_utc": "2026-01-09 17:50:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzccybe",
          "author": "Wiszcz",
          "text": "All WebSockets/sse/etc are great and work great in test env. But if you want to put in into production, check ALL, and I mean ALL middleware (F5, proxy, apigateway, firewall, etc) if they ALL support that fancy new tech (for them websockets are new tech) and in which version. You could be surprised.",
          "score": 1,
          "created_utc": "2026-01-13 12:43:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyiiefi",
          "author": "Unsounded",
          "text": "How often and how much data is passed back and forth? Websockets are fairly lightweight, but so is polling until itâ€™s not. Both are going to maintain open connections, polling comes with the benefit of simple client retries on network failures and not having to worry about re-establishing upon connection failures. \n\nAPI Gateway can manage websocket connections for you, and integrate natively with lambda. So that isnâ€™t too much of a complication.",
          "score": 0,
          "created_utc": "2026-01-09 01:29:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyindrw",
              "author": "PrivateMattersHelp",
              "text": "I have the job id and the only data i need back is the status, as I want to update it in the UI when it's complete and then get results from completed job from another dynamodb table.",
              "score": 2,
              "created_utc": "2026-01-09 01:55:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q7il9e",
      "title": "Do we really need System Designing?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q7il9e/do_we_really_need_system_designing/",
      "author": "mendu-vada",
      "created_utc": "2026-01-08 17:50:32",
      "score": 11,
      "num_comments": 28,
      "upvote_ratio": 0.68,
      "text": "So, I recently joined as Full Stack Intern in a early startup (3-4 months old).\n\nIt is an product based startup, including me there are 5 members in total.\n\nI don't know why but I found myself really interested in learning system designing.\n\nAlso, I am more focused on backend so maybe it is a common thing.\n\nIt's been more than a month since I have joined them, and I came to know that this guys really don't care about system designing or they really don't understand what and why system design exists.\n\nAfter many meetings with the founder regarding the process and the features needed to built, I used to ask the fellow members (they are just newly passed out guys, they do have internship experience but not senior level type) about how we will manage the traffic of users once the product goes live.\n\nThe product do contains large amount of features, including ML parts also.\n\nThough I also only know about the theory concepts of system design like basic only, but still I suggested them to use different servers to handle the traffic.\n\nEven for 3-4 other topics, i tried to convince them that no doubt we don't need it now but if product gets successful if would definitely.\n\nStill, they neglected me saying everything can be managed on one server only, we will do it.\n\nSo, I am really confused about this thing.\n\nI mean, are they right? Or I am just trying to showcase me as a more knowledgeable person than them?\n\nThe real developers, please share your thoughts.\n\nWon't feel bad even if I get mocked, just a intern mind trying to clear it's path.\n\n(Edit: Thank you everyone who took their time to comment and provide the real guidance which really helped me getting the things clear.\n\nSo, I have came to a point that I should concern more about the system designing once the product gets successful and the traffic coming is really high and things really need to be managed properly.)",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q7il9e/do_we_really_need_system_designing/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nyg46ur",
          "author": "serverhorror",
          "text": "Deciding that you will run on a single server _is_ system design.\n\nWill it work forever? Not if you're successful, then again: Why would you want to introduce the complexities of a distributed system if you don't need it.\n\nThat's not even taking the financial factor into account.",
          "score": 37,
          "created_utc": "2026-01-08 18:44:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymbwta",
              "author": "mendu-vada",
              "text": "So, we should buy the server with better resources first right?\n\nAfter the product gets successful then decide for another server according to the traffic amount.",
              "score": 1,
              "created_utc": "2026-01-09 16:22:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nymgwg2",
                  "author": "serverhorror",
                  "text": "There's not enough information for me to have any opinion about when to buy what.\n\nFor all I know, your story talks about a bunch of interns. The best course of action, that I can see, is to delay decisions so you can gather more experience.",
                  "score": 1,
                  "created_utc": "2026-01-09 16:44:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyg89kc",
          "author": "justUseAnSvm",
          "text": "System design is all about tradeoffs. That's the single most important thing to know, and as you gain experience in how roadmap strategy plays out in terms of business impact, you'll start to get an understanding of what the right tradeoffs are to make and why.\n\nFor right now, the single most important thing to a start up without a product is getting to a working product that customers love (or sales can sell) as quickly as possible. This implies a software development system optimizes for feature velocity. Set the architecture up, and support the development of features which are fast to build, and fast to delete.\n\nA company survives with a product and terrible tech debt, they don't survive if there's no product but a scalable system.",
          "score": 15,
          "created_utc": "2026-01-08 19:02:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymcb8k",
              "author": "mendu-vada",
              "text": "Thank you for commenting, helped me clear my misconceptions.",
              "score": 1,
              "created_utc": "2026-01-09 16:24:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nypp8v9",
                  "author": "justUseAnSvm",
                  "text": "Just in there. This stuff makes a lot more sense with experience, and there's really no way to gain that but concerted effort over time.",
                  "score": 1,
                  "created_utc": "2026-01-10 02:06:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfv9x5",
          "author": "notAGreatIdeaForName",
          "text": "With one big server you can do a lot in the first place, you can add resources without complicating the whole thing very much - this is called vertical scaling.\n\nIf you have multiple servers / replicated apps (horizontal scaling) that increases the complexity a lot and this should be done if needed (this can also mean realistically if it needed after a week just build it right away).\n\nIf you want to be able to develop a large system where the parts are scalable indidually and can be owned by separate teams you can introduce microservices but suddenly you also buy eventual consistency.\n\nAlso there are other factors than just load: Maybe you want to have some sort of fault-tolerance: This can also justify multiple servers or repliations of the apps.\n\nAfter all: Be realistic, you verly likely won't have 100 million users and need 1000 servers that need some automated tooling to manage the servers themselves and you do not need to introduce a lot of problems that need to be taken care of from the start - this becomes important only at a certain stage.\n\nBy now focus on building a great product and not on solving scalability issues that are not likely to happen.",
          "score": 7,
          "created_utc": "2026-01-08 18:06:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygc1lz",
          "author": "Mountain_Sandwich126",
          "text": "TlDR always start with a modular monolith. Brand new start up with a small team does not justify microservices. \n\nAs others pointed out, core principles of system design is understanding trade-offs and making pragmatic decisions that enable the team to deliver at pace.\n\nEven within a monolith there are alot of decisions that need to be made to ensure the system can be maintained long term and evolved.",
          "score": 3,
          "created_utc": "2026-01-08 19:18:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyfw0na",
          "author": "Malacath816",
          "text": "A well designed system is more likely to achieve the desired business value - whether itâ€™s a customer facing app, something internal or something else. \n\nDepending on the industry, regulations, risks and customer types - that increased likelihood could lead to significant profit, or not. Think: the difference between a well architected autopilot in a plane vs one-and-done games on the App Store. \n\nSo depending on many factors, a well designed system can be vital - or not. Itâ€™s one set of tools in the wider toolbox of technology building. \n\nA badly designed system, goes the other way. \n\nThink of it like building a boat. Someone people may just need a raft to cross the river once, so if youâ€™re in the raft building business you donâ€™t need heavy design. Other people need nuclear submarines, which do need heavy design. If I am building rafts, then actually having lots of system design could make it more expensive and less profitable. Or, it could help me build a factory which automates building rafts. \n\nWithout knowing more about your industry and business itâ€™s hard to say, but this could be an opportunity for optimisation or a wasted effort in your specific firm. As a whole economy, many firms require systems thinking.",
          "score": 3,
          "created_utc": "2026-01-08 18:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyfwq2g",
              "author": "Malacath816",
              "text": "Also, on your specific example - monoliths are easy to maintain and build. Generally, stick to the monolith until you canâ€™t anymore. Microservices will overcomplicate a young product.",
              "score": 5,
              "created_utc": "2026-01-08 18:13:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyg10p1",
                  "author": "cstopher89",
                  "text": "Exactly what I think as well. You should do a monolith until you have a reason not to. Microservices are more for organization in large orgs than something needed technically. A well designed modular monolith can scale independently in a similar way if you make the modules in it independent. \n\nImo a startup with a handful of engineers should never do microservices.",
                  "score": 3,
                  "created_utc": "2026-01-08 18:31:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfxy82",
          "author": "YakRepresentative336",
          "text": "Yes we need System Design, \n\nbefore that we need Architecture for the high level concept, \n\nbalancing between cost, depth of features, and so onâ€¦\nlike looking for characteristics *illitites and balance trade-off,\n\nAs startup they maybe want to scale vertically, go easy for fast time to market, because of budget and little customer",
          "score": 3,
          "created_utc": "2026-01-08 18:18:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyg4qxu",
          "author": "Glove_Witty",
          "text": "At the very least you should run the application and the database on separate machines. You should run user traffic through an api gateway or a load balancer. Other things can come later as you get more volume.",
          "score": 3,
          "created_utc": "2026-01-08 18:47:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nygf756",
              "author": "Malacath816",
              "text": "Why? Surely it depends on the constraintsâ€¦?",
              "score": 1,
              "created_utc": "2026-01-08 19:32:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nygfmzo",
                  "author": "Glove_Witty",
                  "text": "Such as? Almost all back end systems are like this (except the get more moving parts as they scale.)",
                  "score": 0,
                  "created_utc": "2026-01-08 19:34:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyqvm3v",
          "author": "_MJomaa_",
          "text": "There is nothing stopping you from having a single Next.js deployment with Inngest as background worker. Actually it makes life a lot easier regarding preview deployments and versioning.\n\nThat's the recommended approach by many to have a modular monolith and it can scale via serverless / fluid compute.\n\nHowever it all depends on the requirements. For example you wanna do some heavy, automated sync in the background, then I would have a dedicated service for that so it will never go down. Or you have VoIP services that are written in C++, no way you can have only one deployment target now.",
          "score": 3,
          "created_utc": "2026-01-10 06:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nygcm9l",
          "author": "No_Pomegranate7508",
          "text": "Would recommend watching this video from Neal Ford (especially starting from 9:20). \n\n[https://www.youtube.com/watch?v=uQ\\_sSC9gAsU](https://www.youtube.com/watch?v=uQ_sSC9gAsU)",
          "score": 2,
          "created_utc": "2026-01-08 19:21:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyj81eg",
          "author": "No_Indication_1238",
          "text": "You are asking good questions. And they are keeping you, correctly, at bay. When you have more clients and more traffic, you will handle it through distributed computing, like you stated. When exactly will you do it? When you actually get the clients. Why not now? Because it's a waste of time without clients. Can you handle all the traffic on one server? Yes, if you don't have a ton of clients. So just listen when they talk.",
          "score": 2,
          "created_utc": "2026-01-09 03:47:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyka0pk",
          "author": "Disastrous_Poem_3781",
          "text": "This is called YAGNI. It means \"you aren't going to need it\".",
          "score": 2,
          "created_utc": "2026-01-09 08:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nym22vk",
          "author": "atika",
          "text": "Every system will have some kind of architecture, whether you are consciously designed it or not. Even with a somewhat competent architect, it might end up being a poor design. \n\nWhat do you think the chances are of being a good design, if â€œit just happensâ€?",
          "score": 2,
          "created_utc": "2026-01-09 15:38:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyr7cq1",
          "author": "nderflow",
          "text": "Don't build things until you know why you need them.  Effort spent on scalability is wasted when you abandon your first product idea.  If you are trying to decide what to spend your capacity on, make sure you have pre-production environments before you even consider scaling out.\n\nAlso, if you are an intern, who is mentoring you?",
          "score": 2,
          "created_utc": "2026-01-10 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nytjich",
              "author": "mendu-vada",
              "text": "There's a girl in the team who gives me the task and end of the day reviews it, also they gave me the permission to work on production after 1 week of training.\n\nI had told them to let me work on production because I have decent knowledge and was bored building todo apps in first training week.\n\nTeam members are great, just i had the misconceptions of adding more servers to the app.\n\nThat's why I posted on reddit, this is my first post and now I knew that if I had to get genuine answers then use reddit.",
              "score": 1,
              "created_utc": "2026-01-10 17:40:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nymh6ky",
          "author": "symbiat0",
          "text": "If you're in a startup then you should know that building what you need for today to get to the next funding milestone is likely gonna be the priority. Often a big integration for an important customer gets prioritized so expect that. Those priorities might mean making some compromises - make sure you document the why of it. Also it *will* change, sometimes a lot. It's rare you will get buy-in to replace and throw away an old design when it will no longer serve future needs but if you can make the case, do the due diligence.",
          "score": 1,
          "created_utc": "2026-01-09 16:46:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q94pv9",
      "title": "API Gateway, BFF, and GraphQL Explained for System Design Interviews",
      "subreddit": "softwarearchitecture",
      "url": "https://javarevisited.substack.com/p/api-gateway-backend-for-frontend",
      "author": "javinpaul",
      "created_utc": "2026-01-10 13:58:08",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q94pv9/api_gateway_bff_and_graphql_explained_for_system/",
      "domain": "javarevisited.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1pyppzt",
      "title": "Recommendations for Postgraduate Programs or MBAs in Software Architecture?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1pyppzt/recommendations_for_postgraduate_programs_or_mbas/",
      "author": "manubecks",
      "created_utc": "2025-12-29 15:51:17",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nIâ€™m looking for recommendations for a postgraduate program or an MBA focused on Software Architecture.\n\nMy main interest is in programs that go beyond theory and cover real-world architectural decision making, such as:\n\nSystem design and architectural patterns\n\nScalability and distributed systems\n\nMicroservices vs monoliths\n\nCloud architecture and trade-offs\n\nDocumentation and communication of architecture\n\nOnline or on-site programs are both fine.\nIf youâ€™ve taken a course yourself or have direct experience with one, Iâ€™d really appreciate your thoughts (pros/cons, depth, applicability in real projects, etc.).\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pyppzt/recommendations_for_postgraduate_programs_or_mbas/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nwtvrj2",
          "author": "Great_Pattern_1988",
          "text": "You might not need a degree. SEI offers a certificate program that covers a lot of what you've listed.  Three courses and an exam.  You should at least look into it.",
          "score": 1,
          "created_utc": "2025-12-31 00:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3in5z",
              "author": "manubecks",
              "text": "Thanks a lot!",
              "score": 1,
              "created_utc": "2026-01-01 16:17:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyniw7",
      "title": "How do you enforce escalation processes across teams?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1pyniw7/how_do_you_enforce_escalation_processes_across/",
      "author": "StartingVibe",
      "created_utc": "2025-12-29 14:23:26",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "In environments with multiple teams and external dependencies, how do you enforce that escalation processes are actually respected?\n\nSpecifically:\n\n* required inputs are always provided\n* ownership is clear\n* escalations donâ€™t rely on calls or tribal knowledge\n\nOr does it still mostly depend on people chasing others on Slack?\n\nLooking for real experiences, not theoretical frameworks.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pyniw7/how_do_you_enforce_escalation_processes_across/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nwk66nc",
          "author": "ERP_Architect",
          "text": "In practice, escalation only works when itâ€™s designed into the workflow, not documented as a process people are expected to remember.\n\nThe teams Iâ€™ve seen succeed do three things consistently. First, they make escalation entry structured. If required inputs arenâ€™t there, the escalation literally canâ€™t be raised. That removes back and forth and Slack chasing. Second, ownership is role based, not person based. A queue or rotation beats â€œask whoever is around.â€ Third, escalations are visible by default. A shared board or system that shows status, age, and next action creates pressure without anyone nagging.\n\nIf escalation depends on calls or DMs, itâ€™s already broken. People will always route around friction. The goal is to make the right path the easiest path, and the wrong one annoying enough that it fades out naturally.\n\nMost real improvements come from tightening intake and visibility, not adding more rules.",
          "score": 6,
          "created_utc": "2025-12-29 15:40:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk8rn7",
              "author": "StartingVibe",
              "text": ">",
              "score": 2,
              "created_utc": "2025-12-29 15:52:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwjvcd6",
          "author": "ProbsNotManBearPig",
          "text": "My experience is only top down enforced processes work to cut across teams. You need actual standardized forms/templates and enforce they are filled out between handoffs as evidence. But if someone at the top isnâ€™t enforcing people follow it, no one will. Or some will, but it doesnâ€™t work if some donâ€™t follow it. I find this to be an issue at larger companies where the only common upper management across teams might be VP level and they arenâ€™t getting involved in these things. So then the solution is some levels of management down the chain need to coordinate and agree and they tend to be in their own worlds at big companies, not focused on coordinating with other directors on processes. \n\nSo all that is to say, itâ€™s real easy in theory, but my experience is itâ€™s hard at big companies where management of all the teams involved are not necessarily tightly synced on processes and their views on enforcing/following them. At smaller companies, the VP of engineering might actually be involved in these things and enforce that the different teams involved actually follow the processes. \n\nJust my experience. Iâ€™m sure some larger companies have their shit together, but I havenâ€™t seen it myself.",
          "score": 2,
          "created_utc": "2025-12-29 14:45:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjwlt3",
              "author": "StartingVibe",
              "text": "This matches what Iâ€™ve seen as well at my company. Defining processes and templates is the easy part. Enforcement is where things break, especially as orgs grow and management attention gets diluted.\n\nCurious: have you ever seen enforcement work *without* constant top-down pressure, or does it always come back to people chasing others?",
              "score": 1,
              "created_utc": "2025-12-29 14:51:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzj71f",
      "title": "Alternatives to apigee? anyone else frustrated just us",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1pzj71f/alternatives_to_apigee_anyone_else_frustrated/",
      "author": "Ron_Swanson_1990",
      "created_utc": "2025-12-30 14:15:51",
      "score": 11,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "We're on apigee and honestly considering other options, the GCP lock-in is annoying since we use multiple clouds, and our bill keeps climbing for features we barely use. Setting up new apis feels overly complicated with all the XML configs, and nobody on our team really understands how half of it works anymore.\n\nThe part that's frustrating is we started using kafka heavily this year and apigee doesn't support it at all so now we're managing two completely separate systems for apis and event streams. Anyone else dealing with this or found alternatives that handle both?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pzj71f/alternatives_to_apigee_anyone_else_frustrated/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nwqga05",
          "author": "SpaceGerbil",
          "text": "Why would an API Gateway support event streams? Two completely different interface implementations. You may want to look at Kong as an API gateway alternative if you still need something enterprise worthy. If you want something no fuss lightweight, I've used Spring API Gateway",
          "score": 8,
          "created_utc": "2025-12-30 14:25:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwr3pu1",
              "author": "gaelfr38",
              "text": "I'm not sure I would ever recommend using this pattern (exposing Kafka through an API GW) but it does exist in several products, like in [Gravitee](https://www.gravitee.io/platform/kafka-gateway)",
              "score": 1,
              "created_utc": "2025-12-30 16:22:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqigkv",
          "author": "Exterminate007",
          "text": "google is clearly pushing everyone towards full gcp with apigeex, the on-prem version is basically abandonware at this point",
          "score": 3,
          "created_utc": "2025-12-30 14:37:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqiiq1",
          "author": "Maleficent_Mine_6741",
          "text": "we switched off apigee, we consolidated everything into gravitee handles both apis and kafka natively in one platform, the no code policy stuff is way easier than apigees xml configs and the developer portal auto generates docs from openapi spec, but real talk gravitee's graphql support is limited and the community is way smaller so troubleshooting obscure issues takes longer but at least we're not juggling two separate systems anymore",
          "score": 4,
          "created_utc": "2025-12-30 14:37:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqidvp",
          "author": "This_Minimum3579",
          "text": "the xml policy configuration in apigee is painful, feels like we're stuck in 2010",
          "score": 1,
          "created_utc": "2025-12-30 14:37:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbbmre",
      "title": "Advice Regarding Databases?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qbbmre/advice_regarding_databases/",
      "author": "Adventurous_Rough792",
      "created_utc": "2026-01-12 23:55:23",
      "score": 10,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "At work I'm developing an internal CRM. I'm using Vue js for the front end and Laravel for the REST API. This CRM has a multitenant structure, so I have a master database and then each user group has its own dedicated database. So far so good. \n\nMy manager told me to use Mongo DB to save the Activity logs and everything related to tasks. He said that MySQL doesn't maintain such a large amount of data and therefore it crashes. \n\nSo now I find myself managing tasks on one side and users on the other. \n\nDo you think this is a good approach? \n\nOr is there a better solution? \n\nHave you had experience with hybrid databases?\n\nThanks for your time",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qbbmre/advice_regarding_databases/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz9gn0i",
          "author": "Leonobrien",
          "text": "I think your manager would be surprised at how far MySQL can go when the structure is deliberate. It has been rock solid for 30yrs.\n\nI would first ask how these logs are intended to be used (what are the use cases). Is it a regulatory requirement, internal reporting etc. this will influence design rationale. \n\nI would then ask how long the activity logs need to be retained for, what volumes are you expect, what growth and how critical they are to the service you are providing. This will influence scale and service need in the design. Simple things as table truncation are easy and effective where long term retention isn't needed. For long time series storage solutions (an option in Postgres) you can even down sample data points over time.\n\nFinally, consider the ongoing operational support. Sure it's easy to spin up a DB service in most cloud environments, but you still need internal support knowledge and experience for the life of the solution. This needs to be factored in.\n\nWith that I would then do some rough options analysis. Why a NoSQL like MongoDB and not Postgres, or logstash, or even using Grafana and graphite.  Understand and discuss the trade offs and then make the decision. Otherwise it's personal opinion that increases code complexity.",
          "score": 5,
          "created_utc": "2026-01-13 00:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzawct1",
          "author": "as5777",
          "text": "Why do you build a crm in 2026 ?!",
          "score": 3,
          "created_utc": "2026-01-13 05:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfmhwc",
              "author": "Adventurous_Rough792",
              "text": "because they asked to do it hahha",
              "score": 1,
              "created_utc": "2026-01-13 22:20:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzi8b3k",
                  "author": "as5777",
                  "text": "Is it core business ?",
                  "score": 1,
                  "created_utc": "2026-01-14 08:18:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzayyj1",
          "author": "Voss00",
          "text": "At work we have Many mysql tables with billions of rows, some even tens of billions (34billion or something is the largest)\n\nStoring this much in the way we do it isn't ideal, but definitely possibly, it just depends on usecase and query patterns.",
          "score": 3,
          "created_utc": "2026-01-13 05:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzaz75t",
          "author": "alien3d",
          "text": "noob manager .MyIsam for log because 0 tranasaction",
          "score": 3,
          "created_utc": "2026-01-13 05:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9q1n6",
          "author": "Doctuh",
          "text": "[Just Use Postgres](https://www.manning.com/books/just-use-postgres)",
          "score": 2,
          "created_utc": "2026-01-13 01:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza5u3i",
          "author": "Aggressive_Ad_5454",
          "text": "Hmm. YouTube relies on MySql. So does Facebook. So does WordPress, globally. Itâ€™s possible your manager is ignorant.",
          "score": 1,
          "created_utc": "2026-01-13 02:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nza96d4",
              "author": "dariusbiggs",
              "text": "No one in their right mind uses MySQL for anything.\n\nBut then we voluntarily work with computers for a living so we cannot possibly be sane.",
              "score": 1,
              "created_utc": "2026-01-13 02:54:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzav8z4",
              "author": "symbiat0",
              "text": "Quite likely actually.",
              "score": 1,
              "created_utc": "2026-01-13 05:03:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzbhwkb",
              "author": "Qinistral",
              "text": "Thatâ€™s misleading. Sure there maybe some minor portions of YouTube that use mySQL but the core functionality uses Google DBs like spammer and big table.\n\nHowever OP obviously doesnâ€™t need Google scale.",
              "score": 1,
              "created_utc": "2026-01-13 08:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzatj7d",
          "author": "Glove_Witty",
          "text": "Welcome to microservices.",
          "score": 1,
          "created_utc": "2026-01-13 04:51:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbnhp3",
          "author": "saravanasai1412",
          "text": "From solution architect point of view there is no answer for these type of questions. The most generic answer is It depends. \n\nAsk questions like why mongo DB why not MySQL. There is no argument that mongo db scale and MySQL wonâ€™t.  Choosing mongo DB only for scale it not right. \n\nAsk him the access pattens and how data is used. My opinion on current approach would introduce more complexity in operations. \n\nIf devs are not used to mongo db itâ€™s a learning curve & maintenance over head. \n\nIf itâ€™s disposable logs. Just use SQL database and delete after retention needs simple. I have seen 5 millions rows in table with 140 columns sql will fly. \n\nTake decision based on your needs team capacity not just because it scales.",
          "score": 1,
          "created_utc": "2026-01-13 09:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdtvxm",
          "author": "frankwiles",
          "text": "My advice would be to to use PostgreSQL as it scales a touch better (not that MySQL canâ€™t do what you need) but it has conditional indexes, partitioned tables, and good JSON support all of which you will find useful on a project like this.",
          "score": 1,
          "created_utc": "2026-01-13 17:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf7q1s",
          "author": "Ok-Scientist9904",
          "text": "i think the right fit here depends on what is the purpose of this activity log. Audit, compliance, regulatory etc. Do you need a front end that pulls up the activity log or is this activity log fed into an observability platform? From my experience MongoDB is good for fast reads, however its not meant for heavy simultaneous writes and can cause issues during heavy load.",
          "score": 1,
          "created_utc": "2026-01-13 21:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfvwvo",
          "author": "HosseinKakavand",
          "text": "Honestly MySQL can handle a lot more than people think â€” the \"it crashes with large data\" thing is usually about missing indexes or bad queries, not the database itself. That said, sharded setups work great if you're disciplined. You just need to be careful to keep them in sync, which is where patterns like Saga can be helpful. It can also be tricky spinning up new DBs on the fly, and requires infra automation. We're discussing these types of multi-step transactional workflows in [here](https://www.reddit.com/r/luthersystems/).",
          "score": 1,
          "created_utc": "2026-01-13 23:07:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa8d5d",
      "title": "From Cloudflare Zero-trust to Tailscale",
      "subreddit": "softwarearchitecture",
      "url": "https://blog.frankel.ch/cloudflare-zero-trust-tailscale/",
      "author": "nfrankel",
      "created_utc": "2026-01-11 19:22:44",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qa8d5d/from_cloudflare_zerotrust_to_tailscale/",
      "domain": "blog.frankel.ch",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q19v0y",
      "title": "Residues: Time, Change & Uncertainty in Software Architecture â€¢ Barry O'Reilly",
      "subreddit": "softwarearchitecture",
      "url": "https://youtu.be/D8qQUHrksrE?list=PLEx5khR4g7PINwOsYrkwz3lTTJUYoXC53",
      "author": "goto-con",
      "created_utc": "2026-01-01 17:38:28",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q19v0y/residues_time_change_uncertainty_in_software/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q4zb3k",
      "title": "How to elegantly handle large number of errors in a large codebase?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q4zb3k/how_to_elegantly_handle_large_number_of_errors_in/",
      "author": "tejveeer",
      "created_utc": "2026-01-05 21:51:14",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "I'm designing a google classroom clone as a learning experience. I realized I don't know how to manage errors properly besides just throwing and catching wherever, whenever. Here are the issues I'm encountering.\n\nRight now I have three layers. The controllers, services, and repositories.\n\nThere might be errors in the repository layer that need to be handled in the service layer, or handled in the controller layer. These errors may be silenced in that place, or propagated up all the way to the frontend. So we need to be concerned with:\n\n1. Catching errors at the right boundary\n2. Propagating them further if necessary\n\nThen there's the issue of creating errors consistently. There will be many errors that are of the same kind. I may end up creating a message for one kind of error in one way, then a completely different error message for the same kind of error in the same file (or service).\n\nSo I would say error management applies to the following targets: creating errors, handling errors at their boundaries, and propagating them further.\n\nFor each target, we need to be concerned with consistency and completeness. Thus we have the following concerns: \n\n1. Error creation\n   1. Have we consistently created errors? \n   2. Have we created the errors necessary? \n2. Error handling\n   1. Have we consistently handled the same kind of errors at their boundaries? \n   2. Have we covered all the errors' boundaries?\n3. Error propagation\n   1. Have we consistently propagated the same kind of errors?\n   2. Have we propagated all the errors necessary?\n\nHow do we best answer these concerns?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q4zb3k/how_to_elegantly_handle_large_number_of_errors_in/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxwg7w9",
          "author": "who_am_i_to_say_so",
          "text": "You have two schools of thought: throwing and trapping/logging the error wherever it happens, and the other is returning the error all the up the call stack to the ui. \n\nSome errors you will want to inform the user. If a user is saving something and thereâ€™s a 500 error, you would want to inform them vaguely of it: â€œwhoops! There was an error. Please try againâ€.  That can be in the controller layer. \n\nOthers you want to happen noisily in the logs to inspect later. Maybe itâ€™s a queue job that can rerun again in an hour, wonâ€™t affect the user experience. Thatâ€™s an example of something going wrong in the service layer.\n\nThere is no one size fits all- it has to fit the situation.",
          "score": 9,
          "created_utc": "2026-01-05 22:30:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwosno",
          "author": "ings0c",
          "text": "This is a good use case for middleware - treat error handling as a cross-cutting concern. Spreading error handling code around your app, as a pattern rather than through necessity, can get very messy.\n\nSay youâ€™re trying to read a user from the DB and it fails.\n\nYouâ€™d probably have something in your infrastructure layer to catch that error, log it and retry, if itâ€™s still failing after retrying it gets thrown to bubble up the call stack.\n\nYour service layer may be able to continue despite the failing query, in which case it can. If it canâ€™t, it also allows the error to bubble up.\n\nThat eventually reaches your middleware and you have something that automatically writes out a 500 with a problem details RFC response and logs the error.\n\nErrors relating to user input should be raised as high up the call stack as possible, close to the source. You donâ€™t want an invalid request being partially processed. As soon as a request comes in, validate it, and return a 400 if itâ€™s invalid - your framework can usually do this for you.\n\nIf a request is valid, then it should only fail due to things like a network error, a dependency being unavailable, etc, which the middleware approach works fine for.\n\nWhy do you find yourself creating many exception classes / messages yourself? Itâ€™s not that this is bad per se, but you donâ€™t need to be catching a DB error and throwing your own custom exception for the same thing, just allow the original exception to bubble up.",
          "score": 3,
          "created_utc": "2026-01-05 23:13:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5ns6v",
      "title": "At what point does ERP customization become technical debt instead of an advantage?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q5ns6v/at_what_point_does_erp_customization_become/",
      "author": "Bestwebhost",
      "created_utc": "2026-01-06 16:47:07",
      "score": 9,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "When we implemented our ERP, we customized heavily to match how the business already operated. At the time, it felt right like  \"why force the business to change for software?\" Now a year later all the upgrades are painful, documentation is messy, and only a few people truly understand how things work under the hood.\n\nSome of the custom logic does give us an edge. But other parts just exist because \"that's how we've always done it,\" even though the original reason is long gone. Now every new request turns into a debate: build another workaround or finally simplify and break habits?\n\nI'm curious how others draw that line. How do you decide which customizations are worth keeping and which should be retired? Do you periodically audit custom logic or does it just accumulate until it becomes a problem? Would love to hear real-world rules of thumb or something like that.\n\nAnd we're getting [Leverage Tech for ERP](https://www.leveragetech.com.au/solutions/erp/) consultation this week, hope they come up with something good. ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q5ns6v/at_what_point_does_erp_customization_become/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "ny20w3e",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 9,
          "created_utc": "2026-01-06 18:53:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny51wva",
              "author": "avm7878",
              "text": "This is why itâ€™s important to have a strong team that can take what users WANT and only deliver what they actually NEED.",
              "score": 1,
              "created_utc": "2026-01-07 04:00:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5eltu",
                  "author": "SolarNachoes",
                  "text": "But over time needs change. Some projects have 10x growth, some get stuck in time and others wither and die and new ones spring up out of nowhere! At some point they are all needed.",
                  "score": 1,
                  "created_utc": "2026-01-07 05:24:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nykxw9o",
              "author": "Bestwebhost",
              "text": "I totally agree with that, old habits die hard and this is a bit of a mess we're in because of that. We do need to move to more standardized systems to simplify things",
              "score": 1,
              "created_utc": "2026-01-09 11:57:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny3o3ux",
          "author": "Euphoric-Usual-5169",
          "text": "I have heard many times that adapting your processes to the ERP is long term the better solution compared to extensive customization. Updates become more and more difficult with customization.",
          "score": 3,
          "created_utc": "2026-01-06 23:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4u1t9",
          "author": "avm7878",
          "text": "For context, I have over 15 years of experience building custom ERP and MES systems. Iâ€™ve also configured complex systems like Salesforce and Workday.\n\nIâ€™m often asked about the debate on build vs. buy. My advice isâ€¦ if you find a system that does what you want out of the box with simple configuration changes, itâ€™s worth buying. The moment you decide to build custom components, youâ€™re entering into vendor-lock and maintenance hell. Itâ€™s better to just build a bespoke system hyper-tailored to your requirements that can grow WITH the business over time.\n\nItâ€™s like buying a suit. If that Walmart suit fits perfectly, then hooray. But once you start trying to alter it to fit (poorly)â€¦ it quickly becomes a better idea to just go get a custom suit.",
          "score": 3,
          "created_utc": "2026-01-07 03:14:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyky3gg",
              "author": "Bestwebhost",
              "text": "My thinking was a balance between custom + off-the-shelf solutions could work best. We'll decide soon",
              "score": 1,
              "created_utc": "2026-01-09 11:59:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nylya9x",
                  "author": "avm7878",
                  "text": "This can absolutely work as long as the boundaries are clear. Even when we built the fully custom ERP, we integrated with off-the-shelf solutions like Teamcenter, Workday, and Tableau. The boundaries for PLM, HR, and reporting were well understood, and therefore easier to maintain and control.\n\nWhen the boundaries are fuzzy, it causes data conflicts and user confusion.",
                  "score": 1,
                  "created_utc": "2026-01-09 15:21:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3c08e",
          "author": "i_be_illin",
          "text": "Almost immediately. Upgrades will be miserable.",
          "score": 1,
          "created_utc": "2026-01-06 22:30:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3fxc1",
              "author": "Frosty_Customer_9243",
              "text": "This is the correct answer. \n\nOne rule to keep in mind against customisation: You are special, but you are not different.",
              "score": 2,
              "created_utc": "2026-01-06 22:49:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5f6j0",
                  "author": "SolarNachoes",
                  "text": "A lot of ERP systems arenâ€™t equipped for on-demand engineering. And some companies have both on-demand and inventory based systems and those get brittle pretty fast.",
                  "score": 1,
                  "created_utc": "2026-01-07 05:28:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny5uj4z",
          "author": "ERP_Architect",
          "text": "From what Iâ€™ve seen, customization turns into technical debt when it stops being intentional and starts being inherited. The moment no one can clearly explain why a piece of logic exists or what breaks if you remove it, upgrades and change become painful by default. How we at Vestra Inet usually think about this is through a real example. We worked with an engineering solutions company whose quoting, pricing, and delivery logic depended on variables like customer location, electrical standards, units of measurement, and highly configurable products. For them, forcing those workflows into a standard ERP would have meant endless patches. A custom ERP made sense because each module had a clear reason to exist and directly supported how the business actually made money.",
          "score": 1,
          "created_utc": "2026-01-07 07:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6f5pf",
          "author": "captain_ankles",
          "text": "Users typically want what theyâ€™ve got already and resist change. EA should be embedding business review and change into the SDLC. I regret not having realised this sooner :(",
          "score": 1,
          "created_utc": "2026-01-07 10:41:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q523tk",
      "title": "researching the best low code development platforms 2026, our devs need to move faster.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q523tk/researching_the_best_low_code_development/",
      "author": "Ancient_Composer2349",
      "created_utc": "2026-01-05 23:39:31",
      "score": 8,
      "num_comments": 23,
      "upvote_ratio": 0.59,
      "text": "our development team is constantly pulled into building simple internal crud apps and admin panels, taking them away from core product work. we're evaluating low code platforms to accelerate this type of development, allowing our devs to focus on complex problems while empowering product managers and business analysts to build simpler tools. we're targeting a 2026 rollout for this new approach.\n\nwe need a platform that offers more power and flexibility than pure no code tools, ideally allowing for custom code (javascript, sql) where needed. it should have strong data modeling, api creation capabilities, and role based security. integration with our existing devops and version control (like git) is important.\n\nwe want to increase our development velocity without sacrificing control. any advice is appreciated.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q523tk/researching_the_best_low_code_development/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxyi02t",
          "author": "NeloXI",
          "text": "Lol, when this fails and actually sets you further back as it always does, do come back and let us know all the funny details.Â ",
          "score": 32,
          "created_utc": "2026-01-06 05:21:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyf25l",
          "author": "nedal8",
          "text": "lul,\n\na tale as old as time.",
          "score": 11,
          "created_utc": "2026-01-06 05:00:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzdzdv",
          "author": "violentlymickey",
          "text": "I would think about how to enable your developers to do this type of work more quickly (via things like templating/cookiecutter, centralized platform with gui, agents etc) rather than non-devs.",
          "score": 11,
          "created_utc": "2026-01-06 10:05:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny3e7ei",
              "author": "HosseinKakavand",
              "text": "Totally agree. The biggest wins weâ€™ve seen come from empowering existing devs, not pushing complexity onto non-devs. The trick is letting engineers focus on domain logic while minimizing glue code, integrations, and orchestration work that consumes cycles. That approach has held up from startups all the way to large orgs like Citi, Allianz, and DLA Piper. We have examples of the approach here: [https://www.reddit.com/r/luthersystems/](https://www.reddit.com/r/luthersystems/)",
              "score": 2,
              "created_utc": "2026-01-06 22:40:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxy0lyw",
          "author": "JadeCikayda",
          "text": "In my experience this never really works - the development team will inevitably own/become involved in \"debugging\" the low code \"apps\"  and it becomes self-defeating - if you want to try this anyway Microsoft's Power Platform is reasonably OK.",
          "score": 16,
          "created_utc": "2026-01-06 03:30:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxqe4e",
          "author": "rwilcox",
          "text": "Can your BA/Product people be trained to work with these tools?\n\nOr are you just handing developers worse tools (because Pat the PO knows when they get frustrated theyâ€™ll have Quinn the developer do it for them, with worse quality tools than Quinn expects).\n\nEdit: but assuming all that, I see a suggestion for MS already, Iâ€™ll throw in a random suggestion that errs more sql than anything else: a data lake with Google Looker might also work",
          "score": 10,
          "created_utc": "2026-01-06 02:33:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzygb7",
          "author": "ERP_Architect",
          "text": "Iâ€™ve seen teams go down this path for the same reason, and the outcome usually depends less on the platform and more on how narrowly itâ€™s scoped.\n\nWhat tends to work is using low code specifically for internal tools that already have well understood patterns. Simple CRUD apps, internal dashboards, approval flows. When the data model is stable and the rules are mostly straightforward, devs get real time back and the business can move faster without constant handoffs.\n\nWhere it starts to break is when these tools slowly drift into core product territory. Once complex logic, performance constraints, or long lived workflows creep in, people either fight the abstraction or start bypassing it with custom code until it becomes hard to reason about.\n\nThe integration points you mentioned matter more than they sound. If versioning, environments, and access control donâ€™t align with how your dev team already works, you end up with shadow systems no one wants to own. Iâ€™ve also seen friction when non devs can build things but not maintain them once the original context is gone.",
          "score": 4,
          "created_utc": "2026-01-06 12:49:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny2zwda",
              "author": "LordWecker",
              "text": "With the biggest issue being; your non-dev staff won't have the experience to recognize when it's growing past its constraints, which means it _will_ grow into something unsustainable.",
              "score": 3,
              "created_utc": "2026-01-06 21:33:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxz3ezo",
          "author": "ruben_vanwyk",
          "text": "Agree with other comments - teach product owners to build their own little tools, that work shouldn't fall on your Product Developers at all.",
          "score": 3,
          "created_utc": "2026-01-06 08:23:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny2mnyn",
          "author": "powdertaker",
          "text": "Congratulations!! You've discovered why SQL was invented in the first place! At the time, it was going to be\"empower anyone\" to query all the data within an organization themselves without having to write a program to do it.. Sounds familiar huh?\n\nSo your requirements are: it should provide maximum flexibility and power, be completely customizable and extensible, integrate with all your current tools and be so simple any PO can do it thus freeing up your developers to do the other 10,000 other things they need to do.\n\nSure. Good luck with that.",
          "score": 3,
          "created_utc": "2026-01-06 20:33:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4jg1y",
          "author": "crunchy_code",
          "text": "ask your devs. not random strangers on the internet. \n\nwhat you want to do will make you progress much slower in the long run. \n\nyour devs and you should pull in the same directions for a unified vision. these quick tools are always proposed by non technical people not wanting to accept and contemplate the complexity of building software while at the same time not trusting your devs opinions. so then better go over their head with these â€œquick and flexible toolsâ€.",
          "score": 2,
          "created_utc": "2026-01-07 02:16:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxywimm",
          "author": "cloudsquall8888",
          "text": "In my old job we used jhipster, which is a generator. Essentially you give it entities and their relationships, and it outputs a spring boot backend and a frontend (you choose either react, angular, or vue iirc) with some user management and admin views with crud. \nIt is customisable, and you can also write your own plugins for more custom generation. \n\nIf your work is simple enough, I guess you could stay \"locked in\" jhipster and keep adding entities and stuff and generating again.\n\nOtherwise, you will need to \"eject\" (iirc this is the term they use), and further development will be like usual. So for this use-case, you would be generating only once at the start (saving you the time of setting up projects).\n\nOf course it is not a silver bullet, it is pretty involved to make it behave exactly how you want it, but down the line you might gain from it. We used it because we too had a lot of projects in the public sector, and wanted to be able to start more quickly.",
          "score": 3,
          "created_utc": "2026-01-06 07:19:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6lpjw",
          "author": "dash_bro",
          "text": "Good luck :))\n\nSeriously though - the better fix might be to either get your PMs be ruthlessly optimising for what they need built (prototypes are NOT MVPs); OR enlist a couple of engineers / block their time to cycle through these tasks.\n\nProtect context switching or get more resources to do this. The low code solutioning rarely works out if team velocity is genuinely dragged down due to these demos.",
          "score": 1,
          "created_utc": "2026-01-07 11:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6pe2s",
          "author": "-TRlNlTY-",
          "text": "Low-code platform means paying expensive professionals specialized in the platform. Unless the scope of the project is pretty restricted and well defined, I'm skeptical of such tools.",
          "score": 1,
          "created_utc": "2026-01-07 12:03:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nypjs36",
          "author": "AnyPainter8566",
          "text": "Have your devs build a a.i. dev to do their work, then fire them all.",
          "score": 1,
          "created_utc": "2026-01-10 01:37:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxe8xu",
          "author": "sleepingthom",
          "text": "Do you already have Office 365? I am biased but I find the Power Platform with SharePoint to be the best. You get intranet, document libraries, lists (SQL tables with built in REST and Graph APIs) permission managementâ€¦ etc. \n\nPower Platform gives you workflow / automation, canvas apps, forms, and Power BI. If you need additional features you can build apps using SPFx which I use a lot to host SPA react apps. \n\nIf you can buy into the whole ecosystem it is fantastic for low code. If you donâ€™t already have O365 licenses itâ€™s probably a tough sell though.",
          "score": 0,
          "created_utc": "2026-01-06 01:27:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxyyd5b",
          "author": "tleipzig",
          "text": "That's what usually happens in larger dev teams, so I think you are in the right track. Try to avoid vendor lockin and libraries that will at some point be in the way of your developers instead of helping them. You can check out Bootify.io which provides all the points you've mentioned based on Spring Boot.",
          "score": 0,
          "created_utc": "2026-01-06 07:36:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzj53u",
          "author": "ssuing8825",
          "text": "Everything is low code now",
          "score": 0,
          "created_utc": "2026-01-06 10:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny1kphh",
          "author": "HosseinKakavand",
          "text": "This really resonates with what weâ€™ve seenâ€“speed without losing control usually means keeping critical logic in code, to get all the best tooling (e.g., versioning, approvals, and auditability), and pushing everything else into the platform. Low code can work for basic CRUD, but it tends to break down once flows span many systems and need more advanced logic and reliable operations. That is where an orchestration layer helps, and why we at Luther chose to keep the process logic as code (what we call the Common Operations script). Luther is built for mega-workflows with workflow templates and hundreds of connectors, to deliver faster. More details are on the Luther Enterprise subreddit: [https://www.reddit.com/r/luthersystems/](https://www.reddit.com/r/luthersystems/)",
          "score": -1,
          "created_utc": "2026-01-06 17:41:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxlv7e",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -13,
          "created_utc": "2026-01-06 02:08:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxoikr",
              "author": "Bodine12",
              "text": "It's amazing how AI-drivel can use so many words to say so little.",
              "score": 8,
              "created_utc": "2026-01-06 02:22:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxy0p7e",
              "author": "JuanPabloElSegundo",
              "text": "Is this some kind of meta comment insinuating to use more AI in development?",
              "score": 2,
              "created_utc": "2026-01-06 03:30:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qc2r07",
      "title": "How do you guys implement a centralized parameter manager for customization in a multi-tenant architecture?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qc2r07/how_do_you_guys_implement_a_centralized_parameter/",
      "author": "Dev_loper031",
      "created_utc": "2026-01-13 20:43:10",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "I'm implementing a modular system based on microservices that will be deployed to multiple environments (tenants), since every client will provide its own infrastructure. In this scenario, i see a need to implement a centralized parameter manager to distribute these variables across the system. How do you usually implement that? Perhaps a simple REST API, or is there a open-source framework you recommend?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qc2r07/how_do_you_guys_implement_a_centralized_parameter/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzfb720",
          "author": "CzyDePL",
          "text": "I don't think you are describing multi-tenancy - it describes when you have multiple clients on a single deployment and need to manage their isolation",
          "score": 4,
          "created_utc": "2026-01-13 21:27:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjukp2",
          "author": "anon10101111",
          "text": "For distributed configuration you may use [https://zookeeper.apache.org/](https://zookeeper.apache.org/)",
          "score": 1,
          "created_utc": "2026-01-14 15:16:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcxc4n",
      "title": "Help regarding a production-ready security architecture for a Java microservices application using Keycloak",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qcxc4n/help_regarding_a_productionready_security/",
      "author": "Gold_Opportunity8042",
      "created_utc": "2026-01-14 19:47:53",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "I am building a microservices-based application that consists of multiple services (service-1, service-2, service-3, etc.), an API Gateway, and a Service Registry. For security, I am usingÂ **Keycloak**.\n\nHowever, I am currently a bit confused about the overall security architecture. I have listed my questions below, and I would really appreciate it if you could share your expertise.\n\n1. From my understanding of the Keycloak architecture: when a client hits our signup or login endpoint, the request should be redirected to Keycloak. After that, everything is handled by Keycloak, which then returns a JWT token that is used to access all protected endpoints. Does this mean that we doÂ **not**Â need to implement our own signup/login endpoints in our system at all?\n2. If my understanding of Keycloak is correct, how can I manage different roles for different user types (for example, Customer and Admin)? I ll have two different endpoints for registering customers and admins, but I am unable to figure out how role assignment and role mapping should work in this case.\n3. Should I use the API Gateway as a single point whereÂ **authentication, authorization, and routing**Â are all handled, leaving the downstream services without any security checks? Or should the API Gateway handle authentication and authorization, while each individual service still has its own security layer to validate the JWT token? what is the standard way for this?\n4. Are there any other important aspects I should consider while designing the security architecture that I might be missing right now?\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qcxc4n/help_regarding_a_productionready_security/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzlroen",
          "author": "Embarrassed-Chain265",
          "text": "0. Maintaining your own keycloak instance is a PITA, you may be better off with a cloud provider in terms of $ and time wasted keeping your keycloak install up to date\n1. Yep, although you will need your own frontend sign up/login pages potentially if you don't use the keycloak defaults\n2. Just add the roles in the keycloak management app (that you will have to host yourself). Or you can add them via API calls from your own services\n3. Authorization should be based on the roles in the JWT inside your app and services, routing in the gateway, and authentication could happen in either\n4. See 0",
          "score": 2,
          "created_utc": "2026-01-14 20:28:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qasu1q",
      "title": "Domain-Composed Models (DCM): a pragmatic middle ground between Active Record and Clean DDD",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qasu1q/domaincomposed_models_dcm_a_pragmatic_middle/",
      "author": "senhaj_h",
      "created_utc": "2026-01-12 11:48:55",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I wrote an article exploring a pattern we converged on in practice when Active Record became too coupled, but repository-heavy Clean DDD felt like unnecessary ceremony for the problem at hand.\n\nThe idea is to keep domain behavior close to ORM-backed models, while expressing business rules in infra-agnostic mixins that depend on explicit behavioral contracts (hooks). The concrete model implements those hooks using persistence concerns.\n\nItâ€™s not a replacement for DDD, and not a defense of Active Record either â€” more an attempt to formalize a pragmatic middle ground that many teams seem to arrive at organically.\n\nThe article uses a simple hotel booking example (Python / SQLAlchemy), discusses trade-offs limits of the pattern, and explains where other approaches fit better.\n\nArticle: [https://medium.com/@hamza-senhajirhazi/domain-composed-models-dcm-a-pragmatic-middle-ground-between-active-record-and-clean-ddd-e44172a58246](https://medium.com/@hamza-senhajirhazi/domain-composed-models-dcm-a-pragmatic-middle-ground-between-active-record-and-clean-ddd-e44172a58246)\n\nIâ€™d be genuinely interested in counter-examples or critiquesâ€”especially from people whoâ€™ve applied DDD in production systems.",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qasu1q/domaincomposed_models_dcm_a_pragmatic_middle/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz5wur5",
          "author": "mexicocitibluez",
          "text": "I read the article and maybe I'm missing something (which is usually the case). Where's the boilerplate in:\n\n1. Loading aggregate from ORM (or repository)\n2. Apply work to that aggregate (behavior encapsulated in aggregate)\n3. Saving\n\nIf what you're after is the ability to coordinate changes between aggregates, then use a saga or state machine. \n\nMaybe it's a language thing, because trying to implement this pattern in C# would result in a much, much more complex app.",
          "score": 3,
          "created_utc": "2026-01-12 14:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbzbhr",
              "author": "senhaj_h",
              "text": "Hello Thank you for your substentive question, i'll try to answer according to what i'm undertanding : \n\nThe loading,Saving aggregate has been omitted on purpose since it wasn't the focus, they are considered given, and they are considered a happy path.\n\nThe pattern i'm describing, it's more when app grows or might grow in complexity then you find yourself implementing DDD either wrong, or too soon, here i'm trying to avoid active record, and in the same time lighten a full pure DDD implementation \n\nThe friction Iâ€™ve seen in practice tends to appear one level *around* a happy path flow, especially in:\n\n* mapping ORM entities â†” domain objects\n* maintaining parallel query/read services for simple aggregate-local facts\n* carrying repositories whose only responsibility is pass-through plus conversion\n\nWhen persistence is relational and stable, that extra indirection often doesnâ€™t buy much *yet*, even though it becomes valuable later as complexity grows.\n\nFor C# i'm not familiar enough with to be able to confirm wether or not it's language thing",
              "score": 2,
              "created_utc": "2026-01-13 10:56:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5r519",
          "author": "CzyDePL",
          "text": "I'll take a look - although I would be more interested in how to create domain models in Django and make models at least a little bit encapsulated",
          "score": 1,
          "created_utc": "2026-01-12 13:37:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q26w95",
      "title": "Outbox vs re-publish job for communication between internal modules",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q26w95/outbox_vs_republish_job_for_communication_between/",
      "author": "0x4ddd",
      "created_utc": "2026-01-02 18:45:41",
      "score": 7,
      "num_comments": 16,
      "upvote_ratio": 0.9,
      "text": "The important part is this consideration is for communication between internal modules and async process status is stored in database.\n\nTypically outbox is used to make sure no events are lost. But outbox has its own cost:\n- amplifies db writes - assume 10k entities inserted per second where each needs to publish an event, now you need to insert 10k additional records to db, which are going to be deleted seconds later by outbox job, so looks like db needs to do 3 times more work (CDC can help a lot though if it is available) - more CPU usage, more IOPS utilization, transactional log burden\n- outbox introduces some additional latency as it typically runs every X seconds\n- implementation with noSQL variants not supporting cross table/collection transactions is more complex than with SQL\n\nFor some cases, outbox or CDC is required - for example where consumer is some other service which does not confirms back.\n\nHowever, in case of communication between internal modules, where you publish event from let's say API layer, then some background process does its own processing and later on publishes success/failure event so API updates its db state and is aware whether process finished or not, what about alternative approach to just have re-publish background job. It queries db and finds unfinished processes with with sone threshold like 5 minute and simply republishes events.\n\nPros:\n- in high throughput systems, much less DB burden (query per X seconds instead of YYYY inserts per second)\n- event publication without delay incurred by outbox/CDC scan leads to better E2E times\n\nCons:\n- not immediately clear whether process is 'hanged' due to failed publication or downstream service failure, if it's downstream failure relublishing will only put more load on downstream service and duplicate events (anyway, idempotent processing should be implemented)\n- usable only when downstream publishes feedback messages at the end of its processing, otherwise no way to know whether 3rd party received event or not\n\nWhat do you think?\n\nFor me:\n- baseline - standard outbox with outbox processor/CDC\n- if you have very good reasons - maybe republishing job could work under specific circumstances \n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q26w95/outbox_vs_republish_job_for_communication_between/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nxauq29",
          "author": "chipstastegood",
          "text": "If the database is a bottleneck, the simple option is to spin up another database instance. The cost of getting another instance pales in comparison to paying developers to design, implement, test, and maintain a more sophisticated solution.",
          "score": 4,
          "created_utc": "2026-01-02 18:58:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxaw63f",
              "author": "0x4ddd",
              "text": "Yeah, good point. If this isn't big ball of monolith where who knows which queries join what, spinning additional db per 'module' or sharding existing db would be easier.",
              "score": 1,
              "created_utc": "2026-01-02 19:05:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxaue9f",
          "author": "elkazz",
          "text": "How is your second method meaningfully different from the outbox pattern?",
          "score": 1,
          "created_utc": "2026-01-02 18:56:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxawe6e",
              "author": "0x4ddd",
              "text": "Only different from technical perspective.",
              "score": 1,
              "created_utc": "2026-01-02 19:06:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxaxuxg",
                  "author": "elkazz",
                  "text": "How so? Sounds like a background process is querying the db every x seconds to find events to publish.. exactly like the outbox.",
                  "score": 1,
                  "created_utc": "2026-01-02 19:13:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxcffdv",
          "author": "yoggolian",
          "text": "I guess the advantage of always-outbox is that things can be processed in a consistent sequence, whereas a replay no-ACK events (or even an outbox-on-failure-to-immediately-send strategy) doesnâ€™t get this without a bunch of work.Â ",
          "score": 1,
          "created_utc": "2026-01-02 23:41:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxdy82w",
          "author": "Glove_Witty",
          "text": "If you are doing 10k transactions per second you really should have some other event bus/streaming system.",
          "score": 1,
          "created_utc": "2026-01-03 05:07:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxew585",
              "author": "0x4ddd",
              "text": "To which I need to publish reliably?\nSo either outbox or some republishing job is required.\n\nUnless I misunderstood you",
              "score": 1,
              "created_utc": "2026-01-03 09:48:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxgn4tt",
                  "author": "Glove_Witty",
                  "text": "Sorry. My bad, I didnâ€™t read your post well enough. You are right about the overhead an outbox places on a database, especially if it is already stressed. \n\nThere are a set of other depending on your technology, business and nonfunctional requirements - eg in your alternative it would work fine in the event you can tolerate out of order messages and the 5 min delay in the rare even that the API commits the DB and fails to publish a message. Other systems are ok dropping messages in this situation.",
                  "score": 2,
                  "created_utc": "2026-01-03 16:32:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxlhmgs",
          "author": "clegginab0x",
          "text": "Something like temporal might work?",
          "score": 1,
          "created_utc": "2026-01-04 08:59:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyoy5u",
      "title": "recommendations on books to architecture",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1pyoy5u/recommendations_on_books_to_architecture/",
      "author": "deadGrimReaper",
      "created_utc": "2025-12-29 15:21:22",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.82,
      "text": "I want to learn strategies to build software like DDD and the architectural patterns like onion or hexagonal, and then implementation patterns like CQRS, I don't want to be confused. is there a book that introduces these hierarchies? or if there are multiple books for each concept. I'm open to other sources like YouTube, too. thanks",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pyoy5u/recommendations_on_books_to_architecture/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nwkq5mz",
          "author": "Alzyros",
          "text": "There's a book recommendation megathread in this sub that's quite helpful",
          "score": 4,
          "created_utc": "2025-12-29 17:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwl62jh",
              "author": "deadGrimReaper",
              "text": "thanks, that was actually helpful",
              "score": 3,
              "created_utc": "2025-12-29 18:28:59",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwv4e0f",
              "author": "Interesting-Good7903",
              "text": "I am new and I am not sure where the megsthread. Mind to share a link?",
              "score": 2,
              "created_utc": "2025-12-31 04:57:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwvvuxx",
                  "author": "Alzyros",
                  "text": "It's one of the community highlights: https://www.reddit.com/r/softwarearchitecture/s/4dE1dXl0fP",
                  "score": 1,
                  "created_utc": "2025-12-31 08:48:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwqbqmu",
          "author": "A_Mosaibh",
          "text": "https://preview.redd.it/rzr6a2zwkcag1.jpeg?width=1179&format=pjpg&auto=webp&s=ac12160c0e9965fa762c4672f2c3441085b3b6eb\n\nThis is a really great book",
          "score": 1,
          "created_utc": "2025-12-30 13:59:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsrdkc",
          "author": "HyperDanon",
          "text": "- \"Hexagonal Architecture\" by Alistair Cockburn\n- \"Enterprise architecture\" by Martin Fowler\n- \"Clean Architecture\" by Robert Martin",
          "score": 1,
          "created_utc": "2025-12-30 21:03:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzz9tp",
      "title": "â€œAgency without governance isnâ€™t intelligence. Itâ€™s debt.â€",
      "subreddit": "softwarearchitecture",
      "url": "/r/u_lexseasson/comments/1pzz8i1/agency_without_governance_isnt_intelligence_its/",
      "author": "lexseasson",
      "created_utc": "2025-12-31 00:57:00",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1pzz9tp/agency_without_governance_isnt_intelligence_its/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "nwvsi1k",
          "author": "Tyhgujgt",
          "text": "It's not that different from people. But you can prompt agent to write down a plan, follow it, test result and then mark which part of the plan was implemented. I'm yet to see a person who can do it this way.Â ",
          "score": 1,
          "created_utc": "2025-12-31 08:16:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxzuw1",
              "author": "lexseasson",
              "text": "I agree with the observation â€” at the individual level, this does look similar to people.\nThe difference shows up at scale.\nHumans get away with not externalizing plans because they share context, intent, and corrective feedback socially. When a person deviates, the blast radius is usually local.\nAgents donâ€™t have that safety net. Once you have multiple agents acting asynchronously across time, tools, and domains, relying on â€œtheyâ€™ll write a plan and follow itâ€ stops working. Not because they canâ€™t â€” but because no one else can inspect, replay, or reason about why something happened later.\nThatâ€™s where governance matters: not as discipline, but as infrastructure. Making intent, assumptions, and success criteria first-class artifacts so decisions remain legible after the fact.\nIn other words, agency without governance isnâ€™t worse than people â€” itâ€™s worse than people at scale.",
              "score": 1,
              "created_utc": "2025-12-31 17:17:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qarbmy",
      "title": "Builder pattern helped clean up messy constructors in my project",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qarbmy/builder_pattern_helped_clean_up_messy/",
      "author": "Possible_Design6714",
      "created_utc": "2026-01-12 10:21:26",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.77,
      "text": "I was working on a part of my system where objects had tons of optional values and a few required ones. I ended up with this giant constructor that was super unreadable and hard to maintain.\n\nSwitched to the *Builder pattern* and wow - the code became way easier to follow: you can chain only the relevant setters and then call `build()` at the end. No more overloads with 7â€“8 parameters, half of which are null half the time.\n\n**Why it helped me:**\n\n* Step-by-step object setup feels more natural.\n* Tests are clearer because itâ€™s obvious what fields youâ€™re setting.\n* Reduces subtle bugs from bad constructor calls.\n\nHas anyone else found design patterns like this helpful in real apps? And do you tend to apply them consciously or just recognize them after they appear in your code?\n\nThoughts? ðŸ‘‡\n\n*Edit:* Iâ€™m using TS/Node, but I know this pattern is classic OOP. Seems like even in modern languages we unknowingly implement similar patterns under the hood. ([Reddit](https://www.reddit.com/r/node/comments/141zf64/how_often_do_you_use_design_patterns_in_your_code/?utm_source=chatgpt.com))\n\nCheckout the full story here: [https://chiristo.dev/blogs/my-tech-pills/series/design-patterns/builder-pattern](https://chiristo.dev/blogs/my-tech-pills/series/design-patterns/builder-pattern)",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qarbmy/builder_pattern_helped_clean_up_messy/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz51ojv",
          "author": "flavius-as",
          "text": "Next step: get rid of that temporal coupling by modelling a state machine with the type system of the language.",
          "score": 5,
          "created_utc": "2026-01-12 10:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzd7z4z",
              "author": "Ok_Zookeepergame1290",
              "text": "Dude, this is the thing that I eventually realized how important this is. Basically 80% of every saas logic are state machines and workflows around them",
              "score": 1,
              "created_utc": "2026-01-13 15:31:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze19mt",
                  "author": "flavius-as",
                  "text": "So, you ready to renounce your love for The Setter?",
                  "score": 1,
                  "created_utc": "2026-01-13 17:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz69cbs",
          "author": "svhelloworld",
          "text": "We use builders all over our test suite. We can pre-populate the builder state with default values and then chain calls to just change the state needed for that test. Removes all sorts of low-grade testing friction.",
          "score": 1,
          "created_utc": "2026-01-12 15:14:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb1jlv",
      "title": "Backend Crud Arch",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qb1jlv/backend_crud_arch/",
      "author": "Double_Ad3148",
      "created_utc": "2026-01-12 17:41:32",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 0.82,
      "text": "Hi everyone. Iâ€™m a junior  developer, currently working alone on a fairly large project. I want to keep the codebase clean, consistent, and built with a solid architecture.\n\nI have a few architectural questions and would really appreciate feedback from more experienced developers.\n\n# 1) Entity / DTO / Response and services\n\nAt the moment, I have many endpoints, and as a result my service layer contains a large number of different DTOs and response classes. This makes the code harder to read and maintain.\n\nIâ€™ve considered several approaches:\n\n* Making services returnÂ **one common DTO**, and mapping it to specific response objects in the controller\n* Or returningÂ **entities directly from services**, and doing the mapping to response objects in controllers (with response classes located near controllers)\n\nThe problem is that when working with entities, unnecessary relations are often fetched, which increases database loadâ€”especially if I always return a single â€œlargeâ€ DTO.  \nAt the same time, according to best practices, services are usually not supposed to return entities directly.\n\nBut what if services always return entities, and mapping is done only in controllers?  \nHow bad (or acceptable) is this approach in real-world projects?\n\nWhich approach is generally considered more correct in production systems?\n\n# 2) Complex business logic and use cases\n\nIâ€™ve been reading books about DDD and Clean Code and tried to reduce the size of my services:\n\n* Part of the business logic was moved into entities\n* Services now look more like use-case scenarios\n\nHowever, some use cases are still quite complex.\n\nFor example:\n\n* There isÂ `UserService.create()`Â which saves a user\n* After that, an email might be sent, related entities might be created, or other services might be called\n\nCurrently, this is implemented using domain events:\n\n    publisher.publish(new UserCreatedEvent(user));\n    \n\nThe downside is that when you open the service code, itâ€™s not always clearÂ **what actually happens**, unless you inspect all the event listeners.\n\nSo Iâ€™m considering another approach:\n\n* `UserService`Â â€” only CRUD operations and repository access\n* `UserUseCaseService`Â â€” orchestration of complex business scenarios\n\nExample:\n\n    userService.create(user);\n    \n    mailService.sendEmail(user.getEmail());\n    userApplicationService.create(user);\n    \n\nThe questions are:\n\n* Is this approach over-engineered?\n* Is it acceptable in production to introduce a separate â€œuse-caseâ€ layer for complex operations?\n\nIâ€™d really appreciate any advice and real-world examples from your experience ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qb1jlv/backend_crud_arch/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz79pgv",
          "author": "Expensive_Garden2993",
          "text": "1. Returning entities from services makes sense, at least I never heard it to be a bad practice.\n\n\n2. It is natural to start using \"use cases\" for orchestration once your services begin to look messy. Just that \"UserUseCaseService\" isn't a proper name for it, use case is a single use case like \"RegisterUserUseCase\".",
          "score": 4,
          "created_utc": "2026-01-12 18:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7qexo",
          "author": "CzyDePL",
          "text": "2) that's the tradeoff between choreography (what I presume you are referring to as current approach in your system - event is published and it's \"handled somewhere\", not knowing where and how) and orchestration (central place coordinating the steps of the process - knows what are possible events and how to handle them - it can be invoking different services/use cases)",
          "score": 3,
          "created_utc": "2026-01-12 19:17:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd52g1",
          "author": "sharpcoder29",
          "text": "Don't do one common dto, too much coupling. Instead, in the places you have over fetching, if it's a read only endpoint, consider a read only query model. This is CQRS. I.e. users/search would just return a UserSearchDto that has id, name. And you have a separate namespace that is ReadModel.Users with a UserQueries class that does what you need. You can use your same ORM in there (different context), or something like Dapper, call a sproc, whatever. Bonus points for using a read-only connection string",
          "score": 3,
          "created_utc": "2026-01-13 15:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcop2v",
          "author": "tr14l",
          "text": "A controller generally just kicks off a process in a handler. Its job is translating the endpoint to the domain and then back to the response. There shouldn't be much logic there as it is only that translation point. Very similar to port interfaces in hexagonal architecture. In fact, you could argue that it's a driving port. \n\nHandlers are the domain orchestrators that stitch together complicated domain logic and calls to driving ports (if that is your architecture and for most web apps I do suggest it, regardless of stack)",
          "score": 2,
          "created_utc": "2026-01-13 13:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7f2fs",
          "author": "never-starting-over",
          "text": "RemindMe! 1 day",
          "score": 1,
          "created_utc": "2026-01-12 18:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7f8qe",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-13 18:26:07 UTC**](http://www.wolframalpha.com/input/?i=2026-01-13%2018:26:07%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/softwarearchitecture/comments/1qb1jlv/backend_crud_arch/nz7f2fs/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fsoftwarearchitecture%2Fcomments%2F1qb1jlv%2Fbackend_crud_arch%2Fnz7f2fs%2F%5D%0A%0ARemindMe%21%202026-01-13%2018%3A26%3A07%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qb1jlv)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-12 18:26:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q9uf2t",
      "title": "Best strategy for removing tenant data at scale",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q9uf2t/best_strategy_for_removing_tenant_data_at_scale/",
      "author": "syscall_cart",
      "created_utc": "2026-01-11 08:44:16",
      "score": 6,
      "num_comments": 14,
      "upvote_ratio": 0.81,
      "text": "We run a multi-tenant SaaS with a fairly large PostgreSQL data warehouse (fact/dimension model). Every table is tenant-scoped via a `\"TenantId\"` column.\n\nWhen a customer churns or requests deletion (GDPR), we remove all of their data across all tables, some of which are very large (multiple GB per tenant). \n\nRight now this is triggered from an Azure Function, but it times out (5 min limit) for large tenants. So weâ€™re redesigning this (changing the timeout to 10 min won't help)\n\nWhat weâ€™re debating is where the deletion logic should live:\n\n**Option A:**  \nHave Postgres do it via a stored procedure / job:\n\n* Discover all tables with `\"TenantId\"`\n* Delete in batches (e.g. `DELETE â€¦ WHERE ctid IN (â€¦ LIMIT N)`)\n* Track progress in a control table\n\n**Option B:**  \nRun our worker in a long running job so it doesn't timeout. Challenge here is we need to build infrastructure around the cleanup job in case it gets killed midway.\n\nWeâ€™re especially worried about:\n\n* Long-running deletes (10â€“60 minutes)\n* Vacuum / bloat\n* Locks\n* Restartability if something crashes\n* Being able to see progress per table\n\nIf youâ€™ve built this at scale:\n\n* Where did you run the deletion loops?\n* Did you use stored procedures, background workers, or something else?\n* Any Postgres-specific gotchas (vacuum, indexes, partitioning, etc)?\n\nLooking for real-world patterns, not theory.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q9uf2t/best_strategy_for_removing_tenant_data_at_scale/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz12o5k",
          "author": "sfboots",
          "text": "If you averaged more than 10gb per tenant in a table, then partition that table by tenant ID so you can just drop that partition",
          "score": 8,
          "created_utc": "2026-01-11 19:56:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0761e",
          "author": "PaulPhxAz",
          "text": "We had to archive billions of rows in our database, it was time based, not tenant ( like delete everything before 2020-01-01 ).  And the time incremented daily -- a rolling archive.  \n\nDo you need it all gone at the same time?  Or if it slowly was removed?\n\nWe figured out about how many rows we could delete in 5 seconds.  That was our basic unit of data to remove at a time. \n\nWe setup a cron job that executed once every 2 minutes to delete that unit of data.  It doesn't matter if it fails once or twice as long as daily we're making progress. \n\nWe didn't order the data, just get X Ids that are available for removal, and then archive those.  \n\nEventually you get through it.  \n\nYou monitor the same way that any cron job is monitored.  We also excluded certain timeframes that were high activity.  We put this logic in a sproc. \n\nFor you, I might make a \"RemoveTenant\" table with the Ids you want to remove and then join to that to figure out what data is eligible. Also, then the UI can write to the table and it no longer becomes your problem.",
          "score": 2,
          "created_utc": "2026-01-11 17:35:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz0z9d6",
              "author": "syscall_cart",
              "text": "that's a neat approach! thank you. the challenge we have is there are 47 tables to delete data from and there is a certain order to follow when deleting data.",
              "score": 1,
              "created_utc": "2026-01-11 19:41:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz1xhcf",
                  "author": "PaulPhxAz",
                  "text": "Isn't that (partially)true no matter what the solution?  You'd have to delete in the correct order.\n\nBut yes, in this solution since you're deleting small chunks at a time you have to make a list of the IDs in a temp table and then join into each table that has a foreign key and delete the related data.\n\nYou'd have a sproc with a Temp table, an insert, then 47 delete statements that join back up and you'd have to know those relations.",
                  "score": 1,
                  "created_utc": "2026-01-11 22:19:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz0yj91",
          "author": "mnp",
          "text": "How about one table per tenant?  Then it's `drop table tenant_12345;` which should be pretty quick.  Then ofc you'd need an index of tenants which would be a row delete.",
          "score": 1,
          "created_utc": "2026-01-11 19:37:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz18502",
              "author": "PrestigiousStrike779",
              "text": "Probably better as a partition than a physical table, but might be a lot of overhead if the tenant data is not very bigÂ ",
              "score": 1,
              "created_utc": "2026-01-11 20:21:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz40gz1",
          "author": "Qinistral",
          "text": "Sounds like you would benefit from a workflow engine. We use Temporal to do work similar to this.",
          "score": 1,
          "created_utc": "2026-01-12 05:01:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxz9el",
          "author": "Blakeacheson",
          "text": "Can you just soft delete and then off-peak you clean up each table independantly?Â ",
          "score": 1,
          "created_utc": "2026-01-11 08:58:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyy0z99",
              "author": "syscall_cart",
              "text": "Thatâ€™s what we do today. Delete operation times out as it takes way too long which is expected as there is a lot of data. Now, simplest is to go with a stored procedure",
              "score": 1,
              "created_utc": "2026-01-11 09:14:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyy9dwy",
                  "author": "Blakeacheson",
                  "text": "Sorry can you elaborate â€¦ right now a request comes in to delete tenant A â€¦ you then run an update on all tables to set deleted_at = now() â€¦ then you have a job that runs off peak every X mins that deletes rows where deleted_at is not null limit N?\n\nCan you confirm thatâ€™s what youâ€™re doing? Because we do that now and itâ€™s extremely effective at massive scaleÂ \n\nYou can also batch the soft delete by creating a table deleted_tenants and have a script set deleted_at = now() limit N where tenant INÂ ",
                  "score": 1,
                  "created_utc": "2026-01-11 10:32:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyy2ajq",
                  "author": "chipstastegood",
                  "text": "Why not use a dedicated server with a queue? This way you wonâ€™t have a time limit on the compute. Not as convenient as a serverless function but it does allow you to scale up the number of nodes if you have a lot of delete requests.",
                  "score": 0,
                  "created_utc": "2026-01-11 09:26:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q2321h",
      "title": "WhatsApp Clone... But Decentralized and P2P Encrypted.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q2321h/whatsapp_clone_but_decentralized_and_p2p_encrypted/",
      "author": "Accurate-Screen8774",
      "created_utc": "2026-01-02 16:27:03",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.8,
      "text": "NOTE: This is still a work-in-progress and partially a close-source project. To view the open source version seeÂ [here](https://github.com/positive-intentions/chat). It has NOT been audited or reviewed. For testing purposes only, not a replacement for your current messaging app. I have open source examples of various part of the app and im sure more investigation needs to be done for all details of this project. USE RESPONSIBLY!\n\nIm aiming to create the \"theoretically\" most secure messaging app. This has to be entirely theoretical because its impossible to create the \"worlds most secure messaging app\". Cyber-security is a constantly evolving field and no system can be completely secure.\n\nIf you'd humor me, i tried to create an exhaustive list of features and practices that could help make my messaging app as secure as possible. Id like to open it up to scrutiny.\n\n[Demo](https://p2p.positive-intentions.com/iframe.html?globals=&id=demo-p2p-messaging--p-2-p-messaging&viewMode=story)\n\n(Im grouping into green, orange and red because i coudnt think of a more appropriate title for the grouping.)\n\nGreen\n\n* P2P - so that it can be decentralized and not rely on a central server for exchanging messages. The project is using WebRTC to establish a p2p connection between browsers.\n* End to end encryption - so that even if the messages are intercepted, they cannot be read. The project is using an application-levelÂ [cascading cipher](https://www.reddit.com/r/crypto/comments/1oi4xqt/multiprotocol_cascading_roundrobin_cipher/)Â on top of the encryption provided by WebRTC. the key sub-protocols involves in the approach areÂ [Signal](https://www.reddit.com/r/signal/comments/1orsjw2/signal_protocol_in_javascript/), MLS and AES. while there has been pushback on the cascading cipher, rest-assured that this is functioning on and application-level and the purpose of the cipher is that it guarantees that the \"stronger\" algoritm comes up on top. any failure will result in a cascading failure... ultimately redundent on top of the mandated WebRTC encryption. i would plan to add more protocols into this cascade to investigate post-quantum solutions.\n* Perfect forward secrecy - so that if a key is compromised, past messages cannot be decrypted. WebRTC already provides a reasonable support for this in firefox. but the signal and mls protocol in the cascading cipher also contribute resiliance in this regard.\n* Key management - so that users can manage their own keys and not rely on a central authority. there is key focus on having local-only encryption keys. sets of keys are generated for each new connection and resued in future sessions.\n* Secure signaling - so that the initial connection between peers is established securely. there are many approaches to secure signaling and while a good approach could be exchanging connection data offline, i would also be further improving this by providing more options. its possible to establish a webrtc connection without a connection-brokerÂ [like this](https://github.com/positive-intentions/chat/issues/6).\n* Minimal infrastructure - so that there are fewer points of failure and attack. in the Webrtc approach, messages can be sent without the need of a central server and would also work in an offline hotspot network.\n* Support multimedia - so that users can share animations and videos. this is important to provide an experience to users that makes the project appraling. there is progress made on theÂ [ui component library](https://ui.positive-intentions.com/)Â to provide various features and functionality users expect in a messaging app.\n* Minimize metadata - so no one knows whoâ€™s messaging who or when. i think the metadata is faily minimal, but ultimately is reletive to how feature-rich i want the application. things like notification that a \"user is typing\" can be disabled, but its a common offering in normal messaging apps. similarly i things read-reciepts can be a useful feature but comes with metadata overhead. i hope to discuss these feature more in the future and ultimately provide the ability to disable this.\n\nOrange\n\n* Open source - moving towards a hybrid approach where relevent repositories are open source.\n* Remove registration - creating a messaging app that eliminates the need for users to register is a feature that i think is desired in the cybersec space. the webapp approach seems to offer the capabilities and is working. as i move towards trying to figure out monetization, im unable to see how registration can be avoided.\n* Encrypted storage - browser based cryptography is fairly capable and its possible to have important data like encryption keys encrypted at rest. this is working well when using passkeys to derive a password. this approach is still not complete because there will be improvements to take advantage of the filesystem API in order to have better persistence. passkeys wont be able to address this easily because they get cleared when you clear the site-data (and you lose the password for decrypting the data).\n* User education - the app is faily technical and i could use a lot more time to provide better information to users. the current website has a lot of technical details... but i think its a mess if you want to find information. this needs to be improved.\n* Offline messaging - p2p messaging has its limitations, but i have an idea in mind for addressing this, by being able to spin up a selfhosted version that will remain online and proxy messages to users when they come online. this is still in the early stages of development and is yet to be demonstrated.\n* Self-destructing messages - this is a common offering from secure messaging apps. it should be relatively simple to provide and will be added as a feature \"soon\".\n* Javascript - there is a lot of rhetiric against using javascript for a project like this because of conerns about it being served over the internet. this is undestandable, but i thinkÂ [concerns can be mitigated](https://www.reddit.com/r/CyberSecurityAdvice/comments/1ev5kqn/is_this_a_secure_messaging_app/). i can provide a selfhostable static-bundle to avoid fetching statics from the intetnet. there is additional investigation towards using service workers to cache the nessesary files for offline. i would like to make an explicit button to \"fetch latests statics\". the functionality is working, but more nees to be done before rolling out this functionality.\n* Decentralized profile: users will want to be able to continue conversations across devices. It's possible to implement a p2p solution for this. This is an ongoing investigation.\n\nRed\n\n* Regular security audits - this could be important so that vulnerabilities can be identified and fixed promptly. security audits are very expensive and until there is any funding, this wont be possible. a spicier alternative here is an in-house security audit. i have made attempts to create such audits for the signal protocols and MLS. im sure i can dive into more details, but ultimately an in-house audit in invalidated by any bias i might impart.\n* Anonymity - so that users can communicate without revealing their identity is a feature many privacy-advocates want. p2p messages has nuanced trandoffs. id like to further investigate onion style routing, so that the origins can be hidden, but i also notice that webrtc is generally discourage when using the TOR network. it could help if users user a VPN, but that strays further from what i can offer as part of my app. this is an ongoing investigation.\n\nAiming to provide industry grade security encapsulated into a standalone webapp. Feel free to reach out for clarity on any details.\n\n [Demo](https://p2p.positive-intentions.com/iframe.html?globals=&id=demo-p2p-messaging--p-2-p-messaging&viewMode=story)\n\nIMPORTANT NOTE: It's worth repeating, this is still a work in progress and not ready to replace any existing solution. Provided for testing, demo and feedback purposes only.",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q2321h/whatsapp_clone_but_decentralized_and_p2p_encrypted/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1q06qzw",
      "title": "We â€œsolvedâ€ C10K years ago yet we keep reinventing it",
      "subreddit": "softwarearchitecture",
      "url": "https://www.kegel.com/c10k.html",
      "author": "Digitalunicon",
      "created_utc": "2025-12-31 07:07:51",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q06qzw/we_solved_c10k_years_ago_yet_we_keep_reinventing/",
      "domain": "kegel.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q0sq32",
      "title": "Plugin system that similar to Figmaâ€™s one",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1q0sq32/plugin_system_that_similar_to_figmas_one/",
      "author": "voldaew",
      "created_utc": "2026-01-01 01:39:59",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.86,
      "text": "I want to build plugin system that should be run on the web without DOM access. It should live in sandbox for security. Imagine an predefined UI component which is like a software function, it takes arguments and it returns values.\n\nconst example = (params) => values\n\nI need an architecture to allow developer that can create their own functions in the UI.\n\nHave you ever built plugin system for web projects? Please let me know your experiences and know-how.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q0sq32/plugin_system_that_similar_to_figmas_one/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nx196w0",
          "author": "mistyharsh",
          "text": "It is nearly impossible to do it. You can try patching window, globalThis and other global objects to prevent access but you cannot do it reliably across multiple plugins.\n\nIf security is paramount, then you have two choices. Put all the external/plugin scripts into a web worker or load them via invisible iframe. Then, you can use the post message API to communicate back and forth.\n\nAlso note that Figma is quite a sophisticated system. It has its own canvas based renderer and doesn't really use DOM, so they can control the API surface.",
          "score": 2,
          "created_utc": "2026-01-01 04:45:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx0tkyb",
          "author": "asdfdelta",
          "text": "Sounds like just an imported script.\n\nWrap your code in a self-invoking scope so that no outside code can inspect or interact with it. Lots of tools and companies use that, easiest way to achieve it is to minify your code. It usually adds the self-invoking scope around it, on top it obfuscates the code itself (which cannot be protected from viewing).\n\nOtherwise maybe a service worker or web component.\n\nGood luck!",
          "score": 1,
          "created_utc": "2026-01-01 02:55:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0kb92",
      "title": "why does metric high cardinality break things",
      "subreddit": "softwarearchitecture",
      "url": "/r/devops/comments/1q0kaqi/why_does_metric_high_cardinality_break_things/",
      "author": "nroar",
      "created_utc": "2025-12-31 18:45:38",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q0kb92/why_does_metric_high_cardinality_break_things/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1q1r1zc",
      "title": "Patching: The Boring Security Practice That Could Save You $700 Million",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/patching-the-boring-security-practice-that-could-save-you-700-million-4d8f8b4b56a1?source=user_profile_page---------2-------------e997ef2a34b8----------------------",
      "author": "trolleid",
      "created_utc": "2026-01-02 06:11:54",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1q1r1zc/patching_the_boring_security_practice_that_could/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nx7q0ge",
          "author": "gbrennon",
          "text": "Later jll read this. Im already in bed",
          "score": -3,
          "created_utc": "2026-01-02 06:45:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}