{
  "metadata": {
    "last_updated": "2026-01-28 16:59:58",
    "time_filter": "week",
    "subreddit": "softwarearchitecture",
    "total_items": 20,
    "total_comments": 125,
    "file_size_bytes": 182607
  },
  "items": [
    {
      "id": "1qol5xl",
      "title": "Have we reached \"Peak Backend Architecture\"?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qol5xl/have_we_reached_peak_backend_architecture/",
      "author": "Brief_Ad_5019",
      "created_utc": "2026-01-27 17:56:22",
      "score": 286,
      "num_comments": 54,
      "upvote_ratio": 0.94,
      "text": "I‚Äôve been working as a Software Architect primarily in the .NET ecosystem for a while, and I‚Äôve noticed a fascinating trend: The architectural \"culture war\" seems to be cooling down.\nA few years ago, every conference was shouting \"Microservices or death.\" \nToday, it feels like the industry leaders, top-tier courses, and senior architects have landed on the same \"Golden Stack\" of pragmatism. It feels like we've reached a state of Architectural Maturity.\n\nThe \"Modern Standard\" as I see it:\n - Modular Monolith First (The Boundary Incubator): This is the default to start. It‚Äôs the best way to discover and stabilize your Bounded Contexts. Refactoring a boundary inside a monolith is an IDE shortcut; refactoring it between services is a cross-team nightmare. You don't split until you know your boundaries are stable.\n\n- The Internal Structure: The \"Hexagonal\" (Ports & Adapters) approach has won. If the domain logic is complex, Clean Architecture and DDD (Domain-Driven Design) are the gold standards to keep the \"Modulith\" maintainable.\n   \n - Microservices as a Social Fix (Conway‚Äôs Law): We‚Äôve finally admitted that Microservices are primarily an organizational tool. They solve the \"too many cooks in the kitchen\" problem, allowing teams to work independently. They are a solution to human scaling, not necessarily technical performance.\n\n - The \"Boring\" Infrastructure:\n   * DB: PostgreSQL for almost everything.\n   * Caching: Redis is the de-facto standard.\n   * Observability: OpenTelemetry (OTEL) is the baseline for logs, metrics, and traces.\n\n - Scalability ‚Äì The Two-Step Approach:\n   * Horizontal Scaling: Before splitting anything, we scale the Monolith horizontally. Put it behind a load balancer, spin up multiple replicas, and let it rip. It‚Äôs easier, cheaper, and keeps data consistency simple.\n   * Extraction as a Last Resort: Only carve out a module if it has unique resource demands (e.g., high CPU/GPU) or requires a different tech stack. But you pay the \"Distribution Tax\": The moment you extract, you must implement the Outbox Pattern to maintain consistency, alongside resiliency patterns (circuit breakers, retries) and strict idempotency across boundaries.\n\nIs the debate over?\nIt feels like we‚Äôve finally settled on a pragmatic middle ground. But I wonder if this is just my .NET/C# bubble.\n\nI‚Äôd love to hear from other ecosystems:\n - Java/Spring Boot: Does the Spring world align with this \"modern standard\"?\n - Node.js/TypeScript: With the rise of frameworks like NestJS, are you guys also moving toward strict Clean Architecture patterns, or is the \"keep it lean and fast\" vibe still dominant?\n - Go/Rust: Are you seeing the same push toward Hexagonal patterns, or does the nature of these languages push you toward a more procedural, \"flat\" structure?\n\nIs there a \"Next Big Thing\" on the horizon, or have we actually reached \"Peak Backend Architecture\" where the core principles won't change for the next decade?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qol5xl/have_we_reached_peak_backend_architecture/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o22ervg",
          "author": "OtterZoomer",
          "text": "This discussion also reminds me of how the initial hype around NoSQL caused a lot of us to misstep and try to wholesale use only NoSQL due to its scaling advantage.\n\nThe problem I think many of us then ran into is that while there is some data that's well suited to simple key-value lookup scenarios, \\*most\\* data is actually highly-relational in nature.  And so this means that if you're hell-bent on \\*only\\* using NoSQL because you are religious about your horizontal-scaling story, you end up having to piece-together a poor-man's version of SQL on top of NoSQL, which ends up being a Frankensteinian mess as well as a metaphorical poor reinvention of the wheel.\n\nI learned it all the hard way <sigh>.  And I eventually just settled on PostgreSQL, as you did.  :)",
          "score": 39,
          "created_utc": "2026-01-27 18:50:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o289fn7",
              "author": "Petesaurus",
              "text": "Very inexperienced, is there a technical reason that the horizontal scaling features of NoSQL couldn't be applied to a relational DBMS?",
              "score": 1,
              "created_utc": "2026-01-28 15:29:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22670k",
          "author": "general_dispondency",
          "text": "This ebbs and flows every 10 or so years. Currently monolith is on top, in a few years microservices will come back rebranded as something else (see SOA), then the added complexity of orchestration will drive everyone back to the monolith. The worst part of this industry is that no one studies the past and looks at how we got where we are. Everyone wants to do the new cool thing, not realizing that we've already done that, and there's a reason that only the people who need it still do it.",
          "score": 108,
          "created_utc": "2026-01-27 18:14:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22ekb3",
              "author": "robhanz",
              "text": "I think this is less \"monolith on top\" and more \"monolith with internal boundaries as an intermediate step towards microservices, *if* they become necessary.\"\n\nIt's a good pattern, because it's one based on evolution, not on a single architecture that will never change.  It honors the changing requirements of services while also creating enough structure to avoid problems going forward.",
              "score": 49,
              "created_utc": "2026-01-27 18:49:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o2286lm",
              "author": "Brief_Ad_5019",
              "text": "Very true! Cloud Providers clearly love to help you with all the complexity and sell you serverless or something. Even if this complexity is often more accidental than essential ;)",
              "score": 14,
              "created_utc": "2026-01-27 18:22:33",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o275hty",
              "author": "randoomkiller",
              "text": "This makes me think, I like to study history, and therefore notice repeating patterns but how can someone at the age of 26 and 1.5YoE see the past 20-30 years of trends and general large overview history of software infra/approaches/why they failed/why some succeeded?",
              "score": 2,
              "created_utc": "2026-01-28 11:47:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o28ebqf",
                  "author": "general_dispondency",
                  "text": "Read and talk to people you work with that have been doing it a long time and learn from their wisdom. You don't have to be their best friend or agree with any personal choices they make. Just be friendly and ask questions that make it sound like you've done a small amount of homework. As someone who's done this for a while, there's nothing I enjoy more then sharing my passion with other people who are just getting into it. Not everyone is like that, but you probably work with one or more people who have that same mindset.",
                  "score": 2,
                  "created_utc": "2026-01-28 15:51:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2281e8",
          "author": "OtterZoomer",
          "text": "One thing I don't see people talking about much is how incredibly \\*high\\* the ceiling now is on vertical scaling.  In the past (15 yrs ago) if you wanted decent scalability you were forced to go horizontal. But that's really not the case anymore.  A single compute can now be outfitted with four top-end EPYC CPUs with 2TB ECC memory and that machine can run 1536 \\*concurrent\\* threads, theoretically all sharing the same process address space of memory (no process context switch and TLB switching for virtual memory).  Not only is this equivalent to 1536 computers of the past (a cloud in a single box), but that shared memory between threads is an enormous advantage in two ways: 1) performance; 2) simplicity of architecture (sharing memory between threads enables much simpler code versus distributed message passing across nodes) - the state is in a single place versus spread about, and access to it requires simple synchronization primitives rather than more complex message passing etc.\n\nAnd this vertical ceiling continues to rise.",
          "score": 51,
          "created_utc": "2026-01-27 18:21:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o229zmh",
              "author": "Brief_Ad_5019",
              "text": "Fascinating fact and numbers! Just think about NVIDIA telling us a whole row in a Datacenter \"behaves like a single GPU\"!\nIt would be super interesting to see how many millions of users you could serve with vertical scalability...",
              "score": 10,
              "created_utc": "2026-01-27 18:30:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22ch3g",
                  "author": "OtterZoomer",
                  "text": "Exactly.  I bought into the whole serverless/microservices hype a few years ago and through hard experience learned the many cons of that approach.  Given the incredibly high ceiling on vertical scaling, my stack is now:\n\nGo API (incredible for network services) - network requests are handled efficiently with both async and threading, maximizing hardware utilization.  My implementation stays simple because it's a single process, yet it scales to incredible levels with threading and a massively multi-core machine.  Yeah you don't start out with the max provisioned machine - you start out cheap and periodically provision more resources as needed.  You can start using a cheap VPS, allocating more RAM and faster CPU and more cores as needed.  Eventually you migrate over to a physical host.  It does mean you need a redundancy story for PostgreSQL but that can be done in multiple ways.  If you don't need 5-6 9's uptime (if you're not a bank) then it's reasonable for you to have a few minutes of downtime at 3am in your most populous user zone for an occasional re-provisioning reboot.  And you can still do load balancing (use NGINX) for the more static-read type of stuff.  Using PostgreSQL as a standard edge, and a host/VPS as your environment, makes it so easy to avoid vendor lock-in too.  You can easily move your solution about to various alternate hosting configs.\n\nAnd it can be a hell of a lot cheaper!",
                  "score": 10,
                  "created_utc": "2026-01-27 18:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o25rrwz",
              "author": "truechange",
              "text": "This is true but I think the use case is more on redundancy / HA.",
              "score": 3,
              "created_utc": "2026-01-28 04:54:11",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o25lo83",
              "author": "missedalmostallofit",
              "text": "And the database?  You systematically put it on different server?  For an app with 1000 users.  Or you increase THE power of one beast?",
              "score": 2,
              "created_utc": "2026-01-28 04:15:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22ej64",
          "author": "iMac_Hunt",
          "text": "To be honest even a monolith with hexagonal pattern + DDD is overkill for a lot of applications out there. Transaction boundaries become tricky and getting many teams to abide to it well can be an uphill battle. I am not saying don‚Äôt do it but it shouldn‚Äôt be the default.\n\nWe have not reached peak backend architecture and we never will because all software is different in terms of needs, size and complexity.",
          "score": 22,
          "created_utc": "2026-01-27 18:49:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22i833",
              "author": "Brief_Ad_5019",
              "text": "Agree!\nI learned the problems with transaction boundaries with domain events being processed eventually consistent the hard way: My FE reloaded an overview directly after the POST call returned -  and I got data that was not yet fully updated.. so what to do? Timeout? üòÇ Websockets? Or just process domain events before returning?\n\nThat's why I intentionally added to use DDD only for domains that are actually complex enough.",
              "score": 4,
              "created_utc": "2026-01-27 19:05:00",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o23q65d",
              "author": "Emotional-Dust-1367",
              "text": "Why does monolith make transaction boundaries tricky?",
              "score": 2,
              "created_utc": "2026-01-27 22:21:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o244yoa",
                  "author": "iMac_Hunt",
                  "text": "The monolith part is fine, I‚Äôm referring to hexagonal and DDD",
                  "score": 2,
                  "created_utc": "2026-01-27 23:35:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o23av1u",
          "author": "lIIllIIlllIIllIIl",
          "text": "I do believe C#/.NET lives in a bubble.\n\nVery few ecosystems are as homogeneous as .NET, and altough homogeneity has some advantages, it has drawbacks too, the main one being stagnation.\n\nFrameworks have a [\"cost of convenience\"](https://surma.dev/things/cost-of-convenience/) to them. They can make some things easier, but it usually comes at the cost of making other things harder.\n\nMost micro-services have ill defined boundaries, and people constantly argue over whether services should be brought together or separated.\n\nConway's law has been [misinterpreted into the opposite of what he meant](https://youtu.be/5IUj1EZwpJY?si=je5ZPNPbuf_9jjTz). Conway wanted to warn people about the dangers of rigid communication structures creating arbitrary fractures in code where there shouldn't be one, but nowadays people use Conway's law to justify creating the very fractures he was trying to prevent.\n\nSOLID, DDD, Clean Architecture & Clean Code are all vaguely defined principles that don't objectively increase productivity and reduce defects. The amount of ceremony required to adhere to these subjective principles often outweigh the benefits they might bring.\n\nI could go on, but I think you get the point. I don't think we reached peak architecture yet, and doubt we ever will. Every piece of software is different.",
          "score": 9,
          "created_utc": "2026-01-27 21:13:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27pebj",
              "author": "HateToSayItBut",
              "text": "Microsoft is an insulated bubble. No modern, innovative startup says \"let's start building with .Net!\"",
              "score": 1,
              "created_utc": "2026-01-28 13:51:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o22hbg1",
          "author": "atika",
          "text": ">Horizontal Scaling: Before splitting anything, we scale the Monolith horizontally. Put it behind a load balancer, spin up multiple replicas, and let it rip. It‚Äôs easier, cheaper, and keeps data consistency simple.\n\nAll the scalability \"best practices\" that architects swear by today, were born when the most powerful server box you could get had two dual core CPUs and memory was measured in megabytes.  \n\nRight now, we have CPUs with hundreds of cores per socket, memory in terrabyte sizes, and hyperconverged storage solutions to easily reach petabytes using prosumer hardware.\n\nStart designing your systems to run on modern hardware.",
          "score": 7,
          "created_utc": "2026-01-27 19:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22jz1u",
              "author": "Brief_Ad_5019",
              "text": "Definitely a key takeaway reading though the comments.\nI still think that horizontal scalability can be helpful for separations of concerns: you can make sure that your sync calls are not getting affected by async work. Or keep your /reports routes away from /overview.",
              "score": 1,
              "created_utc": "2026-01-27 19:12:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22cq9k",
          "author": "mountainlifa",
          "text": "Great post and agree üíØ. Currently refactoring a Frankenstein micro services app built upon AWS serverless into a monolith. It took months just to understand what the heck was going on. Lambdas calling lambdas, Rube Goldberg would be proud! We're likely using fastapi containerized on ecs. As a former SA at AWS I think a lot of noise has gone away because Amazon, Microsoft etc has fired a lot of their \"dev evangelist\" teams and sort of given up to focus on AI shiny objects. They've submitted to the fact that in reality no one outside of amazon is building massively complex applications in this way. Ironically Amazon's payments API is a monolith and no plans to change it lol.",
          "score": 9,
          "created_utc": "2026-01-27 18:41:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28oryh",
              "author": "Brief_Ad_5019",
              "text": "Appreciate that! I would love to see your context-map üòÅ Serverless Big Ball Of Mud - what a time to be alive!",
              "score": 1,
              "created_utc": "2026-01-28 16:36:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2374we",
          "author": "PizzaHuttDelivery",
          "text": "DDD did not receive a major shake up. It is very much the same thing evans and vernon proselitized 10 years ago. If it wasnt for microservices and api design practices, DDD would have been probably dead. \n\nDon't get me wrong. DDD is useful, but if you look at all other techniques, they all kind of went an evolution/revolution. Many ideas have been challenged and overturned all across the board. But DDD is still almost the same as 20 years ago.",
          "score": 4,
          "created_utc": "2026-01-27 20:56:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22o7az",
          "author": "CzyDePL",
          "text": "And it's going to trash because now we need \"agentic architecture\"",
          "score": 3,
          "created_utc": "2026-01-27 19:31:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28lb8a",
              "author": "asdfdelta",
              "text": "Enshittification-as-a-Service",
              "score": 1,
              "created_utc": "2026-01-28 16:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25e72l",
          "author": "Longjumping_Status71",
          "text": "There is nothing you described that doesn‚Äôt match my clients enterprise system, a lot of which is there because of me and the same understandings that you have outlined and I have come to believe as the best way to do things as well. Microservice vs monolith has bounced back and forth in the blogosphere , but once the organization reaches a critical mass I think everyone ends up with some microservices‚Ä¶ usually supporting a monolith as well.",
          "score": 3,
          "created_utc": "2026-01-28 03:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22ryzx",
          "author": "EagleSwiony",
          "text": "Kinda agree beside the DDD and PostgreSQL point. \n\nRegarding Postgre I would not say it's THE standard. IT depends on the industry. In Fintech, I would say MSSQL or even Oracle DB are more prominent and used in those huge Enterprises.",
          "score": 2,
          "created_utc": "2026-01-27 19:48:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25vo3u",
          "author": "flavius-as",
          "text": "There's one more trick with the performance of a modulith behind the LB: you (soft) redirect certain use cases to certain nodes in order to keep the caches warmer.\n\nAnd one more trick to make problems surface: you give each module in the modulith its own db credentials and connection. Guardrail: you keep in check what each module is able to read or write via schemas and permissions.",
          "score": 2,
          "created_utc": "2026-01-28 05:20:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o251cla",
          "author": "markojov78",
          "text": "I really disagree that microservices are some kind of internal organisation tool.\n\nIt's a design pattern for decoupling which has implications on scalability and resilience. Like any other design pattern it's not good for every case but it is all about how software performs in production, not about people who made it.\n\nTreating it as internal organisation tool is what caused so many awful attempts at microservices where boundaries were merely \"administrative\" and never contributed to any decoupling ...",
          "score": 4,
          "created_utc": "2026-01-28 02:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228nrg",
          "author": "HowIsDigit8888",
          "text": "None of the good backend architecture has been made yet. We haven't even gotten started except at the drawing board. Everything in deployment is jank",
          "score": 3,
          "created_utc": "2026-01-27 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o22bec2",
              "author": "Brief_Ad_5019",
              "text": "Honestly even this is a bit harsh - I also need to agree that even if this \"Modern Standard\" is now somewhat popular - a lot of new services that I see still don't mind and go crazy..",
              "score": 3,
              "created_utc": "2026-01-27 18:36:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22c2u1",
                  "author": "HowIsDigit8888",
                  "text": "Yeah I guess I did exaggerate the harshness, there are open source stacks that aren't really janky these days",
                  "score": 1,
                  "created_utc": "2026-01-27 18:39:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o23cos0",
          "author": "ConquerQuestOnline",
          "text": "I think that that's a pretty standard architecture these days for apps with strong consistency. Vertical slices, a little DDD flavoring, etc. It's what I'm doing for my current side-project.\n\nAt work, we're very much EDA with event-sourcing, and mostly, it's the right choice.",
          "score": 1,
          "created_utc": "2026-01-27 21:21:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24cm22",
          "author": "symbiat0",
          "text": "Actually I think it depends on use cases for your application. Sometimes a monolith makes sense, sometimes it doesn‚Äôt, I‚Äôve been on both sides of that the past few years‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-28 00:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o25htpa",
          "author": "NTXL",
          "text": "In my humble uneducated opinion  i don‚Äôt think it gets better than serverless",
          "score": 1,
          "created_utc": "2026-01-28 03:53:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28mk4y",
              "author": "asdfdelta",
              "text": "If I had a nickel for every time I've heard that haha.\n\nThere is one constant in software - everything always changes. Especially constraints, which shift the value of different stacks.",
              "score": 1,
              "created_utc": "2026-01-28 16:26:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o266ipg",
          "author": "vbilopav89",
          "text": "No we haven't. I'm known to be a notorious DDD hater and I do hate anything DDD/CA/HA with passion.¬†\n\n\nAlthough, I do admit that DDD had some good parts, only two actually (language, context), but those concepts existed even before.\n\n\nAnything else is crap and nonsense amd I've been writing and ranting about it for years now on LinkedIn. Here's my latest:\nhttps://www.linkedin.com/posts/vb-software_here-is-a-really-really-interesting-side-by-side-activity-7421810292365590528-hJyE?utm_source=share&utm_medium=member_android&rcm=ACoAAAFChn4BJtC3te8HO2_kZu5V6dFh8zzjQq8",
          "score": 1,
          "created_utc": "2026-01-28 06:42:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26fmte",
          "author": "YahenP",
          "text": "These are tough times in the industry. There's no money, projects are being scrapped, jobs are being cut. At times like these, religious wars about architecture and other abstract matters usually subside. Most are focused on their work and how to avoid wasting it. Doing more with less. When (and if) the industry emerges from the crisis, all these debates will flare up with renewed vigor.",
          "score": 1,
          "created_utc": "2026-01-28 07:59:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26o15h",
          "author": "Few_Cauliflower2069",
          "text": "Nope. The tooling and infrastructure around microservices failed, it was harder to debug and keep track of the system landscape so now people are going back to monoliths. Microservices are far superior in terms of scalability, security, maintenance, development speed and many more areas though",
          "score": 1,
          "created_utc": "2026-01-28 09:16:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27i8y7",
          "author": "tr14l",
          "text": "Modular monolith requires a certain amount of guard dogging, too. \n\nPersonally, Conway's law is the driver for me. I let deployment/ownership needs drive segregation. Where there is a unit of people, that unit should map to business function, and they should have their own deployable artifacts that do not require coordination with anyone else. \n\nI also demand people are ruthless about mitigation of outside dependency elsewhere in the company. If you don't control it, you should be bitching about using it. Obviously third party tools with support are more of an exception, but a culture of wanting full control should be in place. You want me to use your email gateway? How about go eff yourself. I can do that myself and I'm not coming to you every time I need a template change for a feature.\n\nYou want me to use your special search service? Nah son. I can handle elastic search myself. Thanks. \n\nYou want me to use your special sauce feature flag BS? Make me.",
          "score": 1,
          "created_utc": "2026-01-28 13:12:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27j64a",
          "author": "Jolly-Lie4269",
          "text": "Shit engineers are always going to create shitty stuff",
          "score": 1,
          "created_utc": "2026-01-28 13:17:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22w3mp",
          "author": "justUseAnSvm",
          "text": "*Let me tell you something, I haven't even begun to peak. And when I do peak, you'll know. Because I'm gonna peak so hard that everybody in Philadelphia's gonna feel it.*",
          "score": 1,
          "created_utc": "2026-01-27 20:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o226inz",
          "author": "atika",
          "text": "Fuck no. DDD is a horrible way to decompose your system.",
          "score": -3,
          "created_utc": "2026-01-27 18:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2283a7",
              "author": "dbcoder",
              "text": "what is your preferred methodology?",
              "score": 8,
              "created_utc": "2026-01-27 18:22:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o22f0x7",
                  "author": "atika",
                  "text": "Let's not get into a religious debate here, m'kay?\n\nBut look at [this video](https://www.youtube.com/watch?v=Q6RfMmMwhvM), somewhere around 5 min. mark, he starts to get into service granularity.",
                  "score": -5,
                  "created_utc": "2026-01-27 18:51:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o22951x",
              "author": "Brief_Ad_5019",
              "text": "Oh you think so? After a learning curve I am big believer. I found that things like ubiquitous language and a clean domain model is super helpful - especially during testing.\n\nSomehow I feel that we have forgotten the fundamentals of how a class should encapsulate data AND it's domain logic while we have  have adopted EntityFramework, created an anemic domain model and put everything in the controller üòÇ",
              "score": 6,
              "created_utc": "2026-01-27 18:26:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o22jx56",
                  "author": "CzyDePL",
                  "text": "Yup, it's not even about DDD - it's basic OOP. I blame Java for \"class oriented programming\" vs proper OOP",
                  "score": 2,
                  "created_utc": "2026-01-27 19:12:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o237qq5",
          "author": "Charming-Raspberry77",
          "text": "Get to a few thousand requests per second and your modulith on pg will look less attractive by the day. Big scale breaks everything.",
          "score": 0,
          "created_utc": "2026-01-27 20:59:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o227ig1",
          "author": "InternationalEnd8934",
          "text": "nowhere close peak. that will be quantum computers and quantum entanglement data transmission that is faster than light",
          "score": -7,
          "created_utc": "2026-01-27 18:19:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlb3ow",
      "title": "OpenAI‚Äôs PostgreSQL scaling: impressive engineering, but very workload-specific",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qlb3ow/openais_postgresql_scaling_impressive_engineering/",
      "author": "mddubey",
      "created_utc": "2026-01-24 02:46:32",
      "score": 80,
      "num_comments": 8,
      "upvote_ratio": 0.91,
      "text": "I am a read only user of reddit, but OpenAI‚Äôs recent blog on scaling PostgreSQL finally pushed me to write. The engineering work is genuinely impressive ‚Äî especially how far they pushed a single-primary Postgres setup using read replicas, caching, and careful workload isolation.\n\nThat said, I feel some of the public takeaways are being over-generalized. I‚Äôve seen people jump to the conclusion that distributed databases are ‚Äúover-engineering‚Äù or even a ‚Äúfalse need.‚Äù While I agree that many teams start with complex DB clustering far too early, it isn‚Äôt fair ‚Äî or accurate ‚Äî to dismiss distributed systems altogether.\n\nIMO, most user-facing OpenAI product flows can tolerate eventual consistency. I can‚Äôt think of a day-to-day feature that truly requires strict read-after-write semantics from a primary RDBMS. Login/signup, token validation, rate limits, chat history, recent conversations, usage dashboards, and even billing metadata are overwhelmingly read-heavy and cache-friendly, with only a few infrequent edge cases (e.g., security revocations or hard rate-limit enforcement) requiring tighter consistency that don‚Äôt sit on common user paths.\n\nThe blog also acknowledges using **Cosmos DB for write-heavy workloads**, which is a sharded, distributed database. So this isn‚Äôt really a case of scaling to hundreds of millions of users purely on Postgres. A more accurate takeaway is that **Postgres was scaled extremely well for read-heavy workloads,** while high-write paths were pushed elsewhere.\n\nThis setup works well for OpenAI because writes are minimal, transactional requirements are low, and read scaling is handled via replicas and caches. It wouldn‚Äôt directly translate to domains like **fintech, e-commerce, or logistics** with high write contention or strong consistency needs. The key takeaway isn‚Äôt that distributed databases are obsolete ‚Äî it‚Äôs that minimizing synchronous writes can dramatically simplify scaling, when your workload allows it.\n\nRead the blog here: https://openai.com/index/scaling-postgresql/\n\n**PS:** I may have used ChatGPT to discuss & polish my thoughts. Yes, the irony is noted.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qlb3ow/openais_postgresql_scaling_impressive_engineering/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1db8o5",
          "author": "adfx",
          "text": "It was an interesting read and I appreciate the honesty in the post¬†",
          "score": 9,
          "created_utc": "2026-01-24 04:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dq2ul",
              "author": "mddubey",
              "text": "thank you :)",
              "score": 1,
              "created_utc": "2026-01-24 05:54:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3n10",
          "author": "PmMeCuteDogsThanks",
          "text": "Thanks, for once an interesting post¬†",
          "score": 3,
          "created_utc": "2026-01-24 07:50:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fsxf0",
              "author": "mddubey",
              "text": "Thank you",
              "score": 2,
              "created_utc": "2026-01-24 15:26:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1emwa7",
          "author": "ryan_the_dev",
          "text": "I agree. Nice write up. Not too long.",
          "score": 1,
          "created_utc": "2026-01-24 10:46:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fsygv",
              "author": "mddubey",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-01-24 15:26:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1euku8",
          "author": "KingOfDerpistan",
          "text": "I wonder: what types or workloads are routed to CosmosDB, and which ones to PG? You mentioned write-heavy, are these conversational logs?",
          "score": 1,
          "created_utc": "2026-01-24 11:54:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fwnff",
              "author": "mddubey",
              "text": "It is not me mentioning but more of them mentioning that write heavy! No information about what exactly!\n\nAlthough If I were to guess things which need strong consistency like sing-in/token generation, token invalidation, any settings changes which needs to be in effect instantly will be going to directly postgrss!\n\nThe write heavy stuff I was initially thinking of was actually messages but then why store them in db at all, it should be blob storage or file storage probably!\n\nThe other thing could be stuff which require read/check before write. Which is similar to what we said for postgress! Right now it feels like they are slowly migrating the postgress to cosmos as you can do all the stuff said in 2nd para with more scale! But we can only guess from outside üòü",
              "score": 1,
              "created_utc": "2026-01-24 15:44:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnn5ur",
      "title": "Can we please moderate ai slop?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qnn5ur/can_we_please_moderate_ai_slop/",
      "author": "analcocoacream",
      "created_utc": "2026-01-26 17:49:21",
      "score": 61,
      "num_comments": 10,
      "upvote_ratio": 0.91,
      "text": "I came to this sub hoping for high quality discussions instead it just ai slop spam now",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qnn5ur/can_we_please_moderate_ai_slop/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1zyonf",
          "author": "asdfdelta",
          "text": "This one has been tough to moderate, frankly.\n\nOftentimes these aren't bots but real people using AI to generate a post about a valid architectural topic. It technically fits within the spirit of the sub, but is just lower quality. Restricting lower quality posts gets really subjective when you take writing method into account. Some of the posts are written by non-English speakers that use AI to create a coherent discourse.\n\nI'm happy to add a rule and moderate as I can find (and is reported) if the community at large wants this.",
          "score": 1,
          "created_utc": "2026-01-27 11:33:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uz0cw",
          "author": "Xgamer4",
          "text": "Ah come on, you don't want to figure out how software can give you temporal sovereignty? \n\n(+1 to the request)",
          "score": 26,
          "created_utc": "2026-01-26 18:00:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1wkupu",
              "author": "analcocoacream",
              "text": "I‚Äôd like to start by understanding what was at stake but I guess no one knows not even op",
              "score": 2,
              "created_utc": "2026-01-26 22:11:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v0i9t",
          "author": "FlowOfAir",
          "text": "Dead internet theory üò©",
          "score": 14,
          "created_utc": "2026-01-26 18:06:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vk8hk",
          "author": "Bonejob",
          "text": "Report it, if the moderators are not dealing with it, apply to be a moderator to help out.",
          "score": 3,
          "created_utc": "2026-01-26 19:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1w041w",
          "author": "SpaceGerbil",
          "text": "How can we use AI to solve this problem?\n\n\n*ducks*",
          "score": 5,
          "created_utc": "2026-01-26 20:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wj645",
          "author": "GrogRedLub4242",
          "text": "It needs better moderation period. More posts truly about architecture. Less ESL. (English slop.)",
          "score": 2,
          "created_utc": "2026-01-26 22:03:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wp6z2",
          "author": "Tyhgujgt",
          "text": "But what about my groundbreaking new architecture üò∞, diagrams in dms üòâ",
          "score": 0,
          "created_utc": "2026-01-26 22:31:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoapzj",
      "title": "The snake oil that is the Ai economy",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qoapzj/the_snake_oil_that_is_the_ai_economy/",
      "author": "ParsleyFeeling3911",
      "created_utc": "2026-01-27 10:55:50",
      "score": 37,
      "num_comments": 19,
      "upvote_ratio": 0.77,
      "text": "This might be the greatest scam in human history. Not because it's the most evil or the most profitable, though the numbers are staggering, but because of how perfectly it's designed. The product creates the illusion that justifies the investment. The investment funds better illusions, everyone involved, from the builders to the buyers, has reasons to believe it's real.\n\nHere's what's actually happening. Companies are spending hundreds of billions building data centers, chips, training runs. They're selling this on the promise of transformative intelligence, systems that understand and reason. What they're delivering is sophisticated pattern matching that needs constant supervision and makes up facts with complete confidence.\n\nThe gap between promise and delivery isn't new. But the scale is unprecedented, and the mechanism is interesting.\n\nHistorical snake oil actually contained stuff, alcohol, cocaine, morphine. It did things. The scam wasn't selling nothing, it was selling a cureall when you had a substance with narrow effects and bad side effects.\n\nModern AI has real capabilities. Text generation, translation, code assistance, image recognition. These work. The scam is in the wrapping‚Äîselling pattern-matching as intelligence, selling tools that need supervision as autonomous agents, selling probability distributions as understanding.\n\nWhen you sell cocaine as a miracle cure, customers feel better temporarily. When you sell pattern-matching as general intelligence, markets misprice the future. The difference is scale. This isn't twenty dollar bottles on street corners. It's company valuations in the hundreds of billions, government policy decisions, and infrastructure investment that makes sense only if the promises are true.\n\nChatbots work like cold readers. Not because anyone sat down and decided to copy psychics, but because the same optimization pressures produce the same behaviors.\n\nA cold reader mirrors your language to build rapport. An LLM predicts continuations that match your style. A cold reader makes high-probability guesses that sound insightful‚Äîeveryone has experienced loss. An LLM generates statistically likely responses. Both deliver with confidence that makes vagueness feel authoritative. Both adapt based on feedback. Both fill gaps with plausible-sounding details, whether that's a spirit name or a fabricated citation. Both retreat to disclaimers when caught.\n\nThe psychological effect is identical. You feel understood. The system seems smart. The experience validates the marketing claims. And this isn't accidental‚Äîsomeone chose to optimize for helpfulness over accuracy, to sound confident, to avoid hedging, to mirror your tone. These design choices create the cold reading effect whether that's the stated goal or not.\n\nMarketing creates expectations for intelligence. The interface confirms those expectations through cold reading dynamics. Your experience validates the hype. Markets respond with investment. With billions on the line, companies need to maintain the perception of revolutionary capability, so marketing intensifies. To justify valuations, systems get tuned to be even more helpful, more confident‚Äîbetter at seeming smart. Which creates better user experiences. Which validates more marketing.\n\nEach cycle reinforces the others. The gap between capability and perception widens while appearing to narrow. And the longer it runs, the harder it becomes to reset expectations without market collapse.\n\nThe consequences compound. Capital misallocation on a massive scale‚Äîtrillions in infrastructure for capabilities that may never arrive. Companies restructuring and cutting jobs for automation that doesn't work unsupervised. Critical systems integrating unreliable AI into healthcare, law, education. And every confidently generated falsehood makes it harder to distinguish truth from plausible-sounding fabrication.\n\nWhat makes this potentially the greatest scam in history isn't just the scale. It's that the people running it might be true believers. They're caught in their own hype cycle, pricing their equity on futures that can't materialize because they won't invest in the control infrastructure that could actually deliver on the promises.\n\nThe control systems needed‚Äîverification, grounding, deterministic replay, governance‚Äîcost almost nothing compared to the GPU budget. One training run could fund the entire reliability infrastructure. But there's no hype in guardrails. There's only hype in bigger models and claims about approaching AGI.\n\nSo we keep building capacity for a future that can't arrive, not because the technology is fundamentally incapable, but because the systems around it are optimized for hype over reliability.\n\nAnd here's what makes it perfect: If this is the greatest scam in history, it's also the most perfectly designed one‚Äîbecause the product actively participates in selling itself.\n\nCan you call it a scam if it is not the intent? Well, someone choose to design the chat bots to operate the way they do, and it‚Äôs a known problem that is effectively treated as unsolvable so I have to say that the faith in the future doesn't excuse the deception in the present.\n\n\n\n\n\n[https://github.com/thepoorsatitagain/Ai-control-](https://github.com/thepoorsatitagain/Ai-control-)",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qoapzj/the_snake_oil_that_is_the_ai_economy/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1zycto",
          "author": "ResidentTicket1273",
          "text": "Yes, this exposes a weakness in a system that optimises for specific set of circumstances whose interactions reinforce themselves within the system. It's a classic runaway/feedback loop problem. The system will re-balance itself, but the shock is going to be significant.",
          "score": 10,
          "created_utc": "2026-01-27 11:31:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1zyxo9",
              "author": "ParsleyFeeling3911",
              "text": "I think that there are ways to make AI almost as useful as they claim it to be, its jsut that no one is funding it because no one is being called out on deception.\n\nlet this article go viral (I knowit wont) and they will throw some money around and make it happen.",
              "score": 0,
              "created_utc": "2026-01-27 11:35:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o200kqd",
                  "author": "ResidentTicket1273",
                  "text": "In business automation, where non-deception is important, there is lots of money going into building guardrails to try and tame LLM responses, but the problem there is that you end up with systems that are 99% guardrails, and which, were the same effort expended into building a traditional system, would end up reproducing the same results without an expensive and inefficient LLM being located at the centre.",
                  "score": 6,
                  "created_utc": "2026-01-27 11:48:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o226w6b",
          "author": "Worldly_Expression43",
          "text": "And yet this is an ai written post lmfao",
          "score": 9,
          "created_utc": "2026-01-27 18:17:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23xsct",
              "author": "engiNARF",
              "text": "Ctrl + F \"‚Äî\" lol. Like copying your friend's CS homework without changing the variable names.",
              "score": 3,
              "created_utc": "2026-01-27 22:58:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27eo1p",
                  "author": "magick_bandit",
                  "text": "I have a hiring manager friend who has started asking candidates what keyboard sequence creates an emdash when they see it on their cover letter, etc.",
                  "score": 1,
                  "created_utc": "2026-01-28 12:50:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o26bk4j",
              "author": "ParsleyFeeling3911",
              "text": "Did anyone say that ai did not have uses?",
              "score": 1,
              "created_utc": "2026-01-28 07:24:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o20jbkg",
          "author": "wllmsaccnt",
          "text": "There doesnt seem to be a downside from the perspective of the large tech companies.¬†\n\n\nIf one of them actually matches the hype (a tall order), there is a whole new paradigm to control. Huge potential profits for the lottery ticket holder.\n\n\nIf things remain as-is, they will continue to expand and keep selling the hype.\n\n\nIf the hype dies down and investment stops, then they are still left with a bunch of data centers that they can sell or repurpose to loss lead traditional hosted services.",
          "score": 2,
          "created_utc": "2026-01-27 13:46:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21n72w",
          "author": "Ascending_Valley",
          "text": "There is a large bubble populated with noise and distraction, but the kernel of validity and research is advancing very quickly.  The path is unpredictable, and short-term effects are likely overestimated, with headlines dominated by better bots of various types.\n\nHowever, the technology is advancing in directions that increasingly provide human-level capability and beyond in most disciplines.  From AlphaGo to AlphaFold to coding and math tools and image and text processing, you cannot dismiss much of AI progress as just hype.",
          "score": 1,
          "created_utc": "2026-01-27 16:52:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26c0mz",
              "author": "ParsleyFeeling3911",
              "text": "Progress is definately happening, but is it happening inside the model, or outside of it?",
              "score": 1,
              "created_utc": "2026-01-28 07:28:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o26cg9z",
          "author": "Bane_Returns",
          "text": "I am working on Edge AI, and I only know that if war happens we have 0 chance to beat them.¬†",
          "score": 1,
          "created_utc": "2026-01-28 07:31:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26mlho",
          "author": "alexduckkeeper_70",
          "text": "I can see AI is incredibly useful for programming (especially basic tasks, providing you monitor it), but I don't see a route to massive profitability for those supplying it.¬†",
          "score": 1,
          "created_utc": "2026-01-28 09:02:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20jtxe",
          "author": "Adamzxd",
          "text": "If you don‚Äôt see the massive value of AI, as a developer, as a software architect, there is no hope for you. \n\nBut then again you used AI to write this üëã",
          "score": -1,
          "created_utc": "2026-01-27 13:49:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o222cxg",
              "author": "Clyde_Frag",
              "text": "The cost to use AI assisted tools will go up once the VC funding goes away but you‚Äôre just a weirdo Luddite if you think these tools don‚Äôt make you more productive. Times change, and it‚Äôs best to change with them.",
              "score": 1,
              "created_utc": "2026-01-27 17:57:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o217ue6",
              "author": "Lilacsoftlips",
              "text": "At this point it seems obvious that a large percentage of code changes 3-5 years from now will be made, merged and deployed via agents. And orgs that have been using continuous deployment/shift left stuff are in the best position to do so. Maybe if you are not in a modern org, it‚Äôs harder to see.¬†",
              "score": -3,
              "created_utc": "2026-01-27 15:45:35",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o206jpl",
          "author": "Rare_Huckleberry_734",
          "text": "The truth will always come out. When it does, won't be pretty for some.",
          "score": 1,
          "created_utc": "2026-01-27 12:31:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjxzgn",
      "title": "SOLID Principles Explained for Modern Developers (2026 Edition)",
      "subreddit": "softwarearchitecture",
      "url": "https://javarevisited.substack.com/p/how-to-be-a-solid-programmer-in-2026",
      "author": "javinpaul",
      "created_utc": "2026-01-22 15:46:50",
      "score": 23,
      "num_comments": 8,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qjxzgn/solid_principles_explained_for_modern_developers/",
      "domain": "javarevisited.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o17d6pf",
          "author": "steve-7890",
          "text": "2026 and inside the same old sh\\*t with bad examples and no remarks when not to use them (**what's even more important**).",
          "score": 16,
          "created_utc": "2026-01-23 08:18:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17k9ls",
              "author": "minoso2",
              "text": "in what situation would you not use one of these solid principles?",
              "score": 2,
              "created_utc": "2026-01-23 09:24:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17syes",
                  "author": "steve-7890",
                  "text": "There are tones of materials on that:\n\n\\* Watch: Dan North, CUPID talk\n\n\\* Read: A philosophy of Software Design book\n\nFor instance:\n\n\\* OCP inside the module causes a lot of redundant abstractions that increases cognitive load - without any real benefits.   \n\\* DIP - again, inside the module - same as above   \n\\* SRP - nobody knows what \"Single\" means here. If applied everywhere causes a lot of small objects, hard to grasp.",
                  "score": 7,
                  "created_utc": "2026-01-23 10:43:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1aozf1",
                  "author": "Drevicar",
                  "text": "Given that the SOLID principles are solutions to specific problems that may occur during the software development process, then you would not apply any of the solutions when none of the problems are present in a significant quantity. Where \"significant quantity\" is subjective and based on trade-off analysis, but if the quatity is == 0 then you don't even need the subjective part.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18av0w",
          "author": "AvoidSpirit",
          "text": "Solid is basically the bible of programming. As in everybody interprets it differently and can use it to explain anything ever.\n\nYou either have enough experience to organize things and then you can call it whatever. Or you don't and then knowing the definition of something like \"Single Responsibility\" or \"Interface segregation\" is practically irrelevant.",
          "score": 2,
          "created_utc": "2026-01-23 12:56:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmceo1",
      "title": "Avoiding Redis as a single point of failure feedback on this approach?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qmceo1/avoiding_redis_as_a_single_point_of_failure/",
      "author": "saravanasai1412",
      "created_utc": "2026-01-25 07:16:08",
      "score": 20,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": "Hey all,  \n\nThis post is re-phrased version of my last post to discussed but it conveyed different message. so am asking the question different.\n\nI been thinking about how to handle Redis failures more gracefully. Redis is great, but when it goes down, a lot of systems just‚Ä¶ fall apart . I wanted to avoid that and keep the app usable even if Redis is unavailable.\n\nHere‚Äôs the rough approach am experimenting with\n\n* Redis is treated as a **fast cache**, not something the system fully depends on\n* There‚Äôs a **DB-backed cache table** that acts as a fallback\n* All access goes through a small cache manager layer\n\nFlow is pretty simple\n\n* When Redis is healthy:\n   * Writes go to DB (for durability) and Redis\n   * Reads come from Redis\n* When Redis starts failing:\n   * A **circuit breaker** trips after a few errors\n   * Redis calls are skipped entirely\n   * Reads/writes fall back to the DB cache\n* To avoid hammering the DB during Redis downtime:\n   * A **token bucket rate limiter** throttles fallback reads\n* Recovery\n   * After a cooldown, allow one Redis probe\n   * If it works, switch back to normal\n   * Cache warms up naturally over time\n\nNot trying to be fancy here no perfect cache consistency, no sync jobs, just predictable behavior when Redis is down.\n\nI am curious:\n\n* Does this sound reasonable or over-engineered?\n* Any obvious failure modes I might be missing?\n* How do you usually handle Redis outages in your systems?\n\nWould love to hear other approaches or war stories \n\nhttps://preview.redd.it/qnc3xpne4gfg1.png?width=1646&format=png&auto=webp&s=d844d303866502e85d82bc2585f6a575e67d44cd\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qmceo1/avoiding_redis_as_a_single_point_of_failure/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1l1kmk",
          "author": "ccb621",
          "text": "Your rephrasing largely resembles the original post. What more are you hoping to learn that you didn‚Äôt already?\n\nAs others asked/suggested: why do you even need a cache? What are your SLAs, and what bottlenecks have you actually profiled and measured?\n\nYour posts focus on Redis as if it is a must-have, but you don‚Äôt provide any evidence to support this.¬†\n\nKeep it simple.¬†",
          "score": 17,
          "created_utc": "2026-01-25 08:09:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nfu93",
              "author": "Buttleston",
              "text": "I didn't see the previous post, but this also pre-supposes that redis is going to go down.  Is that actually something that happens?  I've used redis for a decade and never had any kind of downtime",
              "score": 1,
              "created_utc": "2026-01-25 17:16:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1njepc",
                  "author": "Long_Drink1680",
                  "text": "Don't they mean the backend not being able to access Redis (like when the connection pool is exhausted) and not the actual Redis servers being down???",
                  "score": 3,
                  "created_utc": "2026-01-25 17:32:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1lll9r",
          "author": "SassFrog",
          "text": "Fallbacks like this make your system fragile. Work should be constant and have 1 \"operational state\" to avoid metastability issues.\n\nhttps://aws.amazon.com/builders-library/reliability-and-constant-work/\nhttps://brooker.co.za/blog/2021/05/24/metastable.html\n\nThere are numerous solutions to maintaining consistency of read replicas from authoritative systems or making Redis highly available. I'd reach for those.",
          "score": 3,
          "created_utc": "2026-01-25 11:06:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o7mxl",
          "author": "ThigleBeagleMingle",
          "text": "It‚Äôs 2026 why is this still a problem? You setup replica and fail over in the rare scenario needed.\n\nOne instance will give you 99% uptime, 2 gets 99.9%. Does OP actually understand their use case and SLA ??",
          "score": 2,
          "created_utc": "2026-01-25 19:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kxoc5",
          "author": "Comprehensive-Art207",
          "text": "Have you looked at the Redis API-compatible KeyDB?",
          "score": 3,
          "created_utc": "2026-01-25 07:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lo5sk",
          "author": "configloader",
          "text": "Run redis with sentinel, really small chance redis will go down ;)",
          "score": 1,
          "created_utc": "2026-01-25 11:29:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nqo5l",
              "author": "ccb621",
              "text": "And what happens if/when it does go down?",
              "score": 1,
              "created_utc": "2026-01-25 18:02:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnaoid",
      "title": "software architecture over coding",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qnaoid/software_architecture_over_coding/",
      "author": "Practical_Lake8826",
      "created_utc": "2026-01-26 08:39:47",
      "score": 19,
      "num_comments": 14,
      "upvote_ratio": 0.85,
      "text": "I heard a CEO say that software architecture jobs are going to replace coding jobs, how does it make sense",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qnaoid/software_architecture_over_coding/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1sdlv9",
          "author": "virtualstaticvoid",
          "text": "They don't. Maybe the CEO believes that AI will replace the need for developers coding.",
          "score": 27,
          "created_utc": "2026-01-26 08:58:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tpj8f",
              "author": "Lazy_Film1383",
              "text": "But he heard a cto say this.. you are more of an architect with agentic development. You handle agents and architect the solution",
              "score": 3,
              "created_utc": "2026-01-26 14:39:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1t2ely",
          "author": "BaronOfTheVoid",
          "text": "Don't take CEOs seriously. 90% of their job is guessing. And if they are correct accidentally then they boast how great their foresight and sense of responsibility is.\n\nActually, it would make more sense if AIs replace CEOs than any kind of developer or engineer. Guessing is what AIs are relatively good at.",
          "score": 13,
          "created_utc": "2026-01-26 12:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1u7i00",
              "author": "Thin_Driver_4596",
              "text": "There was a study that compared human CEOs with AI for a small sample and found that AI outperformed human CEOs 100% of times, often significantly.",
              "score": 4,
              "created_utc": "2026-01-26 16:01:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1tfhq5",
          "author": "beginetienne",
          "text": "Software architecture expertise and business domain expertise goes together.  Good programmers create code that is easy to read and maintain based off decisions made by architects...  Logs that are easy to parse that contain relevant data, proper error handling etc.\n\nA senior programmer will spot gaps in the architects design quickly.  Will the AI do that or will it try to please me?\n\nIf the architect creates garbage context for the AI, you will get garbage results.  It might be more even more difficult for a director/manager to identify who and what is causing the bottleneck in the project.\n\nI guess it can replace some coding jobs but not all.  Everybody is guessing like this CEO, time will tell.  In the business application world (ERPs, WMSs, CRMs, EDI etc) most of the time, bottlenecks are NOT caused by programmers (in my experience).",
          "score": 6,
          "created_utc": "2026-01-26 13:48:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tpzv9",
              "author": "edgmnt_net",
              "text": ">In the business application world (ERPs, WMSs, CRMs, EDI etc) most of the time, bottlenecks are NOT caused by programmers (in my experience).\n\nThat's true, but it's more of a self-fulfilling prophecy because those businesses *choose* to work on stuff where code has reduced impact. It's a very large niche (possibly the largest), but it's hardly all there is to software development.\n\nIt appears that the CEO is under the impression that all coding is a code monkey job and that architects don't need to concern themselves with such details. I think that's a stretch even in the best case scenario because plenty of such businesses end up accumulating a lot of cruft and tech debt. This only sort of works in an extremely flat, purely horizontally-scaled project, but even then you generally have some sort of central platform which requires being mindful of what you do. As soon as you need to build on top of things and you need to manage complexity because you can't just throw 100 more devs at it, this fails spectacularly.\n\nE.g. you're building something that needs to be relatively safe and secure, which you really cannot do without higher abstraction or special techniques which neither mere architects nor code monkeys know. You can't just throw more simple tests at it. It's far, far more likely you need something deeply-embedded into the code and it has to be something which can be reasoned about.",
              "score": 3,
              "created_utc": "2026-01-26 14:41:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1sqnko",
          "author": "Consistent-Ad-180",
          "text": "Maybe,they think about a software architect + copilot.   \n  \nWhat is the problem of regular developer when they start a new project?  \n\\- They choose the technology they know, not the best for the needed work.  \n\\- They prompt to make that function and then make another. And everything become tigh coupled and impossible to read and manage.\n\nI started courses in \"archmentor.dev\" and there are a lot of examples of how to think in tehnologies and solutions, not in code. Because the developer knows how to code. The architect knows what to choose.",
          "score": 4,
          "created_utc": "2026-01-26 10:56:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1wjuuh",
          "author": "GrogRedLub4242",
          "text": "CEO jobs can be replaced with LLM chatbots",
          "score": 5,
          "created_utc": "2026-01-26 22:07:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tq6lk",
          "author": "Gunny2862",
          "text": "CEOs have way more of a hive mind than you would think.",
          "score": 3,
          "created_utc": "2026-01-26 14:42:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1to7iq",
          "author": "Constant_Physics8504",
          "text": "Well architects and designers write requirements. There is a goal for requirements to be written as prompts to AI and for SWE teams to be dwindling",
          "score": 2,
          "created_utc": "2026-01-26 14:32:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ukq7t",
          "author": "Training_Bug2340",
          "text": "It does make sense to me",
          "score": 2,
          "created_utc": "2026-01-26 16:58:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1uumqx",
          "author": "the-fluent-developer",
          "text": "This is based on the notion that AI will automate away some or even all of the coding. Plus we need strong boundaries to reduce the blast radius of code changes that are made by AI without thorough human inspection.",
          "score": 2,
          "created_utc": "2026-01-26 17:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o24xzz8",
          "author": "Mysterious-Rent7233",
          "text": ">I heard a CEO say that software architecture jobs are going to replace coding jobs, how does it make sense\n\nMy own work has transitioned from 10% architecture to 90% coding to 80% architecture 20% coding due to AI. (putting aside all other aspects like communication, prioritization, etc.)\n\nSo yeah, that's one example of a software architecture job replacing a coding job.",
          "score": 1,
          "created_utc": "2026-01-28 02:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1t0a5b",
          "author": "Big_Conflict3293",
          "text": "If you don‚Äôt understand how ai is doing all of this, you‚Äôre already behind.¬†",
          "score": -1,
          "created_utc": "2026-01-26 12:13:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkw6og",
      "title": "Designing a Redis-resilient cache for fintech flows looking for feedback & pitfalls",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qkw6og/designing_a_redisresilient_cache_for_fintech/",
      "author": "saravanasai1412",
      "created_utc": "2026-01-23 16:56:19",
      "score": 16,
      "num_comments": 31,
      "upvote_ratio": 0.94,
      "text": "Hey all,\n\nIm working on a backend system in a fintech context where **correctness matters more than raw performance**, and I love some community feedback on an approach am considering.\n\nThe main goal is simple\n\nRedis is great, but I don‚Äôt want it to be a **single point of failure**.\n\nHigh-level idea\n\n* Redis is treated as a **performance accelerator**, not a source of truth\n* PostgreSQL acts as a **durable fallback**\n\nHow the flow works\n\n**Normal path (Redis healthy):**\n\n* Writes go to DB (durable)\n* Writes also go to Redis (fast path)\n* Reads come from Redis\n\n**If Redis starts failing:**\n\n* A **circuit breaker** trips after a few failures\n* Redis is temporarily isolated\n* All reads/writes fall back to a DB-backed cache table\n\n**To protect the DB during Redis outages:**\n\n* A **token bucket rate limiter** throttles fallback DB reads & writes\n* Goal is controlled degradation, not max throughput\n\n**Recovery**\n\n* After a cooldown, the circuit breaker allows a single probe\n* If Redis responds, normal operation resumes\n\n**Design choices I‚Äôm unsure about**\n\nI‚Äôm intentionally keeping this simple, but I‚Äôd love feedback on\n\n* Using a **DB-backed cache table** as a Redis fallback - good idea or hidden foot-gun?\n* Circuit breaker + rate limiter in the app layer - overkill or reasonable?\n* Token bucket for DB protection - would you do something else?\n* Any failure modes I might be missing?\n* Alternative patterns you‚Äôve seen work better in production?\n\nupdate flow image for better understanding \n\nhttps://preview.redd.it/zt3qiirw48fg1.png?width=1646&format=png&auto=webp&s=e40813fcb14802ffe71b5bfe1611601577190c9b",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qkw6og/designing_a_redisresilient_cache_for_fintech/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o19qdos",
          "author": "Dry_Author8849",
          "text": "Caching in fintech is a risky move. You need to be very careful on what are you caching.\n\nSo, some reads, like account balances shouldn't be cached. It also depends where are you using the cache. If your API hits the cache is a bad idea.\n\nIf you proceed anyways, then test every operation in a highly concurrent scenario and see if everything pass.\n\nCheers!",
          "score": 12,
          "created_utc": "2026-01-23 17:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19s1r2",
          "author": "Mundane_Cell_6673",
          "text": "What is reads vs write ratio?\n\nWhat is the point of rate limiting writes in case when redis goes down, won't you have an inconsistent state in db? How do reads work here? What if you have too many read requests and your redis is still down?",
          "score": 4,
          "created_utc": "2026-01-23 17:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19z7u0",
              "author": "saravanasai1412",
              "text": "No we write data to db first. If redis fails db act as source of truth. Let me give bit context about how we using redis.  We start a transaction and store a transaction Id for that session for 10 min. In 10 min they need to complete the transaction. Mostly these data is on redis.\n\nWhat am trying to do now is in case of redis failure those transaction will be dropped now. To avoid it db write will fallback.\n\nRate limiting db writes and reads. We having an average load in our system is 13k request per min. So redis failure can bring our database down due to sudden spike. So am planning to control it",
              "score": 1,
              "created_utc": "2026-01-23 17:50:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19vxn5",
          "author": "mrGoodMorning2",
          "text": "**Normal path (Redis healthy):**\n\n* Writes go to DB (durable)\n* Writes also go to Redis (fast path)\n* Reads come from Redis\n\nMy first thought when I read this was that if you write the data to Redis and it dies before writing to DB it will lead to loss of data, this is FINE, but it depends on how CRITICAL the data is. Don't use it for anything payments related (transactions, accounts balances, payment instruments etc)  \n\n\nThe circuit braker and rate limiter for the DB seem fine.\n\n* Using a **DB-backed cache table** as a Redis fallback - good idea or hidden foot-gun?\n\nYou didn't tell us any specific number for reads/writes per second, so I don't think we can answer you, but introducing any new component can be a hidden foot-gun, especially when they share the same data and you have no local transactions between them.  \nIf you want performance can't you make a new index or write data in batches or have separate tables for reads/writes or read replica?   \n\n\n* Alternative patterns you‚Äôve seen work better in production?\n\nWhat we in my company(fin-tech) is put all of the payments as events in Kafka and then when polling events we take an entire batch and persist the batch at once, reducing transactions. Another thing we do is split up core data and metadata in separate tables and not just one huge table, which increases contention",
          "score": 3,
          "created_utc": "2026-01-23 17:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19xocx",
              "author": "DevelopmentScary3844",
              "text": "Good points. I have similar thoughts and how about this one:\n\nWrites to db invalidate redis entry first, update db, update redis.",
              "score": 1,
              "created_utc": "2026-01-23 17:43:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19z2q2",
              "author": "saravanasai1412",
              "text": "No we write data to db first. If redis fails db act as source of truth. Let me give bit context about how we using redis.  We start a transaction and store a transaction Id for that session for 10 min. In 10 min they need to complete the transaction. Mostly these data is on redis. \n\nWhat am trying to do now is in case of redis failure those transaction will be dropped now. To avoid it db write will fallback. \n\nRate limiting db writes and reads. We having an average load in our system is 13k request per min. So redis failure can bring our database down due to sudden spike. So am planning to control it",
              "score": 1,
              "created_utc": "2026-01-23 17:50:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1a1vul",
                  "author": "mrGoodMorning2",
                  "text": "13k requests a minute is about \\~216 request a second, which Postgre should be able to handle (if you have decent hardware). Even you have double the traffic it should be fine. Think about the DB performance optimizations I mentioned above.\n\nAlso since I don't fully understand what you store in Redis, I'll ask the stupid question of why does the cache have to be distributed? Can't you make an in-memory cache in the app?",
                  "score": 3,
                  "created_utc": "2026-01-23 18:02:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1w1nyv",
              "author": "Mehazawa",
              "text": "\\>What we in my company(fin-tech) is put all of the payments as events in Kafka and then when polling events we take an entire batch and persist the batch at once, reducing transactions.\n\nJust curious is it reliable? afaik kafka doesn't guarantee that the data won't be lost on 100%, with replication it is quite safe probably. but I was thinking fin-tech is working in other direction, first sync is written to the persitent and then we process it asychronously with cdc.",
              "score": 1,
              "created_utc": "2026-01-26 20:46:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xs6u9",
                  "author": "mrGoodMorning2",
                  "text": "It's reliable if you're replicating your data and you have brokers in multiple availability zones in one region, meaning our brokers are guaranteed to not be on the same server rack.\n\n\\>first sync is written to the persitent and then we process it asychronously with cdc.  \nThis is also viable, but writing to the DB first and then syncing it with Kafka is slower and we've had problems with the cdc (Oracle Golden Gate) in our company. It's a single point of failure and its in the DB stack so the Database administrators have visibility and troubleshoot its problems. Because of that we made the decision to directly publish to Kafka. Technically Kafka is a storage too, you can have as much retention as so you want and you can replay events with it as well.",
                  "score": 1,
                  "created_utc": "2026-01-27 01:52:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bfkrx",
          "author": "configloader",
          "text": "Skip redis.\nUse db. Reads that doesnt need to be correct all the time can use secondary db servers",
          "score": 4,
          "created_utc": "2026-01-23 21:52:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kaeou",
              "author": "BornSpecific9019",
              "text": "from personal experience, redis is orders of magnitude faster than postgres\n\nthe problem is keeping it in sync w db (cache invalidation, etc). one of the fun problems worth thinking about",
              "score": 1,
              "created_utc": "2026-01-25 04:45:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1klqo6",
                  "author": "configloader",
                  "text": "Ofc it is. B",
                  "score": 1,
                  "created_utc": "2026-01-25 06:00:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1kviot",
                  "author": "Pto2",
                  "text": "The problem of keeping two sources of truth in sync is the sort of thing I would really try to avoid trying to solve if I were working in a financial context!",
                  "score": 1,
                  "created_utc": "2026-01-25 07:18:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bllmj",
          "author": "ryan_the_dev",
          "text": "You will learn more by implementing it vs asking Reddit.",
          "score": 4,
          "created_utc": "2026-01-23 22:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j7jxl",
              "author": "Material-Smile7398",
              "text": "This is the correct answer",
              "score": 3,
              "created_utc": "2026-01-25 01:02:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o19rwmi",
          "author": "Ambitious-Sense2769",
          "text": "I wouldn‚Äôt even mess with caching critical data on a financial system. If correctness matters 100% and if the data isn‚Äôt correct and has huge consequences if it‚Äôs wrong, why even take a risk? Just shard the main db enough to meet the demand you guys need and use locks properly",
          "score": 3,
          "created_utc": "2026-01-23 17:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19wbsk",
          "author": "IcyUse33",
          "text": "You could overload the DB cache.\n\nUse an L1 cache instead (in-memory on the web/app server)",
          "score": 2,
          "created_utc": "2026-01-23 17:37:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19y6y9",
              "author": "saravanasai1412",
              "text": "But I feel it may grow without bound and can bring the system down. We trying to make the system to handle failure gracefully.",
              "score": 1,
              "created_utc": "2026-01-23 17:46:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ayomq",
                  "author": "IcyUse33",
                  "text": "Your L1 cache should take care of that. Frameworks like asp.net core have auto eviction and memory support built in.",
                  "score": 2,
                  "created_utc": "2026-01-23 20:33:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ag1pi",
          "author": "ben_bliksem",
          "text": "We use in-memory with MSSQL as a fallback/distributed. With some affinity setup for the same IPs to attempt hitting the same instances of some of our services the cache setup is more than enough - in memory speed when it hits, reliability of the databases.\n\nObviously this won't work for all setups, but it does when reliability is more important than soaring every millisecond you can.",
          "score": 2,
          "created_utc": "2026-01-23 19:06:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kb17x",
          "author": "BornSpecific9019",
          "text": "interesting idea, but not reliable enough IMO.\n\nconsider looking at how tiger beetle handles financial data and degradation.\n\nhttps://github.com/tigerbeetle/tigerbeetle",
          "score": 2,
          "created_utc": "2026-01-25 04:48:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kucvo",
              "author": "saravanasai1412",
              "text": "No for ledger we are not using this. Its we can think of redis cache fallback as redis is single point of failure in our system. To over come that we doing this mostly its meta data which need to complete the transaction.",
              "score": 1,
              "created_utc": "2026-01-25 07:08:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19ntcd",
          "author": "Responsible_Act4032",
          "text": "IF this isn't an LLM created marketing post, for a service or technology that competes with Redis  I don't know what is. No one speaks or formats posts like this.",
          "score": 2,
          "created_utc": "2026-01-23 16:58:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19r3te",
              "author": "jeffbell",
              "text": "OP is a four year old reddit account and a sensible LinkedIn.\n\nI think it's just a high effort post that is written with more than average formality.",
              "score": 6,
              "created_utc": "2026-01-23 17:13:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19q7x3",
              "author": "saravanasai1412",
              "text": "Its not a marketing post. Its the question I have formatted with LLM to give the context quick without annoying the people. I don't see anything wrong here. LLM helping me to articulate the question much clear & sharper.",
              "score": 2,
              "created_utc": "2026-01-23 17:09:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19w3l6",
                  "author": "BarfingOnMyFace",
                  "text": "I don‚Äôt see anything wrong with that, OP",
                  "score": 4,
                  "created_utc": "2026-01-23 17:36:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j7nnt",
              "author": "Material-Smile7398",
              "text": "What service do you see being sold here?",
              "score": 1,
              "created_utc": "2026-01-25 01:02:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1tbrhs",
                  "author": "Responsible_Act4032",
                  "text": "Well we wait for the responses to flow, these are seed marketing posts.",
                  "score": 1,
                  "created_utc": "2026-01-26 13:27:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uuadq",
          "author": "the-fluent-developer",
          "text": "If your Redis is not local, are you sure there is a performance benefit over serving data from the databases' in-memory cache?",
          "score": 1,
          "created_utc": "2026-01-26 17:39:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmw7bi",
      "title": "We‚Äôve given up on keeping our initial arch docs up to date. Should I worry? Or are we setting ourselves up for pain later?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qmw7bi/weve_given_up_on_keeping_our_initial_arch_docs_up/",
      "author": "Independent-Run-4364",
      "created_utc": "2026-01-25 21:34:34",
      "score": 16,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "At my current team, we started out with decent arch docs ‚Äúhow the system works‚Äù pages. But then we shipped for a few weeks, priorities changed, a couple of us made small exceptions and now we don't use them anymore and they r lost in time.\n\nAs the one who‚Äôs supposed to keep things running long term, I‚Äôm not sure if this is just normal and harmless, or if it's gonna hurt us later.\n\nIf you‚Äôve been in this situation: should we just accept it? If not when could it start to cause problems?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qmw7bi/weve_given_up_on_keeping_our_initial_arch_docs_up/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1p7wpq",
          "author": "Seawolf87",
          "text": "Put a few hours in every other sprint. Make it intentional and time boxed. It doesn't have to be a slog or an every day thing. Just trying your best will begin to develop good habits for the team. Slightly delayed docs are better than no docs.\nEdit:¬†Definitely don't assign this to the same person every time, spread the load so it feels fair",
          "score": 9,
          "created_utc": "2026-01-25 21:51:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1pr7xy",
          "author": "SolarNachoes",
          "text": "Do you have retrospectives? That‚Äôs a good time to do a quick Q/A on any major architectural changes. Then you can go update docs and then update the team.\n\nOr you can use AI now to analyze recent changes and create a summary.",
          "score": 7,
          "created_utc": "2026-01-25 23:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qeakh",
          "author": "anotherchrisbaker",
          "text": "Someone needs to own the arch docs. That doesn't mean they need to do all the updates themselves, but they're on the hook of they get out of date",
          "score": 2,
          "created_utc": "2026-01-26 01:10:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1s7ywi",
          "author": "Ambitious_Fruit6231",
          "text": "It causes problems further down the line when team members believe none of the documentation can be trusted. This may or may not be true ( perhaps 60% is still valid) but once people think that way they don't read or trust it and it becomes of no value. And then everyone is just poking around in the code. \n\nI know it sounds obvious, but try to focus on the big / important stuff and ensure it keeps up to date. If possible partition / organize it accordingly so it's easier to manage and keep the top level stuff separate and 100% correct.",
          "score": 1,
          "created_utc": "2026-01-26 08:07:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1sgulo",
          "author": "vladis466",
          "text": "I disagree with the other comments. As long as there is a way to trace the evolution of the arch- 90% of the value is in people putting thoughts down in a coherent way to agree on a strategy.",
          "score": 1,
          "created_utc": "2026-01-26 09:28:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yq57b",
          "author": "Qinistral",
          "text": "Hate to say it but this is something AI is pretty decent at. You can make a rule to have AI do it when it makes changes or on PR, or just do it ad-hoc. Point it at your recent set of tickets/prs and say \"update these docs\".",
          "score": 1,
          "created_utc": "2026-01-27 05:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zbhkq",
          "author": "HenryWolf22",
          "text": "It‚Äôs normal early on, but gaps compound. Missing docs hurt onboarding, debugging, and scaling. Even minimal updates prevent future headaches. Schedule lightweight reviews to keep them usable.",
          "score": 1,
          "created_utc": "2026-01-27 08:05:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zc694",
          "author": "virtualstaticvoid",
          "text": "Yes, there will be pain. \n\nI've found that including documentation  with our ADRs, not only provides a log of the architecture changes, but also the context, so they remain relevant and don't need to be updated to make sense. \n\nThen, when the architecture changes, you add a new ADR and new documentation as part of the work to implement. \n\nInclude the time needed as part of the task / ticket / story points, etc.",
          "score": 1,
          "created_utc": "2026-01-27 08:12:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlr6i1",
      "title": "DoorDash Applies AI to Safety Across Chat and Calls, Cutting Incidents by 50%",
      "subreddit": "softwarearchitecture",
      "url": "https://www.infoq.com/news/2026/01/doordash-safechat-ai-safety/",
      "author": "rgancarz",
      "created_utc": "2026-01-24 16:18:56",
      "score": 12,
      "num_comments": 8,
      "upvote_ratio": 0.77,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qlr6i1/doordash_applies_ai_to_safety_across_chat_and/",
      "domain": "infoq.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1gmlij",
          "author": "MeggaMortY",
          "text": "Looking at that diagram, the \"AI\" which is making the profanity/sexual content pedictions could just as well been a classical ML model. Nothing ground breaking, but sure more of this AI than stupid undressing bots and slop generators.",
          "score": 12,
          "created_utc": "2026-01-24 17:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j744o",
              "author": "AvailableFalconn",
              "text": "Transformer-based text embeddings were the SOTA way to train safety models in 2019. ¬†Modeling the semantics of text is what they were designed for. ¬†But rebranding it as LLM gets you promoted.",
              "score": 5,
              "created_utc": "2026-01-25 00:59:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1idqdl",
              "author": "eemamedo",
              "text": "Yup. The profanity/sexual content could have a simple multi-class classifier. LLM does the skip the entire \"training on data\" step but in the end, introduces new problems.",
              "score": 1,
              "created_utc": "2026-01-24 22:28:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1n6an8",
                  "author": "CommodoreQuinli",
                  "text": "Yea but that takes effort versus this approach, I‚Äôve looked into building one for our top level routing layer but was convinced to just use a smaller parameter pertained",
                  "score": 1,
                  "created_utc": "2026-01-25 16:35:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1k2nir",
              "author": "its_k1llsh0t",
              "text": "Everything is AI, if you believe!",
              "score": 1,
              "created_utc": "2026-01-25 03:56:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1h5urm",
          "author": "BigWheelsStephen",
          "text": "Implemented something very similar for my company:\n- fast & cheap check for each messages via LLM\n- long & expensive checks on full part of conversation\nNot cheap in the end but works fine and can do more than ML models that can ‚Äúonly‚Äù work to detect insults and stuff like that while predators can be more subtile.",
          "score": 2,
          "created_utc": "2026-01-24 19:03:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp2ifs",
      "title": "How do you automate your architecture inner loop?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qp2ifs/how_do_you_automate_your_architecture_inner_loop/",
      "author": "vmgolubev",
      "created_utc": "2026-01-28 05:23:50",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.92,
      "text": "Hi!\nRecently I realized that my current approach with ADRs and diagrams in drawio sucks:)\nDrawio is great at the beginning, but after some time it becomes hard to manage with updates in all of the c4 diagrams that was created.\nI want to have the same experience as developer - think, write, commit! Any advice on tools that might help me? ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qp2ifs/how_do_you_automate_your_architecture_inner_loop/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o27jxge",
          "author": "ch1pch4p",
          "text": "Without you defining what truly is \"inner loop architecture\", I'll assume you mean \"you architechting on your own, regardless if it's been shared or not.\" \n\nYou can take a crack at mermaid/puml. Text based diagramming. What you lose (or just becomes a bit hard) on the placement of the diagram you gain in text based work: proper source control, quick interations, etc.\n\nMermaid is also rendered in github so that's helpful for devs, not quite for business. You could make a github action, though, to bundle and export your diagrams to pdf or use your doc repo (I'm thinking Confluence) API to update pages. I know draw io can take in mermaid docs as import, so you may need to get creative on how the rubber meets the road on that one.\n\nI use dendron for all my notes. Dead repo now, but still useful in current form. Use schemas to organize your thoughts, and use the lookup to find what you were working on faster.\n\nI'm a text based guy at heart - I figure if I can describe what I know, well, in words, pictures can augment the information.\n\nI say all this, typing on my phone, with highly questionable grammar haha",
          "score": 3,
          "created_utc": "2026-01-28 13:21:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27tvae",
              "author": "never-starting-over",
              "text": "Seconded on mermaid.\n\nThey even have a (kinda janky) visual editor at https://mermaid.live/play\n\nI also use LLMs to help mr write documentation and I think diagrams as text really help them understand what I'm conveying.\n\nWith that said, I wanted so hard for PlantUML to work, but the extra setup required and no native integration with GitHub just makes it a shoehorn choice for the kind of businesses I work with. How I wish it were as batteries included and supported as mermaid is on GitHub and Notion.",
              "score": 2,
              "created_utc": "2026-01-28 14:14:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o27wa48",
                  "author": "ch1pch4p",
                  "text": "LLM are a force multiplier, for sure. \n\nI'd recommend using a local plug-in in vscode. I hate dumping ideas into the internet... You lose control of them.\n\nFor puml, try the jetty server with docker. It really is a breeze to set up if you haven't tried. But.. Yea no github rendering, so if that's your jam, then skip it.",
                  "score": 1,
                  "created_utc": "2026-01-28 14:26:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o28ji2h",
          "author": "SolarNachoes",
          "text": "Likec4 (use AI to spin up a vite app)\n\nD4\n\nMermaid\n\nPlantUML\n\nI use AI now exclusively to edit said documents. Either from scratch or from an exiting app.",
          "score": 0,
          "created_utc": "2026-01-28 16:13:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnsteg",
      "title": "Probabilistic Processing Unit (PPU) ‚Äî exact inference over massive discrete networks without sampling.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/gallery/1qnsteg",
      "author": "Undo-life",
      "created_utc": "2026-01-26 21:01:28",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qnsteg/probabilistic_processing_unit_ppu_exact_inference/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1wb1qu",
          "author": "fabkosta",
          "text": "Sounds cool, but without any explanation whatsoever it's not really understandable what you did. You might want to share more details on the idea to get any engagement.",
          "score": 7,
          "created_utc": "2026-01-26 21:27:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1we4dt",
              "author": "Undo-life",
              "text": "Yeah so the core thing is probabilistic computing, what I did is just how probabilistic computing would be if it was purely on software: \n1.Core idea\nInstead of sampling (Monte-Carlo) or pretending everything is 0/1, I treat the whole puzzle as one big Bayesian network: 729 nodes (9 rows √ó 9 cols √ó 9 digits). Each node is a probability, 0-1.\n2.How it solves\nI wire up Sudoku rules as factors: ‚Äúexactly one 5 in this row‚Äù, ‚Äúexactly one 3 in this box‚Äù, etc. Then I run exact belief propagation‚Äîmessages bounce around until every probability is consistent. No random numbers, no ‚Äúmaybe‚Äù, just closed-form updates.\n3.Why it‚Äôs fast\nBecause it‚Äôs exact, it converges in 3-4 iterations. No 100k samples to average out. My laptop (old Pentium) finishes in 0.3s; 100k MC samples still leave a 66% error rate after 5s.\n4.Current limits\nOnly works on finite discrete variables (so Sudoku is perfect). Continuous stuff needs discretisation ‚Üí explosion of states. That‚Äôs the next hill to climb.",
              "score": 2,
              "created_utc": "2026-01-26 21:41:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wfsuy",
          "author": "Undo-life",
          "text": "Op here I think the post got quite confusing but but I'll try to wrap up the core idea, I built a custom inference engine to see if a \"probabilistic\" approach could beat standard sampling on a classic constraint problem. As a test, I used Sudoku.\n\nThe method in simple terms:\n\n1. Model the puzzle as a network of 729 binary variables (81 cells x 9 digits).\n2. Encode the Sudoku rules as constraint equations linking these variables.\n3. Run a message-passing algorithm: each variable and constraint exchanges local probability updates.\n4. After a few iterations, the probabilities converge to 0% or 100%, giving the exact solution.\n\nThe result:\n\n¬∑ My method: 0.30695 seconds, 100% accuracy.\n¬∑ Monte Carlo (100k samples): 4.94645 seconds, ~33.3% accuracy.\n\nWhat this suggests:\nThe benchmark shows that for this structured problem, exact probabilistic inference via message-passing can be faster and more reliable than random sampling, even when simulated on conventional hardware.\n\nWhy I'm posting:\nThis is an early prototype. The underlying algorithm (a form of belief propagation on a factor graph) is known, but the efficiency on this problem was striking to me. I'm exploring if this approach generalizes to other domains like decoding or verification.\n\nI'm happy to discuss the algorithm details, the benchmark setup, or potential next problem domains.",
          "score": 2,
          "created_utc": "2026-01-26 21:48:49",
          "is_submitter": true,
          "replies": [
            {
              "id": "o1ym7vy",
              "author": "JrSoftDev",
              "text": "Congrats on doing something, and this sounds cool.\n\nAs people mentioned in your other post, it's not clear why would Monte Carlo be the best benchmark here. Doesn't look like the right problem for MC.\n\nIn other comment you mentioned the explosion of states as the next hill to climb... but that's a hill impossible to climb, isn't it? Maybe you'll find some problems that appear to be exponential but then they have some intrinsic structure that makes them more tractable, but I'm assuming that's the type of information about a problem that we can get beforehand.\n\nI would like to see how you're passing those messages and how each node handles them and enforces the known constraints.\n\nAlso, for more on Sudoku solvers specifically, this seems to be the right place to look [https://github.com/t-dillon/tdoku](https://github.com/t-dillon/tdoku)\n\nYour exploration for applications to other problems is interesting, I hope that brings you joy and other good things, and keep sharing your findings.",
              "score": 1,
              "created_utc": "2026-01-27 04:46:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1z0yuk",
                  "author": "Undo-life",
                  "text": "Thanks, and it's less about benchmarking the speed and more about solving it probabilistically, but your right that on a cpu the state explosion is a hill, but I think on a real ppu circuit, the nodes will talk among themselves simulatainiously so it won't won't be a problem, but right now it's just me simulating all of that on a cpu",
                  "score": 2,
                  "created_utc": "2026-01-27 06:35:33",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1yvdu6",
          "author": "DeterminedQuokka",
          "text": "I mean it‚Äôs interesting you are solving a deterministic problem with probability which is slightly over engineered but it‚Äôs also cool. \n\nI do similar things with Japanese puzzles so I totally get the urge.",
          "score": 1,
          "created_utc": "2026-01-27 05:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1z1uoh",
              "author": "Undo-life",
              "text": "Haha using probability on rock-solid Sudoku does feel like bringing a tank to a knife fight. I just wanted to see if the engine could still flex when clues aren‚Äôt 100% certain.\n\nWhich Japanese puzzles are you solving tho?",
              "score": 2,
              "created_utc": "2026-01-27 06:42:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1wlewt",
          "author": "heatdeathofpizza",
          "text": "[https://x.com/extropic](https://x.com/extropic)",
          "score": 0,
          "created_utc": "2026-01-26 22:14:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmr85v",
      "title": "Self Referencing Tables vs Closure Tables - Which one would you choose",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qmr85v/self_referencing_tables_vs_closure_tables_which/",
      "author": "LiveAccident5312",
      "created_utc": "2026-01-25 18:34:41",
      "score": 9,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I'm trying to create a schema design for large scale multi purpose e-commerce platform and while creating \"categories\" relation I found out that categories are hard to manage because products can have indefinite depth of sub-categories (such as, for Tshirts it can be Apparel -> Men -> Tshirts but for laptops it can be Electronics -> Laptops). So to solve this problem I've found two solutions-\n\n1. using self referencing tables and creating infinite category depth by referencing to parent category\n\n2. using clouser table to add ancestor\\_id and descent\\_id to each category with the depth value.\n\nBoth solutions come with its own advantages and drawbacks. What's your suggestion? Also it would be great if anyone can share his/her experience designing a practical ecommerce database schema.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qmr85v/self_referencing_tables_vs_closure_tables_which/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1obl7f",
          "author": "sfboots",
          "text": "Don't use option 2 or pure EAV, both are too complicated\n\nA simple Hierarchical model will be fine. (your option 1)\n\n  One thing I've see is for each node to have the string \"full path for display\" to allow UI display without extra queries to read the grandparents etc..  This make adding or editing categories more complicated but can be huge speedup for the more common read case.",
          "score": 4,
          "created_utc": "2026-01-25 19:30:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1o535h",
          "author": "Conscious_Plankton_8",
          "text": "go with option one for the categories works good as i see with my integrations with ecommerce apis that i work with 75% of them uses this approach that is the parent id for sub cats.\n\nthe most challenging thing is the products options and variants and variants types this is the true hell",
          "score": 2,
          "created_utc": "2026-01-25 19:01:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o61os",
              "author": "LiveAccident5312",
              "text": "How do you manage it (variants, attributes and all). I'm thinking of using the EAV model (an attribute table for managing attributes and one composite table to connect it with products). Maybe a similar model for variants too.",
              "score": 2,
              "created_utc": "2026-01-25 19:05:53",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o9nw1",
                  "author": "Conscious_Plankton_8",
                  "text": "it is based on how you want to manage things based on my experience it was more complicated than that like each variant can have a price and the main product can have no variant and can have variants with the same price as the main product also the variants can be tight to only one product or not also do we will support packages that is group of a product or not \n\nWE CAN go more deep \n\nbut my recommendation for you is that \ncheck the leaders ecommerce apis like shopify and try to understand the schema they applying \n\nalso go step by step for planning these models after understanding your project requirements for what you are building, There are no right or wrong its based on the use case",
                  "score": 1,
                  "created_utc": "2026-01-25 19:21:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o246fdz",
          "author": "severoon",
          "text": "Making this decision boils down to the query patterns you want to support. Collect together all of the ideas for how to structure this data, then all of the query patterns, and figure out the pros and cons of each approach for each query pattern.\n\nOne of the problems with option 1 is that it merges category information and hierarchical structure into the same table, which may not be a good thing over the long term. Option 2 also feels pretty heavyweight depending on the use cases.\n\nYou could think about a middleground, such as having a Categories table that just stores categories with no structure, and a hierarchy table that stores the immediate parent of each category as well as its full list of ancestors (including the immediate parent) as an array type. The advantage here is that you can create a multi-valued index on the ancestor column to allow for efficient querying for subtrees, while the parent column allows for fast querying of immediate children.\n\nIf you have a need to frequently and transactionally change nodes that are roots of large subtrees, this would obviously be disruptive because you need to lock all of the nodes in the subtree to update the ancestor lists. But there are strategies to deal with this, like in normal operation those kinds of changes could be saved up and applied in bulk at a scheduled time. Or, if that's not acceptable, when moving a large subtree you could create a new root node and tombstone the old one, and then move things over a bit at a time using only write locking. (This is an approach that would make sense in the context of a high-traffic, high-load distributed DB.)",
          "score": 2,
          "created_utc": "2026-01-27 23:42:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1olmbm",
          "author": "therealkevinard",
          "text": "For science, check out the Nested Set Model (and its variants).   \nIt‚Äôs a really interesting (big IMO here) pattern for unbounded hierarchies. \n\nTldr: there‚Äôs _no_ direct relation (in the FK sense) between parent and child.   \nInstead, all records are assigned a geometric left and right integer value.   \nThe hierarchy, then, is implicitly derived as the set of records whose left and right values are within the bounds of the parent.  \n\nYou can visualize as a wall of legos or a flame graph where left/right draw a block on a coordinate plane, and the blocks that end up stacked on a parent block are child blocks.  \n\nIn numbers:  \nCat1 (0,5)     \nCat2 (6,10)     \nCat1a (1,3)    \nCat1b (2,4)    \n\nThen getting subcats of cat1 is `select id where left >= Cat1.left AND right <= Cat1.right` \n\nAll variants supported unbounded hierarchies- nesting at millions of levels is no sweat.     \n\n*this isn‚Äôt a general-purpose model. If it‚Äôs the right solution for the app, it‚Äôs amazing. If it‚Äôs not, you‚Äôll hate every day of it.",
          "score": 1,
          "created_utc": "2026-01-25 20:15:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ouq0u",
              "author": "LiveAccident5312",
              "text": "This one looks cool but I'm sure that I will end up in the situation you mentioned in the last.‚ò†Ô∏è",
              "score": 1,
              "created_utc": "2026-01-25 20:54:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkuj5r",
      "title": "Handling likes at scale",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qkuj5r/handling_likes_at_scale/",
      "author": "rkaw92",
      "created_utc": "2026-01-23 15:55:44",
      "score": 9,
      "num_comments": 19,
      "upvote_ratio": 0.77,
      "text": "Hi, I'm tackling a theoretical problem that can soon become very practical. Given a website for sharing videos, assume a new video gets uploaded and gains immediate popularity. Millions of users (each with their own account) start \"liking\" it. As you can imagine, the goal is to handle them all so that:\n\n\\* Each user gets immediate feedback that their like has been registered (whether its impact on the total is immediate or delayed is another thing)\n\n  \n\\* You can revoke your like at any time\n\n  \n\\* Likes are not duplicated - you cannot impart more than 1 like on any given video, even if you click like-unlike a thousand times in rapid succession\n\n  \n\\* The total number of likes is convergent to the number of the users who actually expressed a like, not drifting randomly like Facebook or Reddit comment counts (\"bro got downcommented\" ‚ò†Ô∏è)\n\n  \n\\* The solution should be cheap and effective, not consume 90% of a project's budget\n\n  \n\\* Absolute durability is not a mandatory goal, but convergence is (say, 10 seconds of likes lost is OK, as long as there is no permanent inconsistency where those likes show up to some people only, or the like-giver thinks their vote is counted where really it is not)\n\n  \nPreviously, I've read tens of articles of varying quality on Medium and similar places. The top concepts that seem to emerge are:\n\n\\* Queueing / streaming by offloading to Kafka (of course - good for absorbing burst traffic, less good for sustained hits)\n\n\\* Relaxing consistency requirements (don't check duplicates at write time, deduplicate in the background - counter increment/decrement not transactional)\n\n\\* Sharded counters (cut up hot partitions into shards, reconstruct at read time)\n\n  \nMy problem is, I'm not thrilled by these proposed solutions. Especially the middle one sounds more like CV padding material than actual code I'd like to see running in production. Having a stochastic anti-entropy layer that recomputes the like count for a sample of my videos all the time? No thank you, I'm not trying to reimplement ScyllaDB. Surely there must be a sane way to go about this.\n\n  \nSo now I'm back to basics. From trying to conceptualize the problem space, I got this:\n\n\\* For every user, there exists a set of the videos they have liked\n\n\\* For every video, there exists a set of the users who have liked it\n\n\\* These sets are not correlated in any way: any user can like any video, so no common sharding key can be found (not good!)\n\n\\* Therefore, the challenge lies in the transformation from a dataset that's trivially shardable by userID to another, which is shardable by videoID (but suffers from hot items)\n\n  \nIf we naively shard the user/like pairs by user ID, we can potentially get strong consistency when doing like generation. So, for any single user, we could maintain a strongly-consistent and exhaustive set of \"you have liked these videos\". Assuming that no user likes a billion videos (we can enforce this!), really hot or heavy shards should not come up. It is very unlikely that very active users would get co-located inside such a \"like-producing\" shard.\n\n  \nBut then, reads spell real trouble. In order to definitely determine the total likes for any video, you have to contact \\*all\\* user shards and ask them \"how many likes for this particular vid?\". It doesn't scale: the more user shards, the more parallel reads. That is a sure-fire sign our service is going to get slower, not faster.\n\n  \nIf we shard by the userID/videoID pair, instead? This helps, but only if we apply a 2-level sharding algorithm: for each video, nominate a subset of shards (servers) of size N. Then, based on userID, pick from among those nominated ones. Then, we still have hot items, but their load is spread over several physical shards. Retrieving the like count for any individial video requires exactly N underlying queries. On the other hand, if a video is sufficiently popular, the wild firehose of inbound likes can still overflow the processing capacity of N shards, since there is no facility to spread the load further if a static N turns out to be not enough.\n\nNow, so far this is the best I could come up with. When it comes to the value of N (each video's likes spread over \\*this many\\* servers), we could find its optimal value. From a backing database's point of view, there probably exists some optimum R:W ratio that depends on whether it uses a WAL, if it has B-Tree indices, etc...\n\n\n\nBut let's look at it from a different angle. A popular video site will surely have a read-side caching layer. We can safely assume the cache is not dumb as a rock, and will do request coalescing (so that a cache miss doesn't result in 100,000 RPS for this video - only one request, or globally as many requests as there are physical cache instances running).\n\n  \nNow, the optimum N looks differently: instead of wondering \"how many read requests times N per second will I get on a popular video\", the question becomes: how long exactly is my long tail of unpopular videos? What minimum cache hit rate do I have to maintain to offset the N multiplier for reads?\n\n  \nSo, for now these are my thoughts. Sorry if they're a bit all over the place.\n\n  \nAll in all, I'm wondering: is there anything else to improve? Would you design a \"Like\" system for the Web differently? Or maybe the \"write now, verify later\" technique has a simple trick I'm not aware of to make it worth it?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qkuj5r/handling_likes_at_scale/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o19znt5",
          "author": "IlliterateJedi",
          "text": "It might be worth checking out Designing Data-Intensive Applications.  I remember the early chapters went into a lot of detail about strategies Twitter used to handle similar problems.  I wouldn't be surprised if 'likes' were covered in the book later on.",
          "score": 7,
          "created_utc": "2026-01-23 17:52:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ageoi",
              "author": "halfxdeveloper",
              "text": "That is a good book.",
              "score": 1,
              "created_utc": "2026-01-23 19:07:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1aptm0",
              "author": "rkaw92",
              "text": "Coincidentally, I have this book next to me right now. It's a great book that has transformed how I think about data, particularly in terms of graphs. Alas, nothing seems to be of direct relevance to the question at hand. I have the first edition, though - not sure if the 2nd is out yet or if it mentions some more social-network-adjacent topics. Would be cool if it did.",
              "score": 1,
              "created_utc": "2026-01-23 19:52:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1afen1",
          "author": "halfxdeveloper",
          "text": "This is a case study in over-engingeering. It‚Äôs a like system not telemetry in an ICU. Do optimistic updates on the UI. The backend is a queue system with retry. If your product is so stable and developed that you have the opportunity to focus on this, then I‚Äôm purely jealous. I don‚Äôt want this to come off as mean. I‚Äôm trying to impart that this one of those ‚Äúmake it work and optimize later.‚Äù",
          "score": 6,
          "created_utc": "2026-01-23 19:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1aw6av",
              "author": "rkaw92",
              "text": "I agree it is likely overengineering. And I wish the product was already mature. The thing is, the scenario is the following: on day 1, a popular content creator launches a video on the site. Nobody uses it, and then suddenly 10 million users storm the proverbial door. There's no in-between, there is only panic-surge mode. My aim is to make this happen on a shoestring budget. To be clear, this is not some life-or-death situation, and frankly I don't even get paid for this. It's a hobby project. But if I can, I'd very much like to see it happen.\n\nThe last point is also why I'd like to keep moving parts to a minimum. If I can toss the message broker infra, for sure I will. If I'm reasonably convinced that it would help me absorb traffic spikes for a \"slashdot moment\", it stays. What I'd like to avoid most is inefficiency - for example, pointing a Kafka firehose at some poor system that will throw concurrency errors like memcached.",
              "score": 1,
              "created_utc": "2026-01-23 20:21:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19dn9c",
          "author": "heavy-minium",
          "text": "It's old and I'm not sure if it reflects the reality of more modern architectures nowadays, but still, this paper might be interesting to you: [TAO: Facebook‚Äôs Distributed Data Store for the Social Graph](https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf) . I haven't read it, but my gut-feeling is that is goes toward edge on graph per like, and extensive memcache usage for counts.\n\nI would look more into research papers in general, for example stuff like [Feeding Frenzy: Selectively Materializing Users' Event Feeds](https://sns.cs.princeton.edu/assets/papers/2010-sigmod-silberstein.pdf?utm_source=chatgpt.com)\n\nAnyway, looking for older research papers is probably going to deliver better insights than whatever those Medium blogs are describing.",
          "score": 4,
          "created_utc": "2026-01-23 16:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1a8kz7",
          "author": "mrGoodMorning2",
          "text": "\\* Queueing / streaming by offloading to Kafka (of course - good for absorbing burst traffic, less good for sustained hits)\n\nWhat don't like you about this solution? Each like/dislike being an event into a queue and then consuming a batch of events and persisting an entire batch into the DB once sounds good to me as a start. Kafka and scale easily with increasing topic partitions.\n\nWhat minimum cache hit rate do I have to maintain to offset the N multiplier for reads? Isn't this just finding a balance between client demand (how many popular videos are watched at any given time) and how much money you have for caching servers.",
          "score": 3,
          "created_utc": "2026-01-23 18:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1at41r",
              "author": "rkaw92",
              "text": "Well, I don't like that Kafka is often presented as a silver bullet that magically solves throughput problems. Yes, I could do a routing strategy where I use the video ID for partitioning, but then:\n\n\\* The like/unlike action is not idempotent, so the deduplication needs to happen downstream...\n\n\\* ...OR I need the target database checkpointing to be coupled with the Kafka offset management (transactionally remember where I left off), plus the message producer needs to have exactly-once semantics somehow, e.g. by using the Idempotent Producer feature\n\n\\* And additionally now we can get a hot partition in Kafka.\n\nFor the deduplication, I have considered Kafka Streams, but it's somewhat hard to produce a viable window (likes don't expire), and then we're just shifting the load from something that's plainly visible (like Redis) to... an embedded RocksDB instance? Not sure I like this trade-off.",
              "score": 1,
              "created_utc": "2026-01-23 20:07:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1namkt",
          "author": "Electrical_Effort291",
          "text": "I think I understand the requirements somewhat, so given your budget constraints, I'd design it like this:\n\nShard by users - each shard maintains a deterministic set of \"videos that this user liked\". Let's say you have N such servers. Each server maintains an in-memory counter for likes on each video in that shard. Every say 1 second, it sends this map of video:count to a central server that updates this into a key-value database (like DynamoDB) to update the global count. Once the central server acknowledges the update, each shard can clear out entries from it's in-memory map. This way, each shard doesn't actually have to keep a counter for all videos, just a counter for all videos that had like/dislike activity in the last second (or few seconds if the update needs to be retried a few times). Given the typical distribution of popular videos, there will only be at most a few million videos with like/dislike activities in a few seconds in each shard, so we are only talking about several MB of memory to maintain this. \n\nThe tradeoff here is that global counts will be several seconds out of date, but it's all consolidated into a single place, so serving reads isn't a problem (key-value stores can serve pretty high throughput, which can be further increased by a simple in-memory cache on the read server). \n\nSome ballpark estimates on why I think this will work - even if we assume every single person on the planet is an active user, we're looking at < 100 shards (with 80M users per shard, which is pretty reasonable to handle state for). So then the central server needs to accept < 100 requests per second to update the global counts. Assuming a million distinct videos in each message, with good array packing, this would be \\~8MB message size. The update process itself is easy since it's just adding a delta for each video. Of course, with packet losses/duplicate messages from the shards to the central server, the counts could be a little off, but I think that's acceptable - you only wanted exact counts per user, not per video.",
          "score": 2,
          "created_utc": "2026-01-25 16:54:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nhonk",
              "author": "rkaw92",
              "text": "I have considered something like this as well. This \"sending to central\" process (data collection) is a crucial piece. My problem with it is, it needs to be idempotent in the presence of failures, so it can't be plain increment in the central DB (Dynamo, Scylla, etc.). Can't use counter columns. At the same time, for the sender (the user shard), it should stop the world in terms of local increments. So, you have a potentially long time where the user shard's counters are locked while syncing to central.\n\nThis could be mitigated by not blocking anything, but instead operating only on deltas. So, I'd store the counters per-user and per-video in each shard, and when this has been applied to central, I'd decrement all the counters by as much as I managed to send in this batch. This seems more workable, but still needs careful synchronization and retry safety.\n\nBetween the user shards and the central \"likes-per-video\" DB, there needs to exist some form of progress tracking for retries and idempotence. The like counts should be updated only if they haven't been before. For this, we'd need to centrally track which updates we've applied from which shard, and these need to be used as some idempotence keys within the update process.\n\nNow, of course, DynamoDB, Cassandra nor Scylla are not transactional. You get LWT, but conditionally applying a batch operation comes with so many limitations that it might not work in this case at all. I wonder if I should look for an OLAP DB instead which enables this: perhaps ClickHouse with ReplacingMergeTree and some summing aggregation, or maybe even more exotic ones like QuestDB (last I saw it, it was developing quite nicely).",
              "score": 1,
              "created_utc": "2026-01-25 17:24:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nlu7s",
                  "author": "Electrical_Effort291",
                  "text": "\\>it needs to be idempotent in the presence of failures\n\nAre you sure you need this? Most online platforms showing like counts will show something like 8.1M likes - the actual counter can be off by 50k and it won't affect the display accuracy. A simple 3-5 retry mechanism on each shard is more than enough - it obviously doesn't guarantee idempotency/no data loss, but with typical uptimes for servers this won't be a problem in practice. You can make it even more rare by sending a unique message id with each unique message and on the central server, keep a list of the last 10 message ids seen from each server. I would really question the need for knowing *exact* like counts per-video.   \n  \n\\>At the same time, for the sender (the user shard), it should stop the world in terms of local increments.\n\nYou don't need to stop the world - local increments can continue as usual. You just need to decrement the local counter-map when a message is acknowledged from the server. Of course the decrement operation needs to lock the map, but something like ConcurrentHashMap in Java is more than adequate for this operation. You don't need to block local increments for the entire duration of the remote call. \n\nIf you want transactional semantics for the storing db, then just use a Postgres database (say in RDS). The data sizes we're talking about (maybe a trillion videos, 8 bytes per video so several TB of data) is something Postgres can easily handle with an index.",
                  "score": 2,
                  "created_utc": "2026-01-25 17:42:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o19r7ng",
          "author": "joelparkerhenderson",
          "text": "Here's what I would try first if you're curious about how to try it yourself:\n\n\\- Install Redis, run redis-benchmark, and look at GET, SET, INCR, DECR. On my older MacBook, each of those do 200,000 operations per second.\n\n\\- Install Rust, write a simple HTTPS server that accepts GET, PUT, PATCH, and call Redis with these.\n\n\\- Use any benchmark tool you like, such as wrk or oha, to discover your throughput of Axum PATCH to Redis INCR meaning \"like\" & DECR meaning \"unlike\".\n\n\\- If the throughput is lower than you need, then buy more CPU power because it's cheaper than development complexity. If your Redis is running low on memory, then buy more RAM because it's cheaper than development complexity. For lots of CPUs and RAM in one server I especially like the Oxide rack.\n\n\\- To get the counts from Redis into your persistent database such as Postgres, add an Axum step to use Redis LPUSH to track the recently-changed keys. Create a background batch job, such as every 10 minutes, to copy the key counts into Postgres.\n\n\\- To track a user's state per video, use Redis Streams and specifically event sourcing. Drain these with typical async background job into Postgres. The core idea here is that you're separating the concept of the video like count and the concept of one user state with regard to one video.\n\nOnce you get the above working, that's your baseline for what's possible using a simple system.\n\n\\- Deploy your video web project, and look at real world data.\n\n\\- If your traffic is growing so fast that you're going to outgrow your Oxide rack within one quarter or so, then look at vendors such as Redis Enterprise and Upstash, and also look at hiring a software engineer who has skills in scaling social networks and working knowledge of the various Facebook papers and Google papers about upscaling.\n\n\\- You mention below \"offloading this to a tier 2 storage + restoring sounds like a recipe for a synchronization nightmare\". In practice it's easier than it sounds, because it's such a good fit for background batch jobs and eventual consistency.\n\n\\- You mention below how to know if a user has liked a video. Start with your persistent storage e.g. Postgres and using a read-only replica when you build the user-video page or UI/UX. By the way, this is not a single source of truth, strictly speaking, because of database replica limitations (e.g. CAP & PACELC), yet in practice it's close enough to get you started. As above, you can try starting this way, so you can benchmark it. If/when you're approaching capacity limits, such as within one quarter or so, then revisit.",
          "score": 2,
          "created_utc": "2026-01-23 17:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19yova",
              "author": "IcyUse33",
              "text": "HyperLogLog could help here as well. It's not an exact amount of likes, but it is fast and it's good enough for estimates like \"1.2k likes\".",
              "score": 3,
              "created_utc": "2026-01-23 17:48:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1aop3c",
                  "author": "rkaw92",
                  "text": "I have considered HLL, but then, retracting a like becomes problematic. Sure, I could do an adaptive strategy like \"<100 likes = counter, >=100 go to HLL\", but then I admit the conditional logic and the edge behavior (including rescinding likes) are rather scary.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:46:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1anwum",
              "author": "rkaw92",
              "text": "Thanks for your response! Yeah, Redis is seriously great. An amazing piece of technology. It's my top pick so far, considering throughput. I concur with the numbers - I've had easy 200K+ results in the past; it's a good baseline. The thing is, if storage exceeds RAM, all hell breaks loose. If this was for counters only, it might be fine (as many keys as videos), but the \"no duplicate likes\" requirement seems to cause memory explosion, because now you have to remember who liked what. It's not just quantitative. I'm already considering some strong Redis-friendly optimizations like \"video IDs and user IDs shall be densely-packed integers with no holes\" - supposedly good old Redis can then use an optimized array-based index for the keys.\n\nI guess my main problem is this: Redis is well-suited for hot data, but it could be hard to justify keeping global state (counts + who liked what) forever, and offloading this to a \"tier 2\" storage + restoring sounds like a recipe for a synchronization nightmare. This is why I immediately jumped to sharding - I assume that I will run out of memory on a single host, so I'd rather plan for it than mitigate in other ways. The alternative, of course, is to splurge on RAM sticks, perhaps run 1 Redis instance per core, and see how many GBs/core we can manage :P\n\n  \nAlso I'm thinking more of a Set-type data structure for the change detection. A List doesn't seem too practical, given how when, say, a new super-popular K-Pop video drops, the same one item is going to be receiving likes all over. It's a niche case, sure, but AFAIK surges like these are normal for video sites like YouTube.\n\n100% finding people who have already solved this in the past would be great. The thing is, this is for an extremely underfunded volunteer project (think: budget = pocket lint), so in this case, the people is me :-|\n\n  \nThanks for mentioning the \"Oxide rack\" - right now, I'm trying to squeeze as much as possible out of truly frugal resources, so buying servers by the rack is not realistic. I have done some minor self-hosted DC work in the past, but this looks promising. Here's hoping the current RAM prices don't throw a monkey wrench in these guys' day-to-day.",
              "score": 2,
              "created_utc": "2026-01-23 19:43:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1adxrx",
          "author": "saravanasai1412",
          "text": "I would approach this problem as you need a sustainable solution without fancy tech. I would go with simple redis bloom filters which helps you to check that user liked the video or not .for quick lookup. It may have false positive but you check how LinkedIn does it. They just store recent post like in cache and sync with database in async.  \n\nNext issue counting the like. just use redis increment simple ans easy. Just sync it with db in certain interval.",
          "score": 1,
          "created_utc": "2026-01-23 18:56:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1auuy1",
              "author": "rkaw92",
              "text": "And that would work, but there still needs to be something that definitely (not probabilistically) tells the user if they have liked the item. As in, the information needs to live somewhere. It can't be Local Storage, because a user will inevitably want to access the system from multiple devices. Also, I'd like to keep a single source of truth for this, if at all possible.",
              "score": 1,
              "created_utc": "2026-01-23 20:15:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ns71f",
          "author": "Admirable_Swim_6856",
          "text": "Optimistic updates on the UI end so feedback is immediate, couple that with a job queue with retries for ensuring each like action is guaranteed",
          "score": 1,
          "created_utc": "2026-01-25 18:09:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlyzrq",
      "title": "Reference Project Layout for Modular Software",
      "subreddit": "softwarearchitecture",
      "url": "https://gist.github.com/ewaldbenes/a7879a187cedb47ed9744ad2929e5d79",
      "author": "ewaldbenes",
      "created_utc": "2026-01-24 21:09:25",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qlyzrq/reference_project_layout_for_modular_software/",
      "domain": "gist.github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1v5qtw",
          "author": "codingfox7",
          "text": "It's a bit similar to MIM Application Architecture, I've presented here: [Addressing the 'gray area' between High-Level and Low-Level Design - a Software Design tutorial](https://www.reddit.com/r/softwarearchitecture/comments/1pkr75u/addressing_the_gray_area_between_highlevel_and/)\n\nWhat troubles me, though, in your design is that it's too prescriptive. It's just too big for many microservices we've been writing (although some bigger ones are even more complicated).\n\nI would also anticipate some pushback from communities like Go (and some modern C# which tries to be more lean), because your version has two big \"layers\" (for me layers are an archaism). I mean, you have some code for \"bounded-context-B\" in \"infrastructure\" and some in \"core\". Why to break modularity and spread a module across layers? I'm writing this because you entitled this post \"Modular\", but instead \"layers\" were the first thing I've noticed here ;)\n\nI also don't get division between \"inbound\" and \"outbound\". Websockets, for instance, are duplex mechanism, so why are they in \"inbound\"? Would you split infrastructure of Websockets (or other like message queue, SSE) between \"inbound\" and \"outbound\" if both incoming and outgoing are used?\n\nBut yeah, that are only my opinions. The most important thing is whether the design works in your projects. It's far better than some 6-7 layer \"clean architectures\" I've seen.",
          "score": 1,
          "created_utc": "2026-01-26 18:28:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk67l9",
      "title": "Workflow Designer/Engine",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qk67l9/workflow_designerengine/",
      "author": "DesignMinute5049",
      "created_utc": "2026-01-22 20:44:33",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "We‚Äôre evaluating workflow engines to act as a central integration layer between SAP, AD/Entra ID, ticketing systems, and other platforms. Which solution would you recommend that provides robust connectors/APIs and integration capabilities? A graphical workflow designer is a nice-to-have but not strictly required.",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qk67l9/workflow_designerengine/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o14auh8",
          "author": "kartas39",
          "text": "n8n, airflow, kestra",
          "score": 2,
          "created_utc": "2026-01-22 21:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o171ezm",
          "author": "engx_ninja",
          "text": "Depends on quality attributes. Camunda might be enough, but Apache beam, or smth even more robust might be required",
          "score": 1,
          "created_utc": "2026-01-23 06:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o185p3o",
          "author": "itmanager_kz",
          "text": "Temporal",
          "score": 1,
          "created_utc": "2026-01-23 12:23:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1d8420",
          "author": "HosseinKakavand",
          "text": "If your workflow systems are starting to struggle under point-to-point choreography, I'd recommend [Luther](https://enterprise.luthersystems.com) designed for mega workflows. It takes a different approach‚Äîuses a Common Operational Script instead of visual builders, so you get Git versioning and all the dev tooling benefits while the platform handles connectors, retries, and error handling. It's scaled well at some of the largest corporations and has connectors for [SAP](https://www.reddit.com/r/luthersystems/comments/1qlcae7/sap_s4hana_connector_for_luther_workflownative/), Entra ID, [ServiceNow](https://www.reddit.com/r/luthersystems/comments/1qlcj7l/servicenow_connector_for_luther_enterprise/), and tons of other enterprise systems.",
          "score": 1,
          "created_utc": "2026-01-24 03:49:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qobhcf",
      "title": "I find system design abstract",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qobhcf/i_find_system_design_abstract/",
      "author": "piggy_piglet",
      "created_utc": "2026-01-27 11:37:32",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.84,
      "text": "I‚Äôve been reading system design interviews questions here and there.\n\nHowever, I find it very abstract and easy to forget afterwards. While reading, I sort of understand but I don‚Äôt think I fully understand. Afterwards, I forget about everything.\n\nIs it due to my lack of experience? Lack of knowledge? Being stupid? Or am I missing anything? Is it better that I just go ahead and build some personal projects instead? ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qobhcf/i_find_system_design_abstract/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o200ku6",
          "author": "PrinceUkaegbu",
          "text": "You're not stupid lol, system design feels abstract because its abstraction without constraints. \n\nMost system design content is taught backwards. You're given solutions (caches, queues< sharding, cap) without first experiencing the pressure that makes those solutions necessary. Without pressure, nothing sticks. \n\nsystem design only becomes concrete when something breaks: Latency spikes, write conflicts, costs explode, or maybe one bad request takes everything down. \n\nUntil you've felt that, diagrams feel like trivia. This is why building small, real projects help. Not to practice system design, but to create failure modes. Once you hit those limits, the abstractions suddenly make sense and stay with you. \n\nInterviews compress years of experience into hypotheticals. reading them without context will always feel hallow. The concepts make sense once you've seen why they exist.",
          "score": 15,
          "created_utc": "2026-01-27 11:48:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20p42n",
              "author": "Prateeeek",
              "text": "As a mid level developer, I always feel I have to cram all this information because I can't let the system fail and I want to get it right in the first place. This naturally makes me overengineer solutions sometimes, what are your views on failures in general?",
              "score": 1,
              "created_utc": "2026-01-27 14:16:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20rswa",
                  "author": "PrinceUkaegbu",
                  "text": "Look this is something i battle with not even just in software development but life in general.\n\nwanting to \"get it right the first time\" is a common trap. Its rational when failure feels expensive, but it also pushes you towards designing for imagined futures instead of the one you're actually in.\n\nFailure isn't valuable by default, contained failure is. The goal isn't to let systems blow up, its to make sure when they do fail, the blast radius is small and informative.\n\nEarly on, it's usually better to bias towards simpler designs with clear limits, then watch where they break. That tells you what actually needs hardening. overengineering upfront just seems to hide those signals.\n\nMost good system design comes from asking \"what's the cheapest failure i can learn from?\" rather than \"how do i prevent all failure\"\n\nInterviews make it look like people design perfect systems in one pass, in reality, most systems are a series of corrections made after reality disagrees with the one diagram.\n\nThat's been my experience, at least. lol",
                  "score": 5,
                  "created_utc": "2026-01-27 14:29:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o201dvu",
          "author": "ImpossibleClub4045",
          "text": "I‚Äôd focus on your experience and being able to talk to specific pieces of your resume which are system design related.\n\nArchitecture is VERY use case specific, async / synchronous patterns, choreography/orchestration patterns, when cache is needed. When to use those, in what situation. It is‚Ä¶ by design‚Ä¶ abstract without a use case lol.\n\nHelps to have a good understanding of the toolkit you have used and what you are aware of in the market. What are good patterns across a full stack‚Ä¶ not everyone knows everything but if you are aware of the software available and how it interacts from front end >> middleware >> backend ( including auth / controls / observability / etc,, ) you are well positioned to answer any questions or at least get close",
          "score": 2,
          "created_utc": "2026-01-27 11:54:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20vr4n",
          "author": "Icy_Screen3576",
          "text": "The way i did it is to start with standalone system design patterns. Microsoft and AWS have great resources on that. The problem is that these are standalone and give shallow examples. Next, you need to apply them, better on your real projects, you will benefit from the discussions with teammates. \n\nOne thing that helped me is categorizing them like that.\n\nhttps://preview.redd.it/f9n2zw8rmwfg1.png?width=1536&format=png&auto=webp&s=6dea63e1207741cac8fa11b0bd41a257ec7b96a3\n\nYou see it is not an easy journey. The kind of pain that will surely provide you with long term gain.",
          "score": 2,
          "created_utc": "2026-01-27 14:49:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20n46u",
          "author": "Ambitious_Fruit6231",
          "text": "Thinking about the design for something that you ( or anybody else) are not going to build is by its nature going to feel abstract. And so it doesn't really stick. Others have given good advice in this respect, so the only thing to add is to think about the \"why\". Why is this solution or pattern commonly used to solve this problem/provide this functionality. What makes it better than other options. \n\nExpanding on this is to develop criteria for evaluating design choices in addition to the functional requirements. Does portability matter? Does it matter more than performance? In the future might I sell this to another customer who wants much the same but with some different features / functionality - can I design with reuse in mind.\n\nTypically good architecture/design supports these additional criteria but there is a value in thinking about them in their own right",
          "score": 1,
          "created_utc": "2026-01-27 14:06:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o211mff",
          "author": "SolarNachoes",
          "text": "Start with small systems and learn them well. Redesign them and know the trade offs and why each choice was made. Do mock whiteboard interviews.",
          "score": 1,
          "created_utc": "2026-01-27 15:17:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qm0alq",
      "title": "What Would You Change in This Tech Stack?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qm0alq/what_would_you_change_in_this_tech_stack/",
      "author": "KodKodKO",
      "created_utc": "2026-01-24 21:59:43",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.83,
      "text": "Hey everyone,\n\nBefore we disappear into dev land for the next 6 months, I‚Äôd love a sanity check.\n\nWe‚Äôre building an app for some recruiter friends that ran very long-running niche recruiting for specialised industries. The idea is simple: those recruiters need to jungle multi-year recruiting cycles with a pool of 200+ candidates in each role, so based on conversation/ notes emails/responses etc :\n\n‚Ä¢\textracts key info via LLM (candidate, next steps, follow-up date, etc.)\n\n‚Ä¢\tcreates follow-up tasks\n\n‚Ä¢\toptionally generates a Gmail draft\n\n‚Ä¢\ttriggers reminders (‚Äúfollow up in 3 days‚Äù)\n\n‚Ä¢\teventually supports teams (orgs, roles, audit logs, etc.)\n\nBasically: AI-powered ‚Äúdon‚Äôt forget to follow up‚Äù for long-term candidates.\n\n‚∏ª\n\nCurrent Stack\n\nFrontend\n\n‚Ä¢\tNext.js 14 (App Router)\n\n‚Ä¢\tTypeScript\n\n‚Ä¢\tTailwind\n\n‚Ä¢\tDeployed on Vercel\n\nBackend\n\n‚Ä¢\tPostgres (Vercel Postgres or Supabase)\n\n‚Ä¢\tPrisma\n\n‚Ä¢\tAuth.js / NextAuth (Google OAuth + email login)\n\nAI\n\n‚Ä¢\tClaude API (structured outputs ‚Üí tasks + draft emails)\n\nIntegrations\n\n‚Ä¢\tGmail API (OAuth, starting with draft creation only)\n\n‚Ä¢\tLinkedIn (TBD ‚Äî maybe just store URLs or build a Chrome extension)\n\n‚∏ª\n\nWhat we‚Äôre trying to avoid\n\nWe‚Äôre two engineers. We don‚Äôt want to over-engineer.\n\nBut we also don‚Äôt want to wake up in 6 months and realize:\n\n‚Ä¢\tserverless + Prisma was a mistake\n\n‚Ä¢\twe should‚Äôve separated frontend/backend earlier\n\n‚Ä¢\tGmail OAuth/token refresh becomes a nightmare\n\n‚Ä¢\twe needed a job queue from day one\n\n‚∏ª\n\nBe brutally honest and roast my stack if needed, I‚Äôd rather pivot now than refactor everything later.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qm0alq/what_would_you_change_in_this_tech_stack/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1ksgqn",
          "author": "Waste_Cup_4551",
          "text": "I‚Äôd use Drizzle over Prisma, and Tanstack Start over Next. But if you‚Äôre already experienced with Next, stick to that.\nTanstack Start now has pretty good boilerplate support for different support, plus an mcp server for your AI coding agents.",
          "score": 1,
          "created_utc": "2026-01-25 06:53:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1irhcg",
          "author": "Glove_Witty",
          "text": "How many active users are you planning for. I‚Äôd also say with ai that the costs of fixing architectural mistakes are going down - so, in the future, if you want to hosts on plain old AWS it would not be that big a deal.",
          "score": 1,
          "created_utc": "2026-01-24 23:38:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kpfn3",
              "author": "KodKodKO",
              "text": "Thank you, looking between 100-200 MAUs for the MVP",
              "score": 2,
              "created_utc": "2026-01-25 06:28:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o94xq",
                  "author": "Glove_Witty",
                  "text": "Think you are good. As you scale you‚Äôll want to get off supabase and vercel for cost reasons.",
                  "score": 1,
                  "created_utc": "2026-01-25 19:19:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjuaa4",
      "title": "Is there a technology for a canonical, language-agnostic business data model?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qjuaa4/is_there_a_technology_for_a_canonical/",
      "author": "nounoursnoir",
      "created_utc": "2026-01-22 13:17:59",
      "score": 7,
      "num_comments": 23,
      "upvote_ratio": 0.77,
      "text": "I'm looking for opinions on whether what I'm describing exists, or if it's a known unsolved problem.\n\nI wish I could model my business data in a single, canonical format dedicated purely to *semantics*, independent of programming languages and serialization concerns.\n\nToday, every representation is constrained by its environment:\n\n* In JS, a matrix is a list of lists or a custom object or a Three Matrix4\n* In Python, it's a NumPy array\n* In Protobuf, it's a verbose set of nested messages\n* In a database, it's likely a raw JSON.\n\nEach of these representations leaks implementation details and forces compromises. None of them feel like an ideal way to express *what the data fundamentally is* from a pure functional, business perspective.\n\nWhat I'd like is:\n\n* One unique source of truth for business data semantics\n* All other representations (JS, Python, Protos, etc.) being constrained projections of that model (ideally a compiler would provide this for us, similarly to how gRPC's protoc compiler provides clients and servers in multiple languages based on a set of messages and RPCs)\n* Each target being free to add its own idioms and logic (methods, performance structures, syntax), but not redefine meaning\n\nThink of something closer to a semantic or algebraic model of data, rather than a serialization format or programming language type system.\n\nThe most similar thing I can think of is Cucumber or Gherkin for automated tests (although you hand-write the code associated with each sentence).\n\nDoes something like this exist for a whole system architecture (even partially)?  \nIf not, is this a known design space (IDLs, ontologies, DSLs, type theory, etc.) that people actively explore?\n\nI'm interested both in existing tools and in why this might be fundamentally hard or impractical.\n\nThank you.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qjuaa4/is_there_a_technology_for_a_canonical/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o11nvys",
          "author": "steve-7890",
          "text": "It sounds like you're looking for: UML and/or BPML.\n\nBut remember, \"The paper accepts everything\". Code won't.",
          "score": 5,
          "created_utc": "2026-01-22 13:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o127dyu",
              "author": "nounoursnoir",
              "text": "BPML/BPMN model *process semantics*, not *data semantics*. I'm looking for a canonical, language-agnostic way to define what business data **is**, not how workflows execute.  \nUML is one way to represent a model, but it's still a language with its own constraints and conventions. It can define an implementation-like structure, yet it's tied to its own notation and doesn't capture the full semantic meaning of the data. In that regard it is not much different from any other language, like Python or JS.\n\nThe matrix is a good example to outline the difference between implementation and meaning.  \nThe concept of matrix is a 2D grid that allows for complex mathematical operations, useful in domains like physics, 3D or machine learning. Notions of data structures like an array, a dict, a class, a struct or whatever are irrelevant in this *conceptual* realm.\n\nYou're right to bring up UML, in that it's likely better suited to most industry needs and priorities. Often, defining structure and workflow is sufficient, and creating a business data model completely decoupled from technical implementation can be too niche. That said, such a system would probably have many advantages: I believe it would be simpler, more accessible to non-technical users, and more flexible.",
              "score": 2,
              "created_utc": "2026-01-22 15:27:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14oc1n",
                  "author": "Ok-East-515",
                  "text": "Are you describing actual human language but trying to make it complicated?¬†",
                  "score": 2,
                  "created_utc": "2026-01-22 22:15:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o17ac5l",
                  "author": "steve-7890",
                  "text": "You're not gonna find exactly what you are looking for ;) \n\nOn the other hand, you've already found it in a form of Code. So there are hundreds  of syntaxes you're looking for, just pick one.\n\nhttps://preview.redd.it/idnktt0y02fg1.png?width=650&format=png&auto=webp&s=18e9d9360f7c880381832097c730d7440005e51c",
                  "score": 2,
                  "created_utc": "2026-01-23 07:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11qohx",
          "author": "NeuronSphere_shill",
          "text": "We built a whole software stack around this idea.\n\nModel once, then you can code gen N implementations.",
          "score": 2,
          "created_utc": "2026-01-22 14:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o121jbv",
              "author": "nounoursnoir",
              "text": "What is the support of the model? What technology?",
              "score": 2,
              "created_utc": "2026-01-22 14:59:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12a18j",
          "author": "BarfingOnMyFace",
          "text": "Not sure I follow you. You could build a centralized model by normalizing to whatever extent suits you in a database, no? That becomes your unique source of truth for data semantics‚Ä¶. If all these different structure types will be representing the same underlying data but in different consumable packages, why not‚Ä¶ standardize what you need, semantically, to database table(s)? Not stored as JSON, but as a set of shared attributes.\n\nWhy wouldn‚Äôt this work for you? Sorry if I‚Äôm being dense.",
          "score": 2,
          "created_utc": "2026-01-22 15:40:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12dvum",
              "author": "nounoursnoir",
              "text": "A database, like a language or a communication protocol, all *implement* concepts. Structure and technical constraints are inherently tied to this modelization. What I'm looking for is a perfect separation between the *semantic* and the *implementation*. I would like to be able to define a part of my system as a 4x4 Matrix, knowing what a 4x4 matrix is *in principle*, and only when this pure representation is made, I can define implementations for it in the different technologies that I use.",
              "score": 0,
              "created_utc": "2026-01-22 15:57:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o12oz4v",
                  "author": "BarfingOnMyFace",
                  "text": "Hmmm, I dunno, I‚Äôm clueless on this. lol. I did do some googling and AI review to try and resolve my cluelessness, and it all come back saying there is no such tooling out there. here were the closest suggestions on this from chat:\n\n‚úÖ Algebraic / semantic core (exists, but academic)\n\nAlgebraic specification languages\n\t‚Ä¢\tCASL\n\t‚Ä¢\tOBJ\n\t‚Ä¢\tMaude\n\nThey let you say:\n\t‚Ä¢\tA Matrix4x4 exists\n\t‚Ä¢\tThese operations exist\n\t‚Ä¢\tThese laws must hold\n\t‚Ä¢\tNo representation is implied\n\nProblem:\nThey stop before code",
                  "score": 1,
                  "created_utc": "2026-01-22 16:47:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1321dt",
          "author": "cybDrachir",
          "text": "What about JsonSchema?",
          "score": 1,
          "created_utc": "2026-01-22 17:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17ze6d",
              "author": "nounoursnoir",
              "text": "Looks good for structure/validation, but not semantics.",
              "score": 2,
              "created_utc": "2026-01-23 11:37:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o137yma",
          "author": "GrogRedLub4242",
          "text": "SQL/DDL",
          "score": 1,
          "created_utc": "2026-01-22 18:12:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17zn7y",
              "author": "nounoursnoir",
              "text": "Like any other language, they define implementation, not meaning.",
              "score": 1,
              "created_utc": "2026-01-23 11:39:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13r35k",
          "author": "Turtlestacker",
          "text": "Not sure I understand your question tbh but keel.so must have solved something like this?",
          "score": 1,
          "created_utc": "2026-01-22 19:37:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1803hg",
              "author": "nounoursnoir",
              "text": "This looks great, thanks!",
              "score": 1,
              "created_utc": "2026-01-23 11:42:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o148o3k",
          "author": "aphillippe",
          "text": "Are you taking about a logical data model? A representation of the business domain‚Äôs data requirements in its ‚Äòpurest‚Äô form, abstract of any implementation or technical detail. It can be helpful in a model-first approach to sketch out what the data looks like in abstract, and then design the various physical data models (UI, service layer data dictionary, operational database, ODS, data warehouse all referring and mapping back to the logical model. It becomes the blueprint for all physical data models, and also ties neatly into anything behavioural (service operations, data warehouse facts etc.) as those behavioural artifacts (transaction, order, whatever) have all been modelled already. Or maybe I just find it useful since I‚Äôm a data guy",
          "score": 1,
          "created_utc": "2026-01-22 20:59:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17zrl9",
              "author": "nounoursnoir",
              "text": "Exactly!",
              "score": 1,
              "created_utc": "2026-01-23 11:40:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e917g",
          "author": "UnreasonableEconomy",
          "text": "Dumping some thoughts on the matter\n\n---\n\nHave you ever worked with typescript?\n\nI'm not talking about js + java-like types. \n\nThe typescript type system, particularly the \"type\" type. \n\nIt's not a 1-1 match for what you're looking for, and it doesn't really transpile automatically into anything. \n\nBasically what it allows you to do is define arbitrary types and how they relate to each other. \n\nYou can define operators that constrain what these types do, or how they transform between one another. \n\nAll this is implementation independent.\n\nWe use this for dimensional analysis and linear algebra in our gis system. This allows us to reason about what data is, and when. If you divide meters by seconds, the type system knows it's meters per second. it's impossible to confuse pixels per second on the device screen manifold with pixels per second in the render context with meters per second on the earth manifold, even though they're all related.\n\nHow are they calculated or represented under the hood? Kind of irrelevant at this level.\n\n---\n\n> why this might be fundamentally hard or impractical.\n\nTypescript is incredibly powerful, but I think you're gonna have a hard time staffing for people that can use it like this. Maybe the TS subreddit if you ask around, but it's not something the average sw engineer is going to be capable of or comfortable working with. It's closer to prolog or haskell than anything else. (never worked with haskell though)\n\nNow using the produced types? That's easy. But type development and maintenance might not be worth it. \n\nthat's why we cheat a lot, but try to keep the cheating contained. E.g., rotation matrices are just functions coming out of factories. You could abstract it into a matrix, but the matrix type isn't ready (we don't yet know how to mix units and manifolds and matrices in a useful way, plus low ROI to solving this (IMO hard) problem).\n\n---\n\nAlso, from experience, it's not something you can design up front. Sometimes the ergonomics are just crap. The ROI of all that hard work might not be there, and just using it out of sunk cost also doesn't make sense. In college you learn about DSLs in eclipse, but realistically, reality is hard. BDUF doesn't work, and redesigning DSLs like in xtext/xtend or whatever was a pain IIRC.\n\nMy \"DSL\" of choice is JSON or YAML. This, with a validator and good docs is super good enough for so many cases. Custom parsers are just too unwieldy to maintain, especially if you only have a handful of DSL users anyways.\n\n---\n\nTL;DR:\n\nYeah, it's a real hard problem, with no real solution. Academics and researchers tend to come up with a bunch of stuff that doesn't survive contact with reality. Maybe one day, but today is not that day. Beware of the tarpit lol.",
          "score": 1,
          "created_utc": "2026-01-24 08:39:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o124w5e",
          "author": "arnedh",
          "text": "Look into Archimate: BusinessObject, DataObject, Artifact, Representation and relations to from/to/among these. Archimatetool.org",
          "score": 1,
          "created_utc": "2026-01-22 15:15:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o180gzh",
              "author": "nounoursnoir",
              "text": "Interesting, thanks",
              "score": 1,
              "created_utc": "2026-01-23 11:45:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qng3uz",
      "title": "Service layer in MVC architecture",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qng3uz/service_layer_in_mvc_architecture/",
      "author": "sowhatelsee",
      "created_utc": "2026-01-26 13:32:36",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "Does anyone make MVC diagrams and add a separate box like a service layer or engine? I tried to search for examples but I didn‚Äôt find any, i wanna know how it may look when adding a service layer\n\nI want to add it because my diagram right now has heavy loads on the controller, so if I want to separate it, the instructor said to use a separated class to avoid this design flow\n\nIf you have any resources that could be helpful here, I would rlly appreciate it! ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qng3uz/service_layer_in_mvc_architecture/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1tgq1y",
          "author": "ryan_the_dev",
          "text": "In my mind MVC is more of a frontend framework. You can bolt on the backend stuff, but I would treat MVC app like I would any frontend/bff. \n\nThat being said, of course you need to talk to dependencies. Using a service layer or any other integration type layer. \n\nIn terms of diagrams, I don‚Äôt typically map out application architecture. We usually have a predefined way we are going to structure the application. \n\nDifferent techniques for different needs.",
          "score": 3,
          "created_utc": "2026-01-26 13:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1tj9jo",
          "author": "PassengerExact9008",
          "text": "Absolutely, adding a service layer in MVC is a great way to offload business logic from controllers. In your diagram, place a Service/Engine box between the Controller and Model to show that controllers delegate processing to services, keeping each layer focused and clean. For clear, quick visual iteration of this and other architectural ideas, try Digital Blue Foam, it helps you sketch and refine diagrams fast without getting bogged down in tooling.",
          "score": -2,
          "created_utc": "2026-01-26 14:07:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}