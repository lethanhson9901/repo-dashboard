{
  "metadata": {
    "last_updated": "2026-02-08 16:49:56",
    "time_filter": "week",
    "subreddit": "softwarearchitecture",
    "total_items": 20,
    "total_comments": 132,
    "file_size_bytes": 158117
  },
  "items": [
    {
      "id": "1qtw76q",
      "title": "We skipped system design patterns, and paid the price",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qtw76q/we_skipped_system_design_patterns_and_paid_the/",
      "author": "Icy_Screen3576",
      "created_utc": "2026-02-02 14:16:04",
      "score": 320,
      "num_comments": 48,
      "upvote_ratio": 0.97,
      "text": "We ran into something recently that made me rethink a system design decision while working on an event-driven architecture. We have multiple Kafka topics and worker services chained together, a kind of mini workflow.\n\n[Mini Workflow](https://preview.redd.it/fgm3nejx93hg1.png?width=3750&format=png&auto=webp&s=7965432a4731f2658c648475b1d90593a0f69282)\n\nThe entry point is a legacy system. It reads data from an integration database, builds a JSON file, and publishes the entire file directly into the first Kafka topic.\n\n# The problem\n\nOne day, some of those JSON files started exceeding Kafka’s default message size limit. Our first reaction was to ask the DevOps team to increase the Kafka size limit. It worked, but it felt similar to increasing a database connection pool size.\n\nThen one of the JSON files kept growing. At that point, the DevOps team pushed back on increasing the Kafka size limit any further, so the team decided to implement chunking logic inside the legacy system itself, splitting the file before sending it into Kafka.\n\nThat worked too, but now we had custom batching/chunking logic affecting the stability of an existing working system.\n\n# The solution\n\nWhile looking into system design patterns, I came across the Claim-Check pattern.\n\n[Claim-Check Pattern](https://preview.redd.it/3lmiy1t9a3hg1.png?width=3332&format=png&auto=webp&s=890aa3c5d542d979c9e7eb0d564dcfa576aa9276)\n\nInstead of batching inside the legacy system, the idea is to store the large payload in external storage, send only a small message with a reference, and let consumers fetch the payload only when they actually need it.\n\n# The realization\n\nWhat surprised me was realizing that simply looking into existing system design patterns could have saved us a lot of time building all of this.\n\nIt’s a good reminder to pause and check those patterns when making system design decisions, instead of immediately implementing the first idea that comes to mind.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qtw76q/we_skipped_system_design_patterns_and_paid_the/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o35sph1",
          "author": "Estel-3032",
          "text": "I remember that in my first job one of the other engineers said 'so let's check what kind of wheels people are using out there before we start inventing our own' to a roughly similar situation and it stuck with me.",
          "score": 116,
          "created_utc": "2026-02-02 14:33:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35yybb",
              "author": "Icy_Screen3576",
              "text": "Sounds like a pragmatic engineer.",
              "score": 27,
              "created_utc": "2026-02-02 15:05:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o363lmg",
          "author": "czlowiek4888",
          "text": "Yeah, this is exactly what you should do.\n\nDon't treat messages in your system as a data storage ( it is convenient though ) but more like notifications.\n\nYou just want to send event telling you what happened, not necessarily why, how, where and when. All those other information you should get on your own from database or other storage when you think it's necessary.\n\nIn real time system you usually want to have each message under ~1.4kb this is the frame size in which your messages are send over the network.\n\nBecause if you need to pass larger messages you will need wait for the all other frames that together create single message.\n\nThis way if you send only 1 frame you can go crazy fast.\n\nAlso Kafka uses stores messages to be replied when necessary, you will be able to store more messages.\n\nYou also may want to think about private replies of messages. For example you have service that receives http request, you send event and await other in response. You need to know how to send a response event to the instance of app that holds http socket file to be able to respond to the http request with the event data.\n\nIt's a bit more advanced but it is what many event driven systems need.",
          "score": 24,
          "created_utc": "2026-02-02 15:28:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36bjxc",
              "author": "Icy_Screen3576",
              "text": "Well said. Keeping messages small and event-focused made things a lot simpler.",
              "score": 5,
              "created_utc": "2026-02-02 16:06:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3663cp",
          "author": "bigkahuna1uk",
          "text": "I think everyone should read [Enterprise Integration Patterns](https://www.enterpriseintegrationpatterns.com/patterns/messaging/toc.html) by Gregor Hophe  .\n\nOver 20 years old but still highly relevant today.",
          "score": 43,
          "created_utc": "2026-02-02 15:40:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3694rz",
              "author": "bobaduk",
              "text": "Came here to say exactly this. IIRC the patterns are all described online, so you can skim and get a vague sense, then go back to look deeper when you need something.\n\nMessaging patterns have been established for a long time, and it's worth being familiar with the prior art.",
              "score": 6,
              "created_utc": "2026-02-02 15:54:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o369oto",
              "author": "Icy_Screen3576",
              "text": "Thanks for sharing!",
              "score": 2,
              "created_utc": "2026-02-02 15:57:19",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3bmq78",
              "author": "garden_variety_sp",
              "text": "And every pattern has been implemented by Apache Camel, the GOAT of integration frameworks. And 100% free and open source.",
              "score": 1,
              "created_utc": "2026-02-03 11:07:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o35qx62",
          "author": "AzureMate",
          "text": "Clever! Thanks for sharing!",
          "score": 15,
          "created_utc": "2026-02-02 14:24:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35uopk",
              "author": "Icy_Screen3576",
              "text": "You are welcome! Glad it helped.",
              "score": 1,
              "created_utc": "2026-02-02 14:43:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o363xwc",
          "author": "tesseraphim",
          "text": "It worked, the system kept chugging for quite some time. That's a win. It could have worked for 10 years, some systems do. The question is, how much change you needed to do. Trick is not to build up front, but make sure the seams are there so you can easily change.",
          "score": 7,
          "created_utc": "2026-02-02 15:30:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35y599",
          "author": "Few_Wallaby_9128",
          "text": "It does come at a price, right? an extra single point of failure, extra latency, possible network failures and managament (dns/cert renewals/fws), and logic to handle the lifecycle, synchronization and deletion of the data in the storage. If the growing json was the problem, dynamic zipping of it could have worked wonders at a fraction of the total cost of maintenance.",
          "score": 13,
          "created_utc": "2026-02-02 15:01:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35zedo",
              "author": "europeanputin",
              "text": "Software engineering is always full of trade-offs. I have a non-fixed size JSONs, but due to compliance reasons there's no way that they could be pulled on-demand, and I simply have to store them all, regardless of their size. Some documents are about the size of 10mb after doing the compression.",
              "score": 14,
              "created_utc": "2026-02-02 15:08:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o364msa",
              "author": "czlowiek4888",
              "text": "You shouldn't think about it as a trade off.\nThis is the one and only correct way.\n\nSending events with the data is anti pattern imho.\nWhat if you want to add CQRS and you regenerate your state from commands?\nAnd now data you hold in your messages is no longer valid because it was changed how app processes things so your message as a storage approach make you not able to perform state regeneration.",
              "score": 4,
              "created_utc": "2026-02-02 15:33:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3650bk",
                  "author": "czlowiek4888",
                  "text": "Also what if your messages store personal information that you are obligated to remove in certain situations.\nYou will be deleting messages and this will lead to inability to regenerate state as disaster recovery mechanism.",
                  "score": 3,
                  "created_utc": "2026-02-02 15:35:24",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38r9ib",
                  "author": "pins17",
                  "text": "For OP's scenario I agree. But in general it really depends on the use case.\n\nFor example, in high-frequency scenarios like price updates on energy or commodity markets, it is standard practice to include the market data directly in the event. That is the whole point. Doing a lookup for every event would introduce unacceptable latency and massive load on the source system. The same applies to telemetry data in fleet management.\n\nClaim Check has its merits, as you mentioned, but it also has downsides. While the producer is free from temporal/runtime coupling, the consumer is not, which negates one of the main benefits of asynchronous architecture.",
                  "score": 2,
                  "created_utc": "2026-02-02 22:56:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o35s577",
          "author": "cstopher89",
          "text": "This is a nice pattern. I use it for Azure Service Bus messages to handle large payloads.",
          "score": 2,
          "created_utc": "2026-02-02 14:30:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35w6va",
              "author": "Icy_Screen3576",
              "text": "In our case it was an on-prem Kafka broker, with the payload in external storage. Do you usually pair Service Bus with Blob Storage for this?",
              "score": 2,
              "created_utc": "2026-02-02 14:51:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o37zfxv",
                  "author": "01acidburn",
                  "text": "Yep",
                  "score": 1,
                  "created_utc": "2026-02-02 20:42:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o38qc8h",
                  "author": "cstopher89",
                  "text": "Yeah blob storage works well for this",
                  "score": 1,
                  "created_utc": "2026-02-02 22:51:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o36mkf6",
          "author": "nt2g",
          "text": "Great post and great reminder, thank you for sharing!",
          "score": 2,
          "created_utc": "2026-02-02 16:56:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37a0ej",
          "author": "dukemanh",
          "text": "question: what will happen and what should we do if the tiny message already arrived at the consumer but the large payload is not yet available on the file storage?",
          "score": 2,
          "created_utc": "2026-02-02 18:43:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37x2tn",
              "author": "Primary-Juice-4888",
              "text": "Consumer - retry message processing until file is available, perhaps with exponential backoff.\n\nor\n\nProducer - only send a message after the storage write was confirmed.",
              "score": 5,
              "created_utc": "2026-02-02 20:30:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3bmwkd",
          "author": "garden_variety_sp",
          "text": "Did you consider using a more compact wire format like Avro or Protobuf?",
          "score": 2,
          "created_utc": "2026-02-03 11:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3c3yre",
              "author": "Icy_Screen3576",
              "text": "Considered avro, still we would be pushing in the wrong direction. Thinking in tiny events made things simpler. I dont think message brokers are made for large payloads.",
              "score": 1,
              "created_utc": "2026-02-03 13:13:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o36raxr",
          "author": "Samrit_buildss",
          "text": "Really nice write-up. The claim-check pattern here is a great reminder that many scaling problems already have well-known solutions we just forget to look for them under pressure.",
          "score": 2,
          "created_utc": "2026-02-02 17:18:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35utna",
          "author": "Constant_Physics8504",
          "text": "Could’ve been turned into a Dispatch system with MQ quite easily",
          "score": 1,
          "created_utc": "2026-02-02 14:44:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o363d0a",
              "author": "Icy_Screen3576",
              "text": "Yep, we only use claim-check for large payloads. Most messages go through the message broker.",
              "score": 3,
              "created_utc": "2026-02-02 15:27:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o37gnoz",
          "author": "bunsenhoneydew007",
          "text": "We use the claim check pattern extensively on a similar workflow like system. It works extremely well and allows the payload to be agnostic to the event transfer mechanism, which can provide other benefits regarding data processing in the services. (We use eventbridge rather than Kafka).",
          "score": 1,
          "created_utc": "2026-02-02 19:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ktml",
          "author": "Careless-Childhood66",
          "text": "Amen",
          "score": 1,
          "created_utc": "2026-02-02 19:33:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38o6pt",
          "author": "ErgodicMage",
          "text": "I develop distrubuted workflow systems and use Claims all the time.",
          "score": 1,
          "created_utc": "2026-02-02 22:40:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hhofa",
          "author": "Mean_Helicopter_2913",
          "text": "Ngl skipping design patterns can save time upfront but ends up costing you later.  tbh, having that extra layer of abstraction and modularity would have made debugging and scaling much easier.",
          "score": 1,
          "created_utc": "2026-02-04 06:10:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3p0880",
          "author": "blocked909",
          "text": "Learnt something new as a novice",
          "score": 1,
          "created_utc": "2026-02-05 10:26:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3p4hqk",
              "author": "Icy_Screen3576",
              "text": "Great",
              "score": 1,
              "created_utc": "2026-02-05 11:05:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3s16co",
          "author": "ConcreteExist",
          "text": "The service bus I've been integrating with works exactly like the example there, the only payload in the message is a file path to the stored xml file that can then be retrieved from blob storage and parsed accordingly.",
          "score": 1,
          "created_utc": "2026-02-05 20:15:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v825n",
              "author": "Icy_Screen3576",
              "text": "https://preview.redd.it/varzr7p4zthg1.png?width=3257&format=png&auto=webp&s=4df3323efb38acf780e4084dbabed100b1e050e7\n\nSimilar to that?",
              "score": 1,
              "created_utc": "2026-02-06 07:59:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3vyiz3",
                  "author": "ConcreteExist",
                  "text": "Not quite, as the service bus publishes events to a topic that my reader is subscribed to, but the message body is just a file path to be retrieved from a blob storage container.",
                  "score": 1,
                  "created_utc": "2026-02-06 11:58:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3xgaut",
          "author": "CuticleSnoodlebear",
          "text": "Be careful. Now you share a data source across your legacy and new platforms",
          "score": 1,
          "created_utc": "2026-02-06 16:47:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xk2p1",
              "author": "Icy_Screen3576",
              "text": "Good call. Strict access control to external file storage is needed. Usually, they provide a token that should be limited by scope and time. On the other hand, having an observability tool monitoring your message broker topics can also leak those info to an insider. It's a tradeoff we agreed to accept in front of the cost and performance gain we achieved.",
              "score": 1,
              "created_utc": "2026-02-06 17:05:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xgzrb",
          "author": "Rumertey",
          "text": "You didn’t pay the price. This is how design patterns should be used. You need a problem first to build a solution, otherwise you will end up with an over-engineered codebase that no one understands",
          "score": 1,
          "created_utc": "2026-02-06 16:50:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3yb8hs",
          "author": "PassengerExact9008",
          "text": "It’s tempting to skip design patterns to move faster, but your example shows why they’re so useful for managing complexity and keeping things maintainable. Learning to use them early can save a lot of headaches down the road.",
          "score": 1,
          "created_utc": "2026-02-06 19:14:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3mzlai",
          "author": "Dangerous-Sale3243",
          "text": "This seems pretty obvious to me and i would imagine it’s the first thing google or an LLM would tell you to do. Maybe because the dev team doesnt feel they own the infrastructure, they think they need to use software to solve problems with infrastructure is the answer.",
          "score": -1,
          "created_utc": "2026-02-05 01:22:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3osf30",
              "author": "Icy_Screen3576",
              "text": "Not owning the infra is a good catch. I would be hesitant to trust the ai on such matters.",
              "score": 1,
              "created_utc": "2026-02-05 09:11:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv07p2",
      "title": "At what scale does \"just use postgres\" stop being good architecture advice?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qv07p2/at_what_scale_does_just_use_postgres_stop_being/",
      "author": "Designer-Jacket-5111",
      "created_utc": "2026-02-03 18:38:02",
      "score": 103,
      "num_comments": 40,
      "upvote_ratio": 0.93,
      "text": "Every architecture discussion I see ends with someone saying \"just use postgres\" and honestly theyre usually right. Postgres handles way more than people think, JSON columns, full text search, pub/sub, time series data, you name it.\n\nBut there has to be a breaking point where adding more postgres features becomes worse than using purpose-built tools. When does that happen? 10k requests per second? 1 million records? 100 concurrent writers?\n\nIve seen companies scale to billions of records on postgres and Ive seen companies break at 10 million. Ive seen people using postgres as a message queue successfully and Ive seen it be a disaster.\n\nWhat determines when specialized tools become necessary? Is it always just \"when postgres becomes the bottleneck\" or are there other architectural reasons?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qv07p2/at_what_scale_does_just_use_postgres_stop_being/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3dyu5h",
          "author": "Sea_Weather5428",
          "text": "the breaking point for me is when the postgres-specific workarounds start taking more time than learning a purpose-built tool would, like when your jsonb queries start needing 47 indexes",
          "score": 86,
          "created_utc": "2026-02-03 18:39:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e9zq8",
              "author": "sfboots",
              "text": "We stay away from indexes on the Jsonb content and copy a few items to regular columns for indexing",
              "score": 35,
              "created_utc": "2026-02-03 19:30:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3khdoy",
                  "author": "its_k1llsh0t",
                  "text": "We do a mix of both. Index where return speed is less important, then normalize for things where performance is more important. ",
                  "score": 1,
                  "created_utc": "2026-02-04 17:53:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e15qs",
              "author": "ilya47",
              "text": "And when you learn about TOASTs due to jsonb.",
              "score": 13,
              "created_utc": "2026-02-03 18:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gnuke",
                  "author": "clearing_",
                  "text": "giving me ptsd to when we hit the OID limit while i was at a concert",
                  "score": 3,
                  "created_utc": "2026-02-04 02:51:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dzqtl",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 29,
          "created_utc": "2026-02-03 18:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3epfd9",
              "author": "InfluxCole",
              "text": "I think there's also some cost efficiency to worry about as scale goes up. Once you're running up huge monthly bills, it's not that you necessarily couldn't keep going with Postgres, but you could probably save some money by moving to something more tailor-made for the characteristics of your workload. When you reach that, \"this is getting expensive, maybe something more specific would give us the performance we need for cheaper,\" point still heavily depends on your company, budget, team size, etc.",
              "score": 6,
              "created_utc": "2026-02-03 20:43:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eou27",
          "author": "polotek",
          "text": "We had a ruby on rails system with postgres that scaled to 10 billion rows in some tables and still maintained high request throughout. The problem isn't scaling postgres. It's not easy, but it can go way further than most people will ever need. It depends on the complexity of what you're doing and your level of expertise with postgres.\n\nWhat comes after \"just use postgres\" is \"hire some postgres consultants to help you out and keep going\". In general you should only need to reach for a specialized datastore for services that have very specific data access requirements. And still it should be after you tried postgres first.",
          "score": 27,
          "created_utc": "2026-02-03 20:40:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dysgx",
          "author": "Select-Print-9506",
          "text": "its almost never about raw scale, its about operational complexity and team expertise, postgres can handle way more than most companies need if you tune it properly",
          "score": 40,
          "created_utc": "2026-02-03 18:39:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i6ufy",
              "author": "bobaduk",
              "text": "This is the key point. I haven't deployed a relational database in a good long time, because I can get better operational characteristics from other datastores. If I spin up dynamo, chances are that for the way I build software, it'll be fine, and the answer to \"is it up, it is coping with the load\" is yes and move on.\n\nWe were using postgres for a while at $CURRENT_GIG, but for our use case the cost curve was unappealing, particularly when every engineer has a cloud environment of their own to play with, and it was cheaper to adopt a managed time series data store.",
              "score": 2,
              "created_utc": "2026-02-04 09:58:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eftyi",
          "author": "External_Mushroom115",
          "text": "When your domain needs to scale writes  rather than reads.",
          "score": 10,
          "created_utc": "2026-02-03 19:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e3pga",
          "author": "who_am_i_to_say_so",
          "text": "There is no set number. Sometimes 1 million rows it will start performing like a dog- sometimes it’s 20 million rows. \n\nBut a general indicator is if you hit your max connections regularly even with upsizing and pooling (scaling vertically). Then you look into caching, perhaps- or the more expensive option of scaling horizontally.",
          "score": 17,
          "created_utc": "2026-02-03 19:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e88dw",
              "author": "sfboots",
              "text": "Max connections can be misleading if they are not using pgbouncer",
              "score": 8,
              "created_utc": "2026-02-03 19:22:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eyqx7",
              "author": "Typicalusrname",
              "text": "If Postgres performs like a dog with a million records the data model is shit",
              "score": 8,
              "created_utc": "2026-02-03 21:26:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ezlti",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-02-03 21:30:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f143t",
              "author": "Apart-Entertainer-25",
              "text": "Have you tried not running it on a toaster? :)",
              "score": 3,
              "created_utc": "2026-02-03 21:37:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f24zk",
                  "author": "who_am_i_to_say_so",
                  "text": "Relatedly there's this: [https://www.crunchydata.com/blog/postgres-toast-the-greatest-thing-since-sliced-bread](https://www.crunchydata.com/blog/postgres-toast-the-greatest-thing-since-sliced-bread)",
                  "score": 4,
                  "created_utc": "2026-02-03 21:42:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dyvm0",
          "author": "Select-Print-9506",
          "text": "same applies to api management honestly, you can build everything custom on top of nginx or envoy but at some point using something like gravitee or kong saves you from reinventing wheels that dont need reinventing",
          "score": 14,
          "created_utc": "2026-02-03 18:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e9ai4",
          "author": "sfboots",
          "text": "The limit depends heavily on workload and total IO needs assuming correct code and indexes\n\nAlso, some companies stay with Postgres and just use extensions like timescale or move some functions to a different database. \n\nMy company has 3 tables with more than a billion total rows and performance is adequate.  We do partition by time ranges since most use is the last year.",
          "score": 5,
          "created_utc": "2026-02-03 19:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e888l",
          "author": "mountainlifa",
          "text": "I've always wondered this. And when do folks introduce key pair database systems like dynamo into their architecture? ",
          "score": 3,
          "created_utc": "2026-02-03 19:22:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ebfp8",
          "author": "awol-owl",
          "text": "Never, yet at work this week we’re moving to Elasticsearch as our prototype showed it to be a search friendly service. I believe it’ll use more ram to run the new cluster, although I’m hoping the developer experience will be worth it. I’m not convinced yet.",
          "score": 3,
          "created_utc": "2026-02-03 19:37:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e3q9q",
          "author": "pgEdge_Postgres",
          "text": "Oftentimes it's not even the specialized tools that you need, just a tuned configuration and some good insights into your stack! Metrics go a long way towards predicting failures or reacting quickly when they do happen; take the lessons learned and turn them into actual architectural changes, and you can iterate up to those instances of billions of records.\n\nRelated interesting article: [https://openai.com/index/scaling-postgresql/](https://openai.com/index/scaling-postgresql/) if you missed it, OpenAI scaled PostgreSQL to power 800 million ChatGPT users; it powers both ChatGPT and OpenAI's API.",
          "score": 4,
          "created_utc": "2026-02-03 19:01:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lra5t",
              "author": "caught_in_a_landslid",
              "text": "The article is fairly clear that they are not allowing new tables any more and they are using cosmosdb for new things.\n\nIt shows that you can indeed push PG really far, but there's a real reason that other databases Exsist.",
              "score": 1,
              "created_utc": "2026-02-04 21:27:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eivyi",
          "author": "WilliamBarnhill",
          "text": "If you aren't doing rapid prototyping, I'd argue at any scale. You need to be able to articulate to stakeholders what your technology selection candidates were, what the tradeoffs were between them, your rationale for choosing the technology you did, and potential future risks as a result. Sometimes time-to-market is the overwhelming driver, but even then you need to be able to answer 'Why will using Postgres get us there faster, and what problems might we face down the road?'.",
          "score": 2,
          "created_utc": "2026-02-03 20:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f4792",
          "author": "swithek",
          "text": "I once inherited a system that used postgres to store massive json blobs, with a bit of metadata kept in separate indexed columns for filtering. The production database contained nearly a petabyte of data (hundreds of millions of rows) and the queries were painfully slow so I think it’s fair to say postgres wasn’t exactly an ideal choice here",
          "score": 2,
          "created_utc": "2026-02-03 21:51:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ivbmz",
              "author": "bomeki12345678",
              "text": "I'm curious, for your usecase, what dbms is the ideal choice? Saving massive json blobs in a relational databases seems to be not optimal option for me.",
              "score": 1,
              "created_utc": "2026-02-04 13:09:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n0vwt",
                  "author": "shoot2thr1ll284",
                  "text": "I agree with you. \n\nIn this case it seems like they chose Postgres for convenience of having everything in one spot, but large json blobs are not great for most systems. Depending on the use case a document store could work better in this case, but honestly I would treat those large json as files and use a different file serving service like s3 and just keep the url to it on Postgres or in another service. Makes it so that the thing that searches isn’t also responsible for the large amount of data transfer. At some point it just becomes networking and io speed limitations….",
                  "score": 1,
                  "created_utc": "2026-02-05 01:29:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hht0v",
          "author": "kmhosny",
          "text": "OpenAI wrote a post about how they use postgres to serve 800 million customers. https://openai.com/index/scaling-postgresql/\nNot every company is on openAi scale so in 90% of the cases there are optimization steps that can be taken to keep just usibg postgres",
          "score": 2,
          "created_utc": "2026-02-04 06:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eqjob",
          "author": "truechange",
          "text": "When vertical scaling can be solved by offloaded cached data.",
          "score": 1,
          "created_utc": "2026-02-03 20:48:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3exq6q",
          "author": "lambdasintheoutfield",
          "text": "You can do vertical or horizontal sharding which allows you to scale the database extremely effectively.\n\nHorizontal sharding is partitioning a table into more tables with the same exact schema but fewer rows and then an index to track the partitioning/shards. If your queries require large scans of values, this works well and there are numerous partitioning schemes to pick.\n\nVertical sharding is where you partition on columns. If you are specifically querying for data in column subsets, you could just have dedicated tables for those.\n\nIt’s likely both would be helpful, and both reduce storage space. Be careful of your primary keys and indices but this is enormously effective when done right. \n\nIt isn’t too difficult to roll your own postgres orchestrator across multiple nodes. \n\nAll that said, anytime you go distributed, you have to consider HA and fault tolerance. If you are querying a subset of rows on a node that goes down you obviously won’t be getting the data unless you replicate it.\n\nIf you know your access patterns, the critical and/or most frequently accessed data can use a higher replication factor and if a node goes down just route to the replicas. The architecture of your nodes can be a tree structure where each node is a shard and the leaf nodes are the replicas. Use this to inform the load balancer and query routing.",
          "score": 1,
          "created_utc": "2026-02-03 21:21:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gb452",
          "author": "Wiszcz",
          "text": "When cost of working around it's cons is higher than restructuring project.",
          "score": 1,
          "created_utc": "2026-02-04 01:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3grob5",
          "author": "dudeaciously",
          "text": "How does this group feel about concurrent transactions, with critical commit and rollback requirements.  Lots of connections. Then there is a breakage. Does coming back online break data integrity?  \n\nIf so, then the metrics on concurrent users, with operations per transaction would answer OP.",
          "score": 1,
          "created_utc": "2026-02-04 03:13:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jo12a",
              "author": "SpamapS",
              "text": "Yes it comes back consistently, but it can be really slow. You're going to need a hot standby logically replicated to have a chance at your database being online more than 3 nines in this scenario.",
              "score": 1,
              "created_utc": "2026-02-04 15:38:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k68pl",
                  "author": "dudeaciously",
                  "text": "Ah!  So to preserve data integrity in the whole database, replicate the nodes implicitly?  Not only after disaster recovery, but propagation for every committed transaction.\n\nVery cool.  No disagreements.  But I this is heavy, I have never done it.",
                  "score": 1,
                  "created_utc": "2026-02-04 17:02:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jjmmm",
          "author": "andras_gerlits",
          "text": "Scale is rarely the bottleneck. It's usually replication and high-availability.",
          "score": 1,
          "created_utc": "2026-02-04 15:17:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jnlav",
          "author": "SpamapS",
          "text": "It's more that some features scale better than others, so you can keep using the core, but you'll find that stuff that made it easy to build on top of at low scale becomes too expensive at a higher scale.\n\nForeign keys and sub transactions start to become a burden with high concurrency for insurance due to multi transaction shared locks that scale quadratically. You start needing to avoid those at some point.\n\nLarge payloads eventually need to be moved to external object storage or you'll destroy memory usage.\n\nThe real point when postgres can't do it alone is around 4 nines. When you need more than 3 nines really, it's just  complicated to do that with postgres and you'll find some other architecture like a NoSQL or sharding layer like CitusDB will make it simpler.",
          "score": 1,
          "created_utc": "2026-02-04 15:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ll2br",
          "author": "TallGreenhouseGuy",
          "text": "If you’re used to partitioning using Oracle, the Postgres way is just painful to work with.\n\nSo having large tables that you want to partition is quite an ordeal if you want to do range based partitioning using eg date and there are foreign keys/primary keys that are not naturally a part of the partition key.",
          "score": 1,
          "created_utc": "2026-02-04 20:57:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41fcym",
          "author": "CoreyTheGeek",
          "text": "When your director tells you can't because dynamo is so cheap even though the system calls for a relational database... Ahhh I should have been a farmer",
          "score": 1,
          "created_utc": "2026-02-07 06:18:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o444dne",
          "author": "n4jgg",
          "text": "You can achieve a lot of things with PG. Like jsonb is usually quite enough to store fully denormalized data. You wouldn't need a mongodb just because 2-3 use cases need a jsonb. Nor an elastic search cluster, if all you need to have is a primitive search tool.\n\nPub/Sub with Postgres, not a great idea probably. As you might run out of number of connections with enough number of clients which can effectively bring down whole DB.\n\nCaching high volume , short lived data? Probably also not a great idea. As the number of inserted but not vacuumed rows will take huge space and likely to effect overall performance.\n\nIf your work loads aren't heavily based on such cases? You're probably right, just use PG until it proves to be not enough.",
          "score": 1,
          "created_utc": "2026-02-07 17:43:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy7oet",
      "title": "How to Design Systems That Actually Scale? Think Like a Senior Engineer",
      "subreddit": "softwarearchitecture",
      "url": "https://javarevisited.substack.com/p/how-to-scale-like-a-senior-engineer",
      "author": "javinpaul",
      "created_utc": "2026-02-07 07:36:48",
      "score": 62,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qy7oet/how_to_design_systems_that_actually_scale_think/",
      "domain": "javarevisited.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o42a4ou",
          "author": "Ad3763_Throwaway",
          "text": "I rather have a developer which tells you that every problem requires another solution than one who re-iterates things like mentioned in the article.\n\n90% of scalability is just optimizing database queries. Learn how to monitor, troubleshoot and fix these and you can solve most scalability issues. SQL Server easily handles multiple thousands of requests per second.\n\nMultiple web servers / load balancing is mainly needed for always online systems, so you can take one offline to do updates on the other. Most software will never reach the point that the webserver is the limiting factor. You can easily do thousands of request per sec on most as long as you handle requests efficiently.\n\nSecurity is in most case just not inventing the wheel yourself. Use frameworks and don't write your own hashing / encryption algorithms.",
          "score": 18,
          "created_utc": "2026-02-07 11:12:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42gvs7",
              "author": "bittrance",
              "text": "I would rather say that 90% of scalability is picking the right storage solution. RDBMs can indeed handle thousands of queries per seconds. That does not really qualify as scale. No amount of RDBM will serve millions of requests per second unaided.\n\nIt is indeed true that most software systems will never reach a scale worth mentioning. Unfortunately, this has produced a cadre of developers who do not know how to address non-trivial scale, leading them into diminishing return query optimization.",
              "score": 10,
              "created_utc": "2026-02-07 12:12:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43g3n5",
                  "author": "FlatProtrusion",
                  "text": "How do you learn to address non-trivial scale?",
                  "score": 1,
                  "created_utc": "2026-02-07 15:44:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o449vbc",
              "author": "Ambitious-Sense2769",
              "text": "At scale we’re usually talking about millions of requests/transactions per second. Not thousands. Just having one sql db with proper indexing and query optimization still cannot handle millions of transactions per second. So that’s why people have to learn about things like sharding, load balancing, caching, etc. I agree most people won’t touch that scale but that isn’t really the point of “designing systems that scale”.",
              "score": 3,
              "created_utc": "2026-02-07 18:10:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45glf8",
                  "author": "PmMeCuteDogsThanks",
                  "text": "But it is, there are so many clueless developers out there that think that everything needs to ”scale”. \n\nI agree, with millions of transactions per second you can’t use a single database. \n\nBut very few systems have that, and many of those can be optimised in-place. ",
                  "score": 3,
                  "created_utc": "2026-02-07 21:53:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o46akik",
              "author": "yoggolian",
              "text": "Totally - a really good skill to have is being able to tell a scale problem from a tuning problem. An app we had with 25k users and maybe 1000 concurrent doesn’t need scaling solutions, it needs tuning (along with a bit of common sense, like don’t deploy 4x servers in a cluster to run a single container instance with auto scaling turned off, that’s also defined as the smallest possible size - I’m going to pull about $60k in annual cloud cost savings at $work there). ",
              "score": 2,
              "created_utc": "2026-02-08 00:50:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxuf8l",
      "title": "How to approach a technical book?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qxuf8l/how_to_approach_a_technical_book/",
      "author": "FactorLongjumping167",
      "created_utc": "2026-02-06 21:25:30",
      "score": 36,
      "num_comments": 32,
      "upvote_ratio": 0.89,
      "text": "everytime i talk to a senior dev about some confusions i have with some concepts, they suggest me to read a book of 700 pages or so..\nI wanted to ask how do you guys approach such books? i mean do you read them from end to end? how does that work? thank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qxuf8l/how_to_approach_a_technical_book/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3z939s",
          "author": "severoon",
          "text": "I read the book.\n\nPart of being a good reader is being able to quickly extract the information you want on a first pass, and then dive deeper into the details that are relevant to your issue. But honestly, if you don't even give a first pass over the material covered by a book that is presumably cohesive, you don't even know what's already been done on that subject, so it will be hard to even know the right questions to ask.\n\nAlso if you're not getting it, don't just hunker down with the book and try to plough through it. You have the Internet, you have chatgippity.",
          "score": 24,
          "created_utc": "2026-02-06 22:02:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zfsfn",
              "author": "jac4941",
              "text": "> chatgippity\n\nIdk if this was a typo or intentional but I kinda love it 😄",
              "score": 3,
              "created_utc": "2026-02-06 22:37:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zjdxq",
                  "author": "severoon",
                  "text": "Not a typo, this is a pretty common nickname for the Chat Generative Pre-trained Transformer.",
                  "score": 1,
                  "created_utc": "2026-02-06 22:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o411szk",
              "author": "Snoo23533",
              "text": "Agree and this is the kind of answer that was needed here. You can read non-fiction different than a fiction book. Even flipping through to parse models, tables, headers and summaries on a quick read can yield results. Not everbody has time to power through end eto end and tbh a lot of authors core ideas shouldve been a blog post but got expanded so theyd have something to sell.",
              "score": 1,
              "created_utc": "2026-02-07 04:32:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z383j",
          "author": "Aggressive_Ad_5454",
          "text": "Good question.\n\nI read the table of contents carefully.\nI then look at the intro chapters if I'm new to the particular tech.\n\nThen, there's usually a chapter on, I dunno, performance or stability or edge-cases that is basically answering the question \"what's hard about using this?\" I read that chapter really carefully and research the answers. \n\nThat, I find, is an efficient entry to most books of OReilly Media or similar quality and editorial standards.",
          "score": 23,
          "created_utc": "2026-02-06 21:33:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z2lr3",
          "author": "manamonkey",
          "text": "How do you normally approach a book? Have you encountered reference books before?",
          "score": 10,
          "created_utc": "2026-02-06 21:30:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z32t0",
              "author": "PmMeCuteDogsThanks",
              "text": "You mean reading full sentences, using a table of contents, without the support of an AI? Sorry, what was the question again, I was distracted.",
              "score": 17,
              "created_utc": "2026-02-06 21:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z5nem",
          "author": "Adorable-Fault-5116",
          "text": "Are you asking how reading works?\n\nLook I would a) read articles on a topic if it was a casual conversation and I was clueless, and / or b) read the book if it seems core to stuff we are doing or I'm more interested.\n\nYou should read books. Ideally written before 2022 when the slop took over.",
          "score": 6,
          "created_utc": "2026-02-06 21:45:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z5c25",
          "author": "GurglingGarfish",
          "text": "Sneak up on it from behind, so it doesn’t see you, then pounce on it when you’re about 3ft away. That’s how I usually approach them.",
          "score": 17,
          "created_utc": "2026-02-06 21:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z8te1",
              "author": "halfxdeveloper",
              "text": "Do you use a special bait to lure them out? I can’t seem to find a good pack in the wild.",
              "score": 3,
              "created_utc": "2026-02-06 22:01:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o40uxfc",
              "author": "I_Have_A_Snout",
              "text": "If you’re one of the “hygiene is optional” types, also approach from downwind.",
              "score": 2,
              "created_utc": "2026-02-07 03:44:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z2j3w",
          "author": "PmMeCuteDogsThanks",
          "text": "Jesus ",
          "score": 12,
          "created_utc": "2026-02-06 21:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z37k8",
          "author": "100kgoffun",
          "text": "Can you read?",
          "score": 2,
          "created_utc": "2026-02-06 21:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z59ig",
          "author": "Cherveny2",
          "text": "depends on the book really.  Is it something that you know a lot of the subject matter, and only need to learn certain new topics?  find them in the table of contents/index, and go to them directly.\n\nIs the whole subject new?  dig in and read it.\n\nAlso, for extra re-inforncement of learning,  does it have exercises you can do while reading?  Do them. Does it have questionss at the end of each chapter?  Try answering them.  Again ensures you get the concepts.\n\n",
          "score": 2,
          "created_utc": "2026-02-06 21:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zmrah",
          "author": "Spite_Gold",
          "text": "So books, you know, there are words and you can read them, and as you read you learn new stuff. \n\nAnd dont  forget to go to next page as you're done with current page, this is a really rookie mistake!",
          "score": 4,
          "created_utc": "2026-02-06 23:15:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o420lzn",
              "author": "Whole_Ladder_9583",
              "text": "Books are tricky https://youtu.be/pQHX-SjgQvQ?si=l7cScP6QyrNNFtl6",
              "score": 1,
              "created_utc": "2026-02-07 09:39:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z2dr8",
          "author": "flavius-as",
          "text": "Lol.\n\nYes, you read them, understand and learn.\n\nAnd nowadays you even extract knowledge with page numbers for LLM.",
          "score": 1,
          "created_utc": "2026-02-06 21:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z7ull",
          "author": "Marelle01",
          "text": "If this is your first technical book, I recommend reading everything in order and doing all the exercises, if there are any, until you can do them correctly. Keep in mind that some of these books correspond to several hundred hours of undergraduate or graduate-level coursework. Once you have delved into your subject, you will often only need to read a few chapters, as you will have mastered the rest.",
          "score": 1,
          "created_utc": "2026-02-06 21:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z8guz",
          "author": "failsafe-author",
          "text": "I read them end to end.",
          "score": 1,
          "created_utc": "2026-02-06 21:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ze87f",
          "author": "shufflepoint",
          "text": "Be careful. Approach like you would an unfamiliar dog. Walk slowly, approaching from the side rather than head-on. If it seems chill, pet it sides. ",
          "score": 1,
          "created_utc": "2026-02-06 22:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zkrfd",
          "author": "BarfingOnMyFace",
          "text": "Take a stance slightly wider than your shoulders,  left leg out front, keeping your torso facing forward, chin tucked, hands up, and then approach slowly and defensively.",
          "score": 1,
          "created_utc": "2026-02-06 23:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zuwie",
          "author": "Acceptable_Crab4153",
          "text": "This is not a Novel. You select the chapters relevant to you. Then, after reading the summary at the end, you delve deeper into the specific details you need clarification on within the chapter..",
          "score": 1,
          "created_utc": "2026-02-07 00:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zw4yw",
          "author": "WhenSummerIsGone",
          "text": "have you ever gone to school and worked your way through a book? There are no shortcuts.",
          "score": 1,
          "created_utc": "2026-02-07 00:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zzbx2",
          "author": "Forsaken-Victory4636",
          "text": "Depends, but sometimes yes.\n\nI read Mark Lutz’s “ Learning Python” cover to cover  all 1648 pages of it.\n\nIt became the foundation of a career switch from mechanics to software engineering.",
          "score": 1,
          "created_utc": "2026-02-07 00:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o402bae",
          "author": "IlliterateJedi",
          "text": "It depends on the book.  Usually I start at the beginning and read as far as I need to in order to understand the topic.  Thankfully with tools like Claude and ChatGPT I bounce any questions or clarifications off an LLM as needed.  It makes understanding concepts a lot easier in my experience.  It's hard to say with 100% certainty because some books are best used as references where you only read the chapter you need (e.g., Fluent Python or something).",
          "score": 1,
          "created_utc": "2026-02-07 00:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o421hgh",
          "author": "imihnevich",
          "text": "The pages are usually numbered, you start from the lowest number and progressively increase it one by one, if your book is in English, you can read it from the top left corner, left to right, line by line. It's okay to skip pages if you are looking for some specific pieces of information, but it's most fun when you don't.\n\nOn a serious note, you need to always understand how you can apply the information in practice when it comes to technical books. Feynman's technique is an amazing tool",
          "score": 1,
          "created_utc": "2026-02-07 09:48:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4378o1",
          "author": "bomeki12345678",
          "text": "My steps are:\n* Clarify what do I need from the book to get the motive to read it\n* Check the table of contents to get the core keywords, ask gemini to explain them like I'm 5 to get familar with concepts\n* Read each chapters one by one. Always focus on the problem statements that each chapter say first. Try to stop and think little bit how would you solve it using previous chapter knowledge\n* Read the solution slowly until you can understand it.",
          "score": 1,
          "created_utc": "2026-02-07 14:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44fcrc",
          "author": "No_Indication_1238",
          "text": "You sit down every day and read 15 pages. In about a month and half you'll have read the book and have a new perspective on the subject. Keep doing it until you get cracked. That's it. Most of the important stuff isn't in tutorials and videos. Like 95% of the important stuff isn't. If you haven't read books, you're missing out.",
          "score": 1,
          "created_utc": "2026-02-07 18:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48lzyn",
          "author": "Miserable_Disk3045",
          "text": "Some books cover to cover once and then as needed refer. Some books just a few chapters. But these days I usually start with Gemini for refreshing knowledge and go for books for niche areas.",
          "score": 1,
          "created_utc": "2026-02-08 11:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4966kp",
          "author": "symbiat0",
          "text": "Reading a book really is the best way to retain more when learning a subject. I know younger generations really don't want to hear that though...",
          "score": 1,
          "created_utc": "2026-02-08 14:13:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49ll0z",
          "author": "ResolveResident118",
          "text": "Slowly, and from the front so as not to frighten it. They can be skittish. ",
          "score": 1,
          "created_utc": "2026-02-08 15:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z6pb5",
          "author": "tinmanjk",
          "text": "If it's canonical book spend 2-3 months reading it cover to cover. After this you'd probably be 3-4x better dev. Not going to be easy though.",
          "score": 0,
          "created_utc": "2026-02-06 21:51:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv3nr2",
      "title": "Kafka for Architects — designing Kafka systems that have to last",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qv3nr2/kafka_for_architects_designing_kafka_systems_that/",
      "author": "ManningBooks",
      "created_utc": "2026-02-03 20:44:15",
      "score": 33,
      "num_comments": 6,
      "upvote_ratio": 0.85,
      "text": "Hi r/softwarearchitecture,\n\nStjepan from Manning here. We’ve just released a book that’s written for people who have to make architectural calls around event-driven systems and then defend those decisions over time. Mods said it's ok if I post it here:\n\n**Kafka for Architects** by Katya Gorshkova  \n[https://www.manning.com/books/designing-kafka-systems](https://hubs.la/Q041FhV20)\n\n[Kafka for Architects](https://preview.redd.it/guav3ysxachg1.jpg?width=2213&format=pjpg&auto=webp&s=fc59d0f2fef718c70d40b4996d59b6f879992605)\n\nThis isn’t a Kafka API guide or a step-by-step tutorial. It stays at the architecture level and focuses on how Kafka fits into larger systems, especially in organizations where multiple teams depend on the same infrastructure.\n\nA few of the topics the book spends real time on:\n\n* Kafka’s role in enterprise software and where it fits in an overall system design\n* Event-driven architecture as a pattern, including when it helps and when it complicates things\n* Designing data contracts and handling schema evolution across teams\n* Kafka clusters as part of the system’s operational and organizational design\n* Using Kafka for logging, telemetry, data pipelines, and microservices communication\n* Patterns and anti-patterns that tend to appear once Kafka becomes shared infrastructure\n\nWhat I appreciate about this book is that it treats Kafka as an architectural choice, not just a technology. Katya walks through trade-offs you’ll recognize if you’ve ever had to balance team autonomy, data ownership, and long-term maintainability. The examples are grounded in real-world systems, not idealized diagrams.\n\nIf you’re responsible for questions like “Is Kafka the right fit here?”, “How do we keep event contracts stable?”, or “What happens when this system grows to ten teams instead of two?”, this book is written with those concerns in mind.\n\n**For the** r/softwarearchitecture **community:**  \nYou can get **50% off** with the code **PBGORSHKOVA50RE**.\n\nIf you’re already using Kafka as part of a larger system, I’d be interested to hear what architectural challenges you’re currently dealing with.\n\nThanks for having us. It feels great to be here.\n\nCheers,\n\nStjepan",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qv3nr2/kafka_for_architects_designing_kafka_systems_that/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3g22dn",
          "author": "Glathull",
          "text": "Thanks for posting this. Just bought a copy.",
          "score": 2,
          "created_utc": "2026-02-04 00:49:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ksoes",
              "author": "ManningBooks",
              "text": "Thank you.",
              "score": 1,
              "created_utc": "2026-02-04 18:44:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3etpgb",
          "author": "[deleted]",
          "text": "[removed]",
          "score": -1,
          "created_utc": "2026-02-03 21:03:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f1op3",
              "author": "Mosk549",
              "text": "Don’t be racist",
              "score": 0,
              "created_utc": "2026-02-03 21:40:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f275o",
                  "author": "AbbreviationsLow4798",
                  "text": "don’t support fascists ",
                  "score": -2,
                  "created_utc": "2026-02-03 21:42:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qu4aaf",
      "title": "Why does enterprise architecture assume everything will live forever?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qu4aaf/why_does_enterprise_architecture_assume/",
      "author": "eurz",
      "created_utc": "2026-02-02 19:05:15",
      "score": 25,
      "num_comments": 36,
      "upvote_ratio": 0.74,
      "text": "Hi everyone!\n\nWorking in a large org right now and everything is designed like it’ll still be running in 2045. Layers on layers, endless review boards, “strategic” platforms no team can change without six approvals. Meanwhile, half the systems get sunset quietly or replaced by the next reorg. I get the need for stability, but it feels like we optimize for theoretical longevity more than actual delivery.\n\nFor people who like enterprise architecture - what problem is it really solving well, and where does it usually go wrong?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qu4aaf/why_does_enterprise_architecture_assume/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o37gb2r",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 74,
          "created_utc": "2026-02-02 19:12:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37h3cb",
              "author": "LessChen",
              "text": "I would have said the 1980's - so much in the banking world is very old but still works for example.",
              "score": 31,
              "created_utc": "2026-02-02 19:15:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37h2do",
          "author": "Glathull",
          "text": "Because in the enterprise, things *do* live forever.",
          "score": 62,
          "created_utc": "2026-02-02 19:15:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37yfql",
              "author": "BeABetterHumanBeing",
              "text": "To be more precise: in enterprise, a thing lives for as long as the business it's supporting, which is frequently longer than the existence of the company (because business units get spun off in acquisitions, live through bankruptcy, and so on).",
              "score": 10,
              "created_utc": "2026-02-02 20:37:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ikinc",
              "author": "NewFuturist",
              "text": "Even in small companies, if the company is 15 years old, you'll encounter 15 year old code.",
              "score": 7,
              "created_utc": "2026-02-04 11:56:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3q62fi",
              "author": "3sc2002",
              "text": "Came here to say this.  But yeah . . . COBOL is still kicking.",
              "score": 1,
              "created_utc": "2026-02-05 15:03:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37gxbp",
          "author": "ratczar",
          "text": "I was reading Clean Architecture recently, and the part that jumped out and grabbed me by the throat is the idea that good architecture defers big decisions for as long as possible.   \n  \nIt sounds like your org is maybe trying to finalize plans for a big, long-term system, which is a trap that I find a lot of engineers fall into - they want to construct the beautiful, perfect, hyper-efficient machine.   \n  \nBut the business sometimes changes what it wants the machine to do, and if you're tied to a 20 year roadmap then you're a bit fucked when winds change. \n\nFor that reason, you can draw up whatever plans you want, but you should really only move to implementation when you *absolutely have to.*\n\nGreat example: we've had plans for an enterprise-wide address verification service for awhile, but we only moved on it when it became absolutely critical for a client. ",
          "score": 20,
          "created_utc": "2026-02-02 19:14:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37imki",
              "author": "ziksy9",
              "text": ">Great example: we've had plans for an enterprise-wide address verification service for awhile, but we only moved on it when it became absolutely critical for a client. \n\nGiven that is was needed by a client, even if it was implemented there were probably requirements that came in late for that client, so doing it preemptively would have still requires changes, right?\n\nGreat example.",
              "score": 4,
              "created_utc": "2026-02-02 19:22:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o37jet5",
                  "author": "ratczar",
                  "text": "It really came down to some minor details in the implementation, e.g. are we going full hypermedia or are we cramming some critical data into the initial return.\n\nWe ended up deciding to cut a corner initially and crammed it into the initial return, and will refactor later when we have more use cases for the related data.",
                  "score": 1,
                  "created_utc": "2026-02-02 19:26:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o394cg7",
              "author": "phluber",
              "text": "A well-designed enterprise architecture does not leave you fucked when winds change. A well-designed architecture allows for a great amount of flexibility and room for new features.",
              "score": 2,
              "created_utc": "2026-02-03 00:07:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37kwij",
          "author": "ByMunro",
          "text": "IMO enterprise architecture is about flexibility (coupling and cohesion). we dont know what will be in 2045 and hell we dont even know what will be in 3 years from now. all we can do is try to stay as flexible as possible. some systems will stay, so make sure they're designed well. some systems will go, so make sure you can replace them as effortlessly as possible. \n\nand well, you can be quite sure about what is tomorrow. on a enterprise scale you probably (should) also know whats gonna happen in 6 months quite precisely. the more you look into the future, the more abstract it gets. i dont think anyone is planning on exactly your enterprise architecture will look like in 2045. but you'll want to have visions, guidelines, something abstract, so everyone is going in the same direction. as 2045 comes closer, your vision gets more precise. so make sure you can adapt, keep your landscape flexible. \n\nEDIT: should probably add, that this is a perfect world scenario. time, money, politics have quite some\ninfluence",
          "score": 5,
          "created_utc": "2026-02-02 19:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38qqgv",
          "author": "edgmnt_net",
          "text": "Just a comment from someone who dislikes it. I hope we're talking about the same thing, though, because I'm not really sure...\n\nI don't think that's the real reason. One thing might be horizontal dev scaling: plenty of businesses barely build upon anything, it's an endless crunch to appease one customer, then the next and so on. Architecture is simply an attempt to install guardrails so they can hire en masse with low costs and divide up work. Something a very average dev (mediocre even) can handle, because you want lots of them. Another thing is the expectation that software is write-once, especially when they accumulate tons of half-baked features and tech debt that's difficult to repay, they try to extract as much as possible (and impossible).\n\nIn a nutshell, it's trying to scale up work as cheaply as possible, because ultimately a lot of business is just trying to get money to work. As for where and whether it goes wrong objectively, this is also debatable. I would say the main issues are (1) unrealistic expectations and (2) they're missing out on higher impact, more efficient software development that really taps into the potential of this domain (but you need a strong vision for this, it's not just a matter of spending).\n\nIn practical terms, you'll see silos, you'll see layers upon layers that give the impression that everything is decoupled (yet it isn't, the spaghetti is on another level), you'll see work getting split in ways that makes absolutely no sense and results in huge amounts of duplication. Do reduced hiring costs make up for this? Maybe, although I doubt it can beat a small team of people with enough expertise cutting through the bullshit and working under a well-determined, restricted scope. Scope creep and tech debt tends to balloon up costs, so even if it looks like you can satisfy hundreds of customers early on, things eventually slow down greatly and you have to keep adding to the headcount and reducing margins. Until the costs become insane and the project gets sunset (often quite early).",
          "score": 5,
          "created_utc": "2026-02-02 22:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37h0or",
          "author": "Ugiwa",
          "text": "Products don't die that fast.. (unless you're working for Google ig)",
          "score": 3,
          "created_utc": "2026-02-02 19:15:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39cs1z",
          "author": "gbrennon",
          "text": "Because applications that runs in the backend of ur bank were deployed in 1980?",
          "score": 3,
          "created_utc": "2026-02-03 00:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39lg5e",
          "author": "Dicethrower",
          "text": "It's even funnier with games. \"This needs to last 5y, so let's spend 2y making what should take 6 months, and not see the irony in spending 4 times as long to \"\"save time\"\". Oh woops, funding is gone and/or game didn't do so well.\"\n\nYAGNI is not just a catchy phrase. Time saved is time you can spend refactoring when the actual need arises. Anything made before you know it's needed is a waste of time, every time.",
          "score": 2,
          "created_utc": "2026-02-03 01:43:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dic5r",
              "author": "edgmnt_net",
              "text": "It bothers me that often it isn't anything smart either. It's not like they're spending time on mind-blowing techniques and abstractions, it's often just a ton of trivial layers.",
              "score": 2,
              "created_utc": "2026-02-03 17:25:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3fm06b",
              "author": "213737isPrime",
              "text": "It's an art to know just how much \"extra\" effort to put in place as an investment into the future without overdoing it. It's more than zero.",
              "score": 2,
              "created_utc": "2026-02-03 23:21:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3fmsm1",
                  "author": "Dicethrower",
                  "text": "Hard disagree. You never do anything extra that you don't know you need. Trying to predict the future is exactly what YAGNI is all about.",
                  "score": 1,
                  "created_utc": "2026-02-03 23:25:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o38inn4",
          "author": "internetuser",
          "text": "Because the people who are paid to facilitate the process get paid more when the process takes a long time.",
          "score": 2,
          "created_utc": "2026-02-02 22:13:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37inz5",
          "author": "europeanputin",
          "text": "Architecture it's not just about technical designs, but it must also fit the organizational structure. We have a lot of instances where we would benefit from event driven systems and concepts, but we remain strictly RESTful because we don't have the knowledge to support it. When making designs in such cases, it's important to be mindful that at some point this would change, because the other way around increases cost i.e on operations. Hence the designs go into series of discussions which tradeoff to choose, and enterprise architecture essentially helps making these decisions as it allows to estimate the cost for the current project and the cost of changing it in the future. Comparing this to projections of other cost like cost of running operations or other non functionals help making smart business decisions.\n\nAs cost of change needs to be minimized and eventually management will come to their senses, enterprise architecture is sadly required for technical people to cope with ever changing nature of business demands.",
          "score": 1,
          "created_utc": "2026-02-02 19:22:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37ny9k",
          "author": "BarfingOnMyFace",
          "text": "It solves the problem of why we have jobs really well 😄",
          "score": 1,
          "created_utc": "2026-02-02 19:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37o8bs",
          "author": "BarfingOnMyFace",
          "text": "Forever is a long time. I’d argue most software should be architected to survive 3-5 decades. Maybe in some cases in the future, a century or so.",
          "score": 1,
          "created_utc": "2026-02-02 19:48:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3dhunr",
              "author": "edgmnt_net",
              "text": "Survive, definitely. Remain unchanged that long? Nah.",
              "score": 1,
              "created_utc": "2026-02-03 17:22:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3dpmqw",
                  "author": "BarfingOnMyFace",
                  "text": "Yes, just survive. Remain unchanged? Highly unlikely. Even in some far-off future of multi-generational spaceships… change seems inevitable, improvements or new additional features desired, change born out of pure boredom, due to a monotonous longevity of the same look and behaviors… I definitely agree with you- change is frequent. change is an integral part of software, as much as it is an integral part of the human brain constantly programming itself, long after most of its software has been written.",
                  "score": 1,
                  "created_utc": "2026-02-03 17:58:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o37tfwc",
          "author": "D4n1oc",
          "text": "I can't imagine how this would influence your design.\nWe could either do a good architecture that fits the needs or we don't and live with the consequences.\n\nWhat would be the actual design difference for a system that lives 3-5 decades or 10 decades?\n\nWe know one thing for sure. Bad architecture decisions are very expensive and can influence the whole system across all teams causing huge amounts of unknown costs and sometimes make it impossible to change the running system in a necessary way.\n\nWe know the costs for a clean architectural design that minimizes this risk.\n\nIn most cases it's the most expensive part to write software. Because it always creates legacy, technical deps, complexity and needs to be supported.\n\nWriting software should be the last resort. \nIt's much cheaper to make 10 plans and throw them away.",
          "score": 1,
          "created_utc": "2026-02-02 20:13:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37w1sf",
          "author": "Mediocre_Date1071",
          "text": "I hear two different things happening here\n\n- “layers on layers”. This sounds like design optimized for loose coupling, which gives the ability to change out pieces. This architecture for the assumption that things will not be around forever.  \n\n“Endless review boards…platforms no one can change without 6 approvals”. This is large organizations being large. There is so much miscommunication and need to generate buy-in across a large org, just so the pieces fit together reasonably well, that you get these heavy processes. \n\nThis isn’t to say that your company is doing optimal planning and architecture, just that the reasons for what you see may not the presumption of longevity that you surmise. ",
          "score": 1,
          "created_utc": "2026-02-02 20:26:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3blm37",
          "author": "BeastyBaiter",
          "text": "I work as a software dev at an F100 megacorp you've heard of. We retired our mainframes 4 years ago and have numerous critical internal apps from the 1990's.",
          "score": 1,
          "created_utc": "2026-02-03 10:57:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bpmt1",
          "author": "garden_variety_sp",
          "text": "I call it crystal ball architecture, and it’s the most wasteful way to design and build. Design for now and know that evolution is a thing. I view enterprise architecture in much the same way as I view intelligent design. It takes some serious mental gymnastics to believe in it, and those that do are most likely brainwashed.",
          "score": 1,
          "created_utc": "2026-02-03 11:32:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e1fkh",
          "author": "santagoo",
          "text": "We still run systems running COBOL…",
          "score": 1,
          "created_utc": "2026-02-03 18:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eoy61",
          "author": "serious-catzor",
          "text": "I'm not sure I really work with software... This UART worked the same when I was born and it will work the same when I die.",
          "score": 1,
          "created_utc": "2026-02-03 20:41:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gzlbf",
          "author": "Pale_Height_1251",
          "text": "If we only made the software we needed, the industry would lay off half the developers working today.\n\nIt's not really in anybody's interests to make streamlined software and deliver early. Management wants to increase their fiefdoms, developers want to keep their jobs.",
          "score": 1,
          "created_utc": "2026-02-04 04:01:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hc6fb",
          "author": "SpaceCowboy317",
          "text": "Please bro just give me clean boundries bro, please dont let that jr push the jdbc class with the select * bro pleeeaaase",
          "score": 1,
          "created_utc": "2026-02-04 05:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lg500",
          "author": "Bahatur",
          "text": "Is the software licensed?\n\nIf it is, then there’s an accounting dimension because it is an asset; every time you sunset an asset, you have to take a write down on the books.",
          "score": 1,
          "created_utc": "2026-02-04 20:34:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lj2n1",
          "author": "PutPrestigious2718",
          "text": "You build the boat for the open ocean, not the harbor.",
          "score": 1,
          "created_utc": "2026-02-04 20:48:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m6u93",
          "author": "Visa5e",
          "text": "Because confluence pages never die.",
          "score": 1,
          "created_utc": "2026-02-04 22:43:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40bhru",
          "author": "Vegetable_Aside5813",
          "text": "I think that is what makes it enterprise",
          "score": 1,
          "created_utc": "2026-02-07 01:41:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40hvs9",
          "author": "Oobert",
          "text": "Because the odds it outlast your career is very high. Source: I work in enterprise architecture. ",
          "score": 1,
          "created_utc": "2026-02-07 02:20:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwja9l",
      "title": "LinkedIn Re-Architects Service Discovery: Replacing Zookeeper with Kafka and xDS at Scale",
      "subreddit": "softwarearchitecture",
      "url": "https://www.infoq.com/news/2026/02/linkedin-service-discovery/",
      "author": "rgancarz",
      "created_utc": "2026-02-05 11:53:23",
      "score": 25,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qwja9l/linkedin_rearchitects_service_discovery_replacing/",
      "domain": "infoq.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3rzlg6",
          "author": "IlliterateJedi",
          "text": "The actual source: [Scalable, multi-language service discovery at LinkedIn](https://www.linkedin.com/blog/engineering/infrastructure/scalable-multi-language-service-discovery-at-linkedin)",
          "score": 3,
          "created_utc": "2026-02-05 20:08:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3regod",
          "author": "BarfingOnMyFace",
          "text": "GIGO at scale",
          "score": 1,
          "created_utc": "2026-02-05 18:30:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvho5v",
      "title": "Fitness Functions: Automating Your Architecture Decisions",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/fitness-functions-automating-your-architecture-decisions-08b2fe4e5f34",
      "author": "trolleid",
      "created_utc": "2026-02-04 07:00:48",
      "score": 22,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qvho5v/fitness_functions_automating_your_architecture/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwo81f",
      "title": "Architecture Question: Modeling \"Organizational Context\" as a Graph vs. Vector Store",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qwo81f/architecture_question_modeling_organizational/",
      "author": "altraschoy",
      "created_utc": "2026-02-05 15:26:51",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.85,
      "text": "I’m working on a system to improve context retrieval for our internal AI tools (IDEs/Agents), and I’m hitting a limit with standard Vector RAG.\n\nThe issue is structural: Vector search finds \"similar text,\" but it fails to model typed relationships (e.g., `Service A` \\-> `depends_on` \\-> `Service B`).\n\nWe are experimenting with a Graph-based approach (hello arangodb x)) where we map the codebase and documentation into nodes and edges, then expose that via an MCP (Model Context Protocol) server.\n\nThe Technical Question: Has anyone here successfully implemented a \"Hybrid Retrieval\" system (Graph + Vector) for organizational context analysis?\n\nI’m specifically trying to figure out the best schema to map \"Soft Knowledge\" (Slack decisions, PR comments and all the jazz that a PM/PO can produce) to \"Hard Knowledge\" (code from devs/qa) without the graph exploding in size.\n\nWould love to hear about any data structures or schemas you’ve found effective for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qwo81f/architecture_question_modeling_organizational/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3qn3w9",
          "author": "Elvez-The-Elf",
          "text": "Is this an architecture question?",
          "score": 4,
          "created_utc": "2026-02-05 16:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rla4y",
              "author": "asdfdelta",
              "text": "Yes. Architecture requires organizational context to be designed correctly. For example, Conway's Law",
              "score": -4,
              "created_utc": "2026-02-05 19:01:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3s7fkp",
                  "author": "Elvez-The-Elf",
                  "text": "I agree with your statement but still can’t relate it to the post. If I were given this task I would ask my senior AI/Data Engineers for help, not our architects.",
                  "score": 2,
                  "created_utc": "2026-02-05 20:45:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rsdeb",
          "author": "erotomania44",
          "text": "Graph rag was hype. \n\nRead up on anthropic’s contextual retrieval blog post. \n\nIt’s the only way to do search with RAG today.",
          "score": 2,
          "created_utc": "2026-02-05 19:34:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4818dq",
          "author": "Potential-Analyst571",
          "text": "The trick to avoid graph explosion is only promoting durable facts into nodes and keeping everything else as linked evidence blobs with TTL or summarization. For wiring and debugging the pipeline, tools like LangSmith or Traycer AI help keep retrieval and changes traceable so you can see why the model pulled a given context....",
          "score": 1,
          "created_utc": "2026-02-08 08:36:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvpcml",
      "title": "key value storage developed using sqlite b-tree APIs directly",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qvpcml/key_value_storage_developed_using_sqlite_btree/",
      "author": "Fine-Package-5488",
      "created_utc": "2026-02-04 14:02:09",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "SNKV ([https://github.com/hash-anu/snkv](https://github.com/hash-anu/snkv)) is a key–value store implemented directly on top of SQLite’s B-Tree APIs.  \nIt bypasses the SQL query layer and performs operations using SQLite’s internal B-Tree interface, reducing overhead compared to SQL-based access paths.\n\nBenchmark evaluations on mixed workloads show approximately \\~50% performance improvement compared to equivalent SQL query–based operations.\n\nFeedback on the design, implementation choices, performance characteristics, and potential areas for improvement would be welcome.\n\nA usage walkthrough is available here:  \n[https://github.com/hash-anu/snkv/blob/master/kvstore\\_example.md](https://github.com/hash-anu/snkv/blob/master/kvstore_example.md)",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qvpcml/key_value_storage_developed_using_sqlite_btree/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qxk1uk",
      "title": "Should the implementation of Module.Contract layer be in Application or Infra? Modular monolith architecture",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qxk1uk/should_the_implementation_of_modulecontract_layer/",
      "author": "Illustrious-Bass4357",
      "created_utc": "2026-02-06 15:06:19",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.78,
      "text": "if I have a modular monolith where modules need to communicate ( I will start with in memory, sync communication ) \n\nI would have to expose a contract layer that other modules can depend on , like an Interface with dtos etc \n\nbut if I implement this contract layer in application or Infra, I feel it violates the dependency inversion like a contract layer should be an outer layer right? ,if I made the application or infra reference the contract , now application/infra  is dependent on the contract layer \n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qxk1uk/should_the_implementation_of_modulecontract_layer/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3x8bsj",
          "author": "etxipcli",
          "text": "The contract is an abstract description of the functionality the module provides. Think of it less as a code artifact and more like metadata and that might help with the DI concern. Whatever code is using your module still is dependant on what the module does, but it is not dependent on the module itself if that makes sense. The implementation at that point is incidental since all you care about is the functionality. ",
          "score": 2,
          "created_utc": "2026-02-06 16:10:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x9jdy",
              "author": "etxipcli",
              "text": "I'm in Java world, so SLF4J is a good example. We depend on SLF4J but ultimately some logging framework will be doing what matters. Here you will have your one implementation, but the purpose of the abstraction is the same.",
              "score": 1,
              "created_utc": "2026-02-06 16:16:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xo577",
              "author": "Illustrious-Bass4357",
              "text": ">The contract is an abstract description of the functionality the module provides.\n\nyeah like its an interface like this for example  \n\n\n    namespace Restaurants.Contracts\n    { \n      public interface IRestaurantServices\n        {\n            Task<bool> ExistsAsync(Guid restaurantId);\n        }\n    }\n\n\n\n>Whatever code is using your module still is dependent on what the module does, but it is not dependent on the module itself if that makes sense.\n\nit kinda makes sense, but still I think it's dependent on the contract layer of the module, cause it's referencing it , like if I deleted the Module , I would have to to also remove the project reference and make another project that does the same functionality \n\n  \n\n\nalso I was trying to reason of where should the implementation of the contract layer go, and I reached a point where it kinda makes sense to me, that it should go to the application layer, but I shouldn't treat it as a separate layer conceptually , like its an application level functionality but I don't want other modules to depend on my application layer cause that would violate the DI principle , so the contract layer conceptually is still part of the application layer but I make this abstraction so other modules don't interact with the application directly   \n \n\nhttps://preview.redd.it/3z396nl8qwhg1.png?width=905&format=png&auto=webp&s=1a435b71cbd874f8b5cea1aadf3f9edbf2f3e890\n\nthat's my current mental model\n\n  \nAm I understanding this correctly?",
              "score": 1,
              "created_utc": "2026-02-06 17:25:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xrmpc",
                  "author": "etxipcli",
                  "text": "Don't want to speak too authoritatively in general, but your understanding that there is a dependency in the contract of the module is correct.\n\n\nI am used to the contract being it's own module. I see you using the word layer, but the way they should be organized is side by side.  Like have a API module and any number of implementation modules.",
                  "score": 2,
                  "created_utc": "2026-02-06 17:41:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xr0dw",
                  "author": "Mental-Artist7840",
                  "text": "Contracts should be in the core layer (domain in your case).\n\nThis is because the application layer usually depends on the contract interface within their services. \n\nThe implementation for your repository should be in the infrastructure layer. Therefore both the application layer and infra layer depend on the contract.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:38:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3z3yb1",
          "author": "RST1997",
          "text": "In a modular monolith setup I would consider the contracts to be sort of part of the application layer.\nHowever, in code I would make them two different ‘projects’ (as in C# definition of a project).\nWhere other modules reference the contracts package and program against the defined interfaces.\nThen I would have the application implement these interfaces.\n\nThe application layer will most likely also have interface defined. For example an IRestaurantsRepository, the actual repository implementation will then be in the infrastructure layer.\n\nSo contracts will have the IRestaurantService interface.\nApplication will have the IRestaurantsRepository interface and RestaurantService (which implements IRestaurantService).\nInfra will implement the repository.\n\nIt might feel like the dependencies are not all pointing inwards. But they are in the sense that details still depend on policies/abstractions.",
          "score": 2,
          "created_utc": "2026-02-06 21:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43md9u",
          "author": "steve-7890",
          "text": "I think you complicate it more than you need. If you read about modular design, you gonna see there's no \"contract layer\" (you asked for modular design, not layered architecture after all). \n\nIn practice, in lean design (as opposite to enterprise designs), there are two common cases are:\n\n1. Module A delegates its infrastructure code to Module A'. In such case module A defines all the interfaces, and module A' implements them. Example: \"DeviceRegistration\" and \"DeviceRegistration - Infrastructure\" modules. This way \"DeviceRegistration\" has no infra code and doesn't depend on the \"DeviceRegistration - Infrastructure\".\n\n2. Module A needs to delegate flow to Module B. In such case there is a direct dependency, and no amount of tricks with separate abstraction layer won't change it. Put the interfaces in the upstream module, and let downstream module implement them. It the flow is unidirectional, direct dependency between modules, where you invoke classes of the mobule B is fine. It could be as easy as exposing a Facade Pattern. \n\nI was many Java or C# implementations with many abstraction layers, but I think that in the end it's Go that nailed it. It's even built-in into its philosophy by \"consumers to define interfaces instead of producers\" mantra.",
          "score": 1,
          "created_utc": "2026-02-07 16:14:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ygcmx",
          "author": "flavius-as",
          "text": "Your mind model would benefit tremendously if you'd use the terminology from ports and adapters.\n\nYour words \"application\", \"infra\" etc are highly interpretable.\n\nWhoever answers your question does so in their own world model, not yours, and there might be mismatches.\n\nP&A is simple and universal.\n\nTo answer what I think you asked, in Hexagonal terms:\n\nYour contracts should be the outer part of your domain model (=application in hexagonal), and adapters can depend on that.\n\nContracts are for example: DTOs, entities, value objects, repository interfaces (not implementations), use cases.\n\nYour adapters can depend on those, use them respectively implement them.\n\nDirection of dependencies is not violated this way.",
          "score": 1,
          "created_utc": "2026-02-06 19:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yvxgw",
              "author": "SolarNachoes",
              "text": "Their terms are from Clean Architecture.\n\nThey are just asking where to put the contract classes often called domain entities and interfaces in a clean architecture.",
              "score": 1,
              "created_utc": "2026-02-06 20:57:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtsopq",
      "title": "Selenium IDE test Case Migration",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qtsopq/selenium_ide_test_case_migration/",
      "author": "amfromeverywhere",
      "created_utc": "2026-02-02 11:36:05",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "I am trying to design migrating a 20 year old JSF based system to rest controllers + angular. Tough but I feel a vanilla migration for this forum. \n\nWhat's new is they have about 5000 selenium ide suites that only runs on an ancient version of Firefox over a well designed kubernetes cluster and takes in between 5 to 15 hrs depending on how much resources you can dedicate for a run.\n\n  \nThose tests are really really thorough but are the only source of truth of the application functionality. No documents or unit or integration tests are present.\n\n  \nSo question for anyone who has experienced a migration like this:\n\n  \n1. Any effective way of speedy refactoring without waiting for 10 hours for tests feedback?\n\n2. What happens to the tests post migration? There are decades of edge case bug fixes being guarded by this regression suite but no one knows what the tests do. The historical assertions in those tests is what is keeping the system running and we don't want to lose it.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qtsopq/selenium_ide_test_case_migration/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o36edyw",
          "author": "flavius-as",
          "text": "Run the test suite with code coverage on.\n\nThis gives you all tests grouped by use case.\n\nThen split the tests by use case and run only the relevant tests.\n\nThen refactor the code behind the scene for only that use case. One use case at a time.\n\nThen create the rest api which uses the same code like jsf. Write tests for that. Introduce mutation testing. Either both selenium and the new test suite fails, or both succeed.\n\nOnce you got enough mass, you switch users to the new UI and remove the old tests.\n\nI'd suggest a gradual Roadmap in which you migrate certain use cases to the new UI, via the infrastructure routing.",
          "score": 3,
          "created_utc": "2026-02-02 16:19:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3goo1r",
              "author": "amfromeverywhere",
              "text": "Thanks! Run the test suite with code coverage on sounds like a good way to understand what those tests are testing. Sounds intensive but good and safe. I will try it out. Thanks again!",
              "score": 1,
              "created_utc": "2026-02-04 02:56:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3hp67w",
                  "author": "flavius-as",
                  "text": "Yeah. You also don't do it every day, you do initially to extract knowledge, and later in case you feel like you've lost track, although you shouldn't - if you do then it's a signal that you need to strengthen your discipline.",
                  "score": 2,
                  "created_utc": "2026-02-04 07:14:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvk9w9",
      "title": "A Scalable Monorepo Boilerplate with Nx, NestJS, Kafka, CQRS & Docker — Ready to Kickstart Your Next Project",
      "subreddit": "softwarearchitecture",
      "url": "https://github.com/ARG-Software/Nx-Monorepo-Boilerplate",
      "author": "FormalAd7608",
      "created_utc": "2026-02-04 09:41:00",
      "score": 7,
      "num_comments": 8,
      "upvote_ratio": 0.71,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qvk9w9/a_scalable_monorepo_boilerplate_with_nx_nestjs/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3iwqpc",
          "author": "nickchomey",
          "text": "This has to be a joke... Bull, redis AND kafka? ",
          "score": 3,
          "created_utc": "2026-02-04 13:17:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iypfb",
              "author": "FormalAd7608",
              "text": "You just use what you want. You can use them all or just one. One is for messaging, other for background jobs, other for caching. Just pick what you like the most.",
              "score": 1,
              "created_utc": "2026-02-04 13:28:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3j2pjg",
                  "author": "nickchomey",
                  "text": "You've missed the point. Surely 3 separate tools are not needed here. Redis (or, better yet, NATS) could do it all",
                  "score": 2,
                  "created_utc": "2026-02-04 13:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jqw5t",
              "author": "Ok_Cranberry4354",
              "text": "Whats your definition of a joke? For example, are you going to implement something similar to a payment system that needs real-time data streaming, replayability, durable event logs and a bunch of other things like strong delivery guarantees with NATS instead of Kafka which is industry standard when talking about scalability? NATS can do some of these but with implicating limitations.\n\nYou can technically force one tool to do everything but that doesn’t mean you should. At least looking at the repo, this is clearly a boilerplate, not a prescription, that's the idea of a boilerplate, you use a baseline that fits your needs the most and remove what you don't care about. If you're going to leave a comment like that on someone else's free open source contribution at least give some some architectural insight instead of just a reaction that seems pretty personal.",
              "score": 1,
              "created_utc": "2026-02-04 15:51:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3juoks",
                  "author": "nickchomey",
                  "text": "NATS was not the thrust of my comment - it was literally parenthetical to saying Redis could be used standalone. Though, I do stand by it. I'd be curious what the \"implicating limitations\" that you speak of are.\n\nAnd just because something is \"standard\" doesn't make it an appropriate boilerplate starting point - those tend to do best when refined, only bringing on more dependencies and services when absolutely needed. Even still, why not Redpanda over Kafka?\n\nMoreover, if you were to extend that \"standard\" line of thinking to the rest of the choices here, why use NestJS instead of something React-based? (I'm aware Nest and next.js etc are not all that comparable and am most definitely not advocating for anything React-based. Just making the point that Nest isn't an \"industry standard\")",
                  "score": 1,
                  "created_utc": "2026-02-04 16:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49otr2",
          "author": "tesseraphim",
          "text": "Lol, I thought you were building a monorepo, and thought the stack was click bait.",
          "score": 1,
          "created_utc": "2026-02-08 15:53:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwlfp2",
      "title": "Architecture for Flow • Susanne Kaiser & James Lewis",
      "subreddit": "softwarearchitecture",
      "url": "https://youtu.be/NwtE82Wzs_U?list=PLEx5khR4g7PJbSLmADahf0LOpTLifiCra",
      "author": "goto-con",
      "created_utc": "2026-02-05 13:35:15",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qwlfp2/architecture_for_flow_susanne_kaiser_james_lewis/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qw03vu",
      "title": "Clean code architecture and codegen",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qw03vu/clean_code_architecture_and_codegen/",
      "author": "Aggressive_Ad_699",
      "created_utc": "2026-02-04 20:35:06",
      "score": 7,
      "num_comments": 16,
      "upvote_ratio": 0.71,
      "text": "I'm finally giving in and trying a stricter approach to architecting larger systems. I've read a bunch about domains and onions, still getting familiar with the stuff. I like the loose coupling it provides, but managing the interfaces and keeping the structures consistent sounds like a pain.\n\nSo I started working on a UI tool with a codegen service that can generate the skeletons for all the ports, and services, domain entities and adapters. It'll also keep services and interfaces in sync based on direct code changes as well. I also want to provide a nice context map to show which contexts rely on other contexts. It'll try to enforce the basic rules of what structural elements can use, implement or inject others. I'll probably have a CLI interface that complements the UI which could be used in pipelines as well to validate those basic rules. The code will remain mostly directly editable. I'm aiming to do this for Python at first, but it doesn't seem too complicated to extend to other languages.\n\nThoughts about the usefulness of such a tool or clean code / DDD in general?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qw03vu/clean_code_architecture_and_codegen/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3lsz8p",
          "author": "UnreasonableEconomy",
          "text": "This is my personal opinion of course\n\n- Robert Martin: 🚩\n\n---\n\n> I'm finally giving in an trying a more strict approach to architecting larger systems.\n\nCorrect me if I'm wrong, but this sounds like \"nothing I've tried so far worked, so now I'll just do BDUF by the book\"\n\nThis is probably not gonna work out all that well either, but it depends on what you're trying to do.\n\n> So I started working on a UI tool with a codegen service that can generate the skeletons for all the ports, and services, domain entities and adapters\n\nThere have been efforts of this sort since time immemorial, and none of them have really ever stuck around or become universal. \n\nI however don't think it's a waste of your time (if you have the time) to pursue this - you'll learn all the problems associated with these types of prescriptive architectural styles. You'll find out what does and doesn't work. You'll become a bit better at making high level decisions.\n\nSA is as much an art form as it is engineering. Practice and experience are unfortunately no substitute for what you can learn from books.",
          "score": 9,
          "created_utc": "2026-02-04 21:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lyhxr",
              "author": "Aggressive_Ad_699",
              "text": "Thanks, I think that's a good take. Even if it doesn't become an everyday tool for me, let alone others I can still solidify my knowledge about this kind of architecture. Do you know why these tools don't seem to stay around or reach more people?\n\nTo clarify, I've mostly either worked on large legacy systems where I wasn't a part of most of the architectural decisions, or smaller green field projects that I'm yet to see grow to a medium size. I've been told a bunch of times that this kind of architecture isn't worth it, or it's too academic and overcomplicated. So I'm really giving into my own desires to see how it works for me:)",
              "score": 2,
              "created_utc": "2026-02-04 22:01:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3m6zhg",
                  "author": "UnreasonableEconomy",
                  "text": ">  Do you know why these tools don't seem to stay around or reach more people?\n\nI think that's a very good and hard question. I'd love the opinions of others here on this. \n\nMy take is that these tools encode not just a particular style, but define a framework by the nature of how they work. If you want to do anything, you have to do it in a particular way.\n\nSo these \"tools\" \"become\" \"frameworks\". \n\nWhat's a framework? I'd say it's an enforced collection of patterns. (which is what you'll do - you'll select a finite set of patterns that this tool will realize)\n\nBut what's a pattern? Why do we use patterns? I'd say, we use patterns to work around the shortcomings of some environment. It's a way of dealing with reality so we can achieve the outcomes we need. \n\nThe problem is that any finite selection of patterns can only cover a subset of the continuum of implementations required for our business cases. To deal with this, developers sometimes come up with new patterns to deal with the canonical way of doing things. Sometimes that works out fine. Sometimes it doesn't make sense at all.\n\nAs languages and environments and patterns evolve, it sometimes stops making sense to bend over backwards to appease the framework, and working outside of the framework becomes easier. At some point the frameworks becomes either so sidelined or adapted and specialized so it stops being the universal panacea it was supposed to be.\n\nAnd then someone invents a new framework to supplant all these specializations, and the cycle begins anew \n\n# 🤔\n\n---\n\n> I've been told a bunch of times that this kind of architecture isn't worth it, or it's too academic and overcomplicated. So I'm really giving into my own desires to see how it works for me\n\nI think that's good. There's a nugget of truth in everything, and if you have the energy and time and will to prospect for that bit of truth, that's perfect. That's probably the best way to become a good architect, especially if no one else has to suffer from your exploratory decisions lol.",
                  "score": 3,
                  "created_utc": "2026-02-04 22:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mlp32",
              "author": "trainbustram",
              "text": "I do want to say that at many large automotive companies, this tooling to define interfaces explicitly actually does exist and is actually used very often in software modeling, where you explicitly define all of your ports deployments, etc. then generate network and internal artifacts based on that method. Definitely much more useful in a distributed monoliths architecture where boundaries can shift easily from internal to external, rather than in a micro services architecture where the boundaries give or take like the same at all points in time.",
              "score": 2,
              "created_utc": "2026-02-05 00:04:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3phq6d",
                  "author": "Aggressive_Ad_699",
                  "text": "I suppose those are proprietary tools, right? Do you have an example in mind? \nHmm it's interesting you brought up microservices. It might be possible to easily reorganise contexts across repos as well. That's certainly out of scope for now. I'm going to focus on large monorepos first.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:46:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oukk7",
          "author": "edgmnt_net",
          "text": "Possibly hot take here, but merely adding some indirection and layers does *not* make your code loosely-coupled in a meaningful way, it's more like increasing effort, surface for bugs and making it more difficult to refactor. It is a pain because it is a pain. The fact that you're considering code generation is sort of a red flag and generating skeletons won't help when you get hit with a 3k lines PR for what would otherwise be a much simpler change. IMO people should stop this indiscriminate layering nonsense and focus on actual abstractions and designing actual robust APIs (when possible and needed, otherwise it's perfectly fine to write code in a direct style). I'm allowing for indirection where there are particular pain points and people stepping too much on each other's toes, but you need to be conservative about it.",
          "score": 3,
          "created_utc": "2026-02-05 09:32:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pimfr",
              "author": "Aggressive_Ad_699",
              "text": "Not at all, it's a cautionary advice. This was one of the main reasons I've been a bit afraid to look into clean code. You mention focusing on actual abstractions and robust APIs. Do you think the clean architecture with dependency inversion, ports, services, adapters, etc... is on the other end of things? If so could you elaborate on what kind of abstractions/patterns you have in mind?",
              "score": 1,
              "created_utc": "2026-02-05 12:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ptcvq",
                  "author": "edgmnt_net",
                  "text": "For example, compilers often have IRs (intermediate representations) that are carefully considered to allow translation of the code as well as optimizations (which might require combining knowledge of different languages or different CPUs without ending up with a combinatorial explosion of corner cases, some of those pseudoinstructions might not even resemble any concrete instruction). A database may provide a set of primitive operations that are enough to write applications within a certain consistency model and with certain transactional capabilities, possibly with a higher-level / convenience API on to make it easier for simple use cases. A compression library provides APIs that are generally robust for a wide variety of use cases so you don't need to go make changes to it to use it in your own application (and they're not changing the API surface all the time). An operating system kernel needs to provide a driver model such that a diverse set of hardware devices can be managed (some only require initialization, some require to be notified before being powered-down and so on). A JSON parser might provide both streaming and non-streaming parsers (building the entire representation in memory upfront) in a convenient way and for a wide variety of users. All these tend to require rather careful consideration.\n\nIn contrast and at least in a practical sense, stuff related to layered architectures often tends to be applied blindly and as a general recipe, being little more than arbitrary scaffolding. This tends to be compounded by people trying to split work top-down in a trivial way. It's easy to say \"hey, someone should do auth and someone should do the books endpoint\". But then the auth stuff could just be one or two calls into the framework / auth library and it's instead blown to an entire component that barely adds anything (and might even impose needless restrictions). In such cases nobody's really doing any real work of abstracting stuff, they're just writing wrappers for straightforward calls.\n\nSupposedly this sometimes protects against changing requirements but I find that's usually not true, it's just a place where you end up putting ugly hacks that would have been better fixed by large-scale refactoring. It reduces visibility into code and changes because everything is 7 useless layers deep. It makes it \"easy\" to write 100k LOC of code that barely does anything concrete. It makes it hard to write composable helpers because everything is encapsulated too tightly yet it's not robust enough to handle all reasonable use cases. And to some degree it shouldn't be, that's what the framework is for, while you're writing a very specific and concrete application.\n\nOn a somewhat related note, I think a good and even simpler test for code writing ability is to look at how people write functions/methods. Do they split them wisely? One could do either of (1) one big function or (2) a hundred very small functions that presume a bunch of invariants (\"I'm always being passed a non-empty array of at most 3 elements, I'll crash otherwise\"). Those are both pretty poor choices, usually. Or they can be mindful about stuff and split on natural boundaries and where there's least resistance, for example by making a helper that compares JSON objects a certain way and makes at least some sense on its own (either for DRY purposes or simply because it's clearer / more testable on its own). This kind of soft separation is very useful, because you're grouping things logically, making them easier to write / review / confirm they're working, while also allowing the possibility of refactoring at will. But it's not something that's just a recipe and it heavily depends on experience and what the code really does.\n\nThat being said, some layering may be fine, though. It's just that you have to be conservative about it because it has a cost. And it's often overused.",
                  "score": 1,
                  "created_utc": "2026-02-05 13:55:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oaibl",
          "author": "thecreator51",
          "text": "This sounds very useful. DDD and clean architecture help long-term maintainability, but managing interfaces is pain. Codegen skeletons and validation can save time and enforce consistency early.",
          "score": 2,
          "created_utc": "2026-02-05 06:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pixcv",
              "author": "Aggressive_Ad_699",
              "text": "I'm also thinking of something lean. Modern IDEs do a lot, I don't want to attempt to replace things they already do well, just augment their capabilities with the more opinionated rules of this architecture.\n\nHow would you prefer interacting with the tool?\n- Direct schema editing\n- A nice terminal UI\n- A web interface\n\nI want a simple CLI interface as well that can be called from IDE file watchers to update linked interfaces on save for example. The web UI might be slower to work with, but the context map and dependency map it can show might be useful for brainstorming.",
              "score": 1,
              "created_utc": "2026-02-05 12:54:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtsd8g",
      "title": "Questions about adding ElasticSearch to my system",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qtsd8g/questions_about_adding_elasticsearch_to_my_system/",
      "author": "Illustrious-Bass4357",
      "created_utc": "2026-02-02 11:18:39",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "so Im trying to use elastic search in my app for 2 search functions one for foods , and the other for meals , anyways I have some questions\n\n\n\nQ1. Should Elasticsearch indices be created manually (DevOps/Kibana/Terraform), or should the application be responsible for creating them at runtime , or is there's something like db migrations but for ES ?\n\nQ2. If Elasticsearch indices are managed outside the application, how should the app safely depend on them without crashing if an index is missing or renamed? For example, is it okay to just return an empty list when Elasticsearch responds with an error?\n\nQ3. Without migrations like SQL, how are index mapping changes managed over time?  \n\n\nQ4. Should the application be responsible for pushing data into Elasticsearch when DB data changes, or should this be handled externally via CDC (e.g., Debezium) or am I over engineering ?\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qtsd8g/questions_about_adding_elasticsearch_to_my_system/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o39q8ii",
          "author": "james-dev89",
          "text": "For index creation and management look into Index lifecycle management ILM, usually you create an alias and the alias is responsible for rotating the index. \nYou can configure the shards and everything. \nNewer versions of ES have better index management than older ones. \n\nFor data yeah a CDC makes sense but for us, we do index write as a batch operation, after we write to the database we fire off an event that writes to elastic search. \n\nHope this helps.",
          "score": 2,
          "created_utc": "2026-02-03 02:10:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3as015",
          "author": "Low_Satisfaction_819",
          "text": "As someone who has managed a few elasticsearch clusters in my life, consider typesense.",
          "score": 2,
          "created_utc": "2026-02-03 06:21:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxxpvb",
      "title": "Hello, I have big project contract basically signed so I need little guidelines",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qxxpvb/hello_i_have_big_project_contract_basically/",
      "author": "dbo4444",
      "created_utc": "2026-02-06 23:36:08",
      "score": 7,
      "num_comments": 13,
      "upvote_ratio": 0.65,
      "text": "Hello, \n\ni have project that I have to start working on something like real estate platform where users can publish their own real estates for sale. So quite big project. I have 6-7 years experience in software development but mostly ERP and CRM systems, maintaining legacy code and few small and medium websites and web applications built but never something this \"wide\".\n\nTech stack that I will be using **Vue.js + PHP + SQL** because it is something that I have done before and most experienced with (out of those programming languages that you do not have to spend 2000$+ to have licence). \n\n  \nI am still looking at some examples and staring to write down directions that I have to follow but nothing major and not unexpected. \n\nSo, questions for more experience colleagues, where would you start and what to do first...anything that you think would help me?  \n  \nThanks ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qxxpvb/hello_i_have_big_project_contract_basically/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3zt8u9",
          "author": "GrogRedLub4242",
          "text": "you are literally paid to do that. do it. otherwise pay us",
          "score": 35,
          "created_utc": "2026-02-06 23:53:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4007k2",
              "author": "halfxdeveloper",
              "text": "Right?",
              "score": 3,
              "created_utc": "2026-02-07 00:33:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o400bbi",
          "author": "halfxdeveloper",
          "text": "Don’t spend all the money you make because they’ll want a refund when they find out you don’t know what you’re doing.",
          "score": 23,
          "created_utc": "2026-02-07 00:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44rmji",
          "author": "fteq",
          "text": "Start with the boring stuff: scope + MVP + acceptance criteria. For a real estate platform, “wide” explodes fast (roles, listings, messaging, moderation, payments, SEO, maps, spam). I’d write a 1–2 page PRD: user roles, core flows, must-have vs later, and what “done” means. Then design the data model early (Listings, Users, Media, Locations, Status, Audit logs). Also: nail non-functional requirements (security, GDPR/PII, backups). Since you said the contract is basically signed, run it through AI Lawyer to make sure deliverables, milestones, and change-order language protect you before scope creep hits.",
          "score": 3,
          "created_utc": "2026-02-07 19:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o459ruw",
          "author": "No_Flan4401",
          "text": "Start gathering requirement, get a idea on the domain, settle on scope for MVP. From there find the obvious architecture patterns, and start with the simplest and post phone major design to as late as possible. \n\n\nOut of interest what languages requires license? I think I know zero",
          "score": 2,
          "created_utc": "2026-02-07 21:17:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o403l34",
          "author": "zangler",
          "text": "So...you basically still want to support legacy for then?",
          "score": 2,
          "created_utc": "2026-02-07 00:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40zl0a",
          "author": "commanderdgr8",
          "text": "If php, then there would be some readymade project already build, you can use that directly or look at the code and build yourself. You can ask claude also to build for you.",
          "score": 1,
          "created_utc": "2026-02-07 04:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o420tb4",
          "author": "breek727",
          "text": "I would find out what their anticipated running monthly costs are and look to no code as much as possible to get something up and running and then slowly migrate to custom pieces as needed, how many users are they expecting up front etc.",
          "score": 1,
          "created_utc": "2026-02-07 09:41:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40a4n6",
          "author": "kokoricky",
          "text": "That’s just a simple crud, a monolithic is all you need. The backend won’t be anything special as long as u cache aggressively, your frontend will need some optimisations to make scrolling smooth with data fetching.\nYour most complicated component would be the postgresdb schema and overall deployment.",
          "score": -1,
          "created_utc": "2026-02-07 01:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o418meo",
          "author": "never-starting-over",
          "text": "That doesn't sound like a quite big project. The free advice I can give you is consider using a CMS like Directus, Strapi or whatever.",
          "score": 0,
          "created_utc": "2026-02-07 05:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o404de3",
          "author": "roynoise",
          "text": "dm me if you'd like a coach. We can split the project.",
          "score": -2,
          "created_utc": "2026-02-07 00:57:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zswzt",
          "author": "TheMightyTywin",
          "text": "Make sure you’re getting some money paid up front or are being paid by the hour, this is a big project that will take at least a year.\n\nYou should create a separate backend and front end, and use established frameworks for each. I’d recommend nextjs and node.js with Postgres. You could use laravel if you really want to stick with php but since I’m assuming your plan is to use ai agents you might as well use node.\n\nIf you use nextjs and node, you’ll have tons of options when it comes to deployment.\n\nNext consider iac, terraform is the industry standard.\n\nNext you’ll need to prep your ai agents. You need to create architecture, patterns, and docs that they can follow. Resist the urge to start building features. Instead, fill the project with best practices documentation about all relevant details, and settle on the software architecture patterns you want the agents to use.\n\nCreate your Claude.md and agents.md files, point them at your docs.\n\nFigure out how you’re going to do testing: unit, integration, e2e etc. Create docs, update agents files\n\nFinally, create an example feature and implement with ai agent. Make sure it is *perfect* as the agents will copy it.\n\nAfter that, you just need to write down all the features with as much detail as possible and give it to Ai agents. Make sure they write and run lots of tests.",
          "score": -4,
          "created_utc": "2026-02-06 23:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40jgma",
          "author": "thabc",
          "text": "Check out Claude Code.",
          "score": -2,
          "created_utc": "2026-02-07 02:30:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtn6gw",
      "title": "The Power of Bloom filters",
      "subreddit": "softwarearchitecture",
      "url": "https://pradyumnachippigiri.substack.com/p/the-power-of-bloom-filters-in-system",
      "author": "Comfortable-Fan-580",
      "created_utc": "2026-02-02 06:10:28",
      "score": 6,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qtn6gw/the_power_of_bloom_filters/",
      "domain": "pradyumnachippigiri.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o35eya0",
          "author": "inflammatoryusername",
          "text": "r/deadinternettheory",
          "score": 6,
          "created_utc": "2026-02-02 13:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o36vkmy",
              "author": "thegreatjho",
              "text": "Dunno, after reading it there are enough grammar errors to make me think it’s at least human edited if nothing else. Likely by a ESL writer.",
              "score": 1,
              "created_utc": "2026-02-02 17:38:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qtuyi1",
      "title": "Have to extract large number of records from the DB and store to a Multipart csv file",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qtuyi1/have_to_extract_large_number_of_records_from_the/",
      "author": "MasterA96",
      "created_utc": "2026-02-02 13:25:17",
      "score": 5,
      "num_comments": 11,
      "upvote_ratio": 0.86,
      "text": "I have to design a flow for a new requirement. Our product code base is quite huge and the initial architects have made sure that no one has to write data intensive code themselves. They have pre-written frameworks/utilities for most of the things. \n\nBasically, we hardly get to design any such thing ourselves hence I lack much experience of it and my post might seem naive so please excuse me for it.\n\n(EDITED) The requirement was that we will be using RabbitMQ so the user request to service A will send a message to the queue and there will be a consumer service B which would use Apache Camel, would go through routes (I mean so it's already asynchronous) to finally requesting records from the join of tables. (Just a simple inner join, nothing complex) Those records might or might not need processing and have to be written to a multipart file of type csv, which would be sent to another API to another service C.\n\nWe're using PostgreSQL. I've figured out the Camel routing part (again using existing utilities). Designed a sort of LLD. Now the real question was fetching records and writing to csv without running into OOM issue. It seems to be the main focus of my technical architect.\n\nI've decided on using - (EDITED)\n\nJdbcTemplate.query using RowCallBackHandler\n\n(Might use JdbcTemplate.queryForStream(...), since I'm on Java 17 so better to use streams rather than RowCallBackHandler, but there are other factors like connection stays open, fetchSize on individual statement isn't possible)\n\nWould be using a setFetchSize(500) - Might change the value depending on the tradeoffs as per further discussions.\n\nMight use setMaxRows as well.\n\nThe query would be time period based so can add that time duration in the query itself.\n\nThen I'll be using CSVPrinter/BufferWriter/OutputStream to write it to the Multipart file (which is in memory not on disk). [Not so clear on this, still figuring out]\n\nEDIT - \nSo, service C is one of the microservice which would eventually store the file as zip in a table. DB processing can be done in chunks but still file would be in memory. So have decided to stream write to a temporary file on disk, then stream read it and stream write to a compressed zip and then send it to service C. I'm currently doing a POC of this approach if that's even possible or not.\n\n\nThis is just a discussion. I need suggestions regarding how I can use JdbcTemplate, CSVPrinter, Streams better.\n\n\nI know it's nothing complex but I want to do it right. I used to work on a C# project (shit project) for 4.5 yrs and moved to Java, 2 yrs back. Roast me but help me get better please. Thank you. ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qtuyi1/have_to_extract_large_number_of_records_from_the/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o37uxvj",
          "author": "two-point-zero",
          "text": "Look a bit confusing to me. Help me to understand:\n\n-Why rabbit?it means that the  process that read the file (call it the producer) is not on the same software/machine/system of the one that read them? ( Call it the consumer). So producer read from your db, write messages in rabbit and a different system will read and build the csv?\n\n- camel is not enough to be \"automatically async\" because of routes. A route can be fully sync, and to be honest in most case you want to be sync ( for example if you are dealing with transactions). The async is because of Rabbit,and it's async between producer and consumer still the consumer part might or might not be sync with the thread that read from rabbit.\n\n- if messages goes from postgeres to the CVS producer via rabbit,the advance of using streamed query highly depends on how you write on rabbit.do you send rows in bulk ( 10,100,1000s each rabbit message) or one message each row ( very poor performance wise)?\n\n- by the way if performance is a concerns,don't use text formats for rabbit messages (Jason,XML ) goes for a binary serialization library.\n\n- what is a multiparty csv? There are csv payload and multipart http requests. Multipart csv look new to me.\n\nClarify these please and we can continue...\n\nEdit: and most important how \"big\" is big here? Thousands of records?millions? billions? A 10MB csv, 100 MB? GB?",
          "score": 2,
          "created_utc": "2026-02-02 20:20:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3adv7z",
              "author": "MasterA96",
              "text": "Sure, I'll elaborate. RabbitMQ is used just after we receive the user request. Basically the request message will go to RabbitMQ and gets consumed by another service which would then produce the file and send to a 3rd service.\n\nWe can rule out the Camel routing part here. I just mentioned RabbitMQ and Camel to tell that DB call would happen on a separate thread so it's already non-blocking in one way.\n\nSo the DB call and CSV formation both are part of the RabbitMQ consumer service.\n\nIt will be sending the Multipart File which will be of type CSV. \n\nHow the 3rd service will send/display that file to the user is completely out of my control.\n\nNumber of records currently atmost are 100k, but will increase with time. The table is already monthly partitioned and indexed.\n\nAlso, I know it might not take that much memory right now but my team's main concern is that we should still not bring and store everything in memory.\n\nEdit: To summarize I want to know how can I use JdbcTemplate in a better way as per this usecase.",
              "score": 1,
              "created_utc": "2026-02-03 04:34:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3b2ahx",
                  "author": "two-point-zero",
                  "text": "Ok.nice.\n\nStarting form the end. If you want to keep a minimum of streaming features you probably want to use chunked data (if http 1) or DATA frames (if http/2) but you need to be sure that both your http client and API server are able to manage them (which should be..but check to be sure).\n\nIf not, you have to send the CVS file all in once,that means load in memory all at once. Or you might want to save it on some third party storage and send link to the external API to retrieve if it too big.\n\nConsidering you can actually send data in streaming fashion, I think I would go for paginate the query on db assign each page to a different thread, read and process In parallel (you may want to track some row key or page number to reorder data at the end),and then compose and reorder it need. With camel should be matter of a couple of stages to accomplish that, patterns of scatter and recompose is well supported.\n\nThen you can buffer your rows,once processed and reordered and send them when ready again in chunk,or data as said before.\n\nBut keep in mind,if you need  any summarized data (like the average of some value,thing like that )you cannot avoid to load  everything in memory at least once.\n\nI would keep thing parametric like:\n\n- records in a page \n- number of thread for reading\n -dimension of chunk\n- dimension of http post body buffer\n\nAnd play a bit to see what fit best. Speed and Memory and resources are always connected,high speed, more parallelism,more memory and more resources but faster.\n\nLess parellism,or smaller db pages,will consume less resources but requires more time to complete..you need to try and tweak.\n\nIdeally if you want to stream directly from db to API,you cannot use more that one thread for db page (no parallelism, one page at time, out of db through API sync)\n\nThe more parallel thread you will use the more memory you need because worst case, to build an ordered chunk you might need to keep in Memory all the required pages until you fill the write buffer. I.e. you spin 3 thread and read page 1,2,3 but thread 3 finish first. To send all the records ordered you need to wait for thread 1 an 2 to complete, so worst case you need all 3 Db pages to be loaded in memory for reordering.\n\nso keep main parameters variables using external properties ( possibly with hot reload or JMX access to change them while running) and find your sweet spot.",
                  "score": 2,
                  "created_utc": "2026-02-03 07:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3bqdba",
          "author": "garden_variety_sp",
          "text": "Camel SQL producer will give you a ResultSetIterator. You can stream from that. Transform if you need to. The Camel file producer will happily take a stream as input and will even chunk it for you if required.",
          "score": 2,
          "created_utc": "2026-02-03 11:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kjoqe",
              "author": "MasterA96",
              "text": "Oh, I'll look into these for sure.",
              "score": 1,
              "created_utc": "2026-02-04 18:04:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3gpfoh",
          "author": "SolarNachoes",
          "text": "Sounds like all you need are batches. Grab rows from DB in a paginated manner and write to CSV.\n\nIs service C external? If not I would save the CSV to storage then pass a reference to it to service C.",
          "score": 2,
          "created_utc": "2026-02-04 03:00:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3kjiqy",
              "author": "MasterA96",
              "text": "Sure.\n\nNo, service C is one of the microservice which would eventually store the file as zip in a table. DB processing can be done in chunks but still file would be in memory. So have decided to stream write to a disk, then stream read it and stream write to a compressed zip and then send it to service C. I'm currently doing a POC of this approach if that's even possible or not.",
              "score": 1,
              "created_utc": "2026-02-04 18:03:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3lltec",
                  "author": "SolarNachoes",
                  "text": "Storing large binary in a database is often not a good idea.",
                  "score": 1,
                  "created_utc": "2026-02-04 21:01:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3lw16s",
          "author": "InstantCoder",
          "text": "If you are using PostgreSQL you can let it generate csv for you in a very efficient and fast way.  And you can directly stream this file with MultiPart. \n\nUse the CopyManager class and use the copy command inside it. \n\n[here is an example](https://github.com/quarkiverse/quarkus-fluentjdbc/blob/main/examples/src/main/java/com/acme/fluentjdbc/controller/FruitResource.java). Check from line 230.",
          "score": 1,
          "created_utc": "2026-02-04 21:50:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qttlp8",
      "title": "Flashcard, Anki for Certified Professional for\nSoftware Architecture (CPSA)®",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qttlp8/flashcard_anki_for_certified_professional_for/",
      "author": "yisi11",
      "created_utc": "2026-02-02 12:22:34",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "Would anyone known if there are any flashcards, or an anki deck that could help in the preparation for the CPSA?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qttlp8/flashcard_anki_for_certified_professional_for/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": []
    }
  ]
}