{
  "metadata": {
    "last_updated": "2026-01-18 16:49:53",
    "time_filter": "week",
    "subreddit": "softwarearchitecture",
    "total_items": 20,
    "total_comments": 132,
    "file_size_bytes": 161843
  },
  "items": [
    {
      "id": "1qdhlca",
      "title": "The U.S. Gov once threw a hacker in solitary because they thought he could WHISTLE nuclear launch codes. I wish I was joking.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qdhlca/the_us_gov_once_threw_a_hacker_in_solitary/",
      "author": "Suspicious-Case1667",
      "created_utc": "2026-01-15 11:57:07",
      "score": 59,
      "num_comments": 14,
      "upvote_ratio": 0.82,
      "text": "Okay Reddit, gather around, because this is one of those stories where reality is so stupid it loops back into entertainment.\n\nSo, in the 90s, hacker Kevin Mitnick gets arrested. Fine. He did hack stuff. Cool.\nBut here‚Äôs where everything goes full WTF levels unknown to mankind:\n\nA federal judge was convinced genuinely, unironically convinced that Kevin could ‚Äústart a nuclear war by whistling into a phone.‚Äù\n\nLet me repeat that:\nA man was thrown in solitary confinement because someone thought he could blow up the world using dial-up noises.\nThis wasn‚Äôt satire.\nThis was the United States justice system.\nThey literally banned him from:\nUsing a phone\nTouching a computer\nBeing near anything with ‚Äútones‚Äù\nAnd kept him in solitary like he was a human rootkit about to self-replicate\nAll because they believed he was some kind of mythical techno-wizard who could whistle binary like a Final Boss NPC.\n\nMeanwhile, actual cybersecurity experts were like:\n\n‚ÄúUh‚Ä¶ that‚Äôs not‚Ä¶ how anything works.‚Äù\nAnd the court was like:\n‚ÄúShhhhh. He‚Äôs dangerous. He knows‚Ä¶ computers.‚Äù\n\nThe whole thing became one of the biggest controversies in cybercrime history because it showed just how hilariously clueless the system was about technology.\n\nImagine going to prison because a judge thinks you might be able to hack NORAD with your mouth.\n",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qdhlca/the_us_gov_once_threw_a_hacker_in_solitary/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzppld9",
          "author": "jonathon8903",
          "text": "From my understanding of his story it was an unfortunate mix of an overzealous prosecutor and a judge who didn't understand technology to have any doubt. I'd like to hope that a similar situation couldn't happen today.",
          "score": 17,
          "created_utc": "2026-01-15 11:59:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzpw4hy",
              "author": "Isogash",
              "text": "The system generally assumes that the defense is responsible for defending against stupid prosecutions, so this guy's lawyers were clearly not good enough. I don't necessarily agree but that's still how it works.\n\nI'd like to think that we've come a long way since and you are more likely to get lawyers who are tech-savvy enough to cut through the bullshit.",
              "score": 3,
              "created_utc": "2026-01-15 12:45:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzq3pbv",
                  "author": "caboosetp",
                  "text": "The lawyer doesn't even need to be tech savvy. They need to be smart enough to listen to their client and find an expert who can testify.¬†",
                  "score": 7,
                  "created_utc": "2026-01-15 13:31:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzq4b6o",
          "author": "Fine-Ad9168",
          "text": "Whistling into phones to hack was very much a thing.  I don't think think you could launch missiles, I think you could just get free long distance calls.",
          "score": 10,
          "created_utc": "2026-01-15 13:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpukx2",
          "author": "eduanlenine",
          "text": "And now we all have to watch boring videos from his security training company",
          "score": 8,
          "created_utc": "2026-01-15 12:35:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqf0tk",
          "author": "pandershrek",
          "text": "You can indeed hack an air gapped machine with tone injection. It was shown with a PoC less than 20 years ago, so this actually isn't far fetched because most scada systems are hilariously insecure.\n\nOn the tail of Stuxxnet they did a ton of work at kapersky to understand how so many exploits went into one attack. \n\nThough this might have been from a presentation at black hat before that",
          "score": 5,
          "created_utc": "2026-01-15 14:32:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqk2zj",
          "author": "Exotic_eminence",
          "text": "üÜì KEVIN\n\nTo be fair he did do a lot of hacking with his mouth \n\nüëÑ \n\nIt‚Äôs called social engineering",
          "score": 6,
          "created_utc": "2026-01-15 14:57:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpq3ig",
          "author": "Comprehensive-Art207",
          "text": "‚ÄùEverything is computer.‚Äù /DJT",
          "score": 6,
          "created_utc": "2026-01-15 12:03:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpuyks",
          "author": "GForce1975",
          "text": "Mitnick was a cocky guy and didn't do himself any favors..but that was pretty ridiculous.\n\nRemember all the \"free mitnick\" merch?",
          "score": 5,
          "created_utc": "2026-01-15 12:37:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o045q5d",
              "author": "pioo84",
              "text": "Man, I'm frickin old.",
              "score": 2,
              "created_utc": "2026-01-17 15:06:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzr4u8p",
          "author": "arnedh",
          "text": "There was both the possibility of changing the mode of a telephone call with 2600Hz and of submitting numbers as DTMF tones. https://en.wikipedia.org/wiki/Phreaking.\n\nWith DTMF tones, he might even have been capable of controlling equipment and hitting higher privileges with ABCD codes. https://en.wikipedia.org/wiki/DTMF_signaling, https://en.wikipedia.org/wiki/Precise_tone_plan\n\nWhat you could possibly achieve vs a government computer with those two techniques (and clicking/tapping the right rhythm) is not clear to me, but if I were the judge, I think I would err on the side of caution - the government wouldn't be likely to be forthcoming with specifics on what one could or couldn't hack, especially if security was below par  - imagine the headlines.",
          "score": 3,
          "created_utc": "2026-01-15 16:32:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzro69i",
          "author": "hxtk3",
          "text": "You say it's totally unreasonable like that should mean it's not true, but at one point in the development of the US ICBM-based nuclear deterrent, the design was such that a brown-out in rural Montana would've triggered a nuclear war. So I'm not surprised that someone unfamiliar with cybersecurity wouldn't immediately write off the possibility.",
          "score": 2,
          "created_utc": "2026-01-15 17:59:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztk4jc",
          "author": "bigbearandy",
          "text": "TBF, those of us who know how to whistle multi-frequency tones were able to drop a POTS line into maintenance mode and do a number of things you would otherwise need a blue box to achieve. However, hacking NORAD was not one of those things. At the time, the Defense Nuclear Agency access nodes were wide open, so you didn't need to whistle; you just social-engineered a login. \n\nDirect attacks weren't Mitnick's style; he usually collaborated with his partner, Roscoe, to physically penetrate a facility for intelligence and later used that information to hack it. \n\nAny sufficiently advanced methodology seems like magic, I guess. It was no fun for Mitnick when he was in prison, though, because he couldn't even call anybody to get money on his books.",
          "score": 2,
          "created_utc": "2026-01-15 23:16:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv9r4a",
          "author": "MattAtDoomsdayBrunch",
          "text": "hah! That's not how it works. The internet is a series of tubes.",
          "score": 1,
          "created_utc": "2026-01-16 05:09:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdjwx8",
      "title": "How to Make Architecture Decisions: RFCs, ADRs, and Getting Everyone Aligned",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/how-to-make-architecture-decisions-rfcs-adrs-and-getting-everyone-aligned-ab82e5384d2f",
      "author": "trolleid",
      "created_utc": "2026-01-15 13:45:44",
      "score": 44,
      "num_comments": 3,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qdjwx8/how_to_make_architecture_decisions_rfcs_adrs_and/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": [
        {
          "id": "nzqft3f",
          "author": "wampey",
          "text": "Thanks for this. Having listened to fundamentals of software architecture, I heard much of this but did not have it in paper form.",
          "score": 3,
          "created_utc": "2026-01-15 14:36:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrrib5",
              "author": "Frosty_Customer_9243",
              "text": "https://preview.redd.it/2fa1lc931kdg1.png?width=504&format=png&auto=webp&s=51bf9e4a0e76f12d88b39dd5c1cb8d561eadd279\n\nCheck out Andrew Harmel-Laws book \"Facilitating Software Architecture\"  \n[https://facilitatingsoftwarearchitecture.com/](https://facilitatingsoftwarearchitecture.com/)",
              "score": 5,
              "created_utc": "2026-01-15 18:14:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzscjyv",
          "author": "DeathByWater",
          "text": "I've been doing something similar lately, but with a few stages and a virtual/collaborative whiteboard. It goes a bit like this:\n\n\n- Introduce the problem/feature/proposed UX for addressing that problem (synchronous) - let people sit with it for a couple of days\n- Draw up a picture/diagram of the proposed technical implementation on a board and share with team (async)\n- Talk over in a shared meeting; make adjustments, add sticky notes (sync)\n- In that meeting - or a following one if it's bigger - ask the team to draw boxes around the various bits that might represent individual tasks (sync)\n\n\nBy the time backlog refinement/sprint planning/whatever you call it comes around, people are already familiar with the overall big picture (literally) and the scoping of an initial set of tasks. It's working fairly well; now just working on templating/scaling that process so other members of the team can drive it too.",
          "score": 3,
          "created_utc": "2026-01-15 19:49:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa83h4",
      "title": "Anyone actually keep initial architecture docs up to date and not abandoned after few months? Ours always rot",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qa83h4/anyone_actually_keep_initial_architecture_docs_up/",
      "author": "Independent-Run-4364",
      "created_utc": "2026-01-11 19:12:46",
      "score": 40,
      "num_comments": 31,
      "upvote_ratio": 1.0,
      "text": "At my current team, we started out with decent arch docs ‚Äúhow the system works‚Äù pages. Then we shipped for a few weeks, priorities changed, a couple of us made small exceptions and now suddenly we don't use the them anymore and they r lost in time.\n\nIf you‚Äôve found a way to keep this from rotting, what‚Äôs the trick? like ADRs that people would actually read ? some sort of PR gate and checklist? or do you just accept it and rely on code review + tribal knowledge?\n\nWould love to hear what‚Äôs worked ! (or what you tried that was a total waste of time)\n\nEDIT: Thanks everyone for your advice !!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qa83h4/anyone_actually_keep_initial_architecture_docs_up/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz0uvpf",
          "author": "sfboots",
          "text": "I update the top level architecture documentation around when we hire a new person.   Then I get the new developer to review and bookmark it\n\nWe are growing slowly so that is every other year",
          "score": 22,
          "created_utc": "2026-01-11 19:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz12d0i",
          "author": "ThigleBeagleMingle",
          "text": "_Do you keep arch docs up day?_ lol that‚Äôs a good one. \n\nAnything not automated or blocking ci/cd ain‚Äôt happening.",
          "score": 12,
          "created_utc": "2026-01-11 19:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0x47v",
          "author": "Saki-Sun",
          "text": "1. Replace architectural documentation with simplified on boarding style docs.\n\n\n2. Stick them as close to the code as possible.\n\n\n\n3. Do your jobs and keep them up to date you lazy bastards.",
          "score": 12,
          "created_utc": "2026-01-11 19:31:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1aloc",
          "author": "RGBrewskies",
          "text": "docs are out of date more or less instantly\n\nit's one of the reasons why agile manifesto pushes working code over comprehensive documentation \n\nbut try explaining that to middle management",
          "score": 6,
          "created_utc": "2026-01-11 20:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0wvcn",
          "author": "DeathByWater",
          "text": "The rot hasn't set in at my current posting. Potential reasons:\n\n\n- Modeling in C4 (simple), and only down two top levels of hierarchy; leaving the details out because:\n- Details change frequently and are hard to maintain, but the source of truth for the details is the IaC, not the diagrams\n- Getting the team involved in planning frequently means they have to be up to date as an effective piece of communication¬†",
          "score": 8,
          "created_utc": "2026-01-11 19:30:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz149oy",
              "author": "Rednavoguh",
              "text": "So even the top level docs are modeled in C4? Does that serve your needs? Here we are comtemplating building up a new capability model but I'm very curious to hear from you if C4 can provide the same information",
              "score": 1,
              "created_utc": "2026-01-11 20:03:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz1by5m",
                  "author": "DeathByWater",
                  "text": "Yeah, the highest level arch doc is the C4 one. It's fine. Maybe a couple of caveats:\n\n\n- It's a startup and we've only been going about 18 months, so not a lot of time for cruft to build up\n- Very limited number of services yet; nothing like the sprawl you get at an enterprise after 20 years\n- I'm deliberately sticking to pretty simple architecture; REST APIs, pubsub, and queues from which event messages are consumed.\n\n\nThat said, I've worked with a very large media company that worked with more much more complex sets of services and architecture, and it seemed to work for them too.\n\n\nI just use a miro board with links to different frames, but they used LeanIX to keep on top of it.\n\n\nThere are of course confluence docs and READMEs etc that describe more of the detail; but pictures have higher information density than words for the broad strokes.",
                  "score": 1,
                  "created_utc": "2026-01-11 20:39:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz0v981",
          "author": "caprica71",
          "text": "They usually rot \n\nArchitect KPIs are usually about how many projects they worked on. Not about keeping things up to date after the project is over.\n\nI have lost track of how many current states I have had to rebuild from scratch.   Sometimes you get lucky and a operational team will have great docs, but mostly they are a shambles",
          "score": 3,
          "created_utc": "2026-01-11 19:23:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz0vurq",
          "author": "Masked_Solopreneur",
          "text": "When you say initial, it sounds like you are going from greenfield to continious dev/ops. I find the need for suchs docs lower in continious dev/ops than in the beginning of a project.¬†\nThink about what value you want from the docs or if you really just seek to have them properly maintained.¬†\nIf the value is clear for you, you need to communicate it to your team so you can engage in maintenance. Principles, like Did, can be helpfull here.¬†\nKeeping docs close to code (mermaid etc.) I find helpfull for software architecture. A central codebase for broader context can also work.\nI have also had success with using tracing as the source of truth for integrations.¬†",
          "score": 2,
          "created_utc": "2026-01-11 19:26:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1b1d3",
          "author": "virtualstaticvoid",
          "text": "The \"trick\" I think is in the team culture and values - if the team wants to keep documentation up to date, then it will be so.\n\nIn my experience, that's rarely the case, unless their is a business driver or the project is open source. \n\nI've found that including documentation _together_ with each ADR, not only provides an event log of the evolution of the system, but also defines the context for the documentation, so it remains relevant and thus doesn't need to be updated as such. If the architecture changes, you add a new ADR and new documentation.",
          "score": 2,
          "created_utc": "2026-01-11 20:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1b3yh",
          "author": "anotherchrisbaker",
          "text": "Someone needs to own them. Maybe lead or EM?",
          "score": 2,
          "created_utc": "2026-01-11 20:35:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz1vw37",
          "author": "felipasset",
          "text": "Automate architecture documention generation and automate architecture rules checking.",
          "score": 2,
          "created_utc": "2026-01-11 22:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3h3aj",
          "author": "arihoenig",
          "text": "Ours are petrified at the moment of sign-off. They are nothing more than a mechanism to get buy in from stakeholders and once that Rubicon is crossed, they become fossils.\n\nThey do serve a secondary purpose in that the process of creating the document helps clarify the design in the architects mind.\n\nIt isn't a big deal because we can just get copilot to document the architecture from the current source and that is way better anyway.",
          "score": 2,
          "created_utc": "2026-01-12 03:06:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4ue5x",
          "author": "JosephineRoberts_",
          "text": "The only way I‚Äôve seen docs not rot is making them part of the change, not homework after. We keep a tiny set of ‚Äúliving‚Äù docs in the repo (high-level diagram, invariants, contracts) and any PR that breaks one has to update it, same as tests. Everything else is allowed to be historical junk and we don‚Äôt pretend it‚Äôs current.",
          "score": 2,
          "created_utc": "2026-01-12 09:18:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz67oup",
          "author": "symbiat0",
          "text": "You could make the doc updates mandatory to pass a PR... üòè",
          "score": 2,
          "created_utc": "2026-01-12 15:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz12zwl",
          "author": "ziksy9",
          "text": "This is one place where Agentic workflows really shine.  I have a plethora of decision docs, feature specs, planned features, architecture designs, analyzses, security reviews, etc.\n\nI'm trying to get better at every major milestone to update these docs by telling AI to identify any misconceptions, architectural alignment, missing features, and missing information. Believe it or not, it works quite well to ensure things are aligned, the readme is up to date, any make file changes are documented. Once files haven't been modified for a long while they are moved to docs/decisions, docs/archive, etc. anything in the top docs/ layer is a live or continually changing document.\n\nThis also is a force multiplier because you can refer to these docs for context when prompt engineering and since it's all in version control, you can see the history as needed.\n\nFor every epic, arch decision, or proposed feature plan, a document is added along with local backlog.md tasks also stored in the repo for reference. This has been a game changer since everything you need to know, work on, and what the current code status is, is contained right in the local repo along with detailed work history with document references.",
          "score": 4,
          "created_utc": "2026-01-11 19:58:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz1vss7",
              "author": "fartnugges",
              "text": "Could you share a bit more about how you do this? How does AI identify missing information, features, etc? Like, do you have a back and forth chat with it, or have it review your code base?",
              "score": 2,
              "created_utc": "2026-01-11 22:11:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nz2a07g",
                  "author": "ziksy9",
                  "text": "Plan: I give the agent the readme, where the docs are located, what my intentions are, and what personas it should use (golang architect etc which I have agents set up for). I have it build context for the task at the top level without deep diving.\n\nCollect: I ask the AI to run multiple agents with this context and identify features that are not documented, documentation that is out of date with the code base, etc.  I have a backlog.md MCP running, and have it create tickets to update documentation with code and doc references.\n\nExecute: I review and approve all the tickets created and have a specific agent use that context to do a deep dive and either accept the results, or have a conversation about the focused task until I'm happy with the result. Having local tickets helps if the context or session is lost/corrupt.\n\nReview: for each task I have it complete, I manually review, close the ticket and commit, then move to the next.\n\nThis works even better when you have swagger docs built in to your APIs, unit tests, and acceptance tests that can be run and referenced for even more context.\n\nThe biggest thing is providing enough context around a specific task without having the world in a single session to provide focus.\n\nFor new features:\nI generally use claude Opus for the harder things, and  Sonnet for information gathering.  I will also tell Claude to use multiple personas to review architecture decisions, provide alternatives, and come to a consensus between multiple personas. Some are architecture experts, some are business, some security, some are cloud experts.  Then I converse to fill any gaps or other alternatives and have it write out an implementation plan and once happy, create backlog tickets for the new feature. Rinse, repeat.\n\nAt checkpoints I will do a documentation scub, a security audit, test coverage check, lint, etc to keep project health acceptable and limit any cruft.\n\nFor existing applications. I have had Claude generate all the documentation, mermaid charts (all markdown) for application design and workflows, codebase layout with descriptions, and provide explanations of the purpose of modules and a glossary. I found it's the fastest way to get up to date and a mostly correct understanding of a system before I start deep diving in the codebase.",
                  "score": 4,
                  "created_utc": "2026-01-11 23:22:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz1wc95",
          "author": "PaulPhxAz",
          "text": "I find having \"Documents\" is the issue.\n\nYou should have a wiki with living pages that the team has signed-on to steward the knowledgebase.  And they have to be useful and concise.\n\nThat is a big ask, and a big cultural shift for a lot of people.",
          "score": 1,
          "created_utc": "2026-01-11 22:14:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2037x",
          "author": "failsafe-author",
          "text": "I thinks this is hard to do well, and actually would be one of the best use cases for AI. If every PR went through an AI check to ensure the architecture docs are still accurate, that would make it so much easier to keep them in sync.",
          "score": 1,
          "created_utc": "2026-01-11 22:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2x18n",
          "author": "tarwn",
          "text": "Split your docs into 3 types of docs:\n\n1. How to setup the system, run it, run tests, etc.\n2. The map: start at the highest possible level, explain how it works and why, then go one level deeper in every area that matters, do it again\n3. The change: an RFC or ADR when a change is made that describes why the change is made\n\nUpdate #1 when you make major changes, every time someone sets up the system then update it if it's out of date.\n\nUpdate #2 when you make major changes to the system as part of the definition of done. Review it every 4 months for the things that got forgotten. Kill detail levels that never get updated.\n\nUpdate #3 never. They are point in time documents, \"this is why we did that thing 3 years ago\". Now you know why you thought you were doing it and if you're assumptions were wrong so it's easier to rip it out and try the simpler thing you should have done instead.\n\n  \nI would not use AI Agents to write these docs. Generally there isn't enough data made available to them to understand what maters most in your environment. People will already read only about half of what you document, so keep it as focused and as contextual to what matters in your environment as possible. Anything else just waters down the percentage of attention people will actually spend on it. Do consider using AI Agents to review #1 and #2 for gaps or drift as a faster way to detect that updates have been missed.",
          "score": 1,
          "created_utc": "2026-01-12 01:19:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz2xe9b",
          "author": "Admirable_Swim_6856",
          "text": "Every new hire should be updating the docs as they learn the system. otherwise, AI is actually very good at documentation, an automation to update docs on a regular cycle would work.",
          "score": 1,
          "created_utc": "2026-01-12 01:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3089s",
          "author": "alien3d",
          "text": "When we start using jquery - When people start learn react. When we start react , when people bragging vue /selvte.  Your arc doc , can't update as the framework keep updating each time.  But you can update the data flow diagram / business requirement document (brd) or some call as product requirement document(prd)",
          "score": 1,
          "created_utc": "2026-01-12 01:36:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4hc7d",
          "author": "atika",
          "text": "If you include your docs in the repo, you can make sure in the pull request that they are updated. Otherwise the PR doesn‚Äôt get approved.",
          "score": 1,
          "created_utc": "2026-01-12 07:16:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5trl2",
          "author": "gororuns",
          "text": "IMO it's better to use RFCs and ADRs to record major architectural decisions that were made at that point in time, these don't necessarily need to be updated.",
          "score": 1,
          "created_utc": "2026-01-12 13:52:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz943hx",
          "author": "robogame_dev",
          "text": "Generate your docs using an automatic process that reads them from the code.",
          "score": 1,
          "created_utc": "2026-01-12 23:12:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzaxqtq",
          "author": "themessymiddle",
          "text": "It gets risky when code is changing and the behavior is different than what the rest of the org is referencing. But docs aren‚Äôt fun to make, automating the architecture docs as code changes definitely helps",
          "score": 1,
          "created_utc": "2026-01-13 05:20:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbkxof",
      "title": "How to setup Architecture Governance | Learnings and Practices",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qbkxof/how_to_setup_architecture_governance_learnings/",
      "author": "sahil000005",
      "created_utc": "2026-01-13 07:18:53",
      "score": 30,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "I‚Äôm an architect working on establishing an **Architecture Governance** process in my organization.\n\nI understand this is a subjective topic, but I‚Äôd love to learn how others have approached it‚Äîwhat has worked well and what hasn‚Äôt.\n\nMy primary focus is on defining **guardrails and architecture guidelines** that enable teams to work independently, with minimal involvement from architects, while still avoiding significant architectural deviations.\n\nLooking forward to hearing real-world experiences and lessons learned.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qbkxof/how_to_setup_architecture_governance_learnings/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzbssxk",
          "author": "HandsOnArch",
          "text": "Just my 2 cents based on what worked (and failed) for us:\n\n**Priority 1: Make architecture decisions explicit and visible.**  \nWe learned that if it‚Äôs not documented, it effectively doesn‚Äôt exist.  \nWe require ADR/ADL-style docs, very pragmatically in Confluence (beter would be probably Git or Jira).\n\nFor us, governance worked best when we focused first on everything that is visible from the outside:\n\n* APIs & contracts\n* Logging & monitoring standards\n* Third-party components\n* Reused / managed platform services\n* ... \n\n**Priority 2: Keep internal architecture deliberately flexible.**  \nThis was an important learning for us. Governance easily overreaches here.  \nWe had to learn to ‚Äúpick our battles‚Äù and leave teams freedom internally to keep speed and ownership high.\n\nIf I were to start again, I would probably look into Architecture as Code earlier. I personally don‚Äôt have strong tool recommendations yet, so for us it always came down to a cost/benefit trade-off: documentation in Confluence vs. technical enforcement via tooling.\n\nWhat helped us a lot in the beginning were guild / community structures. They gave us a natural place to grow standards and patterns together with the people actually doing the work. Our experience was that governance has to be built with the teams, otherwise commitment stays shallow.\n\nOne more thing we learned:  \nSomeone needs to clearly own moderation and final decisions. Without a visible decision owner, we never really converged on a shared strategy.",
          "score": 12,
          "created_utc": "2026-01-13 09:57:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbxx5x",
              "author": "sahil000005",
              "text": "Great to hear about your experience.\n\nWe already have a fairly mature product, and we‚Äôre now looking to introduce architecture governance. Do you think it‚Äôs practical to start capturing ADRs (or other architectural artifacts) for an existing system at this stage?\n\nI‚Äôm struggling to figure out how to move forward because the backlog is quite large, and it‚Äôs not clear whether we‚Äôll ever reach a point where the documentation becomes truly beneficial‚Äîgiven that a significant portion of the system would need to be captured retrospectively.\n\nWould love to hear how you approached this, or what you‚Äôd recommend in a similar situation.",
              "score": 3,
              "created_utc": "2026-01-13 10:44:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzcli2e",
                  "author": "HandsOnArch",
                  "text": "The good news is: governance introduction does not have to turn into a refactoring program.\n\nI would start with ADR- and policy-based guardrails that explicitly shape future decisions (e.g. \"for new or significantly changed components, we do X / use Y\").\n\nI would treat technical debt and distance-to-target as a separate, incremental investment topic with explicit cost/benefit decisions.\n\nFor me, those are two different problems ‚Äî and I would handle them separately.\n\nJust my personal take:\n\nI wouldn‚Äôt wait for the perfect framework, the perfect tooling or the perfect org setup. I would start with a small, accepted core of architects in the middle, give them a clear mandate to moderate and decide, and grow from there.\n\nEven a lightweight governance setup already helps:\n\n\\- to avoid building new technical debt\n\n\\- to even make technical debt visible and understandable in the first place\n\n\\- to create clarity in decisions\n\n\\- in the long run to make everyone who touches architecture stronger\n\n  \nGood luck!",
                  "score": 1,
                  "created_utc": "2026-01-13 13:36:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzcjuv6",
          "author": "violentlymickey",
          "text": "You could write some \"golden path\" documentation in a central location for system design patterns and agreed upon common approaches. The problem is enforcement, but this can be slightly mitigated with static analysis tools if you're willing to put some effort into getting those written. Best approach would be to automate as much as you can, with for instance CI jobs.\n\nYou don't want to go too heavy handed on this though. It's better to have guidelines and have teams implement them the way they see fit rather than enforce a singular style, especially if your teams work on different stacks.",
          "score": 3,
          "created_utc": "2026-01-13 13:26:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd3pln",
          "author": "GrogRedLub4242",
          "text": "write the core architecture constraints or interfaces down in a doc (possibly with diagrams.) make them avail",
          "score": 2,
          "created_utc": "2026-01-13 15:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf8ruq",
          "author": "Ok-Scientist9904",
          "text": "In addition to ADRs, I would also set up a decision matrix for teams on items that MUST come to an architect for review. Like- anytime using a new technology or open source is proposed, Integrating with third party systems etc",
          "score": 2,
          "created_utc": "2026-01-13 21:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze2kd1",
          "author": "themessymiddle",
          "text": "ADRs like others have mentioned, I also like to document principles, constraints, basically anything that would help teams make decisions. Different things work for different companies, so I‚Äôd say start lightweight with some guidance that helps in the specific areas where teams aren‚Äôt yet able to work as independently as you‚Äôd like. Good governance is easier when it‚Äôs built up over time",
          "score": 1,
          "created_utc": "2026-01-13 18:03:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzenvug",
          "author": "dudeaciously",
          "text": "We have a standard set of diagrams, UML based.  Any architecture can not be explained by a single perspective.  Some diagrams are optional.  \n\nOnce everyone is working in similar visual language, it becomes easier for the peer group to suggest clarifications and fine grained problems.\n\nArchitects don't create erroneous things.  But we sometimes don't go down good paths in favor of certain concerns.  Ignoring CI/CD, not sufficient data architecture, not sufficiently scalable, secure, etc.",
          "score": 1,
          "created_utc": "2026-01-13 19:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgcna0",
          "author": "lexseasson",
          "text": "https://medium.com/@eugeniojuanvaras You can take a walk there are some solutions",
          "score": 1,
          "created_utc": "2026-01-14 00:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhzh6c",
          "author": "aphillippe",
          "text": "The technical side of things should be a secondary concern. Primary concern should be the people and process. If peers aren‚Äôt bought in to the benefits and pay attention to decisions, if you only have carrot and no stick, if senior leadership doesn‚Äôt back you up, it is all performative",
          "score": 1,
          "created_utc": "2026-01-14 06:57:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaqpg2",
      "title": "I keep learning this in system design: one pattern alone rarely gives you a full solution.",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qaqpg2/i_keep_learning_this_in_system_design_one_pattern/",
      "author": "Icy_Screen3576",
      "created_utc": "2026-01-12 09:43:00",
      "score": 18,
      "num_comments": 6,
      "upvote_ratio": 0.79,
      "text": "I hit this again while working on a **flight search system**.\n\n[Initial State](https://preview.redd.it/p6p8jeei1wcg1.png?width=1205&format=png&auto=webp&s=54838216ce2e0a682f11b5db63475f4af3ce96cb)\n\n# The problem\n\n* Call multiple flight providers\n* Each responds at a different speed\n* Some fail\n* Users expect immediate results\n\nNo single pattern covered all of that.\n\n# What didn‚Äôt work\n\n* **Synchronous calls** ‚Üí blocked by the slowest provider\n* **Async +** `Task.WhenAll` ‚Üí still waits for everyone\n* **Background threads + polling** ‚Üí fragile under restarts and scale\n\nEach approach solved part of the problem.\n\n# What worked\n\nThe solution was possible when **combining patterns**, each covering a different concern:\n\n* **Scatter‚ÄìGather** ‚Üí parallel provider calls\n* **Publish‚ÄìSubscribe** ‚Üí decouple dispatch from providers\n* **Correlation ID** ‚Üí track one search across async boundaries\n* **Aggregator** ‚Üí merge partial responses safely\n* **Async Reply over HTTP** ‚Üí return immediately\n* **Hexagonal Architecture** ‚Üí the code structure discipline\n\nTogether, they formed a stable flow.\n\n[Request Flow](https://preview.redd.it/oyqrv50l1wcg1.png?width=4011&format=png&auto=webp&s=d3cee75dbae048339bf7d6e14fa9cea410b9f968)\n\n# User Interface\n\n[Progressive Results](https://preview.redd.it/y1ex8s1n1wcg1.png?width=1201&format=png&auto=webp&s=31a11c964a9907e717c543179aa77f056cfb4fa8)\n\nI uploaded the [code](https://github.com/justifiedcode/flight-search-system) to github for those who want to explore.\n\n‚Äî HH",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qaqpg2/i_keep_learning_this_in_system_design_one_pattern/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz9vzq7",
          "author": "halfxdeveloper",
          "text": "I wouldn‚Äôt say users want immediate results. Users want immediate feedback. Their frustration comes from not knowing if the website crashed or if there aren‚Äôt any results. I‚Äôve found skeletons aren‚Äôt enough. And lazy loading also isn‚Äôt enough when the results shuffle with new data intermittently.",
          "score": 9,
          "created_utc": "2026-01-13 01:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbtuui",
              "author": "Icy_Screen3576",
              "text": "Client polling for status with correlation id worked well.",
              "score": 3,
              "created_utc": "2026-01-13 10:07:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o029cco",
          "author": "Strange-Engine3102",
          "text": "I can understand that this is for scale but why wouldn't async and updating the frontend whenever the results come in work for smaller scale?",
          "score": 2,
          "created_utc": "2026-01-17 06:09:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03lsvo",
              "author": "Icy_Screen3576",
              "text": "It works. Just limited to vertical scaling.",
              "score": 1,
              "created_utc": "2026-01-17 13:14:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbnspu",
          "author": "saravanasai1412",
          "text": "In paper it looks good. Start engineering this for scale. What happens if 10k concurrent users accessing this system. \n\nDoes this works.",
          "score": 1,
          "created_utc": "2026-01-13 09:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbu7sv",
              "author": "Icy_Screen3576",
              "text": "Good idea to test that. It must be a more instances thing.",
              "score": 2,
              "created_utc": "2026-01-13 10:10:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qdnu1l",
      "title": "How do teams actually keep requirements stable once development starts?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qdnu1l/how_do_teams_actually_keep_requirements_stable/",
      "author": "TobyNartowski",
      "created_utc": "2026-01-15 16:16:27",
      "score": 16,
      "num_comments": 24,
      "upvote_ratio": 0.91,
      "text": "I‚Äôve run into this situation more than once and also hear similar stories from other developers, so I‚Äôm curious how common this is among more experienced teams. \n\nIn many Agile setups, it feels like work starts before things are truly nailed down. Requirements are good enough to begin, but not really complete, and then they keep evolving while implementation is already in progress. Late in the process, someone suddenly realizes there‚Äôs a missing dependency - a contract, an external system, some approval that wasn‚Äôt accounted for upfront. \n\nAt the same time, different phases blur together. Analysis isn‚Äôt really finished, but coding begins because of deadlines. Development isn‚Äôt fully reviewed yet, but testing has already started. Releases get planned while there are still known open risks. \n\nThere doesn‚Äôt seem to be a clear point where everyone agrees: ‚Äúthis is the current truth we‚Äôre working against.‚Äù \n\nWhat I‚Äôm trying to understand is how teams that work well avoid this turning into constant rework and frustration. Do you rely on explicit handoffs or contracts between roles, or some kind of commitment point before starting implementation? \n\nHow do you handle changes once work is already underway without everything becoming reactive? I‚Äôm less interested in theory or framework definitions and more in what actually works in practice.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qdnu1l/how_do_teams_actually_keep_requirements_stable/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzr279o",
          "author": "RipProfessional3375",
          "text": "You don't. Requirements shift as understanding shifts and the situation changes over time.  \nThe question is not: how to keep requirements stable?  \nThe question is: how to keep the code flexible?",
          "score": 74,
          "created_utc": "2026-01-15 16:21:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrtajk",
              "author": "Electronic_Yam_6973",
              "text": "Typically the people giving the requirements barely know what they are doing and what the really want. Or at least don‚Äôt do a great job at communicating what they mean",
              "score": 9,
              "created_utc": "2026-01-15 18:22:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzuacmd",
                  "author": "Comfortable_Ask_102",
                  "text": "This is a known feature of software development. Users **will never** **know** what they want, it's up to you (the development team) to figure it out since you're the experts in all software things. Our elders noticed this and came up with the agile principles, the relevant one for this case is: \"responding to change over following a plan.\"\n\nI believe the pragmatic programmers came up with the analogy of *digging* for requirements, rather than *gathering* requirements. Gathering implies the requirements are there, perfectly clear in the user's mind for you to pick them with your hand and take them to your shop. The reality is that requirements are buried under layers of assumptions, incomplete information and cargo cult. Users will tell you what they *think they want*, it's up to you to figure what they *actually need*.",
                  "score": 6,
                  "created_utc": "2026-01-16 01:40:00",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzryntb",
                  "author": "RipProfessional3375",
                  "text": "Yep, and only as the project takes shape will they realize it more and more, this is normal.",
                  "score": 7,
                  "created_utc": "2026-01-15 18:46:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzs4se8",
              "author": "TobyNartowski",
              "text": "Totally agree that requirements will always change.  \nWhat I keep running into is a slightly different pain: assumptions change late, but there‚Äôs no clear trace that the delivery conditions actually changed. When the deadline slips, it just looks like the team didn‚Äôt deliver - even though they were working against a moving target. Or worse, people start hacking things together last-minute just to hit the date, fully aware that code quality is taking a hit.",
              "score": 2,
              "created_utc": "2026-01-15 19:13:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzsdzg9",
                  "author": "RipProfessional3375",
                  "text": "Then the problem is feedback cycle and scope of delivery.\n\nIn other words, show what you have once a week, and don't have more in a deployment than what you can show in 15-30 minutes.",
                  "score": 7,
                  "created_utc": "2026-01-15 19:55:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzs8zrd",
              "author": "One-History-1783",
              "text": "And anticipate the futurn shifting. ( In code and with the project )",
              "score": 1,
              "created_utc": "2026-01-15 19:32:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzr3iyw",
          "author": "NoForm5443",
          "text": "My cheat has always been to build \\*simple\\* software \\*fast\\*. Requirements don't usually change within a month.",
          "score": 7,
          "created_utc": "2026-01-15 16:26:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzr8ww7",
              "author": "cstopher89",
              "text": "Yep same here. Build the MVP based on what we know now and iterate as we learn more.",
              "score": 3,
              "created_utc": "2026-01-15 16:51:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuozyn",
          "author": "Effective-Total-2312",
          "text": "Most companies \"adopted scrum\" or other \"agile frameworks\" but they still want the deadlines from waterfall. Models like scrum are not designed for rigid deadlines, they're designed for uncertainty, which means no specific date as to when development is complete.\n\nSo, what can you do ? My guess is to just surf the wave as best as you can, try to deliver the best possible software, don't allow management to harrass you, and try to negotiate deadlines if needed and possible. Otherwise, change companies.",
          "score": 4,
          "created_utc": "2026-01-16 03:01:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr3asg",
          "author": "Masked_Solopreneur",
          "text": "The goal is to create value, not build software according to specs.¬†\nFind out where the risks are highest, be it technical or other product risks, and try to understand those parts first or make them flexible.¬†\nIf some parts require heavy lifting to change, communicate this to other parts of the org and seek ways to test things with low investments.¬†",
          "score": 5,
          "created_utc": "2026-01-15 16:25:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrkbre",
          "author": "SeriousDabbler",
          "text": "Yeah the design process asks us to immerse ourselves in thr problem domain and understand the interactions. It may be tempting to expect that you can get a group of passionate and clever individuals together to solve a problem and they'll agree on a solution. They won't.\n\nAt best your requirements will be a compromise between what the individual stakeholders want and what's practical given the technology choices\n\nDuring design, analysis, development, testing, and support after you ship you'll detect requirements that you couldn't have predicted. That's ok\n\nBy all means try to confirm your requirements ahead of time to reduce rework and costs but feedback is part of the process\n\nIt doesn't mean you're stupid or wrong. It's just the process.\n\nThis was something I've had to wrestle with as an architect. Many times the architecture is wrong in ways that we only detect later. I used to give myself a hard time about it but now I accept it as part of the discipline",
          "score": 3,
          "created_utc": "2026-01-15 17:42:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzr9wrm",
          "author": "Glove_Witty",
          "text": "You need good product management and you need to release as small units of features as possible. \n\nIf you have a set of big features that roll into a major version release then agile (in the way you guys seem to be practicing it) is going to have the exact problems that you are seeing.",
          "score": 2,
          "created_utc": "2026-01-15 16:55:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzri7ch",
          "author": "two-point-zero",
          "text": "Should Agile promote the deliverable of something valuable each iteration? If a task is worth to be done for the value it can give NOW, even if not spec complete, it could be started in the next Sprint, and refined later right?\n\nOf course it should be up to the product owner to give priorities and to decide that there are enough information to start develop a feature even if incomplete; and to the team to give higher story points to uncomplete/unclear features,so they can eventually be postponed to a better time in the future when more infos will be available.",
          "score": 2,
          "created_utc": "2026-01-15 17:33:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzri7ri",
          "author": "OhMyGodItsEverywhere",
          "text": "I develop agreements with whoever I am delivering to (PM, customer, etc.) on what it looks like and what it means to agree on requirements and deadlines.\n\nIn essence: \"These are the requirements for a given deadline. Changes to the requirements after sign-off will incur a cost: push the deadline, add a payment, remove other requirements, or stick to the agreement. If you agree, sign here.\"\n\nYou can only estimate based on the conditions that you're given. When new conditions come in, estimates have to change.\n\nThis isn't so much a software architecture thing, more of a contractual or organizational thing. But in terms of software architecture, you'll want to design your systems to be be quickly modifiable and verifiable in anticipation of regular requirement changes: good version control, small PRs, readable code, single-responsibility modules, automated tests and builds, smooth continuous integration, and reviews to stick to these principles.",
          "score": 1,
          "created_utc": "2026-01-15 17:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01u1e0",
              "author": "Mysterious-Rent7233",
              "text": "It's deeply harmful to the project when you are in that kind of adversarial relationship. It is better to both have no deadlines and also be flexible to changes in requirements. Rather than using one as a hostage to avoid changes to the other.",
              "score": 1,
              "created_utc": "2026-01-17 04:15:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzrnd3k",
          "author": "aWesterner014",
          "text": "The trick is differentiating between an evolving understanding of requirements and scope creep.",
          "score": 1,
          "created_utc": "2026-01-15 17:56:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzsz8gc",
          "author": "admiral_nivak",
          "text": "Have a good product team who understand what you need to build as the MVP or feature, build, enhance, rinse repeat.\n\nIf you are running out of time cut scope, long hours are only used as a last resort for prod issues or contractual deadlines.",
          "score": 1,
          "created_utc": "2026-01-15 21:34:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztdj6s",
          "author": "severoon",
          "text": "When you start a new project, the most important thing is to prioritize work on the **core** functionality.\n\n other words, don't try to choose the requirements you want to target and treat them as stable, no, that's the wrong way around. Instead, try to choose the most stable requirements, and cut, cut, cut the list down to the absolute, bare bones minimum. This is known as defining the MIP, the minimum implementable product.\n\nFor example, if you are a bank, you have customers and those customers have certain characteristics in your system: a name, an address, a username, a DOB, etc. They have that stuff today, they will have that stuff tomorrow, and they will have that stuff 50 years from now. These customers have checking and savings accounts. Those are products your bank offers today, tomorrow, and will likely offer 50 years from now. Etc.\n\nBuild the most stable and durable stuff first. The most stable aspects of a system are also the things that are the most solid foundation for your system. IOW, these things can support lots of incoming dependency. Conversely, you should be careful to not build dependency in the other direction, where highly stable things depend upon things you add later which are not so stable over time. Also, you should make sure the most stable and depended-upon elements of your system are the most *dependable*‚Äîthat is, they are the most well-tested, documented, etc.\n\nThe only way this can go wrong is if your org decides they want to be a completely different business. If you are a bank and you build a bunch of bank stuff and then the board of directors says, hey, we decided we're going to be a movie theater company instead, you're kind of screwed.\n\nAnother milestone of such a project is the MVP, the minimum viable product. This is the most stripped-down version of your system that actually does something useful for customers. This, again, should be defined by cutting requirements down as much as possible, meaning that it may only be useful to your friendliest alpha customers, whom you've reached out to and gotten on board to be early adopters willing to use and give feedback on your limited alpha release.\n\nWhen you first sit down to design your new project, you start by blue skying all of the cool stuff you want it to eventually do, then you ruthlessly cut it down to the smallest possible thing you can implement, then on top of that highly stable, highly dependable backbone you implement the requirements that is the smallest possible thing you can roll out to actual customers, and you do that.\n\nThe next step from here is to start adding requirements that will bring in the largest number of additional customers for the least amount of effort. If you add a few more reqs, maybe you can add another 10% of your customer base to your alpha. Great, do that. This is how you continue building milestones until you have a beta that serves 75‚Äí90% of your customer base. Once you've run the beta with a significant customer base long enough to know it's stable, you start migrating customers to the beta and stress test it, it passes, you bring it out of beta, and then at some point you EOL the existing product and force-migrate your customers to the new thing. Along the way you will undoubtedly find that the long tail of customers you were supporting before will never migrate because you can't meet their requirements, and they're just not worth it unless those are on your roadmap somehow anyway.\n\nYour approach to this in your questions isn't quite right:\n\n>In many Agile setups, it feels like work starts before things are truly nailed down. Requirements are good enough to begin, but not really complete, and then they keep evolving while implementation is already in progress.\n\nWhen you say \"requirements\" you are not being specific enough. The reqs for the MIP should *absolutely* be nailed down and carved in stone. The only way this isn't the case is if the org doesn't know what business it's in, because that's how you chose these requirements for the MIP. Either that or, much more likely, this isn't how those requirements were chosen and you're not working on an MIP.\n\nIn that case, yea, if you don't approach a large, complex project in a disciplined way, life sucks. (But, then again, what do you expect? Doing things wrong is unnecessarily hard and frustrating.)\n\nI think a lot of times management looks at processes like Agile or other methodologies as a substitution for doing all of the hard planning work I describe above. They are not. Whatever methodology you choose, that's just a set of tools for execution and tracking of the necessary planning work. But none of these are going to save you if you're just doing the wrong things in the wrong order. Perfect tracking of a requirement that was inserted way too early in the project plan as it whips everyone around isn't a solution to any kind of problem, it's an indicator that there *is* a problem that someone needs to step in and solve by moving planning around. (And this kind of stuff will happen, and the methodologies do help identify them.)\n\nThe point of these methodologies is communication. Once a bunch of smart people have gotten into a room and come up with a plan, how do you convey that plan and map it down to specific daily tasks that everyone can execute? That's the big where Agile or whatever can shine.\n\nI often see another mistake that orgs make along the same lines, which is not respecting [Conway's law](https://en.wikipedia.org/wiki/Conway%27s_law). I've been on teams where management is told about Conway's law and they respond by saying, oh, this is great, now that I know about it, I can break it and, because we're aware we're breaking it, we can navigate around the potential pitfalls. This way, I get the org chart I want to manage, and we can have the technical architecture we want to produce, and those two things can stay decoupled.\n\nThey all learn in the end that, no, they're not decoupled, and that doesn't doesn't. You either build your architecture around your team structure, or you structure your teams around your architecture, and you have to choose. There is no third path.",
          "score": 1,
          "created_utc": "2026-01-15 22:42:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztwmtf",
          "author": "PhaseMatch",
          "text": "The key concepts in agile software development are \n\n\\- make change cheap, easy, fast and safe (no new defects)  \n\\- get fast feedback on the (user) value that change created\n\nWe use valuable, working software as a probe to uncover what the user actually needs;  while this is less efficient (in terms of delivery) you don't create anything that is not valuable, and you create it in value order.\n\nThis was central to Extreme Programming (XP), which is broadly what \"agile\" meant prior to about 2010, when Scrum starts to take off as a (Google trends) term.\n\nThe XP \"planning game\" dealt with how to address this in terms of delivery, along with having an onsite customer who dynamically collaborated with the team in place of fixed, upfront requirements.\n\n\\- Jeff Patton's stuff (User Story Mapping) deals with this approach   \n\\- Alistair Cockburn's \"Elephant Carpaccio\" exercise for developers is an example\n\nWhat works in practice is XP;  it was created by developers, for developers, as an integrated approach.\n\nGetting your codebase to the point where you can do this is half the battle.",
          "score": 1,
          "created_utc": "2026-01-16 00:23:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzy2q16",
          "author": "Constant_Physics8504",
          "text": "Design for the system, then design for the subsystem, then design for the teams. Once that traceability is done, you can say oh when I change X this is what is impacted, and redesign if necessary",
          "score": 1,
          "created_utc": "2026-01-16 16:32:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzmvx3",
          "author": "steve-7890",
          "text": "The answer was already posted here, so I will just propose a good book that can help you understand why it's the way it is: *Rapid Development*, Steve McConnell",
          "score": 1,
          "created_utc": "2026-01-16 20:45:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01tmjc",
          "author": "Mysterious-Rent7233",
          "text": ">In many Agile setups, it feels like work starts before things are truly nailed down. Requirements are good enough to begin, but not really complete, and then they keep evolving while implementation is already in progress. Late in the process, someone suddenly realizes there‚Äôs a missing dependency - a contract, an external system, some approval that wasn‚Äôt accounted for upfront.\n\nWelcome to the real world. It's our job to deal with it, not to fight it. You can of course expend SOME effort to get requirements up front, but complaining that many arrive late is like complaining about rain on your beach day.\n\n>At the same time, different phases blur together. Analysis isn‚Äôt really finished, but coding begins because of deadlines. Development isn‚Äôt fully reviewed yet, but testing has already started. Releases get planned while there are still known open risks.\n\nYou mean that the [waterfall does not act as a proper waterfall](https://en.wikipedia.org/wiki/Waterfall_model)? This was first noted around 1970.",
          "score": 1,
          "created_utc": "2026-01-17 04:12:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04kyzm",
          "author": "reijndael",
          "text": "Since few comments here give any practical advice‚Ä¶ True skill is in finding code abstractions that don‚Äôt need to be rewritten or piled on top of every time there‚Äôs a request for a change. Use practices like modular code, TDD, small classes, SOLID, event sourcing. All of these and other practices exist precisely to mitigate the inherent chaotic nature of the job. The industry operates more like a research project with ups and downs rather than a factory where you have a clear pipeline of what needs to happen when.",
          "score": 1,
          "created_utc": "2026-01-17 16:20:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qeonrb",
      "title": "350PB, Millions of Events, One System: Inside Uber‚Äôs Cross-Region Data Lake and Disaster Recovery",
      "subreddit": "softwarearchitecture",
      "url": "https://www.infoq.com/news/2026/01/uber-hivesync-data-lake/",
      "author": "rgancarz",
      "created_utc": "2026-01-16 18:52:04",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qeonrb/350pb_millions_of_events_one_system_inside_ubers/",
      "domain": "infoq.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qe60h4",
      "title": "How do big systems handle this?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qe60h4/how_do_big_systems_handle_this/",
      "author": "After_Ad139",
      "created_utc": "2026-01-16 04:16:25",
      "score": 15,
      "num_comments": 16,
      "upvote_ratio": 0.89,
      "text": "How you‚Äôd handle a traffic spike. You confidently say, ‚Äúrate limiting‚Äù and start sketching Redis and token buckets. ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qe60h4/how_do_big_systems_handle_this/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzv3ni2",
          "author": "markojov78",
          "text": "Horizontal scaling, load balancing.\n\nIt's a simple phrase but the whole system has to be designed to be able to do that...",
          "score": 15,
          "created_utc": "2026-01-16 04:29:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvqfyp",
          "author": "naedyr000",
          "text": "Big systems are complex, so there's no simple answer.\n\nBut you can only be sure it can handle load that you've actually seen. Anything else is just theoretical.\n\nSo you generate load or respond when there's a spike, and fix the issues that you see. Metrics and observability are key.\n\nThen use auto scaling etc to scale down from that high load state. I always think of it as scaling down to save money, rather than scaling up to handle load.",
          "score": 9,
          "created_utc": "2026-01-16 07:19:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv28ts",
          "author": "no1SomeGuy",
          "text": "Global CDN, Scale Out, Load Balancing, Elastic Services.",
          "score": 10,
          "created_utc": "2026-01-16 04:20:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwm6c2",
          "author": "dashingThroughSnow12",
          "text": "It depends the requirements.\n\nAre we talking 5% spikes? 20%? 100%? 10000% (ex we are running a Super Bowl ad)? Are we talking across the board (ex opening up a new feature) spike or some bad actors?",
          "score": 1,
          "created_utc": "2026-01-16 11:58:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx7imo",
          "author": "Wh00ster",
          "text": "How fast does it need to respond?\n\nWould it take too long for autoscaling? Can you predict spikes and pre-allocate more servers?",
          "score": 1,
          "created_utc": "2026-01-16 14:07:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzydj9g",
          "author": "Mundane_Cell_6673",
          "text": "Systems have auto scaling based policies on CPU usage or other key indicators (request count, messages in queue)\n.if large traffic increase is instantaneous then you either reject requests (load shedding) or be slightly overprovisioned",
          "score": 1,
          "created_utc": "2026-01-16 17:20:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv3qm5",
          "author": "saravanasai1412",
          "text": "Just configure auto scale on your instance or just add load balancer.",
          "score": 0,
          "created_utc": "2026-01-16 04:29:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvm84x",
              "author": "Ok-Macaron-3844",
              "text": "‚Ä¶ only to find out app servers were not the bottleneck, but your database is ü´£",
              "score": 6,
              "created_utc": "2026-01-16 06:43:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzvrvo8",
                  "author": "klekmek",
                  "text": "Time to shard",
                  "score": 5,
                  "created_utc": "2026-01-16 07:31:50",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nzvq9zt",
                  "author": "saravanasai1412",
                  "text": "Do database base replication and setup read and right paths simple and straight forward.\n\nAdd proxy like pg bouncer which helps your with connection handling.",
                  "score": 2,
                  "created_utc": "2026-01-16 07:17:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzvi32b",
              "author": "symbiat0",
              "text": "This üëÜüèΩ",
              "score": 1,
              "created_utc": "2026-01-16 06:10:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzw6zom",
                  "author": "serverhorror",
                  "text": "Now your relational database becomes a bottleneck to the horizontally scaling Webservers \n\nWhat now?",
                  "score": 1,
                  "created_utc": "2026-01-16 09:50:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qdkpoi",
      "title": "I created a C4 model authoring tool using Python called buildzr",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qdkpoi/i_created_a_c4_model_authoring_tool_using_python/",
      "author": "scribe-kiddie",
      "created_utc": "2026-01-15 14:18:00",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hello, fellow software architects!\n\nLast year, I started writing a Python C4 model authoring tool, and today it has come to a point where I feel good enough to share it with you guys so you can start playing around with it locally and render the C4 model views with PlantUML.\n\nUnder the hood, it follows Structurizr's schema (see [https://github.com/structurizr/json](https://github.com/structurizr/json) ) when storing the model in-memory and when writing it into a JSON file. So it is also compatible with any Structurizr-compatible rendering tool.\n\nYou can find out more about it in [https://buildzr.dev](https://buildzr.dev)\n\n# Quick Example\n\nHere's an example code straight from the README (I use image because Reddit doesn't support syntax highlighting -- if you want to copy, head out to [https://buildzr.dev](https://buildzr.dev) ).\n\n[Creating a workshop, and defining the models and their relationships.](https://preview.redd.it/q6u11qeipidg1.png?width=1600&format=png&auto=webp&s=59cd8c21adbf2dde5d212e95d782788ca3001bb5)\n\n[Next, we create two standard structurizr views: a SystemContextView and a ContainerView](https://preview.redd.it/oc2n9gnruidg1.png?width=1600&format=png&auto=webp&s=db276a5ecfb2cbc446e9c7ae201572c1dbfbab42)\n\n[You can \\`import\\` themes \\(icons and\\/or colors\\) and apply it to styles.](https://preview.redd.it/dya5hqe4widg1.png?width=1600&format=png&auto=webp&s=32418939a7983cf344334e0aca0dadae2f1f8732)\n\n[Finally, we can export the workspace to JSON; or, to PlantUML, or SVG to be rendered later.](https://preview.redd.it/nlybgr5eqidg1.png?width=1600&format=png&auto=webp&s=d6ff42dba15ea76995b93d667fd2e359c4fe8bc1)\n\n[Bonus: Mypy will complain about illegal relationships!](https://preview.redd.it/zhwxm1yrqidg1.png?width=1594&format=png&auto=webp&s=0f07772a5f82d0d857b2f385b3c7b268b87bc94d)\n\n# Works in Jupyter Notebook\n\nYou can also render the model in Jupyter Notebook, which I think will be useful for iteratively working on the models and views. Below is the screenshot from VS Code:\n\nhttps://preview.redd.it/1jetaw77tidg1.png?width=1600&format=png&auto=webp&s=a67d78cf6e80f06c4dc9512a81f375e89b285e0c\n\n# Features\n\n* ***Intuitive Pythonic Syntax***: Use Python's context managers (`with` statements) to create nested structures that naturally mirror your architecture's hierarchy.\n* ***Programmatic Creation***: Use buildzr's DSL APIs to programmatically create C4 model architecture diagrams. Great for automation!\n* ***Advanced Styling***: Style elements beyond just tags --- target by direct reference, type, group membership, or custom predicates for fine-grained visual control. Just take a look at [Styles](https://buildzr.dev/user-guide/styles/)!\n* ***Cloud Provider Themes***: Add AWS, Azure, Google Cloud, Kubernetes, and Oracle Cloud icons to your diagrams with IDE-discoverable constants. No more memorizing tag strings! See [Themes](https://buildzr.dev/user-guide/themes/).\n* ***Type Safety***: Write Structurizr diagrams more securely with extensive type hints and Mypy support.\n* ***Standards Compliant***: Stays true to the Structurizr JSON schema standards. buildzr uses datamodel-code-generator to automatically generate the low-level representation of the Workspace model.\n* ***Rich Toolchain***: Uses the familiar Python programming language and its rich toolchains to write software architecture models and diagrams!\n\n# Find out more\n\nThanks for reading this far!\n\nIf you're interested, feel free to ask me any questions about the project.\n\nGitHub repo: [https://github.com/amirulmenjeni/buildzr](https://github.com/amirulmenjeni/buildzr)\n\nDocumentation here: [https://buildzr.dev](https://buildzr.dev)",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qdkpoi/i_created_a_c4_model_authoring_tool_using_python/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qbbmre",
      "title": "Advice Regarding Databases?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qbbmre/advice_regarding_databases/",
      "author": "Adventurous_Rough792",
      "created_utc": "2026-01-12 23:55:23",
      "score": 11,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "At work I'm developing an internal CRM. I'm using Vue js for the front end and Laravel for the REST API. This CRM has a multitenant structure, so I have a master database and then each user group has its own dedicated database. So far so good. \n\nMy manager told me to use Mongo DB to save the Activity logs and everything related to tasks. He said that MySQL doesn't maintain such a large amount of data and therefore it crashes. \n\nSo now I find myself managing tasks on one side and users on the other. \n\nDo you think this is a good approach? \n\nOr is there a better solution? \n\nHave you had experience with hybrid databases?\n\nThanks for your time",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qbbmre/advice_regarding_databases/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz9gn0i",
          "author": "Leonobrien",
          "text": "I think your manager would be surprised at how far MySQL can go when the structure is deliberate. It has been rock solid for 30yrs.\n\nI would first ask how these logs are intended to be used (what are the use cases). Is it a regulatory requirement, internal reporting etc. this will influence design rationale. \n\nI would then ask how long the activity logs need to be retained for, what volumes are you expect, what growth and how critical they are to the service you are providing. This will influence scale and service need in the design. Simple things as table truncation are easy and effective where long term retention isn't needed. For long time series storage solutions (an option in Postgres) you can even down sample data points over time.\n\nFinally, consider the ongoing operational support. Sure it's easy to spin up a DB service in most cloud environments, but you still need internal support knowledge and experience for the life of the solution. This needs to be factored in.\n\nWith that I would then do some rough options analysis. Why a NoSQL like MongoDB and not Postgres, or logstash, or even using Grafana and graphite.  Understand and discuss the trade offs and then make the decision. Otherwise it's personal opinion that increases code complexity.",
          "score": 4,
          "created_utc": "2026-01-13 00:20:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzawct1",
          "author": "as5777",
          "text": "Why do you build a crm in 2026 ?!",
          "score": 5,
          "created_utc": "2026-01-13 05:11:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfmhwc",
              "author": "Adventurous_Rough792",
              "text": "because they asked to do it hahha",
              "score": 1,
              "created_utc": "2026-01-13 22:20:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzi8b3k",
                  "author": "as5777",
                  "text": "Is it core business ?",
                  "score": 1,
                  "created_utc": "2026-01-14 08:18:56",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o05h3z2",
                  "author": "InternationalEnd8934",
                  "text": "this is dumb. next they will ask you to roll their own encryption",
                  "score": 0,
                  "created_utc": "2026-01-17 18:49:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzayyj1",
          "author": "Voss00",
          "text": "At work we have Many mysql tables with billions of rows, some even tens of billions (34billion or something is the largest)\n\nStoring this much in the way we do it isn't ideal, but definitely possibly, it just depends on usecase and query patterns.",
          "score": 3,
          "created_utc": "2026-01-13 05:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzaz75t",
          "author": "alien3d",
          "text": "noob manager .MyIsam for log because 0 tranasaction",
          "score": 3,
          "created_utc": "2026-01-13 05:31:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9q1n6",
          "author": "Doctuh",
          "text": "[Just Use Postgres](https://www.manning.com/books/just-use-postgres)",
          "score": 2,
          "created_utc": "2026-01-13 01:10:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza5u3i",
          "author": "Aggressive_Ad_5454",
          "text": "Hmm. YouTube relies on MySql. So does Facebook. So does WordPress, globally. It‚Äôs possible your manager is ignorant.",
          "score": 1,
          "created_utc": "2026-01-13 02:36:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nza96d4",
              "author": "dariusbiggs",
              "text": "No one in their right mind uses MySQL for anything.\n\nBut then we voluntarily work with computers for a living so we cannot possibly be sane.",
              "score": 1,
              "created_utc": "2026-01-13 02:54:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzav8z4",
              "author": "symbiat0",
              "text": "Quite likely actually.",
              "score": 1,
              "created_utc": "2026-01-13 05:03:19",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzbhwkb",
              "author": "Qinistral",
              "text": "That‚Äôs misleading. Sure there maybe some minor portions of YouTube that use mySQL but the core functionality uses Google DBs like spammer and big table.\n\nHowever OP obviously doesn‚Äôt need Google scale.",
              "score": 1,
              "created_utc": "2026-01-13 08:11:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzatj7d",
          "author": "Glove_Witty",
          "text": "Welcome to microservices.",
          "score": 1,
          "created_utc": "2026-01-13 04:51:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbnhp3",
          "author": "saravanasai1412",
          "text": "From solution architect point of view there is no answer for these type of questions. The most generic answer is It depends. \n\nAsk questions like why mongo DB why not MySQL. There is no argument that mongo db scale and MySQL won‚Äôt.  Choosing mongo DB only for scale it not right. \n\nAsk him the access pattens and how data is used. My opinion on current approach would introduce more complexity in operations. \n\nIf devs are not used to mongo db it‚Äôs a learning curve & maintenance over head. \n\nIf it‚Äôs disposable logs. Just use SQL database and delete after retention needs simple. I have seen 5 millions rows in table with 140 columns sql will fly. \n\nTake decision based on your needs team capacity not just because it scales.",
          "score": 1,
          "created_utc": "2026-01-13 09:05:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdtvxm",
          "author": "frankwiles",
          "text": "My advice would be to to use PostgreSQL as it scales a touch better (not that MySQL can‚Äôt do what you need) but it has conditional indexes, partitioned tables, and good JSON support all of which you will find useful on a project like this.",
          "score": 1,
          "created_utc": "2026-01-13 17:24:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzf7q1s",
          "author": "Ok-Scientist9904",
          "text": "i think the right fit here depends on what is the purpose of this activity log. Audit, compliance, regulatory etc. Do you need a front end that pulls up the activity log or is this activity log fed into an observability platform? From my experience MongoDB is good for fast reads, however its not meant for heavy simultaneous writes and can cause issues during heavy load.",
          "score": 1,
          "created_utc": "2026-01-13 21:11:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfvwvo",
          "author": "HosseinKakavand",
          "text": "Honestly MySQL can handle a lot more than people think ‚Äî the \"it crashes with large data\" thing is usually about missing indexes or bad queries, not the database itself. That said, sharded setups work great if you're disciplined. You just need to be careful to keep them in sync, which is where patterns like Saga can be helpful. It can also be tricky spinning up new DBs on the fly, and requires infra automation. We're discussing these types of multi-step transactional workflows in [here](https://www.reddit.com/r/luthersystems/).",
          "score": 1,
          "created_utc": "2026-01-13 23:07:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qarbmy",
      "title": "Builder pattern helped clean up messy constructors in my project",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qarbmy/builder_pattern_helped_clean_up_messy/",
      "author": "Possible_Design6714",
      "created_utc": "2026-01-12 10:21:26",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 0.81,
      "text": "I was working on a part of my system where objects had tons of optional values and a few required ones. I ended up with this giant constructor that was super unreadable and hard to maintain.\n\nSwitched to the *Builder pattern* and wow - the code became way easier to follow: you can chain only the relevant setters and then call `build()` at the end. No more overloads with 7‚Äì8 parameters, half of which are null half the time.\n\n**Why it helped me:**\n\n* Step-by-step object setup feels more natural.\n* Tests are clearer because it‚Äôs obvious what fields you‚Äôre setting.\n* Reduces subtle bugs from bad constructor calls.\n\nHas anyone else found design patterns like this helpful in real apps? And do you tend to apply them consciously or just recognize them after they appear in your code?\n\nThoughts? üëá\n\n*Edit:* I‚Äôm using TS/Node, but I know this pattern is classic OOP. Seems like even in modern languages we unknowingly implement similar patterns under the hood. ([Reddit](https://www.reddit.com/r/node/comments/141zf64/how_often_do_you_use_design_patterns_in_your_code/?utm_source=chatgpt.com))\n\nCheckout the full story here: [https://chiristo.dev/blogs/my-tech-pills/series/design-patterns/builder-pattern](https://chiristo.dev/blogs/my-tech-pills/series/design-patterns/builder-pattern)",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qarbmy/builder_pattern_helped_clean_up_messy/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz51ojv",
          "author": "flavius-as",
          "text": "Next step: get rid of that temporal coupling by modelling a state machine with the type system of the language.",
          "score": 7,
          "created_utc": "2026-01-12 10:28:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzd7z4z",
              "author": "Ok_Zookeepergame1290",
              "text": "Dude, this is the thing that I eventually realized how important this is. Basically 80% of every saas logic are state machines and workflows around them",
              "score": 2,
              "created_utc": "2026-01-13 15:31:50",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nze19mt",
                  "author": "flavius-as",
                  "text": "So, you ready to renounce your love for The Setter?",
                  "score": 2,
                  "created_utc": "2026-01-13 17:58:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzryesz",
          "author": "Expensive_Garden2993",
          "text": "How is this:\n\n      const booking = new RoomBookingBuilder(customer, room, stay)\n        .withPromoCode(\"SUMMER25\")\n        .withSpecialRequests(\"Room with ocean view\")\n        .withPaymentMethod(...)\n        .build();\n\nBetter than this?\n\n    const booking = new RoomBooking({\n      promoCode: \"SUMMER25\",\n      specialRequests: \"Room with ocean view\",\n      paymentMethod: ...,\n    })\n    \n\n  \nYou can do step by step as well:\n\n    // step 1: get promo code\n    const promoCode = getPromoCode()\n    // step 2: get something else\n    const somethingElse = getIt()\n    // build:\n    const booking = new RoomBooking({ ... })",
          "score": 1,
          "created_utc": "2026-01-15 18:45:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz69cbs",
          "author": "svhelloworld",
          "text": "We use builders all over our test suite. We can pre-populate the builder state with default values and then chain calls to just change the state needed for that test. Removes all sorts of low-grade testing friction.",
          "score": 1,
          "created_utc": "2026-01-12 15:14:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qa8d5d",
      "title": "From Cloudflare Zero-trust to Tailscale",
      "subreddit": "softwarearchitecture",
      "url": "https://blog.frankel.ch/cloudflare-zero-trust-tailscale/",
      "author": "nfrankel",
      "created_utc": "2026-01-11 19:22:44",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qa8d5d/from_cloudflare_zerotrust_to_tailscale/",
      "domain": "blog.frankel.ch",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qcxc4n",
      "title": "Help regarding a production-ready security architecture for a Java microservices application using Keycloak",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qcxc4n/help_regarding_a_productionready_security/",
      "author": "Gold_Opportunity8042",
      "created_utc": "2026-01-14 19:47:53",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I am building a microservices-based application that consists of multiple services (service-1, service-2, service-3, etc.), an API Gateway, and a Service Registry. For security, I am using¬†**Keycloak**.\n\nHowever, I am currently a bit confused about the overall security architecture. I have listed my questions below, and I would really appreciate it if you could share your expertise.\n\n1. From my understanding of the Keycloak architecture: when a client hits our signup or login endpoint, the request should be redirected to Keycloak. After that, everything is handled by Keycloak, which then returns a JWT token that is used to access all protected endpoints. Does this mean that we do¬†**not**¬†need to implement our own signup/login endpoints in our system at all?\n2. If my understanding of Keycloak is correct, how can I manage different roles for different user types (for example, Customer and Admin)? I ll have two different endpoints for registering customers and admins, but I am unable to figure out how role assignment and role mapping should work in this case.\n3. Should I use the API Gateway as a single point where¬†**authentication, authorization, and routing**¬†are all handled, leaving the downstream services without any security checks? Or should the API Gateway handle authentication and authorization, while each individual service still has its own security layer to validate the JWT token? what is the standard way for this?\n4. Are there any other important aspects I should consider while designing the security architecture that I might be missing right now?\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qcxc4n/help_regarding_a_productionready_security/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzlroen",
          "author": "Embarrassed-Chain265",
          "text": "0. Maintaining your own keycloak instance is a PITA, you may be better off with a cloud provider in terms of $ and time wasted keeping your keycloak install up to date\n1. Yep, although you will need your own frontend sign up/login pages potentially if you don't use the keycloak defaults\n2. Just add the roles in the keycloak management app (that you will have to host yourself). Or you can add them via API calls from your own services\n3. Authorization should be based on the roles in the JWT inside your app and services, routing in the gateway, and authentication could happen in either\n4. See 0",
          "score": 2,
          "created_utc": "2026-01-14 20:28:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzrgqnn",
          "author": "Glove_Witty",
          "text": "1. There is an extra step in the standard authentication flow. Keycloak gives you an id token and you go back to keycloak for an access token. I forget now how much of this the keycloak api hides but it is important to know there are 2 tokens. The access token lets you access resources. \n\nRegarding the actual question - in the standard oicd flow the idp (keycloak) displays the login an captures the clients credentials - you never see them. You can style the keycloak login box. You want this because it is how single sign in federation works. On a mobile device it is a little ugly but most people use an embedded web page. \n\n2. Your access token has claims that represent what the user can do. You can add your own claims and have keycloak add them to the tokens. How you use the claims depends on how you have designed authorization in your app. \n\n3. Yes. Best practice is to check authorization at every layer. Have an api gateway and authorize there. This is good because you can reject illegitimate traffic there and not let it inside the system. Also check the jwt and claims in your services. Checking the token is lightweight - just a signature check and it is built into the web frameworks of most stacks. \n\n4. If you haven‚Äôt look up the Google beyond prod document. This has a lot of security architecture guidance. Also, get a thorough understanding of oicd flows.",
          "score": 1,
          "created_utc": "2026-01-15 17:26:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc2r07",
      "title": "How do you guys implement a centralized parameter manager for customization in a multi-tenant architecture?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qc2r07/how_do_you_guys_implement_a_centralized_parameter/",
      "author": "Dev_loper031",
      "created_utc": "2026-01-13 20:43:10",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "I'm implementing a modular system based on microservices that will be deployed to multiple environments (tenants), since every client will provide its own infrastructure. In this scenario, i see a need to implement a centralized parameter manager to distribute these variables across the system. How do you usually implement that? Perhaps a simple REST API, or is there a open-source framework you recommend?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qc2r07/how_do_you_guys_implement_a_centralized_parameter/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzfb720",
          "author": "CzyDePL",
          "text": "I don't think you are describing multi-tenancy - it describes when you have multiple clients on a single deployment and need to manage their isolation",
          "score": 4,
          "created_utc": "2026-01-13 21:27:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjukp2",
          "author": "anon10101111",
          "text": "For distributed configuration you may use [https://zookeeper.apache.org/](https://zookeeper.apache.org/)",
          "score": 2,
          "created_utc": "2026-01-14 15:16:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o00rfv1",
          "author": "analcocoacream",
          "text": "Doesn‚Äôt configmap or env vars solve the issue?",
          "score": 1,
          "created_utc": "2026-01-17 00:11:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03o9gb",
          "author": "RipNTear666",
          "text": "You can have a config service to store tenant-specific data",
          "score": 1,
          "created_utc": "2026-01-17 13:30:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03v99n",
          "author": "Ok_Swordfish_7676",
          "text": "multi tenant architecture share the same environment  and db ( isolated by its tenant id )\n\nunless one tenant have a special request, that requires full isolation",
          "score": 1,
          "created_utc": "2026-01-17 14:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07fa5u",
          "author": "virtualstaticvoid",
          "text": "If the services are running on AWS, you could use the [Parameter Store](https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html), or an equivalent service on Azure, Google Cloud, etc, instead of building something yourself.",
          "score": 1,
          "created_utc": "2026-01-18 00:44:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qasu1q",
      "title": "Domain-Composed Models (DCM): a pragmatic middle ground between Active Record and Clean DDD",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qasu1q/domaincomposed_models_dcm_a_pragmatic_middle/",
      "author": "senhaj_h",
      "created_utc": "2026-01-12 11:48:55",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I wrote an article exploring a pattern we converged on in practice when Active Record became too coupled, but repository-heavy Clean DDD felt like unnecessary ceremony for the problem at hand.\n\nThe idea is to keep domain behavior close to ORM-backed models, while expressing business rules in infra-agnostic mixins that depend on explicit behavioral contracts (hooks). The concrete model implements those hooks using persistence concerns.\n\nIt‚Äôs not a replacement for DDD, and not a defense of Active Record either ‚Äî more an attempt to formalize a pragmatic middle ground that many teams seem to arrive at organically.\n\nThe article uses a simple hotel booking example (Python / SQLAlchemy), discusses trade-offs limits of the pattern, and explains where other approaches fit better.\n\nArticle: [https://medium.com/@hamza-senhajirhazi/domain-composed-models-dcm-a-pragmatic-middle-ground-between-active-record-and-clean-ddd-e44172a58246](https://medium.com/@hamza-senhajirhazi/domain-composed-models-dcm-a-pragmatic-middle-ground-between-active-record-and-clean-ddd-e44172a58246)\n\nI‚Äôd be genuinely interested in counter-examples or critiques‚Äîespecially from people who‚Äôve applied DDD in production systems.",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qasu1q/domaincomposed_models_dcm_a_pragmatic_middle/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz5wur5",
          "author": "mexicocitibluez",
          "text": "I read the article and maybe I'm missing something (which is usually the case). Where's the boilerplate in:\n\n1. Loading aggregate from ORM (or repository)\n2. Apply work to that aggregate (behavior encapsulated in aggregate)\n3. Saving\n\nIf what you're after is the ability to coordinate changes between aggregates, then use a saga or state machine. \n\nMaybe it's a language thing, because trying to implement this pattern in C# would result in a much, much more complex app.",
          "score": 3,
          "created_utc": "2026-01-12 14:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbzbhr",
              "author": "senhaj_h",
              "text": "Hello Thank you for your substentive question, i'll try to answer according to what i'm undertanding : \n\nThe loading,Saving aggregate has been omitted on purpose since it wasn't the focus, they are considered given, and they are considered a happy path.\n\nThe pattern i'm describing, it's more when app grows or might grow in complexity then you find yourself implementing DDD either wrong, or too soon, here i'm trying to avoid active record, and in the same time lighten a full pure DDD implementation \n\nThe friction I‚Äôve seen in practice tends to appear one level *around* a happy path flow, especially in:\n\n* mapping ORM entities ‚Üî domain objects\n* maintaining parallel query/read services for simple aggregate-local facts\n* carrying repositories whose only responsibility is pass-through plus conversion\n\nWhen persistence is relational and stable, that extra indirection often doesn‚Äôt buy much *yet*, even though it becomes valuable later as complexity grows.\n\nFor C# i'm not familiar enough with to be able to confirm wether or not it's language thing",
              "score": 2,
              "created_utc": "2026-01-13 10:56:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz5r519",
          "author": "CzyDePL",
          "text": "I'll take a look - although I would be more interested in how to create domain models in Django and make models at least a little bit encapsulated",
          "score": 1,
          "created_utc": "2026-01-12 13:37:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qb1jlv",
      "title": "Backend Crud Arch",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qb1jlv/backend_crud_arch/",
      "author": "Double_Ad3148",
      "created_utc": "2026-01-12 17:41:32",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.82,
      "text": "Hi everyone. I‚Äôm a junior  developer, currently working alone on a fairly large project. I want to keep the codebase clean, consistent, and built with a solid architecture.\n\nI have a few architectural questions and would really appreciate feedback from more experienced developers.\n\n# 1) Entity / DTO / Response and services\n\nAt the moment, I have many endpoints, and as a result my service layer contains a large number of different DTOs and response classes. This makes the code harder to read and maintain.\n\nI‚Äôve considered several approaches:\n\n* Making services return¬†**one common DTO**, and mapping it to specific response objects in the controller\n* Or returning¬†**entities directly from services**, and doing the mapping to response objects in controllers (with response classes located near controllers)\n\nThe problem is that when working with entities, unnecessary relations are often fetched, which increases database load‚Äîespecially if I always return a single ‚Äúlarge‚Äù DTO.  \nAt the same time, according to best practices, services are usually not supposed to return entities directly.\n\nBut what if services always return entities, and mapping is done only in controllers?  \nHow bad (or acceptable) is this approach in real-world projects?\n\nWhich approach is generally considered more correct in production systems?\n\n# 2) Complex business logic and use cases\n\nI‚Äôve been reading books about DDD and Clean Code and tried to reduce the size of my services:\n\n* Part of the business logic was moved into entities\n* Services now look more like use-case scenarios\n\nHowever, some use cases are still quite complex.\n\nFor example:\n\n* There is¬†`UserService.create()`¬†which saves a user\n* After that, an email might be sent, related entities might be created, or other services might be called\n\nCurrently, this is implemented using domain events:\n\n    publisher.publish(new UserCreatedEvent(user));\n    \n\nThe downside is that when you open the service code, it‚Äôs not always clear¬†**what actually happens**, unless you inspect all the event listeners.\n\nSo I‚Äôm considering another approach:\n\n* `UserService`¬†‚Äî only CRUD operations and repository access\n* `UserUseCaseService`¬†‚Äî orchestration of complex business scenarios\n\nExample:\n\n    userService.create(user);\n    \n    mailService.sendEmail(user.getEmail());\n    userApplicationService.create(user);\n    \n\nThe questions are:\n\n* Is this approach over-engineered?\n* Is it acceptable in production to introduce a separate ‚Äúuse-case‚Äù layer for complex operations?\n\nI‚Äôd really appreciate any advice and real-world examples from your experience üôå",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qb1jlv/backend_crud_arch/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz79pgv",
          "author": "Expensive_Garden2993",
          "text": "1. Returning entities from services makes sense, at least I never heard it to be a bad practice.\n\n\n2. It is natural to start using \"use cases\" for orchestration once your services begin to look messy. Just that \"UserUseCaseService\" isn't a proper name for it, use case is a single use case like \"RegisterUserUseCase\".",
          "score": 6,
          "created_utc": "2026-01-12 18:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7qexo",
          "author": "CzyDePL",
          "text": "2) that's the tradeoff between choreography (what I presume you are referring to as current approach in your system - event is published and it's \"handled somewhere\", not knowing where and how) and orchestration (central place coordinating the steps of the process - knows what are possible events and how to handle them - it can be invoking different services/use cases)",
          "score": 3,
          "created_utc": "2026-01-12 19:17:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd52g1",
          "author": "sharpcoder29",
          "text": "Don't do one common dto, too much coupling. Instead, in the places you have over fetching, if it's a read only endpoint, consider a read only query model. This is CQRS. I.e. users/search would just return a UserSearchDto that has id, name. And you have a separate namespace that is ReadModel.Users with a UserQueries class that does what you need. You can use your same ORM in there (different context), or something like Dapper, call a sproc, whatever. Bonus points for using a read-only connection string",
          "score": 3,
          "created_utc": "2026-01-13 15:17:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzcop2v",
          "author": "tr14l",
          "text": "A controller generally just kicks off a process in a handler. Its job is translating the endpoint to the domain and then back to the response. There shouldn't be much logic there as it is only that translation point. Very similar to port interfaces in hexagonal architecture. In fact, you could argue that it's a driving port. \n\nHandlers are the domain orchestrators that stitch together complicated domain logic and calls to driving ports (if that is your architecture and for most web apps I do suggest it, regardless of stack)",
          "score": 2,
          "created_utc": "2026-01-13 13:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz7f2fs",
          "author": "never-starting-over",
          "text": "RemindMe! 1 day",
          "score": 1,
          "created_utc": "2026-01-12 18:26:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz7f8qe",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-13 18:26:07 UTC**](http://www.wolframalpha.com/input/?i=2026-01-13%2018:26:07%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/softwarearchitecture/comments/1qb1jlv/backend_crud_arch/nz7f2fs/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fsoftwarearchitecture%2Fcomments%2F1qb1jlv%2Fbackend_crud_arch%2Fnz7f2fs%2F%5D%0A%0ARemindMe%21%202026-01-13%2018%3A26%3A07%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qb1jlv)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-12 18:26:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o026kbp",
          "author": "Strange-Engine3102",
          "text": "1)Directly returning entities for small queries makes sense but try to have a dto for larger and complicated entities. Dtos are mainly to decouple and you would have to rewrite everything again if you consider adding some sort of caching layer in the future which you likely will.\n\n2)The trade off is fine debugging choreography events would be harder but just limit it to a few services and try to ensure which service sent the event in the header or in the message if debugging is a headache.",
          "score": 1,
          "created_utc": "2026-01-17 05:47:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qduq1l",
      "title": "Design problem: grouping raw punch events into overlapping shifts",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qduq1l/design_problem_grouping_raw_punch_events_into/",
      "author": "MERAKtaneous",
      "created_utc": "2026-01-15 20:25:05",
      "score": 5,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hey everyone, I‚Äôm running into a time-based data processing problem and would love some design advice.\nI have two modules: one imports raw punch events from biometric machines (just employee ID + timestamp), and the other lets me define shifts. Using these, I try to figure out which shift an employee worked, whether they were late, overtime, etc. Day shifts work perfectly fine, but night shifts and overlapping shifts are causing issues.\nShifts are very flexible: some start early, others late, many cross midnight, and some overlap. Because of this, grouping punches by calendar day doesn‚Äôt work. Processing is done by a scheduled job that must run at a specific time. The problem is that at that moment, some shifts are still in progress while others are starting, which leads to incomplete or incorrect grouping‚Äîfor example, a punch during a night shift might be interpreted as a full shift or a very short one.\nI‚Äôm looking for a general approach to assign raw timestamped events to shifts when shifts can overlap or be incomplete at processing time. Any patterns, strategies, or best practices would be super helpful.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qduq1l/design_problem_grouping_raw_punch_events_into/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzslu2v",
          "author": "RipProfessional3375",
          "text": "Do you have a form of storage? You will need to carry active shifts over between two sessions.\n\n\\- Only make decisions on closed work periods (2 punch events, 1 start and 1 end)  \n\\- Only make decisions on ended shifts (planned end time is before now)\n\nCarry the others over into the next job.",
          "score": 3,
          "created_utc": "2026-01-15 20:32:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuwo3z",
          "author": "RobSterling",
          "text": "This gets a lot easier with punch types attached to a time punch (i.e. shift start/end, break start/end, lunch start/end, etc.)\n\nAssuming you have a user‚Äôs intention (start/end) it‚Äôs pretty easy to identify missing punches and create pairings. This concept also relates to scheduled shifts that have start/end times.\n\nYou can look for overlapping time spans to match time punch pairs with shifts and use minute thresholds for identifying early/late punches.\n\nGenerally, to solve overnight shifts, you should always include the prior day‚Äôs punches and shifts to ensure nothing was left out. You may be looking at a Monday-Sunday calendar but care about data Sunday-Sunday for example.\n\nAt some point labor laws can feed into your rules and you can make assumptions about missed punches if they‚Äôre further apart than a state‚Äôs mandatory break period (but understand that varies from state to state and even city to city in some places so it‚Äôs best to let users configure these rules for themselves).\n\nYou should be in a good position storing raw punch data, creating a higher level construct like punch pairs (being able to identify when one is missing), then finally matching that against your shift data using overlapping time ranges.\n\nKnowing that certain punch pairs are contained or required helps suggest further missing data (for example: a pair of break punches suggests you may be missing the shift punch pair or uninterrupted shifts over n-hours may be missing breaks.",
          "score": 1,
          "created_utc": "2026-01-16 03:45:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzzpy35",
          "author": "dbrownems",
          "text": "If you have overlapping shifts, how can you tell if a punch is for the shift that just started, or is late for a previous shift?  Normally an employee is scheduled for a shift ahead of time.\n\nSo, you would have a list of scheduled shifts for each employee, then for each punch, apply rules to associate it with a scheduled shift, then mark/emit each punch in/out as \"unscheduled\", \"early\", \"on-time\", \"late\".",
          "score": 1,
          "created_utc": "2026-01-16 21:00:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbdobi",
      "title": "Cron Vs Queues",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qbdobi/cron_vs_queues/",
      "author": "fborgesss",
      "created_utc": "2026-01-13 01:21:13",
      "score": 5,
      "num_comments": 22,
      "upvote_ratio": 0.78,
      "text": "If I hypothetically had a cron job processing 500k users (batched ***for*** statement), and sometimes my instance runs out of memory and dies: does that justify the complexity of implementing queue solutions like SQS or RabbitMQ? What's the right approach here?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qbdobi/cron_vs_queues/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nz9tfnt",
          "author": "Buttleston",
          "text": "I would start with \"why does it run out of memory\" and see if you can fix that, before doing anything else.",
          "score": 24,
          "created_utc": "2026-01-13 01:29:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9uagu",
              "author": "fborgesss",
              "text": "sound advice and it's probably possible. but it's only one of the problems. I'm trying to understand what's the industry standard for that amount of operations in a cron job. ~~and I'm tired of talking to an LLM~~",
              "score": 1,
              "created_utc": "2026-01-13 01:34:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz9uoru",
                  "author": "Buttleston",
                  "text": "There's no such standard\n\nAt best the only question is \"does it finish before the next cron job runs\", or if you want it faster than that, \"does it finish fast enough\"\n\nA queue becomes relatively required if you want to split the work up between workers, but is not required for one worker.  After all you could just record the last batch number/identifier you finished and if it dies, restart from there.",
                  "score": 12,
                  "created_utc": "2026-01-13 01:36:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz9tsge",
          "author": "anotherchrisbaker",
          "text": "This is a great use-case for a queue. Automatic retries with exponential back off. Optional dead letter queue for persistent failures. You can control how much concurrency you want, or just do them one by one.",
          "score": 11,
          "created_utc": "2026-01-13 01:31:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9vnvb",
              "author": "general_dispondency",
              "text": "I'm trying to figure out how queues are more complex than batch processing? ... OP could create a queue db table and have consumers without introducing any new tech. Scaling, failures, processing and monitoring are so much easier with queues are so much easier to deal with vs processing the world in a single run.",
              "score": 5,
              "created_utc": "2026-01-13 01:42:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz9u95p",
          "author": "scissor_rock_paper",
          "text": "I think it depends on how quickly you want to recover from the failure. Cron usually runs on a schedule so you won't recover until the next interval. With a queue, you could add a task/message for each user or batches of users. Then if a task fails you could retry just that task. Queues are also great for increasing parallelism as you can process multiple tasks concurrently across multiple workers.\n\nIf you have 500k users you likely have more places where a queue could be useful.",
          "score": 6,
          "created_utc": "2026-01-13 01:34:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza8nby",
          "author": "bustedmagnet",
          "text": "We had the same problem at my job with large batch jobs running out of memory. The solution was to use Spring Batch because the memory footprint is limited to the size of the chunk being processed. The usage of queues might only work if the batch is all or nothing. But yeah, it would depend a lot on why the memory is running out.",
          "score": 2,
          "created_utc": "2026-01-13 02:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nza9rqa",
          "author": "quincycs",
          "text": "I built a queue that‚Äôs only processed by a cron so there you go.  üôÇ ‚Äî don‚Äôt pick one or the other, do both. \n\nEvery five minutes a cron triggered program sends a http request to process a filling up Postgres table.  Grabs X rows and processes them either one by one or via concurrency of Y.   I define X and Y via the http request defined by the cron. \n\nIf it goes too slow then I adjust X and Y , but also I have the option to scale up instances so that the cron http is load balanced across those instances.",
          "score": 2,
          "created_utc": "2026-01-13 02:57:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz9wzrc",
          "author": "AdrianHBlack",
          "text": "I would take a look at libraries for job processing in your programming language/framework. For instance in Elixir we have Oban that uses PostgreSQL to do it, but in ruby there is sidekick (i think?), etc \n\nThat might be useful and less operational burden than RabbitMQ and other ¬´¬†real queue¬†¬ª",
          "score": 1,
          "created_utc": "2026-01-13 01:49:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzainvt",
              "author": "midasgoldentouch",
              "text": "The most popular Ruby gem is Sidekiq but pretty much every option for job processing is going to use queues.  You‚Äôd usually use the whenever gem to implement the cron scheduling for whatever your background jobs are - it‚Äôs pretty much always built on top of that.",
              "score": 1,
              "created_utc": "2026-01-13 03:45:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nza4hmn",
          "author": "Electrical_Effort291",
          "text": "All you really need is persistence (so you don‚Äôt lose info on the 500k users) and a checkpoint mechanism (so you know how far you are down the list). You can do this with a queue (like Kafka) or a database (MySQL), or if you have access to a reliable disk, roll your own with files. Any of these is ok depending on the reliability needs of your situation",
          "score": 1,
          "created_utc": "2026-01-13 02:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzak8ph",
          "author": "PaulPhxAz",
          "text": "I would try to chunk users.  Like, do 50k, clean up, do the next 50k, clean up memory, etc.",
          "score": 1,
          "created_utc": "2026-01-13 03:54:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzauc6e",
          "author": "Glove_Witty",
          "text": "You could always checkpoint your batch job so you\nCan pick up from where it crashed.",
          "score": 1,
          "created_utc": "2026-01-13 04:57:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzawdw9",
          "author": "Admirable_Swim_6856",
          "text": "Yes, a queue job for each batch would be a good idea.",
          "score": 1,
          "created_utc": "2026-01-13 05:11:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbeu2q",
          "author": "VerboseGuy",
          "text": "First thing to do: reduce memory usage of your app. Maybe your app loads all 500 employees into memory.\n\nNext thing: horizontal scaling",
          "score": 1,
          "created_utc": "2026-01-13 07:42:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbodar",
          "author": "Isogash",
          "text": "You shouldn't be running out of memory even if it's only a single instance if your batched for loop is correctly cleaning up after each batch. Are there any queries you're using that might be pulling in more data than you need? E.g. a query that pulls the whole dataset down instead of just the batched portion.\n\nYou can also use persistent batch processing, where you just record how far you are through the operation in the database, or create a batch queue table in your database.",
          "score": 1,
          "created_utc": "2026-01-13 09:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbpgno",
          "author": "BalthazarBulldozer",
          "text": "I usually let crons pick up work and then assign them to queues for long running or large batches. That's always worked great for me",
          "score": 1,
          "created_utc": "2026-01-13 09:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbvo5t",
          "author": "edgmnt_net",
          "text": "Why is this a cron job and not some sort of maintenance periodic task within your application? A queue server and distributed processing might be overkill for this. Doing this natively, in-process simplifies some things.",
          "score": 1,
          "created_utc": "2026-01-13 10:23:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfugoj",
          "author": "HosseinKakavand",
          "text": "Yeah 500k users dying mid-batch is rough. Queues like SQS definitely help because you get retry logic and can process smaller chunks without blowing up memory. The tradeoff is complexity ‚Äî suddenly you're managing dead letter queues, visibility timeouts, etc. I've been working on a [project](https://www.reddit.com/r/luthersystem) which handles this kind of thing with built-in backpressure and transactional guarantees, so if a batch fails it picks up where it left off.",
          "score": 1,
          "created_utc": "2026-01-13 23:00:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzhdr8v",
          "author": "DallasActual",
          "text": "I'd cut over to queued design when there were more than 10 operations to track.\n\n  \n500,000? With error handling, observability, and scaling considerations? Using cron for that would be insanity.",
          "score": 1,
          "created_utc": "2026-01-14 04:12:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzlsdnt",
          "author": "InternationalEnd8934",
          "text": "I don't much see the point of using cronjobs. Time is a shitty orchestrator unless it's a I need one backup a day kind of thing",
          "score": 1,
          "created_utc": "2026-01-14 20:32:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzsonii",
          "author": "RipProfessional3375",
          "text": "Depends.  \nA queue is communication + (temporary) persistence.\n\nYou need a form of persistence to prevent your interrupted batch job from being interrupted halfway without a way of knowing what you did and what you didn't do.\n\nIf you can read this information from your data destination, you don't need a queue. Your application can read the destination to figure out where to pick back up.\n\nIf you can't, a queue will do, but so will any form of persistence.\n\nIf you are both the writer and the reader of a queue,  it is literally just ordered, temporary storage in a fancy jacket.",
          "score": 1,
          "created_utc": "2026-01-15 20:45:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qc6z5o",
      "title": "Advice for a fluent graph builder",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qc6z5o/advice_for_a_fluent_graph_builder/",
      "author": "atomicplebeian",
      "created_utc": "2026-01-13 23:25:16",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hello, I'm very new to graphs and relatively new to design patterns so hopefully this is not a dumb question. I'm trying to figure out the best approach for a graph builder, with syntax like\n\n    const graph = new GraphBuilder().directed().weighted().build();\n    // builder also has bipartite(), cyclic() etc\n    \n    then I can graph.addNodes(nodeData).addEdges(edgeData);\n    // and graph.depthFirstSearch(...), graph.nearestNeighbours(...) etc\n    \n\nFirst question, does this approach make sense or will I run into major issues? And for the graph class itself, how should I go about implementing its functions? Would it make sense for the builder to build up one strategy and the graph executes like:\n\n    // class Graph ...\n    \n    addNodes(...) {\n       this.strategy.addNodes()\n    }\n\nor am I going down a dark path/ there is a better way\n\nOverall rationale for the approach is to avoid having to implement each combination of graph type   \n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qc6z5o/advice_for_a_fluent_graph_builder/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "nzgzazm",
          "author": "ArtSpeaker",
          "text": "As an indirect answer, this is a fun challenge! So many choices to make  \n1 - The more you want your graph to be able to do, the less it does.  \n2 - the properties of the graph are VERY tied to the properties of the edges and nodes, so inherritance or not, there's some. hard choices to make to keep the data you provide and the methods you operate with, compatible.  \n3 - data storage vs representation. Once you have a happy in-ram solution, I'd recommend you think about how this best works when you have thousands of nodes and edges.\n\nIf you aren't pressed for time, or have to turn this in for an assignment or whatever, just... pick one. Pick a strategy, and then focus on the tests -- the tests where you pretend to use different graphs for different purposes withe the same \"super\" handling it all. That should give you a lot of \"ah-hah\" behind what you like and what you don't. Maybe it's efficient but the API gets super clunky? Maybe the API is smooth but it just... can't do certain things even though it looks like it should?\n\nAt the end of the day, any ugliness on the inside, that makes it beautiful on the outside, is worthwhile. Graphs are to be used, after all.\n\nHighly recommend.",
          "score": 2,
          "created_utc": "2026-01-14 02:45:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfhkdq",
      "title": "How solve business cyclic dependency between module ?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qfhkdq/how_solve_business_cyclic_dependency_between/",
      "author": "Ok-Professor-9441",
      "created_utc": "2026-01-17 16:25:57",
      "score": 4,
      "num_comments": 26,
      "upvote_ratio": 0.71,
      "text": "Hi\n\nWe want to decompose the app in severals domain, one domain will be transalted to a Java Module (Spring Modulith)\n\n**Business rules requiring cross-domain coordination**\n\n* When creating a new Order, we must update the related Article status to \"sell\".\n* When an Article price changes, we must update all Orders that are not validated yet with the new price.\n\n**Problem**\n\nThe domains Order and Article are both large and contain many business rules. Some rules must update state across modules, which seems to introduce a cycle:\n\n* Order needs to call/update Article\n* Article needs to call/update Order\n\nWith Spring Modulith module rules, this becomes:\n\n    order -> article\n    article -> order\n\n‚Ä¶which is a cyclic dependency and fails the *no cycle violation rule*.\n\n\n\n[Module Article depends on Order, module Order depends on Article](https://preview.redd.it/oenqt8ngrxdg1.png?width=1066&format=png&auto=webp&s=1b76d7607e15e3b72312fb9347634b6fc5400728)\n\n**Questions**\n\n* I s a cyclic dependency acceptable between module?\n* If cycles are discouraged, what is the recommended way to model this kind of cross-domain business logic while keeping modules independent?\n\n**What we considered**\n\n1. Allow cycles and disable Spring Modulith checks This works, but defeats the purpose of enforcing module boundaries.\n2. Put `Order` and `Article` in the same module Works, but we are afraid the result will become one big module, which we want to avoid.\n3. Add an orchestration module Example: `sales-orchestration` depends on both order and article But then we expect other domain pairs to have similar cross-domain rules (document <-> client, etc.), so we don‚Äôt know:\n   * how many orchestration modules are needed\n   * how to prevent orchestration from becoming a ‚Äúgod module‚Äù",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qfhkdq/how_solve_business_cyclic_dependency_between/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o04par6",
          "author": "never-starting-over",
          "text": "Hmm. I find it curious that an order can change the price of an article.\n\nNonetheless, perhaps you need an ephemeral representation of an Article that represents what the Article was at the time it was created. In a similar accounting/sales domain I worked we called this a Line Item, belonging to an Invoice (akin to your Order).\n\nThis would allow you to:\n\n1. Keep the actual billable separate from the billable reference used for other new Orders\n2. Have distinct logic for how updates cascade, or don't cascade, to and from Articles in Orders (line items). Maybe you can eliminate cascading from here to Articles altogether even, eliminating the cyclical dependency.\n  - On the topic of eliminating cyclical dependency with this, is it key that Orders really can change the original Article reference (price)? I'm interested to know more about the use-case if you don't think this approach works for your domain.\n\nI'd put this in the Order domain.\n\nThoughts?",
          "score": 2,
          "created_utc": "2026-01-17 16:40:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04x85m",
              "author": "Ok-Professor-9441",
              "text": "The order don't change the article price. I think UML diagram isn't clear.\nIt's `OrderProvider` that call `OrderUseCase.updateOrderWithNewPrice()`\n\nThe Business rules are :\n- When creating a new Order, we must update the related Article status to \"sell\".\n\n```\nOrderUseCase.createOrder() {\n    // 1. create the order and persist\n\n    // 2. Call article markSell\n    articleProvider.markSell()\n}\n```\n\n- When an Article price changes, we must update all Orders that are not validated yet with the new price.\n```\nArticleUseCase.updatePrice() {\n    // 1. findById(articleId), set the new price and persist\n\n    // 2. Call document to update all document not valedated yet\n    documentProvider.updateArticlePrice()\n}\n```",
              "score": 1,
              "created_utc": "2026-01-17 17:17:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o052eqf",
          "author": "thiem3",
          "text": "Didn't read all the way through, sorry, but when you mentioned something about an aggregate updatong across several modules, I was sceptical. This video might interest you, about how an aggregate is split and represented across modules.\n\nhttps://youtu.be/hev65ozmYPI?si=Ag5vno1lWEaszTIQ",
          "score": 2,
          "created_utc": "2026-01-17 17:41:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o04tatn",
          "author": "Wesd1n",
          "text": "To me it sounds like orders update a status on the article.¬†\n\n\nI don't see why it is orders responsibility to update article.¬†\n\n\nOrders deals with orders and nothing else. Otherwise I might as well be one domain module.¬†\n\n\nIf they are truly distinct then the caller of orders should update the article.\n\n\nCould be via events or a two calls async calls that you can easily undo incase of failure.\n\n\n\n\nTo me the proposed thoughts do present a non negotiable cyclic coupling which I would try to avoid.\n\n\n\n\nOn the other hand article owns the product who has been ordered.¬†\nSo the order system could call for an updated price upon viewing or further action.\nThen we avoid having to update duplicate data.\nSure there will be a few calls but caching it for a moment based on what is written shouldn't be a problem.¬†\n\n\nThen orders will always know if there is an issue an you don't have to worry about the same stale data.",
          "score": 1,
          "created_utc": "2026-01-17 16:59:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04y1aw",
              "author": "Ok-Professor-9441",
              "text": "> Could be via events or a two calls async calls that you can easily undo incase of failure. \n\nAgree with you about event or async call but we want ACID transaction guaranted. So do it, in case of event or async call we must hendle distributed transaction and want to avoid it.\n\nIt's why we prefer synchronous calling and a \"simple modular architecture\".\n\n> I don't see why it is orders responsibility to update article.  \n\nSo, who is responsible to update it ?",
              "score": 2,
              "created_utc": "2026-01-17 17:21:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o05k383",
                  "author": "Freed4ever",
                  "text": "I don't understand all the domain logic (too lazy to think it all through üòä) but could be an inventory manager or something like that, after an order is placed, it places/delegates an event to a queue / async call and the inventory manager would update the article, fulfilment, notification to the customer, etc. - just off the top of my head, but you get the idea.",
                  "score": 1,
                  "created_utc": "2026-01-17 19:03:29",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o05kzvq",
                  "author": "Freed4ever",
                  "text": "Btw, why do you guys think Acid is required? Is it really true that failure to update the article would result in an order failure? If so, I would re-think the business process to be frank. Once a customer placed an order, and payment got processed, the order should be considered valid, what happen downstream (failure to update inventory, etc) shouldnt fail the order.",
                  "score": 1,
                  "created_utc": "2026-01-17 19:07:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o05lkeh",
          "author": "GrogRedLub4242",
          "text": "bad English makes this painful to read and harder to understand",
          "score": 1,
          "created_utc": "2026-01-17 19:10:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0679b8",
          "author": "ings0c",
          "text": "Can you give more context? What is an ‚Äúarticle‚Äù - it can have many meanings.\n\nWhat are users ordering?\n\nI think being less abstract will make it easier for everyone to understand.",
          "score": 1,
          "created_utc": "2026-01-17 20:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o06aoki",
          "author": "severoon",
          "text": ">Is a cyclic dependency acceptable between module?\n\nNo.\n\n>If cycles are discouraged, what is the recommended way to model this kind of cross-domain business logic while keeping modules independent?\n\nModule dependencies must form a DAG. The DAG can have one or more \"root\" modules which are completely independent, and these modules should define entities and possibly behaviors of the business that are the most stable, i.e., unlikely to change over versions and time.\n\nIn this case, it's possible to have articles in stock but no orders, whereas it is not possible to have orders if you have no articles for sale. The existence of orders depends upon the prior existence of articles, so the direction of dependency in your model should reflect that by having Order depend upon Article.\n\nThe simple way to do this is to invert the reverse dependency (Article -> Order). There are several ways to do this.\n\nThe simplest and most straightforward way is to define the Order interface in the Article module. In this approach, since Order interface belongs to the Article module, when the Article class depends upon that interface there's no issue. Then in the Order module, OrderImpl depends on the Order interface, which is the Order -> Article module dependency.\n\nHowever, you may not want the Order interface to belong to the Article module because this means that Article is responsible for defining what an Order is, and when that definition changes you have to deploy the Article module (and, of course, all dependent modules). This may or may not make sense for your app.\n\nMore likely, nothing in the Article module needs to know or care about anything to do with orders, and so the Order interface shouldn't live in that module. In this case, you need to understand what information they both share. For example, when an article changes price, orders need that updated info, which implies that there needs to be a shared price map.\n\nOne way to do this is to simply have something in the Article module update the price map and then notify listeners of the change. When an order is created, it registers a listener and will receive the updated price map whenever it changes, once the order is validated, it deregisters its listener. Maybe the price map is large and that's a lot of data flying around when only one price changes. In that case, you can publish just the articles that are updated instead of the entire price map.\n\nThis approach can be fragile, though, depending on what your deployment looks like. If these modules are guaranteed to be deployed in the same runtime, it's probably not a big deal, but one point of modularizing is being able to scale different parts of the system independently, so you shouldn't assume these will always and forever be deployed together. Better would be to assume they could be placed on different servers someday, but that means you now need something durable like a reliable database queue. You need monitoring that will notify and page when the queue gets backed up, etc.\n\nAnother solution is to simply update the price map in the DB. When an order is created, it reads all of the prices and treats them as provisional, and then if there are any price changes at validation, it processes those however it needs to. This could be simple, just updating the number, or complicated, like splitting the order into two and validating the sub-order with items that didn't change, then round tripping the sub-order containing updated prices with the customer. The customer might want to put through the sub-order with unchanged prices right away, or hold off and deal with the updated items so the system can merge the two back together into a single order or whatever.\n\nIf this happened here, though, it seems likely that a proper solution is going to necessitate fixing a bunch of other issues that come up. Such is the nature of spaghetti dependencies. If you run into that situation trying to fix this problem, that would be a red flag that you need to step back and treat this as a separate project unto itself. You will likely need to get your arms around the current out-of-control dependency structure before you can even contemplate fixing individual use cases because they will continue to pivot through the current dependency structure. (That is the problem with losing control of deps in the first place.)",
          "score": 1,
          "created_utc": "2026-01-17 21:16:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o06k89f",
              "author": "Ok-Professor-9441",
              "text": "You have summarised all the assumptions perfectly. Thank you for taking the time to explain everything in detail!\n\n**Solution 1 : Dependency Inversion** \n\nAgree with you and you identify the problem correctly\n\n>you may not want the Order interface to belong to the Article module because this means that Article is responsible for defining what an Order is\n\n**Solution 2 ‚Äî Shared pricing data / contract** \n\nThen, when you say\n\n>For example, when an article changes price, orders need that updated info, which implies that there needs to be a shared price map.\n\nI‚Äôm trying to understand whether you recommend:\n\n* a small shared ‚Äúpricing contracts‚Äù module (pure interfaces / DTOs),\n* or an event-driven approach (publish price updates / subscribe),\n* or something closer to an orchestration module.\n\n>  If these modules are guaranteed to be deployed in the same runtime, it's probably not a big deal,\n\nYes, modular monolithic application\n\n**Solution 3 : Use DB directly** \nI'll think about it.",
              "score": 1,
              "created_utc": "2026-01-17 22:04:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0757xb",
                  "author": "severoon",
                  "text": "Which of the approaches I would recommend from your bulleted list depends upon the architecture, scaling considerations, infrastructure you already have (or could have) available to you, and many other things.\n\nWhen I step back and think about this, it makes sense to me that the price list of articles already has to exist in the database anyway. That being the case, the Order module could simply look it up as I explain in the last bit of my post above. However, you are not jumping at this solution, which leads me to think that these modules currently use different databases. IOW, this is a microservice architecture.\n\nIf that's so, there's your problem. The microservice architecture is based on the notion that each team gets to own one or more small stovepipes that sit atop their own little data store, and the one and only dependency each of these microservices ever has to worry about is its own API at the top of its stack. The point of this approach is to adhere to Conway's law, giving each team total control over its own little slice of the universe and the ability to entirely and explicitly define the contract they agree to support. When one team needs something from another team, there's no choice but to negotiate and settle the requested API, so nothing flies under the radar.\n\nMost orgs that try this approach don't fully commit to it, though. At some point, instead of hitting the User service every time you want user data, other microservices just start caching that user info in their own data source. This works well enough until users get removed and the cached services continue serving that user's data. This is inconvenient, and then because of GDPR, it becomes illegal. Oops!\n\nBut they still don't want to hit the User service API, so instead why not just put a pub/sub queue at the bottom of the User service so when users are removed, anyone who cares gets notified? Sounds good, except there's no way to control who subscribes to this queue, and for what purpose. So now the queue needs to become a fully fledged queue service with access controls, etc, etc. And BTW, oops, now the User service is supporting the API at the top of its stack as well as the format of the User object it puts on the queue, and since it has no idea why others are consuming that data and what they're using it for, it can only grow in backwards compatible ways over time but nothing can be removed.\n\nBefore this got out of control, though, this queue solved this problem so well why not do the same thing in other places? So queues proliferate, and now all services are supporting APIs at the top and bottom of their stacks. (It's easier to get data from a queue rather than via an API, so that's how a lot of functionality gets implemented. So much for explicit contracts and negotiations!) No one really knows what clients are consuming what data for what purpose, so this microservices architecture has basically just become a mechanism for uncontrolled dependency to proliferate through the entire system. At this point, many orgs give up and just start giving services direct access to each other's data sources, or worse, they start allowing mid-stack cross-talk, and things have effectively become a poorly architected monolith at this point.\n\nThis is why most orgs find that adopting a microservice architecture turns out to be nothing more than a years-long way to accumulate tech debt.",
                  "score": 1,
                  "created_utc": "2026-01-17 23:51:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o089zj8",
          "author": "Isogash",
          "text": "You're probably splitting things up wrong if you're having to do a lot of cross system updating like that.\n\nFor a sales and order service, you want the service to hold a representation of the items it's selling e.g. an SKU, price and either stock counters or access to a stock-keeping service that it can reserve stock from. The service then produces events when anything of interest happens. These events can be both specific to an order, but also to an item, in which case the Article service can listen for orders of that item being fulfilled.\n\nThe key here is that the service has some abstract concept of the current things it can sell and what price it can sell them at, it's not *just* order entities, it's all of the order-relevant components and behaviours of all of your entities.",
          "score": 1,
          "created_utc": "2026-01-18 03:30:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09gy1y",
          "author": "flavius-as",
          "text": "You don't split by aggregate root, but by use case.\n\nThen aggregate roots will emerge within each module.\n\nYou might end up with duplicate Article classes, which vary wildly (because very different use cases).\n\nYour real dependency is a data dependency: only one module must be allowed to modify a data field.\n\nAlternate methodology: you map all use cases to all fields and characterize the relationship with: read, write, decision (use case uses data field as a decision variable), then you cut through this dependency graph in subgraphs in order to minimize dependencies.",
          "score": 1,
          "created_utc": "2026-01-18 09:02:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09s6p9",
              "author": "Ok-Professor-9441",
              "text": ">  Your real dependency is a data dependency: only one module must be allowed to modify a data field. \n\nWe want module specialized by business logic. I think in case of microservices having Order microservices and Article microservices is good. To manage relationship\n- When creating a new Order, we must update the related Article status to \"sell\".\n- When an Article price changes, we must update all Orders that are not validated yet with the new price.\n\nWe must implements SAGA or something else. To avoid this complexity we want keep business module but in monolith application\n\n**In this case do you think duplicated classes could be the solution ?**",
              "score": 1,
              "created_utc": "2026-01-18 10:47:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09wg2g",
                  "author": "flavius-as",
                  "text": "Of course. That's what I just said.\n\nThe thinking for modulith is similar to microservices.\n\nIn fact, modulith is nothing but microservices but restricted to the logical view of the system, as opposed to the physical view of the system (which would be microservices).\n\nYou should restrict with database permissions the access to tables and columns in isolated cases to specific usernames and give each module a separate username and password and connection. This way you can ensure via governance that no module does \"creative\" things.",
                  "score": 1,
                  "created_utc": "2026-01-18 11:25:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o09no8m",
          "author": "Infeligo",
          "text": "This can be handled using event. Direct dependencies between modules should form a DAG. I use \"who should know about whom\" mental model. In your case, Orders know about Articles (direct dependency), but Article don't know they are being sold by Orders. Articles can also emit events, e.g. when the price changes. Order module listens to price changes and reacts accordingly. This can all be done using Spring Modulith, which has dedicated facilities for domain events.",
          "score": 1,
          "created_utc": "2026-01-18 10:05:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o09pyyy",
          "author": "Jealous-Implement-51",
          "text": "If you only looking for solution, you can use getRequiredSerivice method. Better to revise your implementation, it shouldn't have this kind of problem.",
          "score": 1,
          "created_utc": "2026-01-18 10:26:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09sb61",
              "author": "Ok-Professor-9441",
              "text": "Could be explain the purpose of `getRequiredSerivice` and how it solve cyclic dependencies ?",
              "score": 1,
              "created_utc": "2026-01-18 10:48:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09tt3b",
                  "author": "Jealous-Implement-51",
                  "text": "It‚Äôs the method from `IServiceProvider`. It essentially hides dependencies during startup, and only resolves when needed, however it will still fail if both service try to use each other during construction. Best way is to refactor the project, this might be something you need. https://learn.microsoft.com/en-us/azure/architecture/guide/architecture-styles/n-tier",
                  "score": 1,
                  "created_utc": "2026-01-18 11:01:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}