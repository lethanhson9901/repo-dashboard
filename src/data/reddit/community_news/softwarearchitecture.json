{
  "metadata": {
    "last_updated": "2026-02-10 09:19:59",
    "time_filter": "week",
    "subreddit": "softwarearchitecture",
    "total_items": 17,
    "total_comments": 100,
    "file_size_bytes": 125572
  },
  "items": [
    {
      "id": "1qv07p2",
      "title": "At what scale does \"just use postgres\" stop being good architecture advice?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qv07p2/at_what_scale_does_just_use_postgres_stop_being/",
      "author": "Designer-Jacket-5111",
      "created_utc": "2026-02-03 18:38:02",
      "score": 105,
      "num_comments": 40,
      "upvote_ratio": 0.93,
      "text": "Every architecture discussion I see ends with someone saying \"just use postgres\" and honestly theyre usually right. Postgres handles way more than people think, JSON columns, full text search, pub/sub, time series data, you name it.\n\nBut there has to be a breaking point where adding more postgres features becomes worse than using purpose-built tools. When does that happen? 10k requests per second? 1 million records? 100 concurrent writers?\n\nIve seen companies scale to billions of records on postgres and Ive seen companies break at 10 million. Ive seen people using postgres as a message queue successfully and Ive seen it be a disaster.\n\nWhat determines when specialized tools become necessary? Is it always just \"when postgres becomes the bottleneck\" or are there other architectural reasons?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qv07p2/at_what_scale_does_just_use_postgres_stop_being/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3dyu5h",
          "author": "Sea_Weather5428",
          "text": "the breaking point for me is when the postgres-specific workarounds start taking more time than learning a purpose-built tool would, like when your jsonb queries start needing 47 indexes",
          "score": 87,
          "created_utc": "2026-02-03 18:39:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e9zq8",
              "author": "sfboots",
              "text": "We stay away from indexes on the Jsonb content and copy a few items to regular columns for indexing",
              "score": 36,
              "created_utc": "2026-02-03 19:30:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3khdoy",
                  "author": "its_k1llsh0t",
                  "text": "We do a mix of both. Index where return speed is less important, then normalize for things where performance is more important. ",
                  "score": 1,
                  "created_utc": "2026-02-04 17:53:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3e15qs",
              "author": "ilya47",
              "text": "And when you learn about TOASTs due to jsonb.",
              "score": 13,
              "created_utc": "2026-02-03 18:49:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gnuke",
                  "author": "clearing_",
                  "text": "giving me ptsd to when we hit the OID limit while i was at a concert",
                  "score": 3,
                  "created_utc": "2026-02-04 02:51:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dzqtl",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 31,
          "created_utc": "2026-02-03 18:43:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3epfd9",
              "author": "InfluxCole",
              "text": "I think there's also some cost efficiency to worry about as scale goes up. Once you're running up huge monthly bills, it's not that you necessarily couldn't keep going with Postgres, but you could probably save some money by moving to something more tailor-made for the characteristics of your workload. When you reach that, \"this is getting expensive, maybe something more specific would give us the performance we need for cheaper,\" point still heavily depends on your company, budget, team size, etc.",
              "score": 7,
              "created_utc": "2026-02-03 20:43:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eou27",
          "author": "polotek",
          "text": "We had a ruby on rails system with postgres that scaled to 10 billion rows in some tables and still maintained high request throughout. The problem isn't scaling postgres. It's not easy, but it can go way further than most people will ever need. It depends on the complexity of what you're doing and your level of expertise with postgres.\n\nWhat comes after \"just use postgres\" is \"hire some postgres consultants to help you out and keep going\". In general you should only need to reach for a specialized datastore for services that have very specific data access requirements. And still it should be after you tried postgres first.",
          "score": 28,
          "created_utc": "2026-02-03 20:40:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3dysgx",
          "author": "Select-Print-9506",
          "text": "its almost never about raw scale, its about operational complexity and team expertise, postgres can handle way more than most companies need if you tune it properly",
          "score": 39,
          "created_utc": "2026-02-03 18:39:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i6ufy",
              "author": "bobaduk",
              "text": "This is the key point. I haven't deployed a relational database in a good long time, because I can get better operational characteristics from other datastores. If I spin up dynamo, chances are that for the way I build software, it'll be fine, and the answer to \"is it up, it is coping with the load\" is yes and move on.\n\nWe were using postgres for a while at $CURRENT_GIG, but for our use case the cost curve was unappealing, particularly when every engineer has a cloud environment of their own to play with, and it was cheaper to adopt a managed time series data store.",
              "score": 2,
              "created_utc": "2026-02-04 09:58:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eftyi",
          "author": "External_Mushroom115",
          "text": "When your domain needs to scale writes  rather than reads.",
          "score": 8,
          "created_utc": "2026-02-03 19:58:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e3pga",
          "author": "who_am_i_to_say_so",
          "text": "There is no set number. Sometimes 1 million rows it will start performing like a dog- sometimes itâ€™s 20 million rows. \n\nBut a general indicator is if you hit your max connections regularly even with upsizing and pooling (scaling vertically). Then you look into caching, perhaps- or the more expensive option of scaling horizontally.",
          "score": 17,
          "created_utc": "2026-02-03 19:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3e88dw",
              "author": "sfboots",
              "text": "Max connections can be misleading if they are not using pgbouncer",
              "score": 8,
              "created_utc": "2026-02-03 19:22:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3eyqx7",
              "author": "Typicalusrname",
              "text": "If Postgres performs like a dog with a million records the data model is shit",
              "score": 8,
              "created_utc": "2026-02-03 21:26:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ezlti",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": -2,
                  "created_utc": "2026-02-03 21:30:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3f143t",
              "author": "Apart-Entertainer-25",
              "text": "Have you tried not running it on a toaster? :)",
              "score": 3,
              "created_utc": "2026-02-03 21:37:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f24zk",
                  "author": "who_am_i_to_say_so",
                  "text": "Relatedly there's this: [https://www.crunchydata.com/blog/postgres-toast-the-greatest-thing-since-sliced-bread](https://www.crunchydata.com/blog/postgres-toast-the-greatest-thing-since-sliced-bread)",
                  "score": 4,
                  "created_utc": "2026-02-03 21:42:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dyvm0",
          "author": "Select-Print-9506",
          "text": "same applies to api management honestly, you can build everything custom on top of nginx or envoy but at some point using something like gravitee or kong saves you from reinventing wheels that dont need reinventing",
          "score": 14,
          "created_utc": "2026-02-03 18:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e9ai4",
          "author": "sfboots",
          "text": "The limit depends heavily on workload and total IO needs assuming correct code and indexes\n\nAlso, some companies stay with Postgres and just use extensions like timescale or move some functions to a different database. \n\nMy company has 3 tables with more than a billion total rows and performance is adequate.  We do partition by time ranges since most use is the last year.",
          "score": 4,
          "created_utc": "2026-02-03 19:27:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e888l",
          "author": "mountainlifa",
          "text": "I've always wondered this. And when do folks introduce key pair database systems like dynamo into their architecture?Â ",
          "score": 5,
          "created_utc": "2026-02-03 19:22:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ebfp8",
          "author": "awol-owl",
          "text": "Never, yet at work this week weâ€™re moving to Elasticsearch as our prototype showed it to be a search friendly service. I believe itâ€™ll use more ram to run the new cluster, although Iâ€™m hoping the developer experience will be worth it. Iâ€™m not convinced yet.",
          "score": 3,
          "created_utc": "2026-02-03 19:37:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e3q9q",
          "author": "pgEdge_Postgres",
          "text": "Oftentimes it's not even the specialized tools that you need, just a tuned configuration and some good insights into your stack! Metrics go a long way towards predicting failures or reacting quickly when they do happen; take the lessons learned and turn them into actual architectural changes, and you can iterate up to those instances of billions of records.\n\nRelated interesting article: [https://openai.com/index/scaling-postgresql/](https://openai.com/index/scaling-postgresql/) if you missed it, OpenAI scaled PostgreSQL to power 800 million ChatGPT users; it powers both ChatGPT and OpenAI's API.",
          "score": 4,
          "created_utc": "2026-02-03 19:01:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lra5t",
              "author": "caught_in_a_landslid",
              "text": "The article is fairly clear that they are not allowing new tables any more and they are using cosmosdb for new things.\n\nIt shows that you can indeed push PG really far, but there's a real reason that other databases Exsist.",
              "score": 1,
              "created_utc": "2026-02-04 21:27:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3eivyi",
          "author": "WilliamBarnhill",
          "text": "If you aren't doing rapid prototyping, I'd argue at any scale. You need to be able to articulate to stakeholders what your technology selection candidates were, what the tradeoffs were between them, your rationale for choosing the technology you did, and potential future risks as a result. Sometimes time-to-market is the overwhelming driver, but even then you need to be able to answer 'Why will using Postgres get us there faster, and what problems might we face down the road?'.",
          "score": 2,
          "created_utc": "2026-02-03 20:12:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f4792",
          "author": "swithek",
          "text": "I once inherited a system that used postgres to store massive json blobs, with a bit of metadata kept in separate indexed columns for filtering. The production database contained nearly a petabyte of data (hundreds of millions of rows) and the queries were painfully slow so I think itâ€™s fair to say postgres wasnâ€™t exactly an ideal choice here",
          "score": 2,
          "created_utc": "2026-02-03 21:51:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ivbmz",
              "author": "bomeki12345678",
              "text": "I'm curious, for your usecase, what dbms is the ideal choice? Saving massive json blobs in a relational databases seems to be not optimal option for me.",
              "score": 1,
              "created_utc": "2026-02-04 13:09:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n0vwt",
                  "author": "shoot2thr1ll284",
                  "text": "I agree with you. \n\nIn this case it seems like they chose Postgres for convenience of having everything in one spot, but large json blobs are not great for most systems. Depending on the use case a document store could work better in this case, but honestly I would treat those large json as files and use a different file serving service like s3 and just keep the url to it on Postgres or in another service. Makes it so that the thing that searches isnâ€™t also responsible for the large amount of data transfer. At some point it just becomes networking and io speed limitationsâ€¦.",
                  "score": 1,
                  "created_utc": "2026-02-05 01:29:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hht0v",
          "author": "kmhosny",
          "text": "OpenAI wrote a post about how they use postgres to serve 800 million customers. https://openai.com/index/scaling-postgresql/\nNot every company is on openAi scale so in 90% of the cases there are optimization steps that can be taken to keep just usibg postgres",
          "score": 2,
          "created_utc": "2026-02-04 06:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3eqjob",
          "author": "truechange",
          "text": "When vertical scaling can be solved by offloaded cached data.",
          "score": 1,
          "created_utc": "2026-02-03 20:48:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3exq6q",
          "author": "lambdasintheoutfield",
          "text": "You can do vertical or horizontal sharding which allows you to scale the database extremely effectively.\n\nHorizontal sharding is partitioning a table into more tables with the same exact schema but fewer rows and then an index to track the partitioning/shards. If your queries require large scans of values, this works well and there are numerous partitioning schemes to pick.\n\nVertical sharding is where you partition on columns. If you are specifically querying for data in column subsets, you could just have dedicated tables for those.\n\nItâ€™s likely both would be helpful, and both reduce storage space. Be careful of your primary keys and indices but this is enormously effective when done right. \n\nIt isnâ€™t too difficult to roll your own postgres orchestrator across multiple nodes. \n\nAll that said, anytime you go distributed, you have to consider HA and fault tolerance. If you are querying a subset of rows on a node that goes down you obviously wonâ€™t be getting the data unless you replicate it.\n\nIf you know your access patterns, the critical and/or most frequently accessed data can use a higher replication factor and if a node goes down just route to the replicas. The architecture of your nodes can be a tree structure where each node is a shard and the leaf nodes are the replicas. Use this to inform the load balancer and query routing.",
          "score": 1,
          "created_utc": "2026-02-03 21:21:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gb452",
          "author": "Wiszcz",
          "text": "When cost of working around it's cons is higher than restructuring project.",
          "score": 1,
          "created_utc": "2026-02-04 01:39:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3grob5",
          "author": "dudeaciously",
          "text": "How does this group feel about concurrent transactions, with critical commit and rollback requirements.  Lots of connections. Then there is a breakage. Does coming back online break data integrity?  \n\nIf so, then the metrics on concurrent users, with operations per transaction would answer OP.",
          "score": 1,
          "created_utc": "2026-02-04 03:13:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3jo12a",
              "author": "SpamapS",
              "text": "Yes it comes back consistently, but it can be really slow. You're going to need a hot standby logically replicated to have a chance at your database being online more than 3 nines in this scenario.",
              "score": 1,
              "created_utc": "2026-02-04 15:38:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3k68pl",
                  "author": "dudeaciously",
                  "text": "Ah!  So to preserve data integrity in the whole database, replicate the nodes implicitly?  Not only after disaster recovery, but propagation for every committed transaction.\n\nVery cool.  No disagreements.  But I this is heavy, I have never done it.",
                  "score": 1,
                  "created_utc": "2026-02-04 17:02:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3jjmmm",
          "author": "andras_gerlits",
          "text": "Scale is rarely the bottleneck. It's usually replication and high-availability.",
          "score": 1,
          "created_utc": "2026-02-04 15:17:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jnlav",
          "author": "SpamapS",
          "text": "It's more that some features scale better than others, so you can keep using the core, but you'll find that stuff that made it easy to build on top of at low scale becomes too expensive at a higher scale.\n\nForeign keys and sub transactions start to become a burden with high concurrency for insurance due to multi transaction shared locks that scale quadratically. You start needing to avoid those at some point.\n\nLarge payloads eventually need to be moved to external object storage or you'll destroy memory usage.\n\nThe real point when postgres can't do it alone is around 4 nines. When you need more than 3 nines really, it's just  complicated to do that with postgres and you'll find some other architecture like a NoSQL or sharding layer like CitusDB will make it simpler.",
          "score": 1,
          "created_utc": "2026-02-04 15:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ll2br",
          "author": "TallGreenhouseGuy",
          "text": "If youâ€™re used to partitioning using Oracle, the Postgres way is just painful to work with.\n\nSo having large tables that you want to partition is quite an ordeal if you want to do range based partitioning using eg date and there are foreign keys/primary keys that are not naturally a part of the partition key.",
          "score": 1,
          "created_utc": "2026-02-04 20:57:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41fcym",
          "author": "CoreyTheGeek",
          "text": "When your director tells you can't because dynamo is so cheap even though the system calls for a relational database... Ahhh I should have been a farmer",
          "score": 1,
          "created_utc": "2026-02-07 06:18:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o444dne",
          "author": "n4jgg",
          "text": "You can achieve a lot of things with PG. Like jsonb is usually quite enough to store fully denormalized data. You wouldn't need a mongodb just because 2-3 use cases need a jsonb. Nor an elastic search cluster, if all you need to have is a primitive search tool.\n\nPub/Sub with Postgres, not a great idea probably. As you might run out of number of connections with enough number of clients which can effectively bring down whole DB.\n\nCaching high volume , short lived data? Probably also not a great idea. As the number of inserted but not vacuumed rows will take huge space and likely to effect overall performance.\n\nIf your work loads aren't heavily based on such cases? You're probably right, just use PG until it proves to be not enough.",
          "score": 1,
          "created_utc": "2026-02-07 17:43:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy7oet",
      "title": "How to Design Systems That Actually Scale? Think Like a Senior Engineer",
      "subreddit": "softwarearchitecture",
      "url": "https://javarevisited.substack.com/p/how-to-scale-like-a-senior-engineer",
      "author": "javinpaul",
      "created_utc": "2026-02-07 07:36:48",
      "score": 74,
      "num_comments": 9,
      "upvote_ratio": 0.89,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qy7oet/how_to_design_systems_that_actually_scale_think/",
      "domain": "javarevisited.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o42a4ou",
          "author": "Ad3763_Throwaway",
          "text": "I rather have a developer which tells you that every problem requires another solution than one who re-iterates things like mentioned in the article.\n\n90% of scalability is just optimizing database queries. Learn how to monitor, troubleshoot and fix these and you can solve most scalability issues. SQL Server easily handles multiple thousands of requests per second.\n\nMultiple web servers / load balancing is mainly needed for always online systems, so you can take one offline to do updates on the other. Most software will never reach the point that the webserver is the limiting factor. You can easily do thousands of request per sec on most as long as you handle requests efficiently.\n\nSecurity is in most case just not inventing the wheel yourself. Use frameworks and don't write your own hashing / encryption algorithms.",
          "score": 32,
          "created_utc": "2026-02-07 11:12:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42gvs7",
              "author": "bittrance",
              "text": "I would rather say that 90% of scalability is picking the right storage solution. RDBMs can indeed handle thousands of queries per seconds. That does not really qualify as scale. No amount of RDBM will serve millions of requests per second unaided.\n\nIt is indeed true that most software systems will never reach a scale worth mentioning. Unfortunately, this has produced a cadre of developers who do not know how to address non-trivial scale, leading them into diminishing return query optimization.",
              "score": 10,
              "created_utc": "2026-02-07 12:12:14",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o43g3n5",
                  "author": "FlatProtrusion",
                  "text": "How do you learn to address non-trivial scale?",
                  "score": 1,
                  "created_utc": "2026-02-07 15:44:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o449vbc",
              "author": "Ambitious-Sense2769",
              "text": "At scale weâ€™re usually talking about millions of requests/transactions per second. Not thousands. Just having one sql db with proper indexing and query optimization still cannot handle millions of transactions per second. So thatâ€™s why people have to learn about things like sharding, load balancing, caching, etc. I agree most people wonâ€™t touch that scale but that isnâ€™t really the point of â€œdesigning systems that scaleâ€.",
              "score": 7,
              "created_utc": "2026-02-07 18:10:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o45glf8",
                  "author": "PmMeCuteDogsThanks",
                  "text": "But it is, there are so many clueless developers out there that think that everything needs to â€scaleâ€.Â \n\nI agree, with millions of transactions per second you canâ€™t use a single database.Â \n\nBut very few systems have that, and many of those can be optimised in-place.Â ",
                  "score": 3,
                  "created_utc": "2026-02-07 21:53:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4f5eso",
                  "author": "ThlintoRatscar",
                  "text": "Scaling isn't just about mass.  It's about running out of possible resources and dealing with how to add more.\n\nSharding ( partitioning work into independent batches, dispatching the requests to the appropriate nodes, and coordinating answers and errors ), load balancing ( smoothing resource spikes ), and caching ( read sharding ) aren't just for databases.",
                  "score": 1,
                  "created_utc": "2026-02-09 12:09:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o46akik",
              "author": "yoggolian",
              "text": "Totally - a really good skill to have is being able to tell a scale problem from a tuning problem. An app we had with 25k users and maybe 1000 concurrent doesnâ€™t need scaling solutions, it needs tuning (along with a bit of common sense, like donâ€™t deploy 4x servers in a cluster to run a single container instance with auto scaling turned off, thatâ€™s also defined as the smallest possible size - Iâ€™m going to pull about $60k in annual cloud cost savings at $work there).Â ",
              "score": 2,
              "created_utc": "2026-02-08 00:50:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e8iqu",
          "author": "TheLox95",
          "text": "[Let's handle 1 million requests per second](https://youtu.be/W4EwfEU8CGA?si=zgq03gv0GbkAu7Fu)",
          "score": 1,
          "created_utc": "2026-02-09 07:00:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzv32f",
      "title": "How do you validate architecture decisions early without senior review?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qzv32f/how_do_you_validate_architecture_decisions_early/",
      "author": "MainWild1290",
      "created_utc": "2026-02-09 04:53:26",
      "score": 38,
      "num_comments": 35,
      "upvote_ratio": 0.93,
      "text": "When designing systems I often struggle with questions like:\n\n* Will this Kafka setup handle real production load?\n* Should I scale DB with replicas or caching first?\n* Is this architecture fine or secretly fragile?\n\nSenior architecture reviews are valuable but not always accessible, and generic AI answers often feel shallow.\n\n**I'm curious:**\n\nHow do experienced engineers validate architecture decisions early?\n\n* Do you rely on design patterns?\n* Internal review processes?\n* Load testing?\n* Something else?\n\nI'm exploring ways to structure architecture reasoning better, so really interested in hearing real workflows from this community.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qzv32f/how_do_you_validate_architecture_decisions_early/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o4e6zu4",
          "author": "ronakg",
          "text": "You should be discussing important architectural decisions with other members of the team during other forums or 1:1s. Brainstorming ideas is one of the most important aspects of designing something big. When I finally have the design doc ready, I mention all such folks as contributors to the design. Don't try to build things on your own.",
          "score": 27,
          "created_utc": "2026-02-09 06:47:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4evpog",
              "author": "SkyPL",
              "text": "Remember the dangers of the design by committee - any such group discussion should be made with an appropriate means of mitigating the risks of outspoken people with Dunning-Kruger derailing the architecture.",
              "score": 6,
              "created_utc": "2026-02-09 10:46:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4f05vf",
                  "author": "Ok_Slide4905",
                  "text": "Yeah thatâ€™s why most docs have approved reviewers. Anyone can view and comment but only approved reviewers can sign off. No different from code review.",
                  "score": 2,
                  "created_utc": "2026-02-09 11:26:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4dzn3c",
          "author": "WhirlyDurvy",
          "text": "A lot of my prevalidation comes from experience. When I don't have experience in a particular technology, I set up a dummy load test in isolation to try to break it, then scale accordingly. \n\nBut before launching something major that's going to see large load quickly, you want to understand your account service limits, have a full model of the scaling profile of each technology, a real integration scale test in dev, and a fall back plan.",
          "score": 17,
          "created_utc": "2026-02-09 05:44:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4e1xr2",
              "author": "MainWild1290",
              "text": "Do you follow a checklist when evaluating architecture risks, or is it more intuition built from experience? one thing i m exploring is whether structural reasoning prompts could help less experienced engineers think through these same steps earlier.",
              "score": 3,
              "created_utc": "2026-02-09 06:03:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4e4qrs",
                  "author": "No-Risk-7677",
                  "text": "When it comes to risks ATAM might be what you are looking for: https://youtu.be/fsLe8Q3oTEQ?si=gGeqObdcE5u7y9Fd",
                  "score": 3,
                  "created_utc": "2026-02-09 06:27:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4gai3y",
              "author": "LordWecker",
              "text": "I like that you renamed it _pre_validation. \n\nWith experience people get better at guessing what will be valid, but real validation is whether or not it performs its goals.",
              "score": 1,
              "created_utc": "2026-02-09 16:06:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4gvu8w",
          "author": "BanaTibor",
          "text": "The telltale sign of a good architecture that it is easy to change. The real question is not \"will this handle the load?\" but \"how hard will it be to change?\".",
          "score": 3,
          "created_utc": "2026-02-09 17:47:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kgdbe",
              "author": "MainWild1290",
              "text": "Yaa thats great, thank you",
              "score": 1,
              "created_utc": "2026-02-10 05:35:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4dztll",
          "author": "flavius-as",
          "text": "Disclaimer: the following has been generated with AI, by a prompt I've been working on for over a year, with many biases built-in (over 100 rules and principles). Specifically for your questions I've instructed it specifically to focus on the time dimension of decision making.\n\nHere it goes, as if I worded it, only better (English is not my native language) and properly formatted.\n\n**Architecture is not an engineering standard; it is a derivative function of business intent.**\n\nThe mistake most developers make is trying to validate their designs against \"Best Practices\" or abstract notions of \"Scale.\" A Senior Architect doesn't do that. They validate against the organization's runway.\n\nIf you lack a mentor to review your work, you must simulate this mindset by filtering every decision through the current stage of your company.\n\n### 1. The \"Survival\" Filter (Pre-Product/Market Fit)\nIn this phase, the organization is testing hypotheses. The biggest risk isn't \"system failure\"; it's building the wrong thing efficiently.\n*   **Validation Rule:** Optimize for **Reversibility**.\n*   **The Kafka Decision:** If you ask \"Will Kafka handle the load?\", you are asking the wrong question. The right question is: \"Does the operational tax of running Kafka prevent us from pivoting next month?\"\n*   **Verdict:** Kafka is likely invalid here. It forces structural rigidity. A monolith with a simple Postgres job queue is \"valid\" because itâ€™s easy to delete or change when the business idea fails.\n\n### 2. The \"Sales Promise\" Filter (Growth Phase)\nOnce the business has traction, validation shifts from \"Can we change it?\" to \"Can we honor the contract?\"\n*   **Validation Rule:** Optimize for **The specific promise Sales made.**\n*   **Replica vs. Caching:** Don't choose based on tech blogs. Look at the Service Level Agreement (SLA).\n    *   **Scenario A:** Sales sold a financial reporting tool. The promise is **Accuracy**. Caching is now a liability because stale data breaks the promise. You validate by proving you can scale via Read Replicas (ACID compliance).\n    *   **Scenario B:** Sales sold a social feed. The promise is **Responsiveness**. Stale data is annoying but acceptable. You validate by implementing aggressive Caching (Redis/CDN).\n\n### 3. The \"Fermi\" Filter (The Math Check)\nExperienced engineers rarely guess about load. We use **Fermi Estimation** to prove safety before writing code. This is your strongest tool against \"Imposter Syndrome.\"\n\n**The Calculation:**\nDon't worry about \"Real Production Load\" in the abstract. Calculate it.\n*   **Formula:** `Total Users` Ã— `Daily Active %` Ã— `Requests Per User` Ã· `Seconds in Day`.\n*   **Example:** 100k users. 10% active. 50 clicks each.\n    *   `10,000 users * 50 reqs = 500,000 reqs/day`.\n    *   `500,000 / 86,400 seconds = ~5.7 requests per second`.\n*   **The Validation:** A Raspberry Pi can handle 6 requests per second. You don't need Kubernetes; you need a single VPS. Decision validated.\n\n### 4. The \"Bus Factor\" Filter (Secret Fragility)\nYou asked: *\"Is this architecture fine or secretly fragile?\"*\n\nFragility is rarely about code breaking; it's about **Cognitive Solvency**. The \"cost\" of an architecture is the amount of human memory required to operate it.\n\n*   **The Test:** If you (the architect) are asleep or hit by a bus, can the most junior person on the team fix a critical bug at 3 AM?\n*   **The Trap:** If your \"valid\" architecture requires understanding Event Sourcing, three different message brokers, and a custom sharding layer, it is structurally insolvent for a small team.\n\n**Summary:**\nTo validate early, stop looking for \"correctness.\" Look for alignment.\n*   **Early Stage:** Valid = Reversible.\n*   **Growth Stage:** Valid = Honors the SLA.\n*   **Scale Stage:** Valid = Reduces Cognitive Load per developer.\n\nFor what it's worth, I've seen more startups die from over-engineering \"Google-scale\" solutions for zero users than from their servers melting down. Build for the problem you have right now.",
          "score": 12,
          "created_utc": "2026-02-09 05:46:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4el2wl",
              "author": "DER_PROKRASTINATOR",
              "text": "This is really good, AI-generated or not. Optimizing for reversibility in the beginning was a huge take-away over my career, after seeing projects fail.",
              "score": 4,
              "created_utc": "2026-02-09 09:01:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4e1gst",
              "author": "flavius-as",
              "text": "It's crazy people don't look at the actual content. A shame.",
              "score": 6,
              "created_utc": "2026-02-09 05:59:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4e5bzl",
              "author": "No-Risk-7677",
              "text": "Awesome explanations.\n\nWhere can I read more about theses filters 1 - 4?",
              "score": 2,
              "created_utc": "2026-02-09 06:32:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4e5u97",
                  "author": "flavius-as",
                  "text": "It leans a lot into 3X by Kent Beck.\n\nhttps://www.oreilly.com/library/view/tidy-first/9781098151232/",
                  "score": 4,
                  "created_utc": "2026-02-09 06:36:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4g1qpe",
              "author": "flavius-as",
              "text": "What I use to plan the switch from one strategy to the next:\n\nPlanning meetings with business people which I shadow, to gauge where they want to take the company in the next 3, 6, 12, 24 months.",
              "score": 1,
              "created_utc": "2026-02-09 15:23:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i6skt",
          "author": "theycanttell",
          "text": "Performance testing",
          "score": 2,
          "created_utc": "2026-02-09 21:35:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kh0md",
              "author": "MainWild1290",
              "text": "Yaa, sometimes real performance testing gives much clearer signals than just guessing early.",
              "score": 1,
              "created_utc": "2026-02-10 05:40:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4jc54z",
          "author": "Both-Fondant-4801",
          "text": "How do you validate architecture decisions early without senior review? .. well most of the time you don't and you can't. Hence the usual practice would be to develop POC's, proof of concepts to validate that the system is feasible.. and with some load testing, be able to show with numbers and some level of objectivity how the system would eventually scale. ",
          "score": 2,
          "created_utc": "2026-02-10 01:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kieut",
              "author": "MainWild1290",
              "text": "Yaa, got it, building POCs and validating with real numbers seems like a practical way to reduce uncertainty early. Thanks you",
              "score": 2,
              "created_utc": "2026-02-10 05:51:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4e7w0w",
          "author": "JackSpyder",
          "text": "Discuss, design, hypothesise and test to validate.\n\nIf youre making a big change to solve X. You need to measure X now, and measure X afterwards. Otherwise you're just pissing in the wind.",
          "score": 2,
          "created_utc": "2026-02-09 06:54:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fzwpo",
              "author": "MainWild1290",
              "text": "Got it and do you usually decide what to measure before making the change?",
              "score": 1,
              "created_utc": "2026-02-09 15:14:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4g082e",
                  "author": "JackSpyder",
                  "text": "Are you changing something or building new?\n\nWhat are the goals and outcomes?\n\nThis really becomes a bit boring typical \"it depends\" lol.",
                  "score": 2,
                  "created_utc": "2026-02-09 15:16:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ebuvu",
          "author": "dariusbiggs",
          "text": "You don't even consider those questions initially.\n\nKeep it as simple as possible, the biggest costs to focus on are Security, Privacy, Observability, and Testing. Everything else is a secondary concern.\n\nLoad testing is a part of Testing, this gives you actionable data from your Observability systems. Only with good Observability can intelligent and informed decisions be made on the questions you asked regarding horizontal and vertical scaling of various components.\n\nReal data from your Observability systems provide the necessary information needed about how and what the end users need/use.\n\nAll of the third party systems are interfaces to a service. Your database service, your event notification bus is a service, etc. You don't care at the design time about what implementation is used, all you care about is what you need the service to do.\n\nYou should know and understand the basic tools and patterns, idempotency, CQRS, event sourcing, event streaming, BDD, TDD, DDD, REST, PubSub, Database normalization, etc. That allows you to identify which pattern or approach is best for the various components of the things being designed.",
          "score": 3,
          "created_utc": "2026-02-09 07:31:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fzbd9",
              "author": "MainWild1290",
              "text": "Do you usually plan monitoring and logging from the beginning, or add it later as the system grows?",
              "score": 1,
              "created_utc": "2026-02-09 15:11:20",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ht1ft",
                  "author": "dariusbiggs",
                  "text": "Observability from the start, without it you know nothing about your software.",
                  "score": 2,
                  "created_utc": "2026-02-09 20:27:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4du6bn",
          "author": "alien3d",
          "text": "real advise as senior . stop thinking . Delegate delegate . Is this static information rarely change - cdn . It is reusable session cross server - redis . Even some old senior trap in this scenario, we want all those  latest thing but it is really working ?",
          "score": 2,
          "created_utc": "2026-02-09 05:03:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eb99x",
          "author": "Charming-Raspberry77",
          "text": "A senior review is not always fool proof either. I am a firm believer on clean architecture + early load testing on the dev/staging environment. Just keep it simple and load it yourself, even at the poc stage most ghosts will come out.",
          "score": 2,
          "created_utc": "2026-02-09 07:25:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4frxry",
              "author": "MainWild1290",
              "text": "When you do early load testing, do you try to simulate real user behavior or just push the system hard to see where it breaks?",
              "score": 1,
              "created_utc": "2026-02-09 14:31:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4h16pt",
                  "author": "Charming-Raspberry77",
                  "text": "I write small scenarios with a tool such as gatling. Any system/ hardware will break the how is very important.",
                  "score": 2,
                  "created_utc": "2026-02-09 18:12:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxuf8l",
      "title": "How to approach a technical book?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qxuf8l/how_to_approach_a_technical_book/",
      "author": "FactorLongjumping167",
      "created_utc": "2026-02-06 21:25:30",
      "score": 37,
      "num_comments": 33,
      "upvote_ratio": 0.89,
      "text": "everytime i talk to a senior dev about some confusions i have with some concepts, they suggest me to read a book of 700 pages or so..\nI wanted to ask how do you guys approach such books? i mean do you read them from end to end? how does that work? thank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qxuf8l/how_to_approach_a_technical_book/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3z939s",
          "author": "severoon",
          "text": "I read the book.\n\nPart of being a good reader is being able to quickly extract the information you want on a first pass, and then dive deeper into the details that are relevant to your issue. But honestly, if you don't even give a first pass over the material covered by a book that is presumably cohesive, you don't even know what's already been done on that subject, so it will be hard to even know the right questions to ask.\n\nAlso if you're not getting it, don't just hunker down with the book and try to plough through it. You have the Internet, you have chatgippity.",
          "score": 24,
          "created_utc": "2026-02-06 22:02:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zfsfn",
              "author": "jac4941",
              "text": "> chatgippity\n\nIdk if this was a typo or intentional but I kinda love it ðŸ˜„",
              "score": 5,
              "created_utc": "2026-02-06 22:37:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zjdxq",
                  "author": "severoon",
                  "text": "Not a typo, this is a pretty common nickname for the Chat Generative Pre-trained Transformer.",
                  "score": 1,
                  "created_utc": "2026-02-06 22:57:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o411szk",
              "author": "Snoo23533",
              "text": "Agree and this is the kind of answer that was needed here. You can read non-fiction different than a fiction book. Even flipping through to parse models, tables, headers and summaries on a quick read can yield results. Not everbody has time to power through end eto end and tbh a lot of authors core ideas shouldve been a blog post but got expanded so theyd have something to sell.",
              "score": 1,
              "created_utc": "2026-02-07 04:32:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z383j",
          "author": "Aggressive_Ad_5454",
          "text": "Good question.\n\nI read the table of contents carefully.\nI then look at the intro chapters if I'm new to the particular tech.\n\nThen, there's usually a chapter on, I dunno, performance or stability or edge-cases that is basically answering the question \"what's hard about using this?\" I read that chapter really carefully and research the answers. \n\nThat, I find, is an efficient entry to most books of OReilly Media or similar quality and editorial standards.",
          "score": 26,
          "created_utc": "2026-02-06 21:33:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z2lr3",
          "author": "manamonkey",
          "text": "How do you normally approach a book? Have you encountered reference books before?",
          "score": 10,
          "created_utc": "2026-02-06 21:30:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z32t0",
              "author": "PmMeCuteDogsThanks",
              "text": "You mean reading full sentences, using a table of contents, without the support of an AI? Sorry, what was the question again, I was distracted.",
              "score": 18,
              "created_utc": "2026-02-06 21:33:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z5nem",
          "author": "Adorable-Fault-5116",
          "text": "Are you asking how reading works?\n\nLook I would a) read articles on a topic if it was a casual conversation and I was clueless, and / or b) read the book if it seems core to stuff we are doing or I'm more interested.\n\nYou should read books. Ideally written before 2022 when the slop took over.",
          "score": 5,
          "created_utc": "2026-02-06 21:45:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z5c25",
          "author": "GurglingGarfish",
          "text": "Sneak up on it from behind, so it doesnâ€™t see you, then pounce on it when youâ€™re about 3ft away. Thatâ€™s how I usually approach them.",
          "score": 17,
          "created_utc": "2026-02-06 21:44:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z8te1",
              "author": "halfxdeveloper",
              "text": "Do you use a special bait to lure them out? I canâ€™t seem to find a good pack in the wild.",
              "score": 5,
              "created_utc": "2026-02-06 22:01:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o40uxfc",
              "author": "I_Have_A_Snout",
              "text": "If youâ€™re one of the â€œhygiene is optionalâ€ types, also approach from downwind.",
              "score": 2,
              "created_utc": "2026-02-07 03:44:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z2j3w",
          "author": "PmMeCuteDogsThanks",
          "text": "JesusÂ ",
          "score": 13,
          "created_utc": "2026-02-06 21:30:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z37k8",
          "author": "100kgoffun",
          "text": "Can you read?",
          "score": 2,
          "created_utc": "2026-02-06 21:33:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z59ig",
          "author": "Cherveny2",
          "text": "depends on the book really.  Is it something that you know a lot of the subject matter, and only need to learn certain new topics?  find them in the table of contents/index, and go to them directly.\n\nIs the whole subject new?  dig in and read it.\n\nAlso, for extra re-inforncement of learning,  does it have exercises you can do while reading?  Do them. Does it have questionss at the end of each chapter?  Try answering them.  Again ensures you get the concepts.\n\n",
          "score": 2,
          "created_utc": "2026-02-06 21:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zmrah",
          "author": "Spite_Gold",
          "text": "So books, you know, there are words and you can read them, and as you read you learn new stuff. \n\nAnd dont  forget to go to next page as you're done with current page, this is a really rookie mistake!",
          "score": 3,
          "created_utc": "2026-02-06 23:15:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o420lzn",
              "author": "Whole_Ladder_9583",
              "text": "Books are tricky https://youtu.be/pQHX-SjgQvQ?si=l7cScP6QyrNNFtl6",
              "score": 1,
              "created_utc": "2026-02-07 09:39:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z2dr8",
          "author": "flavius-as",
          "text": "Lol.\n\nYes, you read them, understand and learn.\n\nAnd nowadays you even extract knowledge with page numbers for LLM.",
          "score": 1,
          "created_utc": "2026-02-06 21:29:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z7ull",
          "author": "Marelle01",
          "text": "If this is your first technical book, I recommend reading everything in order and doing all the exercises, if there are any, until you can do them correctly. Keep in mind that some of these books correspond to several hundred hours of undergraduate or graduate-level coursework. Once you have delved into your subject, you will often only need to read a few chapters, as you will have mastered the rest.",
          "score": 1,
          "created_utc": "2026-02-06 21:56:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z8guz",
          "author": "failsafe-author",
          "text": "I read them end to end.",
          "score": 1,
          "created_utc": "2026-02-06 21:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ze87f",
          "author": "shufflepoint",
          "text": "Be careful. Approach like you would an unfamiliar dog. Walk slowly, approaching from the side rather than head-on. If it seems chill, pet it sides. ",
          "score": 1,
          "created_utc": "2026-02-06 22:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zkrfd",
          "author": "BarfingOnMyFace",
          "text": "Take a stance slightly wider than your shoulders,  left leg out front, keeping your torso facing forward, chin tucked, hands up, and then approach slowly and defensively.",
          "score": 1,
          "created_utc": "2026-02-06 23:04:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zuwie",
          "author": "Acceptable_Crab4153",
          "text": "This is not a Novel. You select the chapters relevant to you. Then, after reading the summary at the end, you delve deeper into the specific details you need clarification on within the chapter..",
          "score": 1,
          "created_utc": "2026-02-07 00:02:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zw4yw",
          "author": "WhenSummerIsGone",
          "text": "have you ever gone to school and worked your way through a book? There are no shortcuts.",
          "score": 1,
          "created_utc": "2026-02-07 00:10:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zzbx2",
          "author": "Forsaken-Victory4636",
          "text": "Depends, but sometimes yes.\n\nI read Mark Lutzâ€™s â€œ Learning Pythonâ€ cover to cover Â all 1648 pages of it.\n\nIt became the foundation of a career switch from mechanics to software engineering.",
          "score": 1,
          "created_utc": "2026-02-07 00:28:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o402bae",
          "author": "IlliterateJedi",
          "text": "It depends on the book.  Usually I start at the beginning and read as far as I need to in order to understand the topic.  Thankfully with tools like Claude and ChatGPT I bounce any questions or clarifications off an LLM as needed.  It makes understanding concepts a lot easier in my experience.  It's hard to say with 100% certainty because some books are best used as references where you only read the chapter you need (e.g., Fluent Python or something).",
          "score": 1,
          "created_utc": "2026-02-07 00:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o421hgh",
          "author": "imihnevich",
          "text": "The pages are usually numbered, you start from the lowest number and progressively increase it one by one, if your book is in English, you can read it from the top left corner, left to right, line by line. It's okay to skip pages if you are looking for some specific pieces of information, but it's most fun when you don't.\n\nOn a serious note, you need to always understand how you can apply the information in practice when it comes to technical books. Feynman's technique is an amazing tool",
          "score": 1,
          "created_utc": "2026-02-07 09:48:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4378o1",
          "author": "bomeki12345678",
          "text": "My steps are:\n* Clarify what do I need from the book to get the motive to read it\n* Check the table of contents to get the core keywords, ask gemini to explain them like I'm 5 to get familar with concepts\n* Read each chapters one by one. Always focus on the problem statements that each chapter say first. Try to stop and think little bit how would you solve it using previous chapter knowledge\n* Read the solution slowly until you can understand it.",
          "score": 1,
          "created_utc": "2026-02-07 14:58:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44fcrc",
          "author": "No_Indication_1238",
          "text": "You sit down every day and read 15 pages. In about a month and half you'll have read the book and have a new perspective on the subject. Keep doing it until you get cracked. That's it. Most of the important stuff isn't in tutorials and videos. Like 95% of the important stuff isn't. If you haven't read books, you're missing out.",
          "score": 1,
          "created_utc": "2026-02-07 18:37:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48lzyn",
          "author": "Miserable_Disk3045",
          "text": "Some books cover to cover once and then as needed refer. Some books just a few chapters. But these days I usually start with Gemini for refreshing knowledge and go for books for niche areas.",
          "score": 1,
          "created_utc": "2026-02-08 11:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4966kp",
          "author": "symbiat0",
          "text": "Reading a book really is the best way to retain more when learning a subject. I know younger generations really don't want to hear that though...",
          "score": 1,
          "created_utc": "2026-02-08 14:13:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49ll0z",
          "author": "ResolveResident118",
          "text": "Slowly, and from the front so as not to frighten it. They can be skittish.Â ",
          "score": 1,
          "created_utc": "2026-02-08 15:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4axtya",
          "author": "BanaTibor",
          "text": "I usually sneak upon them mission impossible style, while crooning the music in my head. ",
          "score": 1,
          "created_utc": "2026-02-08 19:28:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z6pb5",
          "author": "tinmanjk",
          "text": "If it's canonical book spend 2-3 months reading it cover to cover. After this you'd probably be 3-4x better dev. Not going to be easy though.",
          "score": 0,
          "created_utc": "2026-02-06 21:51:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv3nr2",
      "title": "Kafka for Architects â€” designing Kafka systems that have to last",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qv3nr2/kafka_for_architects_designing_kafka_systems_that/",
      "author": "ManningBooks",
      "created_utc": "2026-02-03 20:44:15",
      "score": 34,
      "num_comments": 6,
      "upvote_ratio": 0.88,
      "text": "Hi r/softwarearchitecture,\n\nStjepan from Manning here. Weâ€™ve just released a book thatâ€™s written for people who have to make architectural calls around event-driven systems and then defend those decisions over time. Mods said it's ok if I post it here:\n\n**Kafka for Architects** by Katya Gorshkova  \n[https://www.manning.com/books/designing-kafka-systems](https://hubs.la/Q041FhV20)\n\n[Kafka for Architects](https://preview.redd.it/guav3ysxachg1.jpg?width=2213&format=pjpg&auto=webp&s=fc59d0f2fef718c70d40b4996d59b6f879992605)\n\nThis isnâ€™t a Kafka API guide or a step-by-step tutorial. It stays at the architecture level and focuses on how Kafka fits into larger systems, especially in organizations where multiple teams depend on the same infrastructure.\n\nA few of the topics the book spends real time on:\n\n* Kafkaâ€™s role in enterprise software and where it fits in an overall system design\n* Event-driven architecture as a pattern, including when it helps and when it complicates things\n* Designing data contracts and handling schema evolution across teams\n* Kafka clusters as part of the systemâ€™s operational and organizational design\n* Using Kafka for logging, telemetry, data pipelines, and microservices communication\n* Patterns and anti-patterns that tend to appear once Kafka becomes shared infrastructure\n\nWhat I appreciate about this book is that it treats Kafka as an architectural choice, not just a technology. Katya walks through trade-offs youâ€™ll recognize if youâ€™ve ever had to balance team autonomy, data ownership, and long-term maintainability. The examples are grounded in real-world systems, not idealized diagrams.\n\nIf youâ€™re responsible for questions like â€œIs Kafka the right fit here?â€, â€œHow do we keep event contracts stable?â€, or â€œWhat happens when this system grows to ten teams instead of two?â€, this book is written with those concerns in mind.\n\n**For the** r/softwarearchitecture **community:**  \nYou can get **50% off** with the code **PBGORSHKOVA50RE**.\n\nIf youâ€™re already using Kafka as part of a larger system, Iâ€™d be interested to hear what architectural challenges youâ€™re currently dealing with.\n\nThanks for having us. It feels great to be here.\n\nCheers,\n\nStjepan",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qv3nr2/kafka_for_architects_designing_kafka_systems_that/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3g22dn",
          "author": "Glathull",
          "text": "Thanks for posting this. Just bought a copy.",
          "score": 2,
          "created_utc": "2026-02-04 00:49:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ksoes",
              "author": "ManningBooks",
              "text": "Thank you.",
              "score": 1,
              "created_utc": "2026-02-04 18:44:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3etpgb",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 0,
          "created_utc": "2026-02-03 21:03:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3f1op3",
              "author": "Mosk549",
              "text": "Donâ€™t be racist",
              "score": 0,
              "created_utc": "2026-02-03 21:40:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3f275o",
                  "author": "AbbreviationsLow4798",
                  "text": "donâ€™t support fascistsÂ ",
                  "score": -2,
                  "created_utc": "2026-02-03 21:42:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzib6d",
      "title": "Caching in 2026: Fundamentals, Invalidation, and Why It Matters More Than Ever",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/caching-in-2026-fundamentals-invalidation-and-why-it-matters-more-than-ever-867fee46e98b",
      "author": "trolleid",
      "created_utc": "2026-02-08 19:30:26",
      "score": 30,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qzib6d/caching_in_2026_fundamentals_invalidation_and_why/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwja9l",
      "title": "LinkedIn Re-Architects Service Discovery: Replacing Zookeeper with Kafka and xDS at Scale",
      "subreddit": "softwarearchitecture",
      "url": "https://www.infoq.com/news/2026/02/linkedin-service-discovery/",
      "author": "rgancarz",
      "created_utc": "2026-02-05 11:53:23",
      "score": 24,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qwja9l/linkedin_rearchitects_service_discovery_replacing/",
      "domain": "infoq.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3rzlg6",
          "author": "IlliterateJedi",
          "text": "The actual source: [Scalable, multi-language service discovery at LinkedIn](https://www.linkedin.com/blog/engineering/infrastructure/scalable-multi-language-service-discovery-at-linkedin)",
          "score": 3,
          "created_utc": "2026-02-05 20:08:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3regod",
          "author": "BarfingOnMyFace",
          "text": "GIGO at scale",
          "score": 1,
          "created_utc": "2026-02-05 18:30:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvho5v",
      "title": "Fitness Functions: Automating Your Architecture Decisions",
      "subreddit": "softwarearchitecture",
      "url": "https://lukasniessen.medium.com/fitness-functions-automating-your-architecture-decisions-08b2fe4e5f34",
      "author": "trolleid",
      "created_utc": "2026-02-04 07:00:48",
      "score": 23,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qvho5v/fitness_functions_automating_your_architecture/",
      "domain": "lukasniessen.medium.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r08fhs",
      "title": "Event sourcing vs event streams",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1r08fhs/event_sourcing_vs_event_streams/",
      "author": "Ok-Scientist9904",
      "created_utc": "2026-02-09 16:15:18",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 0.87,
      "text": "I am having a fairly hard time try to differentiate at a high level how event sourcing and event streams are different. Is it just that event sourcing came from DDD world and event streams from the internet companies. Both give me immutability, both allow me to build my views/projections from the events, both give me audit, both allow other processes to listen and do something. So are they the same?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1r08fhs/event_sourcing_vs_event_streams/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o4gczco",
          "author": "flavius-as",
          "text": "Event sourcing goes through an event stream, one event at a time, to create a projection into an aggregate root.",
          "score": 10,
          "created_utc": "2026-02-09 16:17:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hdfkt",
          "author": "jacobatz",
          "text": "Event sourcing means using events as the source of truth for the state of your system. Ie. you need to project the state from the recorded events to know the state of the system.\n\nEvent streaming on the other hand just means using a stream of events for some purpose. For instance to communicate change to the domain between systems.",
          "score": 7,
          "created_utc": "2026-02-09 19:10:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ge6sa",
          "author": "ch1pch4p",
          "text": "No, not the same. \n\nEvent Sourcing - events are stored sequentially and can be used to recreate a state of the app (like you said, be projected from to represent a relational data model, if you so pleased). \nEvent Streaming - could mean one or more things: how the events are transported (think Kafka or Kinesis). Or could mean instead of batching things, it's done one after another, which then those streams can be \"widened\" to allow for more parallel event processing to take place\n\nAt least that's my 0.05 cents (bye bye pennies)\n\nBig morale of the story - Ubiquitous language strikes again. Might be a good app idea... Tech Dictionary! Probably already out there.",
          "score": 6,
          "created_utc": "2026-02-09 16:23:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4gmw9x",
              "author": "gbrennon",
              "text": "ðŸ¤£",
              "score": 1,
              "created_utc": "2026-02-09 17:04:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4imaxe",
          "author": "No_Package_9237",
          "text": "https://event-driven.io/en/event_streaming_is_not_event_sourcing/ is an excellent article on the topic you mention",
          "score": 3,
          "created_utc": "2026-02-09 22:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gr2uk",
          "author": "kqr_one",
          "text": "event sourcing - storing data within module \n\nevent stream - moving data between modules",
          "score": 2,
          "created_utc": "2026-02-09 17:24:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4gs13l",
          "author": "ufukty",
          "text": "They are as same as Java and JavaScript ;)\n\nThere is a quite good conference recording on YouTube explaining many reasons and quirks of event sourcing in a very short amount of time and in a very clear way. I donâ€™t remember the name sadly. He was talking about ES experience in banking, cost of ES decreasing with storage prices, storing previous versions of the application to be able to restore the previous state from snapshots later etc.",
          "score": 2,
          "created_utc": "2026-02-09 17:29:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4hyuot",
          "author": "utilitydelta",
          "text": "Event Sourcing == deterministic, per-aggregate ordering && optimistic concurrency control on append. that's an event sourcing database. Everything else is Event Streaming. For example, if Kafka implemented [https://issues.apache.org/jira/browse/KAFKA-2260](https://issues.apache.org/jira/browse/KAFKA-2260) you could classify it as an event sourcing database, as long as you used the aggregate id as the message key and never re-balanced partitions.",
          "score": 2,
          "created_utc": "2026-02-09 20:55:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l2hgi",
          "author": "lutzh-reddit",
          "text": "Unfortunately \"event stream\" is not very well defined. It's also used for a sequence of events in event sourcing that are connected, e.g. relate to the same entity. Case in point: [https://docs.kurrent.io/getting-started/concepts.html#event-stream](https://docs.kurrent.io/getting-started/concepts.html#event-stream)",
          "score": 1,
          "created_utc": "2026-02-10 08:55:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4l2tuo",
          "author": "lutzh-reddit",
          "text": "Event Sourcing: A persistence strategy within a service.  \nEvent Streaming: A communication pattern between multiple services.\n\nI try to explain it in this blog post: [https://www.reactivesystems.eu/2022/06/09/event-collaboration-event-sourcing.html](https://www.reactivesystems.eu/2022/06/09/event-collaboration-event-sourcing.html)\n\nhttps://preview.redd.it/m3pi4cq8tmig1.jpeg?width=640&format=pjpg&auto=webp&s=763c7eb44f577268754943578d055c420d901384\n\n",
          "score": 1,
          "created_utc": "2026-02-10 08:58:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4irh64",
          "author": "paradroid78",
          "text": "An event stream is exactly what it says. A stream of events. It makes no statement about where the events come from, what they mean, or how they should be consumed. As per another comment, it's just a way of moving data around.\n\nEvent sourcing is when your actual source of truth is a list of events rather than a traditional relational model, and the only way to reason about the current state of your data is to replay those events and aggregate them into a meaningful model (which is usually snapshotted at regularly intervals to save you having to replay every event from the start of time just to run simple queries against your data). It's inspired by accounting ledgers, and typically paired with CQRS, which means different consumers of your events can do their own aggregation and maintain their own \"projections\".\n\nIn its most purist form, this is often not a great fit for a lot of problem spaces, so a lot of real-world systems are built on hybrid architectures instead, meaning they adopt parts of event sourcing, but the system of record remains a traditional database.",
          "score": 0,
          "created_utc": "2026-02-09 23:22:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvk9w9",
      "title": "A Scalable Monorepo Boilerplate with Nx, NestJS, Kafka, CQRS & Docker â€” Ready to Kickstart Your Next Project",
      "subreddit": "softwarearchitecture",
      "url": "https://github.com/ARG-Software/Nx-Monorepo-Boilerplate",
      "author": "FormalAd7608",
      "created_utc": "2026-02-04 09:41:00",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 0.79,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qvk9w9/a_scalable_monorepo_boilerplate_with_nx_nestjs/",
      "domain": "github.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3iwqpc",
          "author": "nickchomey",
          "text": "This has to be a joke... Bull, redis AND kafka?Â ",
          "score": 3,
          "created_utc": "2026-02-04 13:17:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iypfb",
              "author": "FormalAd7608",
              "text": "You just use what you want. You can use them all or just one. One is for messaging, other for background jobs, other for caching. Just pick what you like the most.",
              "score": 1,
              "created_utc": "2026-02-04 13:28:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3j2pjg",
                  "author": "nickchomey",
                  "text": "You've missed the point. Surely 3 separate tools are not needed here. Redis (or, better yet, NATS) could do it all",
                  "score": 2,
                  "created_utc": "2026-02-04 13:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3jqw5t",
              "author": "Ok_Cranberry4354",
              "text": "Whats your definition of a joke? For example, are you going to implement something similar to a payment system that needs real-time data streaming, replayability, durable event logs and a bunch of other things like strong delivery guarantees with NATS instead of Kafka which is industry standard when talking about scalability? NATS can do some of these but with implicating limitations.\n\nYou can technically force one tool to do everything but that doesnâ€™t mean you should. At least looking at the repo, this is clearly a boilerplate, not a prescription, that's the idea of a boilerplate, you use a baseline that fits your needs the most and remove what you don't care about. If you're going to leave a comment like that on someone else's free open source contribution at least give some some architectural insight instead of just a reaction that seems pretty personal.",
              "score": 1,
              "created_utc": "2026-02-04 15:51:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3juoks",
                  "author": "nickchomey",
                  "text": "NATS was not the thrust of my comment - it was literally parenthetical to saying Redis could be used standalone. Though, I do stand by it. I'd be curious what the \"implicating limitations\" that you speak of are.\n\nAnd just because something is \"standard\" doesn't make it an appropriate boilerplate starting point - those tend to do best when refined, only bringing on more dependencies and services when absolutely needed. Even still, why not Redpanda over Kafka?\n\nMoreover, if you were to extend that \"standard\" line of thinking to the rest of the choices here, why use NestJS instead of something React-based? (I'm aware Nest and next.js etc are not all that comparable and am most definitely not advocating for anything React-based. Just making the point that Nest isn't an \"industry standard\")",
                  "score": 1,
                  "created_utc": "2026-02-04 16:09:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o49otr2",
          "author": "tesseraphim",
          "text": "Lol, I thought you were building a monorepo, and thought the stack was click bait.",
          "score": 0,
          "created_utc": "2026-02-08 15:53:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwo81f",
      "title": "Architecture Question: Modeling \"Organizational Context\" as a Graph vs. Vector Store",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qwo81f/architecture_question_modeling_organizational/",
      "author": "altraschoy",
      "created_utc": "2026-02-05 15:26:51",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.8,
      "text": "Iâ€™m working on a system to improve context retrieval for our internal AI tools (IDEs/Agents), and Iâ€™m hitting a limit with standard Vector RAG.\n\nThe issue is structural: Vector search finds \"similar text,\" but it fails to model typed relationships (e.g., `Service A` \\-> `depends_on` \\-> `Service B`).\n\nWe are experimenting with a Graph-based approach (hello arangodb x)) where we map the codebase and documentation into nodes and edges, then expose that via an MCP (Model Context Protocol) server.\n\nThe Technical Question: Has anyone here successfully implemented a \"Hybrid Retrieval\" system (Graph + Vector) for organizational context analysis?\n\nIâ€™m specifically trying to figure out the best schema to map \"Soft Knowledge\" (Slack decisions, PR comments and all the jazz that a PM/PO can produce) to \"Hard Knowledge\" (code from devs/qa) without the graph exploding in size.\n\nWould love to hear about any data structures or schemas youâ€™ve found effective for this.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qwo81f/architecture_question_modeling_organizational/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3qn3w9",
          "author": "Elvez-The-Elf",
          "text": "Is this an architecture question?",
          "score": 5,
          "created_utc": "2026-02-05 16:23:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3rla4y",
              "author": "asdfdelta",
              "text": "Yes. Architecture requires organizational context to be designed correctly. For example, Conway's Law",
              "score": -5,
              "created_utc": "2026-02-05 19:01:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3s7fkp",
                  "author": "Elvez-The-Elf",
                  "text": "I agree with your statement but still canâ€™t relate it to the post. If I were given this task I would ask my senior AI/Data Engineers for help, not our architects.",
                  "score": 2,
                  "created_utc": "2026-02-05 20:45:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rsdeb",
          "author": "erotomania44",
          "text": "Graph rag was hype. \n\nRead up on anthropicâ€™s contextual retrieval blog post. \n\nItâ€™s the only way to do search with RAG today.",
          "score": 2,
          "created_utc": "2026-02-05 19:34:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4818dq",
          "author": "Potential-Analyst571",
          "text": "The trick to avoid graph explosion is only promoting durable facts into nodes and keeping everything else as linked evidence blobs with TTL or summarization. For wiring and debugging the pipeline, tools like LangSmith or Traycer AI help keep retrieval and changes traceable so you can see why the model pulled a given context....",
          "score": 1,
          "created_utc": "2026-02-08 08:36:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwlfp2",
      "title": "Architecture for Flow â€¢ Susanne Kaiser & James Lewis",
      "subreddit": "softwarearchitecture",
      "url": "https://youtu.be/NwtE82Wzs_U?list=PLEx5khR4g7PJbSLmADahf0LOpTLifiCra",
      "author": "goto-con",
      "created_utc": "2026-02-05 13:35:15",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qwlfp2/architecture_for_flow_susanne_kaiser_james_lewis/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qvpcml",
      "title": "key value storage developed using sqlite b-tree APIs directly",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qvpcml/key_value_storage_developed_using_sqlite_btree/",
      "author": "Fine-Package-5488",
      "created_utc": "2026-02-04 14:02:09",
      "score": 8,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "SNKV ([https://github.com/hash-anu/snkv](https://github.com/hash-anu/snkv)) is a keyâ€“value store implemented directly on top of SQLiteâ€™s B-Tree APIs.  \nIt bypasses the SQL query layer and performs operations using SQLiteâ€™s internal B-Tree interface, reducing overhead compared to SQL-based access paths.\n\nBenchmark evaluations on mixed workloads show approximately \\~50% performance improvement compared to equivalent SQL queryâ€“based operations.\n\nFeedback on the design, implementation choices, performance characteristics, and potential areas for improvement would be welcome.\n\nA usage walkthrough is available here:  \n[https://github.com/hash-anu/snkv/blob/master/kvstore\\_example.md](https://github.com/hash-anu/snkv/blob/master/kvstore_example.md)",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qvpcml/key_value_storage_developed_using_sqlite_btree/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qw03vu",
      "title": "Clean code architecture and codegen",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qw03vu/clean_code_architecture_and_codegen/",
      "author": "Aggressive_Ad_699",
      "created_utc": "2026-02-04 20:35:06",
      "score": 8,
      "num_comments": 16,
      "upvote_ratio": 0.72,
      "text": "I'm finally giving in and trying a stricter approach to architecting larger systems. I've read a bunch about domains and onions, still getting familiar with the stuff. I like the loose coupling it provides, but managing the interfaces and keeping the structures consistent sounds like a pain.\n\nSo I started working on a UI tool with a codegen service that can generate the skeletons for all the ports, and services, domain entities and adapters. It'll also keep services and interfaces in sync based on direct code changes as well. I also want to provide a nice context map to show which contexts rely on other contexts. It'll try to enforce the basic rules of what structural elements can use, implement or inject others. I'll probably have a CLI interface that complements the UI which could be used in pipelines as well to validate those basic rules. The code will remain mostly directly editable. I'm aiming to do this for Python at first, but it doesn't seem too complicated to extend to other languages.\n\nThoughts about the usefulness of such a tool or clean code / DDD in general?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qw03vu/clean_code_architecture_and_codegen/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3lsz8p",
          "author": "UnreasonableEconomy",
          "text": "This is my personal opinion of course\n\n- Robert Martin: ðŸš©\n\n---\n\n> I'm finally giving in an trying a more strict approach to architecting larger systems.\n\nCorrect me if I'm wrong, but this sounds like \"nothing I've tried so far worked, so now I'll just do BDUF by the book\"\n\nThis is probably not gonna work out all that well either, but it depends on what you're trying to do.\n\n> So I started working on a UI tool with a codegen service that can generate the skeletons for all the ports, and services, domain entities and adapters\n\nThere have been efforts of this sort since time immemorial, and none of them have really ever stuck around or become universal. \n\nI however don't think it's a waste of your time (if you have the time) to pursue this - you'll learn all the problems associated with these types of prescriptive architectural styles. You'll find out what does and doesn't work. You'll become a bit better at making high level decisions.\n\nSA is as much an art form as it is engineering. Practice and experience are unfortunately no substitute for what you can learn from books.",
          "score": 9,
          "created_utc": "2026-02-04 21:35:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3lyhxr",
              "author": "Aggressive_Ad_699",
              "text": "Thanks, I think that's a good take. Even if it doesn't become an everyday tool for me, let alone others I can still solidify my knowledge about this kind of architecture. Do you know why these tools don't seem to stay around or reach more people?\n\nTo clarify, I've mostly either worked on large legacy systems where I wasn't a part of most of the architectural decisions, or smaller green field projects that I'm yet to see grow to a medium size. I've been told a bunch of times that this kind of architecture isn't worth it, or it's too academic and overcomplicated. So I'm really giving into my own desires to see how it works for me:)",
              "score": 2,
              "created_utc": "2026-02-04 22:01:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3m6zhg",
                  "author": "UnreasonableEconomy",
                  "text": ">  Do you know why these tools don't seem to stay around or reach more people?\n\nI think that's a very good and hard question. I'd love the opinions of others here on this. \n\nMy take is that these tools encode not just a particular style, but define a framework by the nature of how they work. If you want to do anything, you have to do it in a particular way.\n\nSo these \"tools\" \"become\" \"frameworks\". \n\nWhat's a framework? I'd say it's an enforced collection of patterns. (which is what you'll do - you'll select a finite set of patterns that this tool will realize)\n\nBut what's a pattern? Why do we use patterns? I'd say, we use patterns to work around the shortcomings of some environment. It's a way of dealing with reality so we can achieve the outcomes we need. \n\nThe problem is that any finite selection of patterns can only cover a subset of the continuum of implementations required for our business cases. To deal with this, developers sometimes come up with new patterns to deal with the canonical way of doing things. Sometimes that works out fine. Sometimes it doesn't make sense at all.\n\nAs languages and environments and patterns evolve, it sometimes stops making sense to bend over backwards to appease the framework, and working outside of the framework becomes easier. At some point the frameworks becomes either so sidelined or adapted and specialized so it stops being the universal panacea it was supposed to be.\n\nAnd then someone invents a new framework to supplant all these specializations, and the cycle begins anew \n\n# ðŸ¤”\n\n---\n\n> I've been told a bunch of times that this kind of architecture isn't worth it, or it's too academic and overcomplicated. So I'm really giving into my own desires to see how it works for me\n\nI think that's good. There's a nugget of truth in everything, and if you have the energy and time and will to prospect for that bit of truth, that's perfect. That's probably the best way to become a good architect, especially if no one else has to suffer from your exploratory decisions lol.",
                  "score": 3,
                  "created_utc": "2026-02-04 22:44:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mlp32",
              "author": "trainbustram",
              "text": "I do want to say that at many large automotive companies, this tooling to define interfaces explicitly actually does exist and is actually used very often in software modeling, where you explicitly define all of your ports deployments, etc. then generate network and internal artifacts based on that method. Definitely much more useful in a distributed monoliths architecture where boundaries can shift easily from internal to external, rather than in a micro services architecture where the boundaries give or take like the same at all points in time.",
              "score": 2,
              "created_utc": "2026-02-05 00:04:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3phq6d",
                  "author": "Aggressive_Ad_699",
                  "text": "I suppose those are proprietary tools, right? Do you have an example in mind? \nHmm it's interesting you brought up microservices. It might be possible to easily reorganise contexts across repos as well. That's certainly out of scope for now. I'm going to focus on large monorepos first.",
                  "score": 1,
                  "created_utc": "2026-02-05 12:46:20",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oukk7",
          "author": "edgmnt_net",
          "text": "Possibly hot take here, but merely adding some indirection and layers does *not* make your code loosely-coupled in a meaningful way, it's more like increasing effort, surface for bugs and making it more difficult to refactor. It is a pain because it is a pain. The fact that you're considering code generation is sort of a red flag and generating skeletons won't help when you get hit with a 3k lines PR for what would otherwise be a much simpler change. IMO people should stop this indiscriminate layering nonsense and focus on actual abstractions and designing actual robust APIs (when possible and needed, otherwise it's perfectly fine to write code in a direct style). I'm allowing for indirection where there are particular pain points and people stepping too much on each other's toes, but you need to be conservative about it.",
          "score": 3,
          "created_utc": "2026-02-05 09:32:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pimfr",
              "author": "Aggressive_Ad_699",
              "text": "Not at all, it's a cautionary advice. This was one of the main reasons I've been a bit afraid to look into clean code. You mention focusing on actual abstractions and robust APIs. Do you think the clean architecture with dependency inversion, ports, services, adapters, etc... is on the other end of things? If so could you elaborate on what kind of abstractions/patterns you have in mind?",
              "score": 1,
              "created_utc": "2026-02-05 12:52:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3ptcvq",
                  "author": "edgmnt_net",
                  "text": "For example, compilers often have IRs (intermediate representations) that are carefully considered to allow translation of the code as well as optimizations (which might require combining knowledge of different languages or different CPUs without ending up with a combinatorial explosion of corner cases, some of those pseudoinstructions might not even resemble any concrete instruction). A database may provide a set of primitive operations that are enough to write applications within a certain consistency model and with certain transactional capabilities, possibly with a higher-level / convenience API on to make it easier for simple use cases. A compression library provides APIs that are generally robust for a wide variety of use cases so you don't need to go make changes to it to use it in your own application (and they're not changing the API surface all the time). An operating system kernel needs to provide a driver model such that a diverse set of hardware devices can be managed (some only require initialization, some require to be notified before being powered-down and so on). A JSON parser might provide both streaming and non-streaming parsers (building the entire representation in memory upfront) in a convenient way and for a wide variety of users. All these tend to require rather careful consideration.\n\nIn contrast and at least in a practical sense, stuff related to layered architectures often tends to be applied blindly and as a general recipe, being little more than arbitrary scaffolding. This tends to be compounded by people trying to split work top-down in a trivial way. It's easy to say \"hey, someone should do auth and someone should do the books endpoint\". But then the auth stuff could just be one or two calls into the framework / auth library and it's instead blown to an entire component that barely adds anything (and might even impose needless restrictions). In such cases nobody's really doing any real work of abstracting stuff, they're just writing wrappers for straightforward calls.\n\nSupposedly this sometimes protects against changing requirements but I find that's usually not true, it's just a place where you end up putting ugly hacks that would have been better fixed by large-scale refactoring. It reduces visibility into code and changes because everything is 7 useless layers deep. It makes it \"easy\" to write 100k LOC of code that barely does anything concrete. It makes it hard to write composable helpers because everything is encapsulated too tightly yet it's not robust enough to handle all reasonable use cases. And to some degree it shouldn't be, that's what the framework is for, while you're writing a very specific and concrete application.\n\nOn a somewhat related note, I think a good and even simpler test for code writing ability is to look at how people write functions/methods. Do they split them wisely? One could do either of (1) one big function or (2) a hundred very small functions that presume a bunch of invariants (\"I'm always being passed a non-empty array of at most 3 elements, I'll crash otherwise\"). Those are both pretty poor choices, usually. Or they can be mindful about stuff and split on natural boundaries and where there's least resistance, for example by making a helper that compares JSON objects a certain way and makes at least some sense on its own (either for DRY purposes or simply because it's clearer / more testable on its own). This kind of soft separation is very useful, because you're grouping things logically, making them easier to write / review / confirm they're working, while also allowing the possibility of refactoring at will. But it's not something that's just a recipe and it heavily depends on experience and what the code really does.\n\nThat being said, some layering may be fine, though. It's just that you have to be conservative about it because it has a cost. And it's often overused.",
                  "score": 1,
                  "created_utc": "2026-02-05 13:55:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oaibl",
          "author": "thecreator51",
          "text": "This sounds very useful. DDD and clean architecture help long-term maintainability, but managing interfaces is pain. Codegen skeletons and validation can save time and enforce consistency early.",
          "score": 2,
          "created_utc": "2026-02-05 06:25:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pixcv",
              "author": "Aggressive_Ad_699",
              "text": "I'm also thinking of something lean. Modern IDEs do a lot, I don't want to attempt to replace things they already do well, just augment their capabilities with the more opinionated rules of this architecture.\n\nHow would you prefer interacting with the tool?\n- Direct schema editing\n- A nice terminal UI\n- A web interface\n\nI want a simple CLI interface as well that can be called from IDE file watchers to update linked interfaces on save for example. The web UI might be slower to work with, but the context map and dependency map it can show might be useful for brainstorming.",
              "score": 1,
              "created_utc": "2026-02-05 12:54:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxk1uk",
      "title": "Should the implementation of Module.Contract layer be in Application or Infra? Modular monolith architecture",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qxk1uk/should_the_implementation_of_modulecontract_layer/",
      "author": "Illustrious-Bass4357",
      "created_utc": "2026-02-06 15:06:19",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 0.74,
      "text": "if I have a modular monolith where modules need to communicate ( I will start with in memory, sync communication ) \n\nI would have to expose a contract layer that other modules can depend on , like an Interface with dtos etc \n\nbut if I implement this contract layer in application or Infra, I feel it violates the dependency inversion like a contract layer should be an outer layer right? ,if I made the application or infra reference the contract , now application/infra  is dependent on the contract layer \n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qxk1uk/should_the_implementation_of_modulecontract_layer/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3x8bsj",
          "author": "etxipcli",
          "text": "The contract is an abstract description of the functionality the module provides. Think of it less as a code artifact and more like metadata and that might help with the DI concern. Whatever code is using your module still is dependant on what the module does, but it is not dependent on the module itself if that makes sense. The implementation at that point is incidental since all you care about is the functionality.Â ",
          "score": 2,
          "created_utc": "2026-02-06 16:10:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x9jdy",
              "author": "etxipcli",
              "text": "I'm in Java world, so SLF4J is a good example. We depend on SLF4J but ultimately some logging framework will be doing what matters. Here you will have your one implementation, but the purpose of the abstraction is the same.",
              "score": 1,
              "created_utc": "2026-02-06 16:16:26",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3xo577",
              "author": "Illustrious-Bass4357",
              "text": ">The contract is an abstract description of the functionality the module provides.\n\nyeah like its an interface like this for example  \n\n\n    namespace Restaurants.Contracts\n    { \n      public interface IRestaurantServices\n        {\n            Task<bool> ExistsAsync(Guid restaurantId);\n        }\n    }\n\n\n\n>Whatever code is using your module still is dependent on what the module does, but it is not dependent on the module itself if that makes sense.\n\nit kinda makes sense, but still I think it's dependent on the contract layer of the module, cause it's referencing it , like if I deleted the Module , I would have to to also remove the project reference and make another project that does the same functionality \n\n  \n\n\nalso I was trying to reason of where should the implementation of the contract layer go, and I reached a point where it kinda makes sense to me, that it should go to the application layer, but I shouldn't treat it as a separate layer conceptually , like its an application level functionality but I don't want other modules to depend on my application layer cause that would violate the DI principle , so the contract layer conceptually is still part of the application layer but I make this abstraction so other modules don't interact with the application directly   \n \n\nhttps://preview.redd.it/3z396nl8qwhg1.png?width=905&format=png&auto=webp&s=1a435b71cbd874f8b5cea1aadf3f9edbf2f3e890\n\nthat's my current mental model\n\n  \nAm I understanding this correctly?",
              "score": 1,
              "created_utc": "2026-02-06 17:25:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xrmpc",
                  "author": "etxipcli",
                  "text": "Don't want to speak too authoritatively in general, but your understanding that there is a dependency in the contract of the module is correct.\n\n\nI am used to the contract being it's own module. I see you using the word layer, but the way they should be organized is side by side.Â  Like have a API module and any number of implementation modules.",
                  "score": 2,
                  "created_utc": "2026-02-06 17:41:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o3xr0dw",
                  "author": "Mental-Artist7840",
                  "text": "Contracts should be in the core layer (domain in your case).\n\nThis is because the application layer usually depends on the contract interface within their services. \n\nThe implementation for your repository should be in the infrastructure layer. Therefore both the application layer and infra layer depend on the contract.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:38:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3z3yb1",
          "author": "RST1997",
          "text": "In a modular monolith setup I would consider the contracts to be sort of part of the application layer.\nHowever, in code I would make them two different â€˜projectsâ€™ (as in C# definition of a project).\nWhere other modules reference the contracts package and program against the defined interfaces.\nThen I would have the application implement these interfaces.\n\nThe application layer will most likely also have interface defined. For example an IRestaurantsRepository, the actual repository implementation will then be in the infrastructure layer.\n\nSo contracts will have the IRestaurantService interface.\nApplication will have the IRestaurantsRepository interface and RestaurantService (which implements IRestaurantService).\nInfra will implement the repository.\n\nIt might feel like the dependencies are not all pointing inwards. But they are in the sense that details still depend on policies/abstractions.",
          "score": 2,
          "created_utc": "2026-02-06 21:37:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43md9u",
          "author": "steve-7890",
          "text": "I think you complicate it more than you need. If you read about modular design, you gonna see there's no \"contract layer\" (you asked for modular design, not layered architecture after all). \n\nIn practice, in lean design (as opposite to enterprise designs), there are two common cases are:\n\n1. Module A delegates its infrastructure code to Module A'. In such case module A defines all the interfaces, and module A' implements them. Example: \"DeviceRegistration\" and \"DeviceRegistration - Infrastructure\" modules. This way \"DeviceRegistration\" has no infra code and doesn't depend on the \"DeviceRegistration - Infrastructure\".\n\n2. Module A needs to delegate flow to Module B. In such case there is a direct dependency, and no amount of tricks with separate abstraction layer won't change it. Put the interfaces in the upstream module, and let downstream module implement them. It the flow is unidirectional, direct dependency between modules, where you invoke classes of the mobule B is fine. It could be as easy as exposing a Facade Pattern. \n\nI was many Java or C# implementations with many abstraction layers, but I think that in the end it's Go that nailed it. It's even built-in into its philosophy by \"consumers to define interfaces instead of producers\" mantra.",
          "score": 1,
          "created_utc": "2026-02-07 16:14:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ygcmx",
          "author": "flavius-as",
          "text": "Your mind model would benefit tremendously if you'd use the terminology from ports and adapters.\n\nYour words \"application\", \"infra\" etc are highly interpretable.\n\nWhoever answers your question does so in their own world model, not yours, and there might be mismatches.\n\nP&A is simple and universal.\n\nTo answer what I think you asked, in Hexagonal terms:\n\nYour contracts should be the outer part of your domain model (=application in hexagonal), and adapters can depend on that.\n\nContracts are for example: DTOs, entities, value objects, repository interfaces (not implementations), use cases.\n\nYour adapters can depend on those, use them respectively implement them.\n\nDirection of dependencies is not violated this way.",
          "score": 1,
          "created_utc": "2026-02-06 19:39:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yvxgw",
              "author": "SolarNachoes",
              "text": "Their terms are from Clean Architecture.\n\nThey are just asking where to put the contract classes often called domain entities and interfaces in a clean architecture.",
              "score": 1,
              "created_utc": "2026-02-06 20:57:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzjvzh",
      "title": "Functional<>Logical<>Physical Architecture in Software Intensive Systems",
      "subreddit": "softwarearchitecture",
      "url": "/r/systems_engineering/comments/1qzjvfa/functionallogicalphysical_architecture_in/",
      "author": "PeakofConsciousness",
      "created_utc": "2026-02-08 20:29:56",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qzjvzh/functionallogicalphysical_architecture_in/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o4d9nl5",
          "author": "asdfdelta",
          "text": "Your question is correct, you are overcomplicating it.\n\nBehaviors, functions, and requirements are not sequential steps but viewpoints of the same thing.\n\nIn software, we create systems that are n-dimensional in nature. It is a simple structure of logical components, but behavior and requirements (among many other things) are all constructs humans layer on top of the logical components to make sense of it all. It exists detached and purely for our benefit, so that we may design more elegant software systems. The software itself doesn't benefit from behavior analysis or architectural fitness or performance... It just *does*. Same as with Systems Engineering.\n\nSo, separating the real from the imagined we can see that the behavior of a system is one viewpoint we would design. The requirements is another synonym of behavior, but a subset of behaviors that is strictly required. Non-functional requirements are still required, just harder for stakeholders to articulate. Things that if catastrophe arrived, the stakeholders would say 'of course I want that!' Do they want it reasonably performant? Do they want it to be observed so errors are clear and remedies quickly? Do they want failover in case a data center goes out?\n\nAll that to say, your perception of a sequential flow of designing a software intensive system is incorrect.\n\n1) Start with functional requirements\n2) Add non-functional requirements\n3) Identify True Constraints (versus imagined constraints)\n4) Decide what to optimize for, in pursuit of the larger goal\n5) Use comfortable tools & techniques to design your system\n6) Use the previous 5 steps as a guide to redesign what you just made, but to be easily manufactured (coded, released, maintained, and deprecated)",
          "score": 3,
          "created_utc": "2026-02-09 02:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxxpvb",
      "title": "Hello, I have big project contract basically signed so I need little guidelines",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qxxpvb/hello_i_have_big_project_contract_basically/",
      "author": "dbo4444",
      "created_utc": "2026-02-06 23:36:08",
      "score": 6,
      "num_comments": 14,
      "upvote_ratio": 0.64,
      "text": "Hello, \n\ni have project that I have to start working on something like real estate platform where users can publish their own real estates for sale. So quite big project. I have 6-7 years experience in software development but mostly ERP and CRM systems, maintaining legacy code and few small and medium websites and web applications built but never something this \"wide\".\n\nTech stack that I will be using **Vue.js + PHP + SQL** because it is something that I have done before and most experienced with (out of those programming languages that you do not have to spend 2000$+ to have licence). \n\n  \nI am still looking at some examples and staring to write down directions that I have to follow but nothing major and not unexpected. \n\nSo, questions for more experience colleagues, where would you start and what to do first...anything that you think would help me?  \n  \nThanks ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qxxpvb/hello_i_have_big_project_contract_basically/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o3zt8u9",
          "author": "GrogRedLub4242",
          "text": "you are literally paid to do that. do it. otherwise pay us",
          "score": 38,
          "created_utc": "2026-02-06 23:53:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4007k2",
              "author": "halfxdeveloper",
              "text": "Right?",
              "score": 4,
              "created_utc": "2026-02-07 00:33:34",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4c4v5l",
              "author": "Gunny2862",
              "text": "Hahahahaha",
              "score": 3,
              "created_utc": "2026-02-08 23:09:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o400bbi",
          "author": "halfxdeveloper",
          "text": "Donâ€™t spend all the money you make because theyâ€™ll want a refund when they find out you donâ€™t know what youâ€™re doing.",
          "score": 25,
          "created_utc": "2026-02-07 00:34:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44rmji",
          "author": "fteq",
          "text": "Start with the boring stuff: scope + MVP + acceptance criteria. For a real estate platform, â€œwideâ€ explodes fast (roles, listings, messaging, moderation, payments, SEO, maps, spam). Iâ€™d write a 1â€“2 page PRD: user roles, core flows, must-have vs later, and what â€œdoneâ€ means. Then design the data model early (Listings, Users, Media, Locations, Status, Audit logs). Also: nail non-functional requirements (security, GDPR/PII, backups). Since you said the contract is basically signed, run it through AI Lawyer to make sure deliverables, milestones, and change-order language protect you before scope creep hits.",
          "score": 3,
          "created_utc": "2026-02-07 19:39:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o459ruw",
          "author": "No_Flan4401",
          "text": "Start gathering requirement, get a idea on the domain, settle on scope for MVP. From there find the obvious architecture patterns, and start with the simplest and post phone major design to as late as possible.Â \n\n\nOut of interest what languages requires license? I think I know zero",
          "score": 2,
          "created_utc": "2026-02-07 21:17:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o403l34",
          "author": "zangler",
          "text": "So...you basically still want to support legacy for then?",
          "score": 1,
          "created_utc": "2026-02-07 00:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40a4n6",
          "author": "kokoricky",
          "text": "Thatâ€™s just a simple crud, a monolithic is all you need. The backend wonâ€™t be anything special as long as u cache aggressively, your frontend will need some optimisations to make scrolling smooth with data fetching.\nYour most complicated component would be the postgresdb schema and overall deployment.",
          "score": -1,
          "created_utc": "2026-02-07 01:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40zl0a",
          "author": "commanderdgr8",
          "text": "If php, then there would be some readymade project already build, you can use that directly or look at the code and build yourself. You can ask claude also to build for you.",
          "score": 0,
          "created_utc": "2026-02-07 04:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o420tb4",
          "author": "breek727",
          "text": "I would find out what their anticipated running monthly costs are and look to no code as much as possible to get something up and running and then slowly migrate to custom pieces as needed, how many users are they expecting up front etc.",
          "score": 0,
          "created_utc": "2026-02-07 09:41:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o404de3",
          "author": "roynoise",
          "text": "dm me if you'd like a coach. We can split the project.",
          "score": -1,
          "created_utc": "2026-02-07 00:57:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3zswzt",
          "author": "TheMightyTywin",
          "text": "Make sure youâ€™re getting some money paid up front or are being paid by the hour, this is a big project that will take at least a year.\n\nYou should create a separate backend and front end, and use established frameworks for each. Iâ€™d recommend nextjs and node.js with Postgres. You could use laravel if you really want to stick with php but since Iâ€™m assuming your plan is to use ai agents you might as well use node.\n\nIf you use nextjs and node, youâ€™ll have tons of options when it comes to deployment.\n\nNext consider iac, terraform is the industry standard.\n\nNext youâ€™ll need to prep your ai agents. You need to create architecture, patterns, and docs that they can follow. Resist the urge to start building features. Instead, fill the project with best practices documentation about all relevant details, and settle on the software architecture patterns you want the agents to use.\n\nCreate your Claude.md and agents.md files, point them at your docs.\n\nFigure out how youâ€™re going to do testing: unit, integration, e2e etc. Create docs, update agents files\n\nFinally, create an example feature and implement with ai agent. Make sure it is *perfect* as the agents will copy it.\n\nAfter that, you just need to write down all the features with as much detail as possible and give it to Ai agents. Make sure they write and run lots of tests.",
          "score": -2,
          "created_utc": "2026-02-06 23:51:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o418meo",
          "author": "never-starting-over",
          "text": "That doesn't sound like a quite big project. The free advice I can give you is consider using a CMS like Directus, Strapi or whatever.",
          "score": -1,
          "created_utc": "2026-02-07 05:23:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o40jgma",
          "author": "thabc",
          "text": "Check out Claude Code.",
          "score": -2,
          "created_utc": "2026-02-07 02:30:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}