{
  "metadata": {
    "last_updated": "2026-01-25 08:55:26",
    "time_filter": "week",
    "subreddit": "softwarearchitecture",
    "total_items": 20,
    "total_comments": 111,
    "file_size_bytes": 179217
  },
  "items": [
    {
      "id": "1qlb3ow",
      "title": "OpenAI‚Äôs PostgreSQL scaling: impressive engineering, but very workload-specific",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qlb3ow/openais_postgresql_scaling_impressive_engineering/",
      "author": "mddubey",
      "created_utc": "2026-01-24 02:46:32",
      "score": 74,
      "num_comments": 8,
      "upvote_ratio": 0.91,
      "text": "I am a read only user of reddit, but OpenAI‚Äôs recent blog on scaling PostgreSQL finally pushed me to write. The engineering work is genuinely impressive ‚Äî especially how far they pushed a single-primary Postgres setup using read replicas, caching, and careful workload isolation.\n\nThat said, I feel some of the public takeaways are being over-generalized. I‚Äôve seen people jump to the conclusion that distributed databases are ‚Äúover-engineering‚Äù or even a ‚Äúfalse need.‚Äù While I agree that many teams start with complex DB clustering far too early, it isn‚Äôt fair ‚Äî or accurate ‚Äî to dismiss distributed systems altogether.\n\nIMO, most user-facing OpenAI product flows can tolerate eventual consistency. I can‚Äôt think of a day-to-day feature that truly requires strict read-after-write semantics from a primary RDBMS. Login/signup, token validation, rate limits, chat history, recent conversations, usage dashboards, and even billing metadata are overwhelmingly read-heavy and cache-friendly, with only a few infrequent edge cases (e.g., security revocations or hard rate-limit enforcement) requiring tighter consistency that don‚Äôt sit on common user paths.\n\nThe blog also acknowledges using **Cosmos DB for write-heavy workloads**, which is a sharded, distributed database. So this isn‚Äôt really a case of scaling to hundreds of millions of users purely on Postgres. A more accurate takeaway is that **Postgres was scaled extremely well for read-heavy workloads,** while high-write paths were pushed elsewhere.\n\nThis setup works well for OpenAI because writes are minimal, transactional requirements are low, and read scaling is handled via replicas and caches. It wouldn‚Äôt directly translate to domains like **fintech, e-commerce, or logistics** with high write contention or strong consistency needs. The key takeaway isn‚Äôt that distributed databases are obsolete ‚Äî it‚Äôs that minimizing synchronous writes can dramatically simplify scaling, when your workload allows it.\n\nRead the blog here: https://openai.com/index/scaling-postgresql/\n\n**PS:** I may have used ChatGPT to discuss & polish my thoughts. Yes, the irony is noted.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qlb3ow/openais_postgresql_scaling_impressive_engineering/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o1db8o5",
          "author": "adfx",
          "text": "It was an interesting read and I appreciate the honesty in the post¬†",
          "score": 8,
          "created_utc": "2026-01-24 04:09:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dq2ul",
              "author": "mddubey",
              "text": "thank you :)",
              "score": 1,
              "created_utc": "2026-01-24 05:54:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3n10",
          "author": "PmMeCuteDogsThanks",
          "text": "Thanks, for once an interesting post¬†",
          "score": 3,
          "created_utc": "2026-01-24 07:50:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fsxf0",
              "author": "mddubey",
              "text": "Thank you",
              "score": 2,
              "created_utc": "2026-01-24 15:26:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1emwa7",
          "author": "ryan_the_dev",
          "text": "I agree. Nice write up. Not too long.",
          "score": 1,
          "created_utc": "2026-01-24 10:46:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fsygv",
              "author": "mddubey",
              "text": "Thank you",
              "score": 1,
              "created_utc": "2026-01-24 15:26:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1euku8",
          "author": "KingOfDerpistan",
          "text": "I wonder: what types or workloads are routed to CosmosDB, and which ones to PG? You mentioned write-heavy, are these conversational logs?",
          "score": 1,
          "created_utc": "2026-01-24 11:54:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1fwnff",
              "author": "mddubey",
              "text": "It is not me mentioning but more of them mentioning that write heavy! No information about what exactly!\n\nAlthough If I were to guess things which need strong consistency like sing-in/token generation, token invalidation, any settings changes which needs to be in effect instantly will be going to directly postgrss!\n\nThe write heavy stuff I was initially thinking of was actually messages but then why store them in db at all, it should be blob storage or file storage probably!\n\nThe other thing could be stuff which require read/check before write. Which is similar to what we said for postgress! Right now it feels like they are slowly migrating the postgress to cosmos as you can do all the stuff said in 2nd para with more scale! But we can only guess from outside üòü",
              "score": 1,
              "created_utc": "2026-01-24 15:44:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qif3i2",
      "title": "What math actually helped you reason about system design?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qif3i2/what_math_actually_helped_you_reason_about_system/",
      "author": "TrappedInLogic",
      "created_utc": "2026-01-20 22:03:46",
      "score": 43,
      "num_comments": 22,
      "upvote_ratio": 0.94,
      "text": "I‚Äôm a Master‚Äôs student specializing in Networks and Distributed Systems. I build and implement systems, but I want to move toward a more rigorous design process.\n\nI‚Äôm trying to reason about system architecture and components¬†**before writing code**. My goal is to move beyond ‚Äúreasonable assumptions‚Äù toward a framework that gives¬†**mathematical confidence**¬†in properties like soundness, convergence, and safety.\n\n**The Question:**¬†What is the¬†**ONE specific mathematical topic or theory**¬†that changed your design process?\n\nI‚Äôm not looking for general advice on ‚Äúlearning the fundamentals.‚Äù I want the specific ‚Äúclick‚Äù moment where a formal framework replaced an intuitive guess for you.\n\nSpecifically:\n\n* What was the topic/field?\n* How did it change your approach to designing systems or proving their properties?\n* Bonus: Any book or course that was foundational for you.\n\nI‚Äôve seen fields like Control Theory, Queueing Theory, Formal Methods, Game Theory mentioned, but I want to know which ones¬†**really transformed your approach to system design**. What was that turning point for you?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qif3i2/what_math_actually_helped_you_reason_about_system/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0qyovi",
          "author": "MoustacheApocalypse",
          "text": "Not exactly a math answer but learning the difference between centralized versus distributed paradigms. \n\nIt is more logic or philosophy than math but it covers everything from system architecture to organizational change management, Conway's Law included. \n\nOnce you start down the path of distributed teams and distributed microservices based architecture, it is helpful to keep in mind that you are not only designing the system but also designing how the system is built and maintained. Very important when working on large systems in large corporate environments.",
          "score": 21,
          "created_utc": "2026-01-20 22:14:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0r6rts",
              "author": "OneHumanBill",
              "text": "There's a mathy answer in here. \n\nA centralized system like a hub and spoke model or even a tree model have a communications complexity of O(N).  Simple.  But that model also lacks resilience.  In other words, it's too simple. \n\nA decentralized system like a web has a communications complexity of O(N^2).  It's more complex but much more resilient while still being tractable.\n\nI have a personal belief that there's something near magical about O(N^2) being the measurable complexity of a system.  For me that goes beyond system architecture and into things like social structures. \n\nIsn't it interesting for example that when humanity turned ownership of their personal relationships over to social media companies, that this is when society started forming echo chambers and really going nuts?  I think this in large regard has to do with the fact that your relationships with extended friends on Facebook aren't really yours anymore.  They're Facebook's, and we've moved human relationships from the web model that has existed since the time of chimpanzees to a simple hub-and-spoke model. \n\nO(N^2), I'm telling you, it's special.  It trends naturally toward resilience and self-healing.  Go elsewhere at your peril.",
              "score": 12,
              "created_utc": "2026-01-20 22:55:35",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0sbmhs",
                  "author": "MoustacheApocalypse",
                  "text": "At a previous gig I used to speak and write about cyclomatic complexity as a measure of a system's maintainability or lack thereof. Maybe a similar thought.",
                  "score": 3,
                  "created_utc": "2026-01-21 02:42:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0r1wrf",
          "author": "vplatt",
          "text": "I'm interested to see other answers here, but I feel like you're asking for theoretical math frameworks to apply to engineering in a way that would let you deterministically descend through a system design.  \n\nIn reality, that's not how it works.  It's just as much art as anything approaching real engineering; much less science or being provably correct.  For starters, you almost never know all the requirements up front and your design must adapt over time to accommodate new requirements as they are discovered or asserted.\n\nTo help guide a system design, one chooses an organizing architecture and follows design principles in order to ensure consistency, try ensure the necessary degree of resilience, scalability, and correctness - all custom to the specific situation.  \n\nA big help in doing this is to know what patterns have been followed before for the kind of work you're doing.  A patterns repository is a big help with that.  Of course, there are different types of patterns as well.  \n\n- Design patterns\n- Architectural patterns\n- Anti-patterns\n- Analysis patterns\n\nThese are simply languages that help you talk about your system.  You can have opinions all day long, but if you cannot communicate your ideas in a way that allows others to participate in the creation of your system, then it's not going to enjoy much success and patterns help communicate your approach in a clear way and give you some guidelines to use in creating it.  \n\nTo be clear, even if you don't believe a word of what I'm saying, you will use patterns anyway.  Everyone adopts the repeatable practices that form a path of least resistance.  But you can do that in a fully conscious way and avoid trying to reinvent the wheel at every turn.  \n\n\n| **Category**               | **URL**                                                                                                                                                           | **Description**                                                                                         |\n| -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |\n| **Design patterns**        | [https://refactoring.guru/design-patterns](https://refactoring.guru/design-patterns) ([Refactoring Guru][1])                                                      | A comprehensive and practical catalog of software design patterns with explanations and examples.       |\n| **Architectural patterns** | [https://architectural-patterns.net/](https://architectural-patterns.net/) ([architectural-patterns.net][2])                                                      | A site focused on patterns used in software system architecture with explanations and pattern catalogs. |\n| **Anti-patterns**          | [https://kb.segersian.com/software-architecture/topics/anti-patterns/](https://kb.segersian.com/software-architecture/topics/anti-patterns/) ([Knowledgebase][3]) | A general knowledge base listing common software anti-patterns and explanations.                        |\n| **Analysis patterns**      | [https://en.wikipedia.org/wiki/Software_analysis_pattern](https://en.wikipedia.org/wiki/Software_analysis_pattern) ([Wikipedia][4])                               | Wikipedia entry explaining analysis patterns in software engineering with definitions and context.      |\n\n[1]: https://refactoring.guru/design-patterns?utm_source=chatgpt.com \"Design Patterns\"\n[2]: https://architectural-patterns.net/?utm_source=chatgpt.com \"Home | Architectural Patterns\"\n[3]: https://kb.segersian.com/software-architecture/topics/anti-patterns/?utm_source=chatgpt.com \"Anti Patterns | Knowledgebase\"\n[4]: https://en.wikipedia.org/wiki/Software_analysis_pattern?utm_source=chatgpt.com \"Software analysis pattern\"\n\nEdit:  Refactoring Guru also refers to https://metapatterns.io/, which is a pretty nice resource as well.",
          "score": 19,
          "created_utc": "2026-01-20 22:30:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rs7vk",
              "author": "midasgoldentouch",
              "text": "Love Refactoring Guru.  Going to spend my allowance this year on the book.",
              "score": 3,
              "created_utc": "2026-01-21 00:52:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0r1b07",
          "author": "therealkevinard",
          "text": "Maybe elementary, but Little‚Äôs Law is a priceless trinomial. I was always a sucker for a good polynomial, though lol. \n\nI‚Äôd seen it before, ofc, but it really fell into place during an incident response a few years ago. With LL, I could decisively dial-in application config to get the outcome I wanted (vs repeated dials to eventually hone in on the target)  \n\nI‚Äôve since used it in preliminary diagrams to gauge the impact of architectural changes on up- and down-stream components, and it shows up on at least a few of my grafana dashboards.",
          "score": 10,
          "created_utc": "2026-01-20 22:27:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0r770r",
              "author": "Effective-Total-2312",
              "text": "I hadn't heard about this one, very intuitive but nice !",
              "score": 2,
              "created_utc": "2026-01-20 22:57:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qyq0t",
          "author": "justUseAnSvm",
          "text": "Look up the LSMT paper: [https://www.cs.umb.edu/\\~poneil/lsmtree.pdf](https://www.cs.umb.edu/~poneil/lsmtree.pdf) they make a very good argument for why a log structured DB would work using a disk model (3.1), which is just a heuristic calculated on access speed.\n\nI should say, soundness, convergence, and safety are difficult to prove, nigh impossible with pen and paper. That's when folks reach towards something like TLA+, Alloy, or even Lean3 or something like that.\n\nPersonally, I've been able to get away with just thinking about systems in terms of invariants (what must always be true), and show that progress will be made with the only tests being quickcheck or hedgehog. If you can encapsulate your system into a statemachine, or to a simple interface, generative testing gets you pretty far.",
          "score": 4,
          "created_utc": "2026-01-20 22:14:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0rfey1",
              "author": "TrappedInLogic",
              "text": "I‚Äôve been eyeing TLA+ mainly for reasoning about safety properties.\n\nI‚Äôm still trying to understand its limits in practice: how much can formal methods like TLA+ actually tell you about performance characteristics (throughput, latency..), versus just functional correctness?\n\nIs it fair to think of TLA+ as closer to an *operational-semantics / state-transition specification* of a system, rather than a tool meant for performance analysis?",
              "score": 2,
              "created_utc": "2026-01-20 23:42:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0sscia",
                  "author": "justUseAnSvm",
                  "text": "You're instinct is right: TLA is really built around set theory/logic, and temporal operators (eventually/always) that are required to prove safety/liveness/termination conditions. Really interesting stuff, because those eventually/always logical statements can be effectively evaluated with Bochi automata which is some really cool maths.\n\nHowever, if you are talking performance analysis, then there are ways to get answers with TLA+, like counting \"for loop ticks\" or \"memory access\" for a random input of length N, but there's usually a better/faster way if you're after just performance numbers.\n\nFinally, the really hard thing about TLA+, and I think why it's only rarely used, is that you still don't get around the problem that your model and code diverge. The TLA+ spec might be used at the beginning of development to prove your state machines semantics will work, but 14 months and a new team later, things change.",
                  "score": 3,
                  "created_utc": "2026-01-21 04:24:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0rqf6z",
          "author": "MattAtDoomsdayBrunch",
          "text": "Two things that have influenced my thinking.\n\n[A Note On Distributed Computing](https://waldo.scholars.harvard.edu/sites/g/files/omnuum6261/files/waldo/files/waldo-94.pdf)\n\nand \n\n[The Unix Philosophy](https://en.wikipedia.org/wiki/Unix_philosophy)",
          "score": 3,
          "created_utc": "2026-01-21 00:41:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rnpd3",
          "author": "engx_ninja",
          "text": "Just read software architecture in practice. It gives you mathematical confidence in your architectural tactics and how they address your measurable quality attribute scenarios.",
          "score": 2,
          "created_utc": "2026-01-21 00:27:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0soyhp",
          "author": "uncountable_sheep",
          "text": "entropy and type algebra as a proxy for \"how complex is this design\".\n\nIn particular, designing programs to only represent \"useful\" states, and all combinations of useful states, and excluding impossible ones.\n\nIt helps describe exactly how a program is introducing incidental complexity, and you can measure the degree via relative entropy. \n\nThat said, knowing what sets of states to compare is a trick, but it's a useful technique to explain why one design is better than another.\n\nAlso, in practice, perceived entropy is much lower than type algebra suggests due to semantics and (cognitive) chunking, which is significantly harder to quantify, but might be possible with NLP style techniques. Something that has inferior type entropy, but better semantic chunking may well be preferable.\n\nI've yet to see any literature about it, just a pet theory of mine at the moment, feel free to recommend things or related works.",
          "score": 2,
          "created_utc": "2026-01-21 04:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rewzd",
          "author": "ElderberryNo6893",
          "text": "https://en.wikipedia.org/wiki/Graph_theory",
          "score": 2,
          "created_utc": "2026-01-20 23:39:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rinhv",
          "author": "asdfdelta",
          "text": "(Systems Theory)[https://en.wikipedia.org/wiki/Systems_theory] governs absolutely everything, and imho is one of the core parts of being an architect versus anything else. Architects that don't think with a Systems lens tend to struggle with larger problem sets.",
          "score": 2,
          "created_utc": "2026-01-20 23:59:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0rze9p",
          "author": "chipstastegood",
          "text": "What helped me was not math but physics, specifically simulations and experiments in physics. Concept of simulations has helped me model software better. And experiments have helped me write better tests, especially black box tests because in physics you often have to look at externally visible behavior to infer something about what‚Äôs going on inside a system.",
          "score": 1,
          "created_utc": "2026-01-21 01:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0td8i2",
          "author": "Ill_Cod4557",
          "text": "Mostly discrete math graphs, queues, probabilities, and basic linear algebra help reason about scalability, trade-offs, and performance more than heavy calculus ever did.",
          "score": 1,
          "created_utc": "2026-01-21 07:04:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ucxis",
          "author": "Fresh-Secretary6815",
          "text": "graph theory, stochastic processes",
          "score": 1,
          "created_utc": "2026-01-21 12:21:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wjrdy",
          "author": "Corendiel",
          "text": "Trying to find the truth with mathematics might feel like a noble goal but in reality models are much more complex. Finding the perfect design for a scenario might ignore a lot of constraints down the line and potential evolution and changes. \n\nLike in real architecture building the strongest building is not always the right goal. You must make a lot of trade offs in various dimentions. Experimentation and time will tell the best design.\n\nIt doesn't mean don't try to architect stuff but don't focus on the ultimate perfect solution that will be valid for a hot minute. Getting things done in the minimal amount of time and focus on adaptability and sustainability.",
          "score": 1,
          "created_utc": "2026-01-21 18:48:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ynbji",
          "author": "MathematicianSome289",
          "text": "Category theory and combinatorics come to mind with regard to system composition",
          "score": 1,
          "created_utc": "2026-01-22 00:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z3snw",
          "author": "cromwellryan",
          "text": "In hindsight, I learned things from many of my math classes. Geometry taught me about formal proofs which have been valuable when writing architecture recommendations and explaining the reasoning for a decomposed system. Algebra is useful for modeling and predicting or forecasting what a system might do in certain scenarios. Calculus has helped me think about the flow and change of systems. I‚Äôd throw Physics into that category, because it matches how I think of systems and change over time.",
          "score": 1,
          "created_utc": "2026-01-22 02:26:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qi0jf2",
      "title": "Silent failures are worse than crashes",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qi0jf2/silent_failures_are_worse_than_crashes/",
      "author": "CompetitiveUnit7360",
      "created_utc": "2026-01-20 13:03:13",
      "score": 27,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "Failures are unavoidable when you build real systems.  \nSilent failures are a choice.\n\nOne lesson that keeps repeating itself for me, it's not whether your system fails, it's how it fails.\n\nhttps://preview.redd.it/56rmp6uy5ieg1.png?width=2786&format=png&auto=webp&s=f89bd98b5d4aed94437ff2a4ba0fa8f682b28757\n\nWhile building a job ingestion pipeline, we designed everything around a simple rule:  \ndon‚Äôt block APIs, don't lose data, and never fail quietly.\n\nSo the flow is intentionally boring and predictable:\n\n* async API ‚Üí queue ‚Üí consumer\n* retries with exponential backoff\n* dead letter queue when things still go wrong\n\nIf processing fails, the system retries on its own.  \n\n\nIf it still can't recover, the message doesn't vanish it lands in a DLQ, waiting to be inspected, fixed, and replayed.\n\nNo heroics. No \"it should work\".  \nJust accepting that failures will happen and designing for them upfront.\n\nThis is how production systems should behave:  \nfail loudly, recover gracefully, and keep moving.\n\nWould love to hear how others here think about failures, retries, and DLQs in their systems.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qi0jf2/silent_failures_are_worse_than_crashes/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0o2h3a",
          "author": "oweiler",
          "text": "The DLQ part can be problematic if order is important. It sometimes is better to block.",
          "score": 7,
          "created_utc": "2026-01-20 14:06:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qhrnc",
              "author": "CompetitiveUnit7360",
              "text": "At the moment order is not that important. Main purpose is that making sure that data is there",
              "score": 1,
              "created_utc": "2026-01-20 20:55:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qk9i6",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 3,
          "created_utc": "2026-01-20 21:07:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tltlw",
              "author": "CompetitiveUnit7360",
              "text": "Yeah, it can be lost until unless an external mechanism is not designed to capture it.",
              "score": 1,
              "created_utc": "2026-01-21 08:23:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tv8pn",
                  "author": "[deleted]",
                  "text": "[deleted]",
                  "score": 2,
                  "created_utc": "2026-01-21 09:54:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0r8exj",
          "author": "Exirel",
          "text": "I agree with you, silent failure are a choice, and in my opinion, usually a bad one. When it comes to error, beside the \"never be silent\", I tend to follow these two guidelines:\n\n* **let it crash**: if something isn't suppose to work, just crash, most of the time the crash happens because there is a bug, and you don't fix bug with more code, you fix them with better code; and if you don't like crash reports... well, test more, use better types, get proof that it works!\n* **fail fast**: if you can check a fail condition, check it as early as possible to exit the process as soon as possible, I don't like when a program waste resources to end in a failure that was predictable; for example don't wait a query to the database to fail a data type check",
          "score": 4,
          "created_utc": "2026-01-20 23:04:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tlo2t",
              "author": "CompetitiveUnit7360",
              "text": "Thanks for this",
              "score": 2,
              "created_utc": "2026-01-21 08:22:17",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0tnwr3",
              "author": "LordWecker",
              "text": "It feels fitting that your username is almost elixir",
              "score": 2,
              "created_utc": "2026-01-21 08:43:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0toskv",
                  "author": "Exirel",
                  "text": "The coincidence is not lost on me. :D",
                  "score": 1,
                  "created_utc": "2026-01-21 08:52:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0p36fj",
          "author": "Shekher_05",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-01-20 17:04:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjxzgn",
      "title": "SOLID Principles Explained for Modern Developers (2026 Edition)",
      "subreddit": "softwarearchitecture",
      "url": "https://javarevisited.substack.com/p/how-to-be-a-solid-programmer-in-2026",
      "author": "javinpaul",
      "created_utc": "2026-01-22 15:46:50",
      "score": 23,
      "num_comments": 8,
      "upvote_ratio": 0.76,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qjxzgn/solid_principles_explained_for_modern_developers/",
      "domain": "javarevisited.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o17d6pf",
          "author": "steve-7890",
          "text": "2026 and inside the same old sh\\*t with bad examples and no remarks when not to use them (**what's even more important**).",
          "score": 16,
          "created_utc": "2026-01-23 08:18:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17k9ls",
              "author": "minoso2",
              "text": "in what situation would you not use one of these solid principles?",
              "score": 2,
              "created_utc": "2026-01-23 09:24:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o17syes",
                  "author": "steve-7890",
                  "text": "There are tones of materials on that:\n\n\\* Watch: Dan North, CUPID talk\n\n\\* Read: A philosophy of Software Design book\n\nFor instance:\n\n\\* OCP inside the module causes a lot of redundant abstractions that increases cognitive load - without any real benefits.   \n\\* DIP - again, inside the module - same as above   \n\\* SRP - nobody knows what \"Single\" means here. If applied everywhere causes a lot of small objects, hard to grasp.",
                  "score": 7,
                  "created_utc": "2026-01-23 10:43:44",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1aozf1",
                  "author": "Drevicar",
                  "text": "Given that the SOLID principles are solutions to specific problems that may occur during the software development process, then you would not apply any of the solutions when none of the problems are present in a significant quantity. Where \"significant quantity\" is subjective and based on trade-off analysis, but if the quatity is == 0 then you don't even need the subjective part.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:48:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o18av0w",
          "author": "AvoidSpirit",
          "text": "Solid is basically the bible of programming. As in everybody interprets it differently and can use it to explain anything ever.\n\nYou either have enough experience to organize things and then you can call it whatever. Or you don't and then knowing the definition of something like \"Single Responsibility\" or \"Interface segregation\" is practically irrelevant.",
          "score": 1,
          "created_utc": "2026-01-23 12:56:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhy94c",
      "title": "Every time I face legacy system modernization, the same thought comes back",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qhy94c/every_time_i_face_legacy_system_modernization_the/",
      "author": "Icy_Screen3576",
      "created_utc": "2026-01-20 11:05:35",
      "score": 19,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "\"It would be much easier to start a next-gen system from scratch.\"\n\n[One worker process, one database.](https://preview.redd.it/3l58aw1cbheg1.png?width=3792&format=png&auto=webp&s=bde20bdd6ed7d95c51906784d90387dd76ac0ccb)\n\nThe problem is that the existing system already works. It carries years of edge cases, integrations, reporting, and revenue. I can‚Äôt simply ditch it and start on a greenfield, but I also can‚Äôt keep it as-is: complexity grows with every sprint, cognitive load increases, clear team ownership boundaries become impossible, and time to market slowing down.\n\n**What worked**\n\nLooking into design patterns, I found the Strangler Fig pattern that everyone mentions but in practice, it‚Äôs not enough. You also need an Anti-Corruption Layer (ACL). Without an ACL, you can‚Äôt keep the legacy system running without regression while new hosts run side by side.\n\nThey both allow you to incrementally replace specific pieces of functionality while the legacy system continues to run.\n\n[ The legacy system has no responsibilities left thus can be decommissioned.](https://preview.redd.it/k8xx43u7dheg1.png?width=3156&format=png&auto=webp&s=c7cc931bfe4880623f54e5a2366847d55d724244)\n\n**Important note** \n\nThis kind of service separation should only be done when justified. For example, when you need team ownership boundaries or different hardware requirements. The example here is meant to explain the approach, not to suggest that every monolith should be split.\n\n**One caveat**\n\nThis approach only works for systems where you *can* introduce a strangler. If you‚Äôre dealing with something like a background service ‚Äúbig ball of mud‚Äù with no interception point, then the next-gen is the way.\n\nThis is the [link](https://www.justifiedcode.com/modernizing-a-monolithic-application) where you can find all steps and diagrams, from the initial monolith to the final state, with an optional PDF download.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qhy94c/every_time_i_face_legacy_system_modernization_the/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0rzw6l",
          "author": "HosseinKakavand",
          "text": "Thanks for posting this ‚Äî really insightful. Strangler Fig + ACL are the two patterns we use the most for modernization. You captured the reality well: you can't rewrite overnight, but you also can't keep bolting onto legacy debt forever.\n\nIn your example, how are you handling orchestration between these new services? I noticed the arrows in your diagram look like service-to-service choreography. At [Luther](https://reddit.com/r/luthersystems) we‚Äôve moved toward a \"distributed orchestrator\" model using what we call Common Operation Scripts (COS), with ACLs in front of each system. Have you found choreography enough or do you eventually hit a \"spaghetti\" wall where you need this kind of central coordination?",
          "score": 2,
          "created_utc": "2026-01-21 01:35:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tjl43",
              "author": "Icy_Screen3576",
              "text": "The account acl calls the new service in normal http request thru the gateway/strangler.\n\nhttps://preview.redd.it/mz63p6s2tneg1.png?width=3677&format=png&auto=webp&s=84c7c706d8cf7b83290a24516f16a2b61e20edeb\n\nI used a queue for the email acl.",
              "score": 1,
              "created_utc": "2026-01-21 08:02:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0zzu0u",
          "author": "Quakedogg",
          "text": "We do modernisation through migration to cloud systems successfully. it‚Äôs possible with the right legacy analysis tool. A good tool can lexically analyse static code and derive functional algorithms which can be hand coded or passed to an llm agent to write in a modern language. Some hand holding is required for re architecting. But this can be achieved in record time with the right tools.",
          "score": 1,
          "created_utc": "2026-01-22 05:50:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e4rec",
          "author": "Best-Theory-2201",
          "text": "I agree. Progress is made not by rebuilding, but by layering things. You can look at adding augmentation layers that add new functionality to existing applications while maintaining the original applications in its original state. Especially for web  apps we have some good success with web augmentation software such as Webfuse.",
          "score": 1,
          "created_utc": "2026-01-24 08:00:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhk69j",
      "title": "How to correctly implement intra-modules communication in a modular monolith?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qhk69j/how_to_correctly_implement_intramodules/",
      "author": "Tobi1311",
      "created_utc": "2026-01-19 23:16:51",
      "score": 19,
      "num_comments": 19,
      "upvote_ratio": 0.95,
      "text": "Hi, I'm currently designing an e-commerce system using a modular monolith architecture. I have decided to implement three different layers for each module: Router, to expose my endpoints; Service, for my business logic; and Repository, for CRUD operations. The flow is simple: Router gets a request, passes it to the Service, which interacts with Repository if necessary, and then the response follows the same path back. Additionally, I am using a single PostgreSQL database.\n\nThe problem I'm facing is that but when deciding how to communicate between modules, I have found several options:\n\n* **Dependency Injection (Service Layer):** Injecting, for example, `PaymentService` into `OrderService`. It's simple, but it seems to add coupling and gives `OrderService` unnecessary access to the entire `PaymentService` implementation when I only need a specific method.\n* **Expose modules endpoints:** Using internal HTTP calls. It‚Äôs an option, but it introduces latency and loses some of the \"monolith\" benefits.\n* **Event-bus communication:** Not an option. The application is being designing for a local shop, won't have much traffic so I consider implementing a queue message will be adding unnecesary complexity.\n* **Module Gateway:** Creating a gateway for each module as a single point of access. While it might seem like a single point of failure, I like that it delegates orchestration to a specific class and I think it will scale well. However, I‚Äôm concerned about it becoming a duplicate of the Service layer.\n\nI‚Äôm looking for your opinions, as I am new to system design and this decision is taking up a lot of my research time.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qhk69j/how_to_correctly_implement_intramodules/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0knch0",
          "author": "rkaw92",
          "text": "If you have a procedure that spans several modules from start to finish, it's usually a sign that you need some form of orchestration. But it can be pretty minimal, like a function that calls different modules sequentially. Think about a Use-Case Controller. It would need access to several modules, of course - so inject them as dependencies.\n\nThe thing is, this is now your actual service layer. This is the crux of your application that clients will interact with. They don't really see the underlying modules anymore - they focus on the desired behavior or the \"what\", not the \"how\". You've now composed a rich process controller out of its constituent parts.\n\nTraditionally, this is called a mediator - an object that talks to many different objects to manage a complex process by passing data and calls back and forth. However, lately the word \"mediator\" has been hijacked (mostly by the .NET people, see MediatR) to mean \"a command dispatcher\". Keep this in mind when looking for code examples.",
          "score": 15,
          "created_utc": "2026-01-19 23:56:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0qm18r",
              "author": "Tobi1311",
              "text": "Very interesting, will read about that. Thank you!",
              "score": 1,
              "created_utc": "2026-01-20 21:15:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kiloc",
          "author": "SolarNachoes",
          "text": "DI and move on. You can always refactor to other solutions as long as you access the payment method via DI‚Äôd interface.\n\nIf you don‚Äôt want to use the entire payment service, then you can write a small adapter class. But that‚Äôs getting a bit anal.",
          "score": 9,
          "created_utc": "2026-01-19 23:31:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kqi3r",
              "author": "SmurphsLaw",
              "text": "Just be careful you don‚Äôt make circular dependencies.",
              "score": 5,
              "created_utc": "2026-01-20 00:14:02",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0qmbet",
              "author": "Tobi1311",
              "text": "Yeah, maybe I lost a lot of time trying to overengineering.",
              "score": 1,
              "created_utc": "2026-01-20 21:16:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kvnd2",
          "author": "Character_Respect533",
          "text": "Im building another 'Manager' layer where its purpose is to call the operation in each service. For example, CreateUser in manager layer will call CreateUser in UserModule and also ScheduleSendEmail in NotificationModule.\n\nThe manager layer will call multiple operations in multiple modules so then I can keep the concerns separated",
          "score": 2,
          "created_utc": "2026-01-20 00:41:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mknnz",
          "author": "zlaval",
          "text": "I use in memory buses / application events for this (publishing events from a service and processing them elsewhere). It provides low coupling, high speed and if necessary it is easy to extract later into separate service.",
          "score": 2,
          "created_utc": "2026-01-20 06:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kqh1o",
          "author": "Effective-Total-2312",
          "text": "I don't see the benefit in doing \"modular monolith\". If your system is small enough, you should just use either of the traditional patterns: MVC, Layered, Pipe and Filter or Hex architecture. If your system is too big for those, split into two or three services (not microservices, just different and isolated services), each with one of the mentioned architectures.",
          "score": 2,
          "created_utc": "2026-01-20 00:13:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0kzvhf",
          "author": "theycanttell",
          "text": "Depends on injections",
          "score": 1,
          "created_utc": "2026-01-20 01:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mivat",
          "author": "flavius-as",
          "text": "This decision is not one you make now and never change it again.\n\nEach of your suggestions is valid and what you currently want to do depends on where you are on a Roadmap to transforming the modulith to microservices.\n\nIf you're far from it (or not even planned) then a simple method call should be enough.\n\nIf you're about to split it, some http mechanism or event store or whatever is maybe better to simulate and iron out any required guarantees in an inconsistent system.\n\nThis decision is not one you make now and never change it again. It's a decision you do now based on your current requirements and project planning.\n\nThe key part is to make it such that changing this decision later on the Roadmap to another strategy is easy.\n\nCircular dependencies and all other good design principles still apply, no matter what.",
          "score": 1,
          "created_utc": "2026-01-20 06:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mleo1",
          "author": "olivergierke",
          "text": "Depending on what stack you‚Äôre working in, event-based communication might not be as complicated as you think. I spoke about this at the jChampions Conference 2025:\n\nhttps://www.youtube.com/live/eiFnSevxAdk?si=U_N2xbozjntzW5Gf\n\nThe talk discusses the topic in a context of a Spring application, but the fundamentals should be understandable even outside that.",
          "score": 1,
          "created_utc": "2026-01-20 07:05:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0n0y22",
          "author": "codingfox7",
          "text": "You wrote \"(injecting)¬†PaymentService into OrderService. It's simple, but it seems to add coupling\". Yeah, it does, but all other methods do it too, but not so explicitly. Messaging also makes modules coupled.\n\n\nAdding async communication will bring you many problems inherent to Distributed Systems (e.g. eventual consistency inside a monolith or losing messages during reboot (with in-memory queue), etc.)\n\n\nCheck out my text for detailed explanation: https://codingfox.net.pl/posts/mim/#how-should-business-modules-communicate (subchapter \"How should Business-Modules communicate?\" from \"Simplify your Application Architecture with Modular Design and MIM\").",
          "score": 1,
          "created_utc": "2026-01-20 09:28:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0np38v",
              "author": "Tobi1311",
              "text": "Interesting article you wrote. You say that \"keeping Foreign Keys between modules should be avoided.\",  this made me think about my database design, where I do have foreign keys between modules (supplier in a purchase order, item in a sale\\_item table, item in a stock\\_movement table, etc.), wouldn't not having fk between modules introduce complex queries when information retrieval is needed?. Maybe this is great in a context where migrating to microservices is just about time, isn't it?\n\nAlso, this \"Public API\" you mentioned is like my gateway approach but with a great tech name, if I'm not wrong. I think I would take this way, it makes me feel that is a decision I will not change, because of the shop size, and it will scale well.\n\nWhat about when the communication is between entities from the same module? Let's say ItemService and CategoryService. Maybe this example is too simple, but what about when the intra-module communication gets complex? Is DI enough? Is the public API approach, in this context, a complex solution for a simple problem?",
              "score": 1,
              "created_utc": "2026-01-20 12:48:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0q3i7o",
                  "author": "Wiszcz",
                  "text": "I'm not sure if I understand correctly, but if you have tables in db that belong to module in code, but that table have fk's to tables in other modules - you don't have modular monolith, you have just monolith.",
                  "score": 2,
                  "created_utc": "2026-01-20 19:49:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0ojzp2",
                  "author": "codingfox7",
                  "text": ">wouldn't not having fk between modules introduce complex queries when information retrieval is needed?.¬†\n\nIt's a standard practice. Modules are self-contained, isolated, coherent \"processes\". If you need to make queries between modules and they constantly chat to do any flow, it means you have:\n\na) One big module  \nb) Or a big ball of mud antipattern.\n\nThere're many materials on that. One of the latest I've seen: [https://norbert.tech/blog/2025-10-18/dark-sides-of-modularization/](https://norbert.tech/blog/2025-10-18/dark-sides-of-modularization/) and there are more in \"Resources\" section on my MIM text. If you have FK between tables, any migration to microservice would be impossible (and that's one of heuristic for a well designed module),\n\nIf you need to do reports from data, you have to build a read-model. But that's a different topic.\n\n>Also, this \"Public API\" you mentioned is like my gateway approach but with a great tech name,\n\n\"Public API\" is like a facade. But it doesn't have to be Facade pattern per se or even an interface.\n\n>What about when the communication is between entities from the same module?¬†\n\nI've never seen an \"inter-module communication\" done differently than DI or direct method calls. Well, I have seen an inter (and intra) communication that used an external message queue, but there was a reason for that (time sync was required).\n\nTo be honest, inside one module I don't even create interfaces, because I test whole modules end-to-end (Chicago School of tests), so I don't have to mock anything.",
                  "score": 1,
                  "created_utc": "2026-01-20 15:35:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0npu8q",
          "author": "Tarnell2",
          "text": "If your primary goal is keeping the module in a good state for later removal into a microservice then you‚Äôll likely be looking to implement some sort of Adapter / Anti Corruption Layer and then write up a lot of dependency cruiser rules to enforce that very specific mode of integration. This is more of a Hexagonal approach, and annoying to enforce, so a lot of overhead for a small system\n\nFrom your description though, applying a use case layer to manage complex workflows (only layer able to have knowledge of multiple services being present) is typical of N-Tier architecture and commonly appropriate for smaller applications.",
          "score": 1,
          "created_utc": "2026-01-20 12:52:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgeg58",
      "title": "Regarding Modular Monolith , and Clean Architecture",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qgeg58/regarding_modular_monolith_and_clean_architecture/",
      "author": "Illustrious-Bass4357",
      "created_utc": "2026-01-18 17:22:20",
      "score": 18,
      "num_comments": 28,
      "upvote_ratio": 0.91,
      "text": "Should each module/bounded context have its own separate presentation layer (API controllers, DTOs, endpoints etc.), or is it better or more common to have one single presentation layer (like one big Web API project) that serves all modules?\n\nhttps://preview.redd.it/dwfujgxc65eg1.png?width=489&format=png&auto=webp&s=fc5658a49132abd3973113359f2cfe354f373421\n\n  \nthis is my current project setup and I think the controllers in the WebAPI are getting too overwhelming,",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qgeg58/regarding_modular_monolith_and_clean_architecture/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0bt68l",
          "author": "Qinistral",
          "text": "It depends. If you have a ‚Äúmicroservoce‚Äù that handles say <5 entities, then you can probably do fine having them all together and packaging your classes by function. Otherwise, if you have a monolith with more entities, or multiple major subdomains, or just a LOT of business logic code (as opposed to simple CRUD) then it would be better to create sub-modules or libraries that encompass those sub domains end to end. This might also require you to have one or two common libraries that contain shared code, such as on one hand the server that wires up all the modules to the primary framework and server, and on the other hand a library that sets up the cross cutting shared code for all those individual modules, such as logging and database you know, utilities, and stuff like that. This later is how to do a ‚Äúmodular monolith‚Äù.\n\nYou‚Äôve expressed ‚Äúit‚Äôs overwhelming‚Äù so ya go for it. Split it up see how it feels. Doing that kinda refactor is how you learn and get a feel for it.",
          "score": 9,
          "created_utc": "2026-01-18 17:57:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0d3la9",
              "author": "kareesi",
              "text": "What you‚Äôve described is exactly what we do in my current org. We have a large modular monolith with common modules for the cross-cutting functionality. \n\nWe started with the shared presentation layer approach which made it hard to find anything and got unwieldy fast. Splitting it up helped draw clear boundaries of ownership for teams, and improved searchability/readability quite a bit.",
              "score": 2,
              "created_utc": "2026-01-18 21:45:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0hjdp6",
              "author": "Illustrious-Bass4357",
              "text": "thanks, I‚Äôll probably try to separate the presentation layer for some modules and see how it goes",
              "score": 1,
              "created_utc": "2026-01-19 15:09:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0dao94",
          "author": "gbrennon",
          "text": "Thats a hard decision with huge impacts! \n\nThis decision will involve:\n- size of the team\n- how team members interact\n- technical knowledge of the team\n\n\nMaybe for now it could make sense u choose something like:\n\nDefine bounded contexts with only domain and application layers(if u are going to use layers) and impl infrastructure and presentation as 2 packages. \n\n1 for the infrastructure one and 1 for presentation. \n\nIm suggesting this because if u the teams grow a lot team will just have to refactor 2 layers and u wont have friction with continuous integration and delivery  when all stay in the same project",
          "score": 2,
          "created_utc": "2026-01-18 22:18:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hirti",
              "author": "Illustrious-Bass4357",
              "text": "I‚Äôm a solo dev right now, but I think it would be easier for people, or if I got other devs on the project ,if they decided to work on, say, the Menu module, they would just navigate through that bounded context.\n\nlike rn,  I have in each module the three basic layers (domain, application, and infra), and I thought maybe to make it more separated , or more complete as a whole I should include a presentation layer. that layer would have the controllers and presentation level DTOs, and then an IServiceCollection extension or something similar, and in the WebAPI app just do .AddFoodPresentation\n\nhttps://preview.redd.it/yzx94nrlmbeg1.png?width=361&format=png&auto=webp&s=916af3938583537f6ff6da6d06f1cfeb71b96015",
              "score": 2,
              "created_utc": "2026-01-19 15:06:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0kkfcd",
                  "author": "gbrennon",
                  "text": "that looks well structured bro! good job!\n\ni suggest u have a shared module or things like this to share base classes or interfaces that everyone can rely on like EventHandler and EventPublisher.\n\nfollowing thie approach each bounded context will be decoupled from each other BUT you will have also to have shared Event/Message/Command because  the bounded context that publishess a given integration message is not the same bounded context that will handle it.\n\nusually a domain message(event or commmand) can be kept in the same bounded context buut an integration one usually is across different bound contexts",
                  "score": 1,
                  "created_utc": "2026-01-19 23:41:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0czwa4",
          "author": "SolarNachoes",
          "text": "Each module can be represented in each layer in its own folder.\n\nPresentation\n /Todos\n /Users\n\nInfrastructure\n /Todos\n /Users\n\nEtc\n\nLater if you want to separate the entire module you just grab the appropriate folders.",
          "score": 1,
          "created_utc": "2026-01-18 21:25:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0g5xj1",
          "author": "jshalais_8637",
          "text": "Do you really need all of this? Most of the time it can be avoided, Don't over-engineer when it's not needed since this consumes much time on maintenance, explaining to new members and so on.",
          "score": 1,
          "created_utc": "2026-01-19 09:18:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hj1et",
              "author": "Illustrious-Bass4357",
              "text": "It‚Äôs mainly for learning, tbh. \n\nand also if the app got bigger I think it would benefit me on the long run?",
              "score": 1,
              "created_utc": "2026-01-19 15:07:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ie3v5",
          "author": "codingfox7",
          "text": "The quick advice is: \"in most cases all elements of a module should be bundled together\".\n\nThe longer answer plus some remarks:\n\n* It's best to adhere to the High Cohesion pattern, because it is easier to maintain and understand code that is close together. Check out [https://codingfox.net.pl/posts/mim/#high-cohesion](https://codingfox.net.pl/posts/mim/#high-cohesion) (chapter \"8. Appendix - Introduction to Modular Design\", point \"High Cohesion\") for more explanation \n* Your design also breaks Low Coupling pattern (you coupled all modules in WebApi)\n* As a result, a simple change to e.g. Menus feature, will probably result with changes in 4 modules (in ideal world only one would be affected).\n\nYour question aside, I can tell you that your design is not really modular. It groups code into \"nouns\", so probably any \\_real\\_ business feature requires 2 or 3 modules to coordinate. That's will result in a big ball of mud sooner or later. Modules are self-contained, check out the \"A Module in the Modular Design\" subchapter (the same link as above).\n\nI also think that your modules are unnecessarily split by \"layers\", but is it really helpful?, or rather forces devs to constantly jump around? Instead split code by \"processes\" or \"features\" (and only extract \"infra\" code per module if you need to test it separately). You can see what I mean here: [https://codingfox.net.pl/posts/mim/#business-modules](https://codingfox.net.pl/posts/mim/#business-modules) (chapter \"Business-Modules\" of \"Simplify your Application Architecture with Modular Design and MIM\"). At the end of the article there's a long list of resources you can dig into to learn why I advised you to use these patterns.",
          "score": 1,
          "created_utc": "2026-01-19 17:29:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0imakc",
              "author": "Illustrious-Bass4357",
              "text": "I haven't read the resources yet, but you said that if I want to modify a real business feat I would require 2 or 3 modules and it will result in a big ball of mud sooner or later.  \n  \n(I don't know if I'm right or wrong), but in my design I'm using the contract layer to do exactly that, like, e.g., a Menu module needs to access some data from the Food module to get the ingredients, so the Menu Application layer depends on Food.Contract, which has, an interface and is acting like a contract. Right now I'm doing it synchronously and in memory.\n\nThe thing I think I'm doing wrong here is the fact that the Web API has all the presentation layers of the modules, and, honestly, I'm probably going to refactor them out.",
              "score": 1,
              "created_utc": "2026-01-19 18:06:10",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0iu6or",
                  "author": "codingfox7",
                  "text": "I meant these: \"Menu.Application\", \"Menu.Domain\", \"Menu.Infrastructure\", and \"WebApi\".\n\nYou wrote: \"so the Menu Application layer depends on Food.Contract\". But is just means that Menu depends on Food. What if you will need something from Menu in Food?\n\nThere's a heuristic that says \"a module should be ready to be extracted as a microservice\". In most cases it won't be ever extracted, but the point is it forces you to think about processes, not nouns.\n\nIf you have a large number or direct (method call) or indirect (messages) usages of one module in another, you would be better off merging them.\n\nInstead of thinking about \"Menu\" and \"Food\", make for instance an \"Ordering process\" module and separate \"Delivery process\" module. Arrange data so that each module has all data they need to fulfill it's responsibilities.\n\nI included a long list of resources that describe the modular design process. (Making a design based on nouns is so 1999 ;) )\n\nBTW: A case from just 2 weeks ago. In one of the services a big feature turned out to be unnecessary. I got rid of it in code by removing ONE module (and the registration from program.cs). In types of designs showed here, it should be shotgun surgery through many modules (csprojs) to remove a feature.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:40:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0iugda",
                  "author": "steve-7890",
                  "text": "Extracting presentation layers into modules themselves is a good move.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:42:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ckojh",
          "author": "Effective-Total-2312",
          "text": "That's just crazy. I am not a C# dev, but I would rather die than work in such a messy structure.\n\nLately, people are doing kinda the same thing with different names: vertical slices, package by feature, modular monolith, etc.\n\nIn all cases, the idea is that those setup are good when you start small, and once starting to get somewhat big, you should be able to easily separate each module/feature/slice into entirely different repositories/services.\n\nIf following those trends means having such a mess, I'd rather already go into different services, or use a hexagonal/clean/onion architecture which is much more easily navigable and understandable.",
          "score": -4,
          "created_utc": "2026-01-18 20:06:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0d01zr",
              "author": "Wiszcz",
              "text": "I think hexagonal/clean/onion is the same mess with more boilerplate :) They all try to do the same thing, and all look great on small/simple/crud applications. Add few queue sources, few db's, api versioning and you will always end up with mess.",
              "score": 4,
              "created_utc": "2026-01-18 21:26:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0d1xzk",
                  "author": "Effective-Total-2312",
                  "text": "I honestly don't see how could you end up with that.\n\n\\- Create a layer with your API code  \n\\- Create a layer with your domain logic, entities, and interfaces  \n\\- Create a layer with your infrastructure/external systems code\n\nJust use an IoC library (injects a concrete implementation/adapter for any port used throughout your code), and voil√°, you have your hexagonal/clean/onion architecture. What's simpler than that ?\n\nDependency management is ultra simple; API calls domain code, infrastructure leverages domain interfaces. That's just it.\n\nGranted, I put a services layer in-between my API and domain, handles some orchestration and transformations, as thin as possible. Still pretty easy and almost no boilerplate.",
                  "score": 1,
                  "created_utc": "2026-01-18 21:37:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0daxws",
              "author": "gbrennon",
              "text": "People are just naming things to sound new but its the same things as always üòÖ",
              "score": 1,
              "created_utc": "2026-01-18 22:20:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0hgkp6",
              "author": "Illustrious-Bass4357",
              "text": "can you elaborate more on how do you find it messy?? I literally went for this architecture because it‚Äôs very clean in terms of navigation and separation of concerns , like If you want to modify or update a **Menu-specific** use case, you go to the **Menu bounded context**, then the **Menu.Application** layer, and edit it there. You can‚Äôt really mess up anything in other modules\n\nAlso, I‚Äôm not trying to argue or claim that I‚Äôm right.  \nI‚Äôm still in college and haven‚Äôt worked on a real-world production app yet, so I assumed this structure or architecture would be better for production use.",
              "score": 1,
              "created_utc": "2026-01-19 14:55:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0i8t4m",
                  "author": "Effective-Total-2312",
                  "text": "I would surely bet you're going a very good direction, most developers working out there are really bad. Like really really bad. And again, have in mind I don't work in C#, but in Python, so my backends and applications look drastically different.\n\nI can't elaborate too much in a reddit thread, but there are a few things I care mostly when seeing a component level architecture:\n\n\\- Cohesiveness of each element/layer  \n\\- Efficiency of connections/dependencies\n\nThen, on a more \"mundane\" level, I want the file hierarchy to be easily navigable as well as cognitively easy; if I see more than 5-6 folders in a same directory, that sounds wrong. If I see more than 5-6 files in a directory, that sounds wrong. They're like code smells to me, it's similar to the Single Responsibilty principle, it sounds like there is too much going on there; either it isn't, and some things can be grouped, or it is, and some extraction or re-design can be made.\n\nSome times you may simply need those multiple dirs/files too. That's what happens with \"smells\".\n\nI don't know if my way of doing architecture works out in C#, so take my advice with a grain of salt, but I've had my good share of C# developers turning to Python and being a complete mess trying to understand what to do in a more free environment like Py.",
                  "score": 1,
                  "created_utc": "2026-01-19 17:05:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkw6og",
      "title": "Designing a Redis-resilient cache for fintech flows looking for feedback & pitfalls",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qkw6og/designing_a_redisresilient_cache_for_fintech/",
      "author": "saravanasai1412",
      "created_utc": "2026-01-23 16:56:19",
      "score": 16,
      "num_comments": 27,
      "upvote_ratio": 0.94,
      "text": "Hey all,\n\nIm working on a backend system in a fintech context where **correctness matters more than raw performance**, and I love some community feedback on an approach am considering.\n\nThe main goal is simple\n\nRedis is great, but I don‚Äôt want it to be a **single point of failure**.\n\nHigh-level idea\n\n* Redis is treated as a **performance accelerator**, not a source of truth\n* PostgreSQL acts as a **durable fallback**\n\nHow the flow works\n\n**Normal path (Redis healthy):**\n\n* Writes go to DB (durable)\n* Writes also go to Redis (fast path)\n* Reads come from Redis\n\n**If Redis starts failing:**\n\n* A **circuit breaker** trips after a few failures\n* Redis is temporarily isolated\n* All reads/writes fall back to a DB-backed cache table\n\n**To protect the DB during Redis outages:**\n\n* A **token bucket rate limiter** throttles fallback DB reads & writes\n* Goal is controlled degradation, not max throughput\n\n**Recovery**\n\n* After a cooldown, the circuit breaker allows a single probe\n* If Redis responds, normal operation resumes\n\n**Design choices I‚Äôm unsure about**\n\nI‚Äôm intentionally keeping this simple, but I‚Äôd love feedback on\n\n* Using a **DB-backed cache table** as a Redis fallback - good idea or hidden foot-gun?\n* Circuit breaker + rate limiter in the app layer - overkill or reasonable?\n* Token bucket for DB protection - would you do something else?\n* Any failure modes I might be missing?\n* Alternative patterns you‚Äôve seen work better in production?\n\nupdate flow image for better understanding \n\nhttps://preview.redd.it/zt3qiirw48fg1.png?width=1646&format=png&auto=webp&s=e40813fcb14802ffe71b5bfe1611601577190c9b",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qkw6og/designing_a_redisresilient_cache_for_fintech/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o19qdos",
          "author": "Dry_Author8849",
          "text": "Caching in fintech is a risky move. You need to be very careful on what are you caching.\n\nSo, some reads, like account balances shouldn't be cached. It also depends where are you using the cache. If your API hits the cache is a bad idea.\n\nIf you proceed anyways, then test every operation in a highly concurrent scenario and see if everything pass.\n\nCheers!",
          "score": 12,
          "created_utc": "2026-01-23 17:09:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19s1r2",
          "author": "Mundane_Cell_6673",
          "text": "What is reads vs write ratio?\n\nWhat is the point of rate limiting writes in case when redis goes down, won't you have an inconsistent state in db? How do reads work here? What if you have too many read requests and your redis is still down?",
          "score": 4,
          "created_utc": "2026-01-23 17:17:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19z7u0",
              "author": "saravanasai1412",
              "text": "No we write data to db first. If redis fails db act as source of truth. Let me give bit context about how we using redis.  We start a transaction and store a transaction Id for that session for 10 min. In 10 min they need to complete the transaction. Mostly these data is on redis.\n\nWhat am trying to do now is in case of redis failure those transaction will be dropped now. To avoid it db write will fallback.\n\nRate limiting db writes and reads. We having an average load in our system is 13k request per min. So redis failure can bring our database down due to sudden spike. So am planning to control it",
              "score": 1,
              "created_utc": "2026-01-23 17:50:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19vxn5",
          "author": "mrGoodMorning2",
          "text": "**Normal path (Redis healthy):**\n\n* Writes go to DB (durable)\n* Writes also go to Redis (fast path)\n* Reads come from Redis\n\nMy first thought when I read this was that if you write the data to Redis and it dies before writing to DB it will lead to loss of data, this is FINE, but it depends on how CRITICAL the data is. Don't use it for anything payments related (transactions, accounts balances, payment instruments etc)  \n\n\nThe circuit braker and rate limiter for the DB seem fine.\n\n* Using a **DB-backed cache table** as a Redis fallback - good idea or hidden foot-gun?\n\nYou didn't tell us any specific number for reads/writes per second, so I don't think we can answer you, but introducing any new component can be a hidden foot-gun, especially when they share the same data and you have no local transactions between them.  \nIf you want performance can't you make a new index or write data in batches or have separate tables for reads/writes or read replica?   \n\n\n* Alternative patterns you‚Äôve seen work better in production?\n\nWhat we in my company(fin-tech) is put all of the payments as events in Kafka and then when polling events we take an entire batch and persist the batch at once, reducing transactions. Another thing we do is split up core data and metadata in separate tables and not just one huge table, which increases contention",
          "score": 4,
          "created_utc": "2026-01-23 17:35:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19xocx",
              "author": "DevelopmentScary3844",
              "text": "Good points. I have similar thoughts and how about this one:\n\nWrites to db invalidate redis entry first, update db, update redis.",
              "score": 1,
              "created_utc": "2026-01-23 17:43:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19z2q2",
              "author": "saravanasai1412",
              "text": "No we write data to db first. If redis fails db act as source of truth. Let me give bit context about how we using redis.  We start a transaction and store a transaction Id for that session for 10 min. In 10 min they need to complete the transaction. Mostly these data is on redis. \n\nWhat am trying to do now is in case of redis failure those transaction will be dropped now. To avoid it db write will fallback. \n\nRate limiting db writes and reads. We having an average load in our system is 13k request per min. So redis failure can bring our database down due to sudden spike. So am planning to control it",
              "score": 1,
              "created_utc": "2026-01-23 17:50:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1a1vul",
                  "author": "mrGoodMorning2",
                  "text": "13k requests a minute is about \\~216 request a second, which Postgre should be able to handle (if you have decent hardware). Even you have double the traffic it should be fine. Think about the DB performance optimizations I mentioned above.\n\nAlso since I don't fully understand what you store in Redis, I'll ask the stupid question of why does the cache have to be distributed? Can't you make an in-memory cache in the app?",
                  "score": 3,
                  "created_utc": "2026-01-23 18:02:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bfkrx",
          "author": "configloader",
          "text": "Skip redis.\nUse db. Reads that doesnt need to be correct all the time can use secondary db servers",
          "score": 3,
          "created_utc": "2026-01-23 21:52:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kaeou",
              "author": "BornSpecific9019",
              "text": "from personal experience, redis is orders of magnitude faster than postgres\n\nthe problem is keeping it in sync w db (cache invalidation, etc). one of the fun problems worth thinking about",
              "score": 1,
              "created_utc": "2026-01-25 04:45:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1klqo6",
                  "author": "configloader",
                  "text": "Ofc it is. B",
                  "score": 1,
                  "created_utc": "2026-01-25 06:00:40",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1kviot",
                  "author": "Pto2",
                  "text": "The problem of keeping two sources of truth in sync is the sort of thing I would really try to avoid trying to solve if I were working in a financial context!",
                  "score": 1,
                  "created_utc": "2026-01-25 07:18:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1bllmj",
          "author": "ryan_the_dev",
          "text": "You will learn more by implementing it vs asking Reddit.",
          "score": 3,
          "created_utc": "2026-01-23 22:21:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j7jxl",
              "author": "Material-Smile7398",
              "text": "This is the correct answer",
              "score": 3,
              "created_utc": "2026-01-25 01:02:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o19rwmi",
          "author": "Ambitious-Sense2769",
          "text": "I wouldn‚Äôt even mess with caching critical data on a financial system. If correctness matters 100% and if the data isn‚Äôt correct and has huge consequences if it‚Äôs wrong, why even take a risk? Just shard the main db enough to meet the demand you guys need and use locks properly",
          "score": 3,
          "created_utc": "2026-01-23 17:16:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19wbsk",
          "author": "IcyUse33",
          "text": "You could overload the DB cache.\n\nUse an L1 cache instead (in-memory on the web/app server)",
          "score": 2,
          "created_utc": "2026-01-23 17:37:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19y6y9",
              "author": "saravanasai1412",
              "text": "But I feel it may grow without bound and can bring the system down. We trying to make the system to handle failure gracefully.",
              "score": 1,
              "created_utc": "2026-01-23 17:46:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ayomq",
                  "author": "IcyUse33",
                  "text": "Your L1 cache should take care of that. Frameworks like asp.net core have auto eviction and memory support built in.",
                  "score": 2,
                  "created_utc": "2026-01-23 20:33:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ag1pi",
          "author": "ben_bliksem",
          "text": "We use in-memory with MSSQL as a fallback/distributed. With some affinity setup for the same IPs to attempt hitting the same instances of some of our services the cache setup is more than enough - in memory speed when it hits, reliability of the databases.\n\nObviously this won't work for all setups, but it does when reliability is more important than soaring every millisecond you can.",
          "score": 2,
          "created_utc": "2026-01-23 19:06:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kb17x",
          "author": "BornSpecific9019",
          "text": "interesting idea, but not reliable enough IMO.\n\nconsider looking at how tiger beetle handles financial data and degradation.\n\nhttps://github.com/tigerbeetle/tigerbeetle",
          "score": 2,
          "created_utc": "2026-01-25 04:48:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kucvo",
              "author": "saravanasai1412",
              "text": "No for ledger we are not using this. Its we can think of redis cache fallback as redis is single point of failure in our system. To over come that we doing this mostly its meta data which need to complete the transaction.",
              "score": 1,
              "created_utc": "2026-01-25 07:08:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19ntcd",
          "author": "Responsible_Act4032",
          "text": "IF this isn't an LLM created marketing post, for a service or technology that competes with Redis  I don't know what is. No one speaks or formats posts like this.",
          "score": 2,
          "created_utc": "2026-01-23 16:58:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19r3te",
              "author": "jeffbell",
              "text": "OP is a four year old reddit account and a sensible LinkedIn.\n\nI think it's just a high effort post that is written with more than average formality.",
              "score": 4,
              "created_utc": "2026-01-23 17:13:08",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19q7x3",
              "author": "saravanasai1412",
              "text": "Its not a marketing post. Its the question I have formatted with LLM to give the context quick without annoying the people. I don't see anything wrong here. LLM helping me to articulate the question much clear & sharper.",
              "score": 4,
              "created_utc": "2026-01-23 17:09:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19w3l6",
                  "author": "BarfingOnMyFace",
                  "text": "I don‚Äôt see anything wrong with that, OP",
                  "score": 4,
                  "created_utc": "2026-01-23 17:36:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1j7nnt",
              "author": "Material-Smile7398",
              "text": "What service do you see being sold here?",
              "score": 1,
              "created_utc": "2026-01-25 01:02:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhc4p5",
      "title": "Is my uml diagrams acceptable?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/gallery/1qhc4p5",
      "author": "Aware-Somewhere2086",
      "created_utc": "2026-01-19 18:22:00",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.69,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qhc4p5/is_my_uml_diagrams_acceptable/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0iu37m",
          "author": "n00bz",
          "text": "Everyone does things a little different when it comes to UML modeling. For me, I tend to break up the models into particular features and not show the UML model for the full system as it is too complicated for anyone to read and accurately understand.\n\nPlus the granularity for the level you want to capture is difficult because for a large system you can just say authenticate user and not have to worry about the password reset or other related features when making other diagrams.\n\nIn short I would split this out to multiple use cases to better show the flow and not capture the whole system at once.",
          "score": 25,
          "created_utc": "2026-01-19 18:40:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0iws4u",
              "author": "Aware-Somewhere2086",
              "text": "Thanks I'll work on that",
              "score": 1,
              "created_utc": "2026-01-19 18:52:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0iyho8",
          "author": "Separate_Earth3725",
          "text": "Minor details that might make your life a little easier\n\nSystem Design:\nIt might be easier to turn member class into a base user class as an abstract parent class and create Moderator, Admin, Member, and Visitor. That way you can more easily manage permissions between the different user types ie ‚ÄòAdmin().CreateEvent()‚Äô, ‚ÄòModerator().ApproveNewsStory()‚Äô, etc. \n\nIn the reset password flow, in my experience I don‚Äôt check if the email exists and tell give the user feedback. I just tell the user ‚Äúa recovery email will be sent to that email if it exists‚Äù. If you give specific feedback like that, it can create a security gap where a threat actor is effectively querying your database for user emails.\n\nIn your event registration block, it kinda looks like you‚Äôre storing the users event registration before you confirm their payment and available space. If any of the payment and confirmation process fails or the user quits out of the flow, you‚Äôll need to undo that operation. I would do something like [user requests registration -> check available tickets/space* -> if space, request payment from user -> if payment confirmed, save user registration and inform user]\n*Ideally, you‚Äôre only displaying available events to the user at the very beginning of this workflow but theres always the edge case of there being 1 seat available and another user takes it while the current user is looking at this page\n\n\nNitpicks/Personal Preference:\nI like to add colors to my diagrams :) you‚Äôll be staring at them a lot and it helps when there‚Äôs another layer of visual depth\n\nIn your sequence diagrams, the [alt] and [opt] conditions should be placed at the left most edge of those visual containers so it‚Äôs easier for others to quickly read and process what you‚Äôre doing. Don‚Äôt want to have people looking for ‚Äúif‚Ä¶.what?‚Äù. Sequence diagrams should be read left-to-right AND top-to-bottom\n\nYou‚Äôll get mixed opinions on whether you should number your requests in a sequence. I personally don‚Äôt since the nature of the sequence is already sequential. If I need to draw attention to specific flows, I create separate diagrams or I create separate sections with their own titles on the diagram if their actors and entities are the same. \n\nIn your final sequence diagram that covers event registration, the actors/entities that are only used in the [alt] blocks (payment and participation) should still be defined at the very top of your diagram with the other entities/actors of your system. \n\nThe use case diagrams have a lot of visual noise. It might be better to break them up per user type. Sure, you‚Äôll get some duplicate scenarios but the few seconds spent duplicating the diagrams will be worth making it easier to read.",
          "score": 5,
          "created_utc": "2026-01-19 18:59:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j2o90",
              "author": "Aware-Somewhere2086",
              "text": "Thanks for ur help , i really try to do all it of in great way but this diagrams kinda melting my brain to do as the first time",
              "score": 2,
              "created_utc": "2026-01-19 19:18:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0j3koo",
                  "author": "Separate_Earth3725",
                  "text": "Yeah, for a first time it‚Äôs better than what I could‚Äôve produced at that time. Overtime, you start getting better and faster especially as you get more comfortable with the idea of how the think of technical communication and get better at the tools you‚Äôre using to generate the diagrams.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:22:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0ixqpl",
          "author": "FuckItImLoggingIn",
          "text": "Here is my take: You only need to ask yourself one question - \"does whatever this diagrams tries to convey, manage to convey it?\".   \n  \nEasiest way to find the answer would be to see if the people this targets find it useful.\n\nSo then the question becomes - \"do the people that read this diagram find it useful?\".",
          "score": 5,
          "created_utc": "2026-01-19 18:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j0bvu",
              "author": "Euphoric-Usual-5169",
              "text": "\"\"does whatever this diagrams tries to convey, manage to convey it?\".\n\n  \nThis question led me away from UML. I once read a book about UML and then did a lot of UML diagrams. Turns out nobody really got them. Simple block diagrams and flow charts are way better.",
              "score": 6,
              "created_utc": "2026-01-19 19:07:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0jbpmy",
          "author": "GrogRedLub4242",
          "text": "order of learning: English then programming then UML",
          "score": 3,
          "created_utc": "2026-01-19 20:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o13i3xq",
              "author": "ben_bliksem",
              "text": "Well English, the programming, the circles-squares and line drawings, then go through an existential crisis about how the benefits of rewriting this fucked up monolith doesn't outweigh the fact that you won't be sticking around for long enough for it to matter, then some devops and then maybe, maaaaaybe, you'll learn dome UML which somebody is going to redraw into circles-squares and lines drawing anyway.",
              "score": 3,
              "created_utc": "2026-01-22 18:57:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0j2bt1",
          "author": "Spare-Builder-355",
          "text": "very nice diagrams, you'll get nice grades for them in your school and never come back to uml again.",
          "score": 8,
          "created_utc": "2026-01-19 19:16:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0iy0ax",
          "author": "mr_mark_headroom",
          "text": "Yes it's a good diagram. How will you decompose it?",
          "score": 1,
          "created_utc": "2026-01-19 18:57:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgfr0w",
      "title": "How do you prevent design drift during PR reviews?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qgfr0w/how_do_you_prevent_design_drift_during_pr_reviews/",
      "author": "senthuinc",
      "created_utc": "2026-01-18 18:10:40",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 0.92,
      "text": "Hey folks, looking for some honest feedback on a problem I keep running into during PR reviews.\n\nThe teams I worked and work with rely, at most on a single PR checklist. As systems grow, number of teams grow, org maturity mandates come into effect, we want more comprehensive checks (architecture, security, performance, conventions, etc.), but long checklists quickly get ignored because they slow reviews down - totally empathize with that.\n\nEspecially with LLM-assisted coding becoming more common, I‚Äôm also noticing more design drift - code that works, passes review, but slowly diverges from intended architecture and patterns accumulating technical debts in the blind if you will. This is getting harder to catch with today‚Äôs PR process.\n\nI‚Äôm exploring an idea around making PR checks more adaptive and context-aware, without overwhelming developers.\n\nCurious to hear:\n\nDo you use PR checklists today?\n\n1. Have you experienced checklist fatigue?\n2. Have you noticed increased design drift increasing with LLM-assisted coding?\n3. Would love to hear how others are dealing with this. ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qgfr0w/how_do_you_prevent_design_drift_during_pr_reviews/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0c1ekt",
          "author": "commanderdgr8",
          "text": "one idea is that as much of this checklist should be automated as possible. for example, linting and formatting as pre-commit hooks. See if any items from your checklist can be added as a rule in your linting and formatting. you might want to write a custom linting or formatting rule for your programming language.  This is possible and should be used.  \nSecond is automated test case which can run as part of CI/CD pipeline, particularly security and performance related checks, so even if they are missed in code review, they are caught in the pipeline.  \nWe also use LLM assisted code review, with tools like Code Rabbit and Claude which can catch many of the issues. You can ask teach CodeRabbit about your particular coding pattern, or any issues you want it to identify in future and next time onwards it can identify those issue. Same way you can ask Claude to review your code and find specific issues and in the system prompt you can ask it to find particular way a code is written the flag it.",
          "score": 6,
          "created_utc": "2026-01-18 18:34:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cg6pa",
              "author": "virtualstaticvoid",
              "text": "I agree that automation is key for things like static code analysis and linting, formatting, dependency checks for CVEs, etc, but I don't think there's anything that can easily detect something like architectural drift as such, or deviations from established patterns of the application, or more simply, where the code doesn't reuse helper classes which maybe the developer didn't know about.\n\nDo/can LLM based review tools detect these types of things?",
              "score": 1,
              "created_utc": "2026-01-18 19:44:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0cy1gu",
                  "author": "sansp00",
                  "text": "We use CoPilot at work and ended up building an instruction file derived from the Awesome CoPilot repository. It's been meh so far to be honest. What really worked well was the ArchUnit test suite I added. Even devs liked it since it prevented them from some 'design' pitfalls very early in their work.",
                  "score": 1,
                  "created_utc": "2026-01-18 21:14:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0bziyz",
          "author": "breek727",
          "text": "PR checklists imho are only useful for new engineers to know what to look out for, I.e got your indices right etc. \n\nAnything meaningful should have some level of none ai automation, like static analysis that stuff lives where it should be etc",
          "score": 2,
          "created_utc": "2026-01-18 18:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0c1bub",
          "author": "Most_Double_3559",
          "text": "Most importantly here, PR checklists are a sign of something else: Clunky DevEx (usually in architecture or support).\n\nThe *correct* thing should be the *easy* thing. Then, you won't need to check for people \"doing it the wrong way\" because laziness (the same laziness that makes checklists not work) is now taking care of that for you.\n\nTo your examples:\n\n* Architecture drift / conventions: Ideally the designated architecture should be *easy to plug into.* Suppose you have a list of validators. You should be able to copy a class, add an object to a list somewhere, and done. If it's more complicated, you should be able to just @ inject a new module and still call it a day early. If your architecture / convention requires modifying 5 files to plumb in your new code nobody will ever do it. Don't make them do that, and they'll follow the pattern.\n* Security: Devops security checkers exist, or if you'd prefer, only require reviews if touching designated \"high risk\" files like password management code.\n* Performance: Have a canary environment. Block a release if performance decreases X% day-over-day. Done, no sense in hand wringing about whether some statement takes a bit longer in reviews.\n\nThat seems like a reasonable start to me, though of course this is something that has to evolve in-house over time :)",
          "score": 3,
          "created_utc": "2026-01-18 18:34:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0f5rmv",
          "author": "ryan_the_dev",
          "text": "I have been working on making my LLM very good at following PR checklists and now using multiple agents. \n\nNot gonna lie and say it‚Äôs perfect. Still testing it out at work. \n\nhttps://github.com/ryanthedev/code-foundations",
          "score": 1,
          "created_utc": "2026-01-19 04:21:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0h8p8o",
          "author": "Audzkan",
          "text": "For architecture drifts. You could write architecture tests which are executed in the CI/CD  on the commits to enforce specific architecture rules ti prevent drifts",
          "score": 1,
          "created_utc": "2026-01-19 14:14:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0i4fhc",
          "author": "alonsonetwork",
          "text": "If you establish standards and convention, AI can easily catch on this and you can add it to your pipeline. I setup a bunch of Claude files to steer agents on my code, and I have Haiku review my code for divergence from convention and standards. It does a good enough job.\n\nNow: design and logical correctness is YOUR job to catch. Your fatigue comes from the fact that you're doing everything: design, logic, standards, convention, and probably style and linting. Reduce it.\n\n- As others mentioned: static analysis on things that can be statically analyzed (lint, format, types, test, code coverage)\n- AI can catch standards and convention drift, based on git diffs. You must define your standards and conventions.\n- you focus purely on meaningful, non-robotic work that comes from the outside (requirements, external problems)",
          "score": 1,
          "created_utc": "2026-01-19 16:45:31",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkuj5r",
      "title": "Handling likes at scale",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qkuj5r/handling_likes_at_scale/",
      "author": "rkaw92",
      "created_utc": "2026-01-23 15:55:44",
      "score": 11,
      "num_comments": 15,
      "upvote_ratio": 0.87,
      "text": "Hi, I'm tackling a theoretical problem that can soon become very practical. Given a website for sharing videos, assume a new video gets uploaded and gains immediate popularity. Millions of users (each with their own account) start \"liking\" it. As you can imagine, the goal is to handle them all so that:\n\n\\* Each user gets immediate feedback that their like has been registered (whether its impact on the total is immediate or delayed is another thing)\n\n  \n\\* You can revoke your like at any time\n\n  \n\\* Likes are not duplicated - you cannot impart more than 1 like on any given video, even if you click like-unlike a thousand times in rapid succession\n\n  \n\\* The total number of likes is convergent to the number of the users who actually expressed a like, not drifting randomly like Facebook or Reddit comment counts (\"bro got downcommented\" ‚ò†Ô∏è)\n\n  \n\\* The solution should be cheap and effective, not consume 90% of a project's budget\n\n  \n\\* Absolute durability is not a mandatory goal, but convergence is (say, 10 seconds of likes lost is OK, as long as there is no permanent inconsistency where those likes show up to some people only, or the like-giver thinks their vote is counted where really it is not)\n\n  \nPreviously, I've read tens of articles of varying quality on Medium and similar places. The top concepts that seem to emerge are:\n\n\\* Queueing / streaming by offloading to Kafka (of course - good for absorbing burst traffic, less good for sustained hits)\n\n\\* Relaxing consistency requirements (don't check duplicates at write time, deduplicate in the background - counter increment/decrement not transactional)\n\n\\* Sharded counters (cut up hot partitions into shards, reconstruct at read time)\n\n  \nMy problem is, I'm not thrilled by these proposed solutions. Especially the middle one sounds more like CV padding material than actual code I'd like to see running in production. Having a stochastic anti-entropy layer that recomputes the like count for a sample of my videos all the time? No thank you, I'm not trying to reimplement ScyllaDB. Surely there must be a sane way to go about this.\n\n  \nSo now I'm back to basics. From trying to conceptualize the problem space, I got this:\n\n\\* For every user, there exists a set of the videos they have liked\n\n\\* For every video, there exists a set of the users who have liked it\n\n\\* These sets are not correlated in any way: any user can like any video, so no common sharding key can be found (not good!)\n\n\\* Therefore, the challenge lies in the transformation from a dataset that's trivially shardable by userID to another, which is shardable by videoID (but suffers from hot items)\n\n  \nIf we naively shard the user/like pairs by user ID, we can potentially get strong consistency when doing like generation. So, for any single user, we could maintain a strongly-consistent and exhaustive set of \"you have liked these videos\". Assuming that no user likes a billion videos (we can enforce this!), really hot or heavy shards should not come up. It is very unlikely that very active users would get co-located inside such a \"like-producing\" shard.\n\n  \nBut then, reads spell real trouble. In order to definitely determine the total likes for any video, you have to contact \\*all\\* user shards and ask them \"how many likes for this particular vid?\". It doesn't scale: the more user shards, the more parallel reads. That is a sure-fire sign our service is going to get slower, not faster.\n\n  \nIf we shard by the userID/videoID pair, instead? This helps, but only if we apply a 2-level sharding algorithm: for each video, nominate a subset of shards (servers) of size N. Then, based on userID, pick from among those nominated ones. Then, we still have hot items, but their load is spread over several physical shards. Retrieving the like count for any individial video requires exactly N underlying queries. On the other hand, if a video is sufficiently popular, the wild firehose of inbound likes can still overflow the processing capacity of N shards, since there is no facility to spread the load further if a static N turns out to be not enough.\n\nNow, so far this is the best I could come up with. When it comes to the value of N (each video's likes spread over \\*this many\\* servers), we could find its optimal value. From a backing database's point of view, there probably exists some optimum R:W ratio that depends on whether it uses a WAL, if it has B-Tree indices, etc...\n\n\n\nBut let's look at it from a different angle. A popular video site will surely have a read-side caching layer. We can safely assume the cache is not dumb as a rock, and will do request coalescing (so that a cache miss doesn't result in 100,000 RPS for this video - only one request, or globally as many requests as there are physical cache instances running).\n\n  \nNow, the optimum N looks differently: instead of wondering \"how many read requests times N per second will I get on a popular video\", the question becomes: how long exactly is my long tail of unpopular videos? What minimum cache hit rate do I have to maintain to offset the N multiplier for reads?\n\n  \nSo, for now these are my thoughts. Sorry if they're a bit all over the place.\n\n  \nAll in all, I'm wondering: is there anything else to improve? Would you design a \"Like\" system for the Web differently? Or maybe the \"write now, verify later\" technique has a simple trick I'm not aware of to make it worth it?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qkuj5r/handling_likes_at_scale/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o19znt5",
          "author": "IlliterateJedi",
          "text": "It might be worth checking out Designing Data-Intensive Applications.  I remember the early chapters went into a lot of detail about strategies Twitter used to handle similar problems.  I wouldn't be surprised if 'likes' were covered in the book later on.",
          "score": 4,
          "created_utc": "2026-01-23 17:52:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ageoi",
              "author": "halfxdeveloper",
              "text": "That is a good book.",
              "score": 1,
              "created_utc": "2026-01-23 19:07:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1aptm0",
              "author": "rkaw92",
              "text": "Coincidentally, I have this book next to me right now. It's a great book that has transformed how I think about data, particularly in terms of graphs. Alas, nothing seems to be of direct relevance to the question at hand. I have the first edition, though - not sure if the 2nd is out yet or if it mentions some more social-network-adjacent topics. Would be cool if it did.",
              "score": 1,
              "created_utc": "2026-01-23 19:52:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19dn9c",
          "author": "heavy-minium",
          "text": "It's old and I'm not sure if it reflects the reality of more modern architectures nowadays, but still, this paper might be interesting to you: [TAO: Facebook‚Äôs Distributed Data Store for the Social Graph](https://www.usenix.org/system/files/conference/atc13/atc13-bronson.pdf) . I haven't read it, but my gut-feeling is that is goes toward edge on graph per like, and extensive memcache usage for counts.\n\nI would look more into research papers in general, for example stuff like [Feeding Frenzy: Selectively Materializing Users' Event Feeds](https://sns.cs.princeton.edu/assets/papers/2010-sigmod-silberstein.pdf?utm_source=chatgpt.com)\n\nAnyway, looking for older research papers is probably going to deliver better insights than whatever those Medium blogs are describing.",
          "score": 3,
          "created_utc": "2026-01-23 16:12:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1afen1",
          "author": "halfxdeveloper",
          "text": "This is a case study in over-engingeering. It‚Äôs a like system not telemetry in an ICU. Do optimistic updates on the UI. The backend is a queue system with retry. If your product is so stable and developed that you have the opportunity to focus on this, then I‚Äôm purely jealous. I don‚Äôt want this to come off as mean. I‚Äôm trying to impart that this one of those ‚Äúmake it work and optimize later.‚Äù",
          "score": 5,
          "created_utc": "2026-01-23 19:03:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1aw6av",
              "author": "rkaw92",
              "text": "I agree it is likely overengineering. And I wish the product was already mature. The thing is, the scenario is the following: on day 1, a popular content creator launches a video on the site. Nobody uses it, and then suddenly 10 million users storm the proverbial door. There's no in-between, there is only panic-surge mode. My aim is to make this happen on a shoestring budget. To be clear, this is not some life-or-death situation, and frankly I don't even get paid for this. It's a hobby project. But if I can, I'd very much like to see it happen.\n\nThe last point is also why I'd like to keep moving parts to a minimum. If I can toss the message broker infra, for sure I will. If I'm reasonably convinced that it would help me absorb traffic spikes for a \"slashdot moment\", it stays. What I'd like to avoid most is inefficiency - for example, pointing a Kafka firehose at some poor system that will throw concurrency errors like memcached.",
              "score": 1,
              "created_utc": "2026-01-23 20:21:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1a8kz7",
          "author": "mrGoodMorning2",
          "text": "\\* Queueing / streaming by offloading to Kafka (of course - good for absorbing burst traffic, less good for sustained hits)\n\nWhat don't like you about this solution? Each like/dislike being an event into a queue and then consuming a batch of events and persisting an entire batch into the DB once sounds good to me as a start. Kafka and scale easily with increasing topic partitions.\n\nWhat minimum cache hit rate do I have to maintain to offset the N multiplier for reads? Isn't this just finding a balance between client demand (how many popular videos are watched at any given time) and how much money you have for caching servers.",
          "score": 2,
          "created_utc": "2026-01-23 18:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1at41r",
              "author": "rkaw92",
              "text": "Well, I don't like that Kafka is often presented as a silver bullet that magically solves throughput problems. Yes, I could do a routing strategy where I use the video ID for partitioning, but then:\n\n\\* The like/unlike action is not idempotent, so the deduplication needs to happen downstream...\n\n\\* ...OR I need the target database checkpointing to be coupled with the Kafka offset management (transactionally remember where I left off), plus the message producer needs to have exactly-once semantics somehow, e.g. by using the Idempotent Producer feature\n\n\\* And additionally now we can get a hot partition in Kafka.\n\nFor the deduplication, I have considered Kafka Streams, but it's somewhat hard to produce a viable window (likes don't expire), and then we're just shifting the load from something that's plainly visible (like Redis) to... an embedded RocksDB instance? Not sure I like this trade-off.",
              "score": 1,
              "created_utc": "2026-01-23 20:07:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19r7ng",
          "author": "joelparkerhenderson",
          "text": "Here's what I would try first if you're curious about how to try it yourself:\n\n\\- Install Redis, run redis-benchmark, and look at GET, SET, INCR, DECR. On my older MacBook, each of those do 200,000 operations per second.\n\n\\- Install Rust, write a simple HTTPS server that accepts GET, PUT, PATCH, and call Redis with these.\n\n\\- Use any benchmark tool you like, such as wrk or oha, to discover your throughput of Axum PATCH to Redis INCR meaning \"like\" & DECR meaning \"unlike\".\n\n\\- If the throughput is lower than you need, then buy more CPU power because it's cheaper than development complexity. If your Redis is running low on memory, then buy more RAM because it's cheaper than development complexity. For lots of CPUs and RAM in one server I especially like the Oxide rack.\n\n\\- To get the counts from Redis into your persistent database such as Postgres, add an Axum step to use Redis LPUSH to track the recently-changed keys. Create a background batch job, such as every 10 minutes, to copy the key counts into Postgres.\n\n\\- To track a user's state per video, use Redis Streams and specifically event sourcing. Drain these with typical async background job into Postgres. The core idea here is that you're separating the concept of the video like count and the concept of one user state with regard to one video.\n\nOnce you get the above working, that's your baseline for what's possible using a simple system.\n\n\\- Deploy your video web project, and look at real world data.\n\n\\- If your traffic is growing so fast that you're going to outgrow your Oxide rack within one quarter or so, then look at vendors such as Redis Enterprise and Upstash, and also look at hiring a software engineer who has skills in scaling social networks and working knowledge of the various Facebook papers and Google papers about upscaling.\n\n\\- You mention below \"offloading this to a tier 2 storage + restoring sounds like a recipe for a synchronization nightmare\". In practice it's easier than it sounds, because it's such a good fit for background batch jobs and eventual consistency.\n\n\\- You mention below how to know if a user has liked a video. Start with your persistent storage e.g. Postgres and using a read-only replica when you build the user-video page or UI/UX. By the way, this is not a single source of truth, strictly speaking, because of database replica limitations (e.g. CAP & PACELC), yet in practice it's close enough to get you started. As above, you can try starting this way, so you can benchmark it. If/when you're approaching capacity limits, such as within one quarter or so, then revisit.",
          "score": 3,
          "created_utc": "2026-01-23 17:13:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19yova",
              "author": "IcyUse33",
              "text": "HyperLogLog could help here as well. It's not an exact amount of likes, but it is fast and it's good enough for estimates like \"1.2k likes\".",
              "score": 3,
              "created_utc": "2026-01-23 17:48:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1aop3c",
                  "author": "rkaw92",
                  "text": "I have considered HLL, but then, retracting a like becomes problematic. Sure, I could do an adaptive strategy like \"<100 likes = counter, >=100 go to HLL\", but then I admit the conditional logic and the edge behavior (including rescinding likes) are rather scary.",
                  "score": 2,
                  "created_utc": "2026-01-23 19:46:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1anwum",
              "author": "rkaw92",
              "text": "Thanks for your response! Yeah, Redis is seriously great. An amazing piece of technology. It's my top pick so far, considering throughput. I concur with the numbers - I've had easy 200K+ results in the past; it's a good baseline. The thing is, if storage exceeds RAM, all hell breaks loose. If this was for counters only, it might be fine (as many keys as videos), but the \"no duplicate likes\" requirement seems to cause memory explosion, because now you have to remember who liked what. It's not just quantitative. I'm already considering some strong Redis-friendly optimizations like \"video IDs and user IDs shall be densely-packed integers with no holes\" - supposedly good old Redis can then use an optimized array-based index for the keys.\n\nI guess my main problem is this: Redis is well-suited for hot data, but it could be hard to justify keeping global state (counts + who liked what) forever, and offloading this to a \"tier 2\" storage + restoring sounds like a recipe for a synchronization nightmare. This is why I immediately jumped to sharding - I assume that I will run out of memory on a single host, so I'd rather plan for it than mitigate in other ways. The alternative, of course, is to splurge on RAM sticks, perhaps run 1 Redis instance per core, and see how many GBs/core we can manage :P\n\n  \nAlso I'm thinking more of a Set-type data structure for the change detection. A List doesn't seem too practical, given how when, say, a new super-popular K-Pop video drops, the same one item is going to be receiving likes all over. It's a niche case, sure, but AFAIK surges like these are normal for video sites like YouTube.\n\n100% finding people who have already solved this in the past would be great. The thing is, this is for an extremely underfunded volunteer project (think: budget = pocket lint), so in this case, the people is me :-|\n\n  \nThanks for mentioning the \"Oxide rack\" - right now, I'm trying to squeeze as much as possible out of truly frugal resources, so buying servers by the rack is not realistic. I have done some minor self-hosted DC work in the past, but this looks promising. Here's hoping the current RAM prices don't throw a monkey wrench in these guys' day-to-day.",
              "score": 2,
              "created_utc": "2026-01-23 19:43:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1adxrx",
          "author": "saravanasai1412",
          "text": "I would approach this problem as you need a sustainable solution without fancy tech. I would go with simple redis bloom filters which helps you to check that user liked the video or not .for quick lookup. It may have false positive but you check how LinkedIn does it. They just store recent post like in cache and sync with database in async.  \n\nNext issue counting the like. just use redis increment simple ans easy. Just sync it with db in certain interval.",
          "score": 1,
          "created_utc": "2026-01-23 18:56:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1auuy1",
              "author": "rkaw92",
              "text": "And that would work, but there still needs to be something that definitely (not probabilistically) tells the user if they have liked the item. As in, the information needs to live somewhere. It can't be Local Storage, because a user will inevitably want to access the system from multiple devices. Also, I'd like to keep a single source of truth for this, if at all possible.",
              "score": 1,
              "created_utc": "2026-01-23 20:15:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qh1tzs",
      "title": "Are Transactional Middleware programs still used in backend?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qh1tzs/are_transactional_middleware_programs_still_used/",
      "author": "sexyman213",
      "created_utc": "2026-01-19 11:34:15",
      "score": 9,
      "num_comments": 16,
      "upvote_ratio": 0.85,
      "text": "I'm currently reading the 'Principles of Transaction Processing' book and I can see that a lot of technologies mentioned in the book are no longer used but serve as a good history lesson. The author namedrops several \"transactional\" middleware products/protocols/standards such as - HP‚Äôs ACMS, IBM Trivoli, CORBA, WCF, Java EE, EJB, JNDI, Oracle‚Äôs TimesTen etc. Are these and similar TP monitor tools used anymore or is it all web services and microservies now?\n\nA recurring theme throughout the book is the concept of \"transaction bracketing\" , i.e., handling business process requests as a transaction with ACID properties, not just at a database level but the entire request itself. What are the current technologies used to do this?\n\n  \nEdit: about transaction bracketing",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qh1tzs/are_transactional_middleware_programs_still_used/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0glloz",
          "author": "KaleRevolutionary795",
          "text": "Transactional Middleware is still used. Either as ORM such as¬† Hibernate, EclipseLink or JPA or even an implementation of JTA. You could even reason that Mainframe architecture is a kind of transactional middle ware.\n\n\nSome specfic implementations of that: Corba, java EE (eg Tomcat/websphere), ejb, JNDI are all indead being phased out across most if not all organisations. These are implementations whose downsides have been resolved by evolutions.¬†\n\nYou can add SOAP to that but that's more an API , however the business transactionality is a part of that.¬†\n\n\nSo in effect the answer is yes and no: the statement is a little too broad, but correct in their examples of defunct technologies¬†",
          "score": 5,
          "created_utc": "2026-01-19 11:41:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0gr5ft",
              "author": "sexyman213",
              "text": "A recurring theme throughout the book is the concept of \"transaction bracketing\" , i.e., handling business process requests as a transaction with ACID properties, not just at a database level but the entire request itself. What are the current technologies used to do this?\n\n  \n(I have also editted the post to add this)",
              "score": 2,
              "created_utc": "2026-01-19 12:25:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0gu8pj",
                  "author": "OneHumanBill",
                  "text": "You're right and use of this whole concept has fallen off.  There are two main reasons. \n\nFirst and foremost, practically speaking when you need something to be transactional, it's not the whole process we care about.  It's just the storage part.  If you finish storing the result of a transaction including all in memory processing then really that means you've finished the storage end.  So we're down to a database ACID transaction, and usually a simple one that doesn't require even a multi-phase commit (though they're still out there if you really need them).  Even if you're doing something really old fashioned with a file operation, there's usually an rdbms status or something you need to mark as complete at the end.\n\nSecondly is the lessened need for ACID in many modern applications.  In a bank transaction, it is absolutely necessary to make sure that debits and credits balance, and this is the origin of ACID's necessity.  But if you need to deliver a social media notification then the need to ensure a correct transaction becomes a lot fuzzier.  If something fails, it's not the end of the world, and the notification can be delivered later when things are working better.  This is the world of no-sql where full ACID isn't considered necessary even in storage. \n\nI last saw a CORBA application in the wild about five years ago during a cloud migration.  Everybody was scared to touch the thing.  I spent weeks trying to figure out what it even did.  There didn't seem to be any way to move the stupid thing.  For all I know they left it running forever.",
                  "score": 5,
                  "created_utc": "2026-01-19 12:47:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0px2xc",
                  "author": "Wiszcz",
                  "text": "We used ‚Äútransaction bracketing‚Äù all the time in the monolith era, and we still use it very often. Basically, every local task is done within this transaction bracketing. The implementation is simple: you start a database transaction at the beginning of the request and end it when request processing finishes.\n\nThere are a few problems when a task involves an external service.  \n1 - Trying to do a multi-system transaction is very hard and costly. Because of that, many approaches exist to ensure that work is done fully or not at all, or that partially completed work can be mitigated.  \n2 - If you call one or more external systems during a transaction, it can take a lot of time. During that whole time the transaction stays open, which is very bad for database performance. As a result, we now try a similar approach as with multiple services: ensure that even if part of a task fails, the saved state is recoverable.\n\nIn my opinion, doing this well is much harder and more nuanced than the old transaction bracketing :)",
                  "score": 1,
                  "created_utc": "2026-01-20 19:19:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0gye1j",
          "author": "i_be_illin",
          "text": "Multisystem transactions are too hard. Very difficult to get right and get consistent behavior. Other patterns emerged that make the inevitable failures that occur in distributed systems easier to deal with.",
          "score": 4,
          "created_utc": "2026-01-19 13:15:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0he8a5",
          "author": "RipProfessional3375",
          "text": "Something similar to a distributed transaction system is the optimistic lock used in Event Sourcing. And the append condition in the new Dynamic Consistency Boundary specifications.\n\nThe lock and append condition are more a design and specification than a specific technology.\n\nThe general gist goes: application queries messages, makes a decision, attempts to write messages based on that decision, gets rejected if the query and result they used has become outdated in the meantime.",
          "score": 2,
          "created_utc": "2026-01-19 14:43:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0st9lt",
          "author": "HosseinKakavand",
          "text": "Thanks for sharing this. I hadn't seen the term \"transaction bracketing\" used in a modern context, but it describes exactly how we handle process orchestration in [Luther](https://www.reddit.com/r/luthersystems/). \n\nWe treat each step in a workflow as a bracketed transaction: validating a response, syncing the internal state, and raising the next trigger event all happen as a single atomic unit. We use a \"Common Operating Script\" to define these transitions as deterministic updates that durably drive sagas across business apps like Stripe or SAP. It‚Äôs essentially bringing that TP-monitor rigor back to modern stacks to ensure the process stays consistent even when individual services are unreliable. Definitely adding that book to my list!",
          "score": 1,
          "created_utc": "2026-01-21 04:31:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0u0hu5",
              "author": "sexyman213",
              "text": "do you use any existing solutions to handle the TP monitoring part or do you build everything inhouse?",
              "score": 1,
              "created_utc": "2026-01-21 10:42:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o10jy4l",
          "author": "Revolutionary_Ad7262",
          "text": "I don't think so\n\nPeople usually just implement it on themself. For example idempotency keys or message queues/event sourcing\n\nThe idea of the past was to use RMI like approach for everything and use the platform for such a concerns. Nowadays the lean approach (use simple protocols with simple interfaces) is in the mainstream",
          "score": 1,
          "created_utc": "2026-01-22 08:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hyf8b",
          "author": "Middlewarian",
          "text": "I'm building a [C++ Middleware Writer.](https://www.reddit.com/r/codereview/comments/qo8yq3/c_programs/)  It's an on-line code generator that's implemented as a 3-tier system.  Each of the tiers is implemented using code that's been generated.   The back tier is closed, but the¬†[middle tier](https://github.com/Ebenezer-group/onwards/blob/master/src/tiers/cmwA.cc)¬†is open.  I'm the only user so far.",
          "score": 0,
          "created_utc": "2026-01-19 16:18:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0if39x",
              "author": "sexyman213",
              "text": "what's it used for? also what's a code generator?",
              "score": 1,
              "created_utc": "2026-01-19 17:33:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0it898",
                  "author": "Middlewarian",
                  "text": "It helps build distributed systems.  It writes messaging and serialization code.  This is the [generated code](https://github.com/Ebenezer-group/onwards/blob/master/src/tiers/cmwA.mdl.hh) that I use to build the middle tier of my code generator.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:36:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qh59uv",
      "title": "How did NeetCode make AI hints? Feature",
      "subreddit": "softwarearchitecture",
      "url": "https://i.redd.it/kqk7qq74ebeg1.png",
      "author": "Big_Building_3650",
      "created_utc": "2026-01-19 14:15:59",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qh59uv/how_did_neetcode_make_ai_hints_feature/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0hj950",
          "author": "xAmorphous",
          "text": "Without him commenting I doubt anyone knows for sure. My guess is that it's just either a simple prompt or a langchain graph that has known approaches for a problem.",
          "score": 3,
          "created_utc": "2026-01-19 15:08:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0lj25a",
          "author": "Effective-Total-2312",
          "text": "This doesn't look complex. No need for fine-tuning.  \n  \nMost likely, there is a system prompt explaining the LLM its role, environment, and expected structured output with three hints.  \n  \nThen there is probably a specific prompt for each problem, which explains the LLM one or multiple possibilities of solving a problem (most likely in some kind of pseudo code). Finally, this should include your code snippet for tailored hints.",
          "score": 2,
          "created_utc": "2026-01-20 02:49:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhrbrw",
      "title": "How do you evolve architecture?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qhrbrw/how_do_you_evolve_architecture/",
      "author": "Historical_Ad4384",
      "created_utc": "2026-01-20 04:32:08",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "Hi\n\nI am trying to build process on how to evolve our architecture as features get prioritized progressively over time and the system has to adapt these ever changing business requirements.\n\nI'm finding a hard time in balancing the short wins vs future trophy while documenting the system's architectural evolution as it progresses.\n\nAny advice?",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qhrbrw/how_do_you_evolve_architecture/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0mxcms",
          "author": "Mountain_Sandwich126",
          "text": "Evolutionary architecture is good, thing is \"talking a walk around the pond\" as new i formation / priorities shift. New information could fundamentally shift the direction the architecture goes long term based on evolving business needs.\n\nBasically, architecture should not be stagnant, and should be continually reviewed as the business gains new information and shares it with the broader leadership. \n\nMy 2c, gregor hope has way better material",
          "score": 5,
          "created_utc": "2026-01-20 08:54:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ml9u1",
          "author": "commanderdgr8",
          "text": "One way to use Architectural Decision Records (ADR), this allows you to document your thought process about why, when, what we considered when you make a decision. In this document you can mark that certain decision can be reverted in future when you implement some feature which are still being discussed or in pipeline. Another thing you can do is to create document/diagrams which shows current state vs future desired state.  \nIn the past I have seen that when we plan that our architecture will take one path, but then suddenly management decide to scrap features, or projects, want to introduce some new feature, may be due to Market dynamics, merger or acquisition of other companies. So ADRs are the most useful in when this happens.",
          "score": 3,
          "created_utc": "2026-01-20 07:04:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mmbdv",
          "author": "Leonobrien",
          "text": "Approach the architecture with adaptability, modularity and interoperability as priority qualities. Look for architecture patterns that enable these. \n\nIf all requirements are known, but subject to change, you can design for a target state, and consider the transitional states needed to reach the end state. If not all requirements are known, and current requirements are subject to change, your role is to assist in navigating discussions regarding the cost of change - anything can be done if you have enough money and a willingness to invest.\n\nAll designs will have constraints, you cannot design for all things, everywhere, at all times - as some people might think you can. As your architecture evolves the constraints will also evolve - i.e. transition state 1 may not allow dynamically loading of modules, because you first build foundations.\n\nKeep in mind this comes with trade-offs - security complexity, performance etc. - that should be explored.",
          "score": 2,
          "created_utc": "2026-01-20 07:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13zmvt",
          "author": "serverhorror",
          "text": "By favoring simplicity, easy to change and easy to reason about architecture over \"good\" architecture.\n\nRemove as much as possible, but no more. If you need more than one PowerPoint slide, it's too complicated. Yes, PowerPoint, because I need to be able to communicate things to different audiences and I needwant to be able to explain things in a way so that when they meet, and I'm not there, they still talk about the same things.",
          "score": 2,
          "created_utc": "2026-01-22 20:17:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mpmcl",
          "author": "gbrennon",
          "text": "As every \"good software architect\": it depends üòÖü§£\n\nFirst:\n\nYou should take notes related to trade-offs. \n\nThose trade-offs includes engineering team performanceand this is more relevant than application performance bcs software engineers are more expensive than servers or infrastructure.\n\nYou should noy only implant architecture but train ur engineers so they can perform in this new structure.\n\nYou should also collect everytime feedbacks from the team to understand if u are improving theyre performance or not",
          "score": 1,
          "created_utc": "2026-01-20 07:42:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlr6i1",
      "title": "DoorDash Applies AI to Safety Across Chat and Calls, Cutting Incidents by 50%",
      "subreddit": "softwarearchitecture",
      "url": "https://www.infoq.com/news/2026/01/doordash-safechat-ai-safety/",
      "author": "rgancarz",
      "created_utc": "2026-01-24 16:18:56",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.78,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Article/Video",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qlr6i1/doordash_applies_ai_to_safety_across_chat_and/",
      "domain": "infoq.com",
      "is_self": false,
      "comments": [
        {
          "id": "o1gmlij",
          "author": "MeggaMortY",
          "text": "Looking at that diagram, the \"AI\" which is making the profanity/sexual content pedictions could just as well been a classical ML model. Nothing ground breaking, but sure more of this AI than stupid undressing bots and slop generators.",
          "score": 9,
          "created_utc": "2026-01-24 17:41:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1j744o",
              "author": "AvailableFalconn",
              "text": "Transformer-based text embeddings were the SOTA way to train safety models in 2019. ¬†Modeling the semantics of text is what they were designed for. ¬†But rebranding it as LLM gets you promoted.",
              "score": 3,
              "created_utc": "2026-01-25 00:59:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1idqdl",
              "author": "eemamedo",
              "text": "Yup. The profanity/sexual content could have a simple multi-class classifier. LLM does the skip the entire \"training on data\" step but in the end, introduces new problems.",
              "score": 1,
              "created_utc": "2026-01-24 22:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1k2nir",
              "author": "its_k1llsh0t",
              "text": "Everything is AI, if you believe!",
              "score": 1,
              "created_utc": "2026-01-25 03:56:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1h5urm",
          "author": "BigWheelsStephen",
          "text": "Implemented something very similar for my company:\n- fast & cheap check for each messages via LLM\n- long & expensive checks on full part of conversation\nNot cheap in the end but works fine and can do more than ML models that can ‚Äúonly‚Äù work to detect insults and stuff like that while predators can be more subtile.",
          "score": 2,
          "created_utc": "2026-01-24 19:03:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk67l9",
      "title": "Workflow Designer/Engine",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qk67l9/workflow_designerengine/",
      "author": "DesignMinute5049",
      "created_utc": "2026-01-22 20:44:33",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "We‚Äôre evaluating workflow engines to act as a central integration layer between SAP, AD/Entra ID, ticketing systems, and other platforms. Which solution would you recommend that provides robust connectors/APIs and integration capabilities? A graphical workflow designer is a nice-to-have but not strictly required.",
      "is_original_content": false,
      "link_flair_text": "Tool/Product",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qk67l9/workflow_designerengine/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o14auh8",
          "author": "kartas39",
          "text": "n8n, airflow, kestra",
          "score": 2,
          "created_utc": "2026-01-22 21:09:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o171ezm",
          "author": "engx_ninja",
          "text": "Depends on quality attributes. Camunda might be enough, but Apache beam, or smth even more robust might be required",
          "score": 1,
          "created_utc": "2026-01-23 06:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o185p3o",
          "author": "itmanager_kz",
          "text": "Temporal",
          "score": 1,
          "created_utc": "2026-01-23 12:23:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1d8420",
          "author": "HosseinKakavand",
          "text": "If your workflow systems are starting to struggle under point-to-point choreography, I'd recommend [Luther](https://enterprise.luthersystems.com) designed for mega workflows. It takes a different approach‚Äîuses a Common Operational Script instead of visual builders, so you get Git versioning and all the dev tooling benefits while the platform handles connectors, retries, and error handling. It's scaled well at some of the largest corporations and has connectors for [SAP](https://www.reddit.com/r/luthersystems/comments/1qlcae7/sap_s4hana_connector_for_luther_workflownative/), Entra ID, [ServiceNow](https://www.reddit.com/r/luthersystems/comments/1qlcj7l/servicenow_connector_for_luther_enterprise/), and tons of other enterprise systems.",
          "score": 1,
          "created_utc": "2026-01-24 03:49:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qiq2f4",
      "title": "Biggest architectural constraint in HIPAA telehealth over time?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qiq2f4/biggest_architectural_constraint_in_hipaa/",
      "author": "Andrey_Svyrydov",
      "created_utc": "2026-01-21 06:06:53",
      "score": 7,
      "num_comments": 3,
      "upvote_ratio": 0.74,
      "text": "For those who‚Äôve built HIPAA-compliant telehealth systems: what ended up being the biggest constraint long term - security, auditability, or ops workflows? ",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qiq2f4/biggest_architectural_constraint_in_hipaa/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o0tav2q",
          "author": "alien3d",
          "text": "work before with telehealth company . i would said not easy because most think about  own but forget the client streaming capability.",
          "score": 3,
          "created_utc": "2026-01-21 06:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0v4f5j",
              "author": "Andrey_Svyrydov",
              "text": "good point tho  \ndo you mean issues with patient devices/network quality, or more around secure video streaming itself?",
              "score": 0,
              "created_utc": "2026-01-21 14:57:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0vb51s",
                  "author": "alien3d",
                  "text": "network",
                  "score": 1,
                  "created_utc": "2026-01-21 15:29:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkqemq",
      "title": "Fixing Systems That ‚ÄòWork‚Äô But Misbehave",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qkqemq/fixing_systems_that_work_but_misbehave/",
      "author": "Suspicious-Case1667",
      "created_utc": "2026-01-23 13:10:58",
      "score": 7,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "ok so hear me out. most failures don‚Äôt come from bad code. they don‚Äôt come from the wrong pattern. they come from humans. from teams. from everyone doing the ‚Äúright thing‚Äù but no one owning the whole thing.\n\nlike one team is all about performance. another is about maintainability. another about compliance. another about user experience. every tradeoff is fine. makes sense. defensible even. but somehow the system slowly drifts away from what it was meant to do.\n\nnothing crashes. metrics look fine. everything ‚Äúworks‚Äù. but when you step back the outcome is‚Ä¶ off. and no one knows exactly where. the hardest problems aren‚Äôt the bugs. they‚Äôre the spaces between teams, between services, between ownership. that‚Äôs where drift lives.\n\nlogs, frontends, APIs, even weird edge cases? they all tell you the truth. they show what the system actually allows, not what the documents say it‚Äôs supposed to do.\n\nfix one module, change one service but if the alignment is off, nothing fixes itself.\n\nso here‚Äôs the real question: if everyone did their job right, who owns the outcome? who is responsible when the system ‚Äúworks‚Äù but still fails? think about that.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qkqemq/fixing_systems_that_work_but_misbehave/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o18e55s",
          "author": "Physical-Compote4594",
          "text": "The hard things are always in the interstitial spaces. That applies to more than just software.¬†",
          "score": 3,
          "created_utc": "2026-01-23 13:16:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18lscz",
          "author": "asdfdelta",
          "text": "This is exactly where architects are meant to play. They're supposed to look beyond their four walls and see the interaction patterns happening, and the problems with it.\n\nArchitecture is a phenomenon when two or more Systems communicate. Architecture happens whether an architect is present or not, people who take the title of Architect are masters (or endeavoring to become masters) of that phenomena. What you described is (Accidental Architecture)[https://medium.com/mavenlink-product-development/what-is-accidental-software-architecture-8dffa46ec1c], and is the most common kind of anti-pattern out there.\n\nNASA calls us a Systems Engineer. Watch this incredible video to get an idea of (how they see a Systems Engineer)[https://youtu.be/E6U_Ap2bDaE?si=NUTwWYpAe61e6xmZ].\n\nArchitects need to see above the noise of their own problems space, think bigger, and solve longitudinal problems. Who owns those problems you described? The masters of systems thinking.",
          "score": 2,
          "created_utc": "2026-01-23 13:58:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18ngmp",
          "author": "ahgreen3",
          "text": ">nothing crashes. metrics look fine. everything ‚Äúworks‚Äù. but when you step back the outcome is‚Ä¶ off.\n\nThis is exactly why I do not believe AI is the be-all end-all in software engineering. \n\n>so here‚Äôs the real question: if everyone did their job right, who owns the outcome? who is responsible when the system ‚Äúworks‚Äù but still fails? think about that.\n\nI believe any organization that has software engineers must have 1 person with complete and absolute responsibility for the technical details of the application. The problem is leadership always wants to be able to override the technical people and then toss the technical person under the bus when leaderships approach fails.",
          "score": 2,
          "created_utc": "2026-01-23 14:06:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18qxog",
          "author": "Lekrii",
          "text": "Architecture always should start with business processes, workflows and value streams.¬†¬†\n\n\nAs an architect, I always feel responsible for the solution.¬† Put together:\n\n\n1. Value stream design\n2. Business process workflow\n3. System to system design\n4. Information/data architecture¬†\n5. Software architecture/design\n6. Infrastructure design\n\n\nIn that order¬†",
          "score": 2,
          "created_utc": "2026-01-23 14:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19fap1",
          "author": "doubletrack_sf",
          "text": "\"Who owns the outcome?\" really depends on the organizational structure.\n\nIf it's a large enterprise, it's likely an Operations team (i.e. Business Operations) or IT since tech infrastructure falls under their domain. Sometimes it's a Digital Transformation team when those are in place - sometimes a central Center of Excellence model when we're talking business units within an enterprise.\n\nSmaller orgs it could be the CEO, COO, or someone in IT / Ops (like Revenue Operations).\n\nBut we all know this often doesn't happen, and absolutely not often enough. Then we nod our head knowingly when companies launch AI pilots that fail.\n\nIt's often why third parties get pulled in because no team or designated owner has the bandwidth the tackle the full challenge this need requires and to make sure the design are actually aligned to the right business KPIs.",
          "score": 2,
          "created_utc": "2026-01-23 16:19:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k9oc8",
          "author": "severoon",
          "text": "The solution to this problem is that all teams own the e2e problems. \n\nWhen you have end to end tests that break, that should become everyone's problem until it is solved. Any team that feels they can just focus on their own area and, hey, if things don't work it's not our problem, that's bad culture. \n\nEveryone on every team should be focused primarily on the end user experience. Anyone who argues that something else is more important than that should be seen as a red flag.",
          "score": 2,
          "created_utc": "2026-01-25 04:40:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjuaa4",
      "title": "Is there a technology for a canonical, language-agnostic business data model?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qjuaa4/is_there_a_technology_for_a_canonical/",
      "author": "nounoursnoir",
      "created_utc": "2026-01-22 13:17:59",
      "score": 6,
      "num_comments": 23,
      "upvote_ratio": 0.76,
      "text": "I'm looking for opinions on whether what I'm describing exists, or if it's a known unsolved problem.\n\nI wish I could model my business data in a single, canonical format dedicated purely to *semantics*, independent of programming languages and serialization concerns.\n\nToday, every representation is constrained by its environment:\n\n* In JS, a matrix is a list of lists or a custom object or a Three Matrix4\n* In Python, it's a NumPy array\n* In Protobuf, it's a verbose set of nested messages\n* In a database, it's likely a raw JSON.\n\nEach of these representations leaks implementation details and forces compromises. None of them feel like an ideal way to express *what the data fundamentally is* from a pure functional, business perspective.\n\nWhat I'd like is:\n\n* One unique source of truth for business data semantics\n* All other representations (JS, Python, Protos, etc.) being constrained projections of that model (ideally a compiler would provide this for us, similarly to how gRPC's protoc compiler provides clients and servers in multiple languages based on a set of messages and RPCs)\n* Each target being free to add its own idioms and logic (methods, performance structures, syntax), but not redefine meaning\n\nThink of something closer to a semantic or algebraic model of data, rather than a serialization format or programming language type system.\n\nThe most similar thing I can think of is Cucumber or Gherkin for automated tests (although you hand-write the code associated with each sentence).\n\nDoes something like this exist for a whole system architecture (even partially)?  \nIf not, is this a known design space (IDLs, ontologies, DSLs, type theory, etc.) that people actively explore?\n\nI'm interested both in existing tools and in why this might be fundamentally hard or impractical.\n\nThank you.",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qjuaa4/is_there_a_technology_for_a_canonical/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o11nvys",
          "author": "steve-7890",
          "text": "It sounds like you're looking for: UML and/or BPML.\n\nBut remember, \"The paper accepts everything\". Code won't.",
          "score": 5,
          "created_utc": "2026-01-22 13:49:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o127dyu",
              "author": "nounoursnoir",
              "text": "BPML/BPMN model *process semantics*, not *data semantics*. I'm looking for a canonical, language-agnostic way to define what business data **is**, not how workflows execute.  \nUML is one way to represent a model, but it's still a language with its own constraints and conventions. It can define an implementation-like structure, yet it's tied to its own notation and doesn't capture the full semantic meaning of the data. In that regard it is not much different from any other language, like Python or JS.\n\nThe matrix is a good example to outline the difference between implementation and meaning.  \nThe concept of matrix is a 2D grid that allows for complex mathematical operations, useful in domains like physics, 3D or machine learning. Notions of data structures like an array, a dict, a class, a struct or whatever are irrelevant in this *conceptual* realm.\n\nYou're right to bring up UML, in that it's likely better suited to most industry needs and priorities. Often, defining structure and workflow is sufficient, and creating a business data model completely decoupled from technical implementation can be too niche. That said, such a system would probably have many advantages: I believe it would be simpler, more accessible to non-technical users, and more flexible.",
              "score": 2,
              "created_utc": "2026-01-22 15:27:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o14oc1n",
                  "author": "Ok-East-515",
                  "text": "Are you describing actual human language but trying to make it complicated?¬†",
                  "score": 2,
                  "created_utc": "2026-01-22 22:15:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o17ac5l",
                  "author": "steve-7890",
                  "text": "You're not gonna find exactly what you are looking for ;) \n\nOn the other hand, you've already found it in a form of Code. So there are hundreds  of syntaxes you're looking for, just pick one.\n\nhttps://preview.redd.it/idnktt0y02fg1.png?width=650&format=png&auto=webp&s=18e9d9360f7c880381832097c730d7440005e51c",
                  "score": 2,
                  "created_utc": "2026-01-23 07:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o11qohx",
          "author": "NeuronSphere_shill",
          "text": "We built a whole software stack around this idea.\n\nModel once, then you can code gen N implementations.",
          "score": 2,
          "created_utc": "2026-01-22 14:04:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o121jbv",
              "author": "nounoursnoir",
              "text": "What is the support of the model? What technology?",
              "score": 2,
              "created_utc": "2026-01-22 14:59:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12a18j",
          "author": "BarfingOnMyFace",
          "text": "Not sure I follow you. You could build a centralized model by normalizing to whatever extent suits you in a database, no? That becomes your unique source of truth for data semantics‚Ä¶. If all these different structure types will be representing the same underlying data but in different consumable packages, why not‚Ä¶ standardize what you need, semantically, to database table(s)? Not stored as JSON, but as a set of shared attributes.\n\nWhy wouldn‚Äôt this work for you? Sorry if I‚Äôm being dense.",
          "score": 2,
          "created_utc": "2026-01-22 15:40:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12dvum",
              "author": "nounoursnoir",
              "text": "A database, like a language or a communication protocol, all *implement* concepts. Structure and technical constraints are inherently tied to this modelization. What I'm looking for is a perfect separation between the *semantic* and the *implementation*. I would like to be able to define a part of my system as a 4x4 Matrix, knowing what a 4x4 matrix is *in principle*, and only when this pure representation is made, I can define implementations for it in the different technologies that I use.",
              "score": 0,
              "created_utc": "2026-01-22 15:57:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o12oz4v",
                  "author": "BarfingOnMyFace",
                  "text": "Hmmm, I dunno, I‚Äôm clueless on this. lol. I did do some googling and AI review to try and resolve my cluelessness, and it all come back saying there is no such tooling out there. here were the closest suggestions on this from chat:\n\n‚úÖ Algebraic / semantic core (exists, but academic)\n\nAlgebraic specification languages\n\t‚Ä¢\tCASL\n\t‚Ä¢\tOBJ\n\t‚Ä¢\tMaude\n\nThey let you say:\n\t‚Ä¢\tA Matrix4x4 exists\n\t‚Ä¢\tThese operations exist\n\t‚Ä¢\tThese laws must hold\n\t‚Ä¢\tNo representation is implied\n\nProblem:\nThey stop before code",
                  "score": 1,
                  "created_utc": "2026-01-22 16:47:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1321dt",
          "author": "cybDrachir",
          "text": "What about JsonSchema?",
          "score": 1,
          "created_utc": "2026-01-22 17:46:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17ze6d",
              "author": "nounoursnoir",
              "text": "Looks good for structure/validation, but not semantics.",
              "score": 2,
              "created_utc": "2026-01-23 11:37:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o137yma",
          "author": "GrogRedLub4242",
          "text": "SQL/DDL",
          "score": 1,
          "created_utc": "2026-01-22 18:12:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17zn7y",
              "author": "nounoursnoir",
              "text": "Like any other language, they define implementation, not meaning.",
              "score": 1,
              "created_utc": "2026-01-23 11:39:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13r35k",
          "author": "Turtlestacker",
          "text": "Not sure I understand your question tbh but keel.so must have solved something like this?",
          "score": 1,
          "created_utc": "2026-01-22 19:37:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1803hg",
              "author": "nounoursnoir",
              "text": "This looks great, thanks!",
              "score": 1,
              "created_utc": "2026-01-23 11:42:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o148o3k",
          "author": "aphillippe",
          "text": "Are you taking about a logical data model? A representation of the business domain‚Äôs data requirements in its ‚Äòpurest‚Äô form, abstract of any implementation or technical detail. It can be helpful in a model-first approach to sketch out what the data looks like in abstract, and then design the various physical data models (UI, service layer data dictionary, operational database, ODS, data warehouse all referring and mapping back to the logical model. It becomes the blueprint for all physical data models, and also ties neatly into anything behavioural (service operations, data warehouse facts etc.) as those behavioural artifacts (transaction, order, whatever) have all been modelled already. Or maybe I just find it useful since I‚Äôm a data guy",
          "score": 1,
          "created_utc": "2026-01-22 20:59:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17zrl9",
              "author": "nounoursnoir",
              "text": "Exactly!",
              "score": 1,
              "created_utc": "2026-01-23 11:40:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e917g",
          "author": "UnreasonableEconomy",
          "text": "Dumping some thoughts on the matter\n\n---\n\nHave you ever worked with typescript?\n\nI'm not talking about js + java-like types. \n\nThe typescript type system, particularly the \"type\" type. \n\nIt's not a 1-1 match for what you're looking for, and it doesn't really transpile automatically into anything. \n\nBasically what it allows you to do is define arbitrary types and how they relate to each other. \n\nYou can define operators that constrain what these types do, or how they transform between one another. \n\nAll this is implementation independent.\n\nWe use this for dimensional analysis and linear algebra in our gis system. This allows us to reason about what data is, and when. If you divide meters by seconds, the type system knows it's meters per second. it's impossible to confuse pixels per second on the device screen manifold with pixels per second in the render context with meters per second on the earth manifold, even though they're all related.\n\nHow are they calculated or represented under the hood? Kind of irrelevant at this level.\n\n---\n\n> why this might be fundamentally hard or impractical.\n\nTypescript is incredibly powerful, but I think you're gonna have a hard time staffing for people that can use it like this. Maybe the TS subreddit if you ask around, but it's not something the average sw engineer is going to be capable of or comfortable working with. It's closer to prolog or haskell than anything else. (never worked with haskell though)\n\nNow using the produced types? That's easy. But type development and maintenance might not be worth it. \n\nthat's why we cheat a lot, but try to keep the cheating contained. E.g., rotation matrices are just functions coming out of factories. You could abstract it into a matrix, but the matrix type isn't ready (we don't yet know how to mix units and manifolds and matrices in a useful way, plus low ROI to solving this (IMO hard) problem).\n\n---\n\nAlso, from experience, it's not something you can design up front. Sometimes the ergonomics are just crap. The ROI of all that hard work might not be there, and just using it out of sunk cost also doesn't make sense. In college you learn about DSLs in eclipse, but realistically, reality is hard. BDUF doesn't work, and redesigning DSLs like in xtext/xtend or whatever was a pain IIRC.\n\nMy \"DSL\" of choice is JSON or YAML. This, with a validator and good docs is super good enough for so many cases. Custom parsers are just too unwieldy to maintain, especially if you only have a handful of DSL users anyways.\n\n---\n\nTL;DR:\n\nYeah, it's a real hard problem, with no real solution. Academics and researchers tend to come up with a bunch of stuff that doesn't survive contact with reality. Maybe one day, but today is not that day. Beware of the tarpit lol.",
          "score": 1,
          "created_utc": "2026-01-24 08:39:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o124w5e",
          "author": "arnedh",
          "text": "Look into Archimate: BusinessObject, DataObject, Artifact, Representation and relations to from/to/among these. Archimatetool.org",
          "score": 1,
          "created_utc": "2026-01-22 15:15:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o180gzh",
              "author": "nounoursnoir",
              "text": "Interesting, thanks",
              "score": 1,
              "created_utc": "2026-01-23 11:45:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjyjej",
      "title": "Patterns for real-time hardware control GUIs?",
      "subreddit": "softwarearchitecture",
      "url": "https://www.reddit.com/r/softwarearchitecture/comments/1qjyjej/patterns_for_realtime_hardware_control_guis/",
      "author": "WitnessWonderful8270",
      "created_utc": "2026-01-22 16:07:04",
      "score": 6,
      "num_comments": 6,
      "upvote_ratio": 0.8,
      "text": "Building a desktop GUI that sends commands to hardware over TCP and displays live status. Currently using basic MVC but struggling with:\n\n* Hardware can disconnect anytime\n* State lives in both UI and device (sync issues)\n* Commands are async, UI needs to wait/timeout\n\nWhat patterns work well for this? Seen suggestions for MVVM, but most examples are web/mobile apps, not hardware control. Any resources for industrial/embedded UI architecture?\n\nThank you!",
      "is_original_content": false,
      "link_flair_text": "Discussion/Advice",
      "permalink": "https://reddit.com/r/softwarearchitecture/comments/1qjyjej/patterns_for_realtime_hardware_control_guis/",
      "domain": "self.softwarearchitecture",
      "is_self": true,
      "comments": [
        {
          "id": "o12s33g",
          "author": "exodusTay",
          "text": ">State lives in both UI and device (sync issues)\n\nNever ever keep state in UI if device also has it. Poll it and use the last known state at all times.\n\nWhat we did that I think was very useful to create a separate thread that polls the device and UI is updated from this thread using signal/slots(we are using Qt, think signal/slots as callback functions but thread safe).\n\n>Commands are async, UI needs to wait/timeout\n\nI found that commands should change the state of the UI to an intermediary state(like Ready -> Processing) and the device should be polled to move UI to the final state(Example: Procesing -> Moving). It is easier if you know the states the device can be in.\n\nDo not do waiting in the UI. Commands are handled in another thread and that thread should update the UI about what is going on.\n\nI never really did web, but AFAIK this is much like MVC, where model is the device, controller is your thread that communicates with the device and view is your GUI.",
          "score": 3,
          "created_utc": "2026-01-22 17:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o193glf",
              "author": "WitnessWonderful8270",
              "text": "Thank you:)",
              "score": 1,
              "created_utc": "2026-01-23 15:26:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14aj3a",
          "author": "cannibal_catfish69",
          "text": "I've worked on device control systems with web front-ends before.  \n\nExodusTay makes some good points.  100% the device state itself should be the source of truth, and the front-end should reflect that state.\n\nIf you need to update the device from the front-end, you should to tell the controller the new state you want, and have it attempt to update the device, and then sample the state and confirm the change, possibly retrying if necessary.\n\nI would ask some topology questions.  Like, how exactly, is the device controlled?  Are you the authors of the device's connectivity software, or is it a 3rd party, black box situation?  Is the device controller acting as the server or the client?\n\nNetwork client's should be able to detect disconnections and re-connect automatically.\n\nControllers should be designed to anticipate loss of network connectivity and fail safely, maintain the state, follow the schedule, or whatever should happen in the absence of control.\n\nI've done this type of system with node.js running on-premise, presenting as web-server on wifi.\n\nDynamic display of device state changes was done via websocket so that updates could be conveyed to the front-end quickly, and without polling.\n\nThe connectivity of the controlled devices is highly variable.  \n\nBut, if you can choose, node.js makes working with raw sockets easy.  IMO, much less complicated doing networking in C or C++.  \n\nAnd if you can use node.js as the controller, then you can connect to your websocket server as a client - the same way that a web-app running in the browser will connect.  Which basically means you can have a chain of hops over websocket from the controller to the browser, possibly through several client-server layers, including out to the wider internet, if that's a requirement.",
          "score": 2,
          "created_utc": "2026-01-22 21:08:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o193h45",
              "author": "WitnessWonderful8270",
              "text": "Thank you:)",
              "score": 1,
              "created_utc": "2026-01-23 15:26:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12vk7z",
          "author": "Glove_Witty",
          "text": "Mqtt seems like a better protocol for what you are trying to do. You can have the device send state and build a digital twin on a server that the UI interacts with.",
          "score": 1,
          "created_utc": "2026-01-22 17:17:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14rohl",
          "author": "i_be_illin",
          "text": "Make sure you talk to your cyber team about what is allowed in your industrial spaces. You need to make sure what you are building is allowed and that you comply with guardrails. \n\nYou can build something that makes total sense in the IT world that is all a waste of time because some part of your pattern is disallowed in the OT environment.",
          "score": 1,
          "created_utc": "2026-01-22 22:32:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}