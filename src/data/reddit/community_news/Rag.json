{
  "metadata": {
    "last_updated": "2026-02-02 09:09:57",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 97,
    "file_size_bytes": 132561
  },
  "items": [
    {
      "id": "1qocxu9",
      "title": "Ran 30 RAG chunking experiments - found that chunk SIZE matters more than chunking STRATEGY",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "author": "ManufacturerIll6406",
      "created_utc": "2026-01-27 12:49:14",
      "score": 64,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "I kept seeing recommendations that sentence chunking is best for RAG because it \"respects grammatical boundaries.\"\n\nDecided to test it systematically: 4 strategies, 2 datasets, 1,200 retrieval evaluations.\n\nWriteup with methodology and open source code:¬†[Link](https://theprincipledengineer.substack.com/p/its-a-chunking-lie?r=2ivi0a)\n\n**Sentence chunking did dominate initially ‚Äî 96.7% recall vs 80-83% for others.**\n\nThen I noticed something most benchmarks don't report: actual chunk sizes produced.\n\nWhen I configured all strategies with chunk\\_size=1024:\n\n\\- Token: 934 chars (0.91x)\n\n\\- Recursive: 667 chars (0.65x)\n\n\\- Semantic: 1117 chars (1.09x)\n\n\\- Sentence: 3677 chars (3.59x) ‚Üê\n\nSentence chunking was producing chunks 3.6x larger than requested. Larger chunks = more context = better recall. That's a size effect, not a strategy effect.\n\n**When I controlled for actual chunk size (\\~3000 chars across strategies), token chunking matched or beat sentence chunking.**\n\n**Correlation between chunk size and recall: r=0.74 (HotpotQA), r=0.92 (Natural Questions).**\n\nCurious if others have seen similar results or if this breaks down on different datasets.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o20c64u",
          "author": "fixitchris",
          "text": "Depends what you are chunking.",
          "score": 12,
          "created_utc": "2026-01-27 13:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20jhim",
              "author": "ManufacturerIll6406",
              "text": "Exactly and this fw lets you test that on your own strategy and your own documents \n\n[https://github.com/somasays/rag-experiments/tree/main/chunking](https://github.com/somasays/rag-experiments/tree/main/chunking)",
              "score": 1,
              "created_utc": "2026-01-27 13:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o23k80i",
          "author": "durable-racoon",
          "text": "smaller chunks are usually better recall. larger chunks often better for generation. but nothing beats measuring for your specific use case, which you've done.",
          "score": 4,
          "created_utc": "2026-01-27 21:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20wha5",
          "author": "irodov4030",
          "text": "\"Larger chunks = more context = better recall.\"\n\nHow did you measure recall here?",
          "score": 3,
          "created_utc": "2026-01-27 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272ar6",
              "author": "mrFunkyFireWizard",
              "text": "This is also false, your vector points are not accurate if they contain too much context. If it's within the same semantic meaning it's fine but just adding more context is poor design",
              "score": 1,
              "created_utc": "2026-01-28 11:22:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21cwv3",
          "author": "TechnicalGeologist99",
          "text": "There's many steps in retrieval. I find that the main disadvantage of having short chunks is that occasioanly they knock it out the park in terms of relevance and they take up a space in the top K even though they are a useless chunk. \n\nIt may be that some strategies make many such useless chunks. (That are also small). That raises the probability of filling up top K with crap. \n\nYou could repeat this with varying top K to try and measure if that is occurring here",
          "score": 2,
          "created_utc": "2026-01-27 16:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21ewvq",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-27 16:16:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21f5km",
                  "author": "ManufacturerIll6406",
                  "text": "The article shows the results in detail if you are interested\n\n[https://theprincipledengineer.substack.com/p/its-a-chunking-lie](https://theprincipledengineer.substack.com/p/its-a-chunking-lie)",
                  "score": 2,
                  "created_utc": "2026-01-27 16:17:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26q28j",
          "author": "HMM0012",
          "text": "Makes sense; chunk size often drives context retention more than strategy. Controlling for size usually levels performance, so reported strategy wins can be misleading without normalizing chunk length.",
          "score": 2,
          "created_utc": "2026-01-28 09:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zyrl",
          "author": "notAllBits",
          "text": "I would abandon tokenization into chunks. Stream if you can, but cutting any text produces bias from discontinuity. I would follow syntactic and semantic structure when indexing.",
          "score": 1,
          "created_utc": "2026-01-27 15:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o218c9v",
              "author": "Jords13xx",
              "text": "Streaming is definitely an interesting approach, but it can be tricky with context retention. If you maintain some syntactic and semantic boundaries while still chunking, you might strike a balance between continuity and performance. Have you experimented with any hybrid methods?",
              "score": 1,
              "created_utc": "2026-01-27 15:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21cnnp",
                  "author": "ManufacturerIll6406",
                  "text": "Recursive chunking is essentially a hybrid, it tries paragraphs first, falls back to sentences, then words. In my experiments it landed in the middle: better size control than sentence, slightly worse recall than token at equivalent sizes.\n\nThe interesting question is whether there's a \"best of both worlds\" approach: target a specific size but snap to the nearest sentence boundary. You'd get predictable chunk sizes without mid-sentence cuts.\n\nDidn't test that explicitly, but the framework is extensible & would be a straightforward strategy to add - [https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)\n\nMight be worth exploring in a follow-up.",
                  "score": 1,
                  "created_utc": "2026-01-27 16:06:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o223w9s",
          "author": "jrochkind",
          "text": "If you have text that has em, i'd try paragraph chunking (up to certain max size paragraph anyway)",
          "score": 1,
          "created_utc": "2026-01-27 18:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22ip3o",
          "author": "blue-or-brown-keys",
          "text": "Curious if this may be trying to justify method based on outcome. Whats the intution here.",
          "score": 1,
          "created_utc": "2026-01-27 19:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o230ewt",
              "author": "ManufacturerIll6406",
              "text": "Intuition: More text = more semantic information encoded = better chance of matching the query. Also, answers rarely live in a single sentence, larger chunks capture the full context.\n\nThe \"justifying outcome\" concern would apply if I cherry-picked one result. But the correlation held across 30 configs, two datasets, and all four strategies landed on the same trendline (r=0.74 and r=0.92).\n\nCode's open if you want to test on a different dataset.",
              "score": 1,
              "created_utc": "2026-01-27 20:26:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29pm8l",
          "author": "lyonsclay",
          "text": "Wouldn't the optimal chunk size be contingent on your vector size assuming you are using vector similarity to select the chunks? If your chunk is smaller than your vector size then your system is being wasteful, if your chunk is larger than your vector size you are losing information.",
          "score": 1,
          "created_utc": "2026-01-28 19:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aikqp",
              "author": "ManufacturerIll6406",
              "text": "There probably is a sweet spot! In my experiments, recall kept improving up to \\~3000 chars with text-embedding-3-small (1536 dims). Didn't test beyond that. Could be interesting to check.\n\nhttps://preview.redd.it/arrmfswmq5gg1.png?width=1500&format=png&auto=webp&s=98470fd0c77c983075e9e1807d121e82f47fd5bd",
              "score": 2,
              "created_utc": "2026-01-28 21:23:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpce5s",
      "title": "You can now train embedding models 1.8-3.3x faster!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:11:25",
      "score": 32,
      "num_comments": 14,
      "upvote_ratio": 0.95,
      "text": "Hey RAG folks! We collaborated with Hugging Face to enable 1.8-3.3x faster embedding model training with 20% less VRAM, 2x longer context & no accuracy loss vs. FA2 setups.\n\nFull finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default! You can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp etc.\n\nFine-tuning embedding models can improve retrieval & RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.\n\nWe provided many free notebooks with 3 main use-cases to utilize. \n\n* Try the [EmbeddingGemma notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M).ipynb) in a free Colab T4 instance\n* We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!\n\n‚≠ê Guide + notebooks: [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nGitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n\nThanks so much guys! :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o287p7m",
          "author": "z0han4eg",
          "text": "Thanks mate, this a rly big news",
          "score": 3,
          "created_utc": "2026-01-28 15:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27wj9w",
          "author": "Popular_Sand2773",
          "text": "This is very cool. Quick question if we are using these encoders for the base of something else is this still valuable or is it only really for classic fine tuning? If I understand correctly the main speedup came from a new fused kernel correct?",
          "score": 1,
          "created_utc": "2026-01-28 14:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284gga",
              "author": "yoracale",
              "text": "Apologies could you elaborate your first question?\n\nOur main optimizations includes gradient checkpointing, kernels yes and more. You can see gradient checkpointing here: https://unsloth.ai/docs/new/500k-context-length-fine-tuning#unsloth-gradient-checkpointing-enhancements",
              "score": 1,
              "created_utc": "2026-01-28 15:06:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28gjpf",
          "author": "TechySpecky",
          "text": "Why do people fine tune models and when do these lead to superior performance than large well-known embedding models like gemini / qwen ones? For example if I am doing RAG for archeology would it make sense to have a custom embedding model?",
          "score": 1,
          "created_utc": "2026-01-28 16:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o294cnv",
              "author": "Financial-Bank2756",
              "text": "yes, a custom embedding model could help if you have enough domain text and evaluation pairs. Otherwise, a strong general model plus better chunking, metadata filters, hybrid search, and rerankers often beats premature fine-tuning.",
              "score": 3,
              "created_utc": "2026-01-28 17:44:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o294nzx",
                  "author": "TechySpecky",
                  "text": "Yea makes sense, what do you mean by evaluation pairs?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:45:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dh7lf",
          "author": "Aggressive-Solid6730",
          "text": "With how cheap compute is and how small embedding models are (compared to LLMs) I wouldn't think that time and memory are at that much of a premium. I am curious to hear any push-back on this, but I am also curious if in your experiments you saw any additional benefits of using these fine-tuning variants such as LoRA. Did they behave as regularizers making training more stable or were the gains purely speed and memory? The other thing you mention is context length which is fair, but as Google published, the amount of information we are trying to fit into a single vector is already quite limiting.",
          "score": 1,
          "created_utc": "2026-01-29 07:48:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2suc38",
              "author": "yoracale",
              "text": "For embedding model training speed is probably the most important. VRAM less so. If you can save time training why the hell not? And it's not a little speed boost, 2x faster basically means 100% faster than before\nThe gains are only for speed, memory and context length at this time. We don't do any accuracy changes as of this moment üôè",
              "score": 1,
              "created_utc": "2026-01-31 15:19:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eqvq6",
          "author": "Interesting-Town-433",
          "text": "Amazing! Will incorporate into embedding-adapters asap\n\nUniversal Embedding Translation Library\nOutput an embedding from any model into any other model's vector space\n\ngo\nminilm <-> openai\ngoogle <-> openai\ne5 <-> openai\nwith confidence scoring to tell you when it will work\n https://github.com/PotentiallyARobot/EmbeddingAdapters",
          "score": 1,
          "created_utc": "2026-01-29 13:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2su01y",
              "author": "yoracale",
              "text": "Interesting thanks for sharing!",
              "score": 1,
              "created_utc": "2026-01-31 15:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34ch32",
          "author": "Informal-Victory8655",
          "text": "A basic question: how do we prepare data for a embedding model training? \n\nIs it we have to prepare queries and the relevant text documents / text paragraphs that must be retrieved for the given query? \n\nAs I've french law data but no such pairs available.",
          "score": 1,
          "created_utc": "2026-02-02 07:47:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs87ld",
      "title": "Best chunking + embedding strategy for mixed documents converted to Markdown (Docling, FAQs, web data)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "author": "Particular-Gur-1339",
      "created_utc": "2026-01-31 17:19:34",
      "score": 27,
      "num_comments": 12,
      "upvote_ratio": 0.95,
      "text": "Hey folks üëã\nI‚Äôm building a RAG pipeline and could use some advice on chunking and embedding strategies when everything is eventually normalized into Markdown.\n\nCurrent setup\n\nConverting different file types (PDFs, docs, etc.) into Markdown using Docling\nScraping website FAQ pages and storing those as Markdown as well\nEmbedding everything into a vector store for retrieval\n\nStructure of the data\nEach document/page usually has:\nA main heading\nSub-sections under that heading\nMultiple FAQs under each section\nWeb FAQs are often short Q&A pairs\n\nWhat I‚Äôm confused about\nChunking strategy\nShould I chunk by:\nPage\nHeading / sub-heading\nIndividual FAQ (Q + A as one chunk)\n\nHybrid approach (heading context + FAQ chunk)?\n\nChunk size\nFixed token size (for example 300 to 500 tokens)\nOr semantic chunks that vary in size?\nMetadata\n\n\nGoal\nHigh answer accuracy\nAvoid partial or out-of-context answers",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2xli7k",
          "author": "Dapper-Turn-3021",
          "text": "Yeah, you‚Äôre basically on the right track. There‚Äôs no single perfect chunking or embedding strategy, it really depends on your data and what you‚Äôre trying to achieve. Keeping metadata separate is correct you should embed only the actual text content and store the metadata alongside it so you can filter or rank with it later. Embedding metadata usually just adds noise.\nChunking isn‚Äôt fixed either. Sometimes smaller chunks work better, sometimes larger ones, and semantic chunking is often the best option. It‚Äôs normal to tweak your chunking strategy as you see how your retrieval performs.\n\n\nAnd you‚Äôre absolutely right about retrieval. Don‚Äôt rely only on embeddings. Combining embeddings with metadata filtering, keyword or BM25 search, and then adding a reranking step gives much better results in most cases.\n\n\nSo yes, what you described is basically how strong RAG systems are built. we are following same for Zynfo AI to build out chatbot",
          "score": 3,
          "created_utc": "2026-02-01 07:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tpw4u",
          "author": "Curious-Sample6113",
          "text": "Everything depends on your original source. You are on the right path and just have to do a lot of testing.",
          "score": 2,
          "created_utc": "2026-01-31 17:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfmn5",
              "author": "Particular-Gur-1339",
              "text": "What should be my chunking strategy?",
              "score": 1,
              "created_utc": "2026-02-01 02:18:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xc1r6",
                  "author": "Curious-Sample6113",
                  "text": "You optimize your strategy by testing. I don't know what your source looks like. For example: if you are building a legal agent then you will have a series of questions. The ones that fail will reveal the issues with your ingestion. \n\nThere isn't a universal solution. Everything is tailored to the source.",
                  "score": 1,
                  "created_utc": "2026-02-01 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ttf5n",
          "author": "jannemansonh",
          "text": "hit the same wall building doc search workflows... ended up using needle app since it handles the chunking/embedding/rag stuff automatically (has hybrid search built in). you just describe what you need and it builds it vs configuring all the pieces manually",
          "score": 1,
          "created_utc": "2026-01-31 18:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2utz79",
              "author": "Particular-Gur-1339",
              "text": "Didn't get this what is a needle app?",
              "score": 1,
              "created_utc": "2026-01-31 21:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2v26bj",
                  "author": "bwhitts66",
                  "text": "Needle is a tool that streamlines the process of building document search workflows. It automates chunking and embedding so you don‚Äôt have to set everything up manually. It‚Äôs pretty handy if you‚Äôre looking to simplify the RAG pipeline!",
                  "score": 3,
                  "created_utc": "2026-01-31 21:44:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30re8z",
          "author": "Higgs_AI",
          "text": "You‚Äôre asking the wrong question and I mean that in the most helpful way possible.\nThe chunking debate (semantic vs fixed size vs hierarchical) assumes your RAG architecture is correct. It‚Äôs not. You‚Äôre optimizing for retrieval when you should be optimizing for knowledge structure.\nHere‚Äôs the problem with your current setup. Docling to Markdown to Vectors to Retrieval. This pipeline loses the thing that makes FAQs useful, which is the relationships between questions. When you chunk Q+A as atomic units, you lose which questions are siblings under the same topic, which answers reference concepts explained elsewhere, and the hierarchy you already identified (heading to subheading to FAQ).\nWhat you actually want is to stop chunking for embedding and start structuring for reasoning.\nInstead of treating each FAQ as an isolated chunk, build a structure where each FAQ has an ID, knows what topic it belongs to, knows what other FAQs it relates to, and carries its prerequisites. Now your retrieval can find the relevant FAQ by semantic match, pull in related FAQs automatically so you don‚Äôt get out of context answers, and include parent topic context without re-embedding it every time.\nIf you‚Äôre committed to vector RAG, here‚Äôs the practical move. Chunk at FAQ level with Q and A together, you had this right. But prepend the heading hierarchy as metadata, not as embedded text. Store the path like ‚ÄúAccount Settings > Security > Password Recovery‚Äù and at retrieval time inject that context before the FAQ content. This gives you semantic search on the answer content while preserving structural context for the LLM.\nThe hybrid approach you‚Äôre circling around looks like this. Your chunk is the individual FAQ as a Q+A pair. Your metadata is the full heading path, related FAQ IDs, and section summary. Your embedding should be the question plus the first sentence of the answer, not the full text since answers tend to be verbose. At retrieval you grab your top-k FAQs plus their metadata plus the parent section summary.\nToken budget roughly 50 to 100 tokens per FAQ chunk, 20 to 30 tokens for heading context, pull 3 to 5 related FAQs and you‚Äôre at maybe 500 tokens total. That‚Äôs enough for high answer accuracy without blowing up your context window.\nTo hit your specific questions directly. Chunk by FAQ with Q and A together, yes this is correct. Use semantic size not fixed tokens since FAQs vary so let them. Metadata is your secret weapon here, the heading path, section ID, related IDs. And don‚Äôt embed the hierarchy, reference it. Embed the answer, retrieve the structure.\nIf you want to go deeper on this, look into knowledge graphs as a retrieval layer instead of pure vector search. Structure beats embedding for FAQ style content every time. And I do mean EVERY TIME! Just my opinion ü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-01 19:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30z7x9",
          "author": "Live-Guitar-8661",
          "text": "I have a thing and I‚Äôm looking for testers and willing to build a POC for free alongside. DM if you are interested",
          "score": 1,
          "created_utc": "2026-02-01 19:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o313cnn",
          "author": "ampancha",
          "text": "For FAQ-heavy Markdown, heading-anchored chunks with parent context (section title + sub-heading prepended to each chunk) consistently outperform fixed-token splits because the LLM gets retrieval results that carry their own scope. The deeper problem is that without retrieval evaluation and input validation on ingested content, any chunking strategy is an untestable guess, and scraped web pages become an injection surface the moment they land in your vector store. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 20:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xyuw",
          "author": "voycey",
          "text": "There is no single strategy that works, the reason most RAG systems fail is due to poor chunking methodologies.\n\nI will say that there is no real reason not to mix multiple chunking strategies into a single pipeline to ensure you are getting the best retrieval, especially if you are using re-ranking!",
          "score": 1,
          "created_utc": "2026-02-01 22:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32aak9",
          "author": "Odd-Affect236",
          "text": "What benefits does markdown provide when compared to simple plain text?",
          "score": 1,
          "created_utc": "2026-02-01 23:43:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnplpb",
      "title": "Built a tool for visualizing text chunking strategies",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "author": "Crazy-Plan8697",
      "created_utc": "2026-01-26 19:09:49",
      "score": 26,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Hey :)\n\nSome time ago I wanted to learn the fundamentals of RAG, and I started with chunking. Greg Kamradt‚Äôs tool ([https://chunkviz.up.railway.app/](https://chunkviz.up.railway.app/)) helped me understand the basics, and it was a great starting point.\n\nWhile digging deeper, especially when reading papers like [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997), I noticed that **semantic** and **agentic** chunking are showing up more often and are getting adopted in new research. But I couldn‚Äôt find any visualizers that supported those methods, so I tried to build one myself.\n\nI put together **Chunking-Vis**, a small web tool for anyone who wants to explore and learn how different chunking strategies behave. I think it can be especially helpful if you‚Äôre new to RAG and want to see how chunks are formed before they‚Äôre sent to an embedding model or retrieval pipeline.\n\n# What Chunking-Vis supports\n\n* Character-level chunking\n* Word-level chunking\n* Token-level chunking (GPT-4o tokenizer)\n* Recursive chunking\n* Semantic chunking\n* Agentic chunking (Phi-3 powered, available locally on CPU)\n\nThere‚Äôs also a **Snapshot** feature that lets you save and compare different chunking configurations side-by-side, which can make experimentation easier.\n\n# Live Demo\n\n[https://chunkingvis-production.up.railway.app](https://chunkingvis-production.up.railway.app)  \n(Agentic chunking is disabled in the demo due to compute limits.)\n\n# Github Repository\n\n[https://github.com/MichalZnalezniak/Chunking-Vis](https://github.com/MichalZnalezniak/Chunking-Vis)\n\nHope some of you find it useful ‚Äî and if you have ideas, feedback, or suggestions, please let me know. Thank you!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qq8y1q",
      "title": "TextTools ‚Äì High-Level NLP Toolkit Built on LLMs (Translation, NER, Categorization & More)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "author": "Due_Place_6635",
      "created_utc": "2026-01-29 13:45:08",
      "score": 24,
      "num_comments": 3,
      "upvote_ratio": 0.97,
      "text": "Hey everyone! üëã\n\nI've been working on TextTools, an open-source NLP toolkit that wraps LLMs with ready-to-use utilities for common text processing tasks. Think of it as a high-level API that gives you structured outputs without the prompt engineering hassle.\n\nWhat it does:\n\nTranslation, summarization, and text augmentation\n\nQuestion detection and generation\n\nCategorization and keyword extraction\n\nNamed Entity Recognition (NER)\n\nCustom tools for almost anything\n\nWhat makes it different:\n\nBoth sync and async APIs (TheTool & AsyncTheTool)\n\nStructured outputs with validation\n\nProduction-ready tools (tested) + experimental features\n\nWorks with any OpenAI-compatible endpoint\n\nQuick example:\n\n```python\nfrom texttools import TheTool\n\nthe_tool = TheTool(client=openai_client, model=\"your_model\")\nresult = the_tool.is_question(\"Is this a question?\")\nprint(result.to_json())\n```\nCheck it out: https://github.com/mohamad-tohidi/texttools\n\nI'd love to hear your thoughts! If you find it useful, contributions and feedback are super welcome. What other NLP utilities would you like to see added?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2ewxt9",
          "author": "Popular_Sand2773",
          "text": "Hey like the idea of a one stop shop but a lot of these are tasks I would never give to an llm. Have you thought about unifying the BIC options for these different tasks? BERTopic is a good example of a unifying framework that takes the best from everyone.",
          "score": 3,
          "created_utc": "2026-01-29 14:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qos79",
              "author": "Due_Place_6635",
              "text": "Hello, thank you for the reply  \ni completly agree, there are a lot of tasks that an small finetuned BERT model would be far better at them, rather than a giant LLM!\n\nyes, as a matter of fact, it used to be part of the library, but we decided to remove that\n\nbecause: the llm's are open domain, they are kind of like a quick Demo of what is possible...",
              "score": 1,
              "created_utc": "2026-01-31 05:10:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31qx2m",
                  "author": "leppardfan",
                  "text": "Nice work! I think you should put the BERT models back in as another option for the user, depending on the problem.",
                  "score": 2,
                  "created_utc": "2026-02-01 22:01:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp9pmy",
      "title": "How to handle extremely large extracted document data in an agentic system? (RAG / alternatives?)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "author": "Complex-Time-4287",
      "created_utc": "2026-01-28 12:13:17",
      "score": 18,
      "num_comments": 28,
      "upvote_ratio": 0.93,
      "text": "I‚Äôm building an agentic system where users can upload documents. These documents can be *very* large ‚Äî for example, up to **15 documents at once**, where some are **\\~1500 pages** and others **300‚Äì400 pages**. Most of these are financial documents (e.g., tax forms), though not exclusively.\n\nWe have a document extraction service that works well and produces structured layout + document data.  \nHowever, the extracted data itself is also huge, so we **can‚Äôt fit it into the chat context**.\n\n  \n**Current approach**\n\n* The extracted structured data is stored as a **JSON file in cloud storage**\n* We store a **reference/ID in the DB**\n* Tools can fetch the data using this reference when needed\n\n  \n**The Problem**\n\nBecause the agent never directly ‚Äúsees‚Äù or understands the extracted data:\n\n* If a user asks questions about the document content,\n* The agent often can‚Äôt answer correctly, since the data is not in its context or memory\n\n  \n**What we‚Äôre considering**\n\nWe‚Äôre thinking about applying **RAG on the extracted data**, but we have a few concerns:\n\n* Agents run in a chat loop ‚Üí **creation + retrieval must be fast**\n* The data is deeply nested and very large\n* We want minimal latency and good accuracy\n\n**Questions**\n\n1. What are **practical solutions** to this problem?\n2. Which **RAG systems / architectures** would work best for this kind of use-case?\n3. Are there **alternative approaches** (non-RAG) that might work better for large documents?\n4. Any best practices for handling **very large documents** in agentic systems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o27bksd",
          "author": "Mishuri",
          "text": "You must do semantic chunking on the document. Split it up according to logical partitions. Ask LLM to enhance with descriptions metadata. Explicit relationships to other sections. Then embed those. 90% and the expensive part is LLM preprocessing of this. Then you gather context with vector rag and details with agentic rag + subagents to manage context.",
          "score": 6,
          "created_utc": "2026-01-28 12:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dy9y",
              "author": "rshah4",
              "text": "Yes, and then you can have a Table of Contents and make sure each of those sections understand their role in the hierarchical structure. This is what we do and it works well.  \nAlso a database can be useful here as well.",
              "score": 2,
              "created_utc": "2026-01-28 12:46:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27onir",
              "author": "Complex-Time-4287",
              "text": "this it totally possible, but I'm concerned about the time it is likely to take, in a chat it'll feel kind of blocking until the chuking and embedding is complete",
              "score": 1,
              "created_utc": "2026-01-28 13:47:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28i5j8",
              "author": "usernotfoundo",
              "text": "Are there any particular resources you would suggest that go into detail in this process? Currently I have been simply using an llm to process my large paragraph into a list of (observation,recommendation), embedding the observations and retrieving it based on similarity with a query. I feel this is too simplified, and breaking it down into multiple steps like you described could be the way to go, no idea how to start.",
              "score": 1,
              "created_utc": "2026-01-28 16:07:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d1jzt",
          "author": "aiprod",
          "text": "I think what most people are missing here are the strict latency requirements. The user uploads documents in a live chat session and wants to interact with them immediately, correct?\n\nThis rules out time intensive approaches like embeddings or generating summaries or metadata with LLMs.\n\nThere are a few things that could work:\n\nGive the agent a search tool that is based on BM25. Create page chunks from the data (usually a good semantic boundary too), index it into open search or elastic search and let the agent search the index. This is fast and context efficient.\n\nOn top of that, you could add the first one or two pages of each file to the context window of the agent. Usually, the first pages give an indication of what a doc is about. With that knowledge, the agent could make targeted searches inside a specific doc by using a filter with the search queries.\n\n\nAlternatively, you could use the file system based approach that coding agents like Claude code use. Give the agent tools to grep through the files and to read slices of the document. You don‚Äôt have to use an actual file system, it could just be simulated with tools. The agent will grep and slice through the docs to answer questions. RLM is an advanced version of this approach: https://arxiv.org/pdf/2512.24601v1",
          "score": 5,
          "created_utc": "2026-01-29 05:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d5vl6",
              "author": "Complex-Time-4287",
              "text": "That's right! Thanks for the suggestions, I'll try this",
              "score": 1,
              "created_utc": "2026-01-29 06:12:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mnabf",
          "author": "Ecstatic_Heron_7944",
          "text": "Chiming in to offer an alternative perspective: you're doing all this document extraction, table identification, json generating, heavy processing and time consuming work for pages the user hasn't even asked for. In a document with 300 pages, any given query could require a fraction (maybe 10 - 20 pages) for a suitable answer. Could a better approach be to do the search (fast) first and extraction (slow) later - especially after the user is happy to confirm the context? Well, I hope so because this is what I'm building with [ragextract.com](http://ragextract.com) !\n\nTo answer your questions:\n\n1. RAG would work as a way to try narrow the search space for the user query but for financial data, it's unlikely to be sufficient on its own. You'll still need to post-process the pages for accuracy - though you may sometime get away with just winging it with vision models.  \n2. Multimodal RAG works incredibly well if documents don't share a standardised layout ie different statements from different bank. You'd also might want to look at a more optimised retrieval system for pages.  \n3. In practical terms, not that I can think of. Search-first-parse-later is an alternative RAG approach I think is worth exploring in this scenario.  \n4. Best practices for large documents? You probably already know this but go (1) big, (2) async and (3) distributed!",
          "score": 3,
          "created_utc": "2026-01-30 16:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27mwi8",
          "author": "KnightCodin",
          "text": "There are few gaps in the problem summary that might help  \n1. Operational flow :   \nWhen you say \"tools can fetch the data using this reference when needed\" and \"gent never directly ‚Äúsees‚Äù or understands the extracted data\",   \n\\- Where is data going to - directly to the user and not to the Agent/LLM?  \n\\- What is stopping you from presenting the \"summary\" data (Chain of Density compressed) to the Agent so follow up questions can be answered\n\n2. Do you need to answer questions on documents by other users? - Meaning is it a departmental segmentation with multiple users but same overall context or user/documents are isolated?   \n\\- This will provide type of KG and scale   \n\\- Summary \"Contextual Map\"",
          "score": 2,
          "created_utc": "2026-01-28 13:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27riay",
              "author": "Complex-Time-4287",
              "text": "In my agentic system, users can connect third-party MCP tools. If a tool requires access to the extracted data, the agent can pass that data to the specific tool the user has attached, but only when it‚Äôs actually needed.\n\nThe main issue with relying on summaries is that the extracted data itself is already very large and deeply nested JSON. Generating a meaningful summary from it is hard, and even a compressed (Chain-of-Density‚Äìstyle) summary would still fail to answer very specific questions‚Äîfor example, ‚ÄúWhat was the annual income in 2023?‚Äù\n\nRegarding document access and isolation: documents are scoped strictly to the current conversation. Conversations are not user-specific, and there can be multiple conversations, but within each conversation we only reference the documents uploaded in that same context.\n\nDocuments are uploaded dynamically as part of the conversation flow, and only those on-the-go uploads are considered when answering questions or invoking tools.",
              "score": 1,
              "created_utc": "2026-01-28 14:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27twm2",
                  "author": "KnightCodin",
                  "text": "Better :)  \nSimplistic and practical solution (not to be confused with simple) is   \nMulti-tier retrieval:   \n  \nEg : ‚ÄúSummary doc map‚Äù ‚Üí ‚ÄúTargeted Sub-Node‚Äù ‚Üí \"Drill-Down Deep fetch\"\n\nThis will be the most latency-effective for massive bundles.\n\n**SPECIFICITY :**   \n**Tier A: coarse index**\n\n* Embed¬†full **page summaries**,¬†**section headers**, and¬†**table captions**\n* Or **one chunk per page**¬†: Fully summarized (Will say normalized but that will open a whole new can of worms)\n* Path: identify *which pages/sections matter -> use deep fetch to grab that JSON*\n\n**Tier B: targeted extraction retrieval**\n\n* Once you know relevant pages/sections, fetch only that slice from cloud storage:\n   * e.g., pages 210‚Äì218 JSON\n   * or the section subtree for¬†`Income >` What was the annual income in 2023",
                  "score": 1,
                  "created_utc": "2026-01-28 14:14:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27bc02",
          "author": "patbhakta",
          "text": "For financial data, skip the traditional RAG, skip the vector databases, perhaps skip graph rag too. Go with trees, you'll incur more cost but at least your data will be sound.",
          "score": 1,
          "created_utc": "2026-01-28 12:28:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fxm7",
              "author": "yelling-at-clouds-40",
              "text": "Trees are just subset of graphs, but curious: what kind of trees do you suggest (as node hierarchy)?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27nv20",
              "author": "ajay-c",
              "text": "Interesting do you know any tree techniques?",
              "score": 1,
              "created_utc": "2026-01-28 13:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27oswt",
              "author": "Complex-Time-4287",
              "text": "can you please provide some details on this?",
              "score": 1,
              "created_utc": "2026-01-28 13:48:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bjew",
          "author": "arxdit",
          "text": "I‚Äôm handling this via knowledge trees and human supervised document ingestion (you supervise proper slicing and where the document belongs in the knowledge tree - though the AI does make suggestions)\n\nThe AI by itself is very bad at organizing information with no clear rules and will fail spectacularly \n\nSlowly learning through this\n\nYou can check out my solution [FRAKTAG on github](https://github.com/andreirx/FRAKTAG)",
          "score": 1,
          "created_utc": "2026-01-28 12:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27nxzr",
              "author": "ajay-c",
              "text": "Interesting",
              "score": 1,
              "created_utc": "2026-01-28 13:43:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27p3tn",
              "author": "Complex-Time-4287",
              "text": "Looks interesting, I'll check this  \nFor my use-case, we cannot really have a human in the loop, agents are completely autonomous and must proceed on their own",
              "score": 1,
              "created_utc": "2026-01-28 13:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27q1w8",
                  "author": "arxdit",
                  "text": "I want to get there too and I‚Äôm using my human decisions to hopefully ‚Äúteach‚Äù the ai how to do it by itself, and I am gathering data",
                  "score": 1,
                  "created_utc": "2026-01-28 13:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27btme",
          "author": "proxima_centauri05",
          "text": "You‚Äôre not doing anything ‚Äúwrong‚Äù. This is the natural failure mode when the agent only has a pointer to the data instead of an understanding of it. If the model never sees even a compressed view of the document, it‚Äôll confidently answer based on vibes.\n\nWhat‚Äôs worked for me is separating understanding from storage. On ingestion, I generate a thin semantic layer, section summaries, key entities, numbers, obligations, relationships. That layer is small, fast, and always available to the agent. The heavy JSON stays out of the loop unless the agent explicitly needs to verify something.\nTrying to RAG directly over deeply nested extracted data is usually a dead end. It‚Äôs slow, and the signal to noise ratio is awful. Hierarchical retrieval helps a lot, first decide where to look, then pull only that slice, then answr. Latency stays low because most questions never touch the raw data.\n\nFor financial or forms heavy docs, I often skip RAG entirely and just query normalized fields. It‚Äôs boring, but it‚Äôs correct. RAG is great for ‚Äúexplain‚Äù questions, terrible for ‚Äúcalculate‚Äù ones.\n\nI‚Äôm building something in this space too, and the big unlock was treating documents like evolving knowledge objects, not blobs you fetch. Once the agent has a map of the document, it stops hallucinating and starts reasoning.",
          "score": 1,
          "created_utc": "2026-01-28 12:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27tqal",
              "author": "Complex-Time-4287",
              "text": "In my case, the questions are much more likely to be *‚Äúfind‚Äù* questions rather than *‚Äúcalculate‚Äù* ones. For extremely large documents say a 1,500-page PDF containing multiple tax forms summaries or key-entity layers won‚Äôt realistically capture all the essential details.\n\nAlso, I‚Äôm not entirely sure what you mean by ‚Äújust query normalized fields‚Äù in this context.",
              "score": 1,
              "created_utc": "2026-01-28 14:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27oiym",
          "author": "ajay-c",
          "text": "I do have same issues",
          "score": 1,
          "created_utc": "2026-01-28 13:46:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27swp4",
          "author": "Crafty_Disk_7026",
          "text": "Try using a retrieval MCP https://github.com/imran31415/codemode-sqlite-mcp/tree/main. Here's one I made you can try.  This won't require embedding",
          "score": 1,
          "created_utc": "2026-01-28 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u005",
          "author": "Popular_Sand2773",
          "text": "Your instinct that you can't just shove these in the context window is correct and that you need some sort of RAG but what and why depend on the answers to these questions.\n\nWhat questions are your users asking?  \nGiven it is financial documents if it is mainly numbers and tables they care about then you should think about a SQL db and retrieval. Regular semantic embeddings are not very good at highly detailed math. If it's contract minutia then maybe a vector db and semantic embeddings. Likely you'll need both.\n\nHow much of this is noise?  \nYou mention huge documents and tax forms as an example. If a lot of this is stuff your users are never going to query you are paying both in quality and cost for things you won't use and don't need. Figure out what you can prune. \n\nIs there clear structure you can leverage?  \nJust because it's called unstructured text doesn't mean there is no structure at all. If you can narrow down where in the documents you are looking for a specific query based on the inherent structure like sections etc then you can narrow the search space and increase your top-k odds.\n\nAll this to say. It's not about what RAG is best etc it's what problem are you actually trying to solve and why. If you just want a flat quality bump without further thought try knowledge graph embeddings.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28dreo",
          "author": "Det-Nick-Valentine",
          "text": "I have the same problem.\n\nI'm working on an in-company solution like NotebookLM.\n\nIt works very well for small and medium-sized documents, but when the user uploads something large, like legal documents, it doesn't give good responses.\n\nI'm thinking of going for a summary by N chunks and working with re-ranking.\n\nWhat do you think of this approach?",
          "score": 1,
          "created_utc": "2026-01-28 15:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9g7o",
          "author": "pl201",
          "text": "Take a look of open source LightRAG. Per my research and trying, it has the best potential to be used for the requirements you have described in the post. I am working on enhancements so it can be used on a company setting (multi users, workspace, separate embedding LLM and chat LLM, speed the query for a larger knowledge base.  Etc). PM me if you are interested to make it work for your case.",
          "score": 1,
          "created_utc": "2026-01-28 20:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e7nmc",
          "author": "Infamous_Ad5702",
          "text": "We had a similar problem for a client.\n\nNo GPU needed\nThey Can‚Äôt use black box LLM.\nThey Can‚Äôt have hallucinations.\n\nDefence industry so needed to be offline.\n\nWe built a tool that builds an index first. Makes it efficient. Every new query it builds a new Knowledge Graph. \n\nDoes the trick.",
          "score": 1,
          "created_utc": "2026-01-29 11:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eeuh4",
          "author": "TechnicalGeologist99",
          "text": "Hierarchical RAG. The document structure is important. Detect section headers and use them to construct a data tree of each document. \n\nAt the same time extract tags that you will predefined (i.e. financial, design, technical) use those same tags at query time to prefilter. \n\nWhen a section gets many hits from semantic retrieval you will upgrade and retrieve more or all of that section (it's clearly relevant)\n\nEnsure you use query decomposition (fragmenting the users question into multiple questions for multiple retrieval) and rerank those. For large retrievals, group chunks by their section id and summarise them in context of the sub question that was used to retrieve them. And then inject those summaries as documents in the final call. \n\nCongrats you didn't really need an agentic system. \nBut you can always migrate to one if and when the time is right. But don't just go agentic because it's popular. Build your domain and solutions by proving the need (YAGNI)",
          "score": 1,
          "created_utc": "2026-01-29 12:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31qku6",
          "author": "ampancha",
          "text": "The retrieval strategy matters, but the harder problem at scale is what happens after. Large document volumes feeding an agentic system compound fast: unbounded tool calls spiking costs with no attribution, extracted content leaking PII into the context window, and concurrency triggering retry cascades. Whatever architecture you pick, the production controls need to be designed in from day one. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 21:59:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs2uzw",
      "title": "Building a RAG-Based Chat Assistant using Elasticsearch as a Vector Database",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "author": "Leading-Grape-6659",
      "created_utc": "2026-01-31 13:50:21",
      "score": 18,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "Hi everyone üëã\n\n\n\nI recently built a simple RAG (Retrieval-Augmented Generation) chat assistant using Elasticsearch as a vector database.\n\n\n\nThe blog covers:\n\n‚Ä¢ How vector embeddings are stored in Elasticsearch\n\n‚Ä¢ Semantic retrieval using vector search\n\n‚Ä¢ How retrieved context improves LLM responses\n\n‚Ä¢ Real-world use cases like internal knowledge bots\n\n\n\nFull technical walkthrough with code and architecture here:\n\nüëâ [https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends\\_link&sk=d2006b31e40e3c3ed714c18eabf8f271](https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends_link&sk=d2006b31e40e3c3ed714c18eabf8f271)\n\n\n\nHappy to hear feedback or suggestions from folks working with RAG and vector databases!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2sf6zr",
          "author": "cat47b",
          "text": "I did wonder about elastic but the hassle and cost of using that as a store has put me off a bit. Have you looked at Clickhouse?",
          "score": 2,
          "created_utc": "2026-01-31 13:55:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq1hly",
      "title": "PDFstract now supports chunking inspection & evaluation for RAG document pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "author": "GritSar",
      "created_utc": "2026-01-29 06:55:11",
      "score": 15,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been experimenting with different chunking strategies for RAG pipelines, and one pain point I kept hitting was **not knowing whether a chosen strategy actually makes sense for a given document** before moving on to embeddings and indexing.\n\n\n\nSo I added a **chunking inspection & evaluation feature** to an open-source tool I‚Äôm building called **PDFstract**.\n\n\n\nHow it works:\n\n* You **choose a chunking strategy**\n* PDFstract applies it to your document\n* You can **inspect chunk boundaries, sizes, overlap, and structure**\n* Decide if it fits your use case *before* you spend time and tokens on embeddings\n\n\n\nIt sits as the **first layer in the pipeline**:\n\nExtract ‚Üí Chunk ‚Üí (Embedding coming next)\n\n\n\nI‚Äôm curious how others here validate chunking today:\n\n* Do you tune based on document structure?\n* Or rely on downstream retrieval metrics?\n\nWould love to hear what‚Äôs actually worked in production.\n\nRepo if anyone wants to try it:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qohzz3",
      "title": "Multilingual RAG for Legal Documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "author": "mathrb",
      "created_utc": "2026-01-27 16:06:51",
      "score": 14,
      "num_comments": 12,
      "upvote_ratio": 0.89,
      "text": "Hey all,\n\nWe're a small team (not many engineers) building a RAG system for legal documents(contracts, NDAs, terms of service, compliance docs, etc.).\n\nThe multilingual challenge:\n\nOur documents span multiple languages (EN, FR, DE, ES, IT, etc.).\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have docs in a single language (e.g., all French)\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have mixed-language corpora\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some individual documents are bilingual\n\n¬†\n\nFor legal docs, hybrid search (FT search and dense vectors with re rank) seems to be a good candidate for retrieval. One issue I saw is that most implementations relies on language dependent solutions for FT search.\n\nApproaches I've seen discussed:\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Per-language BM25 indexes: Detect language, route to the right index with proper stemmer. Seems correct but adds complexity. How do you handle bilingual documents?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Language-agnostic tokenization: Skip stemming, just split on whitespace. Loses morphological matching but works across languages.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† BGE-M3 sparse vectors: Supposedly handles 100+ languages natively for both dense and sparse. But does it require GPU? What's the cost/perf tradeoff vs traditional BM25?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Translate everything to English: Normalize the knowledge base. Feels wrong for legal where original wording matters and adds a translation failure mode.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Dense-only + reranker : Skip BM25 entirely, use strong multilingual embeddings (BGE-M3, multilingual-e5) and rerank. Loses exact keyword matching.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Qdrant's native BM25 : Qdrant now has built-in BM25 with language configs. Anyone using this for multilingual? How does it compare to dedicated solutions?\n\n¬†\n\nWe‚Äôd rather use managed services when available in the cloud provider we chose (scaleway).\n\nOur constraints:\n\n* Managed PostgreSQL for app data : only supports pgvector, not pg\\_search/ParadeDB. Would require to self-host a postgres for additional extensions.\n* Prefer simplicity: Leaning toward Qdrant over Milvus since it seems easier to operate.\n* Cost-conscious: GPU-heavy solutions for embeddings are a concern.\n* Multi-tenant: Each tenant's documents are typically in one consistent language, but not always.\n\nAnyone would like to share their experience or thoughts on this challenge?  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o21ijp5",
          "author": "patbhakta",
          "text": "1) translate everything to English is a bad solution for legal reasons as the verbage needs to stay in tact. Translation is another can of worms you don't want to go down.\n\n2) you could have multiple vector\\graph\\sql databases for each language, querying them all could be a nightmare though.\n\n3) embeddings, chunking, parsing, are all an issue that needs to be addressed with every RAG.\n\n4) legal documents are like a git repo, multiple versions and annotations, how are going to handle edits?\n\n5) cross referencing legal jurisdiction that periodically changes as well and in multiple countries you'll need a system for that knowledge base\n\n6) the larger your documents grow the worse your AI will get and will hallucinate beyond use. \n\n7) dedups and gardening can help but that's another wrench in the system\n\nPersonally I wouldn't use RAG on your use case, you need a truthful symantec search with citation. You really should seek consultation before going forward on ideas.",
          "score": 6,
          "created_utc": "2026-01-27 16:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nzh2",
              "author": "mathrb",
              "text": "Thanks for your reply\n\n1. Agree, feels wrong in this domain\n2. Agree with the nightmare\n3. As of today, only last version will be handled\n4. Not in scope\n\n\\`truthful symantec search with citation\\`, what kind of systems are you referring to?  \nEdit: I assume you meant semantic search.  \nEven though, we still face some of the same the challenges (minus the generative part at the end), right?",
              "score": 1,
              "created_utc": "2026-01-27 16:55:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21wv03",
                  "author": "patbhakta",
                  "text": "Yes semantic hybrid approach. \nWithout knowing your document size, document length, who the users are, what do they want done, etc it's hard to provide a solution. \n\nIf the scenario was like a legal SaaS where documents needed to be translated then AI is great at that or a summary to avoid reading hundreds of pages. A tuned AI with case logic can also research as a paralegal or even a college level lawyer.\n\nIf you're building an AI tailored to the firm based on its documents, procedures and cases then it's a bit more complex but doable.\n\nIf you're building an AI for automation even that's doable but a different approach. \n\nHave to break down the scope, what it's roadmap is like, budget, legal compliance, security access (who has access to what documents) etc...",
                  "score": 1,
                  "created_utc": "2026-01-27 17:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o269emw",
          "author": "jael_m",
          "text": "You can still do the hybrid search combining dense vector search and text match like BM25. There are some special tokenizers for multilingual text. For example, milvus supports language identifier to automatically detect and apply the proper tokenizer, and the multi-language analyzer for text retrieval.",
          "score": 2,
          "created_utc": "2026-01-28 07:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e9yg",
              "author": "explodedgiraffe",
              "text": "But sparse will still fail on cross language queries right? It would match a query ‚Äúv√©lo‚Äù (French) with ‚Äúbike‚Äù in English ?",
              "score": 1,
              "created_utc": "2026-01-28 07:47:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blzit",
          "author": "Repulsive-Memory-298",
          "text": "Interesting. I mean bm25 is beyond FT right? You could start with FT key word maybe? Where you could milk that by sorting by vector similarity? \n\nI wonder if term frequency would really struggle with combined languages. \n\nAnyways there are lots of future directions. Definitely start with more basic solutions imo, it‚Äôll give you a footing and then you can explore and test more complex solutions. BM25 is pretty advanced FT compared to meat and potato‚Äôs.",
          "score": 1,
          "created_utc": "2026-01-29 00:34:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ekocl",
          "author": "bumkey1101",
          "text": "Yeah I‚Äôve dealt with this a bit ‚Äî I wouldn‚Äôt overthink ‚Äúperfect multilingual BM25‚Äù.\n\nBM25/FT is awesome **when query + doc are same language**, but it‚Äôs basically not gonna do ‚Äúv√©lo‚Äù ‚Üí ‚Äúbike‚Äù unless you start doing synonym/translation stuff. So I‚Äôd just accept that and let **multilingual embeddings handle cross-lang**, and use BM25 for the ‚Äúsame language, exact wording‚Äù wins (which matters a ton in legal).\n\nAlso bilingual docs: don‚Äôt route by *document* language, route by **chunk**. Chunk it, detect lang per chunk, store `chunk_lang`. Then BM25 mostly hits same-lang chunks, dense can still pull cross-lang if needed.\n\nRe: translate everything to English‚Ä¶ yeah no thx for legal. If you ever need translation, do it only as a *query expansion* (translate the query to a couple langs) not the whole corpus.\n\nGiven your Scaleway constraints, honestly **OpenSearch** for the text side is a pretty sane pick. Qdrant BM25 is nice but once you want analyzers/highlighting/etc you‚Äôll end up in search-engine land anyway.\n\nKeep rerank small (top 20‚Äì50) and do embeddings async on ingestion so you‚Äôre not ‚ÄúGPU all day‚Äù üî•.",
          "score": 1,
          "created_utc": "2026-01-29 13:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f4ib9",
          "author": "proxima_centauri05",
          "text": "We‚Äôve been dealing with something similar. What‚Äôs worked reasonably well for us is keeping raw text untouched and making embeddings fully configurable, so we can switch to multilingual models and re embed without changing the pipeline. That already handles cross language semantic recall better than expected, even for mixed language documents.\n\nHybrid is where it gets tricky for legal. Per language BM25 felt correct in theory but added a lot of operational complexity, especially with bilingual docs. Dense only was simpler but missed exact phrasing cases, so reranking became important.\n\nWe‚Äôve avoided translation entirely because original wording matters too much in legal. Curious to hear how others balance keyword precision vs complexity in production.",
          "score": 1,
          "created_utc": "2026-01-29 14:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g2ri1",
          "author": "Dry_Substance_5124",
          "text": "Given your constraints (small team, cost-conscious, multi-tenant), I‚Äôd do: multilingual dense embeddings + reranker, and add a lightweight language-agnostic lexical layer (no stemming) for exact strings, citations, clause numbers, names. Per-language BM25 is ''correct'' but operationally annoying. For bilingual docs, index at the chunk level with language tags. I‚Äôve seen AI Lawyer-style systems win by keeping retrieval simple and spending effort on chunking + eval, not over-building the index zoo.",
          "score": 1,
          "created_utc": "2026-01-29 17:29:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfzko",
      "title": "Convert Charts & Tables to Knowledge Graphs in Minutes | Vision RAG Tuto...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "author": "BitterHouse8234",
      "created_utc": "2026-01-28 16:24:54",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Struggling to extract data from complex charts and tables? Stop relying on broken OCR. In this video, I reveal how to use Vision-Native RAG to turn messy PDFs into structured Knowledge Graphs using Llama 3.2 Vision.  \n  \nTraditional RAG pipelines fail when they meet complex tables or charts. Optical Character Recognition (OCR) just produces a mess of text. Today, we are exploring VeritasGraph, a powerful new tool that uses Multimodal AI to \"see\" documents exactly like a human does.  \n  \nWe will walk through the entire pipeline: ingesting a financial report, bypassing OCR, extracting hierarchical data, and visualizing the connections in a stunning Knowledge Graph.  \n  \nüëá Resources & Code mentioned in this video: üîó GitHub Repo (VeritasGraph): [https://github.com/bibinprathap/VeritasGraph](https://github.com/bibinprathap/VeritasGraph)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qpla70",
      "title": "A framework to evaluate RAG answers in production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "author": "esp_py",
      "created_utc": "2026-01-28 19:26:55",
      "score": 13,
      "num_comments": 10,
      "upvote_ratio": 0.9,
      "text": " How do you know your RAG system is sending correct answers to users? A\n\n\n\nFollowing a recent discussion[ i had here](https://www.reddit.com/r/datascience/comments/1o5n86i/in_production_how_do_you_evaluate_the_quality_of/), I went ahead and  developed a waterfall evaluation framework designed to fail safely and detect hallucinations. \n\nKey components: \n\n\\- Pre-generation retrieval checks \n\n\\- Answerability validation \n\n\\- Faithfulness scoring (NLI, RAGAS, LLM-as-judge) \n\n\\- Answer relevance checks \n\n\n\n[https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach](https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach)\n\nPlease have a read and let me your thoughs, I will share the results soon in the second part.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2a4j6q",
          "author": "No_Kick7086",
          "text": "Nice work, really good article that and covers an area I am looking to address with my own SME+ Rag system. Im curious about the answerability checks., was the classifier or llm more accurate? Im taking a guess at classifier. Look forward to part 2 on this with your results! Cheers",
          "score": 2,
          "created_utc": "2026-01-28 20:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a4tdw",
              "author": "esp_py",
              "text": "On the answerability check I just tried the LLM and it work fine",
              "score": 2,
              "created_utc": "2026-01-28 20:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2so7ux",
                  "author": "Sweet_Access_9996",
                  "text": "Interesting! Did you find any specific limitations or advantages using the LLM for answerability checks compared to a traditional classifier? Would love to hear more about your experience.",
                  "score": 2,
                  "created_utc": "2026-01-31 14:46:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ahg08",
                  "author": "seomonstar",
                  "text": "nice to know",
                  "score": 1,
                  "created_utc": "2026-01-28 21:18:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d01rd",
          "author": "Charming_Group_2950",
          "text": "TrustifAI also seems to solve the same problem with little different approach. It gives a trust score along with explanations for correct or hallucinated responses. Explore more here:¬†https://github.com/Aaryanverma/trustifai",
          "score": 2,
          "created_utc": "2026-01-29 05:27:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e7w6h",
              "author": "No_Kick7086",
              "text": "Looks good. How are the results from it?",
              "score": 1,
              "created_utc": "2026-01-29 11:45:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e9qhx",
                  "author": "Charming_Group_2950",
                  "text": "Benchmarking in progress. You can see the results here in readme of repo. So far results are promising.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bl4r2",
          "author": "Able-Let-1399",
          "text": "Thanks, good stuff when learnings from real projects are shared üëç. Looking forward to next part.\nI wonder: You must have been warned / read about not asking LLM as a judge to validate multiple aspects in the same prompt, so why do you do that?",
          "score": 1,
          "created_utc": "2026-01-29 00:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l23jr",
              "author": "esp_py",
              "text": "Yes, that is a good point! For the LLM as a judge, I didn't get the choice of the prompt, I used an internal package that have a predefined prompt.",
              "score": 1,
              "created_utc": "2026-01-30 11:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo9u4g",
      "title": "I'm looking for an OCR for my RAG.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "author": "AdministrationPure45",
      "created_utc": "2026-01-27 10:04:44",
      "score": 12,
      "num_comments": 31,
      "upvote_ratio": 1.0,
      "text": "Which one do you think is the best among:   \nMistral OCR, LightOnOCR-2, Chandra, OlmOCR 2, Dolphin-v2, LlamaParse, Reducto, Qwen2.5-VL-8B, or DeepSeekOCR?\n\nWhich one do you use? Thanks ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o224zfc",
          "author": "maniac_runner",
          "text": "llmwhisperer if you want to parse complex tables in documents",
          "score": 6,
          "created_utc": "2026-01-27 18:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2009jp",
          "author": "CapitalShake3085",
          "text": "Qwen3 vl",
          "score": 5,
          "created_utc": "2026-01-27 11:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr0q1",
          "author": "zzriyansh",
          "text": "use pymupdf4llm + layout + pro",
          "score": 4,
          "created_utc": "2026-01-27 10:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o201f6p",
          "author": "roydotai",
          "text": "DeepSeekOCR v2 came out recently. I haven‚Äôt gotten around to test it myself yet, but it looked interesting",
          "score": 4,
          "created_utc": "2026-01-27 11:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zzn59",
          "author": "hashiromer",
          "text": "Go with LlamaParse or Reducto if budget allows. I benchmark various PDF parsing solutions for work and LlamaParse and Reducto are far ahead of everything else. However, i only tested at highest compute so ymmv.\n\nYou can also directly use Gemini 3 Flash to convert PDFs to markdown directly with a prompt but you will need to write some basic logic like splitting pages and convert each separately. Gemini on its own beats specialized pipeline based OCRs solutions easily on my internal benchmarks.\n\nIt also depends on complexity of layout by the way, if you are dealing with simple layouts, simpler pipeline based tools like docling,minerU,marker could also work very well.",
          "score": 3,
          "created_utc": "2026-01-27 11:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20aoiw",
              "author": "nofuture09",
              "text": "why reducto? isnt llamaparse better and cheaper?",
              "score": 1,
              "created_utc": "2026-01-27 12:58:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20r6yd",
                  "author": "hashiromer",
                  "text": "Reducto was cheaper last i tested and faster but LlamaParse was slightly more accurate.",
                  "score": 1,
                  "created_utc": "2026-01-27 14:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o202dir",
          "author": "SiebenZwerg",
          "text": "We use Mistral OCR due to high accuracy.",
          "score": 3,
          "created_utc": "2026-01-27 12:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20gus1",
          "author": "AloneSYD",
          "text": "I like a very underrated OCR model nanonets/Nanonets-OCR2-3B FP8 quantized",
          "score": 3,
          "created_utc": "2026-01-27 13:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr1pz",
          "author": "fabkosta",
          "text": "‚ÄúBest‚Äù depends on your data. You need to do a PoC to find out. Also look at Docling, it‚Äôs free.",
          "score": 2,
          "created_utc": "2026-01-27 10:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zt402",
          "author": "Live-Guitar-8661",
          "text": "We use Llama-4-17b with Poppler, probably not the popular choice out there but works for us. Give it a shot if you want to see how well it works: https://orchata.ai\n\nPS- not giving you the link as a plug, just saying if you want to see how they work, it‚Äôs free and you can get a sense of the output in the dashboard. Hope that helps.",
          "score": 2,
          "created_utc": "2026-01-27 10:47:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zbll",
          "author": "it_and_webdev",
          "text": "Docling",
          "score": 2,
          "created_utc": "2026-01-27 15:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o269e9z",
          "author": "sirebral",
          "text": "Smallest Qwen 3 VL variant is wonderful at this task, even with a four bit quant.",
          "score": 2,
          "created_utc": "2026-01-28 07:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zu2u4",
          "author": "instantlybanned",
          "text": "Depends a little on what you need it for (document ocr, read text in images etc.) but for just general purpose OCR on images, Paddle's PP-OCRv5 is probably the state-of-the-art model out there at the moment.",
          "score": 1,
          "created_utc": "2026-01-27 10:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o200kzs",
          "author": "teroknor92",
          "text": "ParseExtract works well for complex documents with tables, handwriting etc. The pricing is very friendly with good ocr and data extraction accuracy.",
          "score": 1,
          "created_utc": "2026-01-27 11:48:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21fnkx",
          "author": "Rich-Emu-1561",
          "text": "An OCR API could help that. I have been using qoest for developers platform for similar document processing. You can check their site to see if it fit your setup.",
          "score": 1,
          "created_utc": "2026-01-27 16:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228kmn",
          "author": "zmanning",
          "text": "[https://99franklin.github.io/ocrbench\\_v2/](https://99franklin.github.io/ocrbench_v2/)",
          "score": 1,
          "created_utc": "2026-01-27 18:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23qx01",
          "author": "No_Thing8294",
          "text": "What do you want to achieve? Qwen-VL is totally different to Deepseek-OCR. Totally different approaches.",
          "score": 1,
          "created_utc": "2026-01-27 22:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23r5r4",
              "author": "No_Thing8294",
              "text": "‚Ä¶and it depends on the kind of input data. Text? Tables, complex PDFs etc.?",
              "score": 1,
              "created_utc": "2026-01-27 22:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25bgby",
          "author": "FrozenBuffalo25",
          "text": "What‚Äôs your hardware?",
          "score": 1,
          "created_utc": "2026-01-28 03:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264vwh",
          "author": "DressMetal",
          "text": "Do you need a local model? Otherwise, Gemini 2.5 flash lite is great at this and it costs next to nothing.",
          "score": 1,
          "created_utc": "2026-01-28 06:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jawc",
          "author": "Ch3mCat",
          "text": "Colqwen 2.5 ?",
          "score": 1,
          "created_utc": "2026-01-28 13:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vumt",
          "author": "Independent-Cost-971",
          "text": "Try using Kudra AI it is very good at complex doc extraction you can even pick what you want to extract",
          "score": 1,
          "created_utc": "2026-01-28 14:24:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsbnr",
          "author": "nnamfuak",
          "text": "Mistral OCR 3 (mistral-ocr-2512)",
          "score": 1,
          "created_utc": "2026-01-29 01:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d5rz5",
          "author": "Whole-Assignment6240",
          "text": "recently i got recommended on mineru. haven't tried yet.",
          "score": 1,
          "created_utc": "2026-01-29 06:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e27ll",
          "author": "patbhakta",
          "text": "OCR has been around for a very long time, way before LLM and AI. What exactly are you extracting? I use 3-4 models at any given time but measuring accuracy they all will fail.",
          "score": 1,
          "created_utc": "2026-01-29 11:00:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ijm4a",
          "author": "Infamous_Ad5702",
          "text": "I made my own tool. Handles PDF‚Äôs well, called it Leonata. Inside I use Apache Tika to handle file conversion. I‚Äôll check what is doing the OCR but it works great.\n\nNo tokens\nNo hallucination \nNo GPU",
          "score": 1,
          "created_utc": "2026-01-30 00:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q8o0d",
          "author": "Great_Fun7005",
          "text": "Run your own and only use an llm as the final fallback, it‚Äôs cheaper and quicker. PyPDF2 ‚Üí Tesseract ‚Üí LLM of choice will cover most of your needs for free then use the llm as the ultimate fall back. I‚Äôve heard good things about Deepseek and mistral and have had good luck with Gemini 2.5 Flash",
          "score": 1,
          "created_utc": "2026-01-31 03:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2295sc",
          "author": "Fresh_Refuse_4987",
          "text": "You can check Reseek. It's an AI tool that automatically extract text from images and PDFs for your RAG pipeline, and uses semantic search so you can find your information easily.",
          "score": 0,
          "created_utc": "2026-01-27 18:26:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qsenyn",
      "title": "MiRAGE: A Multi-Agent Framework for Generating Multimodal, Multihop Evaluation Datasets (Paper + Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "author": "Socaplaya21",
      "created_utc": "2026-01-31 21:21:26",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "TL;DR**:** We developed a multi-agent framework that generates multimodal, multihop QA pairs from technical documents (PDFs containing text, tables, charts). Unlike existing pipelines that often generate shallow questions, MiRAGE uses an adversarial verifier and expert persona injection to create complex reasoning chains (avg 2.3+ hops).\n\n**Paper:** [https://arxiv.org/abs/2601.15487](https://arxiv.org/abs/2601.15487)\n\n**Code:** [https://github.com/ChandanKSahu/MiRAGE](https://github.com/ChandanKSahu/MiRAGE)\n\nHi everyone,\n\nWe've been working on evaluating RAG systems for industrial/enterprise use cases (technical manuals, financial reports, regulations), and (as many have) we hit a recurring problem: standard benchmarks like Natural Questions or MS MARCO don't reflect the complexity of our data.\n\nMost existing eval datasets are single-hop and purely textual. In the real world, our documents are multimodal (*especially* heavy on tables/charts in our use cases) and require reasoning across disjoint sections (multi-hop).\n\nWe built and open-sourced MiRAGE, a multi-agent framework designed to automate the creation of \"Gold Standard\" evaluation datasets from your arbitrary corpora.\n\nInstead of a linear generation pipeline (which often leads to hallucinations or shallow questions), we use a swarm of specialized agents.\n\nInstead of immediate generation, we use a retrieval agent that recursively builds a semantic context window. This agent gathers scattered evidence to support complex inquiries *before* a question-answer pair is formulated, allowing the system to generate multi-hop queries (averaging >2.3 hops) rather than simple keyword lookups.\n\nWe address the reliability of synthetic data through an adversarial verification phase. A dedicated verifier agent fact-checks the generated answer against the source context to ensure factual grounding and verifies that the question does not rely on implicit context (e.g., rejecting questions like \"In the table below...\").\n\nWhile the system handles text and tables well, visual grounding remains a frontier. Our ablation studies revealed that current VLMs still rely significantly on dense textual descriptions to bridge the visual reasoning gap, when descriptions were removed, faithfulness dropped significantly.\n\nThe repo supports local and cloud API model calls. We're hoping this helps others stress test their pipelines.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qngww9",
      "title": "GraphRAG D√©j√† Vu: Freezing Edges = Graph DB Repeat? (Prod Trade-offs)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "author": "dqj1998",
      "created_utc": "2026-01-26 14:05:21",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "**Update (Jan 27, 2026)**:  \nThanks for the great discussion here in r/RAG! Some highlights from related threads:\n- Determinism & reproducibility as key to relational DB win (echoing why Graph DBs struggled).\n- Real prod experiences: keep graphs deterministic/auditable (e.g., calls/imports/FKs), avoid LLM-guessed edges clutter.\n- Links shared: [DDG preprint](https://zenodo.org/records/18373053) and [RoslynIndexer repo](https://github.com/RusieckiRoland/RoslynIndexer) for deterministic code RAG.\n\n---\n\nr/RAG ‚Äî GraphRAG hype (explicit graphs over vector RAG) feels like 70s graph DBs (IMS/CODASYL): explicit relations won benchmarks but lost to relational cuz upfront assumptions brittle.\n\n**Hype vs Reality**\nLLM infers entities/relations ‚Üí persist edges ‚Üí query traversal. Cool for global search, but edges = ingestion-time guesses ‚Üí bias for new intents.\n\n**Core Brittleness**\nFrom my [r/programming post](https://www.reddit.com/r/programming/comments/1pz6pj3/graphrag_is_just_graph_databases_all_over_again/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): Nodes=facts, edges=guesses. Scoped query-time inference (BM25+vectors+rerank) often better for ambiguous RAG (no freeze).\n\n**Pushback & Predictability**\nComments nailed it: auditable edges > opaque LLMs (prod win). Dynamic rebuilds? Viable, but maintenance cost high vs simple hybrid RAG.\n\nShines: stable domains (regs/code deps). Fails: intent-shifting queries.\n\nMedium breakdown:[Medium friend link](https://medium.com/sisai/graphrags-deja-vu-why-are-we-repeating-the-same-mistakes-f6852f54bde0?sk=2692c642e7dfb19e9d552162462384c4)\n\n\nProd experiences? GraphRAG beat baseline RAG on your corpus (e.g., multi-hop QA, latency)? Hybrid + dynamic graphs? Or stick to rerank?\n\nShare benchmarks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1uoi8i",
          "author": "RolandRu",
          "text": "Really happy I found this Reddit btw ‚Äî the topics here are genuinely interesting and they kind of force you to think things through.\n\nAnd yeah, you‚Äôre right: it really depends.\n\nGraphs *can* be brittle when the edges are basically guessed during ingestion (LLM-inferred relations). You‚Äôre sort of freezing assumptions that may not match what people will ask later.\n\nBut it‚Äôs very use-case dependent. For code, I honestly think a dependency graph is pretty much **non-optional**. Calls/imports/inheritance aren‚Äôt opinions ‚Äî they‚Äôre real structure. Without graph expansion you often end up with random snippets, and vanilla RAG struggles badly with questions like ‚Äúwhere does this start?‚Äù or ‚Äúwhat does this change affect?‚Äù, because you‚Äôre missing the whole call chain.",
          "score": 4,
          "created_utc": "2026-01-26 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xa5pj",
              "author": "dqj1998",
              "text": "I totally agree! Seeing your comment about code dependencies resonated with me, I also believe that code is essentially a set of pre-defined dependency chains, and debugging is about constantly patching the gap between these pre-defined chains and reality.\n\nWhile my thinking is still quite rudimentary, that's precisely why I wanted to discuss it further here.\n\nThank you for sharing your real-world experience! In this era where AI is coding faster and faster, this kind of discussion is really interesting‚Äîdo you think AI might represent a spiral of dependency reduction?",
              "score": 2,
              "created_utc": "2026-01-27 00:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xg881",
                  "author": "RolandRu",
                  "text": "I see it like this: dependencies aren‚Äôt going away, because they‚Äôre basically a consequence of architecture (boundaries, responsibilities, contracts). But AI lowers the cost of doing things ‚Äúthe right way‚Äù ‚Äî it‚Äôs easier to add an adapter, an interface, a test, validation, or split a big chunk into smaller parts, without feeling like you‚Äôre wasting time on repetitive stuff. So you‚Äôre less tempted to cram everything into one file/method ‚Äújust because it‚Äôs faster.‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-27 00:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1upwsy",
          "author": "hhussain-",
          "text": "Determinism is what really makes this paradigm *shake*.  \nYour Reddit post and Medium article are excellent ‚Äî they pinpoint both the possibilities *and* the limitations very clearly.\n\nWhat ultimately made relational databases win wasn‚Äôt just performance, but **determinism and reproducibility enabled by a new computing model**.\n\nI had a similar discussion on [r/Rag post](https://www.reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) about deterministic graphs vs probabilistic vectors/embeddings. That thread, along with related discussions, helped isolate the issue to what seems like a missing graph category.\n\nInteresting to see this resurfacing now. I‚Äôve just published a timestamped preprint defining a *Deterministic Domain Graph (DDG)* category:  \n[https://zenodo.org/records/18373053](https://zenodo.org/records/18373053)\n\nI‚Äôm currently working on a framework to construct DDGs in practice, and early experiments suggest this is feasible even for large real-world codebases.",
          "score": 3,
          "created_utc": "2026-01-26 17:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vrjfq",
              "author": "RolandRu",
              "text": "Thanks for sharing the article ‚Äî honestly pretty interesting read.\n\nThis is actually close to where I ended up while building a **code RAG** system. I‚Äôm trying to keep the *edges* deterministic + auditable (calls/imports/inheritance, ReadsFrom/WritesTo, FKs etc.), and I‚Äôm really trying to avoid freezing ‚ÄúLLM-guessed‚Äù relations during ingestion.\n\nI kind of treat vectors as *ranking / fuzzy recall*, but the graph as the closed-world structure that should rebuild the same way every time. For example I force stable outputs (sorted nodes/edges) and I also add missing TABLE nodes so the SQL graph is actually closed (nodes + edges), not half implicit.\n\nOne thing I‚Äôd highlight though: **heuristics ‚â† inference**. I‚Äôm fine with fixed, testable heuristics (like inline SQL detection) ‚Äî even if it‚Äôs not perfect, it‚Äôs still deterministic and you can regression-test it. What I‚Äôm trying to avoid is context-dependent enrichment that changes depending on the model/prompt or whatever the ‚Äúbest guess‚Äù is this week.\n\nIf you‚Äôre curious, this repo is just the **indexing part** (Roslyn/.NET side). The actual RAG pipeline / retrieval is in a separate project:  \n[https://github.com/RusieckiRoland/RoslynIndexer](https://github.com/RusieckiRoland/RoslynIndexer)\n\nAlso curious how you want to handle schema evolution / versioning for DDGs on big real repos ‚Äî do you version the domain spec per build, kind of like a compiler?",
              "score": 3,
              "created_utc": "2026-01-26 20:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xcvpz",
                  "author": "dqj1998",
                  "text": "Wow, thank you both‚Äîthis thread is gold! \n\nu/hhussain, your DDG preprint looks fascinating‚Äîdeterminism + reproducibility as the key to why relational won makes total sense, and early experiments on large codebases sound promising. Will dive into it right away.\n\nu/RolandRu, super appreciate you sharing your setup and the RoslynIndexer repo! Exactly aligns with what I've been thinking: keep the graph as closed-world, deterministic structure (hard facts like calls/imports/inheritance/ReadsFrom/WritesTo/FKs), and treat vectors as fuzzy recall/ranking. Love the emphasis on heuristics (fixed, testable, regression-friendly) vs. context-dependent LLM guesses‚Äîthat's the brittleness killer.Your approach to forcing stable outputs (sorted nodes/edges) and adding missing nodes for closed structure is smart‚Äîavoids the \"half implicit\" mess. Curious on a couple things:\n\n\\* How do you handle schema evolution/versioning in big repos? Per-build domain spec like a compiler, or something else?\n\n\\* Have you seen measurable wins on those chain-tracing queries (e.g., impact analysis) vs. vanilla RAG?\n\n\\* Any thoughts on hybrid with dynamic rebuilds for evolving code, or is pure deterministic the way?\n\nThis ties perfectly into what I'm exploring next: code itself as frozen presuppositions/dependencies, and debug as closing the gap to reality. If you're open, I'd love to reference/link your repo/preprint in upcoming posts (with credit, of course).\n\nThanks again‚Äîthreads like this are why I love posting here!",
                  "score": 3,
                  "created_utc": "2026-01-27 00:30:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qr1suf",
      "title": "Build n8n Automation with RAG and AI Agents ‚Äì Real Story from the Trenches",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "author": "According-Site9848",
      "created_utc": "2026-01-30 10:24:49",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "One of the hardest lessons I learned while building n8n automations with RAG (Retrieval-Augmented Generation) and AI agents is that the problem isn‚Äôt writing workflows its handling real-world chaos. I was helping a mid-sized e-commerce client who sold across Shopify, eBay, and YouTube and the volume of incoming customer questions, order updates and content requests was overwhelming their small team. The breakthrough came when we layered RAG on top of n8n: every new message or order triggers a workflow that first retrieves relevant historical context (past orders, previous customer messages, product FAQs) and then passes it to an AI agent that drafts a response or generates a content snippet. This reduced manual errors drastically and allowed staff to focus on exceptions instead of repetitive tasks. For example, a new Shopify order automatically pulled product specs, checked inventory, created a draft invoice in QuickBooks and even generated a YouTube short highlighting the new product without human intervention. The key insight: start with the simplest reliable automation backbone (parsing inputs ‚Üí enriching via RAG ‚Üí action via AI agents), then expand iteratively. If anyone wants to map their messy multi-platform workflows into a clean, intelligent n8n + RAG setup, I‚Äôm happy to guide and  to help get it running efficiently in real operations.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2nhhpu",
          "author": "Whole-Board4430",
          "text": "Hi i read your post, i am new to RAG. for one of my employers where i'm helping with marketing and implementing basic AI, i am building a RAG agent. For now i just built a simple agent on n8n, 1 workflow to vector the documents into a database, the agent to retrieve it.\n\nWhat i want this agent to be able to do (if possible) is using our business documents + certain books, methods and other data we find important, and transform this working together with a seperate LLM to generate content, help our customers with questions, help our team with their work, onboard new people into the team when nessecarry. Do you mind if i ask you some questions, you now have a basic view of what i am trying to get working",
          "score": 1,
          "created_utc": "2026-01-30 18:46:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp7psa",
      "title": "Compared hallucination detection for RAG: LLM judges vs NLI",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "author": "meedameeda",
      "created_utc": "2026-01-28 10:25:07",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "I looked into different ways to detect hallucinations in RAG. Compared LLM judges, atomic claim verification, and encoder-based NLI.\n\nSome findings:\n\n* LLM judge: 100% accuracy, \\~1.3s latency\n* Atomic claim verification: 100% recall, \\~10.7s latency\n* Encoder-based NLI: \\~91% accuracy, \\~486ms latency (CPU-only)\n\nFor real-time systems, NLI seems like the most reasonable trade-off. \n\nWhat has been your experience with this?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26vl7l",
          "author": "meedameeda",
          "text": "full write up here if interesting: [https://agentset.ai/blog/how-to-detect-hallucinations-in-rag](https://agentset.ai/blog/how-to-detect-hallucinations-in-rag)",
          "score": 4,
          "created_utc": "2026-01-28 10:25:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2d30ki",
              "author": "aiprod",
              "text": "Reading the comment now and seeing you used RAGTruth. It‚Äôs a poor dataset full of errors. Try our modified version linked in my other comment.",
              "score": 1,
              "created_utc": "2026-01-29 05:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2785zr",
          "author": "Upset-Pop1136",
          "text": "we put nli as a fast filter, async llm judge on low-confidence hits, and cache verdicts per doc passage. reduced latency and cost by 70% while keeping recall. try thresholding confidence before invoking expensive checks.",
          "score": 2,
          "created_utc": "2026-01-28 12:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2hvp",
          "author": "aiprod",
          "text": "We tested NLI based detectors like azure groundedness on our Ragtruth++ dataset (https://www.blueguardrails.com/en/blog/ragtruth-plus-plus-enhanced-hallucination-detection-benchmark). And the results were very different. More like 0.35 f1 score.\n\nOur own hallucination detection (agentic verification) scores around 0.8 f1 on the same dataset.\n\nI think your high scores are an indication of a poor quality dataset or some mistakes in the benchmark setup.\n\nHere‚Äôs a video with some numbers for azure and a comparable approach to NLI from scratch (both at 0.35 - 0.45 f1): https://www.blueguardrails.com/en/videos/ragtruth-plus-plus-benchmark-creation",
          "score": 2,
          "created_utc": "2026-01-29 05:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27dxpi",
          "author": "youre__",
          "text": "Seems to have potential if tested for production and applied to certain applications (e.g., where information correctness is a nice-to-have, not a critical requirement).\n\nFrom the test, anything 100% seems fishy. How many samples and what are the error bars after running same test with different seeds? There‚Äôs a ‚Äú66.7%‚Äù precision number in the article, which is oddly clean (2/3), too. Was there a test/validation split with the dataset?\n\nFor hardware testing and comparison, the laptop vs gpt-5 is an interesting comparison. Network latency will be a factor as well as thinking level. So a good test might be to test the NLI over the network, even if on a cloudflare tunnel to simulate cloud. Also test thinking/non-thinking variants of smaller cloud models. This way you can see where the cutoff in performance is. E.g., Can gpt-4o-mini perform just as well as gpt-5 on the dataset? And/Or maybe another cloud hallucination detector?\n\nThis might help ground the comparison and highlight the true benefits against systems people are already using.",
          "score": 1,
          "created_utc": "2026-01-28 12:45:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o290pyn",
          "author": "Financial-Bank2756",
          "text": "Interesting breakdown. These are all post-generation detection, catching hallucinations after the model outputs them. I've been exploring the other side: pre-generation constraint with my project Acatalepsy. which uses\n\n* VIN (Vector Identification Number) ‚Äî constraint operators, not labels\n* ACU (Atomic Claim Unit) ‚Äî immutable identity, mutable confidence\n* Pulse-VIN cycle ‚Äî emission ‚Üí coalescence ‚Üí interrogation ‚Üí sedimentation\n* Confidence vectors ‚Äî multi-axis, decaying, never absolute\n\nhope this helps",
          "score": 1,
          "created_utc": "2026-01-28 17:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eb2pn",
          "author": "Charming_Group_2950",
          "text": "Try TrustifAI. It provides a trust score along with explanations for LLM responses. Explore here:¬†[https://github.com/Aaryanverma/trustifai](https://github.com/Aaryanverma/trustifai)",
          "score": 1,
          "created_utc": "2026-01-29 12:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wedu",
          "author": "ThrowAway516536",
          "text": "I'd take 100% accuracy any day. If you prefer 91% accuracy, I reckon your product and data, where the LLM is integrated, aren't worth much.",
          "score": 1,
          "created_utc": "2026-01-28 10:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26ydem",
              "author": "meedameeda",
              "text": "if latency and cost don‚Äôt matter, 100% accuracy is obviously the right choice (but also depending on your type of production)",
              "score": 2,
              "created_utc": "2026-01-28 10:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qoyyjm",
      "title": "We built a knowledge graph from code using AST extractors. Now we're drowning in edge cases. Roast our approach.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "author": "TraditionalDegree333",
      "created_utc": "2026-01-28 02:39:08",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.85,
      "text": "I'm building a code intelligence platform that answers questions like¬†*\"who owns this service?\"*¬†and¬†*\"what breaks if I change this event format?\"*¬†across 30+ repos.\n\nOur approach: Parse code with tree-sitter AST ‚Üí Extract nodes and relationships ‚Üí Populate Neo4j knowledge graph ‚Üí Query with natural language.\n\nHow It Works:\n\n    Code File\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ tree-sitter AST parse\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Extractors (per file type):\n        ‚îÇ   ‚îú‚îÄ‚îÄ CodeNodeExtractor     ‚Üí File, Class, Function nodes\n        ‚îÇ   ‚îú‚îÄ‚îÄ CommitNodeExtractor   ‚Üí Commit, Person nodes + TOUCHED relationships  \n        ‚îÇ   ‚îú‚îÄ‚îÄ DiExtractor           ‚Üí Spring  ‚Üí INJECTS relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ MessageBrokerExtractor‚Üí Kafka listeners ‚Üí CONSUMES_FROM relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ HttpClientExtractor   ‚Üí RestTemplate calls ‚Üí CALLS_SERVICE\n        ‚îÇ   ‚îî‚îÄ‚îÄ ... 15+ more extractors\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Enrichers (add context):\n        ‚îÇ   ‚îú‚îÄ‚îÄ JavaSemanticEnricher  ‚Üí Classify: Service? Controller? Repository?\n        ‚îÇ   ‚îî‚îÄ‚îÄ ConfigPropertyEnricher‚Üí Link (\"${prop}\") to config files\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ Neo4j batch write (MERGE nodes + relationships)\n\n**The graph we build:**\n\n    (:Person)-[:TOUCHED]->(:Commit)-[:TOUCHED]->(:File)\n    (:File)-[:CONTAINS_CLASS]->(:Class)-[:HAS_METHOD]->(:Function)\n    (:Class)-[:INJECTS]->(:Class)\n    (:Class)-[:PUBLISHES_TO]->(:EventChannel)\n    (:Class)-[:CONSUMES_FROM]->(:EventChannel)\n    (:ConfigFile)-[:DEFINES_PROPERTY]->(:ConfigProperty)\n    (:File)-[:USES_PROPERTY]->(:ConfigProperty)\n\n  \n**The problem we're hitting:**\n\nEvery new framework or pattern = new extractor.\n\n* Customer uses Feign clients? Write FeignExtractor.\n* Uses AWS SQS instead of Kafka? Write SqsExtractor.\n* Uses custom DI framework? Write another extractor.\n* Spring Boot 2 vs 3 annotations differ? Handle both.\n\nWe have 40+ node types and 60+ relationship types now. Each extractor is imperative pattern-matching on AST nodes. It works, but:\n\n1. Maintenance nightmare¬†- Every framework version bump can break extractors\n2. Doesn't generalize¬†- Works for our POC customer, but what about the next customer with different stack?\n3. No semantic understanding¬†- We can extract¬†\\`@KafkaListener\\`but can't answer¬†\"what's our messaging strategy?\"\n\n**Questions:**\n\n1. Anyone built something similar and found a better abstraction?\n2. How do you handle cross-repo relationships? (Config in repo A, code in repo B, deployment values in repo C)\n\n\n\n  \nHappy to share more details or jump on a call. DMs open.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26gra0",
          "author": "Sure_Host_4255",
          "text": "That's what I was exactly was thinking about for spring projects, because mostly you don't need general knowledge, but framework specific. Maybe another step would be to create detailed skill or rule for agent how to use this graph, because for LLM it is just graph, it doesn't know what to do with it and still reads the files to context for it's tasks.\nAs for me even better results was when I wrote spring documentation MCP, then even weak models become stronger.\n\nAnother though, you don't need to support All frameworks, java specific would be enough for commercial product, just choose this niche and keep going, enterprise clients will pay for it.\n\nFor patterns you need to ask LLM to write breaf reviews for graph chains, it could cost 20-100$, depending on repo and model. Also write custom wights algorithms, but it's rather doubtful, because it could work great for 1 project and can harm for another.\nI was participating in similar project for Cobol, and can say you are in the right direction and have similar problems ‚ò∫Ô∏è",
          "score": 1,
          "created_utc": "2026-01-28 08:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26inxl",
              "author": "Sure_Host_4255",
              "text": "So for conclusion:\n1. Write spring docs MCP, it can fetch documentation from GitHub md docs for each version upgrade \n2. Focus just on java frameworks\n3. Ask LLM to create summary for nodes relationship chain",
              "score": 1,
              "created_utc": "2026-01-28 08:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26lewq",
          "author": "TraditionalDegree333",
          "text": "Thank you for the advice, I will explore",
          "score": 1,
          "created_utc": "2026-01-28 08:52:06",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o28aaig",
          "author": "devopstoday",
          "text": "Sounds good approach. Is your project opensource?",
          "score": 1,
          "created_utc": "2026-01-28 15:33:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28fvkv",
              "author": "TraditionalDegree333",
              "text": "No, it‚Äôs not",
              "score": 1,
              "created_utc": "2026-01-28 15:57:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2liwxe",
          "author": "sp3d2orbit",
          "text": "Yeah, I spent years on this problem. You have a nice setup, and this is a valuable problem to solve.\n\n\nI ended up converging on a solution. I don't use standard graph databases anymore. I created a new type of graph database that can store the AST as well as the extracted rules together in one unified format. This helps the inference engine navigate back and forth between code and rules easily. It even helps with abstraction patterns.¬†\n\n\nFor me the atomic unit is a prototype. And a graph of prototypes is called a prototype graph.\n\n\nThen instead of using extractors, I created a general object to graph framework with circular dependency detection and resolution. That lets me call ToPrototype on any object and have it serialized to a prototype graph which can be stored inside the graph database along with the rules.¬†\n\n\nOne of the cool things of this approach is that the inference in learning engines can utilize graphs that contain natural language or code or data all in the same graph structure.",
          "score": 1,
          "created_utc": "2026-01-30 13:17:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qomak1",
      "title": "Chunking strategies for contracts",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "author": "cat47b",
      "created_utc": "2026-01-27 18:34:39",
      "score": 9,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all, after seeing a few posts I'm curious as to if anyone has a tried and true favourite method for chunking.\n\nIf I set the scene as we're making an agentic RAG for contracts, ingesting with OCR, carrying out hybrid search (vector+BM25) and a re-ranker. My main concern is the ingestion/chunking process (garbage in, garbage out).\n\nLooking at OCR results I can see that a paragraph may have 1 line that lands on the next page, or that a bullet list goes across 2 pages or that structurally few headings are used.\n\nFor no good reason semantic chunking has stood out as something that makes logical sense as I just want chunks that in isolation make sense, and hopefully side-step the need for overlap etc. \n\nOne downside to this I can imagine however is that a chunk can be semantically complete, but could otherwise refer to an exception clause. Without both you could end up with poor context to feed an LLM when answering a user query.\n\n  \nSo back to the original question, for the stated use case does anyone have a pattern they would use here?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o234f9p",
          "author": "ManufacturerIll6406",
          "text": "I just published an article today on this subject, also included lightweight framework for evaluating various strategies with statistical rigor\n\n[https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)",
          "score": 5,
          "created_utc": "2026-01-27 20:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23j8hk",
              "author": "cat47b",
              "text": "Your post was excellent and I did read it, would I be right in saying if you had to pick one it'd be sentence based chunking?\n\nIn a sense I don't really care about chunk size as accuracy is more important to me",
              "score": 1,
              "created_utc": "2026-01-27 21:50:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o23lt8e",
                  "author": "ManufacturerIll6406",
                  "text": "Thanks! And yes. If you just want max recall and don't care about chunk size, sentence chunking is a safe default. It naturally produces larger chunks, which is exactly why it wins.\n\nThe caveat is cost and latency. Sentence chunking at 1024 config gives you \\~3500 char chunks. At top-k=5, that's 17,500 chars in your LLM context before you've added the prompt. If that's fine for your use case, go for it.\n\nTL;DR: Sentence chunking works. Now you know \\*why\\* it works.",
                  "score": 1,
                  "created_utc": "2026-01-27 22:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o292824",
          "author": "ampancha",
          "text": "Chunking matters, but with an agentic RAG the higher-order risk is what happens when the agent acts on a hallucinated clause or when contract text contains embedded instructions that hijack the workflow. Contracts are untrusted input at scale. Before optimizing retrieval, I'd map the action boundary: what can this agent actually do, and what stops it from doing something wrong? Sent you a DM.",
          "score": 1,
          "created_utc": "2026-01-28 17:34:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrdj5s",
      "title": "Reranker Strategy: Switching from MiniLM to Jina v2 or BGE m3 for larger chunks?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-01-30 18:24:03",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\nI'm upgrading the reranker in my RAG setup. I'm moving off ms-marco-MiniLM-L12-v2 because its 512-token limit is truncating my 500-word chunks.\n\nI need something with at least a 1k token context window that offers a good balance of modern accuracy and decent latency on a GPU.\n\nI'm currently torn between:\n\n1. jinaai/jina-reranker-v2-base-multilingual\n\n2. BAAI/bge-reranker-v2-m3\n\nIs the Jina model actually faster in practice? Is BGE's accuracy worth the extra compute? If anyone is using these for chunks of similar size, I'd love to hear your experience.\n\nOpen to other suggestions as well!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o31hlfn",
          "author": "ampancha",
          "text": "Switching to a longer-context reranker solves truncation, but it also means more tokens per retrieval hit flowing into your LLM. If you don't have per-query token caps and cost attribution already in place, the accuracy upgrade can quietly double your inference spend. Worth locking that down before you benchmark the new model. Sent you a DM.",
          "score": 2,
          "created_utc": "2026-02-01 21:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qlkx0",
          "author": "Horror-Turnover6198",
          "text": "Theoretically you could chunk again post-retrieval to sub 512, then rerank, and if one of the sub-chunks makes the cut you splice it back together. Or discard the other sub-chunk. Just depends on your chunking strategy I guess. Not sure if that‚Äôs appealing but it may be an option if bge or jina performance isn‚Äôt great.",
          "score": 1,
          "created_utc": "2026-01-31 04:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlyhg",
              "author": "CourtAdventurous_1",
              "text": "Bit if i truncate and then rerank it, wouldn‚Äôt it decrease the accuracy or the actual meaning of the chunk pr something like that?",
              "score": 1,
              "created_utc": "2026-01-31 04:49:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qmz8g",
                  "author": "Horror-Turnover6198",
                  "text": "So let‚Äôs say you retrieved 10 candidates of 500 words each. If you took each candidate and split it into 3 or 4 chunks, you‚Äôd end up with 30 or 40 sub-512 token chunks, that would then get reranked individually. After rerank, let‚Äôs say chunks 7, 14, and 16 ended up in your top_k. You would then lookup which chunk 7, 14 and 16 came from, and just use those. So the meaning of each chunk would make it into your reranker, just not all at once. Some semantic meaning might be lost by breaking up the chunks for the reranker, but as I understand it, it‚Äôs hard to keep tight meaning across much more than 512 tokens in the first place. I‚Äôm totally open to being called out as wrong here though.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpl86",
          "author": "jannemansonh",
          "text": "spent way too much time optimizing rerankers and chunk sizes for my doc workflows... ended up moving those to needle app since the rag stack is built in. still run custom rag for specific use cases though. for your question though - bge m3 accuracy is solid if you've got the compute, jina v2 is faster but check the multilingual overhead if you're english-only",
          "score": 1,
          "created_utc": "2026-01-31 10:39:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqfxko",
      "title": "Tried to Build a Personal AI Memory that Actually Remembers - Need Your Help!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "author": "Tough-Percentage-864",
      "created_utc": "2026-01-29 18:01:20",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "Hey everyone, I was inspired by the **Shark Tank NeoSapien concept**, so I built my own **Eternal Memory system** that doesn‚Äôt just store data - it *evolves with time*.([LinkedIn](https://www.linkedin.com/posts/abhaygupta53_ai-python-eternal-activity-7422690736359415808-PjWd?rcm=ACoAADg6WLoBZEyh7uuClgIZx3hLSD1IzZb81Nc&utm_medium=member_desktop&utm_source=share))\n\nRight now it can:  \n\\-Transcribe audio + remember context  \n\\-  Create **Daily / Weekly / Monthly summaries**  \n\\- Maintain short-term memory that fades into long-term  \n\\- Run **semantic + keyword search** over your entire history\n\nI‚Äôm also working on **GraphRAG for relationship mapping** and **speaker identification** so it knows *who said what*.\n\nI‚Äôm looking for **high-quality conversational / life-log / audio datasets** to stress-test the memory evolution logic.  \n**Does anyone have suggestions? Or example datasets (even just in DataFrame form) I could try?**\n\nExamples of questions I want to answer with a dataset:\n\n* ‚ÄúWhat did I do in **Feb 2024?**‚Äù\n* ‚ÄúWhy was I sad in **March 2024?**‚Äù\n* Anything where a system can actually recall patterns or context over time.\n\nDrop links, dataset names, or even Pandas DataFrame ideas anything helps! üôå\n\n  \n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2g9zjj",
          "author": "Tough-Percentage-864",
          "text": "Repo link --> [https://github.com/Abhay-404/Eternal-Memory](https://github.com/Abhay-404/Eternal-Memory)",
          "score": 2,
          "created_utc": "2026-01-29 18:01:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2rt5dr",
          "author": "vikasprogrammer",
          "text": "I am also interested in this, I have started working on the hardware part using this chip Seeed Studio XIAO Nordic nRF52840 Sense Module (chip -> BLE audio -> app -> AI/memory -> output). I want to combine efforts.",
          "score": 2,
          "created_utc": "2026-01-31 11:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2sgx12",
              "author": "Tough-Percentage-864",
              "text": "Cool Keep buliding üöÄ",
              "score": 1,
              "created_utc": "2026-01-31 14:05:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2iiim8",
          "author": "DetectiveMindless652",
          "text": "Dm me",
          "score": 1,
          "created_utc": "2026-01-30 00:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kqo5a",
              "author": "Tough-Percentage-864",
              "text": "ü§î",
              "score": 1,
              "created_utc": "2026-01-30 09:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ks73z",
                  "author": "DetectiveMindless652",
                  "text": "We‚Äôre working on something adjacent mate",
                  "score": 2,
                  "created_utc": "2026-01-30 09:57:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}