{
  "metadata": {
    "last_updated": "2026-02-25 17:22:58",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 128,
    "file_size_bytes": 157517
  },
  "items": [
    {
      "id": "1rcba6y",
      "title": "What's the best embedding model for RAG in 2026? My retrieval quality is all over the place",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-23 07:44:08",
      "score": 54,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "I've been running a RAG pipeline for a legal document search tool.\n\nCurrently using OpenAI text-embedding-3-large but my retrieval precision is around 78% and I keep getting irrelevant chunks mixed in with good results.\n\nI've seen people mention Cohere embed-v4, Voyage AI, and Jina v3. Has anyone done real benchmarks on production data, not just MTEB synthetic stuff?\n\nSpecifically interested in retrieval accuracy on domain-specific text, latency at scale (10M+ docs), and cost per 1M tokens.\n\nWhat's working for you in production?\n\njust got access to zeroentropy's embeddings. amazing stuff! [zeroentropy.dev](http://zeroentropy.dev)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1lo2",
          "author": "ChapterEquivalent188",
          "text": "garbage in, garbage out.... semantic chunking and a lot of stuff on ingest AND retrieval side. its all here https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit as a basis system to build on it or the long run you may read here https://github.com/2dogsandanerd/RAG_enterprise_core  have fun. im tired of reading every day bout wrappers which you cant trust..... for legal docs you may need a auditrail as well also for ingest AND retrieval-----\nnever use the happy path as it will never make you happy",
          "score": 26,
          "created_utc": "2026-02-23 07:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0acu",
              "author": "revovivo",
              "text": "but how does it solve the problem of chunk sizing and retrieval ",
              "score": 1,
              "created_utc": "2026-02-23 16:11:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74vkvz",
                  "author": "ChapterEquivalent188",
                  "text": "semantic chunking for a start ? ",
                  "score": 1,
                  "created_utc": "2026-02-24 13:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x57m7",
              "author": "krimpenrik",
              "text": "Great resources!! \n\nHave you tested both?",
              "score": 0,
              "created_utc": "2026-02-23 08:25:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x6td3",
                  "author": "ChapterEquivalent188",
                  "text": "sort of ;) working everyday on it to make it perfect..will release some plugins and extensions to the opensourced ones soon\n\n\nedit: if you r intrested of the outcome just send me a set of docs and ill provide you with the rag for retrieval testing",
                  "score": 2,
                  "created_utc": "2026-02-23 08:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2u1w",
          "author": "lucasbennett_1",
          "text": "before switching models its worth figuring out whether the irrelevant chunks are a retrieval problm or a chunking problem.. 78% precision on legal docs can come from either, and they need different fixes,, embedding model swaps sometimes help but chunking strategy for legal text matters a lot because clauses and definitions often span weird boundaries that standard splitters handle badly.. that said BGE M3 does tend to outperform text embedding 3- arge on domain-specific retrieval in most comparison.. its available through several providers like deepinfra or huggingface at lower cost than openai embeddings which helps if you are iterating on a 10M doc corpus and running a lot of test queries",
          "score": 6,
          "created_utc": "2026-02-23 08:02:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x74f5",
              "author": "ChapterEquivalent188",
              "text": "this.  domain spec retrieval is one importend part but never can solve the garbage in and wordsalad problem ",
              "score": 1,
              "created_utc": "2026-02-23 08:44:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78n3xq",
              "author": "liannehynes",
              "text": "I've tried Zeroentropy Embedddings (still in beta) and it outperforms ALL of the above ;) ",
              "score": 0,
              "created_utc": "2026-02-25 00:34:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78q93v",
                  "author": "No_Injury_7940",
                  "text": "+1",
                  "score": 0,
                  "created_utc": "2026-02-25 00:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2nwf",
          "author": "crewone",
          "text": "We have done extensive tests for book data. Voyage is always the best, but also has a high latency. A local Qwen3 embedder delivers 90% of that performance at a fraction of the cost and latency. The rest comes down to chunking strategies.",
          "score": 9,
          "created_utc": "2026-02-23 08:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x7dyb",
              "author": "picturpoet",
              "text": "How do you decide on one chunking strategy versus the other? Are there any best practises for different types of documents/document structures ",
              "score": 1,
              "created_utc": "2026-02-23 08:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xlr9w",
                  "author": "crewone",
                  "text": "Read up on strategies. Then test and compare. (For large book context we use context-aware chunking)",
                  "score": 2,
                  "created_utc": "2026-02-23 11:06:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78njr2",
              "author": "liannehynes",
              "text": "I've tried zeroentropy embeddings and it outperforms voyage :) ",
              "score": 0,
              "created_utc": "2026-02-25 00:37:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c0mrs",
                  "author": "crewone",
                  "text": "Then use that.",
                  "score": 1,
                  "created_utc": "2026-02-25 14:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2cp5",
          "author": "thecontentengineer",
          "text": "Have you tried ZeroEntropy Embeddings? They‚Äôre as good as their rerankers. You should always use ZeroEntropy, even for embeddings!!",
          "score": 6,
          "created_utc": "2026-02-23 07:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78hoxb",
              "author": "liannehynes",
              "text": "yes we did!! ZeroEntropy has the best embeddings compared to all other providers.",
              "score": 1,
              "created_utc": "2026-02-25 00:05:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78hymx",
                  "author": "DarfleChorf",
                  "text": "How do you get access?",
                  "score": 2,
                  "created_utc": "2026-02-25 00:07:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6xa7he",
              "author": "DarfleChorf",
              "text": "Wait they have embeddings too? I only tried their reranker, didn't realize they had an embedding model. Might give it a shot.",
              "score": 1,
              "created_utc": "2026-02-23 09:15:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78hcow",
                  "author": "thecontentengineer",
                  "text": "Yes they have embeddings and they‚Äôre the best we have tried. Way better than Cohere and Voyage.",
                  "score": 0,
                  "created_utc": "2026-02-25 00:03:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x59v8",
          "author": "xeraa-net",
          "text": "Jina v5 just came out; especially for the model size pushing the state of the art: https://jina.ai/news/jina-embeddings-v5-text-distilling-4b-quality-into-sub-1b-multilingual-embeddings/\nBut it will of course always depend on the domain, language, cleanliness of data, quantization,‚Ä¶\n\n\nDisclaimer: I work for Elastic.",
          "score": 2,
          "created_utc": "2026-02-23 08:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yrokw",
          "author": "tom_at_zedly",
          "text": "On legal docs, the model usually isn't the failure point, it's almost always the chunking.\n\nLegal text has too many nested clauses and cross-references that get cut off by standard recursive splitters just butcher. We‚Äôve found that even basic embedding models work fine if you fix the chunking to keep definitions with their clauses (or use semantic chunking). That usually moves the needle way more than switching from OpenAI to Voyage or BGE.",
          "score": 2,
          "created_utc": "2026-02-23 15:31:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xaud9",
          "author": "thefishflaps",
          "text": "Yeah ZeroEntropy reranker is solid, and they just dropped embeddings too. Worth checking out.",
          "score": 1,
          "created_utc": "2026-02-23 09:21:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5ax8",
          "author": "CEBarnes",
          "text": "I‚Äôve had good reliable results with SPL files. (https://dailymed.nlm.nih.gov/dailymed/index.cfm). Step one was to spend 5-6 months building a parser that normalized and populated a database.\n\nIn your case, given you have doc files, would be to structure the data. Headings, line numbers, citations, parties, proposed order, etc. Stack on a categorizer and add its results to the structure. Build a specific schema that has enough flexibility that you don‚Äôt end up with edge cases everywhere. Once you have semi structured data, then populate an old school database and build an API. Lastly, create the skills the AI needs to ‚Äúintelligently,‚Äù use the API. \n\nGranted, even if vibed all the way to the end, this is likely a 9 to 12 month endeavor. But, your results will be magical.",
          "score": 1,
          "created_utc": "2026-02-23 13:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z9oxo",
          "author": "remoteinspace",
          "text": "we've tried a bunch at papr and qwen 4b is best based on our evals",
          "score": 1,
          "created_utc": "2026-02-23 16:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74rdak",
          "author": "Ascending_Valley",
          "text": "I use sentence transformer (and others) and then reduce and transform with a method that weights toward known similar samples. I end up in R50 with simple weighted distance being very effective.",
          "score": 1,
          "created_utc": "2026-02-24 13:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70qww8",
          "author": "liannehynes",
          "text": "ZeroEntropy have SOTA embeddings but i think they're still on beta",
          "score": 0,
          "created_utc": "2026-02-23 21:04:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcb47i",
      "title": "My RAG retrieval accuracy is stuck at 75% no matter what I try. What am I missing?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "author": "Equivalent-Bell9414",
      "created_utc": "2026-02-23 07:34:01",
      "score": 50,
      "num_comments": 34,
      "upvote_ratio": 0.9,
      "text": "I've been building a RAG pipeline for an internal knowledge base, around 20K docs, mix of PDFs and markdown. Using LangChain with ChromaDB and OpenAI embeddings.\n\nI've tried different chunk sizes (256, 512, 1024), overlap tuning, hybrid search with BM25 plus vector, and switching between OpenAI and Cohere embeddings.\n\nStill hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\n\nIs this a chunking problem or an embedding problem? What else should I be trying? Starting to wonder if I need to add a reranking step after retrieval but not sure where to start with that.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1e94",
          "author": "xpatmatt",
          "text": "I had an issue where a lot of documents and data included very similar terms used in very different contexts which made retrieval for any particular query difficult due to irrelevant retrievals. \n\nI had to segment the docs/data into six different vector DBS based on user intent and route queries to the appropriate DB based on the user's intent. Works great now.",
          "score": 17,
          "created_utc": "2026-02-23 07:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x3r92",
              "author": "redditorialy_retard",
              "text": "could you tell me more about this? very intrigued¬†",
              "score": 2,
              "created_utc": "2026-02-23 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x41ly",
                  "author": "xpatmatt",
                  "text": "Yes if you have questions I can answer them",
                  "score": 2,
                  "created_utc": "2026-02-23 08:14:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ykvj4",
              "author": "Cool_Injury4075",
              "text": "Did you try using a reranker? Unlike embeddings, rerankers can understand the intent of the question and filter out all the junk (unless, of course, the documents need context).",
              "score": 2,
              "created_utc": "2026-02-23 14:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xazxb",
          "author": "adukhet",
          "text": "Your problem is not embeddings, try below \n-if you chunk purely by token length, try markdown aware or/and semantic chunking\n-use rerankers but consider latency. Cross-encoders likely fixes semantically similar but irrelevant issues- but if not try late-interaction\n-try query rewriting/query expansion (e.g. HyDE)\n\nBut most importantly you must diagnose where failure arise before changing architecture",
          "score": 5,
          "created_utc": "2026-02-23 09:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x5f9u",
          "author": "ampancha",
          "text": "Reranking with a cross-encoder will likely push you past 80%, but persistent semantic pollution usually means chunking isn't preserving document boundaries or metadata context. The harder problem: your eval set won't cover the queries that actually break in production. You need per-query observability to see which retrievals are failing live, not just aggregate precision. Sent you a DM",
          "score": 3,
          "created_utc": "2026-02-23 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xi3nk",
              "author": "welcome-overlords",
              "text": "Im probably having similar issues. Would be interested in hearing more in DM",
              "score": 1,
              "created_utc": "2026-02-23 10:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xdn8f",
          "author": "StuckInREM",
          "text": "I think sharing a complete pipeline of what you are doing would be useful, what do your metadata look like for the documents to enanche the retrieval phase? recursive split chunking is for sure not optimal, what do your document structure look like in terms of paragraphs? have you tried with a reranker?",
          "score": 3,
          "created_utc": "2026-02-23 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x0fc9",
          "author": "grabGPT",
          "text": "Are you using OCR on PDFs? Have you checked the accuracy?",
          "score": 2,
          "created_utc": "2026-02-23 07:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmckb",
              "author": "Transcontinenta1",
              "text": "I am trying to use deepseeks ocr. Is there a better free one?",
              "score": 1,
              "created_utc": "2026-02-23 11:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o701n3s",
                  "author": "grabGPT",
                  "text": "It depends on whether the documents you're using are handwritten notes or machine printed. In both cases, accuracy will vary",
                  "score": 1,
                  "created_utc": "2026-02-23 19:04:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x1sk8",
          "author": "ggone20",
          "text": "Not enough information to answer your question. What does your corpus look like?",
          "score": 2,
          "created_utc": "2026-02-23 07:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x23jw",
          "author": "AmbitionCrazy7039",
          "text": "You need structural filtering. Try to classify your documents as precise as possible. Maybe you want to build some relational database around it.¬†\n\nFor example, if you query the Knowledge Base for some ‚ÄûManual X‚Äú question, you only want to search similiar manuals. BM25 is only keyword search, most likely not sufficient. In this example keyword filtering might suggests non-manuals because other docs may relate more often to manuals.",
          "score": 2,
          "created_utc": "2026-02-23 07:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x9817",
          "author": "Glass-Combination-69",
          "text": "Throw it into cognee and see if you get 100%. Graph might be what‚Äôs missing",
          "score": 1,
          "created_utc": "2026-02-23 09:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ya4xo",
          "author": "jrochkind",
          "text": "I am not an expert, but have you tried cross-encoder re-ranking?  (Over-fetching, then re-ranking to get your K). \n\nI have not yet myself, but have been considering it.  Oh from your last line it sounds like you too have been considering it but have not tried it. I think that's what would make sense to try?  I would be curious to your results. \n\nI haven't done it, but it seems pretty straightforward, you just feed your over-fetched results to the re-ranker, with your query, and it reorders them, hopefully putting the less relevant ones at the bottom and out of your final selection slice.",
          "score": 1,
          "created_utc": "2026-02-23 13:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yatxf",
          "author": "code_vlogger2003",
          "text": "Hey have you stored any metadata for every chunk such that in the first hand you can verify that my retrieval step is actually returning the exact relevant ground truth answer page numbers or not etc. In this step you can identify whether it's the chunking issue or embedding drift etc.",
          "score": 1,
          "created_utc": "2026-02-23 14:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yk2by",
          "author": "Dense_Gate_5193",
          "text": "Have you tried using RRF with reranking instead?\n\nNornicDB uses BM25+vector search and uses a reranking model (BYOM) https://github.com/orneryd/NornicDB",
          "score": 1,
          "created_utc": "2026-02-23 14:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytm77",
          "author": "namognamrm",
          "text": "You did rerank?",
          "score": 1,
          "created_utc": "2026-02-23 15:40:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za0tz",
          "author": "remoteinspace",
          "text": "have you tried using a knowledge graph? that worked well for us at papr.. got us 92% retrieval accuracy (top 5 results) on stanford's stark benchmark which has arxiv like docs in their data set. dm me and i can help",
          "score": 1,
          "created_utc": "2026-02-23 16:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70caqp",
          "author": "blue-or-brown-keys",
          "text": "\"Still hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\"\n\nTry synthetic data? Summarize the document , store the summary and drop the document. ",
          "score": 1,
          "created_utc": "2026-02-23 19:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z56f",
          "author": "Much-Researcher6135",
          "text": "yes pull back 3x and use a reranker",
          "score": 1,
          "created_utc": "2026-02-24 04:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z5te",
          "author": "WorkingOccasion902",
          "text": "Have you considered Knowledge Graphs?",
          "score": 1,
          "created_utc": "2026-02-24 04:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mov3",
          "author": "Informal-Victory8655",
          "text": "Change embeddings model",
          "score": 1,
          "created_utc": "2026-02-24 07:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mrh0",
          "author": "Tough-Survey-2155",
          "text": "You need Agentic router: https://github.com/hamzafarooq/multi-agent-course/tree/main/Module_3_Agentic_RAG",
          "score": 1,
          "created_utc": "2026-02-24 07:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75u4y3",
          "author": "TransportationFit331",
          "text": "I recommend Mastra.ai",
          "score": 1,
          "created_utc": "2026-02-24 16:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71nke4",
          "author": "Ok-Attention2882",
          "text": "Skill issue",
          "score": 0,
          "created_utc": "2026-02-23 23:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zct5j",
          "author": "Repulsive-Memory-298",
          "text": "It sounds like a troll but adding porn to your datasets calibrates the vector space",
          "score": -2,
          "created_utc": "2026-02-23 17:09:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r90y1p",
      "title": "Built a Document AI that now extracts structured data (thanks to beta feedback)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r90y1p/built_a_document_ai_that_now_extracts_structured/",
      "author": "proxima_centauri05",
      "created_utc": "2026-02-19 14:41:20",
      "score": 37,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "I‚Äôve been building a product called [TalkingDocuments](https://talkingdocuments.com), it lets you work with documents using AI instead of manually digging through them.\n\nOne thing that kept coming up from beta users (thanks to this sub, was able to get some genuine beta testers) was - ‚ÄúRAG Chat is useful, but I need structured data I can actually use.‚Äù\n\nSo I added Data Extraction\n\nInstead of building a completely separate pipeline, I was able to reuse the same underlying infrastructure that already powers the RAG-based chat, the parsing, chunking, embeddings, and retrieval layers were already there. The main work was making the outputs more deterministic and structured (fields, tables, clean exports) rather than conversational.\n\nThe result is that you can now pull usable data from PDFs and long documents without manually hunting through them or post-processing chat responses.\n\nHuge thanks to the beta users who tested early versions and gave thoughtful, honest feedback. This feature exists largely because people were clear about what wasn‚Äôt working and what would actually make the product useful.\n\nStill early, but it‚Äôs moving in a much more practical direction.\n\nIf you deal with document-heavy workflows and care about reliable, structured outputs. I‚Äôd love more feedback.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r90y1p/built_a_document_ai_that_now_extracts_structured/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6dll8t",
          "author": "Khade_G",
          "text": "Love this direction. A lot of teams realize too late that ‚ÄúRAG chat works‚Äù ‚â† ‚Äúdata is usable.‚Äù Structured extraction is where products become operational.\n\nCurious how you‚Äôre handling:\n- Schema drift across different document layouts\n- Missing/ambiguous fields\n- Multi-table PDFs\n- Cross-page references\n- Low-quality scans / OCR noise\n\nIn our experience, deterministic outputs break not because the model fails, but because document structure entropy explodes once you leave clean PDFs.\n\nAre you validating against a labeled document stress-test set yet, or mostly iterating from beta feedback?\n\nWould be interesting to hear how you‚Äôre thinking about extraction robustness as volume scales.",
          "score": 3,
          "created_utc": "2026-02-20 06:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e1ewm",
              "author": "proxima_centauri05",
              "text": "Fully agree. The main challenge is document structure, not the model.\n\nThe RAG pipeline is not linear. Extraction runs as an agentic workflow where retrieval, validation, and re-checks happen iteratively based on schema needs. So that helps a little.\n\nSchema drift is handled by keeping schemas intent-based rather than layout-dependent. Missing or ambiguous fields are allowed instead of forcing guesses. Multi-table PDFs are handled as independent units. Cross-page references are resolved by expanding retrieval when required. But still,  no matter how robust we make it, there're always some outlier PDFs which don't properly fit into this. \n\nOCR is supported for scanned PDFs and images and works well, with quality depending on input. OCR noise reduction is a problem we're focusing on now. I'm trying to involve LLMs as little as possible to make this more robust and generalised and to reduce token usage ofcourse. Also actively working on improving OCR for handwritten documents. Many teams asked specifically if they can use this on legacy handwritten documents, especially in the manufacturing domain.",
              "score": 1,
              "created_utc": "2026-02-20 08:29:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ixytk",
                  "author": "Khade_G",
                  "text": "Intent-based schemas instead of layout-dependent ones is the right move. That‚Äôs usually where teams escape brittle template logic.\n\nWhere we‚Äôve seen systems start breaking at scale isn‚Äôt in normal schema drift, it‚Äôs in entropy stacking:\nSo for example:\n- OCR noise + table fragmentation\n- Cross-page references + partial retrieval windows\n- Handwritten sections embedded in typed forms\n- Similar fields with subtly different semantics (‚ÄúOrder Date‚Äù vs ‚ÄúInvoice Date‚Äù)\n- Tables that change structure mid-document\n\nIndividually manageable. Together, they compound.\nThe teams that harden this layer well usually build a structured document stress corpus across entropy gradients:\n- Clean digital PDFs\n- Semi-structured exports\n- Table-dense multi-page financial docs\n- Low-res scanned contracts\n- OCR-heavy manufacturing forms\n- Handwritten annotations layered over structured layouts\n\nThen they version-test extraction fidelity across those buckets.\n\nEspecially if you‚Äôre minimizing LLM reliance, that corpus becomes your real regression harness.\n\nIf you‚Äôre moving into legacy + handwritten domains, that‚Äôs exactly where a coverage-controlled dataset saves months of reactive debugging.\n\nHappy to compare notes if you‚Äôre formalizing that layer, this is the kind of system that benefits massively from deliberate entropy coverage early.",
                  "score": 2,
                  "created_utc": "2026-02-21 01:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6alrwx",
          "author": "Due_Midnight9580",
          "text": "Does it include image based pdf",
          "score": 1,
          "created_utc": "2026-02-19 19:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bcmlj",
              "author": "proxima_centauri05",
              "text": "Yes. For PDFs with images (Scanned ones), OCR is supported.",
              "score": 1,
              "created_utc": "2026-02-19 21:41:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o699fun",
          "author": "After_Awareness_655",
          "text": "Smart move reusing RAG for structured extraction... beta feedback paying off big time! No more PDF treasure hunts like a frantic pirate; this is the data goldmine we needed. üòÇ How does it handle super messy tables?",
          "score": 1,
          "created_utc": "2026-02-19 15:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69gdnp",
              "author": "proxima_centauri05",
              "text": "Thanks! Messy tables are the hardest part fr.\n\nI preserve the layout during parsing. Really ugly PDFs are still hit-or-miss, but it works well when the table structure is even moderately consistent. Actively improving this with real-world samples.",
              "score": 1,
              "created_utc": "2026-02-19 16:13:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rask5l",
      "title": "What retrievers do you use most in your RAG projects?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rask5l/what_retrievers_do_you_use_most_in_your_rag/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-21 14:32:22",
      "score": 27,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,  \nI‚Äôm curious to know what retrievers you use most in your RAG pipelines. Do you mainly rely on vector search, BM25, hybrid retrieval, or something else?\n\nWould love to hear what works best for you in real projects.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rask5l/what_retrievers_do_you_use_most_in_your_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6lv6ec",
          "author": "Academic_Track_2765",
          "text": "Great question! Always hybrid with metadata filtering.",
          "score": 14,
          "created_utc": "2026-02-21 14:45:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lvwnm",
              "author": "minaminotenmangu",
              "text": "i do it without metadata filtering. the vector search sometimes knows better. But I think with time and if the database gets bigger filtering will return.",
              "score": 5,
              "created_utc": "2026-02-21 14:49:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lwt2e",
                  "author": "Academic_Track_2765",
                  "text": "Typically you want to do metadata filtering before the search space to avoid noise, but yes it can be done at both layers. My databases is currently at 5 million documents so we have mandatory metadata filtering before the vector search, e.g., the users select data range, region, categorized issue type and few other filters and then hybrid retrieval. It honestly blocks out so many noise points and retrieval speeds goes from seconds to milliseconds with better retrieved candidates.",
                  "score": 5,
                  "created_utc": "2026-02-21 14:54:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m0frg",
          "author": "avebrahimi",
          "text": "I have about 5m+ records, I reached this workflow after a dozen tries, first filter using metadata (both original metadata, and llm-based-metadata), then check if BM25 gets high scores, then mix it with vector search, otherwise just use vector search. and finally, reranking is amazing!!!\n\nthe big point is using perfect embedder for your data, to save tokens/money.\n\nDon't forget to show search result summary using LLM, it will please users.",
          "score": 9,
          "created_utc": "2026-02-21 15:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pgpub",
              "author": "Final_Special_7457",
              "text": "What do u use as embedder model ?",
              "score": 1,
              "created_utc": "2026-02-22 02:32:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r7mi5",
                  "author": "avebrahimi",
                  "text": "Depending on use case, but generally:  \n\\- gemini\\_embedding\\_001 is best of class (online), but it's not always economic.\n\n\\- jina-v3 & Qwen3-Embedding-4B & embedding-gemma-300m are not if you want to run on your own GPU.\n\nBut you should have test cases, and embed and retrieve your very own data to check which embedder is best.",
                  "score": 2,
                  "created_utc": "2026-02-22 11:23:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nwah8",
          "author": "FeeMassive4003",
          "text": "We use hybrid: vector search plus keyword search. No rebranding - we just take k docs from each (usually k=5).",
          "score": 2,
          "created_utc": "2026-02-21 20:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nwrlq",
              "author": "marwan_rashad5",
              "text": "Do you use reranking after retrieval with hybrid search ??",
              "score": 2,
              "created_utc": "2026-02-21 21:01:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o517b",
                  "author": "FeeMassive4003",
                  "text": "No, we just take 5 from each. Total of 10 chunks, go to the LLM. It's quite basic; but it works.",
                  "score": 3,
                  "created_utc": "2026-02-21 21:44:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m82mp",
          "author": "Rodda_LBV",
          "text": "Per verificare la qualit√† dei documenti recuperati quali metodi usate?",
          "score": 1,
          "created_utc": "2026-02-21 15:53:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m9mtr",
          "author": "PresentationNew936",
          "text": "Well we mostly use hybrid (bm25 and vector) for best results. But it also highly depends on the embeddings. And of course there is re ranking at the end.",
          "score": 1,
          "created_utc": "2026-02-21 16:01:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mywlz",
              "author": "marwan_rashad5",
              "text": "I haven't used reranking before.\nHow do you do it?",
              "score": 1,
              "created_utc": "2026-02-21 18:08:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6oqpte",
                  "author": "PresentationNew936",
                  "text": "Re-ranking is a step where you take already retrieved documents and sort them again using a smarter model. This model looks at the user‚Äôs question and each document together and gives a better relevance score. It understands context more deeply, so the most useful and accurate documents move to the top.",
                  "score": 1,
                  "created_utc": "2026-02-21 23:48:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6n36k9",
          "author": "Independent-Bag5088",
          "text": "Depends.\n\n1. What type of documents are they - are we looking at more semantic text or numbers matter more?\n2. If query answers require creativity, semantic retrievals work better, but if they are domain-specific, BM25 (keyword match) might be a better option.\n\nIn my case, I have separated my documents into relational database + vector database, for appropriate use-case.\n\nIn my naive opinion, most of the time it depends on the type of document you are dealing with. Domain knowledge on the document would help you design your RAG¬†system appropriately.",
          "score": 1,
          "created_utc": "2026-02-21 18:29:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rebd0i",
      "title": "Built a four-layer RAG memory system for my AI agents (solving the context dilution problem)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-25 12:02:23",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "We all know AI agents suffer from memory problems. Not the kind where they forget between sessions but something like context dilution. I kept running into this with my agents (it's very annoying tbh). Early in the conversation everything's sharp but after enough back and forth the model just stops paying attention to early context. It's buried so deep it might as well not exist.\n\nSo I started building a four-layer memory system that treats conversations as structured knowledge instead of just raw text. The idea is you extract what actually matters from a convo, store it in different layers depending on what it is, then retrieve selectively based on what the user is asking (when needed).\n\nDifferent questions need different layers. If someone asks for an exact quote you pull from verbatim. If they ask about preferences you grab facts and summaries. If they're asking about people or places you filter by entity metadata.\n\nI used workflows to handle the extraction automatically instead of writing a ton of custom parsing code. You just configure components for summarization, fact extraction, and entity recognition. It processes conversation chunks and spits out all four layers. Then I store them in separate ChromaDB collections.\n\nBuilt some tools so the agent can decide which layer to query based on the question. The whole point is retrieval becomes selective instead of just dumping the entire conversation history into every single prompt.\n\nTested it with a few conversations and it actually maintains continuity properly. Remembers stuff from early on, updates when you tell it something new that contradicts old info, doesn't make up facts you never mentioned.\n\nAnyway figured I'd share since context dilution seems like one of those problems everyone deals with but nobody really talks about.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7bggdt",
          "author": "Specific_Expert_2020",
          "text": "Awesome!\n\nI am building something very similar at work.. but i cannot disclose to much.\n\nSimilar approach with criteria and fields to help differentiate the data in the system.\n\nIs there a reason you kept it to 4?\n\nI am learning this whole RAG thing and I see the section mentions \"why 4\" but did you limit it? Or was 4 enough to keep the response accurate.\n\nJust curious :)",
          "score": 3,
          "created_utc": "2026-02-25 12:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bkror",
              "author": "Independent-Cost-971",
              "text": "I only used 4 in the blog because it made testing and demonstrating the results much cleaner and easier to follow. When you‚Äôre explaining RAG concepts, smaller numbers help keep the examples readable and the behavior obvious.\n\nIn a real project, you‚Äôd absolutely use more than 4.",
              "score": 2,
              "created_utc": "2026-02-25 13:19:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bldbj",
                  "author": "Specific_Expert_2020",
                  "text": "Also I hope my comment did not come off as downplaying your post as it is way more technical than what I am working on so great share.\n\nI appreciate the follow up and sharing.\n\nI had concerns of over fielding as I work in a MSSP and thing vary by a variety of fields.\n\nSo as I develop the roll out.. I am seeing the fields scope creeping.\n\nThank you for the insight.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:22:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7b95tc",
          "author": "Independent-Cost-971",
          "text": "I wrote a whole blog about this that goes way deeper if anyone's interested: [https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/](https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/)",
          "score": 4,
          "created_utc": "2026-02-25 12:03:15",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7bq031",
              "author": "arun4567",
              "text": "This is good. I was looking for some thing like this since my agent gets into loops and forgets that its been provided the details before. How do you handle updates of information that's already been provided,",
              "score": 2,
              "created_utc": "2026-02-25 13:48:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7coofs",
          "author": "lez566",
          "text": "I did something similar. I built an AI agent that learns as you talk to it and builds a living profile of you and your needs. Then anytime a message is sent, the tool gives this living profile, a keyword search, a semantic search and the last n messages. It‚Äôs excellent and never forgets.",
          "score": 1,
          "created_utc": "2026-02-25 16:37:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rb4tv5",
      "title": "How do you evaluate your RAG systems (chatbots)?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rb4tv5/how_do_you_evaluate_your_rag_systems_chatbots/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-21 22:42:03",
      "score": 21,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI'm currently building a RAG-based chatbot and I'm curious how people here evaluate their systems.\n\nWhat methods or metrics do you usually use to measure performance? For example: retrieval quality, answer accuracy, hallucinations, etc.\n\nDo you use any specific frameworks, benchmarks, or manual evaluation processes?\n\nI'd love to hear about the approaches that worked well for you.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rb4tv5/how_do_you_evaluate_your_rag_systems_chatbots/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6oizyv",
          "author": "Khade_G",
          "text": "Most teams evaluate RAG at the wrong layer.\nThey measure:\n- Retrieval precision/recall\n- Context relevance\n- Hallucination rate\n- Faithfulness\n\nThat‚Äôs necessary, but not sufficient.\n\nThe harder failures usually show up in:\n\n1Ô∏è‚É£ Retrieval boundary errors\n- Relevant document exists but falls just outside top-k\n- Chunk fragmentation causing partial reasoning\n- Multi-hop answers that require cross-document stitching\n\n2Ô∏è‚É£ Ambiguity handling\n- Question underspecified but model answers confidently\n- Conflicting sources in corpus\n- Time-sensitive drift (outdated documents retrieved)\n\n3Ô∏è‚É£ Entropy stacking\n- OCR noise + bad chunking + long answers\n- Mixed structured/unstructured sources\n- Schema drift in semi-structured data\n\nWhat we‚Äôve seen working with teams building production RAG systems is this:\n\nThe ones that formalize a structured evaluation corpus early (versioned regression set + labeled failure taxonomy) improve measurably release over release.\n\nThe ones that rely purely on live telemetry + ad hoc manual checks tend to fix issues reactively - and drift creeps in.\n\nThe unlock usually isn‚Äôt more metrics - it‚Äôs a curated set of hard queries across known failure classes.\n\nCurious what domain you‚Äôre building in? Evaluation design shifts quite a bit between support bots, internal knowledge bases, and high-risk domains like legal or compliance.",
          "score": 6,
          "created_utc": "2026-02-21 23:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rwky1",
              "author": "SemperPistos",
              "text": "Is there a book or resource you recommend for learning this?",
              "score": 1,
              "created_utc": "2026-02-22 14:21:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6stwpv",
                  "author": "JealousBid3992",
                  "text": "ChatGPT if you want their original source sure",
                  "score": 3,
                  "created_utc": "2026-02-22 16:59:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6wf6fi",
              "author": "harshitsinghai",
              "text": "I thought he's selling some product that solves this problem... but forgot to paste the link",
              "score": 1,
              "created_utc": "2026-02-23 04:41:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6wljd7",
                  "author": "Khade_G",
                  "text": "DM Me",
                  "score": 1,
                  "created_utc": "2026-02-23 05:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pxxdr",
          "author": "Leather-Departure-38",
          "text": "LLM as judge, i use aws bedrock evaluations framework worked pretty well for my team as well",
          "score": 3,
          "created_utc": "2026-02-22 04:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r7154",
          "author": "goedel777",
          "text": "Your CEO feels happy with some cherry picked queries? You are fine.",
          "score": 2,
          "created_utc": "2026-02-22 11:18:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6phhft",
          "author": "BrilliantUse7570",
          "text": "RAG itself is pretty complex. The deeper you go, the more modular it gets, with lots of moving parts. For better results, make sure you optimize RAG at every step, from indexing to generation.",
          "score": 1,
          "created_utc": "2026-02-22 02:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rbgun",
          "author": "According-Lie8119",
          "text": "I wouldn‚Äôt overthink it, I‚Äôd test. And test consistently.\n\nI use a fixed evaluation set that includes:\n\n* Yes/No questions\n* Open questions\n* Paraphrased variations\n* Questions where the system should say ‚ÄúI don‚Äôt know‚Äù\n* A few edge cases to provoke hallucinations\n\nWhenever I change something (chunking, embeddings, retriever settings, prompts, etc.), I run the exact same set again.\n\nMy setup is semi-automated: I send the questions to my endpoint and get back a structured JSON test report (answers, sources, latency, etc.). That makes it easy to compare versions and detect regressions.\n\nAlso very helpful in production: implement a simple feedback mechanism and collect user feedback in a dashboard. Real-world feedback is extremely valuable and often reveals issues your test set doesn‚Äôt cover.\n\nAutomatic metrics help, but without a stable test set and real user signals, it‚Äôs hard to know if you‚Äôre actually improving the system.",
          "score": 1,
          "created_utc": "2026-02-22 11:58:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sy9pf",
          "author": "Able-Let-1399",
          "text": "I‚Äôm building a setup where you build multiple RAG configs at the same time. Output are summarized on 3 metrics - performance, cost and quality.\nQuality is measured via RAGAS and, if configured, LLM-as-a-judge, based on a set of golden questions and answers. Performance is time. Cost is mainly use of tokens which then, depending on provider (including local), is translated into a monetary values.\nMaybe co-development? Send me a message if interested üôÇ",
          "score": 1,
          "created_utc": "2026-02-22 17:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u18hk",
          "author": "Dihedralman",
          "text": "To start checkout Ragas¬†https://docs.ragas.io/en/stable/. It has most of the metrics mentioned on this thread including how to use LLMs as a judge. You will want to customize your metrics as you develop and iterate.¬†",
          "score": 1,
          "created_utc": "2026-02-22 20:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u9lsz",
          "author": "No-Contribution8248",
          "text": "Promptfoo. It‚Äôs great.",
          "score": 1,
          "created_utc": "2026-02-22 21:07:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w7n7r",
          "author": "TraditionalDegree333",
          "text": "I use deepeval to confirm and scorecard tracing",
          "score": 1,
          "created_utc": "2026-02-23 03:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wnq6r",
          "author": "Capital_Direction231",
          "text": "Evaluating RAG systems properly requires separating retrieval quality from generation quality ‚Äî otherwise it‚Äôs hard to know what‚Äôs actually failing.\n\nAt **Exotica IT Solutions**, we typically evaluate RAG chatbots across four layers:\n\n**1. Retrieval Evaluation**\n\n* Precision@k and Recall@k\n* MRR (Mean Reciprocal Rank)\n* Context relevance scoring We test whether the correct documents are even being retrieved before judging the answer.\n\n**2. Groundedness / Faithfulness**  \nWe check whether the generated answer is actually supported by the retrieved context.  \nLLM-as-a-judge scoring works well here, especially when reference answers are limited.\n\n**3. Answer Quality**\n\n* Correctness\n* Completeness\n* Clarity\n* Hallucination rate\n\nThis can be automated partially but usually includes human validation for high-impact systems.\n\n**4. Regression & Drift Monitoring**  \nWe maintain a fixed benchmark dataset of domain-specific queries and re-run it after:\n\n* Embedding model updates\n* Chunking strategy changes\n* Prompt modifications\n* Model upgrades\n\nIf metrics drop, we know exactly where to look.\n\nFor production systems, we also log user feedback signals (thumbs up/down, correction requests) and combine that with automated scoring.\n\nIn short:  \nEvaluate retriever separately, evaluate generator separately, maintain a gold dataset, and monitor continuously.\n\nThat structure has worked well for us in real-world RAG deployments.",
          "score": 1,
          "created_utc": "2026-02-23 05:47:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x93i0",
          "author": "habibaa_ff",
          "text": "developed a light weight library to help with this at the retrieval layer. Inspecting the ranking behaviour. [https://pypi.org/project/retric/](https://pypi.org/project/retric/)",
          "score": 1,
          "created_utc": "2026-02-23 09:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zd0gw",
          "author": "remoteinspace",
          "text": "at papr we use a retrieval loss formula. It was designed to capture accuracy, latency, and cost. Lower values indicate better overall retrieval (high accuracy, low latency). \n\n**Retrieval Loss (without cost)**\n\n    Retrieval-Loss = ‚àílog‚ÇÅ‚ÇÄ(Hit@K) + Œª ¬∑ (Latency_p95 / 100 ms)\n\nWhere:\n\n* **Hit@K**¬†= probability that the correct memory is in the top-K returned set\n* **Latency\\_p95**¬†= tail latency in milliseconds\n* **Œª (lambda)**¬†= weight that says \"every 100 ms of extra wait feels as bad as dropping Hit@5 by one decade\"\n* In practice,¬†**Œª ‚âà 0.5**¬†makes the two terms comparable\n\n**Extended Three-Term Formula (with cost):**\n\n    Retrieval-Loss = ‚àílog‚ÇÅ‚ÇÄ(Hit@K) + ŒªL ¬∑ (Latency_p95 / 100 ms) + ŒªC ¬∑ (Token_count / 1,000)\n\nWhere:\n\n* **ŒªL**¬†= weight for latency (typically 0.5)\n* **ŒªC**¬†= weight for cost (typically 0.01)\n* **Token\\_count**¬†= total number of prompt tokens attributable to retrieval",
          "score": 1,
          "created_utc": "2026-02-23 17:10:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rab7rs",
      "title": "What chunking strategies are you using in your RAG pipelines?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rab7rs/what_chunking_strategies_are_you_using_in_your/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-20 23:31:11",
      "score": 21,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Hey everyone,\n\nI‚Äôm curious what chunking strategies you‚Äôre actually using in your RAG systems. Are you sticking with recursive/character splitting, using semantic chunking, or something more advanced like proposition-based or query-aware approaches?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rab7rs/what_chunking_strategies_are_you_using_in_your/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6ji2v0",
          "author": "Ok_Signature_6030",
          "text": "for most document types recursive splitting with decent overlap still works better than people expect. we tested semantic chunking pretty extensively and the retrieval quality improvement was marginal ‚Äî maybe 3-5% on our evals ‚Äî while adding a lot of complexity and latency from the embedding calls during ingestion.\n\nwhere chunking strategy actually mattered for us was structured documents like contracts and technical specs. for those we switched to section-aware chunking that respects headers and keeps related clauses together. that alone bumped our answer accuracy by about 15% compared to naive 512-token windows.\n\nbiggest lesson was that chunk size matters way more than chunk method. going from 512 to \\~1200 tokens with 200 token overlap made a bigger difference than any fancy chunking algorithm we tried.",
          "score": 11,
          "created_utc": "2026-02-21 03:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1qkh",
              "author": "Mystical_Whoosing",
              "text": "So this 1200 / 200 is token numbers, not characters?",
              "score": 2,
              "created_utc": "2026-02-21 05:32:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pbbw9",
                  "author": "Ok_Signature_6030",
                  "text": "yeah those are token counts ‚Äî 1200 tokens per chunk, 200 tokens of overlap between consecutive chunks. roughly 900-ish words per chunk depending on the tokenizer.\n\n  \nthe overlap is what prevents splitting an answer across chunk boundaries where neither chunk has enough context alone.",
                  "score": 2,
                  "created_utc": "2026-02-22 01:57:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kmto0",
          "author": "cointegration",
          "text": "I have given up on 1 size fits all chunking strategy, different types of documents require different strategies, legal docs and instruction manuals do well with large chunks and semantic boundaries, invoices and receipts do much better with small chunks and metadata. Same for retrieval, BM25 is for precision, vectors for recall. Its quite obvious by now that much preprocessing/filtering is required even before chunking. I seperate out docs into several buckets, each have their own pipeline for ingestion. In a nutshell:\n\nIngestion:  \ndocs --> sorted into buckets by a small local llm --> extract MD/metadata --> chunk based on doc type --> embed\n\nRetrieval:  \nquery --> tfidf gets reduced candidate size --> BM25/Vector search --> gets weighted Top K chunk candidates plus neighbours --> cross-encoder rerank --> final chunks",
          "score": 3,
          "created_utc": "2026-02-21 08:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jzdmp",
          "author": "StarThinker2025",
          "text": "Recursive splitting + overlap is still the baseline for us (600‚Äì800 tokens, ~15% overlap)\n\nSemantic chunking sounds better in theory, but retrieval stability matters more than ‚Äúperfect‚Äù boundaries\n\nWe optimize chunk size based on embedding model + average query length üìàüìàüìà",
          "score": 2,
          "created_utc": "2026-02-21 05:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k8g1t",
          "author": "One_Milk_7025",
          "text": "Chunking is very important part of the ingestion pipeline..\nTo actually see the chunks and its related metadata you need a chunk workbench or visualizer..\ncheckout chunker.veristamp.in for the start you can see all the code, table, heading, txt properly chunked with all the metadata.. you can tweak the settings for the optimal use case",
          "score": 2,
          "created_utc": "2026-02-21 06:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6km4w7",
          "author": "Infamous_Ad5702",
          "text": "2 sentence chunks.",
          "score": 2,
          "created_utc": "2026-02-21 08:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lhe68",
              "author": "marwan_rashad5",
              "text": "I think that 2 Sentence chunking might cause some loss of semantic coherence. What is your opinion about that?",
              "score": 1,
              "created_utc": "2026-02-21 13:20:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6x8ab8",
                  "author": "Infamous_Ad5702",
                  "text": "We have a floating system. It seems super accurate. Validation tests have been spot on. No loss of context. Passes testing well.",
                  "score": 1,
                  "created_utc": "2026-02-23 08:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lgvzu",
          "author": "itsmekalisyn",
          "text": "We have a lot of How to guides and we use page level and it is working well.",
          "score": 2,
          "created_utc": "2026-02-21 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lh4f7",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 13:18:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lh8cf",
                  "author": "itsmekalisyn",
                  "text": "For our case, recursive chunking was better than semantic. But, page level beats everything in our internal metrics.",
                  "score": 2,
                  "created_utc": "2026-02-21 13:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rykrh",
          "author": "viitorfermier",
          "text": "I'm using a Reasoning based RAG on the fiscal code/laws. Tried chunks, embeddings, logic loops, but that didn't worked well (hard to get in context all needed information). Now I just give the LLM article summaries, get back from it db ids of the relevant articles then do another filter on the filtered articles, then generate answer. It more expensive and time consuming, but the answers are better.\n\nProbably at some point I'll try PageIndex [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) \\- if current method doesn't work.",
          "score": 1,
          "created_utc": "2026-02-22 14:32:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r8y68c",
      "title": "Building a Graph RAG system for legal Q&A, need advice on dynamic vs agentic, relations, and chunking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r8y68c/building_a_graph_rag_system_for_legal_qa_need/",
      "author": "Famous_Buffalo_7725",
      "created_utc": "2026-02-19 12:40:59",
      "score": 20,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI‚Äôm building a Graph RAG system to answer legal questions across statutes, case law, and contracts. The goal is high accuracy, strong multi-hop reasoning, and reliable citations.\n\nI‚Äôm trying to decide:\n\n1. Architecture: Is it better to use a static graph, dynamic query-time graph, or agentic Graph RAG for legal domains? What worked best for you in production?\n2. Relations: What are the most effective techniques for creating strong relations between chunks in legal documents? Entity backbone plus LLM triples? Cross-reference edges? NLI or contradiction edges? Temporal and amendment links? or any other approches?\n\nIf you had to pick a small high-impact stack, what would it be?\n\n1. Chunking What chunking strategy works best for law? Clause level, section level, sliding window, or hybrid?\n2. Evaluation How do you measure quality in legal Graph RAG? Citation precision, lawyer review, curated multi-hop Q&A?\n\nWould appreciate practical advice from anyone who have knowledge of Graph RAG  \nThank You",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r8y68c/building_a_graph_rag_system_for_legal_qa_need/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o68nawb",
          "author": "Ok_Signature_6030",
          "text": "legal graph RAG is one of the harder setups because the relationships between documents actually matter for correctness ‚Äî you can't just do similarity search and hope for the best.\n\n  \non architecture: go agentic over static. in legal you constantly need to trace things like \"this clause was amended by X statute on Y date\" or \"this ruling cites and partially overrules case Z.\" a static graph can't adapt to these multi-hop paths well because the traversal pattern changes based on the query type. an agentic approach where the model decides which edges to follow based on the question gives you much better citation chains.\n\n  \nfor chunking: clause-level is the way to go for legal. section-level loses the granularity you need for citation accuracy (you want to point to the exact clause, not a whole section). but keep the section hierarchy as metadata so you can still navigate up/down the document structure.\n\n  \non relations ‚Äî cross-reference edges + temporal links are probably the highest-impact combo. statutes constantly reference each other and get amended over time. NLI edges are cool in theory but expensive to compute at scale and contradiction detection accuracy in legal language is still sketchy.\n\n  \nfor eval: curated multi-hop Q&A sets with lawyer review on the citations is the gold standard. automated metrics alone won't catch hallucinated citations that look plausible.",
          "score": 4,
          "created_utc": "2026-02-19 13:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68r8m2",
              "author": "Famous_Buffalo_7725",
              "text": "u/Ok_Signature_6030 Thanks for the detailed advice, it helped me a lot.\n\nQuick follow-up. In my dataset, the documents mix legal clauses with financial tables in the same file, like covenant definitions plus ratio tables, pricing grids, payment schedules, etc. Here in chunking what approach would you recommend in this mixed setup? Any practical guidance on how you handle tables, especially when questions depend on specific cells or row items?\n\nAlso for relations, I‚Äôm focusing on cross-references, and temporal links first. Do you have any recommended resources (papers, repos, blog posts, or tooling) that helped you build reliable technical relation extraction for legal and finance documents?\n\nThanks again, I appreciate it.",
              "score": 1,
              "created_utc": "2026-02-19 14:04:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68so2b",
                  "author": "Ok_Signature_6030",
                  "text": "for the mixed clause + table problem, you want a dual-chunking strategy. treat text chunks and table chunks as separate node types in your graph. tables should be stored as structured data (row/column format with headers preserved), not flattened text. when a question targets a specific cell, your retriever needs to match against column headers + row identifiers, not just semantic similarity on raw text.\n\n  \npractically, we detect tables during ingestion (most legal PDFs have identifiable table boundaries), extract them with something like camelot or tabula, store them as structured nodes, and link them to the surrounding clause nodes with 'contains\\_table' or 'defined\\_in' edges. so a query about 'what's the leverage covenant ratio threshold' traces from the covenant definition clause to the pricing table.\n\n  \nfor relation extraction resources... blackstone NLP library is probably the best starting point for legal text (handles citation parsing well). for financial cross-references, docling from IBM has decent table extraction. on the academic side, the CUAD dataset and papers around it are solid for contract understanding benchmarks. neo4j's graphrag blog series also has useful patterns for building extraction pipelines.",
                  "score": 2,
                  "created_utc": "2026-02-19 14:12:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ds2uw",
          "author": "CountlessFlies",
          "text": "Have you tried a basic RAG system first? I would first create a benchmark for the task at hand, try a very basic solution first to get a baseline, and only then attempt to implement more complex solutions to see if they actually improve perf (and by how much).",
          "score": 2,
          "created_utc": "2026-02-20 07:02:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68j9t0",
          "author": "One_Milk_7025",
          "text": "To see realtime chunking with variable chunking setting\nView chunker.veristamp.in",
          "score": 1,
          "created_utc": "2026-02-19 13:19:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69pl7f",
          "author": "tom_at_zedly",
          "text": "For the mixed clause + table problem, dual-chunking works well: text chunks and table chunks as separate node types with different handling at retrieval.\n\n\n\nDuring ingestion, detect table boundaries (camelot or tabula for clean PDFs, Docling for messier layouts). Store tables as structured nodes with rows and column headers preserved ‚Äî don't flatten them to prose. Link them to the surrounding clause nodes with typed edges like \\`covenant\\_clause ‚Üí contains\\_table ‚Üí leverage\\_ratio\\_grid\\`.\n\n\n\nThe key insight: semantic similarity alone won't find \"leverage ratio threshold\" in a table. You need BM25 on column headers and row identifiers for the structured stuff, vector search on clause text, then combine at reranking.",
          "score": 1,
          "created_utc": "2026-02-19 16:57:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aefuq",
              "author": "Mediocre-Basket8613",
              "text": "how should i setup the orchestrating framework? should i use sdks or langchain/langgraph or something else?",
              "score": 1,
              "created_utc": "2026-02-19 18:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6am2h4",
                  "author": "tom_at_zedly",
                  "text": "For this kind of setup I'd lean LlamaIndex over LangChain. It has native graph RAG support (PropertyGraphIndex), handles different node types out of the box (so your text chunks and table chunks can live as separate types), and composable retrievers make the BM25 + vector hybrid straightforward.\n\n\n\nLangGraph is better suited for complex agentic flows with branching logic ‚Äî overkill if your main problem is retrieval. Raw SDKs give you full control, but a pain to maintain.",
                  "score": 1,
                  "created_utc": "2026-02-19 19:31:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kr5bk",
          "author": "ampancha",
          "text": "Agentic Graph RAG in legal is powerful but introduces failure modes your chunking strategy won't catch: prompt injection that manipulates citation selection, unbounded tool calls during multi-hop traversal, and missing audit trails for legal work product. If you go agentic, the first controls to scope are tool allowlists, per-query token caps, and a citation verification layer before anything hits a user. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-21 09:29:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcecf4",
      "title": "How I Used AI + RAG to Automate Knowledge Management for a Consulting Firm",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "author": "Safe_Flounder_4690",
      "created_utc": "2026-02-23 10:52:36",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "Recently, I built a workflow for a consulting firm that leverages AI combined with Retrieval-Augmented Generation (RAG) to fully automate knowledge management, transforming a fragmented document system into a centralized, actionable intelligence hub. The pipeline begins by ingesting structured and unstructured client reports, internal documents and market research into a vector database, then AI agents retrieve the most relevant information dynamically, reason over it and generate concise, actionable summaries or recommendations. By layering persistent memory, validation loops and workflow orchestration, the system doesn‚Äôt just fetch data it contextualizes it for consultants, flags potential conflicts, and tracks follow-ups automatically. This approach drastically reduced time spent searching across multiple tools, eliminated duplication errors and improved decision-making speed. What made it successful is the combination of semantic search, structured reasoning and AI-driven content validation, ensuring that consultants always have the most accurate, up-to-date insights at their fingertips. The outcome: higher productivity, faster client delivery and a knowledge system that scales with the firm‚Äôs growth.\nIf AI can summarize thousands of consulting documents in minutes, how much more value could your team create by focusing only on insights instead of searching for them?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xrg3b",
          "author": "prismaticforge",
          "text": "Have you had feedback from the users?  Are they happy with the results and using the tool.  I am curious also how you handle contradictory information from rag?",
          "score": 2,
          "created_utc": "2026-02-23 11:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xnour",
          "author": "jannemansonh",
          "text": "nice setup... building rag pipelines with custom orchestration is solid but maintaining that glue code gets brutal over time. ended up using needle app for similar doc workflows since it handles the vector db + workflow orchestration in one place (just describe what you want vs wiring everything). kept custom stuff for edge cases though",
          "score": 1,
          "created_utc": "2026-02-23 11:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xoaiw",
              "author": "Safe_Flounder_4690",
              "text": "That‚Äôs a valid point maintaining custom orchestration can become complex as systems scale and edge cases increase. Managed platforms can reduce operational overhead and speed up deployment.\n¬†\nThat said, custom pipelines still offer deeper control over retrieval logic, validation and integration with internal processes. The right balance often comes from standardizing core infrastructure while keeping flexibility where business-specific reasoning and accuracy matter most.",
              "score": 1,
              "created_utc": "2026-02-23 11:28:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y2094",
          "author": "Tired__Dev",
          "text": "It‚Äôs the actionable summaries among chunks that would have me worried tbh.",
          "score": 1,
          "created_utc": "2026-02-23 13:10:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y2dma",
          "author": "Semoho",
          "text": "Did you have benchmark o test dataset that how this approach effects the system?",
          "score": 1,
          "created_utc": "2026-02-23 13:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yc7ri",
          "author": "ChapterEquivalent188",
          "text": "how do you test ? do you trust your llm ?",
          "score": 1,
          "created_utc": "2026-02-23 14:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zeo3u",
          "author": "AmphibianNo9959",
          "text": "For personal use, I've been using Reseek to handle a lot of that ingestion and semantic search piece automatically. It pulls text from PDFs and images, tags everything, and makes my own notes and bookmarks searchable in a similar way. ",
          "score": 1,
          "created_utc": "2026-02-23 17:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zrox9",
              "author": "Material-River-2235",
              "text": "Ok, I will go take a look at it.",
              "score": 1,
              "created_utc": "2026-02-23 18:19:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zf2rh",
          "author": "nasnas2022",
          "text": "Can you add few more details on the pipeline",
          "score": 1,
          "created_utc": "2026-02-23 17:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zj2uk",
          "author": "_os2_",
          "text": "We have built something similar with [Skimle](https://skimle.com), but our tool skips RAG completely and instead builds the categorization scheme in the beginning with LLM calls and then retrieves from a structured table rather than at runtime. Enables two-way transparency and stable responses.\n\nWould be great to compare results!",
          "score": 1,
          "created_utc": "2026-02-23 17:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7125c1",
          "author": "Infamous_Ad5702",
          "text": "Well done. Do you build a knowledge graph?\nAnd why Vector?\nI had the same project for a client and went with a custom tool called Leonata. It builds an index and works totally offline. No LLM. No GPU. And No hallucination. \n\nFor my client purpose, semantic retrieval, Vector found similar info but not the best fit.",
          "score": 1,
          "created_utc": "2026-02-23 21:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9sxv7",
      "title": "Introducing Legal RAG Bench",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9sxv7/introducing_legal_rag_bench/",
      "author": "Neon0asis",
      "created_utc": "2026-02-20 11:30:41",
      "score": 16,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "# tl;dr\n\nWe‚Äôre releasing¬†[**Legal RAG Bench**](https://huggingface.co/datasets/isaacus/legal-rag-bench), a new reasoning-intensive benchmark and evaluation methodology for assessing the end-to-end, real-world performance of legal RAG systems.\n\nOur evaluation of state-of-the-art embedding and generative models on Legal RAG Bench reveals that information retrieval is the primary driver of legal RAG performance rather than reasoning. We find that the¬†[Kanon 2 Embedder](https://isaacus.com/blog/introducing-kanon-2-embedder)¬†legal embedding model, in particular, delivers an average accuracy boost of 17 points relative to Gemini 3.1 Pro, GPT-5.2, Text Embedding 3 Large, and Gemini Embedding 001.\n\nWe also infer based on a statistically robust hierarchical error analysis that most errors attributed to hallucinations in legal RAG systems are in fact triggered by retrieval failures.\n\nWe conclude that information retrieval sets the ceiling on the performance of modern legal RAG systems. While strong retrieval can compensate for weak reasoning, strong reasoning often cannot compensate for poor retrieval.\n\nIn the interests of transparency, we have openly released Legal RAG Bench on¬†[Hugging Face](https://huggingface.co/datasets/isaacus/legal-rag-bench), added it to the¬†[Massive Legal Embedding Benchmark (MLEB)](https://isaacus.com/mleb), and have further presented the results of all evaluated models in an interactive explorer shown towards the end of this blog post. We encourage researchers to both scrutinize our data and build upon our novel evaluation methodology, which leverages full factorial analysis to enable hierarchical decomposition of legal RAG errors into hallucinations, retrieval failures, and reasoning failures.\n\n**Source:** [**https://isaacus.com/blog/legal-rag-bench**](https://isaacus.com/blog/legal-rag-bench)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r9sxv7/introducing_legal_rag_bench/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6es8y4",
          "author": "Fetlocks_Glistening",
          "text": "So you're baaically saying retrieval is important? I mean, what exactly is new here?",
          "score": 3,
          "created_utc": "2026-02-20 12:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l98yf",
              "author": "Neon0asis",
              "text": "Well theres a lot new in terms of actually isolating the effects of the two stages of the RAG pipeline. The methodology uses a factorial experiment, where each embedder and LLM is paired together, allowing for the estimation of the causal effect of each component in terms of the outcome variables of correctness and groundedness. \n\nThis type of estimation is only statistically sound if you can establish interlinked ground-truths for both the retriever and LLM at the question level. So if a RAG pipeline gets an answer wrong, we can examine whether the retreiver failed to get the 'gold passage' (this would plausibly explain the failure at the pipeline level), and if it did get the gold passage, how come the LLM was unable to come up with the correct answer despite having  all of the relevant context to do so. In the latter case, we can infer that the LLM's reasoning abilities are at fault, allowing for a clear decomposition, at the pipeline level, between reasoning and retrieval errors.\n\nJust because an intuition exists as popular advice, does not mean it is supported by the science or evidence. There's value in conclusively proving something that seems obvious, particularly if you can better understand the causal pattern and mechanics, allowing you to draw more sophistacted inferences from that seemingly obvious assumption.",
              "score": 2,
              "created_utc": "2026-02-21 12:19:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8ulm8",
      "title": "I need a production grade RAG system",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r8ulm8/i_need_a_production_grade_rag_system/",
      "author": "Several_Job_2507",
      "created_utc": "2026-02-19 09:17:45",
      "score": 16,
      "num_comments": 22,
      "upvote_ratio": 0.84,
      "text": "Hey, I need to build a RAG system for Hindi-speaking folks in India. I'll be using both Hindi and English text. The main thing is, I need to make a production-ready RAG system for students to get the best info from it.\n\nI'm a software developer, but I'm new to RAG and AI. Any good starting points or packages I can use? I need something free for now; if it works out, we can look into paid options. I'm sure there are some open-source solutions out there. Let me know if you have any special insights\n\n\nThankyou.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r8ulm8/i_need_a_production_grade_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o67wjcq",
          "author": "One_Milk_7025",
          "text": "Start from scratch you will learn better..\nYou process pdf or any txt based file to markdown format and then chunk with proper metadata extraction> embed with multilingual embedding model > insert into vector database.. this is ingestion part.\nFor the retrieval you need to embed the query , optionally expand the query and then do similarity search in the vector db.. local lance db or a qdrant cloud or docker hosted gives nice sdk for this .. after similarity search you rank the chunk and send the top 5 to the llm as a context with the query..\nHere you can increase the accuracy by implementing bm25 , use hybrid retrieval..\nImplement rrf, mmr etc..\n\nThe trick is the tool calling and discovery strategy.. you can stuff 20chunk and still won't get proper result.. lazy discovery, temporal knowledge graph would help then for nuisance use case .\n\nSo rag is not a single thing it is different for each use cases.. making a generic rag would take time to cover all the things.. \nSee lightrag for the all in one pack..\nOr built youself with qdrant cloud(embedding +vector store) with custom ingestion pipeline..",
          "score": 5,
          "created_utc": "2026-02-19 10:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67s256",
          "author": "fabkosta",
          "text": "This is the tech stack you need, and it's entirely open source and free:\n\n1. **OCRing**: Docling\n2. **Vector database:** PostgreSQL or Elasticsearch\n3. **Backend framework:** LlamaIndex, Haystack, or some framework by Microsoft / Google (avoid Langchain or Langgraph, I'd say)\n4. **Frontend**: OpenChat (https://www.openchatui.org/) or LibreChat (https://www.librechat.ai/), there are some others\n5. **Hosting**: Up to you.",
          "score": 4,
          "created_utc": "2026-02-19 09:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1xz4",
              "author": "Massive-Mobile-5655",
              "text": "Why to avoid langchain and langraph? I did build one pipeline using these and can you tell me the resources through which i can learn in depth",
              "score": 1,
              "created_utc": "2026-02-21 05:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kj8hc",
                  "author": "fabkosta",
                  "text": "Langchain is over engineered. Langgraph you only need if you need a state machine, which is not the case for most RAG solutions.",
                  "score": 1,
                  "created_utc": "2026-02-21 08:10:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67t18r",
              "author": "Several_Job_2507",
              "text": "I'm mostly worried about indexing and getting the right answer.\nCan you share some of the best practices for indexing or resources so I can do it?",
              "score": 0,
              "created_utc": "2026-02-19 09:53:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6862r7",
                  "author": "HackHusky",
                  "text": "Going through the same stuff as you, man. I found a nice video that had some great tips:  \n[How to Build a Scalable RAG System for AI Apps (Full Architecture)](https://www.youtube.com/watch?v=4KiiKQ9RVvA)",
                  "score": 3,
                  "created_utc": "2026-02-19 11:48:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67twhy",
                  "author": "fabkosta",
                  "text": "You need to:\n\n1. OCR your documents and extract text\n2. Chunk your documents (can use Langchain for that, for example)\n3. Index them using an embedding LLM (you'll also need a chat completion LLM for the user queries)\n4. You have to think hard what to do with images and tables, but that hardly can be generalized in advise since it's usually domain and data-specific.\n\nOther than that, I don't know what you mean with \"worried about indexing\".\n\nGetting a first version up and running is the easy part. The hard part is making it production ready and systematically increasing quality. 80% effort will go into dealing with the messiness of data.\n\n(Disclaimer: I am providing trainings and consulting for this topic cause I know it's not trivial.)",
                  "score": 2,
                  "created_utc": "2026-02-19 10:01:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ds831",
                  "author": "aanghosh",
                  "text": "Indexing can be deterministic provided the query to the vector DB is deterministic. If they query is coming from an AI model then that query is non-deterministic so you have to plan for that non-determinism. Realistically that means accepting and handling hallucinations or non-sense as answers. If you need determinism, maybe you could try to use regular expressions on a static document or format that the LLM cites as a reference. That would provide deterministic queries for you.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:04:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67u3pq",
          "author": "ChapterEquivalent188",
          "text": "have a kickstart and add what you need https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit  if you want to have it with a mcp server use the clawrag .... enterprise level kann be seen here https://github.com/2dogsandanerd/RAG_enterprise_core\n\nanyway, focus on ingestion as garbage in will always let you down at retrieval ",
          "score": 2,
          "created_utc": "2026-02-19 10:03:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67t30m",
          "author": "yafitzdev",
          "text": "Use this as reference: \n[fitz-ai](https://github.com/yafitzdev/fitz-ai)",
          "score": 1,
          "created_utc": "2026-02-19 09:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6997dq",
          "author": "Individual_Yard846",
          "text": "I have a RAG API you can use to this really easily if you'd like, dm me",
          "score": 1,
          "created_utc": "2026-02-19 15:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ceuvq",
          "author": "johnny_5667",
          "text": "igu. 5 buck? ill do it 4 u",
          "score": 1,
          "created_utc": "2026-02-20 01:15:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dcts1",
          "author": "bigwad",
          "text": "typesense is a much simpler elasticstyle style search index with vector and now AI based search leveraging this baked in. Opensource and seems to work well for us as a single binary too.",
          "score": 1,
          "created_utc": "2026-02-20 04:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dqrqj",
          "author": "Educational_Cup9809",
          "text": "try out https://structhub.io. 15000 credits on signup",
          "score": 1,
          "created_utc": "2026-02-20 06:51:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e5oya",
          "author": "Eastern-Pipe-7593",
          "text": "check it out [https://github.com/bienwithcode/AdmissionAgent](https://github.com/bienwithcode/AdmissionAgent)  \nIt's use case for university admission ",
          "score": 1,
          "created_utc": "2026-02-20 09:10:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gty2o",
          "author": "Top-Seaworthiness285",
          "text": "Try it it's totally free https://docmind.vasanthubs.co.in/",
          "score": 1,
          "created_utc": "2026-02-20 18:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hf06w",
          "author": "Little-Ad-1526",
          "text": "I bukld something similiar (Indian urban planning documents) few weeks back, using visual rag.\nDm me",
          "score": 1,
          "created_utc": "2026-02-20 20:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i66x2",
          "author": "Nimrod5000",
          "text": "I used raganything and haystack with milvus standalone.  Works wonderfully",
          "score": 1,
          "created_utc": "2026-02-20 22:26:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67s3wy",
          "author": "MindlessFan9308",
          "text": "I can make a production ready RAG setup for you but ofcourse it comes with a cost and not for free.",
          "score": -3,
          "created_utc": "2026-02-19 09:44:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r99uaf",
      "title": "We built a hybrid retrieval system combining keyword + semantic + neural reranking ‚Äî here's what we learned",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r99uaf/we_built_a_hybrid_retrieval_system_combining/",
      "author": "True-Snow-1283",
      "created_utc": "2026-02-19 20:07:30",
      "score": 15,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "Hey r/RAG,\n\nI've been working on retrieval systems for a while now and wanted to share some insights from building Denser Retriever, an end-to-end retrieval platform.\n\n**The problem we kept hitting:**\n\nPure vector search misses exact matches (product IDs, error codes, names). Pure keyword search misses semantic meaning. Most RAG setups use one or the other, or bolt them together awkwardly.\n\n**Our approach ‚Äî triple-layer retrieval:**\n\n1. **Keyword search**¬†(Elasticsearch BM25) ‚Äî handles exact matches, filters, structured queries\n2. **Semantic search**¬†(dense vector embeddings) ‚Äî catches meaning even when wording differs\n3. **Neural reranking**¬†(cross-encoder) ‚Äî takes the combined candidates and re-scores them with full query-document attention\n\n**Key learnings:**\n\n* Chunk size matters more than embedding model choice. We use 2000-character chunks with 10% overlap (200 characters). This gives\n* For technical docs, keyword search still wins \\~30% of the time over pure semantic. Don't drop it.\n* Reranking top-50 candidates is the sweet spot between latency and accuracy for most use cases.\n* Document parsing quality is the silent killer. Garbage in = garbage out, no matter how good your retrieval is.\n\n**Architecture:**\n\nUpload docs ‚Üí Parse (PDF/DOCX/HTML ‚Üí Markdown) ‚Üí Chunk ‚Üí Embed ‚Üí Index into Elasticsearch (both BM25 and dense vector)\n\nAt query time: BM25 retrieval + vector retrieval ‚Üí merge ‚Üí neural rerank ‚Üí top-K results\n\nWe've open-sourced the core retriever logic and also have a hosted platform at [retriever.denser.ai](http://retriever.denser.ai) if you want to try it without setting up infrastructure.\n\nHappy to answer questions about the architecture or share more specific benchmarks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r99uaf/we_built_a_hybrid_retrieval_system_combining/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6bysa3",
          "author": "Infamous_Ad5702",
          "text": "Sounds like a solid approach. I did the same thing. Vector is weak finds similar.\nI couldn‚Äôt use the LLM for my client (bias, hallucination, cost)\n\nSo we went back to old school; deep semantics and deterministic techniques, pure maths.\n\nSo now we have a deep search tool that maps a knowledge graph for every new query it gets.\n\nIs context specific. Can‚Äôt hallucinate and needs zero GPU. We‚Äôre pumped.",
          "score": 4,
          "created_utc": "2026-02-19 23:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ixe1c",
              "author": "True-Snow-1283",
              "text": "Interesting: So now we have a deep search tool that maps a knowledge graph for every new query it gets. Is it fast?",
              "score": 2,
              "created_utc": "2026-02-21 01:00:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6jza0y",
                  "author": "Infamous_Ad5702",
                  "text": "I think it‚Äôs fast. I‚Äôm bias‚Ä¶would love to get opinions?",
                  "score": 1,
                  "created_utc": "2026-02-21 05:12:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6chaon",
          "author": "Academic_Track_2765",
          "text": "Its not new brother. This type of systems have been in production since 2020-2022. bi-encoders / cross-encoders / cosine search on embeddings + bm25 have been used since 2020, please build something new. Each rag post is the same old stuff, from 2020 to 2022.  Maybe try a DRF model or Gaussian embeddings, please something different / new from the same thing everyone does once they finally realize there is more to embeddings than just throwing them in a DB and wondering why retrieval is so poor. sbert people are ashamed. Also I think you are the same guy trying to sell his product from a while ago LOL. ",
          "score": 3,
          "created_utc": "2026-02-20 01:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ixuxb",
              "author": "True-Snow-1283",
              "text": "Fair point ‚Äî BM25 + bi-encoders + cross-encoders isn't new research. We're not claiming to invent the technique. The value is in making it production-ready and accessible as a managed platform so teams don't have to wire up Elasticsearch, embedding models, and reranking themselves.",
              "score": 1,
              "created_utc": "2026-02-21 01:03:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6ks30q",
              "author": "eurydice1727",
              "text": "lmaooo",
              "score": 1,
              "created_utc": "2026-02-21 09:39:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6nsb10",
              "author": "FeeMassive4003",
              "text": "The guy shared his lesson learned from his interesting work. He didn't claim it is novel. I find this post useful.",
              "score": 1,
              "created_utc": "2026-02-21 20:37:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ntfbd",
                  "author": "Academic_Track_2765",
                  "text": "Good! Glad you enjoyed it.",
                  "score": 2,
                  "created_utc": "2026-02-21 20:43:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6cs03t",
          "author": "datguywelbzy",
          "text": "Why not qmd on GitHub ?",
          "score": 2,
          "created_utc": "2026-02-20 02:36:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6iymit",
              "author": "True-Snow-1283",
              "text": "We open-sourced the core retriever library on GitHub a while ago. The hosted platform adds managed infrastructure on top ‚Äî document parsing, indexing, API ‚Äî for teams that don't want to self-host.\n\nWe also recently built a Claude Code skill that lets you create a knowledge base from PDFs and query it in natural language, all from the terminal: [https://retriever.denser.ai/blog/build-rag-knowledge-base-claude-code](https://retriever.denser.ai/blog/build-rag-knowledge-base-claude-code)",
              "score": 1,
              "created_utc": "2026-02-21 01:07:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rclvtn",
      "title": "first RAG project, really not sure about my stack and settings",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "author": "Kas_aLi",
      "created_utc": "2026-02-23 16:21:16",
      "score": 15,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "Hey guys, so ive been working on my first RAG project. its basically a system that takes medical PDFs (textbooks, clinical guidelines) and builds a knowledge graph from them to generate multi-choice exam questions for a medical exam. \n\nInput style: large textbooks, pdf, images, tables, etc\n\nI have been coding this like a monkey with claude opus 4.6 and codex 5.3 honestly, just prompting my way through it. it works but i have no idea if what im doing is the right approach.\n\nWould love some feedback, good sources or learning resources.  \n  \nHere is my current stack for context:\n\n    PDF ‚Üí Docling (no OCR, native text) ‚Üí markdown export with page breaks\n        ‚Üí heading-based chunker (~768 tok, tiktoken cl100k)\n          ‚Üí noise classifier (regex heuristics, filters TOC/references/headers)\n          ‚Üí batch extraction (3 chunks/batch, 4K token cap, 4 parallel workers)\n            ‚Üí Instructor (JSON mode) + Gemini 2.5 Flash via OpenRouter (it is cheap, but probably there are better now)\n            ‚Üí Pydantic schema: concepts (18 types) + claims (25 predicates) + evidence spans\n            ‚Üí fallback: batch fail ‚Üí individual chunk extraction\n          ‚Üí concept normalization + dedup\n          ‚Üí quality gate (error rate, claims/chunk, evidence/claim, noise ratio, page coverage)\n          ‚Üí embeddings: Qwen3-embedding-8b (1024d) ‚Üí pgvector\n    \n    storage: supabase (27 tables)\n    orchestration: langgraph (for downstream question generation, not ETL)\n    \n    all LLM calls through openrouter",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6z75uc",
          "author": "Higgs_AI",
          "text": "Can I just ask you if this is for taking exams? If so‚Ä¶ DM me. Overall it‚Äôs well put together‚Ä¶ the bones are solid. The fact that it has a quality gate at all puts it ahead of most implementations. ü§∑üèΩ‚Äç‚ôÇÔ∏è\n\nHad to edit: this is really clean work‚Ä¶the schema enforced extraction with Instructor, the quality gate with multiple metrics, the batch fallback logic. Most people building extraction pipelines skip half of what you‚Äôve done here‚Ä¶ bravo brotha. \n\nyour question generation is downstream and separate. What if the extraction and the pedagogy were part of the same adaptive loop where how the learner performs on generated questions feeds back into which concepts need deeper extraction, which claims need more evidence, which connections need to be surfaced?\n\nThere‚Äôs other stuff I‚Äôd say but, I just thought I‚Äôd give you some substance without flooding your shit. Good work",
          "score": 3,
          "created_utc": "2026-02-23 16:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z8hqn",
              "author": "Kas_aLi",
              "text": "Main usecase will be generating new and original exam questions but it should be good at taking the exam as well I guess... I think GPT 5.2 had more than 98% correct answers in this particular exam already so...",
              "score": 1,
              "created_utc": "2026-02-23 16:49:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zp35o",
          "author": "Semoho",
          "text": "Nice orchestration. \n\nYou can have a little academic approach to assess your system. In my opinion, you can create a test dataset from your exam or even from your knowledge of the docs (corpus). Then, run testing on GPT 5.2 and get the answers and also with your system. Now you have benchmark from gpt or other models and your system, so you can check if the results are good or not. We call it the test phase or making a test dataset. This gives you the power to assess your system. (Usually, you can see the baselines on the academic papers that they need to improve the baseline to compete with other approaches.)\n\nAlso, I would recommend using Knowledge Graph if you have relational data, such as something belongs to one paper/book, and there is some other evidence on the other resources.\n\n  \nSome tips and tricks to increase the retrieval phase:\n\n1. Use task-specific embedding models such as [https://huggingface.co/abhinand/MedEmbed-base-v0.1](https://huggingface.co/abhinand/MedEmbed-base-v0.1)\n\n2. Use other vector DBs like Milvus. Vector DB is so easy to set up, but it does not provide good accuracy.\n\n3. Check the knowledge graph.",
          "score": 3,
          "created_utc": "2026-02-23 18:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77spi7",
              "author": "rigatoni-man",
              "text": "I've been building something to test models without a lot of overhead and legwork.  Basically upload your golden dataset and test it against every model out there.\n\nShoot me a message u/Kas_aLi  and I'd love to help you find the best model for free to test what i'm building ( [https://checkstack.ai](https://checkstack.ai) )",
              "score": 1,
              "created_utc": "2026-02-24 21:55:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77toou",
                  "author": "Semoho",
                  "text": "It is a good platform. But as an IR guy to publish different papers in the information retrieval field, I think I do not need it :))  \nHowever, if I find time, I would check your platform",
                  "score": 1,
                  "created_utc": "2026-02-24 22:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zcxj3",
          "author": "shlok-codes",
          "text": "I use DeepSeek nitro chat via openrouter look into DeepSeek and qwen models",
          "score": 2,
          "created_utc": "2026-02-23 17:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ivc1",
          "author": "avebrahimi",
          "text": "Awesome stack.  \nWhat about UI?",
          "score": 1,
          "created_utc": "2026-02-24 07:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tl37",
          "author": "LuckEcstatic9842",
          "text": "You might also want to take a look at LightRAG.",
          "score": 1,
          "created_utc": "2026-02-24 08:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1fde",
          "author": "aidenclarke_12",
          "text": "the heading-based chunker at 768 tokens works for linear textbook prose but clinical tables and cross-references tend to fragment badly across batch boundaries.. the 3 chunks per batch with 4k cap means a pharmacology table that spans two chunks might lose the column headers on one side and the values on the other. \n\nthe quality gate catching this after the fact is better than nothing but fixing it upstream in the chunking logic is probably worth exploring.. \n\non the model side you mentioned openrouter and questioned whether there are better options now.. runpod, together or deepinfra give direct access to the same gemini flash and qwen3 models at lower per-token cost which matters if you're running high extraction volumes on large textbooks",
          "score": 1,
          "created_utc": "2026-02-25 11:00:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcehj1",
      "title": "Built an offline MCP server that stops LLM context bloat using local vector search over a locally indexed codebase.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcehj1/built_an_offline_mcp_server_that_stops_llm/",
      "author": "Trust_Me_Bro_4sure",
      "created_utc": "2026-02-23 11:00:50",
      "score": 13,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Searching through a massive codebase to find the right context for AI assistants like Claude was becoming a huge bottleneck for me‚Äîhurting performance, cost, and accuracy. You can't just dump entire files into the prompt; it instantly blows up the token limit, and the LLM loses track of the actual task.\n\n\n\nInstead of LLM manually hunting for correct files using grep/find &  dumping raw file content into the prompt, I wanted the LLM to have a better search tool.\n\nSo, I built code-memory: an open-source, offline MCP server you can plug right into your IDE (Cursor/AntiGravity) or Claude Code.\n\n\n\nHere is how it works under the hood:\n\n1. Local Semantic Search: It runs vector searches against your locally indexed codebase using jinaai/jina-code-embeddings-0.5b model.¬†\n\n2. Smart Delta Indexing: Backed by SQLite, it checks file modification times during indexing. Unchanged files are skipped, meaning it only re-indexes what you've actually modified.¬†\n\n3. 100% Offline: Your code never leaves your machine.\n\n\n\nIt is heavily inspired by claude-context, but designed from the ground up for large-scale, efficient local semantic search. It's still in the early stages, but I am already seeing noticeable token savings on my personal setup!\n\n\n\nI'd love to hear feedback, especially if you have more ideas!\n\nCheck out the repo here: [https://github.com/kapillamba4/code-memory](https://github.com/kapillamba4/code-memory)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcehj1/built_an_offline_mcp_server_that_stops_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xlz2v",
          "author": "picturpoet",
          "text": "will it handle something like when the dev says look into ‚Äúauth‚Äù and it knows it has to look into authentication files?",
          "score": 1,
          "created_utc": "2026-02-23 11:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmkf0",
              "author": "Trust_Me_Bro_4sure",
              "text": "Yes, This is from one of my personal projects:\n\nhttps://preview.redd.it/wiwlcj5i98lg1.png?width=1178&format=png&auto=webp&s=1bf55ebce664e0007746dfded7ad5d2f9a5edbe1\n\n",
              "score": 1,
              "created_utc": "2026-02-23 11:13:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72nuf3",
          "author": "Oshden",
          "text": "This is awesome! Thanks for sharing it OP!",
          "score": 1,
          "created_utc": "2026-02-24 03:21:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9updr",
      "title": "Structure-first RAG with metadata enrichment (stop chunking PDFs into text blocks)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9updr/structurefirst_rag_with_metadata_enrichment_stop/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-20 12:59:52",
      "score": 13,
      "num_comments": 11,
      "upvote_ratio": 0.84,
      "text": "I think most people are still chunking PDFs into flat text and hoping semantic search works. This breaks completely on structured documents like research papers.\n\nTraditional approach extracts PDFs into text strings (tables become garbled, figures disappear), then chunks into 512-token blocks with arbitrary boundaries. Ask \"What methodology did the authors use?\" and you get three disconnected paragraphs from different sections or papers.\n\nThe problem is research papers aren't random text. They're hierarchically organized (Abstract, Introduction, Methodology, Results, Discussion). Each section answers different question types. Destroying this structure makes precise retrieval impossible.\n\nI've been using structure-first extraction where documents get converted to JSON objects (sections, tables, figures) enriched with metadata like section names, content types, and semantic tags. The JSON gets flattened to natural language only for embedding while metadata stays available for filtering.\n\nThe workflow uses Kudra for extraction (OCR ‚Üí vision-based table extraction ‚Üí VLM generates summaries and semantic tags). Then LangChain agents with tools that leverage the metadata. When someone asks about datasets, the agent filters by content\\_type=\"table\" and semantic\\_tags=\"datasets\" before running vector search.\n\nThis enables multi-hop reasoning, precise citations (\"Table 2 from Methods section\" instead of \"Chunk 47\"), and intelligent routing based on query intent. For structured documents where hierarchy matters, metadata enrichment during extraction seems like the right primitive.\n\nAnyway thought I should share since most people are still doing naive chunking by default.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r9updr/structurefirst_rag_with_metadata_enrichment_stop/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6hc3mj",
          "author": "Icy_Eye3812",
          "text": "Thanks buddy for sharing",
          "score": 1,
          "created_utc": "2026-02-20 19:55:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i6my9",
          "author": "Irisi11111",
          "text": "Try olmOCR: it's inexpensive and extracts images via pipelines. I output text to Markdown files by page, with each page's images in a separate folder, also titled by page. The precision is excellent.",
          "score": 1,
          "created_utc": "2026-02-20 22:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wz5mq",
              "author": "framvaren",
              "text": "this looks really good. Only thing I'm missing is that the page should also be broken into elements, e.g. header and the paragraph and linking them together (e.g. header has child-elements in the paragraphs that follow it - that would enable better traceability across pages) so you can chunk by section/header when using in RAG",
              "score": 1,
              "created_utc": "2026-02-23 07:27:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6wzn08",
                  "author": "Irisi11111",
                  "text": "Very good suggestions!",
                  "score": 1,
                  "created_utc": "2026-02-23 07:32:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6jvkww",
          "author": "New_Animator_7710",
          "text": "I find the separation between representation and filtering particularly interesting.\n\nFlattening to natural language *only* for embedding, while preserving structured metadata for constraint-based filtering, avoids entangling structural signals in dense vectors. Dense models are notoriously bad at reliably encoding hierarchy.",
          "score": 1,
          "created_utc": "2026-02-21 04:44:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lhkgc",
          "author": "Academic_Track_2765",
          "text": "no, let them continue, and then you end up with gold like this.   \n[https://www.reddit.com/r/Rag/comments/1r80vsg/comment/o6lcgae/?context=3](https://www.reddit.com/r/Rag/comments/1r80vsg/comment/o6lcgae/?context=3)",
          "score": 1,
          "created_utc": "2026-02-21 13:21:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wyica",
          "author": "framvaren",
          "text": "Can you share your code?   \nI've been looking into using [unstructured.io](http://unstructured.io) because it partitions a pdf into elements (heading, paragraph, formula, table, etc.) and links them together by adding metadata to each element. But I would like to have an alternative solution that gives me more freedom to determine my own element structure and I don't need all the features of unstructured.",
          "score": 1,
          "created_utc": "2026-02-23 07:21:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra6ifo",
      "title": "Best approach for querying large structured tables with RAG?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1ra6ifo/best_approach_for_querying_large_structured/",
      "author": "According-Lie8119",
      "created_utc": "2026-02-20 20:27:45",
      "score": 13,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôm working on a RAG system that performs very well on unstructured PDFs. Now I‚Äôm facing a different challenge: extracting information from a large structured table.\n\nThe table has:\n\n* \\~200 products (columns)\n* multiple product features (rows)\n* \\~20,000+ cells total\n\nUsers ask questions like:\n\n* ‚ÄúFind products suitable for young people‚Äù\n* ‚ÄúFind products with no minimum order quantity‚Äù\n* ‚ÄúFind products for seniors with good coverage‚Äù\n\nMy current approach:\n\n* Each cell is a chunk\n* Metadata includes¬†`{product_name, feature_name}`\n* Worst case, the Q&A model receives \\~150 small chunks\n* It works reasonably well because the chunks are tiny\n\nHowever, I‚Äôm not sure this is the best long-term solution.\n\nHas anyone dealt with large structured tables in a RAG setup?  \nDid you stay embedding-based, move to SQL + LLM parsing, hybrid approaches, or something else?\n\nWould really appreciate insights or architecture recommendations.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1ra6ifo/best_approach_for_querying_large_structured/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6ic216",
          "author": "fireflux_",
          "text": "Is the data in a structured store like SQL or do you have to parse/extract it from PDFs?\n\nBased on the query samples you shared, the access pattern is less \"semantic\" and more \"tabular\". I'd pre-process data to land in a structured SQL table, then have the LLM query SQL. From there, as part of your retrieval process, the agent converts the user's query to SQL + filters, uses the returned rows to generate a response (that's kind of what you're doing already; your 'cell chunk' is essentially a SQL row/col).\n\nSQL unlocks slicing/dicing of data like filtering, ordering, etc. rather than chunking and using python to wrangle the data\n\nAdditionally, there's a lot of cool blog posts about this, particularly amongst AI startups. Here's one I really like \\[1\\], that goes into detail on RAG for finance (lots of tables). Enjoy!\n\n\\[1\\] [https://www.nicolasbustamante.com/p/lessons-from-building-ai-agents-for?hide\\_intro\\_popup=true](https://www.nicolasbustamante.com/p/lessons-from-building-ai-agents-for?hide_intro_popup=true)",
          "score": 2,
          "created_utc": "2026-02-20 22:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nrwc7",
              "author": "According-Lie8119",
              "text": "Ok, thank you, I will take a closer look at it. So far, I have only worked with completion models and not with an agentic approach. I tried transforming the user message directly into an SQL query, but unfortunately the results are not always deterministic. On the other hand, I am still unsure about the agentic approach because the performance is not optimal. In normal chat scenarios, my system can generate responses very quickly (first token within about two seconds). But thanks. I will give it another try.",
              "score": 1,
              "created_utc": "2026-02-21 20:34:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7azba8",
          "author": "vlg34",
          "text": "For a table this size the cell-per-chunk approach works surprisingly well as you've seen, but it breaks down if questions require comparing values across many products at once - the model ends up with too many fragments.\n\nA common alternative is to convert the table to SQL or a structured datastore and use an LLM to generate queries. ",
          "score": 2,
          "created_utc": "2026-02-25 10:42:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hmysy",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-20 20:49:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hn3jt",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 10 days on [**2026-03-02 20:49:23 UTC**](http://www.wolframalpha.com/input/?i=2026-03-02%2020:49:23%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Rag/comments/1ra6ifo/best_approach_for_querying_large_structured/o6hmysy/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FRag%2Fcomments%2F1ra6ifo%2Fbest_approach_for_querying_large_structured%2Fo6hmysy%2F%5D%0A%0ARemindMe%21%202026-03-02%2020%3A49%3A23%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201ra6ifo)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-20 20:50:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hn6nt",
          "author": "Crisederire",
          "text": "RemindMe!¬†10 days",
          "score": 1,
          "created_utc": "2026-02-20 20:50:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i2pjj",
          "author": "blue-or-brown-keys",
          "text": "I have done this for a customer recently. I send the schema to an LLM and make it generate a plan I can excute locally on a pandas data frame.",
          "score": 1,
          "created_utc": "2026-02-20 22:07:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6j6ors",
              "author": "Routine_Paramedic_82",
              "text": "Are you sending the schema context for all tables to the LLM?",
              "score": 1,
              "created_utc": "2026-02-21 01:58:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6juac1",
          "author": "baba_niv",
          "text": "Right now how are you identifying tables from the pdfs? I mean which py library are you using and how is it performing?\nAlso, how are you spliting the cells as chunks? \nWhy not send the entire table as a json?",
          "score": 1,
          "created_utc": "2026-02-21 04:35:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lvkt2",
              "author": "According-Lie8119",
              "text": "you can now extract tables from PDFs quite reliably using Docling as well as PyMuPDF and export them to Markdown. That part is no longer the bottleneck. The real challenge starts afterward: how should the data be chunked properly? And which retrieval strategy is most effective: standard similarity search and BM25, or something more advanced?",
              "score": 1,
              "created_utc": "2026-02-21 14:47:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6jv8x0",
          "author": "New_Animator_7710",
          "text": "150 chunks per query is fine now ‚Äî but it won‚Äôt scale.\n\nImagine:\n\n* 5,000 products\n* 100 features\n* 500,000 cells\n\nCell-level chunking becomes:\n\n* Expensive\n* Slow\n* Hard to maintain\n\nInstead:\n\n* Pre-compute product embeddings\n* Add sparse keyword index (BM25)\n* Hybrid re-ranking\n\nScalability should drive your architecture choice.",
          "score": 1,
          "created_utc": "2026-02-21 04:42:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6urkyh",
          "author": "japherwocky",
          "text": "I'm surprised nobody has mentioned this, I know I'm a bit late to the game.  If the agent has the schema in it's context window, LLMs are great at writing ad hoc queries.  Set it up as a sort of tool to make sure it's read only, but mostly just let them query in raw SQL.",
          "score": 1,
          "created_utc": "2026-02-22 22:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zdpwv",
          "author": "remoteinspace",
          "text": "at papr we use a graph + vector combo. We use the graph for structured data and at query time we built a predictive memory engine to query for questions like the above. DM me and i can share more. The project is open source",
          "score": 1,
          "created_utc": "2026-02-23 17:14:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7c3rzi",
          "author": "Dense_Gate_5193",
          "text": "i use nornicDB https://github.com/orneryd/NornicDB\n\ni figured out how to speed up HNSW construction on large corpus by 3x. with compressed ann mode you can also fit about 3x as many embeddings in the hnsw index in the same memory space with a latency and slight quality tradeoff. this scales to billions of embeddings",
          "score": 1,
          "created_utc": "2026-02-25 15:00:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9w8u0",
      "title": "Why Standard RAG Often Hallucinates Laws ‚Äî and How I Built a Legal Engine That Never Does (Tested in Italian Legal Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9w8u0/why_standard_rag_often_hallucinates_laws_and_how/",
      "author": "Pretend-Promotion-78",
      "created_utc": "2026-02-20 14:05:47",
      "score": 12,
      "num_comments": 6,
      "upvote_ratio": 0.81,
      "text": "Hi everyone,\n\nHave you ever had that *false confidence* when an LLM answers a technical question ‚Äî only to later realize it confidently cited something incorrect? In legal domains, that confidence is the *number one danger*.\n\nWhile experimenting with a standard RAG setup, the system confidently quoted a statute that seemed plausible‚Ä¶ until we realized that provision was **repealed in 2013**. The issue wasn‚Äôt just old training data ‚Äî it was that the system relied on *frozen knowledge* or poorly verified external sources.\n\nThis was something I had seen mentioned multiple times in other posts where people shared examples of legal documents with entirely fabricated statutes. That motivated me ‚Äî as an Italian developer ‚Äî to solve this problem in the context of **Italian law, where the code is notoriously messy and updates are frequent**.\n\nTo address this structural failure, I built **Juris AI**.\n\n# The Problem with Frozen Knowledge\n\nMost RAG systems are static: you ingest documents once and *hope* they stay valid. That rarely works for legal systems, where legislation evolves constantly.\n\nJuris AI tackles this with two key principles:\n\n**Dynamic Synchronization**  \nEvery time the system starts, it performs an incremental alignment of its sources to ensure the knowledge base reflects the *current state of the law*, not a stale snapshot.\n\n**Data Honesty**  \nIf a norm is repealed or lacks verified text, the system does not guess. It *reports the boundary of verification* instead of hallucinating something plausible but wrong.\n\n# Under the Hood\n\nFor those interested in the architecture but not a research paper:\n\n**Hybrid Graph-RAG**  \nWe represent the legal corpus as a *dependency graph*. Think of this as a connected system where each article knows the law it belongs to and its references.\n\n**Deterministic Orchestration Layer**  \nA proprietary logic layer ensures generation *follows validated graph paths*.  \nFor example, if the graph marks an article as ‚Äúrepealed,‚Äù the system is *blocked from paraphrasing* outdated text and instead reports the current status.\n\n# Results (Benchmark Highlights)\n\nIn stress tests against traditional RAG models:\n\n* **Zero hallucinations on norm validation** ‚Äî e.g., on articles with suffixes like *Art. 155-quinquies*, where standard models often cite repealed content, Juris AI always identified the correct current status.\n* **Cross-Database Precision** ‚Äî in complex scenarios such as linking aggravated theft (Criminal Code *Art. 625*) to civil liability norms (Civil Code *Art. 2043+*), Juris AI reconstructed the entire chain with literal text, while other systems fell back to general paraphrase.\n\n# Why I‚Äôm Sharing This Here\n\nThis is *not* a product pitch. It‚Äôs a technical exploration and I‚Äôm curious:\n\n**From your experience with RAG systems, in which scenarios does a deterministic validation approach become** ***essential*** **versus relying on traditional semantic retrieval alone?**",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r9w8u0/why_standard_rag_often_hallucinates_laws_and_how/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6fo9s8",
          "author": "ChapterEquivalent188",
          "text": "mamma mia. italian law is realy a messy kind of ;) its a sort of endboss. respect!\n\nHow do you handle extraction quality from source documents? standard ocr/layout fails on complex legal\n\nWhat's your validation rate for edge cases? The repealed-content blocking is critical -- we solve it via graph metadata and citation enforcer that validates every claim against source chunks. Sounds like similar philosophy, different implementation",
          "score": 3,
          "created_utc": "2026-02-20 15:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qkwgd",
          "author": "FeeMassive4003",
          "text": "How do you know if update X is applicable to law Y?",
          "score": 2,
          "created_utc": "2026-02-22 07:46:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rkcd5",
              "author": "Pretend-Promotion-78",
              "text": "Great technical question. The key is that we don't treat laws as mere strings of text; we treat them as relational entities within a Knowledge Graph based on the IFLA LRMoo standard.\nHere is how we handle the 'Update X to Law Y' applicability:\nDynamic Synchronization: Every time the app starts, it performs an incremental alignment of sources to detect new amendments or 'novellas'.\nWork/Expression Mapping: We use the graph to distinguish between the 'Work' (the law itself) and its various 'Expressions' (the specific versions of articles over time).\nDeterministic Validation: Our ingestion pipeline extracts validity metadata and explicit regulatory cross-references. If Update X modifies Law Y, our Deterministic Orchestrator validates this link at the graph level, effectively 'flagging' the outdated version.\nEssentially, we don't let the LLM 'guess' applicability through semantic probability. The graph enforces the structural truth before the query is even processed, ensuring the AI only sees what is legally in force at that exact moment.",
              "score": 3,
              "created_utc": "2026-02-22 13:07:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvc99",
          "author": "Top-Seaworthiness285",
          "text": "Try it out here: https://docmind.vasanthubs.co.in/\nIt‚Äôs completely free ‚Äî no login required.",
          "score": 1,
          "created_utc": "2026-02-20 18:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hbfb9",
              "author": "Pretend-Promotion-78",
              "text": "Nice effort, but I think there's a fundamental misunderstanding here. Juris AI isn't just another 'Chat with PDF' tool for generic documents. It‚Äôs a specialized Legal Intelligence engine built specifically to tackle the structural mess of Italian Law.\n\nThe main issue with generic PDF-to-chat tools is that they rely entirely on the file's content and the LLM's 'creativity'‚Äîwhich is a massive liability in the legal domain. We built a **Hybrid Graph-RAG architecture (KuzuDB + LanceDB)** that enforces a deterministic gatekeeper logic. If a statute in the document has been repealed or modified, my system cross-references it with the graph's metadata and kills the hallucination before it even reaches the user. A standard PDF chat tool would simply parrot back whatever is in the file, even if it's legally dead.\n\nGreat for casual use, but professional legal-tech requires structured logical constraints and real-time validity checks, not just basic semantic search.",
              "score": 2,
              "created_utc": "2026-02-20 19:52:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdeibi",
      "title": "Fresh grad learning RAG, feeling lost, looking for guidance",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "author": "savinox23",
      "created_utc": "2026-02-24 12:07:10",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hello, I am a fresh grad trying to learn about RAG and develop my coding skills. I made this simple cooking assistant based on Moroccan recipes. Could you please tell me how I can improve my stack/architecture knowledge and my code?\n\nWhat I currently do is discuss best practices with ChatGPT, try to code it myself using documentation, then have it review my code. But I feel like I'm trying to learn blindly. It's been 6 days and I've only made this sloppy RAG, and I feel like there is a better way to do this.\n\nHere‚Äôs the link to a throwaway repo with my code (original repo has my full name haha):\n\n  \n[https://github.com/Savinoy/Moroccan-cooking-assistant](https://github.com/Savinoy/Moroccan-cooking-assistant?utm_source=chatgpt.com)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o74uxot",
          "author": "RobertLigthart",
          "text": "6 days and you already have a working RAG is not sloppy thats actually decent progress. most people spend weeks just trying to get embeddings to work\n\n  \nthe chatgpt code review loop is fine for learning syntax but for architecture you need to read how other people built theirs. check out the rag-from-scratch series by lance martin on youtube... its hands down the best resource for understanding why the pieces fit together not just how to copy paste them\n\n  \nbiggest thing I'd improve early on: add chunking strategy to your pipeline if you havent already. most beginners just dump full documents into the vector store and wonder why retrieval is bad. experiment with chunk sizes and overlap... makes a massive difference",
          "score": 3,
          "created_utc": "2026-02-24 13:44:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74yjcp",
              "author": "savinox23",
              "text": "Thank you very much, i appreciate it !! I‚Äôll check out the series. As for the chunking strategy, I experimented with a standard text splitter but it was giving me mixed/ incomplete recipe answers, so i made each separate recipe as a chunk and the results were better, but i will check out the resources you mentioned, I‚Äôm sure there are better methods i can try!",
              "score": 1,
              "created_utc": "2026-02-24 14:04:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o78hh8u",
          "author": "Asleep_Carpet_3403",
          "text": "Great to see someone trying to learn through manual coding and 6 days to a working Rag is absolutely impressive. One suggestion is to start using tools like cursor with their auto complete functionality. It'll 10x your coding speed while you still continue to learn and write the complete code yourself. \n\nA good skill to have now is to learn inference (deploying and using ML/DL models) for this you may consider including a data extraction model from scanned pages (images) upstream of your RAG pipeline",
          "score": 2,
          "created_utc": "2026-02-25 00:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78jocr",
              "author": "savinox23",
              "text": "Okay, thank you very much for the advice !!",
              "score": 1,
              "created_utc": "2026-02-25 00:16:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r898lz",
      "title": "Need Advice on RAG App in .net",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r898lz/need_advice_on_rag_app_in_net/",
      "author": "BalanceThen8642",
      "created_utc": "2026-02-18 17:24:35",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.88,
      "text": "I am working on a internal RAG App for my company, where the knowledge base will be comprising of several apps, each with its documentation link sources, Databases and JSON documents. I need your guys advice on the following architecture to ensure my RAG is not only just working but the answers are actually good.\n\nHere's my architecture flow:\n\n1. User will ask a question about some app, then have a router which uses keyword matching initially, falling back to LLM-based routing to determine which data source is the best one.\n2. Then do I need a query transformation like multi-querying and step back to get a better phrased question. Is this step necessary or its just a overhead?\n3. Then using [Qdrant](https://qdrant.tech/documentation/) for vector database which will embed the documents/databases/links on start-up(currently basically creates a snapshot of the data on initial app start-up) and basically do semantic search  using¬†`sentence-transformers/all-MiniLM-L6-v2`¬†embeddings.\n4. Cross-encoder model to score and filter documents from retrieved documents, based on query-document relevance, reducing hallucinations by excluding low-confidence results , taking the top 8 docs since creating a local version of my age old pc.\n5. Answer Generation\n6. On start-up, the application exports database tables and Confluence pages to JSON documents, which are then chunked into 512 even sized chunks with text overlapping and embedded into Qdrant alongside static JSON documents. Is this static method of snapshotting the DB or better creating a pipeline which will repeat after some days\n\n6)Here are my model choices are they good for my self hosted application\n\n* LLM:¬†`llama-3.3-70b-versatile`\n* Embedding:¬†`sentence-transformers/all-MiniLM-L6-v2`\n* Reranker:¬†`cross-encoder/ms-marco-TinyBERT-L-6-v2` If not, what alternatives would you suggest?\n\nAny suggestions or things i can improve upon would be appreciated!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r898lz/need_advice_on_rag_app_in_net/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o63jfg2",
          "author": "pl201",
          "text": "I feel you don‚Äôt have a good understanding on how a RAG system works. I may be wrong, so don‚Äôt be upset on my assessment.\nTake the example of your #1, user asks a question about the doc and your RAG used the keyword to match. Match what? A link to the doc to your app or the link to the content of related doc? You don‚Äôt mention how do you build the system. That‚Äôs the most important part. After the initial key word match, you are going to send to LLM to determine which one is the best answer, but how a LLM knows? LLM has a general knowledge on about everything but don‚Äôt have any knowledge on your document, that‚Äôs reason you want to have RAG in the first place.",
          "score": 2,
          "created_utc": "2026-02-18 18:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66u59t",
              "author": "BalanceThen8642",
              "text": "So the reason behind routing here is that there are fixed number of apps so if a app name appears in query can use keyword matching to get that particular apps knowledge base (json+db+link). \n\nThen sometimes the user might not specify the exact keyword so need a fallback to direct to the correct app data instead of searching through all the app data. Is there any other way then LLM Routing or would Embedding based routing be better here?",
              "score": 1,
              "created_utc": "2026-02-19 04:48:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o638g3m",
          "author": "BalanceThen8642",
          "text": "Not really that much experienced in RAG so there might be some dumbness in architecture : (",
          "score": 1,
          "created_utc": "2026-02-18 17:36:35",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o63f77l",
          "author": "ChapterEquivalent188",
          "text": "The architecture is solid, but the snapshot approach will annoy you in the long run. Invest in the incremental pipeline--- some things i have in mind:\n\nQwen2.5-72B - Mixtral 8x22B for better tool calling only if wnated, testing is key  --minor\nsemantic chunking -\nbm25/key -\nlater --> caching and metadata filter\n\n\nthank me later for this: get ongoing (if things change) golden sets for testing while implementing and never ever use the happy path in business related RAG systems ;)",
          "score": 1,
          "created_utc": "2026-02-18 18:06:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66tohi",
              "author": "BalanceThen8642",
              "text": "Thanks this is very helpful. My current implementation already moved from startup snapshot to a scheduled ingestion service. Not sure about qwen/mistral haven't really tried them yet are they better then llama?",
              "score": 1,
              "created_utc": "2026-02-19 04:44:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67ja45",
                  "author": "ChapterEquivalent188",
                  "text": "its more like i was testing around a bit with models during the weekend and my contextwindow was a bit dirty ;) Its realy something minor and more squeezing a bit more---- the real big steps are in the semnantic chunking for example depending on the sort of data you want to ingest --- my next step would be to focus completely on ingestion and soving the \"garbage in\" problem.... all the tech and llm will not perform as long as they have to handel wordsoup and garbage....if you need some golden sets to test your ingest and rag let me know..... happy path will never make you happy ;) \n\n\nits a complete new world as soon as you know you can trust the answers of your llm ",
                  "score": 1,
                  "created_utc": "2026-02-19 08:16:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o63ftcg",
          "author": "z0han4eg",
          "text": "If your target language is English BGE Reranker works good",
          "score": 1,
          "created_utc": "2026-02-18 18:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6krt1n",
          "author": "ampancha",
          "text": "Your retrieval pipeline is solid, but the gap I'd flag is what happens when the router sends a user to the wrong source, or a prompt injection in the query tricks it into leaking docs from an app they shouldn't access. Multi-source RAG needs per-source access controls and input validation before it's safe for internal users. The static snapshotting question is secondary to whether you have observability on retrieval failures and data freshness. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-21 09:36:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra4r0n",
      "title": "Here‚Äôs how I got to ~76% on FinanceBench and why I think it could be pushed 80‚Äì84%. reachable)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1ra4r0n/heres_how_i_got_to_76_on_financebench_and_why_i/",
      "author": "Aquib8871",
      "created_utc": "2026-02-20 19:20:54",
      "score": 10,
      "num_comments": 2,
      "upvote_ratio": 0.82,
      "text": "I‚Äôve been working on this problem for a while and noticed there aren‚Äôt many post explaining how to solve this FinanceBench (the two post about financebench that are on this subreddit are selling products and didnt talk about the pipeline), especially ones that explain the pipeline decisions. So I thought I‚Äôd share what worked, what didn‚Äôt, and where the remaining gains likely are.\n\nEvaluation & Known Limits\n\nEvaluation was conducted on the 150 public FinanceBench questions.\n\n# Overall outcome\n\n* \\~76% (114 questions) answered correctly\n* 36 failures total\n\n# Failure breakdown\n\n* **24 / 36 failures** were caused by incorrect evidence retrieval\n* **12 / 36 failures** occurred despite correct evidence being retrieved (reasoning / interpretation errors)\n\n# Failure patterns by question type\n\n* **Domain-relevant:** 18 failures\n* **Novel generated:** 14 failures\n* **Metrics-generated:** 4 failures\n\n# Key takeaway\n\n* Most errors stem from missing or noisy retrieval rather than generation quality.\n* When the correct evidence is retrieved, the system answers correctly in most cases, with remaining failures concentrated in interpretive or multi-step financial reasoning.\n* This outperforms the paper‚Äôs Shared Vector Store setup (\\~19%) and approaches Long Context performance (\\~79%) while staying within realistic retrieval constraints.\n\n# What didn‚Äôt work for me\n\n* Multi-query expansion and HYDE mostly introduced noise.\n* RRF fusion didn‚Äôt help because the individual retrievers weren‚Äôt strong enough to begin with.\n* Cross-encoder and LLM rerankers didn‚Äôt separate relevance well at larger candidate sizes.\n* Retrieving directly on raw page text performed worse than using summaries.\n\n# What did work\n\n* API-based embedding models performed noticeably better than open-source ones in this domain.\n* **Page summaries outperformed raw page text** because they compressed the financial signal (entities, metrics, events) into dense semantic form.\n* Moving from separate retrievers (BM25 + dense) to Qdrant hybrid search helped slightly, likely due to better score fusion and indexing behavior.\n\n# Current pipeline\n\n1. Receive user query\n2. Extract company names and relevant year window\n3. Rewrite the query into a retrieval-friendly form using an LLM\n4. Perform hybrid retrieval over **page-level summaries**\n5. Pass retrieved pages through an **LLM relevance judge** to remove clearly irrelevant evidence\n\nThis setup gives \\~72% exact page retrieval at top\\_k = 10.\n\nWhy I think 80‚Äì84% is reachable\n\nThe generator currently uses a simple zero-shot prompt. In about 12 cases, the system retrieved the correct evidence but still failed to produce the answer.  \nI expect stronger prompting strategies (e.g., chain-of-thought reasoning) would resolve many of these cases, but I wasn‚Äôt able to test this further due to token limits.\n\nI would love to hear some suggestion how I can make the retrieval even better, if you have any suggestion please do post it.\n\nI‚Äôm also applying for RAG / LLM internships right now, would appreciate any perspective on how teams view projects like this. - please do give feedback.\n\nlink - [https://github.com/aquib8112/FinanceBench\\_RAG](https://github.com/aquib8112/FinanceBench_RAG)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1ra4r0n/heres_how_i_got_to_76_on_financebench_and_why_i/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6qencs",
          "author": "StarAI1234",
          "text": "Thanks for sharing your insights! Our platform Selonor overcomes most of the issues ny apply different workflows for ingestion and using a set of connected specialist agents. \n\nJust DM if you are interested in internship. \n\nhttps://www.selonor.com/",
          "score": 2,
          "created_utc": "2026-02-22 06:48:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rpvdy",
              "author": "Aquib8871",
              "text": "Please check your DM.\n\n",
              "score": 1,
              "created_utc": "2026-02-22 13:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}