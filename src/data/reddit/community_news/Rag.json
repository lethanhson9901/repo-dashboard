{
  "metadata": {
    "last_updated": "2026-02-01 08:57:56",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 103,
    "file_size_bytes": 140045
  },
  "items": [
    {
      "id": "1qocxu9",
      "title": "Ran 30 RAG chunking experiments - found that chunk SIZE matters more than chunking STRATEGY",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "author": "ManufacturerIll6406",
      "created_utc": "2026-01-27 12:49:14",
      "score": 62,
      "num_comments": 19,
      "upvote_ratio": 0.97,
      "text": "I kept seeing recommendations that sentence chunking is best for RAG because it \"respects grammatical boundaries.\"\n\nDecided to test it systematically: 4 strategies, 2 datasets, 1,200 retrieval evaluations.\n\nWriteup with methodology and open source code:¬†[Link](https://theprincipledengineer.substack.com/p/its-a-chunking-lie?r=2ivi0a)\n\n**Sentence chunking did dominate initially ‚Äî 96.7% recall vs 80-83% for others.**\n\nThen I noticed something most benchmarks don't report: actual chunk sizes produced.\n\nWhen I configured all strategies with chunk\\_size=1024:\n\n\\- Token: 934 chars (0.91x)\n\n\\- Recursive: 667 chars (0.65x)\n\n\\- Semantic: 1117 chars (1.09x)\n\n\\- Sentence: 3677 chars (3.59x) ‚Üê\n\nSentence chunking was producing chunks 3.6x larger than requested. Larger chunks = more context = better recall. That's a size effect, not a strategy effect.\n\n**When I controlled for actual chunk size (\\~3000 chars across strategies), token chunking matched or beat sentence chunking.**\n\n**Correlation between chunk size and recall: r=0.74 (HotpotQA), r=0.92 (Natural Questions).**\n\nCurious if others have seen similar results or if this breaks down on different datasets.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o20c64u",
          "author": "fixitchris",
          "text": "Depends what you are chunking.",
          "score": 11,
          "created_utc": "2026-01-27 13:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20jhim",
              "author": "ManufacturerIll6406",
              "text": "Exactly and this fw lets you test that on your own strategy and your own documents \n\n[https://github.com/somasays/rag-experiments/tree/main/chunking](https://github.com/somasays/rag-experiments/tree/main/chunking)",
              "score": 1,
              "created_utc": "2026-01-27 13:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o23k80i",
          "author": "durable-racoon",
          "text": "smaller chunks are usually better recall. larger chunks often better for generation. but nothing beats measuring for your specific use case, which you've done.",
          "score": 4,
          "created_utc": "2026-01-27 21:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20wha5",
          "author": "irodov4030",
          "text": "\"Larger chunks = more context = better recall.\"\n\nHow did you measure recall here?",
          "score": 3,
          "created_utc": "2026-01-27 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272ar6",
              "author": "mrFunkyFireWizard",
              "text": "This is also false, your vector points are not accurate if they contain too much context. If it's within the same semantic meaning it's fine but just adding more context is poor design",
              "score": 1,
              "created_utc": "2026-01-28 11:22:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21cwv3",
          "author": "TechnicalGeologist99",
          "text": "There's many steps in retrieval. I find that the main disadvantage of having short chunks is that occasioanly they knock it out the park in terms of relevance and they take up a space in the top K even though they are a useless chunk. \n\nIt may be that some strategies make many such useless chunks. (That are also small). That raises the probability of filling up top K with crap. \n\nYou could repeat this with varying top K to try and measure if that is occurring here",
          "score": 2,
          "created_utc": "2026-01-27 16:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21ewvq",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-27 16:16:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21f5km",
                  "author": "ManufacturerIll6406",
                  "text": "The article shows the results in detail if you are interested\n\n[https://theprincipledengineer.substack.com/p/its-a-chunking-lie](https://theprincipledengineer.substack.com/p/its-a-chunking-lie)",
                  "score": 2,
                  "created_utc": "2026-01-27 16:17:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26q28j",
          "author": "HMM0012",
          "text": "Makes sense; chunk size often drives context retention more than strategy. Controlling for size usually levels performance, so reported strategy wins can be misleading without normalizing chunk length.",
          "score": 2,
          "created_utc": "2026-01-28 09:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zyrl",
          "author": "notAllBits",
          "text": "I would abandon tokenization into chunks. Stream if you can, but cutting any text produces bias from discontinuity. I would follow syntactic and semantic structure when indexing.",
          "score": 1,
          "created_utc": "2026-01-27 15:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o218c9v",
              "author": "Jords13xx",
              "text": "Streaming is definitely an interesting approach, but it can be tricky with context retention. If you maintain some syntactic and semantic boundaries while still chunking, you might strike a balance between continuity and performance. Have you experimented with any hybrid methods?",
              "score": 1,
              "created_utc": "2026-01-27 15:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21cnnp",
                  "author": "ManufacturerIll6406",
                  "text": "Recursive chunking is essentially a hybrid, it tries paragraphs first, falls back to sentences, then words. In my experiments it landed in the middle: better size control than sentence, slightly worse recall than token at equivalent sizes.\n\nThe interesting question is whether there's a \"best of both worlds\" approach: target a specific size but snap to the nearest sentence boundary. You'd get predictable chunk sizes without mid-sentence cuts.\n\nDidn't test that explicitly, but the framework is extensible & would be a straightforward strategy to add - [https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)\n\nMight be worth exploring in a follow-up.",
                  "score": 1,
                  "created_utc": "2026-01-27 16:06:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o223w9s",
          "author": "jrochkind",
          "text": "If you have text that has em, i'd try paragraph chunking (up to certain max size paragraph anyway)",
          "score": 1,
          "created_utc": "2026-01-27 18:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22ip3o",
          "author": "blue-or-brown-keys",
          "text": "Curious if this may be trying to justify method based on outcome. Whats the intution here.",
          "score": 1,
          "created_utc": "2026-01-27 19:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o230ewt",
              "author": "ManufacturerIll6406",
              "text": "Intuition: More text = more semantic information encoded = better chance of matching the query. Also, answers rarely live in a single sentence, larger chunks capture the full context.\n\nThe \"justifying outcome\" concern would apply if I cherry-picked one result. But the correlation held across 30 configs, two datasets, and all four strategies landed on the same trendline (r=0.74 and r=0.92).\n\nCode's open if you want to test on a different dataset.",
              "score": 1,
              "created_utc": "2026-01-27 20:26:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29pm8l",
          "author": "lyonsclay",
          "text": "Wouldn't the optimal chunk size be contingent on your vector size assuming you are using vector similarity to select the chunks? If your chunk is smaller than your vector size then your system is being wasteful, if your chunk is larger than your vector size you are losing information.",
          "score": 1,
          "created_utc": "2026-01-28 19:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aikqp",
              "author": "ManufacturerIll6406",
              "text": "There probably is a sweet spot! In my experiments, recall kept improving up to \\~3000 chars with text-embedding-3-small (1536 dims). Didn't test beyond that. Could be interesting to check.\n\nhttps://preview.redd.it/arrmfswmq5gg1.png?width=1500&format=png&auto=webp&s=98470fd0c77c983075e9e1807d121e82f47fd5bd",
              "score": 2,
              "created_utc": "2026-01-28 21:23:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpce5s",
      "title": "You can now train embedding models 1.8-3.3x faster!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:11:25",
      "score": 31,
      "num_comments": 13,
      "upvote_ratio": 0.93,
      "text": "Hey RAG folks! We collaborated with Hugging Face to enable 1.8-3.3x faster embedding model training with 20% less VRAM, 2x longer context & no accuracy loss vs. FA2 setups.\n\nFull finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default! You can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp etc.\n\nFine-tuning embedding models can improve retrieval & RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.\n\nWe provided many free notebooks with 3 main use-cases to utilize. \n\n* Try the [EmbeddingGemma notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M).ipynb) in a free Colab T4 instance\n* We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!\n\n‚≠ê Guide + notebooks: [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nGitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n\nThanks so much guys! :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o287p7m",
          "author": "z0han4eg",
          "text": "Thanks mate, this a rly big news",
          "score": 3,
          "created_utc": "2026-01-28 15:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27wj9w",
          "author": "Popular_Sand2773",
          "text": "This is very cool. Quick question if we are using these encoders for the base of something else is this still valuable or is it only really for classic fine tuning? If I understand correctly the main speedup came from a new fused kernel correct?",
          "score": 1,
          "created_utc": "2026-01-28 14:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284gga",
              "author": "yoracale",
              "text": "Apologies could you elaborate your first question?\n\nOur main optimizations includes gradient checkpointing, kernels yes and more. You can see gradient checkpointing here: https://unsloth.ai/docs/new/500k-context-length-fine-tuning#unsloth-gradient-checkpointing-enhancements",
              "score": 1,
              "created_utc": "2026-01-28 15:06:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28gjpf",
          "author": "TechySpecky",
          "text": "Why do people fine tune models and when do these lead to superior performance than large well-known embedding models like gemini / qwen ones? For example if I am doing RAG for archeology would it make sense to have a custom embedding model?",
          "score": 1,
          "created_utc": "2026-01-28 16:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o294cnv",
              "author": "Financial-Bank2756",
              "text": "yes, a custom embedding model could help if you have enough domain text and evaluation pairs. Otherwise, a strong general model plus better chunking, metadata filters, hybrid search, and rerankers often beats premature fine-tuning.",
              "score": 3,
              "created_utc": "2026-01-28 17:44:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o294nzx",
                  "author": "TechySpecky",
                  "text": "Yea makes sense, what do you mean by evaluation pairs?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:45:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dh7lf",
          "author": "Aggressive-Solid6730",
          "text": "With how cheap compute is and how small embedding models are (compared to LLMs) I wouldn't think that time and memory are at that much of a premium. I am curious to hear any push-back on this, but I am also curious if in your experiments you saw any additional benefits of using these fine-tuning variants such as LoRA. Did they behave as regularizers making training more stable or were the gains purely speed and memory? The other thing you mention is context length which is fair, but as Google published, the amount of information we are trying to fit into a single vector is already quite limiting.",
          "score": 1,
          "created_utc": "2026-01-29 07:48:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2suc38",
              "author": "yoracale",
              "text": "For embedding model training speed is probably the most important. VRAM less so. If you can save time training why the hell not? And it's not a little speed boost, 2x faster basically means 100% faster than before\nThe gains are only for speed, memory and context length at this time. We don't do any accuracy changes as of this moment üôè",
              "score": 1,
              "created_utc": "2026-01-31 15:19:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eqvq6",
          "author": "Interesting-Town-433",
          "text": "Amazing! Will incorporate into embedding-adapters asap\n\nUniversal Embedding Translation Library\nOutput an embedding from any model into any other model's vector space\n\ngo\nminilm <-> openai\ngoogle <-> openai\ne5 <-> openai\nwith confidence scoring to tell you when it will work\n https://github.com/PotentiallyARobot/EmbeddingAdapters",
          "score": 1,
          "created_utc": "2026-01-29 13:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2su01y",
              "author": "yoracale",
              "text": "Interesting thanks for sharing!",
              "score": 1,
              "created_utc": "2026-01-31 15:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmlxfw",
      "title": "Looking for testers: 100% local RAG system with one-command setup",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "author": "primoco",
      "created_utc": "2026-01-25 15:23:30",
      "score": 30,
      "num_comments": 30,
      "upvote_ratio": 0.94,
      "text": "Hey everyone! üëã\n\nI've been working on an open-source RAG system and I'm looking for people willing to test it and give honest feedback.\n\n\\*\\*What it does:\\*\\*\n\nA document processing and Q&A system that runs 100% locally on your machine. No API keys needed, no data leaving your computer.\n\n\\*\\*Why I built it:\\*\\*\n\nI was frustrated with RAG solutions that either required cloud services, complex Docker configurations, or hours of setup. I wanted something that \"just works\" out of the box for businesses that need complete data privacy.\n\n\\*\\*Tech stack:\\*\\*\n\n\\- FastAPI backend\n\n\\- React + Vite frontend\n\n\\- Qdrant for vector storage\n\n\\- Ollama for local LLMs (Qwen2.5 or Mistral 7B)\n\n\\- BAAI/bge-m3 embeddings\n\n\\*\\*Key features:\\*\\*\n\n\\- Single command to start everything (\\`./setup.sh standard\\`)\n\n\\- Completely offline after initial setup\n\n\\- Supports 29 languages\n\n\\- Multi-user with JWT auth and role-based access\n\n\\- OCR support (Apache Tika + Tesseract)\n\n\\- Handles PDF, DOCX, PPTX, XLSX, TXT, MD and more\n\n\\- Tested with 10,000+ documents\n\n\\*\\*What I'm looking for:\\*\\*\n\n\\- Feedback on installation experience (Ubuntu 20.04+)\n\n\\- Real-world testing with your own documents\n\n\\- Bug reports and edge cases\n\n\\- Suggestions for improvements\n\n\\*\\*Repo:\\*\\* [https://github.com/I3K-IT/RAG-Enterprise](https://github.com/I3K-IT/RAG-Enterprise)\n\nI've stress-tested it with large documents (including the Mueller Report) but I'd love to see how it handles different use cases and languages.\n\nHappy to answer any questions!\n\nEDIT: Benchmark script and real-world results now published! See Community Benchmarks in the README.\n\n\\---\n\n\\*Fully open-source under AGPL-3.0. No paid tiers, no telemetry, no external calls.\\*  \n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1o03id",
          "author": "ampancha",
          "text": "Nice work on the local-first setup. One thing worth stress-testing before enterprise users hit it: retrieval-augmented systems are vulnerable to prompt injection via document content, and multi-user setups without per-user rate limits or query attribution can get abused fast. Both failure modes are invisible until production. Sent you a DM with more detail.",
          "score": 3,
          "created_utc": "2026-01-25 18:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o66iq",
              "author": "primoco",
              "text": "Really appreciate this feedback ‚Äî you‚Äôre raising exactly the kind of security considerations that matter for production deployments.\nYou‚Äôre right that both prompt injection via document content and multi-user abuse patterns are often invisible until they hit you in production. These are definitely on my radar for hardening the system.\nChecked your DM ‚Äî thanks for the detailed insights. I‚Äôll follow up there.\nFor anyone else reading: this kind of security-focused feedback is gold. If you spot potential vulnerabilities or have suggestions, Issues and DMs are always welcome.",
              "score": 1,
              "created_utc": "2026-01-25 19:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nbwin",
          "author": "Chrys",
          "text": "I could give it a try but I have a Mac mini. What do you think? It will be too slow?",
          "score": 1,
          "created_utc": "2026-01-25 17:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nu1eo",
              "author": "primoco",
              "text": "Hi Chris, it depends on which Mac Mini you have.\nApple Silicon (M1/M2/M4) with 16GB+ RAM: Should work well! Ollama runs natively on Apple Silicon and uses the integrated GPU. Performance won‚Äôt match a dedicated NVIDIA card, but it‚Äôs definitely usable.\nIntel Mac Mini: Will be slower since it runs on CPU only.\nThe catch: The automated setup.sh script is designed for Ubuntu + NVIDIA. On Mac, you‚Äôd need to set things up manually:\n\t‚àô\tInstall Docker Desktop\n\t‚àô\tInstall Ollama for Mac\n\t‚àô\tRun Qdrant via Docker\n\t‚àô\tStart the backend/frontend manually\nIf you share your Mac Mini specs (chip + RAM), I can give you a better estimate and maybe help with Mac-specific instructions. It would actually be great to have Mac compatibility documented!",
              "score": 1,
              "created_utc": "2026-01-25 18:16:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ogig8",
                  "author": "Chrys",
                  "text": "Mac mini M4 10-core CPU 10-core GPU 16GB/256GB. \nWhich local LLM do you suggest?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:52:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ou98e",
          "author": "fredastere",
          "text": "Def check it out ty",
          "score": 1,
          "created_utc": "2026-01-25 20:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwajj",
          "author": "Character_Pie_5368",
          "text": "Is there a mcp server interface? I have some pentesting frameworks that I‚Äôd like to try and interface with thisz",
          "score": 1,
          "created_utc": "2026-01-26 02:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rs2va",
              "author": "primoco",
              "text": "Not yet ‚Äî MCP server interface isn‚Äôt implemented at the moment.\nCurrently the system exposes a REST API (FastAPI backend on port 8000) that you could target for pentesting. The main endpoints are:\n\t‚àô\t/api/v1/query ‚Äî RAG queries\n\t‚àô\t/api/v1/documents ‚Äî document upload/management\n\t‚àô\t/api/v1/auth ‚Äî JWT authentication\nThat said, MCP support is an interesting idea for the roadmap ‚Äî would make integration with Claude and other tools much smoother.\nIf you run your pentesting frameworks against it and find vulnerabilities, I‚Äôd genuinely appreciate the feedback! There‚Äôs a SECURITY.md for responsible disclosure.\nWhat frameworks are you planning to use? Curious what you‚Äôre testing for.",
              "score": 1,
              "created_utc": "2026-01-26 05:58:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ratzb",
          "author": "floating___around",
          "text": "I am interested. Would a 3090 run that program?",
          "score": 1,
          "created_utc": "2026-01-26 04:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ru7re",
              "author": "primoco",
              "text": "Absolutely! A 3090 with 24GB VRAM is perfect ‚Äî even more than my test setup (RTX 5070 Ti, 16GB).\n\nThe setup script has three profiles:\n Profile   |GPU VRAM|LLM Model  |RAM  |\n |----------|--------|-----------|-----|\n |`minimal` |8-12GB  |Mistral 7B |16GB |\n |`standard`|12-16GB |Qwen2.5 14B|32GB |\n |`advanced`|16-24GB |Qwen2.5 32B|128GB|\n \nWith your 3090 (24GB), you could run:\n \n ```\n ./setup.sh advanced\n ```\n \nThis installs `qwen2.5:32b-instruct-q4_K_M` ‚Äî the most capable model available.\n \nNote: The advanced profile expects 128GB system RAM. If you have less, you can still run `./setup.sh advanced` but consider switching to the 14B model in `docker-compose.yml` if you experience issues.\n \nAlternatively, `./setup.sh standard` with the 14B model is a safe choice that will run great on your hardware.\nLet me know how it goes!\n\n-----\n\nCos√¨ diamo info complete e accurate. Vuoi modificare qualcosa?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2026-01-26 06:14:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rj89p",
          "author": "ggone20",
          "text": "\nSearch performance metrics? ‚ÄòTested on 10,000+ documents‚Äô with just a Qdrant db and an old small models? Idk‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-26 04:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rv6ux",
              "author": "primoco",
              "text": "Qdrant handles vector search efficiently ‚Äî at 10K docs the bottleneck is typically LLM inference, not retrieval. Query times stay in the 2-4 second range in my testing.\nI don‚Äôt have formal benchmark charts published yet, but it‚Äôs something I‚Äôd like to add to the documentation. If you run stress tests on your setup, I‚Äôd be happy to include real-world metrics from the community!",
              "score": 1,
              "created_utc": "2026-01-26 06:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rwkiv",
                  "author": "ggone20",
                  "text": "I really mean retrieval performance for non-trivial queries.",
                  "score": 1,
                  "created_utc": "2026-01-26 06:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rrwi1",
          "author": "AwayLuck7875",
          "text": "Rag ollama vulkan??",
          "score": 1,
          "created_utc": "2026-01-26 05:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rvm6p",
              "author": "primoco",
              "text": "Are you asking about Vulkan support for AMD/Intel GPUs?\nRAG Enterprise is currently tested with NVIDIA GPUs (CUDA). Ollama does have experimental Vulkan support for AMD cards, but I haven‚Äôt tested that combination yet.\nIn theory it should work ‚Äî the backend just calls Ollama‚Äôs API, it doesn‚Äôt care what‚Äôs running underneath. But I can‚Äôt guarantee performance or stability on Vulkan.\nIf you have an AMD GPU and want to try it:\n\t1.\tInstall Ollama with Vulkan support manually\n\t2.\tSkip the automated setup and configure services individually\n\t3.\tPoint the backend to your Ollama instance\nLet me know your GPU ‚Äî happy to help figure out if it‚Äôs worth trying!",
              "score": 1,
              "created_utc": "2026-01-26 06:25:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rxwbk",
          "author": "AwayLuck7875",
          "text": "Maybe byt work",
          "score": 1,
          "created_utc": "2026-01-26 06:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s4h4j",
              "author": "primoco",
              "text": "Sorry, didn't quite catch that ‚Äî could you clarify? Happy to help if you have questions!",
              "score": 1,
              "created_utc": "2026-01-26 07:37:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rzws6",
          "author": "arxdit",
          "text": "Definitely something that many of us are working on.\n\nI have built my own, and it supports both ollama and openai api key.\n\non ollama I rely mostly on qwen3-coder:30b which is nothing short of AMAZING.\n\nI made my own curated indexing & retrieval algos focusing on handling ever expanding knowledge - including an endless conversation manager that does not compact - it's here \\[[https://github.com/andreirx/FRAKTAG](https://github.com/andreirx/FRAKTAG)\\] if you want to check it out\n\nOh and having a CLI makes it usable by claude code",
          "score": 1,
          "created_utc": "2026-01-26 06:59:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s5csz",
              "author": "primoco",
              "text": "Nice! Just checked out FRAKTAG ‚Äî interesting approach with the non-compacting conversation manager. Different philosophy from what I'm doing but I can see the value for ever-expanding knowledge bases.\n\nThe documentation is really thorough ‚Äî that's not easy to maintain, kudos for that.\n\nThe CLI + Claude Code integration is a smart move ‚Äî that's actually something I should consider adding to RAG Enterprise.\n\nqwen3-coder:30b is impressive, though it needs serious VRAM. I've kept the default models smaller (7B-14B) to lower the entry barrier, but power users can definitely swap in larger models.\n\nCool to see others building in this space ‚Äî plenty of room for different approaches! ü§ù",
              "score": 2,
              "created_utc": "2026-01-26 07:45:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1s7eiw",
                  "author": "arxdit",
                  "text": "I wanted to have the capability of targeting enterprise level private deployments - like a law firm",
                  "score": 1,
                  "created_utc": "2026-01-26 08:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zgjsq",
              "author": "GP_103",
              "text": "Pretty cool. Did you actually Claude code that in 13 days. Props.",
              "score": 1,
              "created_utc": "2026-01-27 08:52:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ziacp",
                  "author": "arxdit",
                  "text": "Thank you!\n\nYes - first planned with gemini pro + opus, then passed to claude code. Still using this approach on major features / refactors.\n\nPeppered my codebase with MAP files and now my main problem is babysitting claude code... can't fully trust an AI with code but I'm getting used to its failure modes and I can smell the mistakes much faster",
                  "score": 1,
                  "created_utc": "2026-01-27 09:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zz39u",
          "author": "primoco",
          "text": "UPDATE: Benchmarks are live!\n\nAdded a complete benchmarking suite to the repo:\n\nBenchmark script: python benchmark/rag\\_benchmark.py ‚Äî run it on your hardware\n\nTest documents: Mueller Report, 9/11 Commission Report, Bitcoin Whitepaper, \"Attention Is All You Need\"\n\nReal metrics: Upload times, query latency (mean/median/p95), similarity scores\n\nResults from my setup (Ryzen 9 5950X, 64GB RAM, RTX 5070 Ti):\n\nQuery response: \\~4.3s mean, \\~3.6s median\n\nUpload: 0.6s - 24s depending on document size\n\nFull details in the README: Community Benchmarks section\n\nIf you run the benchmark on your hardware, I'd love to add your results to the comparison table. Open an issue or comment here!",
          "score": 1,
          "created_utc": "2026-01-27 11:37:05",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnplpb",
      "title": "Built a tool for visualizing text chunking strategies",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "author": "Crazy-Plan8697",
      "created_utc": "2026-01-26 19:09:49",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Hey :)\n\nSome time ago I wanted to learn the fundamentals of RAG, and I started with chunking. Greg Kamradt‚Äôs tool ([https://chunkviz.up.railway.app/](https://chunkviz.up.railway.app/)) helped me understand the basics, and it was a great starting point.\n\nWhile digging deeper, especially when reading papers like [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997), I noticed that **semantic** and **agentic** chunking are showing up more often and are getting adopted in new research. But I couldn‚Äôt find any visualizers that supported those methods, so I tried to build one myself.\n\nI put together **Chunking-Vis**, a small web tool for anyone who wants to explore and learn how different chunking strategies behave. I think it can be especially helpful if you‚Äôre new to RAG and want to see how chunks are formed before they‚Äôre sent to an embedding model or retrieval pipeline.\n\n# What Chunking-Vis supports\n\n* Character-level chunking\n* Word-level chunking\n* Token-level chunking (GPT-4o tokenizer)\n* Recursive chunking\n* Semantic chunking\n* Agentic chunking (Phi-3 powered, available locally on CPU)\n\nThere‚Äôs also a **Snapshot** feature that lets you save and compare different chunking configurations side-by-side, which can make experimentation easier.\n\n# Live Demo\n\n[https://chunkingvis-production.up.railway.app](https://chunkingvis-production.up.railway.app)  \n(Agentic chunking is disabled in the demo due to compute limits.)\n\n# Github Repository\n\n[https://github.com/MichalZnalezniak/Chunking-Vis](https://github.com/MichalZnalezniak/Chunking-Vis)\n\nHope some of you find it useful ‚Äî and if you have ideas, feedback, or suggestions, please let me know. Thank you!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qq8y1q",
      "title": "TextTools ‚Äì High-Level NLP Toolkit Built on LLMs (Translation, NER, Categorization & More)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "author": "Due_Place_6635",
      "created_utc": "2026-01-29 13:45:08",
      "score": 23,
      "num_comments": 2,
      "upvote_ratio": 0.97,
      "text": "Hey everyone! üëã\n\nI've been working on TextTools, an open-source NLP toolkit that wraps LLMs with ready-to-use utilities for common text processing tasks. Think of it as a high-level API that gives you structured outputs without the prompt engineering hassle.\n\nWhat it does:\n\nTranslation, summarization, and text augmentation\n\nQuestion detection and generation\n\nCategorization and keyword extraction\n\nNamed Entity Recognition (NER)\n\nCustom tools for almost anything\n\nWhat makes it different:\n\nBoth sync and async APIs (TheTool & AsyncTheTool)\n\nStructured outputs with validation\n\nProduction-ready tools (tested) + experimental features\n\nWorks with any OpenAI-compatible endpoint\n\nQuick example:\n\n```python\nfrom texttools import TheTool\n\nthe_tool = TheTool(client=openai_client, model=\"your_model\")\nresult = the_tool.is_question(\"Is this a question?\")\nprint(result.to_json())\n```\nCheck it out: https://github.com/mohamad-tohidi/texttools\n\nI'd love to hear your thoughts! If you find it useful, contributions and feedback are super welcome. What other NLP utilities would you like to see added?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2ewxt9",
          "author": "Popular_Sand2773",
          "text": "Hey like the idea of a one stop shop but a lot of these are tasks I would never give to an llm. Have you thought about unifying the BIC options for these different tasks? BERTopic is a good example of a unifying framework that takes the best from everyone.",
          "score": 3,
          "created_utc": "2026-01-29 14:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qos79",
              "author": "Due_Place_6635",
              "text": "Hello, thank you for the reply  \ni completly agree, there are a lot of tasks that an small finetuned BERT model would be far better at them, rather than a giant LLM!\n\nyes, as a matter of fact, it used to be part of the library, but we decided to remove that\n\nbecause: the llm's are open domain, they are kind of like a quick Demo of what is possible...",
              "score": 1,
              "created_utc": "2026-01-31 05:10:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qp9pmy",
      "title": "How to handle extremely large extracted document data in an agentic system? (RAG / alternatives?)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "author": "Complex-Time-4287",
      "created_utc": "2026-01-28 12:13:17",
      "score": 20,
      "num_comments": 27,
      "upvote_ratio": 0.96,
      "text": "I‚Äôm building an agentic system where users can upload documents. These documents can be *very* large ‚Äî for example, up to **15 documents at once**, where some are **\\~1500 pages** and others **300‚Äì400 pages**. Most of these are financial documents (e.g., tax forms), though not exclusively.\n\nWe have a document extraction service that works well and produces structured layout + document data.  \nHowever, the extracted data itself is also huge, so we **can‚Äôt fit it into the chat context**.\n\n  \n**Current approach**\n\n* The extracted structured data is stored as a **JSON file in cloud storage**\n* We store a **reference/ID in the DB**\n* Tools can fetch the data using this reference when needed\n\n  \n**The Problem**\n\nBecause the agent never directly ‚Äúsees‚Äù or understands the extracted data:\n\n* If a user asks questions about the document content,\n* The agent often can‚Äôt answer correctly, since the data is not in its context or memory\n\n  \n**What we‚Äôre considering**\n\nWe‚Äôre thinking about applying **RAG on the extracted data**, but we have a few concerns:\n\n* Agents run in a chat loop ‚Üí **creation + retrieval must be fast**\n* The data is deeply nested and very large\n* We want minimal latency and good accuracy\n\n**Questions**\n\n1. What are **practical solutions** to this problem?\n2. Which **RAG systems / architectures** would work best for this kind of use-case?\n3. Are there **alternative approaches** (non-RAG) that might work better for large documents?\n4. Any best practices for handling **very large documents** in agentic systems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o27bksd",
          "author": "Mishuri",
          "text": "You must do semantic chunking on the document. Split it up according to logical partitions. Ask LLM to enhance with descriptions metadata. Explicit relationships to other sections. Then embed those. 90% and the expensive part is LLM preprocessing of this. Then you gather context with vector rag and details with agentic rag + subagents to manage context.",
          "score": 6,
          "created_utc": "2026-01-28 12:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dy9y",
              "author": "rshah4",
              "text": "Yes, and then you can have a Table of Contents and make sure each of those sections understand their role in the hierarchical structure. This is what we do and it works well.  \nAlso a database can be useful here as well.",
              "score": 2,
              "created_utc": "2026-01-28 12:46:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27onir",
              "author": "Complex-Time-4287",
              "text": "this it totally possible, but I'm concerned about the time it is likely to take, in a chat it'll feel kind of blocking until the chuking and embedding is complete",
              "score": 1,
              "created_utc": "2026-01-28 13:47:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28i5j8",
              "author": "usernotfoundo",
              "text": "Are there any particular resources you would suggest that go into detail in this process? Currently I have been simply using an llm to process my large paragraph into a list of (observation,recommendation), embedding the observations and retrieving it based on similarity with a query. I feel this is too simplified, and breaking it down into multiple steps like you described could be the way to go, no idea how to start.",
              "score": 1,
              "created_utc": "2026-01-28 16:07:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d1jzt",
          "author": "aiprod",
          "text": "I think what most people are missing here are the strict latency requirements. The user uploads documents in a live chat session and wants to interact with them immediately, correct?\n\nThis rules out time intensive approaches like embeddings or generating summaries or metadata with LLMs.\n\nThere are a few things that could work:\n\nGive the agent a search tool that is based on BM25. Create page chunks from the data (usually a good semantic boundary too), index it into open search or elastic search and let the agent search the index. This is fast and context efficient.\n\nOn top of that, you could add the first one or two pages of each file to the context window of the agent. Usually, the first pages give an indication of what a doc is about. With that knowledge, the agent could make targeted searches inside a specific doc by using a filter with the search queries.\n\n\nAlternatively, you could use the file system based approach that coding agents like Claude code use. Give the agent tools to grep through the files and to read slices of the document. You don‚Äôt have to use an actual file system, it could just be simulated with tools. The agent will grep and slice through the docs to answer questions. RLM is an advanced version of this approach: https://arxiv.org/pdf/2512.24601v1",
          "score": 6,
          "created_utc": "2026-01-29 05:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d5vl6",
              "author": "Complex-Time-4287",
              "text": "That's right! Thanks for the suggestions, I'll try this",
              "score": 1,
              "created_utc": "2026-01-29 06:12:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mnabf",
          "author": "Ecstatic_Heron_7944",
          "text": "Chiming in to offer an alternative perspective: you're doing all this document extraction, table identification, json generating, heavy processing and time consuming work for pages the user hasn't even asked for. In a document with 300 pages, any given query could require a fraction (maybe 10 - 20 pages) for a suitable answer. Could a better approach be to do the search (fast) first and extraction (slow) later - especially after the user is happy to confirm the context? Well, I hope so because this is what I'm building with [ragextract.com](http://ragextract.com) !\n\nTo answer your questions:\n\n1. RAG would work as a way to try narrow the search space for the user query but for financial data, it's unlikely to be sufficient on its own. You'll still need to post-process the pages for accuracy - though you may sometime get away with just winging it with vision models.  \n2. Multimodal RAG works incredibly well if documents don't share a standardised layout ie different statements from different bank. You'd also might want to look at a more optimised retrieval system for pages.  \n3. In practical terms, not that I can think of. Search-first-parse-later is an alternative RAG approach I think is worth exploring in this scenario.  \n4. Best practices for large documents? You probably already know this but go (1) big, (2) async and (3) distributed!",
          "score": 3,
          "created_utc": "2026-01-30 16:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27mwi8",
          "author": "KnightCodin",
          "text": "There are few gaps in the problem summary that might help  \n1. Operational flow :   \nWhen you say \"tools can fetch the data using this reference when needed\" and \"gent never directly ‚Äúsees‚Äù or understands the extracted data\",   \n\\- Where is data going to - directly to the user and not to the Agent/LLM?  \n\\- What is stopping you from presenting the \"summary\" data (Chain of Density compressed) to the Agent so follow up questions can be answered\n\n2. Do you need to answer questions on documents by other users? - Meaning is it a departmental segmentation with multiple users but same overall context or user/documents are isolated?   \n\\- This will provide type of KG and scale   \n\\- Summary \"Contextual Map\"",
          "score": 2,
          "created_utc": "2026-01-28 13:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27riay",
              "author": "Complex-Time-4287",
              "text": "In my agentic system, users can connect third-party MCP tools. If a tool requires access to the extracted data, the agent can pass that data to the specific tool the user has attached, but only when it‚Äôs actually needed.\n\nThe main issue with relying on summaries is that the extracted data itself is already very large and deeply nested JSON. Generating a meaningful summary from it is hard, and even a compressed (Chain-of-Density‚Äìstyle) summary would still fail to answer very specific questions‚Äîfor example, ‚ÄúWhat was the annual income in 2023?‚Äù\n\nRegarding document access and isolation: documents are scoped strictly to the current conversation. Conversations are not user-specific, and there can be multiple conversations, but within each conversation we only reference the documents uploaded in that same context.\n\nDocuments are uploaded dynamically as part of the conversation flow, and only those on-the-go uploads are considered when answering questions or invoking tools.",
              "score": 1,
              "created_utc": "2026-01-28 14:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27twm2",
                  "author": "KnightCodin",
                  "text": "Better :)  \nSimplistic and practical solution (not to be confused with simple) is   \nMulti-tier retrieval:   \n  \nEg : ‚ÄúSummary doc map‚Äù ‚Üí ‚ÄúTargeted Sub-Node‚Äù ‚Üí \"Drill-Down Deep fetch\"\n\nThis will be the most latency-effective for massive bundles.\n\n**SPECIFICITY :**   \n**Tier A: coarse index**\n\n* Embed¬†full **page summaries**,¬†**section headers**, and¬†**table captions**\n* Or **one chunk per page**¬†: Fully summarized (Will say normalized but that will open a whole new can of worms)\n* Path: identify *which pages/sections matter -> use deep fetch to grab that JSON*\n\n**Tier B: targeted extraction retrieval**\n\n* Once you know relevant pages/sections, fetch only that slice from cloud storage:\n   * e.g., pages 210‚Äì218 JSON\n   * or the section subtree for¬†`Income >` What was the annual income in 2023",
                  "score": 1,
                  "created_utc": "2026-01-28 14:14:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27bc02",
          "author": "patbhakta",
          "text": "For financial data, skip the traditional RAG, skip the vector databases, perhaps skip graph rag too. Go with trees, you'll incur more cost but at least your data will be sound.",
          "score": 1,
          "created_utc": "2026-01-28 12:28:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fxm7",
              "author": "yelling-at-clouds-40",
              "text": "Trees are just subset of graphs, but curious: what kind of trees do you suggest (as node hierarchy)?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27nv20",
              "author": "ajay-c",
              "text": "Interesting do you know any tree techniques?",
              "score": 1,
              "created_utc": "2026-01-28 13:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27oswt",
              "author": "Complex-Time-4287",
              "text": "can you please provide some details on this?",
              "score": 1,
              "created_utc": "2026-01-28 13:48:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bjew",
          "author": "arxdit",
          "text": "I‚Äôm handling this via knowledge trees and human supervised document ingestion (you supervise proper slicing and where the document belongs in the knowledge tree - though the AI does make suggestions)\n\nThe AI by itself is very bad at organizing information with no clear rules and will fail spectacularly \n\nSlowly learning through this\n\nYou can check out my solution [FRAKTAG on github](https://github.com/andreirx/FRAKTAG)",
          "score": 1,
          "created_utc": "2026-01-28 12:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27nxzr",
              "author": "ajay-c",
              "text": "Interesting",
              "score": 1,
              "created_utc": "2026-01-28 13:43:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27p3tn",
              "author": "Complex-Time-4287",
              "text": "Looks interesting, I'll check this  \nFor my use-case, we cannot really have a human in the loop, agents are completely autonomous and must proceed on their own",
              "score": 1,
              "created_utc": "2026-01-28 13:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27q1w8",
                  "author": "arxdit",
                  "text": "I want to get there too and I‚Äôm using my human decisions to hopefully ‚Äúteach‚Äù the ai how to do it by itself, and I am gathering data",
                  "score": 1,
                  "created_utc": "2026-01-28 13:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27btme",
          "author": "proxima_centauri05",
          "text": "You‚Äôre not doing anything ‚Äúwrong‚Äù. This is the natural failure mode when the agent only has a pointer to the data instead of an understanding of it. If the model never sees even a compressed view of the document, it‚Äôll confidently answer based on vibes.\n\nWhat‚Äôs worked for me is separating understanding from storage. On ingestion, I generate a thin semantic layer, section summaries, key entities, numbers, obligations, relationships. That layer is small, fast, and always available to the agent. The heavy JSON stays out of the loop unless the agent explicitly needs to verify something.\nTrying to RAG directly over deeply nested extracted data is usually a dead end. It‚Äôs slow, and the signal to noise ratio is awful. Hierarchical retrieval helps a lot, first decide where to look, then pull only that slice, then answr. Latency stays low because most questions never touch the raw data.\n\nFor financial or forms heavy docs, I often skip RAG entirely and just query normalized fields. It‚Äôs boring, but it‚Äôs correct. RAG is great for ‚Äúexplain‚Äù questions, terrible for ‚Äúcalculate‚Äù ones.\n\nI‚Äôm building something in this space too, and the big unlock was treating documents like evolving knowledge objects, not blobs you fetch. Once the agent has a map of the document, it stops hallucinating and starts reasoning.",
          "score": 1,
          "created_utc": "2026-01-28 12:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27tqal",
              "author": "Complex-Time-4287",
              "text": "In my case, the questions are much more likely to be *‚Äúfind‚Äù* questions rather than *‚Äúcalculate‚Äù* ones. For extremely large documents say a 1,500-page PDF containing multiple tax forms summaries or key-entity layers won‚Äôt realistically capture all the essential details.\n\nAlso, I‚Äôm not entirely sure what you mean by ‚Äújust query normalized fields‚Äù in this context.",
              "score": 1,
              "created_utc": "2026-01-28 14:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27oiym",
          "author": "ajay-c",
          "text": "I do have same issues",
          "score": 1,
          "created_utc": "2026-01-28 13:46:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27swp4",
          "author": "Crafty_Disk_7026",
          "text": "Try using a retrieval MCP https://github.com/imran31415/codemode-sqlite-mcp/tree/main. Here's one I made you can try.  This won't require embedding",
          "score": 1,
          "created_utc": "2026-01-28 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u005",
          "author": "Popular_Sand2773",
          "text": "Your instinct that you can't just shove these in the context window is correct and that you need some sort of RAG but what and why depend on the answers to these questions.\n\nWhat questions are your users asking?  \nGiven it is financial documents if it is mainly numbers and tables they care about then you should think about a SQL db and retrieval. Regular semantic embeddings are not very good at highly detailed math. If it's contract minutia then maybe a vector db and semantic embeddings. Likely you'll need both.\n\nHow much of this is noise?  \nYou mention huge documents and tax forms as an example. If a lot of this is stuff your users are never going to query you are paying both in quality and cost for things you won't use and don't need. Figure out what you can prune. \n\nIs there clear structure you can leverage?  \nJust because it's called unstructured text doesn't mean there is no structure at all. If you can narrow down where in the documents you are looking for a specific query based on the inherent structure like sections etc then you can narrow the search space and increase your top-k odds.\n\nAll this to say. It's not about what RAG is best etc it's what problem are you actually trying to solve and why. If you just want a flat quality bump without further thought try knowledge graph embeddings.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28dreo",
          "author": "Det-Nick-Valentine",
          "text": "I have the same problem.\n\nI'm working on an in-company solution like NotebookLM.\n\nIt works very well for small and medium-sized documents, but when the user uploads something large, like legal documents, it doesn't give good responses.\n\nI'm thinking of going for a summary by N chunks and working with re-ranking.\n\nWhat do you think of this approach?",
          "score": 1,
          "created_utc": "2026-01-28 15:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9g7o",
          "author": "pl201",
          "text": "Take a look of open source LightRAG. Per my research and trying, it has the best potential to be used for the requirements you have described in the post. I am working on enhancements so it can be used on a company setting (multi users, workspace, separate embedding LLM and chat LLM, speed the query for a larger knowledge base.  Etc). PM me if you are interested to make it work for your case.",
          "score": 1,
          "created_utc": "2026-01-28 20:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e7nmc",
          "author": "Infamous_Ad5702",
          "text": "We had a similar problem for a client.\n\nNo GPU needed\nThey Can‚Äôt use black box LLM.\nThey Can‚Äôt have hallucinations.\n\nDefence industry so needed to be offline.\n\nWe built a tool that builds an index first. Makes it efficient. Every new query it builds a new Knowledge Graph. \n\nDoes the trick.",
          "score": 1,
          "created_utc": "2026-01-29 11:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eeuh4",
          "author": "TechnicalGeologist99",
          "text": "Hierarchical RAG. The document structure is important. Detect section headers and use them to construct a data tree of each document. \n\nAt the same time extract tags that you will predefined (i.e. financial, design, technical) use those same tags at query time to prefilter. \n\nWhen a section gets many hits from semantic retrieval you will upgrade and retrieve more or all of that section (it's clearly relevant)\n\nEnsure you use query decomposition (fragmenting the users question into multiple questions for multiple retrieval) and rerank those. For large retrievals, group chunks by their section id and summarise them in context of the sub question that was used to retrieve them. And then inject those summaries as documents in the final call. \n\nCongrats you didn't really need an agentic system. \nBut you can always migrate to one if and when the time is right. But don't just go agentic because it's popular. Build your domain and solutions by proving the need (YAGNI)",
          "score": 1,
          "created_utc": "2026-01-29 12:34:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs2uzw",
      "title": "Building a RAG-Based Chat Assistant using Elasticsearch as a Vector Database",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "author": "Leading-Grape-6659",
      "created_utc": "2026-01-31 13:50:21",
      "score": 18,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "Hi everyone üëã\n\n\n\nI recently built a simple RAG (Retrieval-Augmented Generation) chat assistant using Elasticsearch as a vector database.\n\n\n\nThe blog covers:\n\n‚Ä¢ How vector embeddings are stored in Elasticsearch\n\n‚Ä¢ Semantic retrieval using vector search\n\n‚Ä¢ How retrieved context improves LLM responses\n\n‚Ä¢ Real-world use cases like internal knowledge bots\n\n\n\nFull technical walkthrough with code and architecture here:\n\nüëâ [https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends\\_link&sk=d2006b31e40e3c3ed714c18eabf8f271](https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends_link&sk=d2006b31e40e3c3ed714c18eabf8f271)\n\n\n\nHappy to hear feedback or suggestions from folks working with RAG and vector databases!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2sf6zr",
          "author": "cat47b",
          "text": "I did wonder about elastic but the hassle and cost of using that as a store has put me off a bit. Have you looked at Clickhouse?",
          "score": 2,
          "created_utc": "2026-01-31 13:55:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfzko",
      "title": "Convert Charts & Tables to Knowledge Graphs in Minutes | Vision RAG Tuto...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "author": "BitterHouse8234",
      "created_utc": "2026-01-28 16:24:54",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.94,
      "text": "Struggling to extract data from complex charts and tables? Stop relying on broken OCR. In this video, I reveal how to use Vision-Native RAG to turn messy PDFs into structured Knowledge Graphs using Llama 3.2 Vision.  \n  \nTraditional RAG pipelines fail when they meet complex tables or charts. Optical Character Recognition (OCR) just produces a mess of text. Today, we are exploring VeritasGraph, a powerful new tool that uses Multimodal AI to \"see\" documents exactly like a human does.  \n  \nWe will walk through the entire pipeline: ingesting a financial report, bypassing OCR, extracting hierarchical data, and visualizing the connections in a stunning Knowledge Graph.  \n  \nüëá Resources & Code mentioned in this video: üîó GitHub Repo (VeritasGraph): [https://github.com/bibinprathap/VeritasGraph](https://github.com/bibinprathap/VeritasGraph)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qohzz3",
      "title": "Multilingual RAG for Legal Documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "author": "mathrb",
      "created_utc": "2026-01-27 16:06:51",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "Hey all,\n\nWe're a small team (not many engineers) building a RAG system for legal documents(contracts, NDAs, terms of service, compliance docs, etc.).\n\nThe multilingual challenge:\n\nOur documents span multiple languages (EN, FR, DE, ES, IT, etc.).\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have docs in a single language (e.g., all French)\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have mixed-language corpora\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some individual documents are bilingual\n\n¬†\n\nFor legal docs, hybrid search (FT search and dense vectors with re rank) seems to be a good candidate for retrieval. One issue I saw is that most implementations relies on language dependent solutions for FT search.\n\nApproaches I've seen discussed:\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Per-language BM25 indexes: Detect language, route to the right index with proper stemmer. Seems correct but adds complexity. How do you handle bilingual documents?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Language-agnostic tokenization: Skip stemming, just split on whitespace. Loses morphological matching but works across languages.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† BGE-M3 sparse vectors: Supposedly handles 100+ languages natively for both dense and sparse. But does it require GPU? What's the cost/perf tradeoff vs traditional BM25?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Translate everything to English: Normalize the knowledge base. Feels wrong for legal where original wording matters and adds a translation failure mode.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Dense-only + reranker : Skip BM25 entirely, use strong multilingual embeddings (BGE-M3, multilingual-e5) and rerank. Loses exact keyword matching.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Qdrant's native BM25 : Qdrant now has built-in BM25 with language configs. Anyone using this for multilingual? How does it compare to dedicated solutions?\n\n¬†\n\nWe‚Äôd rather use managed services when available in the cloud provider we chose (scaleway).\n\nOur constraints:\n\n* Managed PostgreSQL for app data : only supports pgvector, not pg\\_search/ParadeDB. Would require to self-host a postgres for additional extensions.\n* Prefer simplicity: Leaning toward Qdrant over Milvus since it seems easier to operate.\n* Cost-conscious: GPU-heavy solutions for embeddings are a concern.\n* Multi-tenant: Each tenant's documents are typically in one consistent language, but not always.\n\nAnyone would like to share their experience or thoughts on this challenge?  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o21ijp5",
          "author": "patbhakta",
          "text": "1) translate everything to English is a bad solution for legal reasons as the verbage needs to stay in tact. Translation is another can of worms you don't want to go down.\n\n2) you could have multiple vector\\graph\\sql databases for each language, querying them all could be a nightmare though.\n\n3) embeddings, chunking, parsing, are all an issue that needs to be addressed with every RAG.\n\n4) legal documents are like a git repo, multiple versions and annotations, how are going to handle edits?\n\n5) cross referencing legal jurisdiction that periodically changes as well and in multiple countries you'll need a system for that knowledge base\n\n6) the larger your documents grow the worse your AI will get and will hallucinate beyond use. \n\n7) dedups and gardening can help but that's another wrench in the system\n\nPersonally I wouldn't use RAG on your use case, you need a truthful symantec search with citation. You really should seek consultation before going forward on ideas.",
          "score": 6,
          "created_utc": "2026-01-27 16:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nzh2",
              "author": "mathrb",
              "text": "Thanks for your reply\n\n1. Agree, feels wrong in this domain\n2. Agree with the nightmare\n3. As of today, only last version will be handled\n4. Not in scope\n\n\\`truthful symantec search with citation\\`, what kind of systems are you referring to?  \nEdit: I assume you meant semantic search.  \nEven though, we still face some of the same the challenges (minus the generative part at the end), right?",
              "score": 1,
              "created_utc": "2026-01-27 16:55:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21wv03",
                  "author": "patbhakta",
                  "text": "Yes semantic hybrid approach. \nWithout knowing your document size, document length, who the users are, what do they want done, etc it's hard to provide a solution. \n\nIf the scenario was like a legal SaaS where documents needed to be translated then AI is great at that or a summary to avoid reading hundreds of pages. A tuned AI with case logic can also research as a paralegal or even a college level lawyer.\n\nIf you're building an AI tailored to the firm based on its documents, procedures and cases then it's a bit more complex but doable.\n\nIf you're building an AI for automation even that's doable but a different approach. \n\nHave to break down the scope, what it's roadmap is like, budget, legal compliance, security access (who has access to what documents) etc...",
                  "score": 1,
                  "created_utc": "2026-01-27 17:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o269emw",
          "author": "jael_m",
          "text": "You can still do the hybrid search combining dense vector search and text match like BM25. There are some special tokenizers for multilingual text. For example, milvus supports language identifier to automatically detect and apply the proper tokenizer, and the multi-language analyzer for text retrieval.",
          "score": 2,
          "created_utc": "2026-01-28 07:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e9yg",
              "author": "explodedgiraffe",
              "text": "But sparse will still fail on cross language queries right? It would match a query ‚Äúv√©lo‚Äù (French) with ‚Äúbike‚Äù in English ?",
              "score": 1,
              "created_utc": "2026-01-28 07:47:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blzit",
          "author": "Repulsive-Memory-298",
          "text": "Interesting. I mean bm25 is beyond FT right? You could start with FT key word maybe? Where you could milk that by sorting by vector similarity? \n\nI wonder if term frequency would really struggle with combined languages. \n\nAnyways there are lots of future directions. Definitely start with more basic solutions imo, it‚Äôll give you a footing and then you can explore and test more complex solutions. BM25 is pretty advanced FT compared to meat and potato‚Äôs.",
          "score": 1,
          "created_utc": "2026-01-29 00:34:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ekocl",
          "author": "bumkey1101",
          "text": "Yeah I‚Äôve dealt with this a bit ‚Äî I wouldn‚Äôt overthink ‚Äúperfect multilingual BM25‚Äù.\n\nBM25/FT is awesome **when query + doc are same language**, but it‚Äôs basically not gonna do ‚Äúv√©lo‚Äù ‚Üí ‚Äúbike‚Äù unless you start doing synonym/translation stuff. So I‚Äôd just accept that and let **multilingual embeddings handle cross-lang**, and use BM25 for the ‚Äúsame language, exact wording‚Äù wins (which matters a ton in legal).\n\nAlso bilingual docs: don‚Äôt route by *document* language, route by **chunk**. Chunk it, detect lang per chunk, store `chunk_lang`. Then BM25 mostly hits same-lang chunks, dense can still pull cross-lang if needed.\n\nRe: translate everything to English‚Ä¶ yeah no thx for legal. If you ever need translation, do it only as a *query expansion* (translate the query to a couple langs) not the whole corpus.\n\nGiven your Scaleway constraints, honestly **OpenSearch** for the text side is a pretty sane pick. Qdrant BM25 is nice but once you want analyzers/highlighting/etc you‚Äôll end up in search-engine land anyway.\n\nKeep rerank small (top 20‚Äì50) and do embeddings async on ingestion so you‚Äôre not ‚ÄúGPU all day‚Äù üî•.",
          "score": 1,
          "created_utc": "2026-01-29 13:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f4ib9",
          "author": "proxima_centauri05",
          "text": "We‚Äôve been dealing with something similar. What‚Äôs worked reasonably well for us is keeping raw text untouched and making embeddings fully configurable, so we can switch to multilingual models and re embed without changing the pipeline. That already handles cross language semantic recall better than expected, even for mixed language documents.\n\nHybrid is where it gets tricky for legal. Per language BM25 felt correct in theory but added a lot of operational complexity, especially with bilingual docs. Dense only was simpler but missed exact phrasing cases, so reranking became important.\n\nWe‚Äôve avoided translation entirely because original wording matters too much in legal. Curious to hear how others balance keyword precision vs complexity in production.",
          "score": 1,
          "created_utc": "2026-01-29 14:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g2ri1",
          "author": "Dry_Substance_5124",
          "text": "Given your constraints (small team, cost-conscious, multi-tenant), I‚Äôd do: multilingual dense embeddings + reranker, and add a lightweight language-agnostic lexical layer (no stemming) for exact strings, citations, clause numbers, names. Per-language BM25 is ''correct'' but operationally annoying. For bilingual docs, index at the chunk level with language tags. I‚Äôve seen AI Lawyer-style systems win by keeping retrieval simple and spending effort on chunking + eval, not over-building the index zoo.",
          "score": 1,
          "created_utc": "2026-01-29 17:29:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpla70",
      "title": "A framework to evaluate RAG answers in production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "author": "esp_py",
      "created_utc": "2026-01-28 19:26:55",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 0.89,
      "text": " How do you know your RAG system is sending correct answers to users? A\n\n\n\nFollowing a recent discussion[ i had here](https://www.reddit.com/r/datascience/comments/1o5n86i/in_production_how_do_you_evaluate_the_quality_of/), I went ahead and  developed a waterfall evaluation framework designed to fail safely and detect hallucinations. \n\nKey components: \n\n\\- Pre-generation retrieval checks \n\n\\- Answerability validation \n\n\\- Faithfulness scoring (NLI, RAGAS, LLM-as-judge) \n\n\\- Answer relevance checks \n\n\n\n[https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach](https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach)\n\nPlease have a read and let me your thoughs, I will share the results soon in the second part.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2a4j6q",
          "author": "No_Kick7086",
          "text": "Nice work, really good article that and covers an area I am looking to address with my own SME+ Rag system. Im curious about the answerability checks., was the classifier or llm more accurate? Im taking a guess at classifier. Look forward to part 2 on this with your results! Cheers",
          "score": 2,
          "created_utc": "2026-01-28 20:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a4tdw",
              "author": "esp_py",
              "text": "On the answerability check I just tried the LLM and it work fine",
              "score": 2,
              "created_utc": "2026-01-28 20:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2so7ux",
                  "author": "Sweet_Access_9996",
                  "text": "Interesting! Did you find any specific limitations or advantages using the LLM for answerability checks compared to a traditional classifier? Would love to hear more about your experience.",
                  "score": 2,
                  "created_utc": "2026-01-31 14:46:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ahg08",
                  "author": "seomonstar",
                  "text": "nice to know",
                  "score": 1,
                  "created_utc": "2026-01-28 21:18:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d01rd",
          "author": "Charming_Group_2950",
          "text": "TrustifAI also seems to solve the same problem with little different approach. It gives a trust score along with explanations for correct or hallucinated responses. Explore more here:¬†https://github.com/Aaryanverma/trustifai",
          "score": 2,
          "created_utc": "2026-01-29 05:27:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e7w6h",
              "author": "No_Kick7086",
              "text": "Looks good. How are the results from it?",
              "score": 1,
              "created_utc": "2026-01-29 11:45:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e9qhx",
                  "author": "Charming_Group_2950",
                  "text": "Benchmarking in progress. You can see the results here in readme of repo. So far results are promising.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bl4r2",
          "author": "Able-Let-1399",
          "text": "Thanks, good stuff when learnings from real projects are shared üëç. Looking forward to next part.\nI wonder: You must have been warned / read about not asking LLM as a judge to validate multiple aspects in the same prompt, so why do you do that?",
          "score": 1,
          "created_utc": "2026-01-29 00:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l23jr",
              "author": "esp_py",
              "text": "Yes, that is a good point! For the LLM as a judge, I didn't get the choice of the prompt, I used an internal package that have a predefined prompt.",
              "score": 1,
              "created_utc": "2026-01-30 11:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq1hly",
      "title": "PDFstract now supports chunking inspection & evaluation for RAG document pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "author": "GritSar",
      "created_utc": "2026-01-29 06:55:11",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "I‚Äôve been experimenting with different chunking strategies for RAG pipelines, and one pain point I kept hitting was **not knowing whether a chosen strategy actually makes sense for a given document** before moving on to embeddings and indexing.\n\n\n\nSo I added a **chunking inspection & evaluation feature** to an open-source tool I‚Äôm building called **PDFstract**.\n\n\n\nHow it works:\n\n* You **choose a chunking strategy**\n* PDFstract applies it to your document\n* You can **inspect chunk boundaries, sizes, overlap, and structure**\n* Decide if it fits your use case *before* you spend time and tokens on embeddings\n\n\n\nIt sits as the **first layer in the pipeline**:\n\nExtract ‚Üí Chunk ‚Üí (Embedding coming next)\n\n\n\nI‚Äôm curious how others here validate chunking today:\n\n* Do you tune based on document structure?\n* Or rely on downstream retrieval metrics?\n\nWould love to hear what‚Äôs actually worked in production.\n\nRepo if anyone wants to try it:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qngww9",
      "title": "GraphRAG D√©j√† Vu: Freezing Edges = Graph DB Repeat? (Prod Trade-offs)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "author": "dqj1998",
      "created_utc": "2026-01-26 14:05:21",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "**Update (Jan 27, 2026)**:  \nThanks for the great discussion here in r/RAG! Some highlights from related threads:\n- Determinism & reproducibility as key to relational DB win (echoing why Graph DBs struggled).\n- Real prod experiences: keep graphs deterministic/auditable (e.g., calls/imports/FKs), avoid LLM-guessed edges clutter.\n- Links shared: [DDG preprint](https://zenodo.org/records/18373053) and [RoslynIndexer repo](https://github.com/RusieckiRoland/RoslynIndexer) for deterministic code RAG.\n\n---\n\nr/RAG ‚Äî GraphRAG hype (explicit graphs over vector RAG) feels like 70s graph DBs (IMS/CODASYL): explicit relations won benchmarks but lost to relational cuz upfront assumptions brittle.\n\n**Hype vs Reality**\nLLM infers entities/relations ‚Üí persist edges ‚Üí query traversal. Cool for global search, but edges = ingestion-time guesses ‚Üí bias for new intents.\n\n**Core Brittleness**\nFrom my [r/programming post](https://www.reddit.com/r/programming/comments/1pz6pj3/graphrag_is_just_graph_databases_all_over_again/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): Nodes=facts, edges=guesses. Scoped query-time inference (BM25+vectors+rerank) often better for ambiguous RAG (no freeze).\n\n**Pushback & Predictability**\nComments nailed it: auditable edges > opaque LLMs (prod win). Dynamic rebuilds? Viable, but maintenance cost high vs simple hybrid RAG.\n\nShines: stable domains (regs/code deps). Fails: intent-shifting queries.\n\nMedium breakdown:[Medium friend link](https://medium.com/sisai/graphrags-deja-vu-why-are-we-repeating-the-same-mistakes-f6852f54bde0?sk=2692c642e7dfb19e9d552162462384c4)\n\n\nProd experiences? GraphRAG beat baseline RAG on your corpus (e.g., multi-hop QA, latency)? Hybrid + dynamic graphs? Or stick to rerank?\n\nShare benchmarks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1uoi8i",
          "author": "RolandRu",
          "text": "Really happy I found this Reddit btw ‚Äî the topics here are genuinely interesting and they kind of force you to think things through.\n\nAnd yeah, you‚Äôre right: it really depends.\n\nGraphs *can* be brittle when the edges are basically guessed during ingestion (LLM-inferred relations). You‚Äôre sort of freezing assumptions that may not match what people will ask later.\n\nBut it‚Äôs very use-case dependent. For code, I honestly think a dependency graph is pretty much **non-optional**. Calls/imports/inheritance aren‚Äôt opinions ‚Äî they‚Äôre real structure. Without graph expansion you often end up with random snippets, and vanilla RAG struggles badly with questions like ‚Äúwhere does this start?‚Äù or ‚Äúwhat does this change affect?‚Äù, because you‚Äôre missing the whole call chain.",
          "score": 4,
          "created_utc": "2026-01-26 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xa5pj",
              "author": "dqj1998",
              "text": "I totally agree! Seeing your comment about code dependencies resonated with me, I also believe that code is essentially a set of pre-defined dependency chains, and debugging is about constantly patching the gap between these pre-defined chains and reality.\n\nWhile my thinking is still quite rudimentary, that's precisely why I wanted to discuss it further here.\n\nThank you for sharing your real-world experience! In this era where AI is coding faster and faster, this kind of discussion is really interesting‚Äîdo you think AI might represent a spiral of dependency reduction?",
              "score": 2,
              "created_utc": "2026-01-27 00:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xg881",
                  "author": "RolandRu",
                  "text": "I see it like this: dependencies aren‚Äôt going away, because they‚Äôre basically a consequence of architecture (boundaries, responsibilities, contracts). But AI lowers the cost of doing things ‚Äúthe right way‚Äù ‚Äî it‚Äôs easier to add an adapter, an interface, a test, validation, or split a big chunk into smaller parts, without feeling like you‚Äôre wasting time on repetitive stuff. So you‚Äôre less tempted to cram everything into one file/method ‚Äújust because it‚Äôs faster.‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-27 00:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1upwsy",
          "author": "hhussain-",
          "text": "Determinism is what really makes this paradigm *shake*.  \nYour Reddit post and Medium article are excellent ‚Äî they pinpoint both the possibilities *and* the limitations very clearly.\n\nWhat ultimately made relational databases win wasn‚Äôt just performance, but **determinism and reproducibility enabled by a new computing model**.\n\nI had a similar discussion on [r/Rag post](https://www.reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) about deterministic graphs vs probabilistic vectors/embeddings. That thread, along with related discussions, helped isolate the issue to what seems like a missing graph category.\n\nInteresting to see this resurfacing now. I‚Äôve just published a timestamped preprint defining a *Deterministic Domain Graph (DDG)* category:  \n[https://zenodo.org/records/18373053](https://zenodo.org/records/18373053)\n\nI‚Äôm currently working on a framework to construct DDGs in practice, and early experiments suggest this is feasible even for large real-world codebases.",
          "score": 3,
          "created_utc": "2026-01-26 17:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vrjfq",
              "author": "RolandRu",
              "text": "Thanks for sharing the article ‚Äî honestly pretty interesting read.\n\nThis is actually close to where I ended up while building a **code RAG** system. I‚Äôm trying to keep the *edges* deterministic + auditable (calls/imports/inheritance, ReadsFrom/WritesTo, FKs etc.), and I‚Äôm really trying to avoid freezing ‚ÄúLLM-guessed‚Äù relations during ingestion.\n\nI kind of treat vectors as *ranking / fuzzy recall*, but the graph as the closed-world structure that should rebuild the same way every time. For example I force stable outputs (sorted nodes/edges) and I also add missing TABLE nodes so the SQL graph is actually closed (nodes + edges), not half implicit.\n\nOne thing I‚Äôd highlight though: **heuristics ‚â† inference**. I‚Äôm fine with fixed, testable heuristics (like inline SQL detection) ‚Äî even if it‚Äôs not perfect, it‚Äôs still deterministic and you can regression-test it. What I‚Äôm trying to avoid is context-dependent enrichment that changes depending on the model/prompt or whatever the ‚Äúbest guess‚Äù is this week.\n\nIf you‚Äôre curious, this repo is just the **indexing part** (Roslyn/.NET side). The actual RAG pipeline / retrieval is in a separate project:  \n[https://github.com/RusieckiRoland/RoslynIndexer](https://github.com/RusieckiRoland/RoslynIndexer)\n\nAlso curious how you want to handle schema evolution / versioning for DDGs on big real repos ‚Äî do you version the domain spec per build, kind of like a compiler?",
              "score": 3,
              "created_utc": "2026-01-26 20:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xcvpz",
                  "author": "dqj1998",
                  "text": "Wow, thank you both‚Äîthis thread is gold! \n\nu/hhussain, your DDG preprint looks fascinating‚Äîdeterminism + reproducibility as the key to why relational won makes total sense, and early experiments on large codebases sound promising. Will dive into it right away.\n\nu/RolandRu, super appreciate you sharing your setup and the RoslynIndexer repo! Exactly aligns with what I've been thinking: keep the graph as closed-world, deterministic structure (hard facts like calls/imports/inheritance/ReadsFrom/WritesTo/FKs), and treat vectors as fuzzy recall/ranking. Love the emphasis on heuristics (fixed, testable, regression-friendly) vs. context-dependent LLM guesses‚Äîthat's the brittleness killer.Your approach to forcing stable outputs (sorted nodes/edges) and adding missing nodes for closed structure is smart‚Äîavoids the \"half implicit\" mess. Curious on a couple things:\n\n\\* How do you handle schema evolution/versioning in big repos? Per-build domain spec like a compiler, or something else?\n\n\\* Have you seen measurable wins on those chain-tracing queries (e.g., impact analysis) vs. vanilla RAG?\n\n\\* Any thoughts on hybrid with dynamic rebuilds for evolving code, or is pure deterministic the way?\n\nThis ties perfectly into what I'm exploring next: code itself as frozen presuppositions/dependencies, and debug as closing the gap to reality. If you're open, I'd love to reference/link your repo/preprint in upcoming posts (with credit, of course).\n\nThanks again‚Äîthreads like this are why I love posting here!",
                  "score": 3,
                  "created_utc": "2026-01-27 00:30:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs87ld",
      "title": "Best chunking + embedding strategy for mixed documents converted to Markdown (Docling, FAQs, web data)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "author": "Particular-Gur-1339",
      "created_utc": "2026-01-31 17:19:34",
      "score": 12,
      "num_comments": 7,
      "upvote_ratio": 0.89,
      "text": "Hey folks üëã\nI‚Äôm building a RAG pipeline and could use some advice on chunking and embedding strategies when everything is eventually normalized into Markdown.\n\nCurrent setup\n\nConverting different file types (PDFs, docs, etc.) into Markdown using Docling\nScraping website FAQ pages and storing those as Markdown as well\nEmbedding everything into a vector store for retrieval\n\nStructure of the data\nEach document/page usually has:\nA main heading\nSub-sections under that heading\nMultiple FAQs under each section\nWeb FAQs are often short Q&A pairs\n\nWhat I‚Äôm confused about\nChunking strategy\nShould I chunk by:\nPage\nHeading / sub-heading\nIndividual FAQ (Q + A as one chunk)\n\nHybrid approach (heading context + FAQ chunk)?\n\nChunk size\nFixed token size (for example 300 to 500 tokens)\nOr semantic chunks that vary in size?\nMetadata\n\n\nGoal\nHigh answer accuracy\nAvoid partial or out-of-context answers",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2tpw4u",
          "author": "Curious-Sample6113",
          "text": "Everything depends on your original source. You are on the right path and just have to do a lot of testing.",
          "score": 2,
          "created_utc": "2026-01-31 17:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfmn5",
              "author": "Particular-Gur-1339",
              "text": "What should be my chunking strategy?",
              "score": 1,
              "created_utc": "2026-02-01 02:18:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xc1r6",
                  "author": "Curious-Sample6113",
                  "text": "You optimize your strategy by testing. I don't know what your source looks like. For example: if you are building a legal agent then you will have a series of questions. The ones that fail will reveal the issues with your ingestion. \n\nThere isn't a universal solution. Everything is tailored to the source.",
                  "score": 1,
                  "created_utc": "2026-02-01 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ttf5n",
          "author": "jannemansonh",
          "text": "hit the same wall building doc search workflows... ended up using needle app since it handles the chunking/embedding/rag stuff automatically (has hybrid search built in). you just describe what you need and it builds it vs configuring all the pieces manually",
          "score": 1,
          "created_utc": "2026-01-31 18:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2utz79",
              "author": "Particular-Gur-1339",
              "text": "Didn't get this what is a needle app?",
              "score": 1,
              "created_utc": "2026-01-31 21:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2v26bj",
                  "author": "bwhitts66",
                  "text": "Needle is a tool that streamlines the process of building document search workflows. It automates chunking and embedding so you don‚Äôt have to set everything up manually. It‚Äôs pretty handy if you‚Äôre looking to simplify the RAG pipeline!",
                  "score": 1,
                  "created_utc": "2026-01-31 21:44:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2xli7k",
          "author": "Dapper-Turn-3021",
          "text": "Yeah, you‚Äôre basically on the right track. There‚Äôs no single perfect chunking or embedding strategy, it really depends on your data and what you‚Äôre trying to achieve. Keeping metadata separate is correct you should embed only the actual text content and store the metadata alongside it so you can filter or rank with it later. Embedding metadata usually just adds noise.\nChunking isn‚Äôt fixed either. Sometimes smaller chunks work better, sometimes larger ones, and semantic chunking is often the best option. It‚Äôs normal to tweak your chunking strategy as you see how your retrieval performs.\n\n\nAnd you‚Äôre absolutely right about retrieval. Don‚Äôt rely only on embeddings. Combining embeddings with metadata filtering, keyword or BM25 search, and then adding a reranking step gives much better results in most cases.\n\n\nSo yes, what you described is basically how strong RAG systems are built. we are following same for Zynfo AI to build out chatbot",
          "score": 1,
          "created_utc": "2026-02-01 07:13:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr1suf",
      "title": "Build n8n Automation with RAG and AI Agents ‚Äì Real Story from the Trenches",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "author": "According-Site9848",
      "created_utc": "2026-01-30 10:24:49",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "One of the hardest lessons I learned while building n8n automations with RAG (Retrieval-Augmented Generation) and AI agents is that the problem isn‚Äôt writing workflows its handling real-world chaos. I was helping a mid-sized e-commerce client who sold across Shopify, eBay, and YouTube and the volume of incoming customer questions, order updates and content requests was overwhelming their small team. The breakthrough came when we layered RAG on top of n8n: every new message or order triggers a workflow that first retrieves relevant historical context (past orders, previous customer messages, product FAQs) and then passes it to an AI agent that drafts a response or generates a content snippet. This reduced manual errors drastically and allowed staff to focus on exceptions instead of repetitive tasks. For example, a new Shopify order automatically pulled product specs, checked inventory, created a draft invoice in QuickBooks and even generated a YouTube short highlighting the new product without human intervention. The key insight: start with the simplest reliable automation backbone (parsing inputs ‚Üí enriching via RAG ‚Üí action via AI agents), then expand iteratively. If anyone wants to map their messy multi-platform workflows into a clean, intelligent n8n + RAG setup, I‚Äôm happy to guide and  to help get it running efficiently in real operations.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2nhhpu",
          "author": "Whole-Board4430",
          "text": "Hi i read your post, i am new to RAG. for one of my employers where i'm helping with marketing and implementing basic AI, i am building a RAG agent. For now i just built a simple agent on n8n, 1 workflow to vector the documents into a database, the agent to retrieve it.\n\nWhat i want this agent to be able to do (if possible) is using our business documents + certain books, methods and other data we find important, and transform this working together with a seperate LLM to generate content, help our customers with questions, help our team with their work, onboard new people into the team when nessecarry. Do you mind if i ask you some questions, you now have a basic view of what i am trying to get working",
          "score": 1,
          "created_utc": "2026-01-30 18:46:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmoxj2",
      "title": "Looking for RAG Engineer / AI Partner ‚Äî Real Estate + SMB Automation (Paid Contract, Long-Term Potential)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "author": "TheGloomWalker",
      "created_utc": "2026-01-25 17:12:28",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Hey everyone, I‚Äôm building a small AI services company focused on deploying custom RAG-based systems and internal AI tools for small and mid-sized businesses (starting with real estate automation and an industrial services company).\n\nI currently have infrastructure running (servers, cloud resources, deployment environment) and guaranteed business interest, but I‚Äôm looking to bring on a technical partner or contractor who can help design and implement production-grade RAG pipelines.\n\nWhat I‚Äôm building initially:\n\nReal estate automation use cases:\n\n\t‚Ä¢\tScraping + ingesting foreclosure / distressed property listings\n\n\t‚Ä¢\tStructured document ingestion (county data, CSVs, PDFs)\n\n\t‚Ä¢\tSearch + semantic querying over listings and owner data\n\n\t‚Ä¢\tEmail / outreach workflow integration (CRM-style pipelines)\n\nEnterprise pilot project (industrial company):\n\n\t‚Ä¢\tInternal document RAG (finance, operations, SOPs, contracts)\n\n\t‚Ä¢\tSecure knowledge base assistant for staff\n\n\t‚Ä¢\tRole-based access control\n\n\t‚Ä¢\tData isolation + security-first architecture\n\nThis company i‚Äôm working initial contract with is real (50m valuation), has active operations, and is willing to deploy AI internally across departments. Their IT team is security-focused, so experience with data isolation, permissioning, private vector DBs, and secure API practices is important.\n\nWhat I‚Äôm looking for:\n\nSomeone with experience in:\n\n\t‚Ä¢\tRAG pipelines (LangChain, LlamaIndex, custom pipelines, etc.)\n\n\t‚Ä¢\tVector DBs (Pinecone, Weaviate, Qdrant, FAISS, Chroma)\n\n\t‚Ä¢\tEmbeddings + chunking strategies\n\n\t‚Ä¢\tAPI integration\n\n\t‚Ä¢\tAuth / security best practices\n\n\t‚Ä¢\tCloud deployment (Docker, VPS, AWS/GCP/Hetzner/etc.)\n\n\t‚Ä¢\tBonus: web scraping + ETL pipelines\n\nCompensation:\n\n\t‚Ä¢\tPaid contract work (budget available)\n\n\t‚Ä¢\tOpportunity for ongoing partnership if things go well\n\n\t‚Ä¢\tOpen to milestone-based payments\n\nWhat I‚Äôd like to see from you:\n\n\t‚Ä¢\tBrief background / experience\n\n\t‚Ä¢\tAny demos, repos, or projects you‚Äôve built\n\n\t‚Ä¢\tWhat stack you prefer working with\n\n\t‚Ä¢\tAvailability\n\nIf you‚Äôre interested, DM me or reply here and we can talk details.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1nnecp",
          "author": "DeadPukka",
          "text": "Have a look at [Graphlit](https://www.graphlit.com). We do this as a platform and can help with custom services. \n\nNo need to build this yourself in 2026.",
          "score": 2,
          "created_utc": "2026-01-25 17:49:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqnhe",
          "author": "ampancha",
          "text": "The enterprise pilot is where this gets interesting. Role-based access control in RAG isn't a UI toggle; it has to happen at retrieval time, or users can still surface documents they shouldn't see through indirect queries. Their IT team will ask how you verify that isolation actually holds under adversarial prompts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-01-25 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1odwl7",
          "author": "pk13055",
          "text": "Sounds interesting [pk13055.com](https://pk13055.com)",
          "score": 1,
          "created_utc": "2026-01-25 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwvna",
          "author": "radicalpeaceandlove",
          "text": "I am an AI/ML engineer leaving Optum, would be open to chatting",
          "score": 1,
          "created_utc": "2026-01-26 02:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rnp1d",
          "author": "Fear_ltself",
          "text": "I‚Äôd love to give it a shot, I‚Äôve been working exactly with these programs for over 2 years and think we could get something up and running with what you‚Äôre looking for very quickly.",
          "score": 1,
          "created_utc": "2026-01-26 05:26:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqkah",
          "author": "BallinwithPaint",
          "text": "Hey, saw your post. This is right up my alley. I specialize in building the exact kind of autonomous RAG and automation pipelines you're describing. My experience includes projects involving live web scraping for data ingestion and building secure, production-grade agentic systems.\n\n\n\nSending you a DM with my portfolio and more details now.",
          "score": 0,
          "created_utc": "2026-01-25 18:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qo9u4g",
      "title": "I'm looking for an OCR for my RAG.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "author": "AdministrationPure45",
      "created_utc": "2026-01-27 10:04:44",
      "score": 9,
      "num_comments": 31,
      "upvote_ratio": 0.92,
      "text": "Which one do you think is the best among:   \nMistral OCR, LightOnOCR-2, Chandra, OlmOCR 2, Dolphin-v2, LlamaParse, Reducto, Qwen2.5-VL-8B, or DeepSeekOCR?\n\nWhich one do you use? Thanks ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o224zfc",
          "author": "maniac_runner",
          "text": "llmwhisperer if you want to parse complex tables in documents",
          "score": 5,
          "created_utc": "2026-01-27 18:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2009jp",
          "author": "CapitalShake3085",
          "text": "Qwen3 vl",
          "score": 5,
          "created_utc": "2026-01-27 11:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr0q1",
          "author": "zzriyansh",
          "text": "use pymupdf4llm + layout + pro",
          "score": 4,
          "created_utc": "2026-01-27 10:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o201f6p",
          "author": "roydotai",
          "text": "DeepSeekOCR v2 came out recently. I haven‚Äôt gotten around to test it myself yet, but it looked interesting",
          "score": 4,
          "created_utc": "2026-01-27 11:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zzn59",
          "author": "hashiromer",
          "text": "Go with LlamaParse or Reducto if budget allows. I benchmark various PDF parsing solutions for work and LlamaParse and Reducto are far ahead of everything else. However, i only tested at highest compute so ymmv.\n\nYou can also directly use Gemini 3 Flash to convert PDFs to markdown directly with a prompt but you will need to write some basic logic like splitting pages and convert each separately. Gemini on its own beats specialized pipeline based OCRs solutions easily on my internal benchmarks.\n\nIt also depends on complexity of layout by the way, if you are dealing with simple layouts, simpler pipeline based tools like docling,minerU,marker could also work very well.",
          "score": 3,
          "created_utc": "2026-01-27 11:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20aoiw",
              "author": "nofuture09",
              "text": "why reducto? isnt llamaparse better and cheaper?",
              "score": 1,
              "created_utc": "2026-01-27 12:58:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20r6yd",
                  "author": "hashiromer",
                  "text": "Reducto was cheaper last i tested and faster but LlamaParse was slightly more accurate.",
                  "score": 1,
                  "created_utc": "2026-01-27 14:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o202dir",
          "author": "SiebenZwerg",
          "text": "We use Mistral OCR due to high accuracy.",
          "score": 3,
          "created_utc": "2026-01-27 12:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20gus1",
          "author": "AloneSYD",
          "text": "I like a very underrated OCR model nanonets/Nanonets-OCR2-3B FP8 quantized",
          "score": 3,
          "created_utc": "2026-01-27 13:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr1pz",
          "author": "fabkosta",
          "text": "‚ÄúBest‚Äù depends on your data. You need to do a PoC to find out. Also look at Docling, it‚Äôs free.",
          "score": 2,
          "created_utc": "2026-01-27 10:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zt402",
          "author": "Live-Guitar-8661",
          "text": "We use Llama-4-17b with Poppler, probably not the popular choice out there but works for us. Give it a shot if you want to see how well it works: https://orchata.ai\n\nPS- not giving you the link as a plug, just saying if you want to see how they work, it‚Äôs free and you can get a sense of the output in the dashboard. Hope that helps.",
          "score": 2,
          "created_utc": "2026-01-27 10:47:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zbll",
          "author": "it_and_webdev",
          "text": "Docling",
          "score": 2,
          "created_utc": "2026-01-27 15:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o269e9z",
          "author": "sirebral",
          "text": "Smallest Qwen 3 VL variant is wonderful at this task, even with a four bit quant.",
          "score": 2,
          "created_utc": "2026-01-28 07:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zu2u4",
          "author": "instantlybanned",
          "text": "Depends a little on what you need it for (document ocr, read text in images etc.) but for just general purpose OCR on images, Paddle's PP-OCRv5 is probably the state-of-the-art model out there at the moment.",
          "score": 1,
          "created_utc": "2026-01-27 10:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o200kzs",
          "author": "teroknor92",
          "text": "ParseExtract works well for complex documents with tables, handwriting etc. The pricing is very friendly with good ocr and data extraction accuracy.",
          "score": 1,
          "created_utc": "2026-01-27 11:48:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21fnkx",
          "author": "Rich-Emu-1561",
          "text": "An OCR API could help that. I have been using qoest for developers platform for similar document processing. You can check their site to see if it fit your setup.",
          "score": 1,
          "created_utc": "2026-01-27 16:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228kmn",
          "author": "zmanning",
          "text": "[https://99franklin.github.io/ocrbench\\_v2/](https://99franklin.github.io/ocrbench_v2/)",
          "score": 1,
          "created_utc": "2026-01-27 18:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23qx01",
          "author": "No_Thing8294",
          "text": "What do you want to achieve? Qwen-VL is totally different to Deepseek-OCR. Totally different approaches.",
          "score": 1,
          "created_utc": "2026-01-27 22:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23r5r4",
              "author": "No_Thing8294",
              "text": "‚Ä¶and it depends on the kind of input data. Text? Tables, complex PDFs etc.?",
              "score": 1,
              "created_utc": "2026-01-27 22:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25bgby",
          "author": "FrozenBuffalo25",
          "text": "What‚Äôs your hardware?",
          "score": 1,
          "created_utc": "2026-01-28 03:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264vwh",
          "author": "DressMetal",
          "text": "Do you need a local model? Otherwise, Gemini 2.5 flash lite is great at this and it costs next to nothing.",
          "score": 1,
          "created_utc": "2026-01-28 06:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jawc",
          "author": "Ch3mCat",
          "text": "Colqwen 2.5 ?",
          "score": 1,
          "created_utc": "2026-01-28 13:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vumt",
          "author": "Independent-Cost-971",
          "text": "Try using Kudra AI it is very good at complex doc extraction you can even pick what you want to extract",
          "score": 1,
          "created_utc": "2026-01-28 14:24:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsbnr",
          "author": "nnamfuak",
          "text": "Mistral OCR 3 (mistral-ocr-2512)",
          "score": 1,
          "created_utc": "2026-01-29 01:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d5rz5",
          "author": "Whole-Assignment6240",
          "text": "recently i got recommended on mineru. haven't tried yet.",
          "score": 1,
          "created_utc": "2026-01-29 06:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e27ll",
          "author": "patbhakta",
          "text": "OCR has been around for a very long time, way before LLM and AI. What exactly are you extracting? I use 3-4 models at any given time but measuring accuracy they all will fail.",
          "score": 1,
          "created_utc": "2026-01-29 11:00:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ijm4a",
          "author": "Infamous_Ad5702",
          "text": "I made my own tool. Handles PDF‚Äôs well, called it Leonata. Inside I use Apache Tika to handle file conversion. I‚Äôll check what is doing the OCR but it works great.\n\nNo tokens\nNo hallucination \nNo GPU",
          "score": 1,
          "created_utc": "2026-01-30 00:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2q8o0d",
          "author": "Great_Fun7005",
          "text": "Run your own and only use an llm as the final fallback, it‚Äôs cheaper and quicker. PyPDF2 ‚Üí Tesseract ‚Üí LLM of choice will cover most of your needs for free then use the llm as the ultimate fall back. I‚Äôve heard good things about Deepseek and mistral and have had good luck with Gemini 2.5 Flash",
          "score": 1,
          "created_utc": "2026-01-31 03:21:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2295sc",
          "author": "Fresh_Refuse_4987",
          "text": "You can check Reseek. It's an AI tool that automatically extract text from images and PDFs for your RAG pipeline, and uses semantic search so you can find your information easily.",
          "score": 0,
          "created_utc": "2026-01-27 18:26:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoyyjm",
      "title": "We built a knowledge graph from code using AST extractors. Now we're drowning in edge cases. Roast our approach.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "author": "TraditionalDegree333",
      "created_utc": "2026-01-28 02:39:08",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "I'm building a code intelligence platform that answers questions like¬†*\"who owns this service?\"*¬†and¬†*\"what breaks if I change this event format?\"*¬†across 30+ repos.\n\nOur approach: Parse code with tree-sitter AST ‚Üí Extract nodes and relationships ‚Üí Populate Neo4j knowledge graph ‚Üí Query with natural language.\n\nHow It Works:\n\n    Code File\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ tree-sitter AST parse\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Extractors (per file type):\n        ‚îÇ   ‚îú‚îÄ‚îÄ CodeNodeExtractor     ‚Üí File, Class, Function nodes\n        ‚îÇ   ‚îú‚îÄ‚îÄ CommitNodeExtractor   ‚Üí Commit, Person nodes + TOUCHED relationships  \n        ‚îÇ   ‚îú‚îÄ‚îÄ DiExtractor           ‚Üí Spring  ‚Üí INJECTS relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ MessageBrokerExtractor‚Üí Kafka listeners ‚Üí CONSUMES_FROM relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ HttpClientExtractor   ‚Üí RestTemplate calls ‚Üí CALLS_SERVICE\n        ‚îÇ   ‚îî‚îÄ‚îÄ ... 15+ more extractors\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Enrichers (add context):\n        ‚îÇ   ‚îú‚îÄ‚îÄ JavaSemanticEnricher  ‚Üí Classify: Service? Controller? Repository?\n        ‚îÇ   ‚îî‚îÄ‚îÄ ConfigPropertyEnricher‚Üí Link (\"${prop}\") to config files\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ Neo4j batch write (MERGE nodes + relationships)\n\n**The graph we build:**\n\n    (:Person)-[:TOUCHED]->(:Commit)-[:TOUCHED]->(:File)\n    (:File)-[:CONTAINS_CLASS]->(:Class)-[:HAS_METHOD]->(:Function)\n    (:Class)-[:INJECTS]->(:Class)\n    (:Class)-[:PUBLISHES_TO]->(:EventChannel)\n    (:Class)-[:CONSUMES_FROM]->(:EventChannel)\n    (:ConfigFile)-[:DEFINES_PROPERTY]->(:ConfigProperty)\n    (:File)-[:USES_PROPERTY]->(:ConfigProperty)\n\n  \n**The problem we're hitting:**\n\nEvery new framework or pattern = new extractor.\n\n* Customer uses Feign clients? Write FeignExtractor.\n* Uses AWS SQS instead of Kafka? Write SqsExtractor.\n* Uses custom DI framework? Write another extractor.\n* Spring Boot 2 vs 3 annotations differ? Handle both.\n\nWe have 40+ node types and 60+ relationship types now. Each extractor is imperative pattern-matching on AST nodes. It works, but:\n\n1. Maintenance nightmare¬†- Every framework version bump can break extractors\n2. Doesn't generalize¬†- Works for our POC customer, but what about the next customer with different stack?\n3. No semantic understanding¬†- We can extract¬†\\`@KafkaListener\\`but can't answer¬†\"what's our messaging strategy?\"\n\n**Questions:**\n\n1. Anyone built something similar and found a better abstraction?\n2. How do you handle cross-repo relationships? (Config in repo A, code in repo B, deployment values in repo C)\n\n\n\n  \nHappy to share more details or jump on a call. DMs open.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26gra0",
          "author": "Sure_Host_4255",
          "text": "That's what I was exactly was thinking about for spring projects, because mostly you don't need general knowledge, but framework specific. Maybe another step would be to create detailed skill or rule for agent how to use this graph, because for LLM it is just graph, it doesn't know what to do with it and still reads the files to context for it's tasks.\nAs for me even better results was when I wrote spring documentation MCP, then even weak models become stronger.\n\nAnother though, you don't need to support All frameworks, java specific would be enough for commercial product, just choose this niche and keep going, enterprise clients will pay for it.\n\nFor patterns you need to ask LLM to write breaf reviews for graph chains, it could cost 20-100$, depending on repo and model. Also write custom wights algorithms, but it's rather doubtful, because it could work great for 1 project and can harm for another.\nI was participating in similar project for Cobol, and can say you are in the right direction and have similar problems ‚ò∫Ô∏è",
          "score": 1,
          "created_utc": "2026-01-28 08:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26inxl",
              "author": "Sure_Host_4255",
              "text": "So for conclusion:\n1. Write spring docs MCP, it can fetch documentation from GitHub md docs for each version upgrade \n2. Focus just on java frameworks\n3. Ask LLM to create summary for nodes relationship chain",
              "score": 1,
              "created_utc": "2026-01-28 08:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26lewq",
          "author": "TraditionalDegree333",
          "text": "Thank you for the advice, I will explore",
          "score": 1,
          "created_utc": "2026-01-28 08:52:06",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o28aaig",
          "author": "devopstoday",
          "text": "Sounds good approach. Is your project opensource?",
          "score": 1,
          "created_utc": "2026-01-28 15:33:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28fvkv",
              "author": "TraditionalDegree333",
              "text": "No, it‚Äôs not",
              "score": 1,
              "created_utc": "2026-01-28 15:57:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2liwxe",
          "author": "sp3d2orbit",
          "text": "Yeah, I spent years on this problem. You have a nice setup, and this is a valuable problem to solve.\n\n\nI ended up converging on a solution. I don't use standard graph databases anymore. I created a new type of graph database that can store the AST as well as the extracted rules together in one unified format. This helps the inference engine navigate back and forth between code and rules easily. It even helps with abstraction patterns.¬†\n\n\nFor me the atomic unit is a prototype. And a graph of prototypes is called a prototype graph.\n\n\nThen instead of using extractors, I created a general object to graph framework with circular dependency detection and resolution. That lets me call ToPrototype on any object and have it serialized to a prototype graph which can be stored inside the graph database along with the rules.¬†\n\n\nOne of the cool things of this approach is that the inference in learning engines can utilize graphs that contain natural language or code or data all in the same graph structure.",
          "score": 1,
          "created_utc": "2026-01-30 13:17:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp7psa",
      "title": "Compared hallucination detection for RAG: LLM judges vs NLI",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "author": "meedameeda",
      "created_utc": "2026-01-28 10:25:07",
      "score": 8,
      "num_comments": 9,
      "upvote_ratio": 0.85,
      "text": "I looked into different ways to detect hallucinations in RAG. Compared LLM judges, atomic claim verification, and encoder-based NLI.\n\nSome findings:\n\n* LLM judge: 100% accuracy, \\~1.3s latency\n* Atomic claim verification: 100% recall, \\~10.7s latency\n* Encoder-based NLI: \\~91% accuracy, \\~486ms latency (CPU-only)\n\nFor real-time systems, NLI seems like the most reasonable trade-off. \n\nWhat has been your experience with this?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26vl7l",
          "author": "meedameeda",
          "text": "full write up here if interesting: [https://agentset.ai/blog/how-to-detect-hallucinations-in-rag](https://agentset.ai/blog/how-to-detect-hallucinations-in-rag)",
          "score": 4,
          "created_utc": "2026-01-28 10:25:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2d30ki",
              "author": "aiprod",
              "text": "Reading the comment now and seeing you used RAGTruth. It‚Äôs a poor dataset full of errors. Try our modified version linked in my other comment.",
              "score": 1,
              "created_utc": "2026-01-29 05:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2785zr",
          "author": "Upset-Pop1136",
          "text": "we put nli as a fast filter, async llm judge on low-confidence hits, and cache verdicts per doc passage. reduced latency and cost by 70% while keeping recall. try thresholding confidence before invoking expensive checks.",
          "score": 2,
          "created_utc": "2026-01-28 12:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2hvp",
          "author": "aiprod",
          "text": "We tested NLI based detectors like azure groundedness on our Ragtruth++ dataset (https://www.blueguardrails.com/en/blog/ragtruth-plus-plus-enhanced-hallucination-detection-benchmark). And the results were very different. More like 0.35 f1 score.\n\nOur own hallucination detection (agentic verification) scores around 0.8 f1 on the same dataset.\n\nI think your high scores are an indication of a poor quality dataset or some mistakes in the benchmark setup.\n\nHere‚Äôs a video with some numbers for azure and a comparable approach to NLI from scratch (both at 0.35 - 0.45 f1): https://www.blueguardrails.com/en/videos/ragtruth-plus-plus-benchmark-creation",
          "score": 2,
          "created_utc": "2026-01-29 05:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27dxpi",
          "author": "youre__",
          "text": "Seems to have potential if tested for production and applied to certain applications (e.g., where information correctness is a nice-to-have, not a critical requirement).\n\nFrom the test, anything 100% seems fishy. How many samples and what are the error bars after running same test with different seeds? There‚Äôs a ‚Äú66.7%‚Äù precision number in the article, which is oddly clean (2/3), too. Was there a test/validation split with the dataset?\n\nFor hardware testing and comparison, the laptop vs gpt-5 is an interesting comparison. Network latency will be a factor as well as thinking level. So a good test might be to test the NLI over the network, even if on a cloudflare tunnel to simulate cloud. Also test thinking/non-thinking variants of smaller cloud models. This way you can see where the cutoff in performance is. E.g., Can gpt-4o-mini perform just as well as gpt-5 on the dataset? And/Or maybe another cloud hallucination detector?\n\nThis might help ground the comparison and highlight the true benefits against systems people are already using.",
          "score": 1,
          "created_utc": "2026-01-28 12:45:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o290pyn",
          "author": "Financial-Bank2756",
          "text": "Interesting breakdown. These are all post-generation detection, catching hallucinations after the model outputs them. I've been exploring the other side: pre-generation constraint with my project Acatalepsy. which uses\n\n* VIN (Vector Identification Number) ‚Äî constraint operators, not labels\n* ACU (Atomic Claim Unit) ‚Äî immutable identity, mutable confidence\n* Pulse-VIN cycle ‚Äî emission ‚Üí coalescence ‚Üí interrogation ‚Üí sedimentation\n* Confidence vectors ‚Äî multi-axis, decaying, never absolute\n\nhope this helps",
          "score": 1,
          "created_utc": "2026-01-28 17:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eb2pn",
          "author": "Charming_Group_2950",
          "text": "Try TrustifAI. It provides a trust score along with explanations for LLM responses. Explore here:¬†[https://github.com/Aaryanverma/trustifai](https://github.com/Aaryanverma/trustifai)",
          "score": 1,
          "created_utc": "2026-01-29 12:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wedu",
          "author": "ThrowAway516536",
          "text": "I'd take 100% accuracy any day. If you prefer 91% accuracy, I reckon your product and data, where the LLM is integrated, aren't worth much.",
          "score": 1,
          "created_utc": "2026-01-28 10:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26ydem",
              "author": "meedameeda",
              "text": "if latency and cost don‚Äôt matter, 100% accuracy is obviously the right choice (but also depending on your type of production)",
              "score": 2,
              "created_utc": "2026-01-28 10:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqfxko",
      "title": "Tried to Build a Personal AI Memory that Actually Remembers - Need Your Help!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "author": "Tough-Percentage-864",
      "created_utc": "2026-01-29 18:01:20",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "Hey everyone, I was inspired by the **Shark Tank NeoSapien concept**, so I built my own **Eternal Memory system** that doesn‚Äôt just store data - it *evolves with time*.([LinkedIn](https://www.linkedin.com/posts/abhaygupta53_ai-python-eternal-activity-7422690736359415808-PjWd?rcm=ACoAADg6WLoBZEyh7uuClgIZx3hLSD1IzZb81Nc&utm_medium=member_desktop&utm_source=share))\n\nRight now it can:  \n\\-Transcribe audio + remember context  \n\\-  Create **Daily / Weekly / Monthly summaries**  \n\\- Maintain short-term memory that fades into long-term  \n\\- Run **semantic + keyword search** over your entire history\n\nI‚Äôm also working on **GraphRAG for relationship mapping** and **speaker identification** so it knows *who said what*.\n\nI‚Äôm looking for **high-quality conversational / life-log / audio datasets** to stress-test the memory evolution logic.  \n**Does anyone have suggestions? Or example datasets (even just in DataFrame form) I could try?**\n\nExamples of questions I want to answer with a dataset:\n\n* ‚ÄúWhat did I do in **Feb 2024?**‚Äù\n* ‚ÄúWhy was I sad in **March 2024?**‚Äù\n* Anything where a system can actually recall patterns or context over time.\n\nDrop links, dataset names, or even Pandas DataFrame ideas anything helps! üôå\n\n  \n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2g9zjj",
          "author": "Tough-Percentage-864",
          "text": "Repo link --> [https://github.com/Abhay-404/Eternal-Memory](https://github.com/Abhay-404/Eternal-Memory)",
          "score": 2,
          "created_utc": "2026-01-29 18:01:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2iiim8",
          "author": "DetectiveMindless652",
          "text": "Dm me",
          "score": 1,
          "created_utc": "2026-01-30 00:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kqo5a",
              "author": "Tough-Percentage-864",
              "text": "ü§î",
              "score": 1,
              "created_utc": "2026-01-30 09:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ks73z",
                  "author": "DetectiveMindless652",
                  "text": "We‚Äôre working on something adjacent mate",
                  "score": 2,
                  "created_utc": "2026-01-30 09:57:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rt5dr",
          "author": "vikasprogrammer",
          "text": "I am also interested in this, I have started working on the hardware part using this chip Seeed Studio XIAO Nordic nRF52840 Sense Module (chip -> BLE audio -> app -> AI/memory -> output). I want to combine efforts.",
          "score": 1,
          "created_utc": "2026-01-31 11:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2sgx12",
              "author": "Tough-Percentage-864",
              "text": "Cool Keep buliding üöÄ",
              "score": 1,
              "created_utc": "2026-01-31 14:05:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qomak1",
      "title": "Chunking strategies for contracts",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "author": "cat47b",
      "created_utc": "2026-01-27 18:34:39",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all, after seeing a few posts I'm curious as to if anyone has a tried and true favourite method for chunking.\n\nIf I set the scene as we're making an agentic RAG for contracts, ingesting with OCR, carrying out hybrid search (vector+BM25) and a re-ranker. My main concern is the ingestion/chunking process (garbage in, garbage out).\n\nLooking at OCR results I can see that a paragraph may have 1 line that lands on the next page, or that a bullet list goes across 2 pages or that structurally few headings are used.\n\nFor no good reason semantic chunking has stood out as something that makes logical sense as I just want chunks that in isolation make sense, and hopefully side-step the need for overlap etc. \n\nOne downside to this I can imagine however is that a chunk can be semantically complete, but could otherwise refer to an exception clause. Without both you could end up with poor context to feed an LLM when answering a user query.\n\n  \nSo back to the original question, for the stated use case does anyone have a pattern they would use here?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o234f9p",
          "author": "ManufacturerIll6406",
          "text": "I just published an article today on this subject, also included lightweight framework for evaluating various strategies with statistical rigor\n\n[https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)",
          "score": 6,
          "created_utc": "2026-01-27 20:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23j8hk",
              "author": "cat47b",
              "text": "Your post was excellent and I did read it, would I be right in saying if you had to pick one it'd be sentence based chunking?\n\nIn a sense I don't really care about chunk size as accuracy is more important to me",
              "score": 1,
              "created_utc": "2026-01-27 21:50:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o23lt8e",
                  "author": "ManufacturerIll6406",
                  "text": "Thanks! And yes. If you just want max recall and don't care about chunk size, sentence chunking is a safe default. It naturally produces larger chunks, which is exactly why it wins.\n\nThe caveat is cost and latency. Sentence chunking at 1024 config gives you \\~3500 char chunks. At top-k=5, that's 17,500 chars in your LLM context before you've added the prompt. If that's fine for your use case, go for it.\n\nTL;DR: Sentence chunking works. Now you know \\*why\\* it works.",
                  "score": 1,
                  "created_utc": "2026-01-27 22:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o292824",
          "author": "ampancha",
          "text": "Chunking matters, but with an agentic RAG the higher-order risk is what happens when the agent acts on a hallucinated clause or when contract text contains embedded instructions that hijack the workflow. Contracts are untrusted input at scale. Before optimizing retrieval, I'd map the action boundary: what can this agent actually do, and what stops it from doing something wrong? Sent you a DM.",
          "score": 1,
          "created_utc": "2026-01-28 17:34:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}