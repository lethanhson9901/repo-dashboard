{
  "metadata": {
    "last_updated": "2026-02-09 03:01:00",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 139,
    "file_size_bytes": 171298
  },
  "items": [
    {
      "id": "1qvjhp4",
      "title": "What are the best resources for RAG in 2026?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "author": "willjacko1",
      "created_utc": "2026-02-04 08:52:16",
      "score": 100,
      "num_comments": 32,
      "upvote_ratio": 0.98,
      "text": "I've been diving deep into RAG architectures lately and wanted to compile/crowdsource the best resources out there. Here's what I've found so far:\n\n**GitHub Repos to Star:**\n- LangChain / LlamaIndex (obviously, but they've evolved a lot)\n- Ragas - for RAG evaluation metrics\n- Chroma / Weaviate / Qdrant - vector DB options with great docs\n- RAGFlow - end-to-end RAG framework\n- Haystack by deepset\n\n**AI Startups to Watch:**\n- Pinecone (vector search infrastructure)\n- Cohere (embeddings + reranking)\n- ZeroEntropy (SoTA Rerankers & Embeddings)\n- Vectara (RAG-as-a-service)\n- LlamaIndex (now a company, not just OSS)\n\n**Communities:**\n- r/RAG (obviously lol)\n- r/LocalLLaMA (great for self-hosted RAG setups)\n- LangChain Discord\n- Context Engineers Discord\n- MLOps Community\n\n**Learning Resources:**\n- LlamaIndex docs (actually really good tutorials)\n- Pinecone learning center\n- \"Building RAG Applications\" courses popping up everywhere\n\nWhat am I missing? Especially interested in:\n1. Any lesser-known GitHub repos that are actually good?\n2. New startups doing interesting RAG work?\n3. YouTube channels or podcasts focused on RAG?\n\nDrop your favorites below üëá",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3imltg",
          "author": "Informal_Tangerine51",
          "text": "Good resource list, but missing the production angle: when RAG fails, can you debug it?\n\nAll these tools help build RAG systems. The gap: when retrieval is wrong, proving what was retrieved requires more than better embeddings or reranking. You need capture of actual chunks, doc versions, timestamps.\n\nWe use similar stack (LlamaIndex, Chroma, rerankers). When Legal asks \"what docs informed this decision,\" the tools retrieved successfully but we can't verify: were chunks stale, which doc version, why these over others.\n\nResources for building RAG are plentiful. Resources for making RAG auditable are scarce. That's the actual production blocker - not accuracy, but provability.\n\nWhat resources exist for RAG evidence capture and incident replay?",
          "score": 16,
          "created_utc": "2026-02-04 12:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iuapn",
              "author": "aiwithphil",
              "text": "Well said.",
              "score": 3,
              "created_utc": "2026-02-04 13:03:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3plcte",
              "author": "ChlorrOfTheMask",
              "text": "Do you have recommendations on resources or tools for making RAG auditable?",
              "score": 1,
              "created_utc": "2026-02-05 13:09:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o40nauc",
              "author": "__init__i",
              "text": "Refer to graphRAG, lightRAG",
              "score": 1,
              "created_utc": "2026-02-07 02:54:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i152i",
          "author": "bravelogitex",
          "text": "no langchain, it is notorious for bad design",
          "score": 8,
          "created_utc": "2026-02-04 09:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i0h32",
          "author": "I_AM_HYLIAN",
          "text": "Context Engineers Discord: [https://discord.gg/F9VNyJzb](https://discord.gg/F9VNyJzb)",
          "score": 3,
          "created_utc": "2026-02-04 08:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i51gj",
          "author": "pgEdge_Postgres",
          "text": "Shameless self promotion:  \n  \nFor a RAG server that works out-of-the-box with PostgreSQL - [https://github.com/pgEdge/pgedge-rag-server](https://github.com/pgEdge/pgedge-rag-server) \n\nCheck out our repos, there's plenty of open-source tools that you might find helpful, like the [docloader](https://github.com/pgEdge/pgedge-docloader) or [vectorizer](https://github.com/pgEdge/pgedge-vectorizer).\n\nThe creator of the RAG server (and MCP server) wrote up a blog series on building a RAG server with PostgreSQL. It's three parts, here's part 1: [https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content](https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content)",
          "score": 5,
          "created_utc": "2026-02-04 09:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i2mlk",
          "author": "ZwombleZ",
          "text": "RAG is so 2024....\n\n\nContext engineering. Agentic rag.",
          "score": 2,
          "created_utc": "2026-02-04 09:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k74hy",
              "author": "visarga",
              "text": "I prefer [text files with navigable links](https://pastebin.com/VLq4CpCT) between paragraphs. Works with grep, works with coding agents, but unlike RAG it does not have myopic context, links follow where logically needed, not where embedding similarity leads. And this is r/w memory, not r/o like RAG. I don't have any chunking issues either, I don't even need to get very good retrieval from the first move because the agent can explore and follow links along the graph. You know who does the same thing? any coding agent navigating a repo.",
              "score": 5,
              "created_utc": "2026-02-04 17:06:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yl91z",
                  "author": "Single-Constant9518",
                  "text": "Navigating with links sounds interesting! It does seem like a more flexible approach, especially for complex tasks. Have you found any specific use cases where this method really outperformed traditional RAG setups?",
                  "score": 1,
                  "created_utc": "2026-02-06 20:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i9mj1",
          "author": "galdahan9",
          "text": "What about aws bedrock knowledge base?",
          "score": 2,
          "created_utc": "2026-02-04 10:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icamx",
          "author": "Infamous_Ad5702",
          "text": "I skipped rag. KG and indexes for me. No vector.",
          "score": 2,
          "created_utc": "2026-02-04 10:48:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4bnsj6",
              "author": "BarrenLandslide",
              "text": "KG needs to be built with domain knowledge. In production systems a difficult task to achieve without dedicated resources for that purpose. How are you building your KG?\nYou can build without domain knowledge, but then it underperforms a well designed RAG imo.",
              "score": 2,
              "created_utc": "2026-02-08 21:37:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4c37vt",
                  "author": "Infamous_Ad5702",
                  "text": "In building the index it learns the domain perfectly. I believe it well outperforms rag. Test it and let me know..?\n\nFor every new query my tool builds a fresh KG on the fly. So it‚Äôs always context relevant.\n\nNo hallucination. No GPU. No tokens. Offline.",
                  "score": 2,
                  "created_utc": "2026-02-08 22:59:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3mzx9x",
              "author": "cockerspanielhere",
              "text": "What is KG?",
              "score": 1,
              "created_utc": "2026-02-05 01:24:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n20rm",
                  "author": "Infamous_Ad5702",
                  "text": "Apologies. I normally remember not to abbreviate. It is knowledge graph. It helps to make rag much easier. No embedding and chunking when I have my index which builds the knowledge graph.",
                  "score": 1,
                  "created_utc": "2026-02-05 01:36:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i3kme",
          "author": "Morphos91",
          "text": "Use postgresql for easy and free vector storage. \nYou could use ollama for local embedding models",
          "score": 1,
          "created_utc": "2026-02-04 09:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iw162",
          "author": "Striking-Bluejay6155",
          "text": "If you're looking for materials on Graph-based RAG or building knowledge/context graphs, [check this out](https://www.falkordb.com/blog/implementing-agentic-memory-graphiti/)",
          "score": 1,
          "created_utc": "2026-02-04 13:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmvmd",
          "author": "lizziejaeger",
          "text": "Ragie.ai! https://www.ragie.ai/",
          "score": 1,
          "created_utc": "2026-02-04 15:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jqexs",
          "author": "ZepSweden_88",
          "text": "Check out MIT RLM paper, RAG is dead üíÄ. Beyond 16k tokens the models starts to get context rot == build the RLM paper together with any chunking RAG.",
          "score": 1,
          "created_utc": "2026-02-04 15:49:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3km1n3",
          "author": "Hansehart",
          "text": "Check out Langfuse. Its open source and help you to monitor requests and e.g tool usage. Its mandatory in production to understand what happens under the hood and to debug. You can use it on cloud or self hosted. And as other mentioned GraphRAG is superior to VectorRAG. I personally have great experience with Haystack+Langfuse+Neo4J",
          "score": 1,
          "created_utc": "2026-02-04 18:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o2bm1",
          "author": "ReverseBlade",
          "text": "[https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap](https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap)",
          "score": 1,
          "created_utc": "2026-02-05 05:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qyfjb",
          "author": "DeadPukka",
          "text": "Check out [Graphlit](https://www.graphlit.com), build your context layer for AI agents.",
          "score": 1,
          "created_utc": "2026-02-05 17:16:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x04bu",
              "author": "Shot_Platypus4420",
              "text": "Hi! You have a really interesting project. I'd like to test it with my bot and see how it performs. But I'm confused about how to estimate my future costs. :) I can't say I've thoroughly studied the graphlit documentation. :) Perhaps my questions weren't well formulated, but the graphlit bot told me there are no data extraction costs, only indexing costs. I've tried searching the documentation to get a clear picture of the pricing and my future costs, but so far without success. :) Could you please provide links to the information I'm looking for?",
              "score": 2,
              "created_utc": "2026-02-06 15:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xnhar",
                  "author": "DeadPukka",
                  "text": "Thanks for trying it out! We are going to add more cost info to the context of the chatbot - good point. \n\nBut you can think of credits as based on compute, storage, LLM tokens and third party API call (transcription, OCR, etc). \n\nYou‚Äôll pay for effort to ingest into the Graphlit project.  And then pay at retrieval time (much smaller cost). \n\nFeel free to join our Discord (linked in docs) and I‚Äôd be happy to help model out your use case with you. (I‚Äôm kirkm* on Discord)",
                  "score": 2,
                  "created_utc": "2026-02-06 17:21:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rl011",
          "author": "pantoniades",
          "text": "Curious- why is pinecone different from the other vector databases you list?",
          "score": 1,
          "created_utc": "2026-02-05 18:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s87jj",
          "author": "Ok_Constant_9886",
          "text": "Personally recommend deepeval over ragas, everyone from my company switched over",
          "score": 1,
          "created_utc": "2026-02-05 20:49:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44io21",
          "author": "jwy411",
          "text": "RAG is end 2026 bro...",
          "score": 1,
          "created_utc": "2026-02-07 18:53:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46f95u",
          "author": "Dependent_Slide4675",
          "text": "Qdrant fait partie int√©grante du backend IA de mon app et c'est tr√®s simple √† installer (1 seule image docker) et √† utiliser. Les performances sont parmi les meilleures du march√© et vous pouvez le self-host sans probl√®me (ce que je fais). Je recommande. ",
          "score": 1,
          "created_utc": "2026-02-08 01:19:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a1lzt",
          "author": "alexk195de",
          "text": "I'm experimenting with keyword based lookup. First step is a LLM-based summarization task which delivers json structure with entities having: summary, location and keywords. On lookup I first collect keywords (they are embedded using Qwen embedding model). Then i collect entities and then sources. I plan to use it for source code giving Claude better context. Recently i found out, that misspellings on words give bad performance with semantic embeddings. So i've literally vibe coded an embedding algo (https://github.com/Alexk-195/embed\\_misspelling) for individual words which can find misspelled words and plan to integrate this for keyword lookup in parallel to semantic embeddings. ",
          "score": 1,
          "created_utc": "2026-02-08 16:56:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdkek",
      "title": "So is RAG dead now that Claude Cowork exists, or did we just fall for another hype cycle?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "author": "ethanchen20250322",
      "created_utc": "2026-02-05 06:12:54",
      "score": 48,
      "num_comments": 27,
      "upvote_ratio": 0.76,
      "text": "Every few months someone declares RAG is dead and I have to update my resume again.\n\nThis time it's because¬†**Claude Cowork**¬†(and similar long-running agents) can \"remember\" stuff across sessions. No more context window panic. No more \"as I mentioned earlier\" when you definitely did not mention it earlier.\n\nSo naturally: \"Why do we even need RAG anymore??\"\n\nI actually dug into this and... It's not that simple (shocking, I know).\n\nBasically:\n\n* **Agent memory**¬†= remembers what IT was doing (task state)\n* **RAG**¬†= retrieves what THE WORLD knows (external facts)\n\nOne is your agent's personal journal. The other is the company wiki it keeps forgetting exists.\n\n**An agent**¬†with perfect memory but no retrieval is like a coworker who remembers every meeting but never reads the docs. We've all worked with that guy.\n\n**A RAG system**¬†with no memory is like that other coworker who reads everything but forgets what you talked about 5 minutes ago. Also that guy.\n\nTurns out the answer is: stack both. Memory for state, retrieval for facts, vector DB(Like Milvus) underneath.\n\nRAG isn't dead. It just got a roommate who leaves dishes in the sink.\n\nüëâ Full breakdown here if you want the deep dive [https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md](https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md)\n\n**TL;DR:**¬†Claude Cowork's memory is for tracking task state. RAG is for grounding the model in external knowledge. They're complementary, not competitive. We can all calm down (for now).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3oeh9p",
          "author": "stingraycharles",
          "text": "Eh, Claude Projects has been able to do that since forever, they just made that same functionality available in Claude Cowork. \n\nIf anything, this is a validation of RAGs, as this functionality is an implementation of it.",
          "score": 14,
          "created_utc": "2026-02-05 07:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3plwdr",
          "author": "TechnicalGeologist99",
          "text": "This is just a misunderstanding by people on what RAG is. \n\nIt is often conflated with: semantic search, on prem/hybrid/in-house solutions to document search. \n\nNote that RAG systems are a high level category of systems. Namely those that have the concerns:\n\n- Automatic retrieval of information \n- Injection of relevant information into a prompt for Autoregressive Causal Inference \n\n RAG does not mean \"embed and retrieve documents to guide an LLM at query time\"\n\nRather RAG is a broad topic of which \"embed and retrieve documents....\" is a member. \n\nKey word search is also RAG if you put the results into an LLM.\n\nRandom search is also RAG if you put the results into an LLM.\n\nHaving the system send an email to an office worker to ask for information at query time is still RAG (it's just not useful) \n\nRAG is not dead. It is a battle tested framework for designing systems. \n\nWhen people say \"RAG is dead\" to me that is a smell that tells me they've never written a line of code in their life and they are likely one of those jumped up sales people earning 250k per year for selling copilot licenses.",
          "score": 12,
          "created_utc": "2026-02-05 13:12:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sem33",
              "author": "ThirDiamondEye",
              "text": "I came to see people correcting the question, and am surprised I only found you.",
              "score": 2,
              "created_utc": "2026-02-05 21:20:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sljf2",
                  "author": "TechnicalGeologist99",
                  "text": "Tis a pet peeve of mine haha",
                  "score": 1,
                  "created_utc": "2026-02-05 21:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oo7f9",
          "author": "eurydice1727",
          "text": "Rag protects security. On premise solutions where companies do not want data exposed to any cloud models especially.",
          "score": 13,
          "created_utc": "2026-02-05 08:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oc06f",
          "author": "scoby_cat",
          "text": "RAG is not ‚Äúthe world‚Äù though",
          "score": 3,
          "created_utc": "2026-02-05 06:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3olrnb",
          "author": "Diligent-Builder7762",
          "text": "two ways of doing same thing, rag and custom pipelines to understand the codebase / using llm driven agents to do it... not much different, one seem to be more future leaning agentic way, the other efficiency.",
          "score": 2,
          "created_utc": "2026-02-05 08:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pb0ku",
          "author": "HealthOk5149",
          "text": "I don't think RAG will be dead anytime soon. Many companies still trying to figure out how to incorporate RAG solutions in their processes. In medical/law/financial sectors they have tons od data in air-gapped environment (due to security reasons/policies ofc especially in EU), so proper on-premise RAG would be a blessing for them imo.",
          "score": 2,
          "created_utc": "2026-02-05 11:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pwd2x",
          "author": "Diligent-Fly3756",
          "text": "https://preview.redd.it/96zneunsoohg1.jpeg?width=1107&format=pjpg&auto=webp&s=5e01383bb79ca9f81fc09569fe84dda2fefe7e9a\n\nHere‚Äôs what Boris Cherny (who invented Claude Code) said.",
          "score": 2,
          "created_utc": "2026-02-05 14:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d5kxq",
              "author": "sbmitchell",
              "text": "Read as \"we just need to grep shit now\"",
              "score": 1,
              "created_utc": "2026-02-09 02:34:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3t260c",
          "author": "zulrang",
          "text": "It doesn't matter what you label it -- it's all context retrieval.  ",
          "score": 2,
          "created_utc": "2026-02-05 23:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3urpnw",
          "author": "voycey",
          "text": "I swear each time I see something like this I am convinced that no one actually knows what RAG is",
          "score": 2,
          "created_utc": "2026-02-06 05:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3obawp",
          "author": "One_Milk_7025",
          "text": "Good explanation.. rag is not dead and when there is no bm25 similarity or regex doesn't work then when the rag shines.. yes claude regex find will work after 3-4 try because it will guess what can be the word for that particular query..\nA agent need both a rag and a memory.. single memory or rag won't survive solo",
          "score": 3,
          "created_utc": "2026-02-05 06:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pmoox",
              "author": "TechnicalGeologist99",
              "text": "Bm25 is not the opposite of RAG. It is a method of retrieval. Any method of retrieval still fits the RAG framework.\n\nSemantic search is an example of retrieval...it is not the definition of RAG",
              "score": 3,
              "created_utc": "2026-02-05 13:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pneuh",
          "author": "Rock--Lee",
          "text": "Ofcourse it's not dead. I'm building a custom GraphRAG with neo4j which I use as a knowledge base and memory service for my app and agents. I'd like Claude Coworker try to use its memory for multiple books and documents across domains, with user notes in there too.\n\nRAG on itself isn't something you replace. It's a technique and Claude uses RAG, but that doesn't make other RAG systems obsolete. That's like saying should I uninstall Outlook now that Claude can also fetch my mails.",
          "score": 1,
          "created_utc": "2026-02-05 13:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ptv70",
          "author": "Main-Space-3543",
          "text": "Not sure this is the right way to think about RAG.  \n\nThe alternative to RAG is not Cowork or Code.  In fact - Cursor / Anthropic used RAG to power their coding agents at one point.   \n\nAnthropic switched from RAG to agentic search because it‚Äôs allowed them to scale past the limits of RAG.  \n\nAll 3 of these things are worth paying attention and to leverage in the right context:\n\n- increasing context windows \n- RAG\n- agentic search \n\nAll 3 make it possible to improve the context of the query handed to the LLM. \n\nCoding agents / cowork - very different patterns.  \n\nBy the way - the LLM is the representation of the worlds data - not RAG.",
          "score": 1,
          "created_utc": "2026-02-05 13:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk3m0",
          "author": "Wooden_Leek_7258",
          "text": "Its a component not the system. Anyone try blending graph, vector and SQL systems in a layerd approach? Working for me.",
          "score": 1,
          "created_utc": "2026-02-05 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rlwjt",
          "author": "infinitejennifer",
          "text": "lol.",
          "score": 1,
          "created_utc": "2026-02-05 19:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sdpfo",
          "author": "satechguy",
          "text": "RAG is dead, again?",
          "score": 1,
          "created_utc": "2026-02-05 21:15:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t3eb7",
          "author": "CEBarnes",
          "text": "RAG makes using a database more flexible. Like talking to ‚ÄòMother‚Äô in the Alien series. Using AI+skills+api seems better than a fixed UI with functions mapped to buttons.",
          "score": 1,
          "created_utc": "2026-02-05 23:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tg48g",
          "author": "chungyeung",
          "text": "Rag, rag never changes.",
          "score": 1,
          "created_utc": "2026-02-06 00:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x0h49",
          "author": "Challseus",
          "text": "I must be doing rag wrong because every time it‚Äôs dead, it still works for me ü§∑üèæ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-06 15:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xmgv6",
          "author": "jeffreyhuber",
          "text": "Cowork does retrieval augmented generation and search. ",
          "score": 1,
          "created_utc": "2026-02-06 17:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41mf7m",
          "author": "Fantastic_suit143",
          "text": "Woah it didn't know that damn I will try to implement it in my own project now this is revolutionary yeah rag really is that person doesn't remember anything even after reading the context earlier on .",
          "score": 1,
          "created_utc": "2026-02-07 07:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o442mqc",
          "author": "Late_Ad_2350",
          "text": "Incompetency level is overwhelming in all of these AI subs...",
          "score": 1,
          "created_utc": "2026-02-07 17:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o449fmz",
          "author": "Usual-Orange-4180",
          "text": "Is a pattern dead now that here is one product from one company?\n\nNo",
          "score": 1,
          "created_utc": "2026-02-07 18:08:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a6az8",
          "author": "mohdgame",
          "text": "I dont think yoi know what RAG means. How does it have anything to do with claude cowork?",
          "score": 1,
          "created_utc": "2026-02-08 17:19:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxct95",
      "title": "My weekend project just got a $1,500 buyout offer.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "author": "Physical_Badger1281",
      "created_utc": "2026-02-06 09:08:40",
      "score": 48,
      "num_comments": 34,
      "upvote_ratio": 0.85,
      "text": "I built a simple RAG (AI) starter kit 2 months ago.\n\nThe goal was just to help devs scrape websites and PDFs for their AI chatbots without hitting anti-bot walls.\n\nProgress:\n- 10+ Sales (Organic)\n- $0 Ad Spend\n- $1,500 Acquisition Offer received yesterday.\n\nI see a lot of people overthinking their startup ideas. This is just a reminder that \"boring\" developer tools still work. I solved a scraping problem, put up a landing page, and the market responded.\n\nI'm likely going to reject the offer and keep building, but it feels good to know the asset has value.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3vl7fc",
          "author": "Available-Appeal-173",
          "text": "Did you open source it?",
          "score": 3,
          "created_utc": "2026-02-06 10:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vri0e",
              "author": "Physical_Badger1281",
              "text": "It‚Äôs built on open-source tech (Next.js, LangChain, Pinecone), but the repo itself is a paid boilerplate.\n\n\nI decided to sell it as a 'Source-Available' product because the real value isn't the stack, but the 100+ hours of glue-code (specifically the Puppeteer scraping config and Auth setup) that saves you from debugging for weeks.\nI kept the price super low ($5 range) so it‚Äôs accessible to almost anyone who wants to skip the setup.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-06 11:02:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xrwyp",
                  "author": "Playwithme408",
                  "text": "Where did you list it.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:43:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vsick",
          "author": "Joy_Boy_12",
          "text": "I didn't understand the use case.\nI have a chatbot but I scrape the website with docling and insert the data to vector db so it will be available for my chatbot.\n\n\nWhere's the need for your product?",
          "score": 3,
          "created_utc": "2026-02-06 11:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vuh35",
              "author": "Physical_Badger1281",
              "text": "Great question. If you're scraping static documentation or PDFs, tools like Docling or standard parsers work perfectly.\n\nThe specific use case FastRAG solves is Client-Side Rendered (SPA) websites. A lot of modern React/Next.js sites return empty HTML until the JavaScript runs. Standard scrapers often fail there.\n\nFastRAG uses a headless browser instance (Puppeteer) to fully hydrate the DOM before scraping, so you catch the content that only appears after JS execution. It‚Äôs overkill for simple docs, but necessary for a robust 'Chat with Any Website' SaaS.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 6,
              "created_utc": "2026-02-06 11:27:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vy3ns",
              "author": "motorsportlife",
              "text": "Curious if you are you hashing files to ensure you don't scrape the same document twice and duplicate entries in the db",
              "score": 2,
              "created_utc": "2026-02-06 11:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w1kej",
                  "author": "Physical_Badger1281",
                  "text": "Currently, we rely on URL-based upserts to prevent database duplication (same URL = overwrite existing vectors)\n\nHowever, actual Content Hashing is on the roadmap for v1.5. It would definitely save on embedding costs for pages that haven't been updated. \n\nIs there a specific library you prefer for that flow?",
                  "score": 1,
                  "created_utc": "2026-02-06 12:20:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o407acm",
          "author": "finnomo",
          "text": "Launched projects cost at least 10 times more than that. Don't sell.",
          "score": 2,
          "created_utc": "2026-02-07 01:15:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40t1bl",
              "author": "Physical_Badger1281",
              "text": "Thanks for the sanity check! üôå\n\nSometimes when you're deep in the code, you forget how much effort it actually took to get from 'localhost' to 'live URL with Gumroad attached.'\n\nI definitely decided to hold. The validation of the offer was nice, but the asset is worth way more to me as a business.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 2,
              "created_utc": "2026-02-07 03:31:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w8289",
          "author": "StatusFoundation5472",
          "text": "True story",
          "score": 1,
          "created_utc": "2026-02-06 13:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w8fdz",
              "author": "Physical_Badger1281",
              "text": "Facts. Just gotta keep building! üöÄ",
              "score": 1,
              "created_utc": "2026-02-06 13:05:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wcqtf",
          "author": "StatusFoundation5472",
          "text": "I checked out your landing page. A question please. What's your experience with gumroad?",
          "score": 1,
          "created_utc": "2026-02-06 13:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z0uxk",
              "author": "AnxietyPrudent1425",
              "text": "Lemon Squeezy is the way to go. Especially if you sell licenses.",
              "score": 2,
              "created_utc": "2026-02-06 21:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40tefk",
                  "author": "Physical_Badger1281",
                  "text": "Lemon Squeezy, okay I'll surely give it a try.\nThanks!",
                  "score": 2,
                  "created_utc": "2026-02-07 03:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wdpah",
          "author": "Interesting-Town-433",
          "text": "How are you able to run an llm on your site without getting crushed by gpu cost?",
          "score": 1,
          "created_utc": "2026-02-06 13:35:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3weh0l",
              "author": "Physical_Badger1281",
              "text": "Great question. Since this is utilizing OpenAI's API, there are no fixed GPU costs - it's purely pay-as-you-go.\n\nFor the live demo, I default to gpt-4o-mini, which is incredibly cheap. I also have rate limiting set up on Vercel to ensure no one user drains the API credits.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-06 13:40:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wg082",
          "author": "jennylane29",
          "text": "Great tool, wish you good luck!\nHow did you market your solution/landing page? I find getting it out there is particularly tricky.",
          "score": 1,
          "created_utc": "2026-02-06 13:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wj8l7",
              "author": "Physical_Badger1281",
              "text": "It really is the hardest part! For this project, I stuck to a 100% organic 'Build in Public' approach:\n- Reddit: Posting deep-dives on the tech stack (like why I chose Puppeteer over Cheerio) in subreddits like r/Nextjs, r/rag and r/SaaS.\n- Twitter/X: Sharing revenue milestones and 'behind the scenes' screenshots.\nI haven't spent a dime on ads. I think developers just appreciate seeing the code/process rather than a flashy marketing video",
              "score": 3,
              "created_utc": "2026-02-06 14:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xj3wf",
          "author": "TalosStalioux",
          "text": "Congrats dude",
          "score": 1,
          "created_utc": "2026-02-06 17:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xjoq4",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-06 17:03:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o434hzl",
          "author": "Top_Yogurtcloset_258",
          "text": "I might actually buy this, I've been trying to scrape client loaded data for a while using Firecrawl, and it's annoying. Have you used Firecrawl before?",
          "score": 1,
          "created_utc": "2026-02-07 14:43:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o434rek",
              "author": "Physical_Badger1281",
              "text": "No, but you can try the demo\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 14:45:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o436526",
                  "author": "Top_Yogurtcloset_258",
                  "text": "It gave an error when I put the link in the demo\n\nWebsite link I put: [https://partners.naeem.cg.sa/book/eleven-wishes-salon](https://partners.naeem.cg.sa/book/eleven-wishes-salon)  \n\n\nError scraping URL.\n\nError message:  \nAn error occurred with your deployment\n\nFUNCTION\\_INVOCATION\\_TIMEOUT\n\ndxb1::7fw5m-1770475864803-9e9fcb110063",
                  "score": 1,
                  "created_utc": "2026-02-07 14:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4cmf9j",
          "author": "Mary_Avocados",
          "text": "Who and how did you get the offer? Just random email?",
          "score": 1,
          "created_utc": "2026-02-09 00:49:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3z0dlp",
          "author": "AnxietyPrudent1425",
          "text": "You can go bigger. $1500 is barely worth the AI credits and food",
          "score": 1,
          "created_utc": "2026-02-06 21:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40sn10",
              "author": "Physical_Badger1281",
              "text": "100%. That‚Äôs exactly why I turned it down.\n\nIf I just sell a couple of licenses a week, I beat that offer in a few months. I‚Äôd rather own a cash-flowing asset than take a quick $1.5k exit. I'm betting on the long game here!\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 03:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qu5zua",
      "title": "OpenClaw enterprise setup: MCP isn't enough, you need reranking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "author": "Queasy-Tomatillo8028",
      "created_utc": "2026-02-02 20:05:44",
      "score": 47,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "OpenClaw, 145k stars in 10 weeks. Everyone's talking about MCP - how agents dynamically discover tools, decide when to use them, etc.\n\nI connected a local RAG to OpenClaw via MCP. My agent now knows when to search my docs vs use its memory.\n\n**The problem:** it was searching at the right time, but bringing back garbage.\n\n**MCP solves the WHEN, not the WHAT**\n\nMCP is powerful for orchestration:\n\n* Agent discovers tools at runtime\n* Decides on its own when to invoke `query_documents` vs answer directly\n* Stateful session, shared context\n\nBut MCP doesn't care about the quality of what your tool returns. If your RAG brings back 10 chunks and 7 are noise, the agent will still use them.\n\n**MCP = intelligence on WHEN to search** **Context Engineering = intelligence on WHAT goes into the prompt**\n\nBoth need to work together.\n\n**The WHAT: reranking**\n\nMy initial setup: hybrid search (vector + BM25), top 10 chunks, straight into context.\n\nResult: agent found the right docs but cited wrong passages. Context was polluted.\n\nThe fix: **reranking**.\n\nAfter search, a model re-scores chunks by actual relevance. You keep only top 3-5.\n\nI use **ZeroEntropy**. On enterprise content (contracts, specs), it goes from \\~40% precision to \\~85%. Classic cross-encoders (ms-marco, BGE) work for generic stuff, but on technical jargon ZeroEntropy performs better.\n\n**The full flow**\n\n    User query via WhatsApp\n        ‚Üì\n    OpenClaw decides: \"I need to search the docs\" (MCP)\n        ‚Üì\n    My RAG tool receives the query\n        ‚Üì\n    Hybrid search ‚Üí 30 candidates\n        ‚Üì\n    ZeroEntropy reranking ‚Üí top 3\n        ‚Üì\n    Only these 3 chunks enter the context\n        ‚Üì\n    Precise answer with correct citations\n\nAgent is smart about WHEN to search (MCP). Reranking ensures what it brings back is relevant (Context Engineering).\n\n**Stack**\n\n* **MCP server:** custom, exposes `query_documents`\n* **Search:** hybrid vector + BM25, RRF fusion\n* **Reranking:** ZeroEntropy\n* **Vector store:** ChromaDB\n\n**Result**\n\nBefore: agent searched at the right time but answers were approximate.\n\nAfter: WhatsApp query \"gardening obligations in my lease\" ‚Üí 3 sec ‚Üí exact paragraph, page, quote. Accurate.\n\n**The point**\n\nMCP is one building block. Reranking is another.\n\nMost MCP + RAG setups forget reranking. The agent orchestrates well but brings back noise.\n\nContext Engineering = making sure every token entering the prompt deserves its place. Reranking is how you do that on the retrieval side.\n\nShootout to some smart folks i met on this discord server who helped me figuring out a lot of things: [Context Engineering](https://discord.gg/F9VNyJzb)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o38a4qg",
          "author": "-Cubie-",
          "text": "I like rerankers, but is this an ad?",
          "score": 6,
          "created_utc": "2026-02-02 21:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39d8if",
          "author": "Informal_Tangerine51",
          "text": "You solved retrieval quality but not retrieval proof. When the agent cites wrong passage despite reranking, can you replay what was actually in those top 3 chunks?\n\nWe use reranking too. Helps accuracy but doesn't solve the debugging problem. Agent extracts wrong data, we know reranking happened, but can't verify: were those top 3 chunks stale? Did reranking score change between dev and prod? What version of the docs were retrieved?\n\nMCP orchestration plus reranking gives better answers. Still can't answer \"prove what the agent saw at 2:47am on case #4521\" because logs show reranking executed, not what content passed through.\n\nFor WhatsApp queries this works great. For production agents where Legal asks for evidence, the gap is: can you capture and verify the actual retrieved content, not just that retrieval happened?\n\nDoes your setup store the reranked chunks with timestamps for replay, or just return them to the agent?",
          "score": 3,
          "created_utc": "2026-02-03 00:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3afsec",
              "author": "hncvj",
              "text": "I'm 100% with this person. We also do re-ranking in our projects having HIPAA compliance. We have to keep logs of every single thing, even the data that was sent to LLMs, PHI, De-id, Returned from LLMs, re-ranked, pulled from KG or Vector Database. Everything must be logged with timestamps.\n\nHowever, this depends on project to project basis. In other projects where there is no compliance and the final output is workable and is not required to be error free, it's ok to not have logs that deeper.",
              "score": 2,
              "created_utc": "2026-02-03 04:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37terf",
          "author": "Edcoopersound",
          "text": "What's your latency like end-to-end? From WhatsApp message to response.",
          "score": 2,
          "created_utc": "2026-02-02 20:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37tqnf",
              "author": "Queasy-Tomatillo8028",
              "text": "2-3 sec total",
              "score": 1,
              "created_utc": "2026-02-02 20:15:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3828ew",
          "author": "apirateiwasmeanttobe",
          "text": "I think what people often forget is that you can put anything behind an mcp tool definition. The good mcp tools behave like a person, with some sort of agency or reactivity, answering not with a wall of text but with curated and well trimmed context. You want to minimize the amount of output so that you don't deplete the context of the calling agent.",
          "score": 1,
          "created_utc": "2026-02-02 20:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39c8cy",
          "author": "blue-or-brown-keys",
          "text": "At Twig MCP handles  RAG noise via strategies, the Redwood(basic RAG strategy) does not do reranking but Cedar and Cypress do.",
          "score": 1,
          "created_utc": "2026-02-03 00:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b40my",
          "author": "primoco",
          "text": "I‚Äôve been banging my head against the same wall with enterprise RAG for months, and you're spot on. The \"toy\" setups like basic MCP or vanilla LangChain wrappers just fall apart the second you feed them high-density documents.\n\nIn my experience, if you aren't obsessing over the retrieval pipeline before the query even hits the LLM, you're just building a very expensive hallucination machine. A few things I‚Äôve learned the hard way:\n\n1. **Hybrid search is the only way out.** If you rely only on vector embeddings for factual stuff (like specific dates or IDs in a 500-page report), you‚Äôre going to get \"semantic blurring.\" You need BM25 keyword matching running alongside your vectors with a tunable alpha. It‚Äôs the only way to catch those \"needle in a haystack\" moments.\n2. **Rerankers are double-edged swords.** I‚Äôve seen Rerankers actually kill the correct result because the threshold was a hair too tight. Now I just pull a wider window (Top-K 20) and let the reranker sort the Top-5 without hard-filtering. It‚Äôs safer and much more consistent.\n3. **Small chunks > Big chunks.** We moved to 600-char chunks with a decent overlap and the \"contextual precision\" shot up. Big chunks just add too much noise and confuse the model.\n4. **Stop the \"vibe-checks.\"** You can‚Äôt tell if a RAG is good just because the answer \"sounds professional.\" I had to build a full eval pipeline to realize my \"best sounding\" model was actually making up half the citations.\n\nEnterprise RAG isn't about which LLM is smarter, it's about how much you can control the data flow.",
          "score": 1,
          "created_utc": "2026-02-03 08:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4673",
          "author": "Shekher_05",
          "text": "Ad Detected",
          "score": 1,
          "created_utc": "2026-02-03 08:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37utrg",
          "author": "Anth-Virtus",
          "text": "Hey, yeah, MCP alone isn't enough for a good RAG.\nThanks for sharing the discord link, I appreciate it",
          "score": 1,
          "created_utc": "2026-02-02 20:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ecml",
          "author": "LeadingFun1849",
          "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nHelp me improve it; you can find the link here to try it out:\n\nWebsite¬†https://dlovable.daveplanet.com\nCODE :¬†https://github.com/davidmonterocrespo24/DaveLovable",
          "score": 1,
          "created_utc": "2026-02-02 21:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qul1mq",
      "title": "NotebookLM For Teams",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:37",
      "score": 43,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3i198c",
          "author": "bravelogitex",
          "text": "I'll start taking a look tomorrow, thx for sharing",
          "score": 1,
          "created_utc": "2026-02-04 09:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmssq",
          "author": "tsquig",
          "text": "Something similar here. [NotebookLM...but more](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w4jxy",
          "author": "gg223422",
          "text": "Interesting concept. I‚Äôll skim the repo and see how the RAG and connectors are implemented",
          "score": 1,
          "created_utc": "2026-02-06 12:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ax6mb",
          "author": "Otherwise_Wave9374",
          "text": "Cool project. The combination of \"team chat\" + internal sources + an agent that can actually take actions is the sweet spot.\n\nIf you have not already, you might want to think about a permissions model for agent actions (read vs write, connector scopes) plus a way to show citations for every claim to keep trust high.\n\nMore agent design notes here if helpful: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 07:05:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxm3z3",
      "title": "I tested Opus 4.6 for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "author": "midamurat",
      "created_utc": "2026-02-06 16:21:47",
      "score": 34,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "I just finished comparing the new Opus 4.6 in a RAG setup against 11 other models.\n\n  \nThe TL;DR results I saw:\n\n* **Factual QA** king: It hit an 81.2% win rate on factual queries\n* **vs. Opus 4.5:** Massive jump in synthesis capabilities (+387 ELO), it no longer degrades as badly on multi-doc queries\n* **vs. GPT-5.1:** 4.6 is more consistent across the board, but GPT-5.1 still wins on deep, long-form synthesis.\n\nVerdict: I'm making this my default for source-critical RAG where accuracy is more imprtant than verbosity.\n\nHappy to answer questions on the data or methodology!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3xv27r",
          "author": "chearmstrong",
          "text": "Using a top-tier model for answer generation in RAG is often unnecessary.\n\nOnce retrieval quality is high and you have post-retrieval steps (relevance filtering / re-ranking, dedupe, metadata filters, chunk stitching), the generator‚Äôs job is mostly summarise, structure, and stay grounded. A cheaper/faster model is usually sufficient.\n\nCommon best practice we‚Äôve seen work well:\n\n- Spend compute on retrieval quality (chunking, filters, re-ranking), not the final generator.\n- Use a fast default generator, and only escalate to a stronger model when signals suggest it‚Äôs needed (low relevance scores, sparse matches, high ambiguity, multi-doc synthesis).\n- Treat generation as a formatting + synthesis step, not the place to ‚Äúfix‚Äù weak retrieval.\n\nIn other words: if you need a very powerful model to get good answers, that‚Äôs often a retrieval problem, not a generation one.",
          "score": 39,
          "created_utc": "2026-02-06 17:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yswzx",
              "author": "midamurat",
              "text": "that's right, i agree. and in this comparison , models were under fixed retrieval + reranking to keep it fair",
              "score": 0,
              "created_utc": "2026-02-06 20:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xrmzb",
          "author": "One_Milk_7025",
          "text": "What is the factual rag means? If the retrieval is perfect any llm can answer that right? How opus is better ? Multi loop or making hyde method to create hypothetical question? For factual rag llm is not needed if the fact checking and rag pipeline is good enough..",
          "score": 3,
          "created_utc": "2026-02-06 17:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ytrk6",
              "author": "midamurat",
              "text": "for factual rag, scifact dataset was used. in theory, what you say might work but in practice, even with the same docs, models differ (like, some over generalize or hide uncertainty). Opus 4.6 was more conservative meaning it actually sticked closer to the source than others",
              "score": -1,
              "created_utc": "2026-02-06 20:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xwfue",
          "author": "Informal-Resolve-831",
          "text": "So how much better it is really?",
          "score": 2,
          "created_utc": "2026-02-06 18:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ysg1n",
              "author": "midamurat",
              "text": "main gain is multi-doc synthesis which is about +387 ELO vs 4.5, much less degradation when sources overlap. or disagree. \n\n(and elo is score from pairwise model-vs-model comparisons using an LLM judge)",
              "score": 1,
              "created_utc": "2026-02-06 20:40:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43s9dj",
          "author": "my_byte",
          "text": "Imagine using the most expensive, premium frontier model for RAG",
          "score": 1,
          "created_utc": "2026-02-07 16:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xg60a",
          "author": "Legitimate-Leek4235",
          "text": "How about Gemini 3.0 flash ? Want to keep costs low and sacrifice bit on quality. How did you measure this ?",
          "score": 1,
          "created_utc": "2026-02-06 16:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xoyhe",
              "author": "midamurat",
              "text": "gemini 3 flash was also very good when I tested especially in terms of being strong in factual RAG. wrote about that too a while ago: [https://agentset.ai/blog/gemini-3-flash](https://agentset.ai/blog/gemini-3-flash) \n\n  \n",
              "score": 0,
              "created_utc": "2026-02-06 17:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xau2g",
          "author": "midamurat",
          "text": "if interested in detail writeup: [https://agentset.ai/blog/opus-4.6-in-rag](https://agentset.ai/blog/opus-4.6-in-rag)",
          "score": 0,
          "created_utc": "2026-02-06 16:22:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o44wiyl",
              "author": "Single-Constant9518",
              "text": "Nice find! That writeup looks detailed. What specific aspects of the new Opus 4.6 did you find most impressive in your testing?",
              "score": 2,
              "created_utc": "2026-02-07 20:05:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48u6a6",
                  "author": "midamurat",
                  "text": "Thank you!  \nI was impressed by how big of an upgrade there was from Opus 4.5 in multi doc queries. Opus 4.6 is much better when reasoning across many docs. Also, it is noticeably better at *not* over-answering. ",
                  "score": 1,
                  "created_utc": "2026-02-08 12:56:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvy3hv",
      "title": "Context Blindness: A Fundamental Limitation of Vector-Based RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "author": "Diligent-Fly3756",
      "created_utc": "2026-02-04 19:22:45",
      "score": 31,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "**Retrieval-Augmented Generation (RAG)** has become the dominant paradigm for grounding large language models (LLMs) in external knowledge. Among RAG approaches, **vector-based retrieval**‚Äîwhich embeds documents and queries into a shared semantic space and retrieves the most semantically similar chunks‚Äîhas emerged as the de facto standard.\n\nThis dominance is understandable: vector RAG is simple, scalable, and fits naturally into existing information-retrieval pipelines. However, as LLM systems evolve from single-turn question answering toward multi-turn, agentic, and reasoning-driven applications, the limitations of vector-based RAG are becoming increasingly apparent.\n\nMany of these limitations are well known. Others are less discussed, yet far more fundamental. This article argues that **context blindness**, the inability of vector-based retrieval to condition on full conversational and reasoning context, is the most critical limitation of vector-based RAG, and one that fundamentally constrains its role in modern LLM systems.\n\n# Commonly Discussed Limitations of Vector-Based RAG\n\n**The Limitations of Semantic Similarity**\n\nVector-based retrieval assumes that semantic similarity between a query and a passage is a reliable proxy for relevance. This assumption breaks down in two fundamental ways.\n\nFirst, similarity-based retrieval often misses what should be retrieved (false negatives). User queries typically express intent rather than the literal surface form of the supporting evidence, and the information that satisfies the intent is often implicit, procedural, or distributed across multiple parts of a document. As a result, truly relevant evidence may share little semantic overlap with the query and therefore fails to be retrieved by similarity search, creating a **context gap** between what the user is trying to retrieve and what similarity search can represent.\n\nSecond, similarity-based retrieval often returns what should not be retrieved (false positives). Even when retrieved passages appear highly similar to the query, similarity does not guarantee relevance, especially in domain-specific documents such as financial reports, legal contracts, and technical manuals, where many sections share near-identical language but differ in critical details such as numerical thresholds, applicability conditions, definitions, or exceptions. Vector embeddings tend to blur these distinctions, creating **context confusion**: passages that appear relevant in isolation are retrieved despite being incorrect given the actual scope, constraints, or exceptions. In professional and enterprise settings, this failure mode is particularly dangerous because it grounds confident answers in plausible but incorrect evidence.\n\n**The Limitations of Embedding Models**\n\nEmbedding models transform passages into vector representations. However, the input length limits of the embedding model force documents to be split into chunks, disrupting their structure and introducing information discontinuities. Definitions become separated from constraints, tables from explanations, and exceptions from governing rules. Although often cited as the main limitation of vector-based RAG, chunking is better viewed as a secondary consequence of deeper architectural constraints.\n\n# The Under-Discussed Core Problem: Context Blindness\n\nA core limitation of vector-based RAG that is rarely discussed is its **context blindness**: the retrieval query cannot carry the full context that led to the question. In modern LLM applications, queries are rarely standalone. They depend on prior dialogue, intermediate conclusions, implicit assumptions, operational context, and evolving user intent. Yet vector-based retrieval operates on a short, decontextualized query that must be compressed into one or more fixed-length vectors.\n\nThis compression is not incidental ‚Äî it is fundamental. A vector embedding has limited representational capacity: it must collapse rich, structured reasoning context into a dense numerical representation that cannot faithfully preserve dependencies, conditionals, negations, or conversational state. As a result, vector-based retrieval is inherently **context-independent**. Documents are matched against a static semantic representation rather than the full reasoning state of the system. This creates a structural disconnect: the LLM reasons over a long, evolving context, while the vector retriever operates on a minimal, compressed, and flattened signal. In other words, **the LLM reasoner is stateful, while the vector retriever is not.** Even with prompt engineering, query expansion, multi-vector retrieval, or reranking, this mismatch persists, because the limitation lies in the representational bottleneck of vectors themselves. The vector retriever remains blind to the very context that determines what ‚Äúrelevant‚Äù means.\n\n# Paradigm Shift: From Context-Independent Semantic Similarity to Context-Dependent Relevance Classification\n\nThe solution to context blindness is not a better embedding model or a larger vector database, but a change in how retrieval itself is formulated. Instead of treating retrieval as a semantic similarity search performed by an external embedding model, retrieval should be framed as a **relevance classification problem** executed by an LLM that has access to the **full reasoning context**.\n\nIn this formulation, the question is no longer ‚ÄúWhich passages are closest to this query in embedding space?‚Äù, but rather ‚ÄúGiven everything the system knows so far‚Äîuser intent, prior dialogue, assumptions, and constraints‚Äîis this piece of content relevant or not?‚Äù Relevance becomes an explicit decision conditioned on context, rather than an implicit signal derived from vector proximity.\n\nBecause modern LLMs are designed to reason over long, structured context, they are naturally well-suited to this role. Unlike embedding models, which must compress inputs into fixed-length vectors and inevitably discard structure and dependencies, LLM-based relevance classification can directly condition on the entire conversation history and intermediate reasoning steps. As a result, retrieval becomes context-aware and adapts dynamically as the user‚Äôs intent evolves.\n\nThis shift transforms retrieval from a standalone preprocessing step into part of the reasoning loop itself. Instead of operating outside the LLM stack as a static similarity lookup, retrieval becomes tightly coupled with decision-making, enabling RAG systems that scale naturally to multi-turn, agentic, and long-context settings.\n\n# Scaling Relevance Classification via Tree Search\n\nA common concern with context-dependent, relevance-classification-based retrieval is token efficiency. **Naively classifying relevance over the entire knowledge base via brute-force evaluation is token-inefficient and does not scale.** However, token inefficiency is not inherent to relevance-classification-based retrieval; it arises from flat, brute-force evaluation rather than **hierarchical classification**.\n\nIn **PageIndex**, retrieval is implemented as a **hierarchical relevance classification** over document structure (sections ‚Üí pages ‚Üí blocks), where relevance is evaluated top-down and entire subtrees are pruned once a high-level unit is deemed irrelevant. This transforms retrieval from exhaustive enumeration into selective exploration, focusing computation only on promising regions. The intuition resembles systems such as **AlphaGo**, which achieved efficiency not by enumerating all possible moves, but by navigating a large decision tree through learned evaluation and selective expansion. Similarly, PageIndex avoids wasting tokens on irrelevant content, enabling context-conditioned retrieval that is both more accurate and more efficient than flat vector-based RAG pipelines that depend on large candidate sets, reranking, and repeated retrieval calls.\n\n# The Future of RAG\n\nThe rise of frameworks such as **PageIndex** signals a broader shift in the AI stack. As language models become increasingly capable of planning, reasoning, and maintaining long-horizon context, the responsibility for finding relevant information is gradually **moving** **from the database layer to the model layer**.\n\nThis transition is already evident in the coding domain. Agentic tools such as **Claude Code** are moving beyond simple vector lookups toward active codebase exploration: navigating file hierarchies, inspecting symbols, following dependencies, and iteratively refining their search based on intermediate findings. Generic document retrieval is likely to follow the same trajectory. As tasks become more multi-step and context-dependent, passive similarity search increasingly gives way to structured exploration driven by reasoning.\n\nVector databases will continue to have important, well-defined use cases, such as recommendation systems and other settings, where semantic similarity **is the objective**. However, their historical role as the default retrieval layer for LLM-based systems is becoming less clear. As retrieval shifts from similarity matching to context-dependent decision-making, agentic systems increasingly demand mechanisms that can reason, adapt, and operate over structure, rather than relying solely on embedding proximity.\n\nIn this emerging paradigm, retrieval is no longer a passive lookup operation. It becomes an integral part of the model‚Äôs reasoning process: executed by the model, guided by intent, and grounded in context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3l1d0c",
          "author": "Diligent-Fly3756",
          "text": "PageIndex's GitHub Repo: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-02-04 19:23:59",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3l468x",
              "author": "CathyCCCAAAI",
              "text": "Thanks for sharing! ",
              "score": 2,
              "created_utc": "2026-02-04 19:37:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3l3sz7",
          "author": "Pure_Squirrel175",
          "text": "Thx for sharing this, very insightful",
          "score": 2,
          "created_utc": "2026-02-04 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lme7z",
          "author": "trollsmurf",
          "text": "\"the dominant paradigm for grounding large language models (LLMs) in external knowledge\"\n\nRAG is semantic search with LLM summary and is a kludge that's used way too much for things it's not at all suited for.\n\nI propose:\n\nFuture content solutions need to generate its own code for querying / modifying whatever the user requests. No RAG/CAG, no embedding, always working on the whole corpus (including formatting) via generated code and an LLM being used for understanding intent and generating cohesive and human-understandable output. The generated code can in turn use embedding or whatever is needed to get the results requested.",
          "score": 2,
          "created_utc": "2026-02-04 21:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m25mm",
          "author": "Informal_Tangerine51",
          "text": "Context-aware retrieval is interesting but doesn't solve the accountability gap. When relevance classification retrieves wrong documents, can you prove which ones were evaluated and why they scored as relevant?\n\nYour hierarchical approach prunes irrelevant subtrees efficiently. But when an agent makes bad decision based on retrieved context, debugging needs more than \"it classified these as relevant\" - needs the actual classification scores, which documents were considered, what caused pruning at each level.\n\nWe hit this with vector RAG: retrieval happens, model decides, incident occurs, and we can't replay what was actually retrieved or how fresh it was. Context-aware retrieval improves accuracy but doesn't automatically capture decision evidence.\n\nFor production agents where compliance asks \"prove what documents informed this,\" does your system capture classification decisions as verifiable artifacts? Or focus on improving retrieval accuracy without evidence trails?",
          "score": 2,
          "created_utc": "2026-02-04 22:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o7vfs",
              "author": "Wooden_Leek_7258",
              "text": "SQL is the anwser... you just log the sql queries and the resulting document provided to the LLM. I built my system with a 'glass box' audit capacity. Vector RAGs are not meant for retrieval trying to make it work is just going to leave people dissapointed. This is what that 'Agentic Mirror' article on Medium this week is groping around.",
              "score": 2,
              "created_utc": "2026-02-05 06:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ncmo8",
              "author": "iamaiimpala",
              "text": "These are such good points. Audit trails and governance are seriously lacking in a lot of solutions, and are non-negotiable for real enterprise level adoption.",
              "score": 1,
              "created_utc": "2026-02-05 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nyles",
          "author": "Wooden_Leek_7258",
          "text": "FFS people.\n\nTL;DR Install an SQL Index. Make it query the SQL instead of probability matching.",
          "score": 2,
          "created_utc": "2026-02-05 04:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3on088",
          "author": "New_Animator_7710",
          "text": "This really nails the real failure mode of vector RAG: **relevance is context-dependent, similarity is not**. Once you move beyond single-turn QA, embeddings become a lossy compression of intent and state. Treating retrieval as LLM-conditioned relevance classification‚Äînot proximity in vector space‚Äîfeels like the inevitable shift for agentic and multi-turn systems. Vector RAG still has its place, but as a heuristic, not the reasoning layer.",
          "score": 2,
          "created_utc": "2026-02-05 08:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5ilw",
          "author": "reddefcode",
          "text": "A hybrid approach often works better; use vector search for initial broad recall, then apply LLM-based reranking or classification on a filtered candidate set.",
          "score": 1,
          "created_utc": "2026-02-05 05:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3paz4h",
          "author": "relcyoj",
          "text": "If we‚Äôre talking about enterprise corpora with many small-to-medium documents (TB scale), PageIndex is not a drop-in replacement for vector-based retrieval.\n\nVector/BM25 retrieval still wins at the **global routing problem**: quickly narrowing millions of documents down to a manageable candidate set with low latency and cost. Using PageIndex alone would require LLM-driven relevance decisions across too many documents, which doesn‚Äôt scale well in tokens or latency.",
          "score": 1,
          "created_utc": "2026-02-05 11:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vg554",
          "author": "nitinmms1",
          "text": "Well, another important aspect is the data on which embedding models are trained. \n\nIf that data does not contain the domain specific jargon, symbolic equivalence or taxonomy the quality of embeddings will not be good.",
          "score": 1,
          "created_utc": "2026-02-06 09:17:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv17hv",
      "title": "We open-sourced our code that outperforms RAPTOR on multi-hop retrieval",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "author": "captainPigggy",
      "created_utc": "2026-02-03 19:14:03",
      "score": 24,
      "num_comments": 7,
      "upvote_ratio": 0.85,
      "text": "We recently open-sourced a RAG system we built for internal use and figured it might be useful to others working on retrieval-heavy applications.\n\nThere‚Äôs no novel algorithm or research contribution here. The system is built by carefully combining existing techniques:\n\n* RAPTOR-style hierarchical trees\n* Knowledge graphs\n* HyDE query expansion\n* BM25 + dense hybrid search\n* Cohere reranker (this alone gave \\~+9%)\n\nOn benchmarks, it slightly outperforms RAPTOR on multi-hop retrieval (72.89% on MultiHop-RAG) and gets \\~99% retrieval accuracy on SQuAD.\n\nWe focused on making this something you can actually install, run, and modify without stitching together a dozen repos.\n\nWe built this for IncidentFox, where we use it to store and retrieve company and team knowledge. Since retrieval isn‚Äôt our product differentiator, we decided to open-source the RAG layer.\n\nRepo: [https://github.com/incidentfox/OpenRag](https://github.com/incidentfox/OpenRag)  \nWrite-up with details and benchmarks: [https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html](https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html)\n\nHappy to answer questions or hear feedback from folks building RAG systems.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ecql6",
          "author": "DashboardNight",
          "text": "Yeah, the Cohere reranker is really good. Unfortunately it remains a catastrophe with their privacy policy, where they can use anything that you provide. A local reranker may be preferable, or even a LLM-reranker using a local model or a GDPR-compliant provider. Other than that, good stuff!",
          "score": 6,
          "created_utc": "2026-02-03 19:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3elfp7",
              "author": "captainPigggy",
              "text": "good point, let it make this clear in readme",
              "score": 2,
              "created_utc": "2026-02-03 20:24:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ee7ua",
          "author": "Oshden",
          "text": "Amazing OP! Thank you for sharing this with the world at large. I‚Äôm definitely gonna star this repo!",
          "score": 3,
          "created_utc": "2026-02-03 19:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3el9hx",
              "author": "captainPigggy",
              "text": "of course thanks!",
              "score": 2,
              "created_utc": "2026-02-03 20:23:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3exmnn",
          "author": "iLoveSeiko",
          "text": "This is really cool compilation of techniques. Thanks for sharing pal",
          "score": 1,
          "created_utc": "2026-02-03 21:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1hgp",
          "author": "Regular-Forever5876",
          "text": "Thank you sir, will have a look into your implementation üôè",
          "score": 1,
          "created_utc": "2026-02-04 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gz6ty",
          "author": "WorkingOccasion902",
          "text": "Can this implement multi-tenant ?",
          "score": 1,
          "created_utc": "2026-02-04 03:59:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyck5p",
      "title": "My RAG pipeline costs 3x what I budgeted...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qyck5p/my_rag_pipeline_costs_3x_what_i_budgeted/",
      "author": "Potential-Jicama-335",
      "created_utc": "2026-02-07 12:28:11",
      "score": 24,
      "num_comments": 41,
      "upvote_ratio": 0.91,
      "text": "Built¬†a RAG system¬†over¬†internal¬†docs. Picked¬†Claude¬†Sonnet because¬†it seemed¬†like the best quality-to-price ratio based¬†on what I read¬†online. Everything¬†worked¬†great in testing.\n\nThen¬†I looked¬†at the bill¬†after¬†a¬†week¬†of¬†production¬†traffic. Way¬†over¬†budget. Turns¬†out the actual¬†cost¬†per¬†query¬†is¬†way¬†higher than what I estimated¬†from¬†the pricing¬†page. Something¬†about¬†how¬†different¬†models tokenize the¬†same¬†context¬†differently, so¬†my¬†8k¬†token¬†retrieval chunks¬†cost¬†more on¬†some¬†models than others.\n\nNow¬†I need¬†to find¬†a model that gives¬†similar¬†quality but actually fits¬†my budget.\n\nAnyone¬†dealt¬†with this?\n\nEdit: Thanks everyone for your suggestions ! I'm so grateful for this community's help.  \nI ended up trying a few solutions with my team, and we finally tried [openmark.ai](https://www.openmark.ai) like someone mentioned for automated testing, we managed to find models that perform better for every step of the agentic flows, and that are much more cost efficient, with fallbacks if necessary. Hopefully, we don't get any surprises anymore. üôè",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qyck5p/my_rag_pipeline_costs_3x_what_i_budgeted/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o43inv4",
          "author": "pl201",
          "text": "Bottom line, You picked the most expensive model to process your doc. Put this way, you paid CEO salary to do a delivery job. There are many models that is 5x to 10x cheaper but are fully capable to process doc for your RAG.",
          "score": 6,
          "created_utc": "2026-02-07 15:56:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44iu75",
              "author": "Potential-Jicama-335",
              "text": "Thanks for your input. Yes I'm learning this the hard way. Now I'm trying to find a quick way to find the most appropriate model(s) for the job, as you say, not the ceo job, but the most cost efficient 'person' for the job.  \nI'm on it, I found a few services that could be better than manual testing, I'm in the process of trying them out.\n\nThanks again, nice way to put this.",
              "score": 2,
              "created_utc": "2026-02-07 18:54:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42jplo",
          "author": "Fantastic_suit143",
          "text": "I built mine for free",
          "score": 6,
          "created_utc": "2026-02-07 12:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42v9by",
              "author": "Joy_Boy_12",
              "text": "Can you explain your pipeline?",
              "score": 1,
              "created_utc": "2026-02-07 13:51:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o42ygls",
                  "author": "Fantastic_suit143",
                  "text": "1. The Input (Vectorization)  \n\nMy Flask backend captures the user's text question.  \nBGE-M3 Hugging face (Local Embedding Model).  \nwhat happens: The model converts the English sentence into a 1024-dimensional vector, which is a list of numbers. It does not comprehend the text; it transforms it into a mathematical point in space.  \nWhy this choice? Running this locally guarantees under 100 milliseconds latency and no cost since it's on hugging face backend (free plan).  \n\n2. The Retrieval\nThe system searches my database for relevant information.  \nFAISS.  \nFAISS calculates the Cosine Similarity between the user's question vector and the 130,000+ chunks of text in mine database. It retrieves the Top 3-5 chunks that are mathematically closest to the query. FAISS is designed for speed. It can scan millions of vectors in milliseconds, even on a CPU which is available on hugging face free plan.  \n\n3. The Prompt Construction  \nThe system creates a large prompt for the AI.  \nThe Logic: It combines three components:  \nRole: \"You are a strict Singapore Law Expert‚Ä¶‚Äù  \nContext: The 3-5 text chunks retrieved by FAISS.  \nQuery: The user's original question.  \n\n4. The Generation (Te \"triplefailover\" brain)  \n The prompt is sent to a Large Language Model (LLM) to generate a human-like answer.  \nA reliable \"Chain of Command\":  \nPrimary: Gemini 2.0 Flash (Fastest, High Context).  \nFailover 1: If Gemini fails due to rate limit or error, it switches to Arcee AI trinity large preview(via OpenRouter).  \nFailover 2: If that fails, it switches to Llama 3.3 70B (via Groq).  \nThis guarantees 99.9% uptime.\n\n5. The Output\nAction: The text response streams back to the frontend.  \nReact + Framer Motion.\n The answer appears in  my chat window and thats it I hope this enough explanation also I didn't even invest a single cent in this\nIf you found this helpful thank you very much ‚ò∫Ô∏èüòÅ",
                  "score": 11,
                  "created_utc": "2026-02-07 14:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o42z85o",
          "author": "ChapterEquivalent188",
          "text": "why not taking your time and go local ;) https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit",
          "score": 6,
          "created_utc": "2026-02-07 14:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44j84j",
              "author": "Potential-Jicama-335",
              "text": "Thats a nice suggestion thank you. My company is set on using cloud API services though.",
              "score": 1,
              "created_utc": "2026-02-07 18:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42kiru",
          "author": "Kathane37",
          "text": "There is token variation between models + language can have an impact + pay a lot of attention to caching.\nAlso sonnet for a 8k context is overkill, you can check fiction bench or the blog post context rot from chroma db to choose a model according to the context you have to deal with.",
          "score": 3,
          "created_utc": "2026-02-07 12:41:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44kbsp",
              "author": "Potential-Jicama-335",
              "text": "Interesting take. Thank you for your input. ",
              "score": 1,
              "created_utc": "2026-02-07 19:02:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44t80w",
          "author": "primoco",
          "text": "I went full local to avoid exactly this problem\nI built a RAG system (RAG Enterprise) and decided early on to keep everything local, both embeddings and inference, no API costs, no surprises.\nMy setup: local embeddings with EmbeddingGemma, local LLM inference running on my own hardware, zero per-query costs once set up.\nTrade-offs I accepted: upfront hardware cost (I run this on an RTX 5070 Ti), quality might not match top-tier API models, slower inference than API calls, need to manage infrastructure yourself.\nBut the benefits: completely predictable costs, no tokenization surprises, full privacy (important for internal docs), scales with hardware not with usage.\nIf your budget is tight and you have the technical capability, going local might be worth considering, the initial investment pays off quickly if you have decent traffic volume.\nThat said, if you need API-level quality, others here have mentioned GPT-4o-mini and Haiku as cheaper alternatives worth testing, just make sure you test with the actual tokenizer before committing.",
          "score": 2,
          "created_utc": "2026-02-07 19:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42k7k7",
          "author": "Educational_Cup9809",
          "text": "gemini 2.5",
          "score": 1,
          "created_utc": "2026-02-07 12:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42om47",
          "author": "Infamous_Ad5702",
          "text": "Yes. I built a tool to fix my problem. Tokens were expensive and gpu was out of reach.\n\nI build an index first and then each new query I auto build a fresh knowledge graph.\n\nI can add docs whenever I like.\n\nIt is offline\nZero hallucinations.\n\nMy defence client needed it this way. \nCheap for me, offline security for them.",
          "score": 1,
          "created_utc": "2026-02-07 13:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42ymnc",
              "author": "_dakota__",
              "text": "Can you please elaborate how it works offline?",
              "score": 1,
              "created_utc": "2026-02-07 14:11:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44kldd",
              "author": "Potential-Jicama-335",
              "text": "Fancy way to solve that. My team wouldn't create our own solution though, we don't have time to do that unfortunately. Interesting take though. Thank you. ",
              "score": 1,
              "created_utc": "2026-02-07 19:03:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42vsip",
          "author": "Joy_Boy_12",
          "text": "How is your pipeline works?",
          "score": 1,
          "created_utc": "2026-02-07 13:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44kwfy",
              "author": "Potential-Jicama-335",
              "text": "Its a multi step compression/semantic retrieval flow with several agentic steps to achieve a good compression rate. Not easy to describe in a few words, I assume like for lots rag pipelines. ",
              "score": 1,
              "created_utc": "2026-02-07 19:05:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o431fy7",
          "author": "hrishikamath",
          "text": "Your biggest culprit is sonnet lol. It‚Äôs probably the most expensive LLM, I use like qwen253B for my agent RAG and it‚Äôs fast and fine. It‚Äôs even a agentic rag. Link: https://github.com/kamathhrishi/stratalens-ai",
          "score": 1,
          "created_utc": "2026-02-07 14:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jdys",
              "author": "Potential-Jicama-335",
              "text": "Yes I'm learning this the hard way thank you. Now I'm trying to find a service to find the most cost efficient models as soon as possible, in the process of trying some out. \n\nThanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-07 18:57:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44wne1",
                  "author": "hrishikamath",
                  "text": "I think its better to not use a service like openrouter, but rather you benchmark your RAG with different LLM's. If use some service with that you will have unexpected issues in prod. ",
                  "score": 1,
                  "created_utc": "2026-02-07 20:06:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45yxpc",
                  "author": "Superb_Plane2497",
                  "text": "[together.ai](http://together.ai) is good although focused on opensource/openweight models, which are good enough. I have a much smaller corpus but three layers of authoritative source (canonical, expert opinion, user organisation override). I got better retrieval results by splitting the parent chunks into child chunks with a small token count, and embedding those. Being smaller, a cheap encoding model can be used, and they work better with specific queries anyway. But embedding is a one time cost, and this approach uses the child chunks to find and retrieve a parent chunk so it doesn't save actual tokens when answering queries, except that perhaps it does better retrieval which is also an optimisation (my retrieval is a BM25/vectorisation hybrid which some hand-coded domain-specific assistance provided to BM25). [together.ai](http://together.ai) has a list of recommended models which do well. I allow the user to swap to kimi-2.5 which lets them paste/upload images to their prompt ... however, each user has a quota so if they do more expensive things, they exhaust their quota more quickly, which is a thinking outside the box solution, perhaps. ",
                  "score": 1,
                  "created_utc": "2026-02-07 23:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o433olw",
          "author": "st0ut717",
          "text": "Local llm is the way to go.",
          "score": 1,
          "created_utc": "2026-02-07 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43b9gs",
          "author": "ArturoNereu",
          "text": "Hi.\n\nIs the cost you mention only for requests to Opus? \n\nI want to understand if that's the case, because depending on your requirements, you can offload the cost by: using a less capable LLM to provide the answer, but use a better embedding model to perform the search.",
          "score": 1,
          "created_utc": "2026-02-07 15:20:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jidy",
              "author": "Potential-Jicama-335",
              "text": "This is exactly the kind of 'two step' solution I'm looking into currently. You're right ahead of me ! ",
              "score": 1,
              "created_utc": "2026-02-07 18:58:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43t8yl",
          "author": "ampancha",
          "text": "Model swapping treats the symptom. The actual failure mode here is missing spend controls: per-user caps, token limits, and attribution so you can see which queries and users drive cost. Without those, any model will eventually surprise you. Also worth auditing your 8k chunk strategy; retrieval filtering and smarter chunking often cut costs 30 to 50 percent without touching quality. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-07 16:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4462cb",
          "author": "tillemetry",
          "text": "What about local hardware costs?",
          "score": 1,
          "created_utc": "2026-02-07 17:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44cf87",
          "author": "Rent_South",
          "text": "The¬†tokenization¬†cost¬†thing¬†is something¬†most¬†people don't realize¬†until¬†the¬†bill hits. The¬†same¬†retrieval context¬†costs¬†different¬†amounts on¬†different providers¬†because¬†they¬†tokenize differently. The¬†\"price¬†per million¬†tokens\" on¬†pricing¬†pages¬†is misleading for¬†exactly¬†this reason.\n\nBefore¬†switching¬†models¬†in¬†a¬†RAG pipeline, run¬†your¬†actual¬†retrieval prompts through¬†[openmark.ai](https://www.openmark.ai)¬†and¬†compare¬†real¬†cost¬†per¬†query¬†alongside¬†accuracy. You¬†might¬†find a¬†model¬†that matches¬†quality¬†at¬†a fraction¬†of the price¬†just¬†because¬†it tokenizes your¬†document¬†chunks¬†more¬†efficiently.",
          "score": 1,
          "created_utc": "2026-02-07 18:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jybi",
              "author": "Potential-Jicama-335",
              "text": "The tokenization angle is interesting. I looked it up quickly and f it can save me some time I'll actually try the tool, because testing everything manually has been a nightmare and really unpractical. I'll speak about it to my supervisor tomorrow. I've got a few options to consider.  \nThanks for your  input.",
              "score": 1,
              "created_utc": "2026-02-07 19:00:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ll3e",
          "author": "TechnicalGeologist99",
          "text": "We still building wrappers? \n\nIf you don't own your AI inference stack then what will you do when market pressure forces prices up.",
          "score": 1,
          "created_utc": "2026-02-07 19:08:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44mg7t",
              "author": "Potential-Jicama-335",
              "text": "Hopefully I'd have fallbacks to other APIs, different provider brands, and competition will drive the costs down, and if not, I'd just switch provider. \n\nMy team likes the flexibility of cloud services, we're also not a huge company that can afford localized solution easily, not to mention the time to spend on it. \n\nSo Ideally we'd have a few fallback models, and switch if 1 is not cost efficient anymore.   \nI'm actually looking into some solutions I found thanks to this thread to find optimal models to do that on a schedule. \n\nTHank you for your input. ",
              "score": 1,
              "created_utc": "2026-02-07 19:12:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44wywx",
                  "author": "TechnicalGeologist99",
                  "text": "Competition does drive costs down. \n\nBut costs are artificially low and inference providers are not profitable. The costs will be passed on to you. The issue isn't lack of competition, it's the cost to manufacture and maintain GPUs.",
                  "score": 1,
                  "created_utc": "2026-02-07 20:07:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45c5qf",
          "author": "Embarrassed_Reply92",
          "text": "I built mine for free! I do my entire RAG pipeline with a local model. It's beautiful. ",
          "score": 1,
          "created_utc": "2026-02-07 21:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46ze63",
          "author": "New_Advance5606",
          "text": "Welcome to the club. Depends the scale.¬† You can RAG a document or a thousand for free.¬† When you push the magnitude of order to a million, it costs money.¬†¬†",
          "score": 1,
          "created_utc": "2026-02-08 03:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48i8mo",
          "author": "KyjenYes",
          "text": "worst ad I have seen",
          "score": 1,
          "created_utc": "2026-02-08 11:17:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qypa0t",
      "title": "Building a Fully Local RAG Pipeline with Qwen 2.5 and ChromaDB",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qypa0t/building_a_fully_local_rag_pipeline_with_qwen_25/",
      "author": "The_Visionary_Grimmy",
      "created_utc": "2026-02-07 21:00:23",
      "score": 23,
      "num_comments": 3,
      "upvote_ratio": 0.85,
      "text": "I recently wrote a short technical walkthrough on building a **fully local Retrieval-Augmented Generation (RAG) pipeline** using **Qwen-2.5** and **ChromaDB**. The focus is on keeping everything self-hosted (no cloud APIs) and explaining the design choices around embeddings, retrieval, and generation.\n\nArticle:  \n[https://medium.com/@mostaphaelansari/building-a-fully-local-rag-pipeline-with-qwen-2-5-and-chromadb-968eb6abd708](https://medium.com/@mostaphaelansari/building-a-fully-local-rag-pipeline-with-qwen-2-5-and-chromadb-968eb6abd708)\n\nI also put the reference implementation here in case it‚Äôs useful to anyone experimenting with local RAG setups:  \n[https://github.com/mostaphaelansari/Optimization-and-Deployment-of-a-Retrieval-Augmented-Generation-RAG-System-](https://github.com/mostaphaelansari/Optimization-and-Deployment-of-a-Retrieval-Augmented-Generation-RAG-System-)\n\nHappy to hear feedback or discuss trade-offs (latency, embedding choice, scaling, etc.).",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qypa0t/building_a_fully_local_rag_pipeline_with_qwen_25/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o474i1c",
          "author": "the-vague-blur",
          "text": "Super cool! I'll try this out! \nOut of curiosity, what were the specs of your computer?",
          "score": 2,
          "created_utc": "2026-02-08 04:01:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48icdd",
              "author": "The_Visionary_Grimmy",
              "text": "**RTX 4070 desktop GPU** with **12 GB of GDDR6X VRAM**",
              "score": 1,
              "created_utc": "2026-02-08 11:18:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o464gho",
          "author": "Academic_Track_2765",
          "text": "Excellent work! yes this is the way to do it if you have GPU / memory available. ",
          "score": 1,
          "created_utc": "2026-02-08 00:14:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qynrqv",
      "title": "\" Hierarchical Agentic RAG (Knowledge Graph + Vector) & JSON RAG \" running fully offline on GTX 1650 (Scale Vs Speed)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qynrqv/hierarchical_agentic_rag_knowledge_graph_vector/",
      "author": "D_E_V_25",
      "created_utc": "2026-02-07 20:01:06",
      "score": 22,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone, I‚Äôm a 1st-year CSE student. I‚Äôve been obsessing over how to run decent RAG pipelines on my consumer laptop (GTX 1650, 4GB VRAM) without relying on any cloud APIs.\n\n‚ÄãI quickly realized that \"one size fits all\" doesn't work when you have limited VRAM. So I ended up building two completely different RAG architectures for my projects, and I‚Äôd love to get some feedback on them.\n\n‚Äã1. The \" HIERARCHICAL AGENTIC RAG WITH HYBRID SEARCH (VRCTOR SEARCH + KNOWLEDGE GRAPH)\" (WiredBrain)::\n\n‚ÄãThe Goal: Handle massive scale (693k chunks) without crashing my RAM.\n\n‚ÄãThe Problem: Standard HNSW indexes were too RAM-heavy and got slow as the dataset grew.\n\n‚ÄãMy Solution: I built a Hierarchical 3-Address Router. Instead of searching everything, it uses a lightweight classifier to route the query to a specific \"Cluster\" (Domain -> Topic -> Entity) before doing the vector search.\n\n‚ÄãThe Result: It cuts the search space by ~99% instantly. I‚Äôm using pgvector to keep the index on system RAM so my GPU is free for generation.\n\n‚ÄãRepo: https://github.com/pheonix-delta/WiredBrain-Hierarchical-Rag\n\n‚Äã2. The \"Speed Demon\" (Axiom Voice Agent)\n\n‚ÄãThe Goal: <400ms latency for a real-time voice assistant.\n‚ÄãThe Problem: Even the optimized Graph RAG was too slow for a fluid conversation.\n\n‚ÄãMy Solution: I built a pure JSON-based RAG. It bypasses the complex graph lookups and loads a smaller, highly specific context directly into memory for immediate \"reflex\" answers. It‚Äôs strictly for the voice agent where speed > depth.\n\n‚ÄãRepo: https://github.com/pheonix-delta/axiom-voice-agent",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qynrqv/hierarchical_agentic_rag_knowledge_graph_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4646g8",
          "author": "Top_Locksmith_9695",
          "text": "Super interesting! Thanks for sharing! Can you give more details on how you did the domain, topic entity?",
          "score": 1,
          "created_utc": "2026-02-08 00:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46x6lv",
              "author": "D_E_V_25",
              "text": "Thanks a lot for giving time to this üòÅüòÅ\n\nNow as for the topic...\n\nMy architectural is divided into gates in the paper I had posted u will see .. domain tagging and 13 gates named with different topics inside that I first tool 2k+ place holders with different names..\n \nSay if maths then inside that chapters and say each chapter with headings and each headings with a sub headings and problems ..\n\nAlthough maths in rag is always a bad idea but in my case it's working with decent percentage of performance also  and the reasons are because of the architecture...\n\nAs u know with such heavy rag local models can't take it up as they will themselves get out of context ...\n\nSo current working whole state I have installed in the laboratory which i can't publicly disclose to the web .. Sorry for that.. But if I get good response this time I plan to move from 250 gb of data inputs to 1tb one as well....\n\nI have open sources the backend with few files missing because I am in the line of getting pre prints acceptance  and journals as well..\n\nBut even if I don't get a valuable feedback... \n\nComments like yours truly motivates me and I feel truly happy to opensource my whole projects ...\n\nPls give a moment to share that and starred the repo it will boost the visibility...\n\nCurrently I have 500+ clones of my projects but sad to see the visibility algorithm works only when u have stars...\nBut no worries .. I am not hungry for stars or fame...\n\nI know a true community and ppl are loving these.. and that was the whole point",
              "score": 2,
              "created_utc": "2026-02-08 03:12:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qy7dhz",
      "title": "Legaltech for Singapore with RAG (version 2)(open source ‚≠ê)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qy7dhz/legaltech_for_singapore_with_rag_version_2open/",
      "author": "Fantastic_suit143",
      "created_utc": "2026-02-07 07:19:06",
      "score": 19,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nA few Days back, I talked about my pet project, which is a RAG-based search engine on Singaporean laws and acts (Scraping 20,000 pages/sec) with an Apple-inspired user interface.\n\nThis project is open source meaning anyone can use my backend logic but do read the license provided in the GitHub.(Star the repo if you liked it.)\n\nThe community posed some fantastic and challenging questions on answer consistency, complex multi-law queries, and hallucinations. These questions were just incredible. Rather than addressing these questions or issues with patches and fixing them superficially, I decided to revisit the code and refactor significant architectural changes.This version also includes reference to page number of the pdf while answering i have achieved that using metadata while I also building the vector database.\n\nI look forward to sharing with you Version 2.\n\nThe following are specific feedback that I received, and how I went about engineering the solutions:\n\nThe Problem: \"How do you ensure answer quality doesn't drop when the failover switches models?\"\n\nThe Feedback: My back-end has a \"Triple Failover\" system (three models, triple the backups!). I was concerned that moving from a high-end model to a backup model would change the \"answer structure\" or \"personality,\" giving a \"jarring\" effect to the user. The V2 Fix: Model-Specific System Instructions. I have no ability to alter the underlying intelligence of my backup models, so I had to normalize the output of my back-end models. I implemented a dynamic instruction set. If the back-end should fail over to Model B, I inject a specific \"system prompt\" to encourage Model B to conform to the same structure as Model A.\n\n2. The Problem: \"Single queries miss the bigger picture (e.g., 'Starting a business' involves Tax, Labor, AND Banking laws).\"\n\nThe Feedback: A simple semantic search for ‚Äústarting a business‚Äù could yield the Companies Act but completely overlook the Employment Act or Income Tax Act. The V2 Fix: Multi-Query Retrieval (MQR). I decided the cost of computation for MQR was worth it. What we now do is, when you pose an open-ended question, an LLM catches the question and essentially breaks it down into sub-questions that could be ‚ÄúBusiness Registration,‚Äù ‚ÄúCorporate Taxation,‚Äù ‚ÄúHiring Regulations,‚Äù etc. It's a more computationally intensive process, but the depth of the answer is virtually night and day from V1.\n\n3. The Problem: \"Garbage In, Garbage Out (Hallucinations)\n\nThe Feedback: If the search results contain an irrelevant document, the LLM has two choices: either hallucinate an answer or say \"I don't know.\"\nThe V2 Fix: Re-Ranking with Cross-Encoders: I decided to introduce an additional validation layer. Once the initial vector search yields the primary results, the Cross-Encoder model \"reads\" them to ensure that they're indeed relevant to the query before passing them along to the LLM. If they're irrelevant, the results are discarded immediately, greatly reducing the incidence of hallucinations and \"confidently wrong\" answers.\n\n4. The Problem: Agentic Capabilities\n\nAgentic Behavior: I‚Äôve improved the backend logic so that it is less passive. It is moving towards becoming an agent that can interpret the ‚Äúintent‚Äù behind the search terms, not just match words.\n\nVersioning: This is the hardest nut to crack, but I've begun to organize the data to enable versioning in subsequent updates.\n\nTech Stack Recap\n\nFrontend: Apple-inspired minimalist design.\n\nUsing: BGE-M3 as text embedder\n\nBackend: Triple Failover System - 3 AI Models\n\nNew in V2: FAISS + Cross-Encoder Re-ranking + Multi-Query Retrieval. I'm still just a student and learning every day. The project is open source, and I would love it if you could tear it apart again so that I could create Version 3. Links:\n\nGitHub Repo: [https://github.com/adityaprasad-sudo/Explore-Singapore/\n](https://github.com/adityaprasad-sudo/Explore-Singapore/)\nThanks for the user who asked those questions‚Äîyou literally shaped this update!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qy7dhz/legaltech_for_singapore_with_rag_version_2open/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4am8io",
          "author": "Academic_Track_2765",
          "text": "nice! I like that you actually used cross encoder reranking with multi query retrieval. I don't have much time, but I will take a look, you can also implement an llm based classifier with the cross-encoder reranking stage to further improve document routing. ",
          "score": 2,
          "created_utc": "2026-02-08 18:34:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4906sa",
          "author": "FakruddinBaba",
          "text": "Great work, my friend üëè Really impressive execution.\n\nI‚Äôm also a student currently learning AI agents and RAG systems using the LangChain ecosystem, and I‚Äôm at a stage where exploration feels a bit overwhelming‚Äîespecially deciding what skills truly matter for industry-grade systems and how to translate learning into meaningful projects.\nMy goal is to move beyond toy demos and build real-world, production-oriented agents: systems that are reliable, optimistic in decision-making, fault-tolerant, and resilient to errors. I‚Äôm particularly curious about designing agents that handle failures, retries, evaluation, logging/observability, and long-term maintainability‚Äîeven if the project itself is conceptually simple.\n\n\nI‚Äôd love to learn from your experience:\n\nHow did you structure your learning journey?\nWhich skills or concepts gave you the biggest leap toward production-level thinking?\n\n\nWhere did you learn these‚Äîresources, projects, or real-world exposure?\nAnd could you suggest a clear roadmap or realistic industry-style project ideas that actually run end-to-end?\n\n\nYour work is genuinely inspiring, and any guidance would mean a lot. Thanks for sharing your knowledge.",
          "score": 1,
          "created_utc": "2026-02-08 13:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ans3r",
              "author": "Fantastic_suit143",
              "text": "Wonderful comment mate well I was always interested in coding and building something out of it so that it's helpful for anyone and can learn by looking at my code \n1. No I didn't structure my learning journey actually I have a time slot like 10pm to 12am and in that I use reddit and basically just watch some lectures and build my knowledge and share it too!\n2.Well learning python and a little bit of how llm's work what is rag gave me a good boost in my productivity \n3.well the biggest material is youtube it basically learned everything from that and sometimes I used claude and gemini to help me with error in my code.for the real world exposure part for now I just exploring the world as I am still learning and new to these things\n4.i think like building something usefull like instead of just using ai in everything we should prioritize it more on fixing our lives like whose gonna use ai button on a fridge when I just want some food \nSo my suggestion is making something that makes ai not hallucinate and helps out daily lives.\n\nThanks God bless you for the journey ahead!‚ò∫Ô∏è",
              "score": 1,
              "created_utc": "2026-02-08 18:41:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxa4fq",
      "title": "Built a Website Crawler + RAG (fixed it last night üòÖ)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:25:37",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 0.85,
      "text": "I‚Äôm **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff üíª), I thought:  \nEveryone is feeding PDFs‚Ä¶ **why not try something that‚Äôs not PDF ingestion?**\n\nSo I focused on fixing the **real problem ‚Äî crawling quality**.\n\nüîó GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What‚Äôs better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well üëÄ  \nFeedback, suggestions, and ‚≠ês are welcome!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3uy709",
          "author": "Legitimate-Fun7608",
          "text": "the crawler quality insight is spot on. most people underestimate how much bad extraction ruins everything downstream.\n\n  \ncurious - how are you handling duplicate content across pages? (like shared headers/footers that make it through, or pages with similar structure). playwright helps but usually need some fuzzy matching too.",
          "score": 2,
          "created_utc": "2026-02-06 06:32:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v3m2h",
              "author": "Cod3Conjurer",
              "text": "    # Deduplication check\n\n                    import hashlib\n\n                    content_hash = hashlib.md5(copied_text.encode('utf-8')).hexdigest()\n\n                    if content_hash in content_hashes:\n\n                        print(f\"Skipping duplicate content: {clean_url}\")\n\n                        continue\n\n                    content_hashes.add(content_hash)\n\nRight now, I just strict content hashing (MD5) to catch identical pages and URL normalization. I haven't implemented footer-stripping or MinHash yet because the vector store is surprisingly creating good separation between the 'content' chunks and the 'boilerplate' chunks on its own üòÇ  \n",
              "score": 1,
              "created_utc": "2026-02-06 07:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv2ks2",
      "title": "Architecture breakdown: Processing 2GB+ of docs for RAG without OOM errors (Python + Generators)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "author": "jokiruiz",
      "created_utc": "2026-02-03 20:04:05",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "Most RAG tutorials teach you to load a PDF into a list. That works for 5MB, but it crashes when you have 2GB of manuals or logs.\n\nI built a pipeline to handle large-scale ingestion efficiently on a consumer laptop. Here is the architecture I used to solve RAM bottlenecks and API rate limits:\n\n1. **Lazy Loading with Generators:** Instead of `docs = loader.load()`, I implemented a Python Generator (`yield`). This processes one file at a time, keeping RAM usage flat regardless of total dataset size.\n2. **Persistent Storage:** Using ChromaDB in persistent mode (on disk), not in-memory. Index once, query forever.\n3. **Smart Batching:** Sending embeddings in batches of 100 to the API with `tqdm` for monitoring, handling rate limits gracefully.\n4. **Recursive Chunking with Overlap:** Critical for maintaining semantic context across cuts.\n\nI made a full code-along video explaining the implementation line-by-line using Python and LangChain concepts.\n\n[https://youtu.be/QR-jTaHik8k?si=a\\_tfyuvG\\_mam4TEg](https://youtu.be/QR-jTaHik8k?si=a_tfyuvG_mam4TEg)\n\nIf you have questions about the `yield` implementation or the batching logic, ask away!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3et0i0",
          "author": "Oshden",
          "text": "Whoa this is awesome! Thanks for sharing!!!",
          "score": 3,
          "created_utc": "2026-02-03 21:00:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hwbel",
              "author": "jokiruiz",
              "text": "thanks! glad you like it!",
              "score": 1,
              "created_utc": "2026-02-04 08:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qur17q",
      "title": "Best open-source embedding model for a RAG system?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "author": "Public-Air3181",
      "created_utc": "2026-02-03 12:48:20",
      "score": 15,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "I‚Äôm an **entry-level AI engineer**, currently in the training phase of a project, and I could really use some guidance from people who‚Äôve done this in the real world.\n\nRight now, I‚Äôm building a **RAG-based system** focused on **manufacturing units‚Äô rules, acts, and standards** (think compliance documents, safety regulations, SOPs, policy manuals, etc.). The data is mostly **text-heavy, formal, and domain-specific**, not casual conversational data.  \nI‚Äôm at the stage where I need to finalize an **embedding model**, and I‚Äôm specifically looking for:\n\n* **Open-source embedding models**\n* Good performance for **semantic search/retrieval**\n* Works well with **long, structured regulatory text**\n* Practical for real projects (not just benchmarks)\n\nI‚Äôve come across a few options like Sentence Transformers, BGE models, and E5-based embeddings, but I‚Äôm unsure which ones actually perform best in a **RAG setup for industrial or regulatory documents**.\n\nIf you‚Äôve:\n\n* Built a RAG system in production\n* Worked with manufacturing / legal / compliance-heavy data\n* Compared embedding models beyond toy datasets\n\nI‚Äôd love to hear:\n\n* Which embedding model worked best for you and **why**\n* Any pitfalls to avoid (chunking size, dimensionality, multilingual issues, etc.)\n\nAny advice, resources, or real-world experience would be super helpful.  \nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3dh4z3",
          "author": "hrishikamath",
          "text": "Probably just start off with: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2? Then iterate over. The big con of using a bad rag model is that you need to retrieve more chunks and use a cross encoder. I used this and got a like 91% on finance bench. So it‚Äôs still a good starting point, you can optimize later. Repo: https://github.com/kamathhrishi/stratalens-ai (going to update blogpost with latest accuracy)",
          "score": 6,
          "created_utc": "2026-02-03 17:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e608x",
          "author": "thecontentengineer",
          "text": "I have tried ZeroEntropy embeddings they are the best I‚Äôve tried.\n\nYou can find them at https://zeroEntropy.dev \n\nThey were in beta when I first tried them, not sure now.",
          "score": 6,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3evjbx",
              "author": "ghita__",
              "text": "oh! hello, im the founder, thank you for mentioning us! we're indeed planning GA release soon! stay tuned for sota open-weight embeddings :)",
              "score": 7,
              "created_utc": "2026-02-03 21:11:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g85bm",
          "author": "laurentbourrelly",
          "text": "Use the filters on the leaderboard to find precisely what you are looking for.\n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)",
          "score": 3,
          "created_utc": "2026-02-04 01:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f0vwh",
          "author": "Informal_Tangerine51",
          "text": "For compliance docs, embedding quality matters but so does proving what was retrieved. You'll hit this when auditors ask \"what regulation informed this decision?\"\n\nWe use BGE-large for similar formal documents. Works well for semantic search. But when extraction is wrong, embeddings don't help you debug. Vector DB logs show query embedding, not what chunks were returned or if they were current versions.\n\nFor your use case, beyond embedding choice: how will you verify retrieved content later? Manufacturing compliance means \"prove this safety decision used regulation version X dated Y.\" Embeddings find relevant chunks, but you need to capture which chunks, from which doc version, retrieved when.\n\nPractical advice: test BGE-large vs E5-large on your actual compliance docs, not benchmarks. More important: design your RAG to store retrieval decisions (chunk IDs, doc versions, timestamps) not just return results. You'll need that evidence trail.\n\nWhat's your plan for handling doc version control when regulations update?",
          "score": 2,
          "created_utc": "2026-02-03 21:36:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gjqh9",
          "author": "sirebral",
          "text": "I really enjoy the Qwen 3 embedding models.  Even the . 6b is quite nice.",
          "score": 2,
          "created_utc": "2026-02-04 02:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ncji1",
              "author": "Disastrous-Nature269",
              "text": "Can confirm",
              "score": 1,
              "created_utc": "2026-02-05 02:36:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jfqgo",
          "author": "Academic_Track_2765",
          "text": "Hello. You can start with the MiniLM embeddings from sentence transformers, but I would encourage you to use the BGE embeddings (also available via sentence transformers). There is a evaluation dashboard that shows you which embedding models perform best for certain tasks, and you should definitely use it. \n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\nlately for my personal projects I have been using the Qwen .6b with their Qwen .6b reranker for the cross encoder stage and lastly the Qwen 3 Next 80B for synthesis. So far its been great!. I would also suggest you experiment with different embedding models. In my earlier years I did a lot of experimentation with embedding models, and BGE models performed well given they produced better results than the E5/GTR models, but the speed didn't come close to the MiniLM models. I have built many RAG systems in production for very long Health Care related documents, with varying complexity. Your biggest challenges will be handling the PII/PHI data, How to embed different document types, handling complex document structures, like nested tables in PDF files, image references etc, but you can use a vision model / ocr model to handle some of that. If you have questions just IM me directly, and I would be happy to help. ",
          "score": 2,
          "created_utc": "2026-02-04 14:58:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k5heu",
          "author": "lfnovo",
          "text": "Qwen3-embedding works wonders for me.",
          "score": 2,
          "created_utc": "2026-02-04 16:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cgc6m",
          "author": "polandtown",
          "text": "Those are the embedding models I'd use. IMO now you need to make a set of test questions - IMO ask your user group for a curated list of such that fits your use case. Then use such to test against each of the embedding models. Done. \n\nThe challenge here is to get a set of stratified example questions.",
          "score": 1,
          "created_utc": "2026-02-03 14:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gisgr",
          "author": "No_Wrongdoer41",
          "text": "embedding models can really struggle when complex reasoning is required. i have a graphrag approach built into a platform where you can upload the docs and we take care of everything else. id love for you to try it (for free) if you are willing! you can drag and drop the docs and then try out the resulting agent in our web app or via api.",
          "score": 1,
          "created_utc": "2026-02-04 02:23:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzj9j4",
      "title": "From RAG-powered LLMs to autonomous agents: the real AI production stack in 2026",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzj9j4/from_ragpowered_llms_to_autonomous_agents_the/",
      "author": "devasheesh_07",
      "created_utc": "2026-02-08 20:06:03",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.8,
      "text": "Over the last few years, most AI conversations revolved around bigger models, benchmarks, and chatbots.\n\nBut looking ahead to 2026, the real shift isn‚Äôt just smarter LLMs ‚Äî it‚Äôs how AI is actually being used in production. AI is moving from assistive software to something closer to a digital workforce.\n\nHere‚Äôs what‚Äôs standing out to me üëá\n\n1. AI is executing work, not just answering prompts\n\nEnterprises are already running AI-powered, end-to-end workflows for billing, customer support, document processing, reporting, and internal ops.\n\nThis isn‚Äôt experimentation or demos anymore ‚Äî it‚Äôs workflow automation at scale.\n\n\n\n2. RAG is becoming the default architecture\n\nRetrieval-Augmented Generation isn‚Äôt optional if you care about accuracy, grounding, or compliance.\n\nConnecting LLMs to internal documents, databases, and vector stores significantly reduces hallucinations and makes AI usable in regulated environments.\n\n\n\n3. LLM progress is about reasoning, not raw size\n\nModel scale mattered early on, but real value now comes from context management, domain awareness, and multi-step reasoning ‚Äî not just parameter count.\n\n\n\n4. AI agents are the real inflection point\n\nAgentic systems don‚Äôt just generate text ‚Äî they plan, use tools, call APIs, and execute multi-step tasks across systems.\n\nLess ‚Äúchatbot‚Äù, more junior employee with supervision and guardrails.\n\n\n\n5. Enterprise AI has moved into production\n\nWe‚Äôre past pilots and proofs-of-concept. Teams are measuring latency, cost, reliability, and productivity gains ‚Äî and AI is starting to affect real KPIs.\n\n\n\n6. Multimodal and domain-specific models are winning\n\nText-only AI is limiting. The future is multimodal systems that understand documents, images, audio, and structured data ‚Äî often fine-tuned or adapted for specific industries.\n\n7. Governance, trust, and compliance are first-class concerns\n\nAudit trails, explainability, bias mitigation, access controls, and data residency are no longer ‚Äúnice to have‚Äù ‚Äî they‚Äôre required for production AI.\n\n8. Edge AI is quietly becoming important\n\nOn-device and local inference unlock lower latency, better privacy, and reduced cloud costs ‚Äî especially for personal and enterprise use cases.\n\nTo me, the takeaway is simple:\n\nThe AI revolution isn‚Äôt coming ‚Äî it‚Äôs already operational.\n\nThe winners won‚Äôt be the ones chasing hype, but the ones building reliable, grounded, agent-driven systems that actually work.\n\nI wrote a longer breakdown here:\n\n [https://www.loghunts.com/rag-llm-agentic-ai-guide-2026](https://www.loghunts.com/rag-llm-agentic-ai-guide-2026)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzj9j4/from_ragpowered_llms_to_autonomous_agents_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4b8h4j",
          "author": "penguinzb1",
          "text": "The agent production gap is real. The jump from \"this works in my demo\" to \"this runs reliably for 10k users\" is where most teams hit the wall.\n\nWhat you said about measuring latency, cost, and reliability is spot on. The problem is most teams only find out their agents are brittle after users start hitting edge cases in production. By then you're firefighting instead of building.\n\nThe governance piece (point 7) ties directly into this‚Äîyou can't just deploy an agent that \"mostly works\" when it's handling regulated data or making consequential decisions. You need to know how it behaves across thousands of scenarios before it touches real users.\n\nWe've been working on simulating agent workflows at scale before deployment to catch these issues early. Helps surface the difference between \"works on my machine\" and \"works in production under load with real user chaos.\"",
          "score": 2,
          "created_utc": "2026-02-08 20:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cbhrs",
          "author": "ArmOk3290",
          "text": "Latency is also a real constraint once you add retrieval, rerank, and tool calls. People talk about multi step agents, but if each step is 2 to 4 seconds, nobody will use it.",
          "score": 1,
          "created_utc": "2026-02-08 23:48:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyvjt1",
      "title": "Lightrag Graph RAG performing worse than open web UI semantic search with similar setup.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qyvjt1/lightrag_graph_rag_performing_worse_than_open_web/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-02-08 01:27:42",
      "score": 15,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hi folks,\nI‚Äôve set up a Graph RAG pipeline using LightRAG with:\nbge-m3 embeddings\nphi-4 / gpt-oss-20B as LLM\nNeo4j for knowledge graph\nMilvus for semantic search\n\nIn parallel, I‚Äôm also running a simpler setup:\nOpen WebUI\nDocling for document parsing\nbge-m3 + vector search (no graph)\n\nSurprisingly, the plain RAG + Docling setup consistently gives better answers for:\ndocument Q&A\nexplanations\nclause lookup\nsummaries\n\nThe Graph RAG feels more brittle and often misses relevant context.\n\nMy questions:\nIs this a known/expected behavior with Graph RAG?\nDoes Graph RAG usually underperform flat RAG unless the use case is explicitly relational?\nAre people using graphs mainly as rerankers / secondary reasoning layers rather than primary retrieval?\n\nWould love to hear real-world experiences before I invest more time tuning the graph side.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qyvjt1/lightrag_graph_rag_performing_worse_than_open_web/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o46k6l7",
          "author": "sp3d2orbit",
          "text": "This is pretty normal from what I‚Äôve seen.\n\nFor document Q&A, flat RAG tends to outperform Graph RAG. Clause lookup, summaries, and explanations depend on preserving local context, and graphs often lose that unless the schema is very deliberately designed. Missing or imperfect edges hurt recall fast.\n\nIn our work we still start with graph-style retrieval, but it‚Äôs ontology-driven rather than document-driven. The graph narrows the candidate space or enforces constraints, and then we fall back to text retrieval for the actual answer. Treating the graph as structure and control, not as the main text retriever, has worked better for us.",
          "score": 4,
          "created_utc": "2026-02-08 01:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46lsz9",
              "author": "wahnsinnwanscene",
              "text": "Do you auto generate the ontology or hand design?",
              "score": 1,
              "created_utc": "2026-02-08 02:00:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o46o4jr",
                  "author": "sp3d2orbit",
                  "text": "Always programmatically generated from the input documents and it can be grounded in a well known ontology like ICD-10 or SNOMED. Or sometimes generate a grounding ontology from a specialized domain. ",
                  "score": 2,
                  "created_utc": "2026-02-08 02:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4899l0",
          "author": "Visual_Algae_1429",
          "text": "20b is nothing for graph rag, use 120b minimum",
          "score": 2,
          "created_utc": "2026-02-08 09:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46qxxg",
          "author": "penguinzb1",
          "text": "yeah docling's chunking is really good for local context. graphs shine when you need to traverse relationships or enforce constraints, but for straight document retrieval they add more overhead than value unless your schema is dialed in",
          "score": 1,
          "created_utc": "2026-02-08 02:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46z7cp",
          "author": "D_E_V_25",
          "text": "Hi buddy!! \nI was one there with the same issue as well.. \n\nhttps://www.reddit.com/r/Rag/s/1ttwNuIlVy \n\nAbove is the post I made here yesterday about hierchial agentic rag with hybrid search ( Knowledge graph + vector search )  and another json rag as well u can choose based on your usage \n..\n\nGive a visit u will get the whole pictur and it might help u set up your own as well  \n\n..\n\nAs for your question I feel ... U might be using graph rag in a wrong and treating it as a king...\n\nIn my 693k stored chunks which is 10% of what raw I had .. 250gb of raw data .. and I stored and things are working as well with a score of 0.878 on scale of 1..\n\nI have made things open source u give a visit and star the repo as very soon I will teach the folks to how to implement them into your own projects ...\n\nCurrently what I made was for my university and sorry but I can't share the whole thing but no worries.. If community loves this idea and projects..\n\nThis time I feel we will together make a 500gb+ thing and whole another level of thing .. üòéüòé",
          "score": 1,
          "created_utc": "2026-02-08 03:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47bn4v",
          "author": "Curious-Sample6113",
          "text": "Not surprised. Saw similar results along the same lines.",
          "score": 1,
          "created_utc": "2026-02-08 04:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49qrkf",
          "author": "No_Wrongdoer41",
          "text": "Me and a small team have built a platform to automatically build a graph for rag out of your documents. We find it's way better than literag. Would love for you to try it (for free) and compare!",
          "score": 1,
          "created_utc": "2026-02-08 16:03:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d0wf3",
              "author": "No_Presence4293",
              "text": "Can i get more info?",
              "score": 1,
              "created_utc": "2026-02-09 02:10:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4d11u0",
                  "author": "No_Wrongdoer41",
                  "text": "yep dming you",
                  "score": 1,
                  "created_utc": "2026-02-09 02:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qvttdv",
      "title": "Bayesian BM25 blends more smoothly with vector scores (less scale mismatch than simple weighted sum)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-04 16:51:46",
      "score": 14,
      "num_comments": 3,
      "upvote_ratio": 0.9,
      "text": "when it comes to retrieval, aggregation methods really matter and yet many people use heuristics which are not always very rigorous.\n\nbm25 scores and dense similarity scores live on very different scales and distributions. Even with normalization, the balance is usually heuristic and dataset‚Äëdependent, so you often end up tuning weights per domain.\n\nrrf ignores score magnitudes and uses only rank positions. That‚Äôs robust to scale mismatch, but it can discard useful confidence information and flatten large gaps between documents, which matters when one signal is clearly stronger.\n\n\\## Experiments\n\n    Setup\n    - Dataset: SQuAD\n    - Metrics: NDCG@10, MRR@10\n    - Dense model: BGE-M3\n    - Compared: weighted-sum (WS) hybrid vs RRF\n    \n    Results\n    - WS (bb25 + Dense): NDCG@10 0.9149, MRR@10 0.8850\n    - WS (BM25 + Dense): NDCG@10 0.9051, MRR@10 0.8717\n    - RRF (BM25 + Dense): NDCG@10 0.8874, MRR@10 0.8483\n\nBayesian BM25 maps BM25 scores into calibrated probabilities using a likelihood and prior model. Once lexical scores are on a probabilistic scale, they combine more naturally with vector scores (also treated as probabilities). In practice this reduces scale mismatch and stabilizes hybrid fusion without heavy tuning.\n\nuse with \\`pip install bb25\\`. happy to share code and details if anyone‚Äôs interested. feedback welcome!\n\n\n\nRepo:¬†[https://github.com/sigridjineth/bb25](https://github.com/sigridjineth/bb25)\n\nLibrary:¬†[http://pypi.org/project/bb25/](http://pypi.org/project/bb25/)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3kchwd",
          "author": "Routine_Paramedic_82",
          "text": "Will test this, looks interesting",
          "score": 3,
          "created_utc": "2026-02-04 17:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r8qvj",
          "author": "Informal_Tangerine51",
          "text": "Interesting approach to scale mismatch but misses the production reliability question: when retrieval quality degrades, can you debug it?\n\nYour weighted-sum beats RRF on SQuAD, but SQuAD is clean evaluation data. In production, retrieval fails for reasons metrics don't capture: stale index, query typos, edge case queries, domain drift after model updates.\n\nWhen retrieval returns wrong documents, you need to know: which fusion method chose this, what were the raw scores, why did BM25 rank differently than dense, what would different weights have returned? Bayesian probabilities help fusion accuracy but don't help incident debugging.\n\nWe hit this with hybrid search on document intake. Metrics looked good, but specific customer queries failed mysteriously. Couldn't reconstruct why fusion chose document A over B without logging raw scores, fusion weights, and retrieval timestamps. Calibrated probabilities make fusion more principled but debugging still requires evidence capture.\n\nFor production systems: are you logging fusion decisions (raw scores, calibrated probs, final ranking) in a way that's replayable later? Or just optimizing for eval metrics?",
          "score": 0,
          "created_utc": "2026-02-05 18:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3raojv",
              "author": "Ok_Rub1689",
              "text": "I smell ai sloppish writing style on you. Scam",
              "score": 1,
              "created_utc": "2026-02-05 18:12:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz41dq",
      "title": "Addressing a fundamental flaw in hybrid search by introducing a Log-Odds Conjunction framework in Bayesian BM25",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qz41dq/addressing_a_fundamental_flaw_in_hybrid_search_by/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-08 08:53:42",
      "score": 13,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "[https://github.com/instructkr/bb25/pull/1](https://github.com/instructkr/bb25/pull/1)\n\nTo the Information Retrieval Community:  \nA significant update has been merged into the Bayesian BM25 (bb25) repository today!\n\nThis update addresses a fundamental flaw in hybrid search known as Conjunction Shrinkage by introducing a Log-Odds Conjunction framework.\n\nIn traditional probabilistic retrieval, calculating the probability that multiple signals are simultaneously satisfied typically relies on the Naive Product Rule.\n\nFor instance, if a document is relevant based on keyword search with a probability of 0.7 and also relevant based on vector semantic search with a probability of 0.7, the standard approach multiplies these to yield 0.49.\n\nIntuitively, however, if two independent pieces of evidence both suggest a document is relevant, our confidence should increase beyond 0.7.\n\nThe product rule causes the final score to decrease toward zero as more signals are added, violating the intuition that corroborating evidence should amplify confidence.\n\nThe solution implemented in this PR resolves this by shifting the calculation from probability space to log-odds space. The mechanism operates in three stages: first, it computes the geometric mean to find the baseline tendency; second, it performs a Log-Odds Transformation to map the bounded probability space to the unbounded log-odds space; and third, it adds a bonus proportional to the logarithm of the number of signals.\n\nThis works because probability space is bounded by 1.0, preventing simple addition. By transforming to log-odds space, we remove this ceiling. Instead of the score shrinking to 0.49, the logic applies an additive bonus for agreeing signals, resulting in amplification where the final score becomes roughly 0.83.\n\nThis implementation is the proof that this structure is not merely a heuristic. The paper demonstrates that rigorous Bayesian inference over multiple signals produces a computational structure formally isomorphic to a feedforward neural network.\n\nThis work proves that the Sigmoid activation function is a mathematical necessity that emerges when converting Bayesian evidence into probability, rather than an arbitrary design choice. Consequently, this implementation demonstrates that a neural network is the natural structure of correct probabilistic reasoning.\n\nThe introduction of Log-Odds Conjunction has yielded measurable improvements on the SQuAD v2.0 benchmark compared to the standard Hybrid OR approach marking a +1.2% improvement.\n\nThis confirms that properly modeling the agreement between text and vector signals yields better ranking performance than simple score summation or probabilistic multiplication. I would like to extend our gratitude to Jaepil for deriving these proofs and contributing the code to bb25.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qz41dq/addressing_a_fundamental_flaw_in_hybrid_search_by/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o48e0fh",
          "author": "Professional-Fox4161",
          "text": "I have a question. I previously understood that BB25 was order-preserving relative to BM25, but the differing performance on several metrics seems to show that it's not true. Could you help me understand ?",
          "score": 2,
          "created_utc": "2026-02-08 10:37:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48rbxy",
          "author": "RobertLigthart",
          "text": "the core intuition makes total sense... if keyword and semantic search both agree a doc is relevant, multiplying probabilities and getting a lower score than either signal alone is obviously wrong. nice to see someone formalize it instead of just hacking around it with weighted sums",
          "score": 2,
          "created_utc": "2026-02-08 12:35:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz1zad",
      "title": "Do companies actually use internal RAG / doc-chat systems in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qz1zad/do_companies_actually_use_internal_rag_docchat/",
      "author": "NetInternational313",
      "created_utc": "2026-02-08 06:49:42",
      "score": 12,
      "num_comments": 28,
      "upvote_ratio": 0.88,
      "text": "I‚Äôm curious how common internal RAG or doc-chat tools really are in practice.\n\nDoes your org have something like:\n\n* chat over internal docs / wikis / tickets\n* an internal knowledge assistant\n* or any RAG-based system beyond a small pilot?\n\nIf yes, is it widely deployed or limited to a few teams?  \nIf no, did it stall at POC due to security, compliance, or other concerns?\n\nGenuinely interested in real-world adoption",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qz1zad/do_companies_actually_use_internal_rag_docchat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o48yy8v",
          "author": "SpectralCoding",
          "text": "We deployed one against our entire internal quality and R&D documentation. 2.2M pages converted to markdown and stored in an Azure AI Search Index. Deployed to ~7,000 users and gets about 250 questions per day. Only a subset of those 7000 work with the data we indexed daily. GPT-5.2 Medium for answer generation. My comment history has some information we got from users about how they use the tool and how it‚Äôs helping them in their daily work. Very interesting responses.\n\nIt‚Äôs costing us about $1k/mo to maintain (OCRing new docs, Search index cost, LLM calls, etc). That‚Äôs more expensive than it probably needs to be but the value we get is more than $0.18/answer on average.",
          "score": 12,
          "created_utc": "2026-02-08 13:28:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4917x7",
              "author": "NetInternational313",
              "text": "Is this tool strictly for internal use, and how did you handle security and access control for such a large doc set?",
              "score": 2,
              "created_utc": "2026-02-08 13:43:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o496vw0",
                  "author": "SpectralCoding",
                  "text": "Yes internal only. Most of the document set is open to all users. However our tool uses Azure AI Search where you can do effectively ‚Äúrecord level‚Äù security with groups. So when you do the retrieval segment they are only searching what they have access to.",
                  "score": 5,
                  "created_utc": "2026-02-08 14:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o491wyp",
              "author": "Thenuges",
              "text": "Are they not worried about the confidential data being sent out to OpenAI? Curious what their thoughts or policies are around this.",
              "score": 2,
              "created_utc": "2026-02-08 13:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o496n67",
                  "author": "SpectralCoding",
                  "text": "No. We use Azure OpenAI and are secured with Microsoft‚Äôs Data Processing Addendum. More of a concern is the tool is so useful and effective it would make an internal bad actors job way easier. A year ago someone would have had to ask around to find the right documents and piece together the entire process for a specific chemical coating or whatever. Now they just ask and it coalesces info from 20 docs and gives them a top tier answer in about 1min. So good for most use cases but also concerning how much data is now immediately available. But that‚Äôs our organization immaturity with document categorization and restricting data access.",
                  "score": 3,
                  "created_utc": "2026-02-08 14:15:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49xfqq",
              "author": "horserino",
              "text": "(disclaimer: I'm mostly a noob on this kind of setup)\n\nHow does the model interface with Azure? Does the model \"call\" a DB search somehow? Or does the search happen first and the results sent in the model's prompt ?",
              "score": 1,
              "created_utc": "2026-02-08 16:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49ygjf",
                  "author": "SpectralCoding",
                  "text": "This is the base project we use and have extended. Lots of docs and videos on it too.\n\nhttps://github.com/Azure-Samples/azure-search-openai-demo\n\nYou answer is here:\n\nhttps://github.com/Azure-Samples/azure-search-openai-demo/releases/tag/2025-11-18\n\nThere is an API layer that uses an LLM to come up with search terms, it searches the index, then creates a new prompt with the data from the search and the users question. Classic RAG pattern. This project is more interesting because it uses some Azure AI Search features to agentically search the index until it either finds relevant info or gives up. Not just a one shot search.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49ybpb",
              "author": "No-Contribution8248",
              "text": "What's the avg latency per question?",
              "score": 1,
              "created_utc": "2026-02-08 16:40:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49z6fn",
                  "author": "SpectralCoding",
                  "text": "Longer since we built or enabled the agentic search capabilities where it will make multiple searches if it doesn‚Äôt find relevant things the first time. Best case for a simple answer is about 2sec-ish? Longer complex multi-faceted answers might take 10sec or so since we have reasoning turned up. But we value completeness of answer and research over speed. We could (and have) turn everything down and generate sub-second streaming response but for our type of data this leads to better accuracy.",
                  "score": 3,
                  "created_utc": "2026-02-08 16:44:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4an39c",
                  "author": "c4cheeku",
                  "text": "I think you‚Äôre taking about similarity search latency. How is this achieved? Given, you‚Äôd do some reasoning and routing followed by retrieval and then answer generation and I am sure you won‚Äôt be able to do so under 10 seconds!!",
                  "score": 1,
                  "created_utc": "2026-02-08 18:37:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47uwl5",
          "author": "mr_dudo",
          "text": "we use one internally for onboarding - mostly hitting our confluence and notion exports. biggest win was honestly just getting the docs into a format the model could actually use. most wikis export as garbage html. ended up using a crawler to convert everything to markdown first, then chunking by headers. way cleaner retrieval than trying to parse the raw exports",
          "score": 4,
          "created_utc": "2026-02-08 07:37:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47yl5x",
              "author": "NetInternational313",
              "text": "That makes sense.\n\nDid you have to put any guardrails or access controls in place (team-based access, doc-level permissions), or is it mostly open internally since it‚Äôs onboarding-focused?\n\n  \nDid the model internally hosted",
              "score": 1,
              "created_utc": "2026-02-08 08:11:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o496mp5",
                  "author": "mr_dudo",
                  "text": "nah we just don't index anything sensitive. onboarding stuff is all public internally anyway - process docs, how to set up your dev env, that kind of thing. way easier than building actual permissions into it. if you need doc-level access control you're gonna have a bad time lol",
                  "score": 1,
                  "created_utc": "2026-02-08 14:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48p6p2",
          "author": "RobertLigthart",
          "text": "from what ive seen most companies have at least tried a POC but the ones that actually make it to production are usually narrow scope... like internal docs search or customer support kb, not the \"ask anything about our company\" thing that everyone demos\n\nthe biggest killer is usually data quality not the RAG pipeline itself. if your internal docs are a mess going in, no amount of fancy chunking or reranking is gonna save it",
          "score": 3,
          "created_utc": "2026-02-08 12:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ya8v",
              "author": "NetInternational313",
              "text": "For the RAGs that made it to prod, did teams have to add doc-level permissions or audit controls, or were those systems usually trusted because of the narrow scope?",
              "score": 1,
              "created_utc": "2026-02-08 13:24:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49yo1a",
          "author": "Educational_Cup9809",
          "text": "I built the whole Custom Assistants as a service internally for my org. It‚Äôs not have 600+ assistants that teams and people created in last few months. This is just beta mode. Some features i built:\n\nData sources: Sharepoint, file uploads (10 formats), confluence, azure blob, zendesk, service now, salesforce.\n\nFeatures:  draft publish mode, auto sync(delta only) file level tracking and metadata tagging. \n\nsecurity: Automated pinecone project and index creation per assistant with. Encryption of chunks.\n\nRBAC: access control using internal AD security groups and roles per assistant (automated)\n\nMCP server and answer generation: Lot of custom filter prompts and assistant level instructions features which are configurable. I also inject Users AD data  for region and other useful user level information for context.\n\nAnother team had already build a model agnostic api layer with openai spec transformation. So i use that for completions and embedding models.\n\nFeedback and insights dashboard. And a conversation history management for analytics.\n\nNow I am diving into graph RAG and structured (database) and Agentic workflows. \n\nHuge success internally.\n\nSecurity and compliance, and also encryption, was discussed heavily 2 years ago before I begin. This should be first layer you properly design and build everything else on top it at all layers",
          "score": 2,
          "created_utc": "2026-02-08 16:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a5ouz",
              "author": "NetInternational313",
              "text": "Looking back, what assumptions you had early on about security, access control, or usage patterns turned out to be wrong once adoption scaled to hundreds of assistants?",
              "score": 1,
              "created_utc": "2026-02-08 17:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47w2t3",
          "author": "Amazing_Alarm6130",
          "text": "We do. Scaled to serve hundreds of employees. Around 10 millions of curated scientific documents.",
          "score": 1,
          "created_utc": "2026-02-08 07:48:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47yexb",
              "author": "NetInternational313",
              "text": "Did you implement any access-control or governance policies to ensure data isolation across teams (e.g., preventing cross-team document access), especially at that scale?",
              "score": 1,
              "created_utc": "2026-02-08 08:09:51",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o47ygks",
              "author": "NetInternational313",
              "text": "do you use internal hosted LLM",
              "score": 1,
              "created_utc": "2026-02-08 08:10:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o48fqss",
              "author": "hashiromer",
              "text": "Do you use any evals?\n\nIn practice, relevance is trivial to check with citations but evaluating completeness of answers/recall seems next to impossible.",
              "score": 1,
              "created_utc": "2026-02-08 10:53:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4agc15",
          "author": "No_Direction_7168",
          "text": "Yes - Many internal docs across tens of thousands.  Then, individual departments make their own to share with tens to hundreds.  I don‚Äôt know their inner workings but I use NotebookLM as a RAG for my personal notes/documents.  The company has also given us personal RAG spaces, but they don‚Äôt work nearly as good as NotebookLM.",
          "score": 1,
          "created_utc": "2026-02-08 18:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ait8e",
              "author": "NetInternational313",
              "text": "When you say the internal RAG doesn‚Äôt work as well, is that mostly about relevance/recall, latency, or how answers are synthesized?",
              "score": 1,
              "created_utc": "2026-02-08 18:18:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4c4cgn",
                  "author": "No_Direction_7168",
                  "text": "Mostly about the quality of the answers.  The responses lack depth and feel like they are sticking close to the actual words of the ingested documents.  The company provided one was more clinical and while it technically contains a correct response.  e.g. I asked it to explain the history and development of a requirement and it responded with the history of the previous revisions of the document.  NLM not only gave me that information but also went on to describe the backstory of the document.. Not sure if it is a backend LLM difference, ingesting/chunking, or Quantization issue but the work one is always a bit lacking.",
                  "score": 1,
                  "created_utc": "2026-02-08 23:05:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4d8pdk",
          "author": "Ecanem",
          "text": "I work for one of the largest professional services firms in the world.  We have many deployed to 10,000-100,000 users.",
          "score": 1,
          "created_utc": "2026-02-09 02:50:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}