{
  "metadata": {
    "last_updated": "2026-02-26 17:15:53",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 119,
    "file_size_bytes": 141230
  },
  "items": [
    {
      "id": "1rcba6y",
      "title": "What's the best embedding model for RAG in 2026? My retrieval quality is all over the place",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-23 07:44:08",
      "score": 53,
      "num_comments": 34,
      "upvote_ratio": 0.91,
      "text": "I've been running a RAG pipeline for a legal document search tool.\n\nCurrently using OpenAI text-embedding-3-large but my retrieval precision is around 78% and I keep getting irrelevant chunks mixed in with good results.\n\nI've seen people mention Cohere embed-v4, Voyage AI, and Jina v3. Has anyone done real benchmarks on production data, not just MTEB synthetic stuff?\n\nSpecifically interested in retrieval accuracy on domain-specific text, latency at scale (10M+ docs), and cost per 1M tokens.\n\nWhat's working for you in production?\n\njust got access to zeroentropy's embeddings. amazing stuff! [zeroentropy.dev](http://zeroentropy.dev)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1lo2",
          "author": "ChapterEquivalent188",
          "text": "garbage in, garbage out.... semantic chunking and a lot of stuff on ingest AND retrieval side. its all here https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit as a basis system to build on it or the long run you may read here https://github.com/2dogsandanerd/RAG_enterprise_core  have fun. im tired of reading every day bout wrappers which you cant trust..... for legal docs you may need a auditrail as well also for ingest AND retrieval-----\nnever use the happy path as it will never make you happy",
          "score": 25,
          "created_utc": "2026-02-23 07:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0acu",
              "author": "revovivo",
              "text": "but how does it solve the problem of chunk sizing and retrieval ",
              "score": 1,
              "created_utc": "2026-02-23 16:11:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74vkvz",
                  "author": "ChapterEquivalent188",
                  "text": "semantic chunking for a start ? ",
                  "score": 1,
                  "created_utc": "2026-02-24 13:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x57m7",
              "author": "krimpenrik",
              "text": "Great resources!! \n\nHave you tested both?",
              "score": 0,
              "created_utc": "2026-02-23 08:25:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x6td3",
                  "author": "ChapterEquivalent188",
                  "text": "sort of ;) working everyday on it to make it perfect..will release some plugins and extensions to the opensourced ones soon\n\n\nedit: if you r intrested of the outcome just send me a set of docs and ill provide you with the rag for retrieval testing",
                  "score": 2,
                  "created_utc": "2026-02-23 08:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2u1w",
          "author": "lucasbennett_1",
          "text": "before switching models its worth figuring out whether the irrelevant chunks are a retrieval problm or a chunking problem.. 78% precision on legal docs can come from either, and they need different fixes,, embedding model swaps sometimes help but chunking strategy for legal text matters a lot because clauses and definitions often span weird boundaries that standard splitters handle badly.. that said BGE M3 does tend to outperform text embedding 3- arge on domain-specific retrieval in most comparison.. its available through several providers like deepinfra or huggingface at lower cost than openai embeddings which helps if you are iterating on a 10M doc corpus and running a lot of test queries",
          "score": 6,
          "created_utc": "2026-02-23 08:02:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x74f5",
              "author": "ChapterEquivalent188",
              "text": "this.  domain spec retrieval is one importend part but never can solve the garbage in and wordsalad problem ",
              "score": 1,
              "created_utc": "2026-02-23 08:44:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78n3xq",
              "author": "liannehynes",
              "text": "I've tried Zeroentropy Embedddings (still in beta) and it outperforms ALL of the above ;) ",
              "score": 0,
              "created_utc": "2026-02-25 00:34:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78q93v",
                  "author": "No_Injury_7940",
                  "text": "+1",
                  "score": 0,
                  "created_utc": "2026-02-25 00:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2nwf",
          "author": "crewone",
          "text": "We have done extensive tests for book data. Voyage is always the best, but also has a high latency. A local Qwen3 embedder delivers 90% of that performance at a fraction of the cost and latency. The rest comes down to chunking strategies.",
          "score": 9,
          "created_utc": "2026-02-23 08:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x7dyb",
              "author": "picturpoet",
              "text": "How do you decide on one chunking strategy versus the other? Are there any best practises for different types of documents/document structures ",
              "score": 1,
              "created_utc": "2026-02-23 08:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xlr9w",
                  "author": "crewone",
                  "text": "Read up on strategies. Then test and compare. (For large book context we use context-aware chunking)",
                  "score": 2,
                  "created_utc": "2026-02-23 11:06:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78njr2",
              "author": "liannehynes",
              "text": "I've tried zeroentropy embeddings and it outperforms voyage :) ",
              "score": 1,
              "created_utc": "2026-02-25 00:37:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c0mrs",
                  "author": "crewone",
                  "text": "Then use that.",
                  "score": 1,
                  "created_utc": "2026-02-25 14:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2cp5",
          "author": "thecontentengineer",
          "text": "Have you tried ZeroEntropy Embeddings? They’re as good as their rerankers. You should always use ZeroEntropy, even for embeddings!!",
          "score": 12,
          "created_utc": "2026-02-23 07:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xa7he",
              "author": "DarfleChorf",
              "text": "Wait they have embeddings too? I only tried their reranker, didn't realize they had an embedding model. Might give it a shot.",
              "score": 2,
              "created_utc": "2026-02-23 09:15:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78hcow",
                  "author": "thecontentengineer",
                  "text": "Yes they have embeddings and they’re the best we have tried. Way better than Cohere and Voyage.",
                  "score": 0,
                  "created_utc": "2026-02-25 00:03:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78hoxb",
              "author": "liannehynes",
              "text": "yes we did!! ZeroEntropy has the best embeddings compared to all other providers.",
              "score": 1,
              "created_utc": "2026-02-25 00:05:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78hymx",
                  "author": "DarfleChorf",
                  "text": "How do you get access?",
                  "score": 2,
                  "created_utc": "2026-02-25 00:07:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xaud9",
          "author": "thefishflaps",
          "text": "Yeah ZeroEntropy reranker is solid, and they just dropped embeddings too. Worth checking out.",
          "score": 4,
          "created_utc": "2026-02-23 09:21:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x59v8",
          "author": "xeraa-net",
          "text": "Jina v5 just came out; especially for the model size pushing the state of the art: https://jina.ai/news/jina-embeddings-v5-text-distilling-4b-quality-into-sub-1b-multilingual-embeddings/\nBut it will of course always depend on the domain, language, cleanliness of data, quantization,…\n\n\nDisclaimer: I work for Elastic.",
          "score": 2,
          "created_utc": "2026-02-23 08:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yrokw",
          "author": "tom_at_zedly",
          "text": "On legal docs, the model usually isn't the failure point, it's almost always the chunking.\n\nLegal text has too many nested clauses and cross-references that get cut off by standard recursive splitters just butcher. We’ve found that even basic embedding models work fine if you fix the chunking to keep definitions with their clauses (or use semantic chunking). That usually moves the needle way more than switching from OpenAI to Voyage or BGE.",
          "score": 2,
          "created_utc": "2026-02-23 15:31:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70qww8",
          "author": "liannehynes",
          "text": "ZeroEntropy have SOTA embeddings but i think they're still on beta",
          "score": 4,
          "created_utc": "2026-02-23 21:04:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5ax8",
          "author": "CEBarnes",
          "text": "I’ve had good reliable results with SPL files. (https://dailymed.nlm.nih.gov/dailymed/index.cfm). Step one was to spend 5-6 months building a parser that normalized and populated a database.\n\nIn your case, given you have doc files, would be to structure the data. Headings, line numbers, citations, parties, proposed order, etc. Stack on a categorizer and add its results to the structure. Build a specific schema that has enough flexibility that you don’t end up with edge cases everywhere. Once you have semi structured data, then populate an old school database and build an API. Lastly, create the skills the AI needs to “intelligently,” use the API. \n\nGranted, even if vibed all the way to the end, this is likely a 9 to 12 month endeavor. But, your results will be magical.",
          "score": 1,
          "created_utc": "2026-02-23 13:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z9oxo",
          "author": "remoteinspace",
          "text": "we've tried a bunch at papr and qwen 4b is best based on our evals",
          "score": 1,
          "created_utc": "2026-02-23 16:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74rdak",
          "author": "Ascending_Valley",
          "text": "I use sentence transformer (and others) and then reduce and transform with a method that weights toward known similar samples. I end up in R50 with simple weighted distance being very effective.",
          "score": 1,
          "created_utc": "2026-02-24 13:24:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcb47i",
      "title": "My RAG retrieval accuracy is stuck at 75% no matter what I try. What am I missing?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "author": "Equivalent-Bell9414",
      "created_utc": "2026-02-23 07:34:01",
      "score": 52,
      "num_comments": 38,
      "upvote_ratio": 0.92,
      "text": "I've been building a RAG pipeline for an internal knowledge base, around 20K docs, mix of PDFs and markdown. Using LangChain with ChromaDB and OpenAI embeddings.\n\nI've tried different chunk sizes (256, 512, 1024), overlap tuning, hybrid search with BM25 plus vector, and switching between OpenAI and Cohere embeddings.\n\nStill hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\n\nIs this a chunking problem or an embedding problem? What else should I be trying? Starting to wonder if I need to add a reranking step after retrieval but not sure where to start with that.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1e94",
          "author": "xpatmatt",
          "text": "I had an issue where a lot of documents and data included very similar terms used in very different contexts which made retrieval for any particular query difficult due to irrelevant retrievals. \n\nI had to segment the docs/data into six different vector DBS based on user intent and route queries to the appropriate DB based on the user's intent. Works great now.",
          "score": 16,
          "created_utc": "2026-02-23 07:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x3r92",
              "author": "redditorialy_retard",
              "text": "could you tell me more about this? very intrigued ",
              "score": 2,
              "created_utc": "2026-02-23 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x41ly",
                  "author": "xpatmatt",
                  "text": "Yes if you have questions I can answer them",
                  "score": 2,
                  "created_utc": "2026-02-23 08:14:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ykvj4",
              "author": "Cool_Injury4075",
              "text": "Did you try using a reranker? Unlike embeddings, rerankers can understand the intent of the question and filter out all the junk (unless, of course, the documents need context).",
              "score": 2,
              "created_utc": "2026-02-23 14:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xazxb",
          "author": "adukhet",
          "text": "Your problem is not embeddings, try below \n-if you chunk purely by token length, try markdown aware or/and semantic chunking\n-use rerankers but consider latency. Cross-encoders likely fixes semantically similar but irrelevant issues- but if not try late-interaction\n-try query rewriting/query expansion (e.g. HyDE)\n\nBut most importantly you must diagnose where failure arise before changing architecture",
          "score": 6,
          "created_utc": "2026-02-23 09:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x5f9u",
          "author": "ampancha",
          "text": "Reranking with a cross-encoder will likely push you past 80%, but persistent semantic pollution usually means chunking isn't preserving document boundaries or metadata context. The harder problem: your eval set won't cover the queries that actually break in production. You need per-query observability to see which retrievals are failing live, not just aggregate precision. Sent you a DM",
          "score": 6,
          "created_utc": "2026-02-23 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xi3nk",
              "author": "welcome-overlords",
              "text": "Im probably having similar issues. Would be interested in hearing more in DM",
              "score": 1,
              "created_utc": "2026-02-23 10:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xdn8f",
          "author": "StuckInREM",
          "text": "I think sharing a complete pipeline of what you are doing would be useful, what do your metadata look like for the documents to enanche the retrieval phase? recursive split chunking is for sure not optimal, what do your document structure look like in terms of paragraphs? have you tried with a reranker?",
          "score": 3,
          "created_utc": "2026-02-23 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x0fc9",
          "author": "grabGPT",
          "text": "Are you using OCR on PDFs? Have you checked the accuracy?",
          "score": 2,
          "created_utc": "2026-02-23 07:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmckb",
              "author": "Transcontinenta1",
              "text": "I am trying to use deepseeks ocr. Is there a better free one?",
              "score": 1,
              "created_utc": "2026-02-23 11:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o701n3s",
                  "author": "grabGPT",
                  "text": "It depends on whether the documents you're using are handwritten notes or machine printed. In both cases, accuracy will vary",
                  "score": 1,
                  "created_utc": "2026-02-23 19:04:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x1sk8",
          "author": "ggone20",
          "text": "Not enough information to answer your question. What does your corpus look like?",
          "score": 2,
          "created_utc": "2026-02-23 07:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x23jw",
          "author": "AmbitionCrazy7039",
          "text": "You need structural filtering. Try to classify your documents as precise as possible. Maybe you want to build some relational database around it. \n\nFor example, if you query the Knowledge Base for some „Manual X“ question, you only want to search similiar manuals. BM25 is only keyword search, most likely not sufficient. In this example keyword filtering might suggests non-manuals because other docs may relate more often to manuals.",
          "score": 2,
          "created_utc": "2026-02-23 07:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mrh0",
          "author": "Tough-Survey-2155",
          "text": "You need Agentic router: https://github.com/hamzafarooq/multi-agent-course/tree/main/Module_3_Agentic_RAG",
          "score": 2,
          "created_utc": "2026-02-24 07:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x9817",
          "author": "Glass-Combination-69",
          "text": "Throw it into cognee and see if you get 100%. Graph might be what’s missing",
          "score": 1,
          "created_utc": "2026-02-23 09:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ya4xo",
          "author": "jrochkind",
          "text": "I am not an expert, but have you tried cross-encoder re-ranking?  (Over-fetching, then re-ranking to get your K). \n\nI have not yet myself, but have been considering it.  Oh from your last line it sounds like you too have been considering it but have not tried it. I think that's what would make sense to try?  I would be curious to your results. \n\nI haven't done it, but it seems pretty straightforward, you just feed your over-fetched results to the re-ranker, with your query, and it reorders them, hopefully putting the less relevant ones at the bottom and out of your final selection slice.",
          "score": 1,
          "created_utc": "2026-02-23 13:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yatxf",
          "author": "code_vlogger2003",
          "text": "Hey have you stored any metadata for every chunk such that in the first hand you can verify that my retrieval step is actually returning the exact relevant ground truth answer page numbers or not etc. In this step you can identify whether it's the chunking issue or embedding drift etc.",
          "score": 1,
          "created_utc": "2026-02-23 14:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yk2by",
          "author": "Dense_Gate_5193",
          "text": "Have you tried using RRF with reranking instead?\n\nNornicDB uses BM25+vector search and uses a reranking model (BYOM) https://github.com/orneryd/NornicDB",
          "score": 1,
          "created_utc": "2026-02-23 14:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytm77",
          "author": "namognamrm",
          "text": "You did rerank?",
          "score": 1,
          "created_utc": "2026-02-23 15:40:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za0tz",
          "author": "remoteinspace",
          "text": "have you tried using a knowledge graph? that worked well for us at papr.. got us 92% retrieval accuracy (top 5 results) on stanford's stark benchmark which has arxiv like docs in their data set. dm me and i can help",
          "score": 1,
          "created_utc": "2026-02-23 16:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70caqp",
          "author": "blue-or-brown-keys",
          "text": "\"Still hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\"\n\nTry synthetic data? Summarize the document , store the summary and drop the document. ",
          "score": 1,
          "created_utc": "2026-02-23 19:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z56f",
          "author": "Much-Researcher6135",
          "text": "yes pull back 3x and use a reranker",
          "score": 1,
          "created_utc": "2026-02-24 04:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z5te",
          "author": "WorkingOccasion902",
          "text": "Have you considered Knowledge Graphs?",
          "score": 1,
          "created_utc": "2026-02-24 04:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mov3",
          "author": "Informal-Victory8655",
          "text": "Change embeddings model",
          "score": 1,
          "created_utc": "2026-02-24 07:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75u4y3",
          "author": "TransportationFit331",
          "text": "I recommend Mastra.ai",
          "score": 1,
          "created_utc": "2026-02-24 16:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hij9h",
          "author": "cointegration",
          "text": "what you need is a cross encoder, it ties the query back to the chunks retrieved to maximise relevance",
          "score": 1,
          "created_utc": "2026-02-26 09:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hk0by",
          "author": "cointegration",
          "text": "1) different types of docs do better with different chunk sizes and different retrieval methods  \n2) long text essays (manuals, legal docs, fiction etc) benefit from larger chunks, vector search and knowledge graphs  \n3) tabular, itemised, charts or docs with many tables (invoices, receipts, performance reports etc) benefit more from BM25 against extracted metadata and shorter chunks  \n4) its a balance between precision, recall and semantic relevance, but strategies must exist for all 3. BM25 for precision, vector search for recall, knowledge graphs for semantic relevance.  \n5) use a cross encoder and rerank above 3, apply your own tweekable weights",
          "score": 1,
          "created_utc": "2026-02-26 09:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71nke4",
          "author": "Ok-Attention2882",
          "text": "Skill issue",
          "score": 0,
          "created_utc": "2026-02-23 23:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zct5j",
          "author": "Repulsive-Memory-298",
          "text": "It sounds like a troll but adding porn to your datasets calibrates the vector space",
          "score": -2,
          "created_utc": "2026-02-23 17:09:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rebd0i",
      "title": "Built a four-layer RAG memory system for my AI agents (solving the context dilution problem)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-25 12:02:23",
      "score": 36,
      "num_comments": 7,
      "upvote_ratio": 0.96,
      "text": "We all know AI agents suffer from memory problems. Not the kind where they forget between sessions but something like context dilution. I kept running into this with my agents (it's very annoying tbh). Early in the conversation everything's sharp but after enough back and forth the model just stops paying attention to early context. It's buried so deep it might as well not exist.\n\nSo I started building a four-layer memory system that treats conversations as structured knowledge instead of just raw text. The idea is you extract what actually matters from a convo, store it in different layers depending on what it is, then retrieve selectively based on what the user is asking (when needed).\n\nDifferent questions need different layers. If someone asks for an exact quote you pull from verbatim. If they ask about preferences you grab facts and summaries. If they're asking about people or places you filter by entity metadata.\n\nI used workflows to handle the extraction automatically instead of writing a ton of custom parsing code. You just configure components for summarization, fact extraction, and entity recognition. It processes conversation chunks and spits out all four layers. Then I store them in separate ChromaDB collections.\n\nBuilt some tools so the agent can decide which layer to query based on the question. The whole point is retrieval becomes selective instead of just dumping the entire conversation history into every single prompt.\n\nTested it with a few conversations and it actually maintains continuity properly. Remembers stuff from early on, updates when you tell it something new that contradicts old info, doesn't make up facts you never mentioned.\n\nAnyway figured I'd share since context dilution seems like one of those problems everyone deals with but nobody really talks about.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7bggdt",
          "author": "Specific_Expert_2020",
          "text": "Awesome!\n\nI am building something very similar at work.. but i cannot disclose to much.\n\nSimilar approach with criteria and fields to help differentiate the data in the system.\n\nIs there a reason you kept it to 4?\n\nI am learning this whole RAG thing and I see the section mentions \"why 4\" but did you limit it? Or was 4 enough to keep the response accurate.\n\nJust curious :)",
          "score": 3,
          "created_utc": "2026-02-25 12:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bkror",
              "author": "Independent-Cost-971",
              "text": "I only used 4 in the blog because it made testing and demonstrating the results much cleaner and easier to follow. When you’re explaining RAG concepts, smaller numbers help keep the examples readable and the behavior obvious.\n\nIn a real project, you’d absolutely use more than 4.",
              "score": 1,
              "created_utc": "2026-02-25 13:19:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bldbj",
                  "author": "Specific_Expert_2020",
                  "text": "Also I hope my comment did not come off as downplaying your post as it is way more technical than what I am working on so great share.\n\nI appreciate the follow up and sharing.\n\nI had concerns of over fielding as I work in a MSSP and thing vary by a variety of fields.\n\nSo as I develop the roll out.. I am seeing the fields scope creeping.\n\nThank you for the insight.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:22:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7coofs",
          "author": "lez566",
          "text": "I did something similar. I built an AI agent that learns as you talk to it and builds a living profile of you and your needs. Then anytime a message is sent, the tool gives this living profile, a keyword search, a semantic search and the last n messages. It’s excellent and never forgets.",
          "score": 2,
          "created_utc": "2026-02-25 16:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b95tc",
          "author": "Independent-Cost-971",
          "text": "I wrote a whole blog about this that goes way deeper if anyone's interested: [https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/](https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/)",
          "score": 2,
          "created_utc": "2026-02-25 12:03:15",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7bq031",
              "author": "arun4567",
              "text": "This is good. I was looking for some thing like this since my agent gets into loops and forgets that its been provided the details before. How do you handle updates of information that's already been provided,",
              "score": 3,
              "created_utc": "2026-02-25 13:48:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rask5l",
      "title": "What retrievers do you use most in your RAG projects?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rask5l/what_retrievers_do_you_use_most_in_your_rag/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-21 14:32:22",
      "score": 27,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,  \nI’m curious to know what retrievers you use most in your RAG pipelines. Do you mainly rely on vector search, BM25, hybrid retrieval, or something else?\n\nWould love to hear what works best for you in real projects.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rask5l/what_retrievers_do_you_use_most_in_your_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6lv6ec",
          "author": "Academic_Track_2765",
          "text": "Great question! Always hybrid with metadata filtering.",
          "score": 13,
          "created_utc": "2026-02-21 14:45:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lvwnm",
              "author": "minaminotenmangu",
              "text": "i do it without metadata filtering. the vector search sometimes knows better. But I think with time and if the database gets bigger filtering will return.",
              "score": 4,
              "created_utc": "2026-02-21 14:49:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lwt2e",
                  "author": "Academic_Track_2765",
                  "text": "Typically you want to do metadata filtering before the search space to avoid noise, but yes it can be done at both layers. My databases is currently at 5 million documents so we have mandatory metadata filtering before the vector search, e.g., the users select data range, region, categorized issue type and few other filters and then hybrid retrieval. It honestly blocks out so many noise points and retrieval speeds goes from seconds to milliseconds with better retrieved candidates.",
                  "score": 4,
                  "created_utc": "2026-02-21 14:54:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m0frg",
          "author": "avebrahimi",
          "text": "I have about 5m+ records, I reached this workflow after a dozen tries, first filter using metadata (both original metadata, and llm-based-metadata), then check if BM25 gets high scores, then mix it with vector search, otherwise just use vector search. and finally, reranking is amazing!!!\n\nthe big point is using perfect embedder for your data, to save tokens/money.\n\nDon't forget to show search result summary using LLM, it will please users.",
          "score": 7,
          "created_utc": "2026-02-21 15:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pgpub",
              "author": "Final_Special_7457",
              "text": "What do u use as embedder model ?",
              "score": 1,
              "created_utc": "2026-02-22 02:32:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6r7mi5",
                  "author": "avebrahimi",
                  "text": "Depending on use case, but generally:  \n\\- gemini\\_embedding\\_001 is best of class (online), but it's not always economic.\n\n\\- jina-v3 & Qwen3-Embedding-4B & embedding-gemma-300m are not if you want to run on your own GPU.\n\nBut you should have test cases, and embed and retrieve your very own data to check which embedder is best.",
                  "score": 2,
                  "created_utc": "2026-02-22 11:23:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nwah8",
          "author": "FeeMassive4003",
          "text": "We use hybrid: vector search plus keyword search. No rebranding - we just take k docs from each (usually k=5).",
          "score": 2,
          "created_utc": "2026-02-21 20:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nwrlq",
              "author": "marwan_rashad5",
              "text": "Do you use reranking after retrieval with hybrid search ??",
              "score": 2,
              "created_utc": "2026-02-21 21:01:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o517b",
                  "author": "FeeMassive4003",
                  "text": "No, we just take 5 from each. Total of 10 chunks, go to the LLM. It's quite basic; but it works.",
                  "score": 3,
                  "created_utc": "2026-02-21 21:44:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m82mp",
          "author": "Rodda_LBV",
          "text": "Per verificare la qualità dei documenti recuperati quali metodi usate?",
          "score": 1,
          "created_utc": "2026-02-21 15:53:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m9mtr",
          "author": "PresentationNew936",
          "text": "Well we mostly use hybrid (bm25 and vector) for best results. But it also highly depends on the embeddings. And of course there is re ranking at the end.",
          "score": 1,
          "created_utc": "2026-02-21 16:01:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mywlz",
              "author": "marwan_rashad5",
              "text": "I haven't used reranking before.\nHow do you do it?",
              "score": 1,
              "created_utc": "2026-02-21 18:08:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6oqpte",
                  "author": "PresentationNew936",
                  "text": "Re-ranking is a step where you take already retrieved documents and sort them again using a smarter model. This model looks at the user’s question and each document together and gives a better relevance score. It understands context more deeply, so the most useful and accurate documents move to the top.",
                  "score": 1,
                  "created_utc": "2026-02-21 23:48:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6n36k9",
          "author": "Independent-Bag5088",
          "text": "Depends.\n\n1. What type of documents are they - are we looking at more semantic text or numbers matter more?\n2. If query answers require creativity, semantic retrievals work better, but if they are domain-specific, BM25 (keyword match) might be a better option.\n\nIn my case, I have separated my documents into relational database + vector database, for appropriate use-case.\n\nIn my naive opinion, most of the time it depends on the type of document you are dealing with. Domain knowledge on the document would help you design your RAG system appropriately.",
          "score": 1,
          "created_utc": "2026-02-21 18:29:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rej56g",
      "title": "Lessons from shipping a RAG chatbot to real users (not just a demo)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rej56g/lessons_from_shipping_a_rag_chatbot_to_real_users/",
      "author": "cryptoviksant",
      "created_utc": "2026-02-25 17:09:55",
      "score": 26,
      "num_comments": 23,
      "upvote_ratio": 0.75,
      "text": "I've been building a chatbot product (bestchatbot.io, works on Discord and websites) where users upload their docs and the bot answers questions from that content. Wanted to share some stuff I learned going from \"cool demo\" to \"people are actually paying for this\" because the gap between those two is way bigger than I expected.\n\n**Vanilla RAG gets you maybe 30% of the way there (no joke)**\n\nWhen I started I did the standard thing. Chunk docs, embed them, retrieve top-k, stuff into context, generate. It worked great on demos. Then real users uploaded real docs and it fell apart. The problem isn't retrieval in isolation, it's that real documents have structure, context, and relationships between sections that get destroyed when you just chunk and embed.\n\n**What actually mattered in production**\n\nWithout going too deep into our specific implementation, here's what moved the needle the most:\n\n* **Document quality > retrieval sophistication.** I spent weeks tweaking retrieval and got maybe 10% better. Then I added better doc preprocessing and got a bigger jump overnight. Garbage in garbage out is painfully real.\n* **Evaluation is everything.** You can't improve what you can't measure. I built a testing interface where I could ask questions and see exactly which sources the bot cited. That feedback loop was more valuable than any architecture change.\n* **Users don't care about your retrieval method.** They care about two things: did it answer correctly, and how fast. Our response time is 10-20 seconds which people complain about constantly. Nobody has ever asked me what embedding model we use.\n* **The knowledge base needs to be treated as a living thing.** We added a system where the bot learns from moderator corrections in Discord automatically. That continuous improvement loop has been surprsingly impactful compared to just static doc retrieval.\n\nMost of the accuracy gains came from boring stuff. Better chunking, better preprocessing, better prompting, testing obsessively. The architecture matters but it's maybe 30% of the outcome. The other 70% is everything around it.\n\nCurious what other people building production RAG systems have found. What moved the needle most for your accuracy?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rej56g/lessons_from_shipping_a_rag_chatbot_to_real_users/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7e9r1j",
          "author": "zsrt13",
          "text": "This is basic 101 stuff. There are no lessons here",
          "score": 8,
          "created_utc": "2026-02-25 20:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dll7a",
          "author": "ChapterEquivalent188",
          "text": "jesus another wrapper and chatwithpdf.......move on ....all your text is shouting \"garbage in, garbage out\"  \n\nyou even called it by yourself, so how do you solve it ? you mention so many deadends.....how about semantic chunking ? how a about specialized parsers? what is your outcome without happypath tests ? If you are the world gratetes chef with the world best recipe, what do you get when your Ingrediens is mixed up mud ?",
          "score": 4,
          "created_utc": "2026-02-25 19:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7epzbm",
              "author": "welcome-overlords",
              "text": "Bro get my feet outta your mouth. Fuck im so bored of these ai posts selling their idea. Can i just talk with fellow fucking humans who are trying to solve these problems and maybe hear some ideas",
              "score": 4,
              "created_utc": "2026-02-25 22:15:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7eq3z7",
                  "author": "welcome-overlords",
                  "text": "If u r a fellow human solving thess issues and maybe getting paid for it so u dont want me to pay u, reply",
                  "score": 1,
                  "created_utc": "2026-02-25 22:15:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7h4u7c",
                  "author": "ChapterEquivalent188",
                  "text": "im here ;) always happy to meet peps focusing on the garbage in problem ",
                  "score": 1,
                  "created_utc": "2026-02-26 07:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7dy9vd",
              "author": "cryptoviksant",
              "text": "you wish this was just a wrapper ;) \n\n[bestchatbot.io](http://bestchatbot.io) \n\nHave a look. If you can built a similar thing with only \"chatwithdpf\" wrapper, I'll give you $1000. On god.",
              "score": -3,
              "created_utc": "2026-02-25 20:05:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7h4k2q",
                  "author": "ChapterEquivalent188",
                  "text": "LOL----https://github.com/2dogsandanerd/RAG_enterprise_core\n\nshow me your results on this and i believe everything you claim ;) \nhttps://github.com/2dogsandanerd/Liability-Trap---Semantic-Twins-Dataset-for-RAG-Testing",
                  "score": 2,
                  "created_utc": "2026-02-26 07:06:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dgjwb",
          "author": "rigatoni-man",
          "text": "I’d love to know more about your evaluation.  How does the interface work?  How/what do you evaluate?",
          "score": 1,
          "created_utc": "2026-02-25 18:43:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dygs0",
              "author": "cryptoviksant",
              "text": "The interface is tied to a backend running on FastAPI, which connects the front-end (Vite) with the backend (python). You can actually have a look at [https://bestchatbot.io](https://bestchatbot.io) , any feedback is welcomed!",
              "score": 0,
              "created_utc": "2026-02-25 20:06:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7eaaav",
                  "author": "rigatoni-man",
                  "text": "Ah I meant how do you / did you test to validate your strategies?",
                  "score": 2,
                  "created_utc": "2026-02-25 21:01:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7h4lvl",
                  "author": "ChapterEquivalent188",
                  "text": "LOL",
                  "score": 1,
                  "created_utc": "2026-02-26 07:07:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gbhdp",
          "author": "hhussain-",
          "text": "Lessons learned, many missed usually. Production is not \"yeah... I can build this, nevermind the post\"\n\nAnyway, for performance you might think of using rust. Either as tour backend, or processors and lib used in python. Some oython lib are already rust or C under the hood. FastPAI is good choice, but rust performance is different level.",
          "score": 1,
          "created_utc": "2026-02-26 03:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ggqvn",
          "author": "nbass668",
          "text": "Sorry there is nothing learned. You just spit out something we all do.. and every comment you spamming your website.\n\nReporting this to the mods as Spam sugar coated with a fake knowledge",
          "score": 1,
          "created_utc": "2026-02-26 04:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hsid1",
              "author": "cryptoviksant",
              "text": "sure go ahead",
              "score": 0,
              "created_utc": "2026-02-26 10:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rb4tv5",
      "title": "How do you evaluate your RAG systems (chatbots)?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rb4tv5/how_do_you_evaluate_your_rag_systems_chatbots/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-21 22:42:03",
      "score": 22,
      "num_comments": 17,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI'm currently building a RAG-based chatbot and I'm curious how people here evaluate their systems.\n\nWhat methods or metrics do you usually use to measure performance? For example: retrieval quality, answer accuracy, hallucinations, etc.\n\nDo you use any specific frameworks, benchmarks, or manual evaluation processes?\n\nI'd love to hear about the approaches that worked well for you.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rb4tv5/how_do_you_evaluate_your_rag_systems_chatbots/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6oizyv",
          "author": "Khade_G",
          "text": "Most teams evaluate RAG at the wrong layer.\nThey measure:\n- Retrieval precision/recall\n- Context relevance\n- Hallucination rate\n- Faithfulness\n\nThat’s necessary, but not sufficient.\n\nThe harder failures usually show up in:\n\n1️⃣ Retrieval boundary errors\n- Relevant document exists but falls just outside top-k\n- Chunk fragmentation causing partial reasoning\n- Multi-hop answers that require cross-document stitching\n\n2️⃣ Ambiguity handling\n- Question underspecified but model answers confidently\n- Conflicting sources in corpus\n- Time-sensitive drift (outdated documents retrieved)\n\n3️⃣ Entropy stacking\n- OCR noise + bad chunking + long answers\n- Mixed structured/unstructured sources\n- Schema drift in semi-structured data\n\nWhat we’ve seen working with teams building production RAG systems is this:\n\nThe ones that formalize a structured evaluation corpus early (versioned regression set + labeled failure taxonomy) improve measurably release over release.\n\nThe ones that rely purely on live telemetry + ad hoc manual checks tend to fix issues reactively - and drift creeps in.\n\nThe unlock usually isn’t more metrics - it’s a curated set of hard queries across known failure classes.\n\nCurious what domain you’re building in? Evaluation design shifts quite a bit between support bots, internal knowledge bases, and high-risk domains like legal or compliance.",
          "score": 7,
          "created_utc": "2026-02-21 23:01:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rwky1",
              "author": "SemperPistos",
              "text": "Is there a book or resource you recommend for learning this?",
              "score": 1,
              "created_utc": "2026-02-22 14:21:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6stwpv",
                  "author": "JealousBid3992",
                  "text": "ChatGPT if you want their original source sure",
                  "score": 3,
                  "created_utc": "2026-02-22 16:59:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6wf6fi",
              "author": "harshitsinghai",
              "text": "I thought he's selling some product that solves this problem... but forgot to paste the link",
              "score": 1,
              "created_utc": "2026-02-23 04:41:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6wljd7",
                  "author": "Khade_G",
                  "text": "DM Me",
                  "score": 1,
                  "created_utc": "2026-02-23 05:29:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6pxxdr",
          "author": "Leather-Departure-38",
          "text": "LLM as judge, i use aws bedrock evaluations framework worked pretty well for my team as well",
          "score": 3,
          "created_utc": "2026-02-22 04:30:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6r7154",
          "author": "goedel777",
          "text": "Your CEO feels happy with some cherry picked queries? You are fine.",
          "score": 2,
          "created_utc": "2026-02-22 11:18:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6phhft",
          "author": "BrilliantUse7570",
          "text": "RAG itself is pretty complex. The deeper you go, the more modular it gets, with lots of moving parts. For better results, make sure you optimize RAG at every step, from indexing to generation.",
          "score": 1,
          "created_utc": "2026-02-22 02:37:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6rbgun",
          "author": "According-Lie8119",
          "text": "I wouldn’t overthink it, I’d test. And test consistently.\n\nI use a fixed evaluation set that includes:\n\n* Yes/No questions\n* Open questions\n* Paraphrased variations\n* Questions where the system should say “I don’t know”\n* A few edge cases to provoke hallucinations\n\nWhenever I change something (chunking, embeddings, retriever settings, prompts, etc.), I run the exact same set again.\n\nMy setup is semi-automated: I send the questions to my endpoint and get back a structured JSON test report (answers, sources, latency, etc.). That makes it easy to compare versions and detect regressions.\n\nAlso very helpful in production: implement a simple feedback mechanism and collect user feedback in a dashboard. Real-world feedback is extremely valuable and often reveals issues your test set doesn’t cover.\n\nAutomatic metrics help, but without a stable test set and real user signals, it’s hard to know if you’re actually improving the system.",
          "score": 1,
          "created_utc": "2026-02-22 11:58:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6sy9pf",
          "author": "Able-Let-1399",
          "text": "I’m building a setup where you build multiple RAG configs at the same time. Output are summarized on 3 metrics - performance, cost and quality.\nQuality is measured via RAGAS and, if configured, LLM-as-a-judge, based on a set of golden questions and answers. Performance is time. Cost is mainly use of tokens which then, depending on provider (including local), is translated into a monetary values.\nMaybe co-development? Send me a message if interested 🙂",
          "score": 1,
          "created_utc": "2026-02-22 17:19:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u18hk",
          "author": "Dihedralman",
          "text": "To start checkout Ragas https://docs.ragas.io/en/stable/. It has most of the metrics mentioned on this thread including how to use LLMs as a judge. You will want to customize your metrics as you develop and iterate. ",
          "score": 1,
          "created_utc": "2026-02-22 20:25:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6u9lsz",
          "author": "No-Contribution8248",
          "text": "Promptfoo. It’s great.",
          "score": 1,
          "created_utc": "2026-02-22 21:07:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6w7n7r",
          "author": "TraditionalDegree333",
          "text": "I use deepeval to confirm and scorecard tracing",
          "score": 1,
          "created_utc": "2026-02-23 03:48:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wnq6r",
          "author": "Capital_Direction231",
          "text": "Evaluating RAG systems properly requires separating retrieval quality from generation quality — otherwise it’s hard to know what’s actually failing.\n\nAt **Exotica IT Solutions**, we typically evaluate RAG chatbots across four layers:\n\n**1. Retrieval Evaluation**\n\n* Precision@k and Recall@k\n* MRR (Mean Reciprocal Rank)\n* Context relevance scoring We test whether the correct documents are even being retrieved before judging the answer.\n\n**2. Groundedness / Faithfulness**  \nWe check whether the generated answer is actually supported by the retrieved context.  \nLLM-as-a-judge scoring works well here, especially when reference answers are limited.\n\n**3. Answer Quality**\n\n* Correctness\n* Completeness\n* Clarity\n* Hallucination rate\n\nThis can be automated partially but usually includes human validation for high-impact systems.\n\n**4. Regression & Drift Monitoring**  \nWe maintain a fixed benchmark dataset of domain-specific queries and re-run it after:\n\n* Embedding model updates\n* Chunking strategy changes\n* Prompt modifications\n* Model upgrades\n\nIf metrics drop, we know exactly where to look.\n\nFor production systems, we also log user feedback signals (thumbs up/down, correction requests) and combine that with automated scoring.\n\nIn short:  \nEvaluate retriever separately, evaluate generator separately, maintain a gold dataset, and monitor continuously.\n\nThat structure has worked well for us in real-world RAG deployments.",
          "score": 1,
          "created_utc": "2026-02-23 05:47:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x93i0",
          "author": "habibaa_ff",
          "text": "developed a light weight library to help with this at the retrieval layer. Inspecting the ranking behaviour. [https://pypi.org/project/retric/](https://pypi.org/project/retric/)",
          "score": 1,
          "created_utc": "2026-02-23 09:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zd0gw",
          "author": "remoteinspace",
          "text": "at papr we use a retrieval loss formula. It was designed to capture accuracy, latency, and cost. Lower values indicate better overall retrieval (high accuracy, low latency). \n\n**Retrieval Loss (without cost)**\n\n    Retrieval-Loss = −log₁₀(Hit@K) + λ · (Latency_p95 / 100 ms)\n\nWhere:\n\n* **Hit@K** = probability that the correct memory is in the top-K returned set\n* **Latency\\_p95** = tail latency in milliseconds\n* **λ (lambda)** = weight that says \"every 100 ms of extra wait feels as bad as dropping Hit@5 by one decade\"\n* In practice, **λ ≈ 0.5** makes the two terms comparable\n\n**Extended Three-Term Formula (with cost):**\n\n    Retrieval-Loss = −log₁₀(Hit@K) + λL · (Latency_p95 / 100 ms) + λC · (Token_count / 1,000)\n\nWhere:\n\n* **λL** = weight for latency (typically 0.5)\n* **λC** = weight for cost (typically 0.01)\n* **Token\\_count** = total number of prompt tokens attributable to retrieval",
          "score": 1,
          "created_utc": "2026-02-23 17:10:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rab7rs",
      "title": "What chunking strategies are you using in your RAG pipelines?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rab7rs/what_chunking_strategies_are_you_using_in_your/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-20 23:31:11",
      "score": 20,
      "num_comments": 14,
      "upvote_ratio": 0.9,
      "text": "Hey everyone,\n\nI’m curious what chunking strategies you’re actually using in your RAG systems. Are you sticking with recursive/character splitting, using semantic chunking, or something more advanced like proposition-based or query-aware approaches?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rab7rs/what_chunking_strategies_are_you_using_in_your/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6ji2v0",
          "author": "Ok_Signature_6030",
          "text": "for most document types recursive splitting with decent overlap still works better than people expect. we tested semantic chunking pretty extensively and the retrieval quality improvement was marginal — maybe 3-5% on our evals — while adding a lot of complexity and latency from the embedding calls during ingestion.\n\nwhere chunking strategy actually mattered for us was structured documents like contracts and technical specs. for those we switched to section-aware chunking that respects headers and keeps related clauses together. that alone bumped our answer accuracy by about 15% compared to naive 512-token windows.\n\nbiggest lesson was that chunk size matters way more than chunk method. going from 512 to \\~1200 tokens with 200 token overlap made a bigger difference than any fancy chunking algorithm we tried.",
          "score": 10,
          "created_utc": "2026-02-21 03:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1qkh",
              "author": "Mystical_Whoosing",
              "text": "So this 1200 / 200 is token numbers, not characters?",
              "score": 2,
              "created_utc": "2026-02-21 05:32:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pbbw9",
                  "author": "Ok_Signature_6030",
                  "text": "yeah those are token counts — 1200 tokens per chunk, 200 tokens of overlap between consecutive chunks. roughly 900-ish words per chunk depending on the tokenizer.\n\n  \nthe overlap is what prevents splitting an answer across chunk boundaries where neither chunk has enough context alone.",
                  "score": 2,
                  "created_utc": "2026-02-22 01:57:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kmto0",
          "author": "cointegration",
          "text": "I have given up on 1 size fits all chunking strategy, different types of documents require different strategies, legal docs and instruction manuals do well with large chunks and semantic boundaries, invoices and receipts do much better with small chunks and metadata. Same for retrieval, BM25 is for precision, vectors for recall. Its quite obvious by now that much preprocessing/filtering is required even before chunking. I seperate out docs into several buckets, each have their own pipeline for ingestion. In a nutshell:\n\nIngestion:  \ndocs --> sorted into buckets by a small local llm --> extract MD/metadata --> chunk based on doc type --> embed\n\nRetrieval:  \nquery --> tfidf gets reduced candidate size --> BM25/Vector search --> gets weighted Top K chunk candidates plus neighbours --> cross-encoder rerank --> final chunks",
          "score": 3,
          "created_utc": "2026-02-21 08:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jzdmp",
          "author": "StarThinker2025",
          "text": "Recursive splitting + overlap is still the baseline for us (600–800 tokens, ~15% overlap)\n\nSemantic chunking sounds better in theory, but retrieval stability matters more than “perfect” boundaries\n\nWe optimize chunk size based on embedding model + average query length 📈📈📈",
          "score": 2,
          "created_utc": "2026-02-21 05:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k8g1t",
          "author": "One_Milk_7025",
          "text": "Chunking is very important part of the ingestion pipeline..\nTo actually see the chunks and its related metadata you need a chunk workbench or visualizer..\ncheckout chunker.veristamp.in for the start you can see all the code, table, heading, txt properly chunked with all the metadata.. you can tweak the settings for the optimal use case",
          "score": 2,
          "created_utc": "2026-02-21 06:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6km4w7",
          "author": "Infamous_Ad5702",
          "text": "2 sentence chunks.",
          "score": 2,
          "created_utc": "2026-02-21 08:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lhe68",
              "author": "marwan_rashad5",
              "text": "I think that 2 Sentence chunking might cause some loss of semantic coherence. What is your opinion about that?",
              "score": 1,
              "created_utc": "2026-02-21 13:20:46",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6x8ab8",
                  "author": "Infamous_Ad5702",
                  "text": "We have a floating system. It seems super accurate. Validation tests have been spot on. No loss of context. Passes testing well.",
                  "score": 1,
                  "created_utc": "2026-02-23 08:56:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6lgvzu",
          "author": "itsmekalisyn",
          "text": "We have a lot of How to guides and we use page level and it is working well.",
          "score": 2,
          "created_utc": "2026-02-21 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lh4f7",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 13:18:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lh8cf",
                  "author": "itsmekalisyn",
                  "text": "For our case, recursive chunking was better than semantic. But, page level beats everything in our internal metrics.",
                  "score": 2,
                  "created_utc": "2026-02-21 13:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6rykrh",
          "author": "viitorfermier",
          "text": "I'm using a Reasoning based RAG on the fiscal code/laws. Tried chunks, embeddings, logic loops, but that didn't worked well (hard to get in context all needed information). Now I just give the LLM article summaries, get back from it db ids of the relevant articles then do another filter on the filtered articles, then generate answer. It more expensive and time consuming, but the answers are better.\n\nProbably at some point I'll try PageIndex [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex) \\- if current method doesn't work.",
          "score": 1,
          "created_utc": "2026-02-22 14:32:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcecf4",
      "title": "How I Used AI + RAG to Automate Knowledge Management for a Consulting Firm",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "author": "Safe_Flounder_4690",
      "created_utc": "2026-02-23 10:52:36",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 0.88,
      "text": "Recently, I built a workflow for a consulting firm that leverages AI combined with Retrieval-Augmented Generation (RAG) to fully automate knowledge management, transforming a fragmented document system into a centralized, actionable intelligence hub. The pipeline begins by ingesting structured and unstructured client reports, internal documents and market research into a vector database, then AI agents retrieve the most relevant information dynamically, reason over it and generate concise, actionable summaries or recommendations. By layering persistent memory, validation loops and workflow orchestration, the system doesn’t just fetch data it contextualizes it for consultants, flags potential conflicts, and tracks follow-ups automatically. This approach drastically reduced time spent searching across multiple tools, eliminated duplication errors and improved decision-making speed. What made it successful is the combination of semantic search, structured reasoning and AI-driven content validation, ensuring that consultants always have the most accurate, up-to-date insights at their fingertips. The outcome: higher productivity, faster client delivery and a knowledge system that scales with the firm’s growth.\nIf AI can summarize thousands of consulting documents in minutes, how much more value could your team create by focusing only on insights instead of searching for them?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xrg3b",
          "author": "prismaticforge",
          "text": "Have you had feedback from the users?  Are they happy with the results and using the tool.  I am curious also how you handle contradictory information from rag?",
          "score": 2,
          "created_utc": "2026-02-23 11:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xnour",
          "author": "jannemansonh",
          "text": "nice setup... building rag pipelines with custom orchestration is solid but maintaining that glue code gets brutal over time. ended up using needle app for similar doc workflows since it handles the vector db + workflow orchestration in one place (just describe what you want vs wiring everything). kept custom stuff for edge cases though",
          "score": 1,
          "created_utc": "2026-02-23 11:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xoaiw",
              "author": "Safe_Flounder_4690",
              "text": "That’s a valid point maintaining custom orchestration can become complex as systems scale and edge cases increase. Managed platforms can reduce operational overhead and speed up deployment.\n \nThat said, custom pipelines still offer deeper control over retrieval logic, validation and integration with internal processes. The right balance often comes from standardizing core infrastructure while keeping flexibility where business-specific reasoning and accuracy matter most.",
              "score": 1,
              "created_utc": "2026-02-23 11:28:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y2094",
          "author": "Tired__Dev",
          "text": "It’s the actionable summaries among chunks that would have me worried tbh.",
          "score": 1,
          "created_utc": "2026-02-23 13:10:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y2dma",
          "author": "Semoho",
          "text": "Did you have benchmark o test dataset that how this approach effects the system?",
          "score": 1,
          "created_utc": "2026-02-23 13:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yc7ri",
          "author": "ChapterEquivalent188",
          "text": "how do you test ? do you trust your llm ?",
          "score": 1,
          "created_utc": "2026-02-23 14:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zeo3u",
          "author": "AmphibianNo9959",
          "text": "For personal use, I've been using Reseek to handle a lot of that ingestion and semantic search piece automatically. It pulls text from PDFs and images, tags everything, and makes my own notes and bookmarks searchable in a similar way. ",
          "score": 1,
          "created_utc": "2026-02-23 17:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zrox9",
              "author": "Material-River-2235",
              "text": "Ok, I will go take a look at it.",
              "score": 1,
              "created_utc": "2026-02-23 18:19:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zf2rh",
          "author": "nasnas2022",
          "text": "Can you add few more details on the pipeline",
          "score": 1,
          "created_utc": "2026-02-23 17:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zj2uk",
          "author": "_os2_",
          "text": "We have built something similar with [Skimle](https://skimle.com), but our tool skips RAG completely and instead builds the categorization scheme in the beginning with LLM calls and then retrieves from a structured table rather than at runtime. Enables two-way transparency and stable responses.\n\nWould be great to compare results!",
          "score": 1,
          "created_utc": "2026-02-23 17:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7125c1",
          "author": "Infamous_Ad5702",
          "text": "Well done. Do you build a knowledge graph?\nAnd why Vector?\nI had the same project for a client and went with a custom tool called Leonata. It builds an index and works totally offline. No LLM. No GPU. And No hallucination. \n\nFor my client purpose, semantic retrieval, Vector found similar info but not the best fit.",
          "score": 1,
          "created_utc": "2026-02-23 21:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9sxv7",
      "title": "Introducing Legal RAG Bench",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9sxv7/introducing_legal_rag_bench/",
      "author": "Neon0asis",
      "created_utc": "2026-02-20 11:30:41",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "# tl;dr\n\nWe’re releasing [**Legal RAG Bench**](https://huggingface.co/datasets/isaacus/legal-rag-bench), a new reasoning-intensive benchmark and evaluation methodology for assessing the end-to-end, real-world performance of legal RAG systems.\n\nOur evaluation of state-of-the-art embedding and generative models on Legal RAG Bench reveals that information retrieval is the primary driver of legal RAG performance rather than reasoning. We find that the [Kanon 2 Embedder](https://isaacus.com/blog/introducing-kanon-2-embedder) legal embedding model, in particular, delivers an average accuracy boost of 17 points relative to Gemini 3.1 Pro, GPT-5.2, Text Embedding 3 Large, and Gemini Embedding 001.\n\nWe also infer based on a statistically robust hierarchical error analysis that most errors attributed to hallucinations in legal RAG systems are in fact triggered by retrieval failures.\n\nWe conclude that information retrieval sets the ceiling on the performance of modern legal RAG systems. While strong retrieval can compensate for weak reasoning, strong reasoning often cannot compensate for poor retrieval.\n\nIn the interests of transparency, we have openly released Legal RAG Bench on [Hugging Face](https://huggingface.co/datasets/isaacus/legal-rag-bench), added it to the [Massive Legal Embedding Benchmark (MLEB)](https://isaacus.com/mleb), and have further presented the results of all evaluated models in an interactive explorer shown towards the end of this blog post. We encourage researchers to both scrutinize our data and build upon our novel evaluation methodology, which leverages full factorial analysis to enable hierarchical decomposition of legal RAG errors into hallucinations, retrieval failures, and reasoning failures.\n\n**Source:** [**https://isaacus.com/blog/legal-rag-bench**](https://isaacus.com/blog/legal-rag-bench)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r9sxv7/introducing_legal_rag_bench/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6es8y4",
          "author": "Fetlocks_Glistening",
          "text": "So you're baaically saying retrieval is important? I mean, what exactly is new here?",
          "score": 3,
          "created_utc": "2026-02-20 12:22:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6l98yf",
              "author": "Neon0asis",
              "text": "Well theres a lot new in terms of actually isolating the effects of the two stages of the RAG pipeline. The methodology uses a factorial experiment, where each embedder and LLM is paired together, allowing for the estimation of the causal effect of each component in terms of the outcome variables of correctness and groundedness. \n\nThis type of estimation is only statistically sound if you can establish interlinked ground-truths for both the retriever and LLM at the question level. So if a RAG pipeline gets an answer wrong, we can examine whether the retreiver failed to get the 'gold passage' (this would plausibly explain the failure at the pipeline level), and if it did get the gold passage, how come the LLM was unable to come up with the correct answer despite having  all of the relevant context to do so. In the latter case, we can infer that the LLM's reasoning abilities are at fault, allowing for a clear decomposition, at the pipeline level, between reasoning and retrieval errors.\n\nJust because an intuition exists as popular advice, does not mean it is supported by the science or evidence. There's value in conclusively proving something that seems obvious, particularly if you can better understand the causal pattern and mechanics, allowing you to draw more sophistacted inferences from that seemingly obvious assumption.",
              "score": 2,
              "created_utc": "2026-02-21 12:19:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rclvtn",
      "title": "first RAG project, really not sure about my stack and settings",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "author": "Kas_aLi",
      "created_utc": "2026-02-23 16:21:16",
      "score": 17,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "Hey guys, so ive been working on my first RAG project. its basically a system that takes medical PDFs (textbooks, clinical guidelines) and builds a knowledge graph from them to generate multi-choice exam questions for a medical exam. \n\nInput style: large textbooks, pdf, images, tables, etc\n\nI have been coding this like a monkey with claude opus 4.6 and codex 5.3 honestly, just prompting my way through it. it works but i have no idea if what im doing is the right approach.\n\nWould love some feedback, good sources or learning resources.  \n  \nHere is my current stack for context:\n\n    PDF → Docling (no OCR, native text) → markdown export with page breaks\n        → heading-based chunker (~768 tok, tiktoken cl100k)\n          → noise classifier (regex heuristics, filters TOC/references/headers)\n          → batch extraction (3 chunks/batch, 4K token cap, 4 parallel workers)\n            → Instructor (JSON mode) + Gemini 2.5 Flash via OpenRouter (it is cheap, but probably there are better now)\n            → Pydantic schema: concepts (18 types) + claims (25 predicates) + evidence spans\n            → fallback: batch fail → individual chunk extraction\n          → concept normalization + dedup\n          → quality gate (error rate, claims/chunk, evidence/claim, noise ratio, page coverage)\n          → embeddings: Qwen3-embedding-8b (1024d) → pgvector\n    \n    storage: supabase (27 tables)\n    orchestration: langgraph (for downstream question generation, not ETL)\n    \n    all LLM calls through openrouter",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6z75uc",
          "author": "Higgs_AI",
          "text": "Can I just ask you if this is for taking exams? If so… DM me. Overall it’s well put together… the bones are solid. The fact that it has a quality gate at all puts it ahead of most implementations. 🤷🏽‍♂️\n\nHad to edit: this is really clean work…the schema enforced extraction with Instructor, the quality gate with multiple metrics, the batch fallback logic. Most people building extraction pipelines skip half of what you’ve done here… bravo brotha. \n\nyour question generation is downstream and separate. What if the extraction and the pedagogy were part of the same adaptive loop where how the learner performs on generated questions feeds back into which concepts need deeper extraction, which claims need more evidence, which connections need to be surfaced?\n\nThere’s other stuff I’d say but, I just thought I’d give you some substance without flooding your shit. Good work",
          "score": 3,
          "created_utc": "2026-02-23 16:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z8hqn",
              "author": "Kas_aLi",
              "text": "Main usecase will be generating new and original exam questions but it should be good at taking the exam as well I guess... I think GPT 5.2 had more than 98% correct answers in this particular exam already so...",
              "score": 1,
              "created_utc": "2026-02-23 16:49:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zp35o",
          "author": "Semoho",
          "text": "Nice orchestration. \n\nYou can have a little academic approach to assess your system. In my opinion, you can create a test dataset from your exam or even from your knowledge of the docs (corpus). Then, run testing on GPT 5.2 and get the answers and also with your system. Now you have benchmark from gpt or other models and your system, so you can check if the results are good or not. We call it the test phase or making a test dataset. This gives you the power to assess your system. (Usually, you can see the baselines on the academic papers that they need to improve the baseline to compete with other approaches.)\n\nAlso, I would recommend using Knowledge Graph if you have relational data, such as something belongs to one paper/book, and there is some other evidence on the other resources.\n\n  \nSome tips and tricks to increase the retrieval phase:\n\n1. Use task-specific embedding models such as [https://huggingface.co/abhinand/MedEmbed-base-v0.1](https://huggingface.co/abhinand/MedEmbed-base-v0.1)\n\n2. Use other vector DBs like Milvus. Vector DB is so easy to set up, but it does not provide good accuracy.\n\n3. Check the knowledge graph.",
          "score": 3,
          "created_utc": "2026-02-23 18:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77spi7",
              "author": "rigatoni-man",
              "text": "I've been building something to test models without a lot of overhead and legwork.  Basically upload your golden dataset and test it against every model out there.\n\nShoot me a message u/Kas_aLi  and I'd love to help you find the best model for free to test what i'm building ( [https://checkstack.ai](https://checkstack.ai) )",
              "score": 1,
              "created_utc": "2026-02-24 21:55:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77toou",
                  "author": "Semoho",
                  "text": "It is a good platform. But as an IR guy to publish different papers in the information retrieval field, I think I do not need it :))  \nHowever, if I find time, I would check your platform",
                  "score": 1,
                  "created_utc": "2026-02-24 22:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zcxj3",
          "author": "shlok-codes",
          "text": "I use DeepSeek nitro chat via openrouter look into DeepSeek and qwen models",
          "score": 2,
          "created_utc": "2026-02-23 17:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ivc1",
          "author": "avebrahimi",
          "text": "Awesome stack.  \nWhat about UI?",
          "score": 1,
          "created_utc": "2026-02-24 07:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tl37",
          "author": "LuckEcstatic9842",
          "text": "You might also want to take a look at LightRAG.",
          "score": 1,
          "created_utc": "2026-02-24 08:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1fde",
          "author": "aidenclarke_12",
          "text": "the heading-based chunker at 768 tokens works for linear textbook prose but clinical tables and cross-references tend to fragment badly across batch boundaries.. the 3 chunks per batch with 4k cap means a pharmacology table that spans two chunks might lose the column headers on one side and the values on the other. \n\nthe quality gate catching this after the fact is better than nothing but fixing it upstream in the chunking logic is probably worth exploring.. \n\non the model side you mentioned openrouter and questioned whether there are better options now.. runpod, together or deepinfra give direct access to the same gemini flash and qwen3 models at lower per-token cost which matters if you're running high extraction volumes on large textbooks",
          "score": 1,
          "created_utc": "2026-02-25 11:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g66sn",
          "author": "New_Direction5479",
          "text": "Awesome stack but Embedding storing go with Qdrant ,add some open source reranker model's also and make 2 application 1. UI to Ingestion , 2. UI to Inference application.",
          "score": 1,
          "created_utc": "2026-02-26 03:02:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1regz4t",
      "title": "I think most RAG quality issues people post about here are actually extraction problems, not retrieval problems",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1regz4t/i_think_most_rag_quality_issues_people_post_about/",
      "author": "yfedoseev",
      "created_utc": "2026-02-25 15:53:50",
      "score": 17,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Every other post in this sub is \"my RAG pipeline hallucinates\" and the replies are always the same: try a different chunking strategy, use a better embedding model, add reranking, etc.\n\nNobody ever says \"go look at what your PDF parser actually output.\"\n\nI did. I took 3,830 real-world PDFs (veraPDF corpus, Mozilla pdf.js tests, DARPA SafeDocs) and ran them through the major Python parsers. Not cherry-picked -- government filings, academic papers, scanned forms, edge cases from the 90s, encrypted files, CJK text, the works.\n\n    Library      Mean     p99      Pass rate\n    ──────────────────────────────────────────\n    pdf_oxide    0.8ms     9ms     100%\n    PyMuPDF      4.6ms    28ms     99.3%\n    pypdfium2    4.1ms    42ms     99.2%\n    pdfminer    16.8ms   134ms     98.8%\n    pdfplumber  23.2ms   189ms     98.8%\n    pypdf       12.1ms    97ms     98.4%\n\nHere's the thing nobody talks about: a 98.4% pass rate on 3,830 docs means \\~60 documents that silently fail. They crash, hang, or return empty strings. Those docs never enter your vector store. When a user asks about content from one of those documents, the retrieval step finds nothing relevant, so the LLM fills in the gap with a confident hallucination.\n\nYou debug the prompt. You debug the retrieval. You never think to check whether the document was even indexed.\n\nI built pdf\\_oxide (Rust, Python bindings) partly because I kept running into this. The thing that made the biggest difference for me wasn't the speed, it was the Markdown output with heading detection:\n\n        from pdf_oxide import PdfDocument\n    \n        doc = PdfDocument(\"paper.pdf\")\n        md = doc.to_markdown(0, detect_headings=True)\n\nYou get actual structure back. Headings, paragraphs, sections. Chunk on section boundaries instead of arbitrary token windows. Each chunk ends up being about one topic instead of the tail end of one section glued to the beginning of another. Retrieval precision went up noticeably for me once I switched to heading-based splits.\n\nBuilt-in OCR too (PaddleOCR via ONNX Runtime). It auto-detects scanned pages and falls back. No Tesseract, no subprocess shelling out, no extra config.\n\n        pip install pdf_oxide\n\nMIT licensed. No AGPL. Runs entirely locally.\n\nLimitations I won't hide: table extraction is basic compared to pdfplumber. There are \\~10 edge-case PDFs that still have minor extraction issues (tracked on GitHub). WASM support isn't done yet.\n\n[github.com/yfedoseev/pdf\\_oxide](http://github.com/yfedoseev/pdf_oxide)   \nDocs: [oxide.fyi](http://oxide.fyi)\n\nGenuine question for this sub: how many of you have actually diffed your parser's output against the source PDF? I'm starting to think a lot of the \"retrieval quality\" problems people debug for weeks are just garbage going in at step one.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1regz4t/i_think_most_rag_quality_issues_people_post_about/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7ci7ij",
          "author": "ChapterEquivalent188",
          "text": "have you tried docling ? \njust in case, i thought iĺl be on my own for ever ;) \nhttps://github.com/2dogsandanerd/RAG_enterprise_core",
          "score": 2,
          "created_utc": "2026-02-25 16:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7he74d",
          "author": "lucasbennett_1",
          "text": "this is the truth..most rag complaints are actually bad extraction in disguise.. inspect your parser output before anything else.. pdf\\_oxide looks like a solid fix for that.",
          "score": 2,
          "created_utc": "2026-02-26 08:35:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f0wlv",
          "author": "SubstantialTea707",
          "text": "Io estraggo le immagini e le leggo con il modello glm ocr, tra estrazione e llm che gira su una 5090 ci perdo 5s a pagina e da ottimi risultati in estrazione",
          "score": 1,
          "created_utc": "2026-02-25 23:10:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fau76",
              "author": "yfedoseev",
              "text": "It works as well if you have this 5 sec for processing",
              "score": 1,
              "created_utc": "2026-02-26 00:05:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fhd5w",
          "author": "patbhakta",
          "text": "For tables, formulas, charts, etc. you need specialized parsers. OCR is fine for text, VL is fine for photos, you need different tools for different things.",
          "score": 1,
          "created_utc": "2026-02-26 00:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gdw1k",
          "author": "hhussain-",
          "text": "What difference rust gave you in here? Or what made you use rust if it is all python ?",
          "score": 1,
          "created_utc": "2026-02-26 03:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gifx9",
              "author": "yfedoseev",
              "text": "Rust gave me a significant performance boost and allowed me to build bindings to most programming languages guaranteed high performance everywhere.",
              "score": 2,
              "created_utc": "2026-02-26 04:17:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gjprz",
          "author": "New_Animator_7710",
          "text": "In our lab, we’ve repeatedly observed that retrieval quality correlates more strongly with structural fidelity of extraction than with embedding choice. Clean section boundaries often outperform swapping to a “better” model.",
          "score": 1,
          "created_utc": "2026-02-26 04:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hb3qw",
          "author": "stevevaius",
          "text": "How about laws? Many laws has sections, sub-sections, articles, quotes etc...? Did you run any test for legal texts?",
          "score": 1,
          "created_utc": "2026-02-26 08:06:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hbmd8",
              "author": "yfedoseev",
              "text": "Honestly, I did and the text conversation works well, but markdown,.something that requires more structure still requires improvement for some corner cases. Legal docs are on my radar. Thank you for your question.",
              "score": 2,
              "created_utc": "2026-02-26 08:10:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hck08",
                  "author": "stevevaius",
                  "text": "Looking fwd to hear about your project, specially on legal text developments. Best regs",
                  "score": 1,
                  "created_utc": "2026-02-26 08:19:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1reivma",
      "title": "Agentic RAG for Dummies v2.0",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1reivma/agentic_rag_for_dummies_v20/",
      "author": "CapitalShake3085",
      "created_utc": "2026-02-25 17:01:02",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Hey everyone! I've been working on **Agentic RAG for Dummies**, an open-source project that shows how to build a modular Agentic RAG system with LangGraph — and today I'm releasing v2.0.\n\nThe goal of the project is to bridge the gap between basic RAG tutorials and real, extensible agent-driven systems. It supports any LLM provider (Ollama, OpenAI, Anthropic, Google) and includes a step-by-step notebook for learning + a modular Python project for building.\n\n## What's new in v2.0\n\n🧠 **Context Compression** — The agent now compresses its working memory when the context exceeds a configurable token threshold, keeping retrieval loops lean and preventing redundant tool calls. Both the threshold and the growth factor are fully tunable.\n\n🛑 **Agent Limits & Fallback Response** — Hard caps on tool invocations and reasoning iterations ensure the agent never loops indefinitely. When a limit is hit, instead of failing silently, the agent falls back to a dedicated response node and generates the best possible answer from everything retrieved so far.\n\n## Core features\n\n- Hierarchical indexing (parent/child chunks) with hybrid search via Qdrant\n- Conversation memory across questions\n- Human-in-the-loop query clarification\n- Multi-agent map-reduce for parallel sub-query execution\n- Self-correction when retrieval results are insufficient\n- Works fully local with Ollama\n\nThere's also a Google Colab notebook if you want to try it without setting anything up locally.\n\nGitHub: https://github.com/GiovanniPasq/agentic-rag-for-dummies",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1reivma/agentic_rag_for_dummies_v20/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r99uaf",
      "title": "We built a hybrid retrieval system combining keyword + semantic + neural reranking — here's what we learned",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r99uaf/we_built_a_hybrid_retrieval_system_combining/",
      "author": "True-Snow-1283",
      "created_utc": "2026-02-19 20:07:30",
      "score": 15,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "Hey r/RAG,\n\nI've been working on retrieval systems for a while now and wanted to share some insights from building Denser Retriever, an end-to-end retrieval platform.\n\n**The problem we kept hitting:**\n\nPure vector search misses exact matches (product IDs, error codes, names). Pure keyword search misses semantic meaning. Most RAG setups use one or the other, or bolt them together awkwardly.\n\n**Our approach — triple-layer retrieval:**\n\n1. **Keyword search** (Elasticsearch BM25) — handles exact matches, filters, structured queries\n2. **Semantic search** (dense vector embeddings) — catches meaning even when wording differs\n3. **Neural reranking** (cross-encoder) — takes the combined candidates and re-scores them with full query-document attention\n\n**Key learnings:**\n\n* Chunk size matters more than embedding model choice. We use 2000-character chunks with 10% overlap (200 characters). This gives\n* For technical docs, keyword search still wins \\~30% of the time over pure semantic. Don't drop it.\n* Reranking top-50 candidates is the sweet spot between latency and accuracy for most use cases.\n* Document parsing quality is the silent killer. Garbage in = garbage out, no matter how good your retrieval is.\n\n**Architecture:**\n\nUpload docs → Parse (PDF/DOCX/HTML → Markdown) → Chunk → Embed → Index into Elasticsearch (both BM25 and dense vector)\n\nAt query time: BM25 retrieval + vector retrieval → merge → neural rerank → top-K results\n\nWe've open-sourced the core retriever logic and also have a hosted platform at [retriever.denser.ai](http://retriever.denser.ai) if you want to try it without setting up infrastructure.\n\nHappy to answer questions about the architecture or share more specific benchmarks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r99uaf/we_built_a_hybrid_retrieval_system_combining/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6bysa3",
          "author": "Infamous_Ad5702",
          "text": "Sounds like a solid approach. I did the same thing. Vector is weak finds similar.\nI couldn’t use the LLM for my client (bias, hallucination, cost)\n\nSo we went back to old school; deep semantics and deterministic techniques, pure maths.\n\nSo now we have a deep search tool that maps a knowledge graph for every new query it gets.\n\nIs context specific. Can’t hallucinate and needs zero GPU. We’re pumped.",
          "score": 6,
          "created_utc": "2026-02-19 23:40:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ixe1c",
              "author": "True-Snow-1283",
              "text": "Interesting: So now we have a deep search tool that maps a knowledge graph for every new query it gets. Is it fast?",
              "score": 2,
              "created_utc": "2026-02-21 01:00:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6jza0y",
                  "author": "Infamous_Ad5702",
                  "text": "I think it’s fast. I’m bias…would love to get opinions?",
                  "score": 1,
                  "created_utc": "2026-02-21 05:12:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6chaon",
          "author": "Academic_Track_2765",
          "text": "Its not new brother. This type of systems have been in production since 2020-2022. bi-encoders / cross-encoders / cosine search on embeddings + bm25 have been used since 2020, please build something new. Each rag post is the same old stuff, from 2020 to 2022.  Maybe try a DRF model or Gaussian embeddings, please something different / new from the same thing everyone does once they finally realize there is more to embeddings than just throwing them in a DB and wondering why retrieval is so poor. sbert people are ashamed. Also I think you are the same guy trying to sell his product from a while ago LOL. ",
          "score": 3,
          "created_utc": "2026-02-20 01:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ixuxb",
              "author": "True-Snow-1283",
              "text": "Fair point — BM25 + bi-encoders + cross-encoders isn't new research. We're not claiming to invent the technique. The value is in making it production-ready and accessible as a managed platform so teams don't have to wire up Elasticsearch, embedding models, and reranking themselves.",
              "score": 1,
              "created_utc": "2026-02-21 01:03:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6ks30q",
              "author": "eurydice1727",
              "text": "lmaooo",
              "score": 1,
              "created_utc": "2026-02-21 09:39:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o6nsb10",
              "author": "FeeMassive4003",
              "text": "The guy shared his lesson learned from his interesting work. He didn't claim it is novel. I find this post useful.",
              "score": 1,
              "created_utc": "2026-02-21 20:37:11",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6ntfbd",
                  "author": "Academic_Track_2765",
                  "text": "Good! Glad you enjoyed it.",
                  "score": 2,
                  "created_utc": "2026-02-21 20:43:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6cs03t",
          "author": "datguywelbzy",
          "text": "Why not qmd on GitHub ?",
          "score": 2,
          "created_utc": "2026-02-20 02:36:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6iymit",
              "author": "True-Snow-1283",
              "text": "We open-sourced the core retriever library on GitHub a while ago. The hosted platform adds managed infrastructure on top — document parsing, indexing, API — for teams that don't want to self-host.\n\nWe also recently built a Claude Code skill that lets you create a knowledge base from PDFs and query it in natural language, all from the terminal: [https://retriever.denser.ai/blog/build-rag-knowledge-base-claude-code](https://retriever.denser.ai/blog/build-rag-knowledge-base-claude-code)",
              "score": 1,
              "created_utc": "2026-02-21 01:07:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1reu6t0",
      "title": "For teams selling internal AI search/RAG: what does user behavior actually look like?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1reu6t0/for_teams_selling_internal_ai_searchrag_what_does/",
      "author": "adukhet",
      "created_utc": "2026-02-25 23:49:07",
      "score": 15,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Just like the title; question for people actually selling RAG/enterprise AI search products (not demos, not internal tools):\n\nHave you ever measured average user session length?\n\nI’m especially curious about real production usage, not benchmarks.  \n\nIf you’re willing to share, it would be super helpful to include:\n\n\\- vertical (legal, support, sales, engineering, etc.)\n\n\\- main use case (knowledge search, support copilot, internal documentation, analyst workflows…)\n\n\\- average time spent in a session\n\n\\- roughly how many queries per session\n\nI’m trying to understand actual behavioral patterns of users interacting with RAG systems. Papers and blog posts talk a lot about retrieval accuracy, but almost nothing about how people actually use these systems once deployed.\n\nHard to get this data without already operating one at scale, so even rough ranges or anonymized observations would be incredibly useful",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1reu6t0/for_teams_selling_internal_ai_searchrag_what_does/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7hpxe5",
          "author": "megAchiever",
          "text": "btw if anyone has such tool would like to incorporate for some of my clients. Reach to me if you have a solid RAG tool, specifically for long pdfs!",
          "score": 1,
          "created_utc": "2026-02-26 10:28:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcehj1",
      "title": "Built an offline MCP server that stops LLM context bloat using local vector search over a locally indexed codebase.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcehj1/built_an_offline_mcp_server_that_stops_llm/",
      "author": "Trust_Me_Bro_4sure",
      "created_utc": "2026-02-23 11:00:50",
      "score": 14,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Searching through a massive codebase to find the right context for AI assistants like Claude was becoming a huge bottleneck for me—hurting performance, cost, and accuracy. You can't just dump entire files into the prompt; it instantly blows up the token limit, and the LLM loses track of the actual task.\n\n\n\nInstead of LLM manually hunting for correct files using grep/find &  dumping raw file content into the prompt, I wanted the LLM to have a better search tool.\n\nSo, I built code-memory: an open-source, offline MCP server you can plug right into your IDE (Cursor/AntiGravity) or Claude Code.\n\n\n\nHere is how it works under the hood:\n\n1. Local Semantic Search: It runs vector searches against your locally indexed codebase using jinaai/jina-code-embeddings-0.5b model. \n\n2. Smart Delta Indexing: Backed by SQLite, it checks file modification times during indexing. Unchanged files are skipped, meaning it only re-indexes what you've actually modified. \n\n3. 100% Offline: Your code never leaves your machine.\n\n\n\nIt is heavily inspired by claude-context, but designed from the ground up for large-scale, efficient local semantic search. It's still in the early stages, but I am already seeing noticeable token savings on my personal setup!\n\n\n\nI'd love to hear feedback, especially if you have more ideas!\n\nCheck out the repo here: [https://github.com/kapillamba4/code-memory](https://github.com/kapillamba4/code-memory)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcehj1/built_an_offline_mcp_server_that_stops_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xlz2v",
          "author": "picturpoet",
          "text": "will it handle something like when the dev says look into “auth” and it knows it has to look into authentication files?",
          "score": 1,
          "created_utc": "2026-02-23 11:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmkf0",
              "author": "Trust_Me_Bro_4sure",
              "text": "Yes, This is from one of my personal projects:\n\nhttps://preview.redd.it/wiwlcj5i98lg1.png?width=1178&format=png&auto=webp&s=1bf55ebce664e0007746dfded7ad5d2f9a5edbe1\n\n",
              "score": 1,
              "created_utc": "2026-02-23 11:13:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72nuf3",
          "author": "Oshden",
          "text": "This is awesome! Thanks for sharing it OP!",
          "score": 1,
          "created_utc": "2026-02-24 03:21:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdeibi",
      "title": "Fresh grad learning RAG, feeling lost, looking for guidance",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "author": "savinox23",
      "created_utc": "2026-02-24 12:07:10",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "Hello, I am a fresh grad trying to learn about RAG and develop my coding skills. I made this simple cooking assistant based on Moroccan recipes. Could you please tell me how I can improve my stack/architecture knowledge and my code?\n\nWhat I currently do is discuss best practices with ChatGPT, try to code it myself using documentation, then have it review my code. But I feel like I'm trying to learn blindly. It's been 6 days and I've only made this sloppy RAG, and I feel like there is a better way to do this.\n\nHere’s the link to a throwaway repo with my code (original repo has my full name haha):\n\n  \n[https://github.com/Savinoy/Moroccan-cooking-assistant](https://github.com/Savinoy/Moroccan-cooking-assistant?utm_source=chatgpt.com)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o74uxot",
          "author": "RobertLigthart",
          "text": "6 days and you already have a working RAG is not sloppy thats actually decent progress. most people spend weeks just trying to get embeddings to work\n\n  \nthe chatgpt code review loop is fine for learning syntax but for architecture you need to read how other people built theirs. check out the rag-from-scratch series by lance martin on youtube... its hands down the best resource for understanding why the pieces fit together not just how to copy paste them\n\n  \nbiggest thing I'd improve early on: add chunking strategy to your pipeline if you havent already. most beginners just dump full documents into the vector store and wonder why retrieval is bad. experiment with chunk sizes and overlap... makes a massive difference",
          "score": 4,
          "created_utc": "2026-02-24 13:44:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74yjcp",
              "author": "savinox23",
              "text": "Thank you very much, i appreciate it !! I’ll check out the series. As for the chunking strategy, I experimented with a standard text splitter but it was giving me mixed/ incomplete recipe answers, so i made each separate recipe as a chunk and the results were better, but i will check out the resources you mentioned, I’m sure there are better methods i can try!",
              "score": 1,
              "created_utc": "2026-02-24 14:04:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o78hh8u",
          "author": "Asleep_Carpet_3403",
          "text": "Great to see someone trying to learn through manual coding and 6 days to a working Rag is absolutely impressive. One suggestion is to start using tools like cursor with their auto complete functionality. It'll 10x your coding speed while you still continue to learn and write the complete code yourself. \n\nA good skill to have now is to learn inference (deploying and using ML/DL models) for this you may consider including a data extraction model from scanned pages (images) upstream of your RAG pipeline",
          "score": 2,
          "created_utc": "2026-02-25 00:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78jocr",
              "author": "savinox23",
              "text": "Okay, thank you very much for the advice !!",
              "score": 1,
              "created_utc": "2026-02-25 00:16:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rdc18r",
      "title": "So what are you all using for RAG in 2026?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rdc18r/so_what_are_you_all_using_for_rag_in_2026/",
      "author": "ReporterCalm6238",
      "created_utc": "2026-02-24 09:50:24",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Looking for easy but effective ways of integratong RAG in my applications. Is there a clear winner framework/tool in terms of performance and quick serup?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rdc18r/so_what_are_you_all_using_for_rag_in_2026/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7449rk",
          "author": "hrishikamath",
          "text": "https://github.com/kamathhrishi/finance-agent I just chain API calls together with pgvector and works well.",
          "score": 5,
          "created_utc": "2026-02-24 10:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o740miv",
          "author": "xeraa-net",
          "text": "1. Retrieval is any retrieval, not just vector search. We all agree on that, right?!\n\n2. Do frameworks still matter as much with all the code generation? \n\n3. Good retrieval features are now table stakes and vector search is a feature, not a product (though I'm biased here).",
          "score": 5,
          "created_utc": "2026-02-24 10:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75i847",
          "author": "yafitzdev",
          "text": "I build a RAG with production use in mind. I use a yaml plugin system to use any LLM (local or cloud). Also implemented epistemic honesty constraints that I benchmark, super important for production use. The retrieval intelligence is the key piece:\n\n[yafitzdev/fitz-ai](https://github.com/yafitzdev/fitz-ai)",
          "score": 2,
          "created_utc": "2026-02-24 15:41:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o793soy",
          "author": "prodigy_ai",
          "text": "If you plan to run RAG in production, AWS and Azure offer managed services for embeddings, vector search, orchestration, and security, which can reduce operational overhead compared to self-hosted setups.",
          "score": 2,
          "created_utc": "2026-02-25 02:08:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75hkxy",
          "author": "http418teapot",
          "text": "Can you share more about your use case, data, and expected queries? It would also be helpful to understand what the current tech stack is that you're trying to integrate into. This info will help people make recommendations that are more fit for you.\n\nIf you're truly looking for quick setup, I recommend Pinecone Assistant as it handles all the data chunking, embedding, search, and reranking for you.",
          "score": 1,
          "created_utc": "2026-02-24 15:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77e1ey",
          "author": "pgEdge_Postgres",
          "text": "Any feedback on our RAG server for PostgreSQL (open source under the PostgreSQL license) would be well appreciated here, if any have used it or want to check it out. [https://github.com/pgEdge/pgedge-rag-server](https://github.com/pgEdge/pgedge-rag-server)",
          "score": 1,
          "created_utc": "2026-02-24 20:48:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a95cx",
          "author": "DeadPukka",
          "text": "Check out [Graphlit](https://www.graphlit.com). Handles all your unstructured data ingestion, embeddings and multimodal search. (Just added TwelveLabs embeddings for video search.)\n\nFree plan and SDKs for Python and TS. Can share as MCP with retrieval tools. \n\nDon’t bother building anything DIY these days.",
          "score": 1,
          "created_utc": "2026-02-25 06:42:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cwcpo",
          "author": "ExtraManagement1329",
          "text": "Depends on your constraints. If data privacy matters or you're in a regulated industry, the managed options  become a problem quickly — your documents sit on their servers.\n\nFor quick setup with everything in one place, I've been building [Essofore](https://aws.amazon.com/marketplace/pp/prodview-ux72f4arqu376) — it's a self-hosted AMI that runs in your own AWS VPC. Handles chunking, embeddings, vector storage and search automatically so you don't have to wire up the pipeline yourself. *Upload a document, run a search query, done*.\n\nIf you're fine with SaaS, LangChain + a vector db Pinecone/Qdrant/Milvus/Chroma/Weaviate is still the path of least resistance for prototyping. If you want self-hosted and simple, worth a look.",
          "score": 1,
          "created_utc": "2026-02-25 17:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g4a7h",
          "author": "astro_abhi",
          "text": "Check out VectraSDK - https://vectra.thenxtgenagents.com , I built it specifically to solve and make an SDK which is open source and LLM Agnostic framework",
          "score": 1,
          "created_utc": "2026-02-26 02:51:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ha1jn",
          "author": "martinschaer",
          "text": "My stack: [SurrealDB](https://surrealdb.com) (vector + graph + …), Pydantic AI, Logfire, Kreuzberg (doc parsing)",
          "score": 1,
          "created_utc": "2026-02-26 07:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hhy2c",
          "author": "Big_Barnacle_2452",
          "text": "If you hit the usual ceiling (wrong chunks, vague answers, bad performance on long/structured docs), it’s worth trying structure-based retrieval instead of only vector search. We built ReasonDB for that: keeps document hierarchy (headings → sections) and lets the LLM navigate the tree instead of “top-k similar chunks.” SQL-like query language (RQL) with SEARCH + REASON clauses. Works with OpenAI, Anthropic, Gemini, Cohere, or open source models (GLM, Kimi).\n\n[https://github.com/reasondb/reasondb](https://github.com/reasondb/reasondb) \\- open source.   \nDocs at [reason-db.devdoc.sh](http://reason-db.devdoc.sh) if you want to try it.",
          "score": 1,
          "created_utc": "2026-02-26 09:12:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9w8u0",
      "title": "Why Standard RAG Often Hallucinates Laws — and How I Built a Legal Engine That Never Does (Tested in Italian Legal Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9w8u0/why_standard_rag_often_hallucinates_laws_and_how/",
      "author": "Pretend-Promotion-78",
      "created_utc": "2026-02-20 14:05:47",
      "score": 13,
      "num_comments": 6,
      "upvote_ratio": 0.79,
      "text": "Hi everyone,\n\nHave you ever had that *false confidence* when an LLM answers a technical question — only to later realize it confidently cited something incorrect? In legal domains, that confidence is the *number one danger*.\n\nWhile experimenting with a standard RAG setup, the system confidently quoted a statute that seemed plausible… until we realized that provision was **repealed in 2013**. The issue wasn’t just old training data — it was that the system relied on *frozen knowledge* or poorly verified external sources.\n\nThis was something I had seen mentioned multiple times in other posts where people shared examples of legal documents with entirely fabricated statutes. That motivated me — as an Italian developer — to solve this problem in the context of **Italian law, where the code is notoriously messy and updates are frequent**.\n\nTo address this structural failure, I built **Juris AI**.\n\n# The Problem with Frozen Knowledge\n\nMost RAG systems are static: you ingest documents once and *hope* they stay valid. That rarely works for legal systems, where legislation evolves constantly.\n\nJuris AI tackles this with two key principles:\n\n**Dynamic Synchronization**  \nEvery time the system starts, it performs an incremental alignment of its sources to ensure the knowledge base reflects the *current state of the law*, not a stale snapshot.\n\n**Data Honesty**  \nIf a norm is repealed or lacks verified text, the system does not guess. It *reports the boundary of verification* instead of hallucinating something plausible but wrong.\n\n# Under the Hood\n\nFor those interested in the architecture but not a research paper:\n\n**Hybrid Graph-RAG**  \nWe represent the legal corpus as a *dependency graph*. Think of this as a connected system where each article knows the law it belongs to and its references.\n\n**Deterministic Orchestration Layer**  \nA proprietary logic layer ensures generation *follows validated graph paths*.  \nFor example, if the graph marks an article as “repealed,” the system is *blocked from paraphrasing* outdated text and instead reports the current status.\n\n# Results (Benchmark Highlights)\n\nIn stress tests against traditional RAG models:\n\n* **Zero hallucinations on norm validation** — e.g., on articles with suffixes like *Art. 155-quinquies*, where standard models often cite repealed content, Juris AI always identified the correct current status.\n* **Cross-Database Precision** — in complex scenarios such as linking aggravated theft (Criminal Code *Art. 625*) to civil liability norms (Civil Code *Art. 2043+*), Juris AI reconstructed the entire chain with literal text, while other systems fell back to general paraphrase.\n\n# Why I’m Sharing This Here\n\nThis is *not* a product pitch. It’s a technical exploration and I’m curious:\n\n**From your experience with RAG systems, in which scenarios does a deterministic validation approach become** ***essential*** **versus relying on traditional semantic retrieval alone?**",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r9w8u0/why_standard_rag_often_hallucinates_laws_and_how/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6fo9s8",
          "author": "ChapterEquivalent188",
          "text": "mamma mia. italian law is realy a messy kind of ;) its a sort of endboss. respect!\n\nHow do you handle extraction quality from source documents? standard ocr/layout fails on complex legal\n\nWhat's your validation rate for edge cases? The repealed-content blocking is critical -- we solve it via graph metadata and citation enforcer that validates every claim against source chunks. Sounds like similar philosophy, different implementation",
          "score": 3,
          "created_utc": "2026-02-20 15:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6qkwgd",
          "author": "FeeMassive4003",
          "text": "How do you know if update X is applicable to law Y?",
          "score": 2,
          "created_utc": "2026-02-22 07:46:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rkcd5",
              "author": "Pretend-Promotion-78",
              "text": "Great technical question. The key is that we don't treat laws as mere strings of text; we treat them as relational entities within a Knowledge Graph based on the IFLA LRMoo standard.\nHere is how we handle the 'Update X to Law Y' applicability:\nDynamic Synchronization: Every time the app starts, it performs an incremental alignment of sources to detect new amendments or 'novellas'.\nWork/Expression Mapping: We use the graph to distinguish between the 'Work' (the law itself) and its various 'Expressions' (the specific versions of articles over time).\nDeterministic Validation: Our ingestion pipeline extracts validity metadata and explicit regulatory cross-references. If Update X modifies Law Y, our Deterministic Orchestrator validates this link at the graph level, effectively 'flagging' the outdated version.\nEssentially, we don't let the LLM 'guess' applicability through semantic probability. The graph enforces the structural truth before the query is even processed, ensuring the AI only sees what is legally in force at that exact moment.",
              "score": 3,
              "created_utc": "2026-02-22 13:07:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6gvc99",
          "author": "Top-Seaworthiness285",
          "text": "Try it out here: https://docmind.vasanthubs.co.in/\nIt’s completely free — no login required.",
          "score": 1,
          "created_utc": "2026-02-20 18:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hbfb9",
              "author": "Pretend-Promotion-78",
              "text": "Nice effort, but I think there's a fundamental misunderstanding here. Juris AI isn't just another 'Chat with PDF' tool for generic documents. It’s a specialized Legal Intelligence engine built specifically to tackle the structural mess of Italian Law.\n\nThe main issue with generic PDF-to-chat tools is that they rely entirely on the file's content and the LLM's 'creativity'—which is a massive liability in the legal domain. We built a **Hybrid Graph-RAG architecture (KuzuDB + LanceDB)** that enforces a deterministic gatekeeper logic. If a statute in the document has been repealed or modified, my system cross-references it with the graph's metadata and kills the hallucination before it even reaches the user. A standard PDF chat tool would simply parrot back whatever is in the file, even if it's legally dead.\n\nGreat for casual use, but professional legal-tech requires structured logical constraints and real-time validity checks, not just basic semantic search.",
              "score": 2,
              "created_utc": "2026-02-20 19:52:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9updr",
      "title": "Structure-first RAG with metadata enrichment (stop chunking PDFs into text blocks)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9updr/structurefirst_rag_with_metadata_enrichment_stop/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-20 12:59:52",
      "score": 12,
      "num_comments": 11,
      "upvote_ratio": 0.81,
      "text": "I think most people are still chunking PDFs into flat text and hoping semantic search works. This breaks completely on structured documents like research papers.\n\nTraditional approach extracts PDFs into text strings (tables become garbled, figures disappear), then chunks into 512-token blocks with arbitrary boundaries. Ask \"What methodology did the authors use?\" and you get three disconnected paragraphs from different sections or papers.\n\nThe problem is research papers aren't random text. They're hierarchically organized (Abstract, Introduction, Methodology, Results, Discussion). Each section answers different question types. Destroying this structure makes precise retrieval impossible.\n\nI've been using structure-first extraction where documents get converted to JSON objects (sections, tables, figures) enriched with metadata like section names, content types, and semantic tags. The JSON gets flattened to natural language only for embedding while metadata stays available for filtering.\n\nThe workflow uses Kudra for extraction (OCR → vision-based table extraction → VLM generates summaries and semantic tags). Then LangChain agents with tools that leverage the metadata. When someone asks about datasets, the agent filters by content\\_type=\"table\" and semantic\\_tags=\"datasets\" before running vector search.\n\nThis enables multi-hop reasoning, precise citations (\"Table 2 from Methods section\" instead of \"Chunk 47\"), and intelligent routing based on query intent. For structured documents where hierarchy matters, metadata enrichment during extraction seems like the right primitive.\n\nAnyway thought I should share since most people are still doing naive chunking by default.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r9updr/structurefirst_rag_with_metadata_enrichment_stop/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6hc3mj",
          "author": "Icy_Eye3812",
          "text": "Thanks buddy for sharing",
          "score": 1,
          "created_utc": "2026-02-20 19:55:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i6my9",
          "author": "Irisi11111",
          "text": "Try olmOCR: it's inexpensive and extracts images via pipelines. I output text to Markdown files by page, with each page's images in a separate folder, also titled by page. The precision is excellent.",
          "score": 1,
          "created_utc": "2026-02-20 22:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6wz5mq",
              "author": "framvaren",
              "text": "this looks really good. Only thing I'm missing is that the page should also be broken into elements, e.g. header and the paragraph and linking them together (e.g. header has child-elements in the paragraphs that follow it - that would enable better traceability across pages) so you can chunk by section/header when using in RAG",
              "score": 1,
              "created_utc": "2026-02-23 07:27:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6wzn08",
                  "author": "Irisi11111",
                  "text": "Very good suggestions!",
                  "score": 1,
                  "created_utc": "2026-02-23 07:32:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6jvkww",
          "author": "New_Animator_7710",
          "text": "I find the separation between representation and filtering particularly interesting.\n\nFlattening to natural language *only* for embedding, while preserving structured metadata for constraint-based filtering, avoids entangling structural signals in dense vectors. Dense models are notoriously bad at reliably encoding hierarchy.",
          "score": 1,
          "created_utc": "2026-02-21 04:44:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lhkgc",
          "author": "Academic_Track_2765",
          "text": "no, let them continue, and then you end up with gold like this.   \n[https://www.reddit.com/r/Rag/comments/1r80vsg/comment/o6lcgae/?context=3](https://www.reddit.com/r/Rag/comments/1r80vsg/comment/o6lcgae/?context=3)",
          "score": 1,
          "created_utc": "2026-02-21 13:21:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6wyica",
          "author": "framvaren",
          "text": "Can you share your code?   \nI've been looking into using [unstructured.io](http://unstructured.io) because it partitions a pdf into elements (heading, paragraph, formula, table, etc.) and links them together by adding metadata to each element. But I would like to have an alternative solution that gives me more freedom to determine my own element structure and I don't need all the features of unstructured.",
          "score": 1,
          "created_utc": "2026-02-23 07:21:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra4r0n",
      "title": "Here’s how I got to ~76% on FinanceBench and why I think it could be pushed 80–84%. reachable)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1ra4r0n/heres_how_i_got_to_76_on_financebench_and_why_i/",
      "author": "Aquib8871",
      "created_utc": "2026-02-20 19:20:54",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "I’ve been working on this problem for a while and noticed there aren’t many post explaining how to solve this FinanceBench (the two post about financebench that are on this subreddit are selling products and didnt talk about the pipeline), especially ones that explain the pipeline decisions. So I thought I’d share what worked, what didn’t, and where the remaining gains likely are.\n\nEvaluation & Known Limits\n\nEvaluation was conducted on the 150 public FinanceBench questions.\n\n# Overall outcome\n\n* \\~76% (114 questions) answered correctly\n* 36 failures total\n\n# Failure breakdown\n\n* **24 / 36 failures** were caused by incorrect evidence retrieval\n* **12 / 36 failures** occurred despite correct evidence being retrieved (reasoning / interpretation errors)\n\n# Failure patterns by question type\n\n* **Domain-relevant:** 18 failures\n* **Novel generated:** 14 failures\n* **Metrics-generated:** 4 failures\n\n# Key takeaway\n\n* Most errors stem from missing or noisy retrieval rather than generation quality.\n* When the correct evidence is retrieved, the system answers correctly in most cases, with remaining failures concentrated in interpretive or multi-step financial reasoning.\n* This outperforms the paper’s Shared Vector Store setup (\\~19%) and approaches Long Context performance (\\~79%) while staying within realistic retrieval constraints.\n\n# What didn’t work for me\n\n* Multi-query expansion and HYDE mostly introduced noise.\n* RRF fusion didn’t help because the individual retrievers weren’t strong enough to begin with.\n* Cross-encoder and LLM rerankers didn’t separate relevance well at larger candidate sizes.\n* Retrieving directly on raw page text performed worse than using summaries.\n\n# What did work\n\n* API-based embedding models performed noticeably better than open-source ones in this domain.\n* **Page summaries outperformed raw page text** because they compressed the financial signal (entities, metrics, events) into dense semantic form.\n* Moving from separate retrievers (BM25 + dense) to Qdrant hybrid search helped slightly, likely due to better score fusion and indexing behavior.\n\n# Current pipeline\n\n1. Receive user query\n2. Extract company names and relevant year window\n3. Rewrite the query into a retrieval-friendly form using an LLM\n4. Perform hybrid retrieval over **page-level summaries**\n5. Pass retrieved pages through an **LLM relevance judge** to remove clearly irrelevant evidence\n\nThis setup gives \\~72% exact page retrieval at top\\_k = 10.\n\nWhy I think 80–84% is reachable\n\nThe generator currently uses a simple zero-shot prompt. In about 12 cases, the system retrieved the correct evidence but still failed to produce the answer.  \nI expect stronger prompting strategies (e.g., chain-of-thought reasoning) would resolve many of these cases, but I wasn’t able to test this further due to token limits.\n\nI would love to hear some suggestion how I can make the retrieval even better, if you have any suggestion please do post it.\n\nI’m also applying for RAG / LLM internships right now, would appreciate any perspective on how teams view projects like this. - please do give feedback.\n\nlink - [https://github.com/aquib8112/FinanceBench\\_RAG](https://github.com/aquib8112/FinanceBench_RAG)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1ra4r0n/heres_how_i_got_to_76_on_financebench_and_why_i/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6qencs",
          "author": "StarAI1234",
          "text": "Thanks for sharing your insights! Our platform Selonor overcomes most of the issues ny apply different workflows for ingestion and using a set of connected specialist agents. \n\nJust DM if you are interested in internship. \n\nhttps://www.selonor.com/",
          "score": 2,
          "created_utc": "2026-02-22 06:48:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6rpvdy",
              "author": "Aquib8871",
              "text": "Please check your DM.\n\n",
              "score": 1,
              "created_utc": "2026-02-22 13:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}