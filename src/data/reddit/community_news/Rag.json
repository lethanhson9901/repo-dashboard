{
  "metadata": {
    "last_updated": "2026-01-30 16:57:45",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 138,
    "file_size_bytes": 176538
  },
  "items": [
    {
      "id": "1qocxu9",
      "title": "Ran 30 RAG chunking experiments - found that chunk SIZE matters more than chunking STRATEGY",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "author": "ManufacturerIll6406",
      "created_utc": "2026-01-27 12:49:14",
      "score": 63,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "I kept seeing recommendations that sentence chunking is best for RAG because it \"respects grammatical boundaries.\"\n\nDecided to test it systematically: 4 strategies, 2 datasets, 1,200 retrieval evaluations.\n\nWriteup with methodology and open source code:¬†[Link](https://theprincipledengineer.substack.com/p/its-a-chunking-lie?r=2ivi0a)\n\n**Sentence chunking did dominate initially ‚Äî 96.7% recall vs 80-83% for others.**\n\nThen I noticed something most benchmarks don't report: actual chunk sizes produced.\n\nWhen I configured all strategies with chunk\\_size=1024:\n\n\\- Token: 934 chars (0.91x)\n\n\\- Recursive: 667 chars (0.65x)\n\n\\- Semantic: 1117 chars (1.09x)\n\n\\- Sentence: 3677 chars (3.59x) ‚Üê\n\nSentence chunking was producing chunks 3.6x larger than requested. Larger chunks = more context = better recall. That's a size effect, not a strategy effect.\n\n**When I controlled for actual chunk size (\\~3000 chars across strategies), token chunking matched or beat sentence chunking.**\n\n**Correlation between chunk size and recall: r=0.74 (HotpotQA), r=0.92 (Natural Questions).**\n\nCurious if others have seen similar results or if this breaks down on different datasets.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o20c64u",
          "author": "fixitchris",
          "text": "Depends what you are chunking.",
          "score": 10,
          "created_utc": "2026-01-27 13:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20jhim",
              "author": "ManufacturerIll6406",
              "text": "Exactly and this fw lets you test that on your own strategy and your own documents \n\n[https://github.com/somasays/rag-experiments/tree/main/chunking](https://github.com/somasays/rag-experiments/tree/main/chunking)",
              "score": 1,
              "created_utc": "2026-01-27 13:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o23k80i",
          "author": "durable-racoon",
          "text": "smaller chunks are usually better recall. larger chunks often better for generation. but nothing beats measuring for your specific use case, which you've done.",
          "score": 4,
          "created_utc": "2026-01-27 21:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20wha5",
          "author": "irodov4030",
          "text": "\"Larger chunks = more context = better recall.\"\n\nHow did you measure recall here?",
          "score": 3,
          "created_utc": "2026-01-27 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272ar6",
              "author": "mrFunkyFireWizard",
              "text": "This is also false, your vector points are not accurate if they contain too much context. If it's within the same semantic meaning it's fine but just adding more context is poor design",
              "score": 1,
              "created_utc": "2026-01-28 11:22:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21cwv3",
          "author": "TechnicalGeologist99",
          "text": "There's many steps in retrieval. I find that the main disadvantage of having short chunks is that occasioanly they knock it out the park in terms of relevance and they take up a space in the top K even though they are a useless chunk. \n\nIt may be that some strategies make many such useless chunks. (That are also small). That raises the probability of filling up top K with crap. \n\nYou could repeat this with varying top K to try and measure if that is occurring here",
          "score": 2,
          "created_utc": "2026-01-27 16:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21ewvq",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-27 16:16:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21f5km",
                  "author": "ManufacturerIll6406",
                  "text": "The article shows the results in detail if you are interested\n\n[https://theprincipledengineer.substack.com/p/its-a-chunking-lie](https://theprincipledengineer.substack.com/p/its-a-chunking-lie)",
                  "score": 2,
                  "created_utc": "2026-01-27 16:17:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26q28j",
          "author": "HMM0012",
          "text": "Makes sense; chunk size often drives context retention more than strategy. Controlling for size usually levels performance, so reported strategy wins can be misleading without normalizing chunk length.",
          "score": 2,
          "created_utc": "2026-01-28 09:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zyrl",
          "author": "notAllBits",
          "text": "I would abandon tokenization into chunks. Stream if you can, but cutting any text produces bias from discontinuity. I would follow syntactic and semantic structure when indexing.",
          "score": 1,
          "created_utc": "2026-01-27 15:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o218c9v",
              "author": "Jords13xx",
              "text": "Streaming is definitely an interesting approach, but it can be tricky with context retention. If you maintain some syntactic and semantic boundaries while still chunking, you might strike a balance between continuity and performance. Have you experimented with any hybrid methods?",
              "score": 1,
              "created_utc": "2026-01-27 15:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21cnnp",
                  "author": "ManufacturerIll6406",
                  "text": "Recursive chunking is essentially a hybrid, it tries paragraphs first, falls back to sentences, then words. In my experiments it landed in the middle: better size control than sentence, slightly worse recall than token at equivalent sizes.\n\nThe interesting question is whether there's a \"best of both worlds\" approach: target a specific size but snap to the nearest sentence boundary. You'd get predictable chunk sizes without mid-sentence cuts.\n\nDidn't test that explicitly, but the framework is extensible & would be a straightforward strategy to add - [https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)\n\nMight be worth exploring in a follow-up.",
                  "score": 1,
                  "created_utc": "2026-01-27 16:06:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o223w9s",
          "author": "jrochkind",
          "text": "If you have text that has em, i'd try paragraph chunking (up to certain max size paragraph anyway)",
          "score": 1,
          "created_utc": "2026-01-27 18:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22ip3o",
          "author": "blue-or-brown-keys",
          "text": "Curious if this may be trying to justify method based on outcome. Whats the intution here.",
          "score": 1,
          "created_utc": "2026-01-27 19:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o230ewt",
              "author": "ManufacturerIll6406",
              "text": "Intuition: More text = more semantic information encoded = better chance of matching the query. Also, answers rarely live in a single sentence, larger chunks capture the full context.\n\nThe \"justifying outcome\" concern would apply if I cherry-picked one result. But the correlation held across 30 configs, two datasets, and all four strategies landed on the same trendline (r=0.74 and r=0.92).\n\nCode's open if you want to test on a different dataset.",
              "score": 1,
              "created_utc": "2026-01-27 20:26:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29pm8l",
          "author": "lyonsclay",
          "text": "Wouldn't the optimal chunk size be contingent on your vector size assuming you are using vector similarity to select the chunks? If your chunk is smaller than your vector size then your system is being wasteful, if your chunk is larger than your vector size you are losing information.",
          "score": 1,
          "created_utc": "2026-01-28 19:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aikqp",
              "author": "ManufacturerIll6406",
              "text": "There probably is a sweet spot! In my experiments, recall kept improving up to \\~3000 chars with text-embedding-3-small (1536 dims). Didn't test beyond that. Could be interesting to check.\n\nhttps://preview.redd.it/arrmfswmq5gg1.png?width=1500&format=png&auto=webp&s=98470fd0c77c983075e9e1807d121e82f47fd5bd",
              "score": 1,
              "created_utc": "2026-01-28 21:23:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qlftqz",
      "title": "Which Vector DB should I use for production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "author": "Cheriya_Manushyan",
      "created_utc": "2026-01-24 06:42:26",
      "score": 41,
      "num_comments": 84,
      "upvote_ratio": 0.96,
      "text": "I see many enterprises using Pinecone, Weaviate, Milvus, Qdrant etc. Based on your experience, which one is best  for production and why? Help a friend out...üôÇ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1ep896",
          "author": "ampancha",
          "text": "If you're already on Postgres, pgvector is underrated. One less system to secure and operate, and recent benchmarks show it's competitive with the dedicated options at moderate scale.\n\nIf you want a purpose-built vector DB, Qdrant. Best latency performance in most independent tests, and the open-source version is production-ready.\n\nEither works. What usually breaks is the stuff around the DB: missing per-user query limits, no spend caps on embedding calls, no alerting when retrieval patterns drift. Sent you a DM",
          "score": 18,
          "created_utc": "2026-01-24 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o08hm",
              "author": "Appropriate_Ant_4629",
              "text": "Came here to describe a pretty extensive test from my workplace that benchmarked many (qdrant, milvus, chroma, pgvector, databricks's vector search feature, building our own with faiss, building our own with hnswlib, etc) ...    \n... but our findings exactly matched yours.  \n\nWhen we reach a billion vectors we'll re-visit Qdrant - because it is impressive technologically and was easy to shard across compute nodes; but at our modest tens-of-millions, pgvector shined.",
              "score": 3,
              "created_utc": "2026-01-25 18:41:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1et06u",
              "author": "Cheriya_Manushyan",
              "text": "Informative, thanks.",
              "score": 2,
              "created_utc": "2026-01-24 11:41:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1i65yu",
                  "author": "LightShadow",
                  "text": "We're using Qdrant for performance and continuing feature development. We didn't want the vector operations mixed with our normal postgres load in case they slowed each other down.",
                  "score": 2,
                  "created_utc": "2026-01-24 21:51:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nr3j6",
              "author": "licjon",
              "text": "Both also work. You can do a 2-stage search starting with Qdrant and then narrowing even more with pgvector. Works well with cross-document search and large corpora.",
              "score": 2,
              "created_utc": "2026-01-25 18:04:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e2jwz",
          "author": "hrishikamath",
          "text": "Postgres w pgvector lol, I serve like 100k documents and it takes like milli seconds for hybrid search. Edit: repo: https://github.com/kamathhrishi/stratalens-ai and blogpost: https://substack.com/home/post/p-181608263",
          "score": 34,
          "created_utc": "2026-01-24 07:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eemh2",
              "author": "ZiKyooc",
              "text": "That is a lot of seconds",
              "score": 12,
              "created_utc": "2026-01-24 09:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fifrm",
                  "author": "hrishikamath",
                  "text": "Sorry I meant milli seconds loll (editing my comment)",
                  "score": 4,
                  "created_utc": "2026-01-24 14:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1e4j11",
              "author": "Ok-Adhesiveness-4141",
              "text": "Right choice.",
              "score": 3,
              "created_utc": "2026-01-24 07:58:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1e5hci",
              "author": "debauch3ry",
              "text": "~~Are you saying pgvector is bad?~~ (confusion resolved after edit!) I found the tech very reliable with 500k vectors indexed with HNSW giving very fast knn searches. As a general DB it's also fairly high tier.",
              "score": 5,
              "created_utc": "2026-01-24 08:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fijjb",
                  "author": "hrishikamath",
                  "text": "I meant milli seconds I edited my comment",
                  "score": 3,
                  "created_utc": "2026-01-24 14:32:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1foxph",
                  "author": "hrishikamath",
                  "text": "Repo: https://github.com/kamathhrishi/stratalens-ai and blogpost how I did it: https://substack.com/home/post/p-181608263",
                  "score": 2,
                  "created_utc": "2026-01-24 15:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ertbv",
              "author": "psanilp",
              "text": "How do you handle chunking? We use the Azure AI search pipeline and thinking of going local RAG. Do you have a ready product or licensable deployment?",
              "score": 2,
              "created_utc": "2026-01-24 11:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1hgho7",
                  "author": "hrishikamath",
                  "text": "Chunking: [https://substack.com/home/post/p-181608263](https://substack.com/home/post/p-181608263) Yes, a kind of ready product is online. also open source: [https://github.com/kamathhrishi/stratalens-ai](https://github.com/kamathhrishi/stratalens-ai)",
                  "score": 2,
                  "created_utc": "2026-01-24 19:50:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f35x9",
              "author": "Straight-Gazelle-597",
              "text": "solid choice to cover at least 90% of the biz needs;-)",
              "score": 2,
              "created_utc": "2026-01-24 12:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fw61c",
              "author": "virgilash",
              "text": "Yeah, op, give PostgreSQL with pgvector a try üòâ",
              "score": 2,
              "created_utc": "2026-01-24 15:42:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1gigea",
                  "author": "Cheriya_Manushyan",
                  "text": "Yeah, definitely!",
                  "score": 1,
                  "created_utc": "2026-01-24 17:22:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1gil92",
              "author": "Cheriya_Manushyan",
              "text": "Thanks a lot for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 17:23:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e29ow",
          "author": "instantlybanned",
          "text": "I use milvus in production and it works extremely well for me.¬†\n\n\nPostgreSQL with pgvector is not an option for me because of the recall that I need and the speed with which I need the results at the scale that I'm working at. With milvus, I have control over the in memory index that's being used as well as the parameters for the index and the query parameters for the approximate search, allowing me to tune it to have high recall at still very fast speeds.¬†\n\n\nEdit: just for context, I'm working with around 200 million vectors.¬†",
          "score": 8,
          "created_utc": "2026-01-24 07:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6gs5",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing",
              "score": 1,
              "created_utc": "2026-01-24 08:15:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ilmze",
              "author": "MeringueInformal7670",
              "text": "I am using Milvus in production as a consulting gig i did for a startup pretty solid perf so far during internal runs currently running on a single node standalone setup do you mind sharing how does your Milvus infra look like for 200 mil vectors. You can DM me as well thanks.",
              "score": 1,
              "created_utc": "2026-01-24 23:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lrktx",
                  "author": "ChoiceEmpty8485",
                  "text": "Sure! For my Milvus setup with 200 million vectors, I use a distributed architecture with multiple nodes to handle the load. We have optimized the index parameters for both recall and speed, and utilize GPU acceleration for faster queries. Happy to share more specific details if you need!",
                  "score": 1,
                  "created_utc": "2026-01-25 11:57:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e7ex8",
          "author": "pberck",
          "text": "I used lancedb in Rust in my last project. It worked well, but it wasn't a huge amount of data.",
          "score": 5,
          "created_utc": "2026-01-24 08:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1etlhw",
              "author": "Cheriya_Manushyan",
              "text": "Is it open source or like Pinecone?",
              "score": 1,
              "created_utc": "2026-01-24 11:46:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1etvny",
                  "author": "pberck",
                  "text": "It is dual license, open source with apache 2.0 and a cloud version which has a commercial licence. I used the OS version.",
                  "score": 3,
                  "created_utc": "2026-01-24 11:48:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fmb3m",
          "author": "ComputationalPoet",
          "text": "ill add opensearch,  several variations of semantic search (approximate knn, exact cosine similarity), hybrid search and obviously bm25.   Scales well and performs great, though ill admit i dont know some of these other options that well.",
          "score": 3,
          "created_utc": "2026-01-24 14:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dyeel",
          "author": "IdeaAffectionate945",
          "text": "Roll your own, way faster and more flexible. Preferably something that's a \"plugin\" to SQL, allowing you to parametrise your vector retrieval saying stuff such as \"select \\* from rag where x, and distance(...)\"\n\nIt's a 100 times more flexible than whatever Pinecone even \\*can\\* give you in theory.\n\nI'm using SQLite and sqliteai-vector ...",
          "score": 10,
          "created_utc": "2026-01-24 07:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dz14n",
              "author": "Cheriya_Manushyan",
              "text": "I‚Äôve personally been using PostgreSQL with pgvector, but I notice many enterprises prefer databases like Pinecone. I‚Äôm trying to understand the real reasons behind this choice.",
              "score": 10,
              "created_utc": "2026-01-24 07:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1e2dnr",
                  "author": "IdeaAffectionate945",
                  "text": "*\"I notice many enterprises prefer databases like Pinecone\"*\n\nMarketing bs. The best filtering you can do is on meta fields. With integrated into the core DB, you've got a bajillion times the speed, and a bajillion times the flexibility on querying the thing.",
                  "score": 5,
                  "created_utc": "2026-01-24 07:39:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1e188g",
                  "author": "Chucki_e",
                  "text": "I also use pgvector and I don't think you need to use any third-party vector database unless you have some special requirements that force you to. As with so many other architectural/technology choices, it's easier to start simple and then scale up when you actually need to - of course with a plan in mind, but I don't imagine migrating your vector database isn't that big of an issue?",
                  "score": 6,
                  "created_utc": "2026-01-24 07:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q151b",
              "author": "Appropriate_Ant_4629",
              "text": "For roll-you-own, do you prefer faiss, hnswlib, or something else?\n\nIn our benchmarking hnswlib vastly outperformed faiss; though this may have been user error.\n\nBut in the end, for a few million vectors it didn't really matter; and pgvector was more convenient; and for a billion vector test, sharding across nodes became the hard part and qdrant seemed to handle it best.",
              "score": 2,
              "created_utc": "2026-01-26 00:05:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rtauk",
                  "author": "IdeaAffectionate945",
                  "text": "*\"do you prefer faiss, hnswlib\"*\n\nsqliteai-vector isn't using any indexing as far as I know, only HW optimisation and *\"quantisation\"*, which makes a 1,580 dimensional vector become like 250 if you wish, almost without loosing quality ...\n\nCheck it out on GitHub if you wish. Technically, it's years ahead of *\"everything else\"* in the space ...\n\nHowever, for me it's just a lib, and I don't care about its implementations. I know it's capable of serving my sjit faster than anything else I've tried for SQLite, and I know it's doesn't leak memory like everything else I've tried for SQLite. And I know it returns results in sub seconds even through a DB with 100,000 items, and it gives me *\"memory guarantees\"* which I love like crazy.\n\nIt might not be as scalable as some of the other solutions (for PostgreSQL, MySQL, etc), but then again I'm not building Twitter or Instagram either ...",
                  "score": 2,
                  "created_utc": "2026-01-26 06:07:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ec1id",
          "author": "Suspicious-Bite6107",
          "text": "People that say pgvector is slow are usually DEVs that want \"to simplify their life\" and dont look or make any benchmark, also because they don't read they still think you have to use postgres  with pgvector only, and forget about pgvectorscale or diskann indexes....thank you for still providing work, I think that even with AI there is enough stupidity in this world to allow to have work for the next 50 years :) \n\nPS - your app isn't special, you are not going to have to handle 100k concurent transactions unless as usuall you keep not using pooling :p",
          "score": 5,
          "created_utc": "2026-01-24 09:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eu7ei",
              "author": "Cheriya_Manushyan",
              "text": "Well, you have a fair point.",
              "score": 1,
              "created_utc": "2026-01-24 11:51:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1errhw",
          "author": "Useful-Disk3725",
          "text": "I had qdrant for some time, really fast. Then switched to MariaDB 11.8. Vector search is really fast, but insert is slow. I think qdrant had a buffering system, and building indexes from time to time (standing 100% cpu spikes regularly). I mad a similar thing, getting vectors to a buffer table without index and in a separate cronned process moving in batches.\n\nThe key is, never mix vector index search with regular searches, that‚Äôs a known limitation in almost all databases. Though overcome is easy (technologies advertised as hybrid search are only hack solutions)",
          "score": 3,
          "created_utc": "2026-01-24 11:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1et5tn",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1la8xl",
          "author": "HealthOk5149",
          "text": "Postgres + pgvector with HNSW indexing or + pgvectorscale with DiskANN indexing when RAM becomes an issue at scale because HNSW needs a lot of RAM",
          "score": 3,
          "created_utc": "2026-01-25 09:26:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e3jfr",
          "author": "Academic_Track_2765",
          "text": "Many options, if you guys use azure use azure search, I have deployed solutions with chromadb, neo4j, weaviate. See what costs the least for your dimensions X documents and use that. I think it mostly comes down to what your company / IT dept is ok with and the costs. You can even build your own if you like.",
          "score": 2,
          "created_utc": "2026-01-24 07:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e8uj9",
              "author": "Cheriya_Manushyan",
              "text": "From your experience, which option do you prefer if the goal is good performance at a low cost?",
              "score": 1,
              "created_utc": "2026-01-24 08:37:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3v7x",
          "author": "Effective-Ad2060",
          "text": "We use qdrant (supports hybrid search out of the box)  \nFor reference:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)",
          "score": 2,
          "created_utc": "2026-01-24 07:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1egvk3",
              "author": "KYDLE2089",
              "text": "+1 we do too work really good",
              "score": 2,
              "created_utc": "2026-01-24 09:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1emf8s",
              "author": "Cheriya_Manushyan",
              "text": "Noted.",
              "score": 0,
              "created_utc": "2026-01-24 10:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e4vaf",
          "author": "nborwankar",
          "text": "Depends on corpus size, whether there is need for joining with relational data and whether there is need to scale up massively",
          "score": 2,
          "created_utc": "2026-01-24 08:01:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eseds",
              "author": "Cheriya_Manushyan",
              "text": "That's an interesting case you shared.",
              "score": 0,
              "created_utc": "2026-01-24 11:36:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gwlhy",
                  "author": "nborwankar",
                  "text": "My default suggestion for workgroup or departmental size enterprise applications is just use Postgres w pgvector which gives you a hybrid relational vector db with ability to join across relational and vector data using just SQL syntax.\n\nIt is not as performant as the pure vector databases but it is very familiar and it is the easiest way to get started while you figure out what you want to do. This is very important for onboarding DB developers.\n\nNot to blow my own horn but I have a book called ‚ÄúVector Databases‚Äù from OReilly coming out in a few months - available on Amazon.\n\nIf you want to read parts of it for free - sign up for the OReilly Platform - it‚Äôs free for 7 days.\n\nThe first chapter deals with these issues. You can also DM me if you have specific questions.",
                  "score": 1,
                  "created_utc": "2026-01-24 18:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1eit8q",
          "author": "RolandRu",
          "text": "In my opinion there is no single ‚Äúbest‚Äù vector DB for production.\n\nI‚Äôm building a code-focused RAG. For now FAISS is enough for me, but I also added BM25 search, hybrid search and a dependency graph between code chunks.\n\nAfter some time I realized that new requirements will only make my custom code more complicated. In practice it feels like I‚Äôm rebuilding features that Weaviate already has (BM25 + hybrid + graph/relations).\n\nQdrant can be faster, but for me the difference like 25ms vs 35ms doesn‚Äôt really matter. Native support for everything I need matters more, so the next step will be migrating to Weaviate and testing it in real use.\n\nAt the same time I will keep FAISS as a nice option for people who want to run the project quickly without setting up a container and configuring Weaviate.\n\nSo Weaviate ‚Äî **if someone thinks this is a mistake, please let me know** üôÇ",
          "score": 2,
          "created_utc": "2026-01-24 10:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esoxa",
              "author": "Cheriya_Manushyan",
              "text": "Noted",
              "score": 1,
              "created_utc": "2026-01-24 11:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1erp4p",
          "author": "psanilp",
          "text": "PostGres with some plugin seems to be the preferred route. Having said that, does anyone here have a 'rag in a box' i can install and deploy at a legal firm?",
          "score": 2,
          "created_utc": "2026-01-24 11:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exwig",
              "author": "seomonstar",
              "text": "anything for legal firms needs heavy skills and lots of custom work. Thats why there is so much money in it. from parsing to chunking to retrieval to context and session management.  There are some open source rag things on github, but I wouldnt be deploying them in a law firm ‚Ä¶",
              "score": 3,
              "created_utc": "2026-01-24 12:21:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1etdu0",
              "author": "Cheriya_Manushyan",
              "text": "By 'rag in a box', you mean low code/no code solution?",
              "score": 1,
              "created_utc": "2026-01-24 11:44:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g34yq",
                  "author": "psanilp",
                  "text": "I understand RAG pipeline and we currently use Azure AI Search with inbuilt vectorisation/hybrid/semantic search . a) There are some issues related to chunking longer docs. b) Some firms who are willing to pay, would like a setup where the RAG is done locally so everything stays within the firewall. Hence no external api calls. So my query was if someone has an end to end solution we can buy/license and deploy on a hardware that is physically placed in the client's premises.",
                  "score": 2,
                  "created_utc": "2026-01-24 16:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1f3g4n",
          "author": "a_developer_2025",
          "text": "Doesn‚Äôt pgvector apply metadata filtering only after searching by vectors? If that‚Äôs still true, it is a big issue if you have small datasets per metadata.",
          "score": 2,
          "created_utc": "2026-01-24 13:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fphel",
          "author": "bzImage",
          "text": "Qdrant.",
          "score": 2,
          "created_utc": "2026-01-24 15:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ftsfa",
          "author": "seomonstar",
          "text": "Postgre scaled to 800 million users with openai, still at their core, so anyone having problems with pgvector should bear in mind its likely a skill issue unless they have 800 million users or more . https://openai.com/index/scaling-postgresql/",
          "score": 2,
          "created_utc": "2026-01-24 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzv2h",
          "author": "Leather-Departure-38",
          "text": "Align with your current cloud infra and db",
          "score": 2,
          "created_utc": "2026-01-24 18:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixl6d",
          "author": "purposefulCA",
          "text": "Our org has mssql. We just pushed all our vectors there. Working fine so far",
          "score": 2,
          "created_utc": "2026-01-25 00:10:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l8ufi",
          "author": "my_byte",
          "text": "Lots of considerations. Personally, my go to if working with a python or nodejs codebase is mongodb. Postgres is also a good choice. Big fan of not adding multiple db's & search engines to your stack. If you end up scaling a ton, migrating to a dedicated search engine isn't terribly hard.",
          "score": 2,
          "created_utc": "2026-01-25 09:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1llcpd",
          "author": "TemporaryMaybe2163",
          "text": "Genuine question: I wonder why nobody commented about Oracle 26ai. \nIs it bad?",
          "score": 2,
          "created_utc": "2026-01-25 11:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mp3in",
          "author": "martinschaer",
          "text": "SurrealDB. It scales without costing a fortune, runs in single node or distributed, and it is multi model in case you want to do hybrid search with BM25 or graph",
          "score": 2,
          "created_utc": "2026-01-25 15:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shv08",
          "author": "jerrysyw",
          "text": "I had used Milvus for embedding store and chunk text with PG wiht  my product",
          "score": 2,
          "created_utc": "2026-01-26 09:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e6g4b",
          "author": "debauch3ry",
          "text": "I bet those enterprises using Pinecone also write their backend software in python or node.js.\n\nI use PaaS cloud postgres DB + pgvector, as I find it a good mix of having control vs delegating the infrastructure to a cloud provider.\n\nQdrant you can run locally via docker which is testament to their confidence in their tech. They're my next-to-try.",
          "score": 3,
          "created_utc": "2026-01-24 08:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eskjy",
              "author": "Cheriya_Manushyan",
              "text": "For most of them, a python based stack is used.",
              "score": 2,
              "created_utc": "2026-01-24 11:37:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dw4nh",
          "author": "PiaRedDragon",
          "text": "Qdrant or neo4j.",
          "score": 2,
          "created_utc": "2026-01-24 06:44:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dwhad",
              "author": "Cheriya_Manushyan",
              "text": "Can you give reasons, focusing on performance and cost compared to other databases?",
              "score": 2,
              "created_utc": "2026-01-24 06:47:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1enimk",
          "author": "crishoj",
          "text": "Are you certain you even need vectors? Have you tried regular keyword search? Have the agent come up with relevant search terms",
          "score": 2,
          "created_utc": "2026-01-24 10:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esw2l",
              "author": "Cheriya_Manushyan",
              "text": "I haven't tried keyword only search, will checkout.",
              "score": 1,
              "created_utc": "2026-01-24 11:40:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1etdxu",
          "author": "Professional_Cup6629",
          "text": "might be a good read: https://agentset.ai/blog/best-vector-db-for-rag",
          "score": 1,
          "created_utc": "2026-01-24 11:44:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ey7jg",
              "author": "Cheriya_Manushyan",
              "text": "Will check.",
              "score": 1,
              "created_utc": "2026-01-24 12:23:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f35tp",
          "author": "Live-Guitar-8661",
          "text": "Postgres",
          "score": 1,
          "created_utc": "2026-01-24 12:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jd3lw",
          "author": "Grocery_Odd",
          "text": "Went through making this decision for a separate project, developed and applied this framework to get an eval-driven solution: [https://github.com/conclude-ai/rag-select](https://github.com/conclude-ai/rag-select)",
          "score": 1,
          "created_utc": "2026-01-25 01:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vt3gt",
          "author": "Green_Crab_9726",
          "text": "FalkorDB",
          "score": 1,
          "created_utc": "2026-01-26 20:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o286yq6",
          "author": "CutPuzzleheaded5884",
          "text": "Turopuffer",
          "score": 1,
          "created_utc": "2026-01-28 15:18:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpce5s",
      "title": "You can now train embedding models 1.8-3.3x faster!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:11:25",
      "score": 29,
      "num_comments": 11,
      "upvote_ratio": 0.94,
      "text": "Hey RAG folks! We collaborated with Hugging Face to enable 1.8-3.3x faster embedding model training with 20% less VRAM, 2x longer context & no accuracy loss vs. FA2 setups.\n\nFull finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default! You can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp etc.\n\nFine-tuning embedding models can improve retrieval & RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.\n\nWe provided many free notebooks with 3 main use-cases to utilize. \n\n* Try the [EmbeddingGemma notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M).ipynb) in a free Colab T4 instance\n* We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!\n\n‚≠ê Guide + notebooks: [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nGitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n\nThanks so much guys! :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o287p7m",
          "author": "z0han4eg",
          "text": "Thanks mate, this a rly big news",
          "score": 3,
          "created_utc": "2026-01-28 15:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27wj9w",
          "author": "Popular_Sand2773",
          "text": "This is very cool. Quick question if we are using these encoders for the base of something else is this still valuable or is it only really for classic fine tuning? If I understand correctly the main speedup came from a new fused kernel correct?",
          "score": 1,
          "created_utc": "2026-01-28 14:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284gga",
              "author": "yoracale",
              "text": "Apologies could you elaborate your first question?\n\nOur main optimizations includes gradient checkpointing, kernels yes and more. You can see gradient checkpointing here: https://unsloth.ai/docs/new/500k-context-length-fine-tuning#unsloth-gradient-checkpointing-enhancements",
              "score": 1,
              "created_utc": "2026-01-28 15:06:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28gjpf",
          "author": "TechySpecky",
          "text": "Why do people fine tune models and when do these lead to superior performance than large well-known embedding models like gemini / qwen ones? For example if I am doing RAG for archeology would it make sense to have a custom embedding model?",
          "score": 1,
          "created_utc": "2026-01-28 16:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o294cnv",
              "author": "Financial-Bank2756",
              "text": "yes, a custom embedding model could help if you have enough domain text and evaluation pairs. Otherwise, a strong general model plus better chunking, metadata filters, hybrid search, and rerankers often beats premature fine-tuning.",
              "score": 3,
              "created_utc": "2026-01-28 17:44:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o294nzx",
                  "author": "TechySpecky",
                  "text": "Yea makes sense, what do you mean by evaluation pairs?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:45:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dh7lf",
          "author": "Aggressive-Solid6730",
          "text": "With how cheap compute is and how small embedding models are (compared to LLMs) I wouldn't think that time and memory are at that much of a premium. I am curious to hear any push-back on this, but I am also curious if in your experiments you saw any additional benefits of using these fine-tuning variants such as LoRA. Did they behave as regularizers making training more stable or were the gains purely speed and memory? The other thing you mention is context length which is fair, but as Google published, the amount of information we are trying to fit into a single vector is already quite limiting.",
          "score": 1,
          "created_utc": "2026-01-29 07:48:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eqvq6",
          "author": "Interesting-Town-433",
          "text": "Amazing! Will incorporate into embedding-adapters asap\n\nUniversal Embedding Translation Library\nOutput an embedding from any model into any other model's vector space\n\ngo\nminilm <-> openai\ngoogle <-> openai\ne5 <-> openai\nwith confidence scoring to tell you when it will work\n https://github.com/PotentiallyARobot/EmbeddingAdapters",
          "score": 1,
          "created_utc": "2026-01-29 13:45:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmlxfw",
      "title": "Looking for testers: 100% local RAG system with one-command setup",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "author": "primoco",
      "created_utc": "2026-01-25 15:23:30",
      "score": 29,
      "num_comments": 30,
      "upvote_ratio": 0.92,
      "text": "Hey everyone! üëã\n\nI've been working on an open-source RAG system and I'm looking for people willing to test it and give honest feedback.\n\n\\*\\*What it does:\\*\\*\n\nA document processing and Q&A system that runs 100% locally on your machine. No API keys needed, no data leaving your computer.\n\n\\*\\*Why I built it:\\*\\*\n\nI was frustrated with RAG solutions that either required cloud services, complex Docker configurations, or hours of setup. I wanted something that \"just works\" out of the box for businesses that need complete data privacy.\n\n\\*\\*Tech stack:\\*\\*\n\n\\- FastAPI backend\n\n\\- React + Vite frontend\n\n\\- Qdrant for vector storage\n\n\\- Ollama for local LLMs (Qwen2.5 or Mistral 7B)\n\n\\- BAAI/bge-m3 embeddings\n\n\\*\\*Key features:\\*\\*\n\n\\- Single command to start everything (\\`./setup.sh standard\\`)\n\n\\- Completely offline after initial setup\n\n\\- Supports 29 languages\n\n\\- Multi-user with JWT auth and role-based access\n\n\\- OCR support (Apache Tika + Tesseract)\n\n\\- Handles PDF, DOCX, PPTX, XLSX, TXT, MD and more\n\n\\- Tested with 10,000+ documents\n\n\\*\\*What I'm looking for:\\*\\*\n\n\\- Feedback on installation experience (Ubuntu 20.04+)\n\n\\- Real-world testing with your own documents\n\n\\- Bug reports and edge cases\n\n\\- Suggestions for improvements\n\n\\*\\*Repo:\\*\\* [https://github.com/I3K-IT/RAG-Enterprise](https://github.com/I3K-IT/RAG-Enterprise)\n\nI've stress-tested it with large documents (including the Mueller Report) but I'd love to see how it handles different use cases and languages.\n\nHappy to answer any questions!\n\nEDIT: Benchmark script and real-world results now published! See Community Benchmarks in the README.\n\n\\---\n\n\\*Fully open-source under AGPL-3.0. No paid tiers, no telemetry, no external calls.\\*  \n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1o03id",
          "author": "ampancha",
          "text": "Nice work on the local-first setup. One thing worth stress-testing before enterprise users hit it: retrieval-augmented systems are vulnerable to prompt injection via document content, and multi-user setups without per-user rate limits or query attribution can get abused fast. Both failure modes are invisible until production. Sent you a DM with more detail.",
          "score": 3,
          "created_utc": "2026-01-25 18:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o66iq",
              "author": "primoco",
              "text": "Really appreciate this feedback ‚Äî you‚Äôre raising exactly the kind of security considerations that matter for production deployments.\nYou‚Äôre right that both prompt injection via document content and multi-user abuse patterns are often invisible until they hit you in production. These are definitely on my radar for hardening the system.\nChecked your DM ‚Äî thanks for the detailed insights. I‚Äôll follow up there.\nFor anyone else reading: this kind of security-focused feedback is gold. If you spot potential vulnerabilities or have suggestions, Issues and DMs are always welcome.",
              "score": 1,
              "created_utc": "2026-01-25 19:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nbwin",
          "author": "Chrys",
          "text": "I could give it a try but I have a Mac mini. What do you think? It will be too slow?",
          "score": 1,
          "created_utc": "2026-01-25 17:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nu1eo",
              "author": "primoco",
              "text": "Hi Chris, it depends on which Mac Mini you have.\nApple Silicon (M1/M2/M4) with 16GB+ RAM: Should work well! Ollama runs natively on Apple Silicon and uses the integrated GPU. Performance won‚Äôt match a dedicated NVIDIA card, but it‚Äôs definitely usable.\nIntel Mac Mini: Will be slower since it runs on CPU only.\nThe catch: The automated setup.sh script is designed for Ubuntu + NVIDIA. On Mac, you‚Äôd need to set things up manually:\n\t‚àô\tInstall Docker Desktop\n\t‚àô\tInstall Ollama for Mac\n\t‚àô\tRun Qdrant via Docker\n\t‚àô\tStart the backend/frontend manually\nIf you share your Mac Mini specs (chip + RAM), I can give you a better estimate and maybe help with Mac-specific instructions. It would actually be great to have Mac compatibility documented!",
              "score": 1,
              "created_utc": "2026-01-25 18:16:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ogig8",
                  "author": "Chrys",
                  "text": "Mac mini M4 10-core CPU 10-core GPU 16GB/256GB. \nWhich local LLM do you suggest?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:52:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ou98e",
          "author": "fredastere",
          "text": "Def check it out ty",
          "score": 1,
          "created_utc": "2026-01-25 20:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwajj",
          "author": "Character_Pie_5368",
          "text": "Is there a mcp server interface? I have some pentesting frameworks that I‚Äôd like to try and interface with thisz",
          "score": 1,
          "created_utc": "2026-01-26 02:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rs2va",
              "author": "primoco",
              "text": "Not yet ‚Äî MCP server interface isn‚Äôt implemented at the moment.\nCurrently the system exposes a REST API (FastAPI backend on port 8000) that you could target for pentesting. The main endpoints are:\n\t‚àô\t/api/v1/query ‚Äî RAG queries\n\t‚àô\t/api/v1/documents ‚Äî document upload/management\n\t‚àô\t/api/v1/auth ‚Äî JWT authentication\nThat said, MCP support is an interesting idea for the roadmap ‚Äî would make integration with Claude and other tools much smoother.\nIf you run your pentesting frameworks against it and find vulnerabilities, I‚Äôd genuinely appreciate the feedback! There‚Äôs a SECURITY.md for responsible disclosure.\nWhat frameworks are you planning to use? Curious what you‚Äôre testing for.",
              "score": 1,
              "created_utc": "2026-01-26 05:58:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ratzb",
          "author": "floating___around",
          "text": "I am interested. Would a 3090 run that program?",
          "score": 1,
          "created_utc": "2026-01-26 04:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ru7re",
              "author": "primoco",
              "text": "Absolutely! A 3090 with 24GB VRAM is perfect ‚Äî even more than my test setup (RTX 5070 Ti, 16GB).\n\nThe setup script has three profiles:\n Profile   |GPU VRAM|LLM Model  |RAM  |\n |----------|--------|-----------|-----|\n |`minimal` |8-12GB  |Mistral 7B |16GB |\n |`standard`|12-16GB |Qwen2.5 14B|32GB |\n |`advanced`|16-24GB |Qwen2.5 32B|128GB|\n \nWith your 3090 (24GB), you could run:\n \n ```\n ./setup.sh advanced\n ```\n \nThis installs `qwen2.5:32b-instruct-q4_K_M` ‚Äî the most capable model available.\n \nNote: The advanced profile expects 128GB system RAM. If you have less, you can still run `./setup.sh advanced` but consider switching to the 14B model in `docker-compose.yml` if you experience issues.\n \nAlternatively, `./setup.sh standard` with the 14B model is a safe choice that will run great on your hardware.\nLet me know how it goes!\n\n-----\n\nCos√¨ diamo info complete e accurate. Vuoi modificare qualcosa?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2026-01-26 06:14:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rj89p",
          "author": "ggone20",
          "text": "\nSearch performance metrics? ‚ÄòTested on 10,000+ documents‚Äô with just a Qdrant db and an old small models? Idk‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-26 04:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rv6ux",
              "author": "primoco",
              "text": "Qdrant handles vector search efficiently ‚Äî at 10K docs the bottleneck is typically LLM inference, not retrieval. Query times stay in the 2-4 second range in my testing.\nI don‚Äôt have formal benchmark charts published yet, but it‚Äôs something I‚Äôd like to add to the documentation. If you run stress tests on your setup, I‚Äôd be happy to include real-world metrics from the community!",
              "score": 1,
              "created_utc": "2026-01-26 06:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rwkiv",
                  "author": "ggone20",
                  "text": "I really mean retrieval performance for non-trivial queries.",
                  "score": 1,
                  "created_utc": "2026-01-26 06:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rrwi1",
          "author": "AwayLuck7875",
          "text": "Rag ollama vulkan??",
          "score": 1,
          "created_utc": "2026-01-26 05:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rvm6p",
              "author": "primoco",
              "text": "Are you asking about Vulkan support for AMD/Intel GPUs?\nRAG Enterprise is currently tested with NVIDIA GPUs (CUDA). Ollama does have experimental Vulkan support for AMD cards, but I haven‚Äôt tested that combination yet.\nIn theory it should work ‚Äî the backend just calls Ollama‚Äôs API, it doesn‚Äôt care what‚Äôs running underneath. But I can‚Äôt guarantee performance or stability on Vulkan.\nIf you have an AMD GPU and want to try it:\n\t1.\tInstall Ollama with Vulkan support manually\n\t2.\tSkip the automated setup and configure services individually\n\t3.\tPoint the backend to your Ollama instance\nLet me know your GPU ‚Äî happy to help figure out if it‚Äôs worth trying!",
              "score": 1,
              "created_utc": "2026-01-26 06:25:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rxwbk",
          "author": "AwayLuck7875",
          "text": "Maybe byt work",
          "score": 1,
          "created_utc": "2026-01-26 06:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s4h4j",
              "author": "primoco",
              "text": "Sorry, didn't quite catch that ‚Äî could you clarify? Happy to help if you have questions!",
              "score": 1,
              "created_utc": "2026-01-26 07:37:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rzws6",
          "author": "arxdit",
          "text": "Definitely something that many of us are working on.\n\nI have built my own, and it supports both ollama and openai api key.\n\non ollama I rely mostly on qwen3-coder:30b which is nothing short of AMAZING.\n\nI made my own curated indexing & retrieval algos focusing on handling ever expanding knowledge - including an endless conversation manager that does not compact - it's here \\[[https://github.com/andreirx/FRAKTAG](https://github.com/andreirx/FRAKTAG)\\] if you want to check it out\n\nOh and having a CLI makes it usable by claude code",
          "score": 1,
          "created_utc": "2026-01-26 06:59:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s5csz",
              "author": "primoco",
              "text": "Nice! Just checked out FRAKTAG ‚Äî interesting approach with the non-compacting conversation manager. Different philosophy from what I'm doing but I can see the value for ever-expanding knowledge bases.\n\nThe documentation is really thorough ‚Äî that's not easy to maintain, kudos for that.\n\nThe CLI + Claude Code integration is a smart move ‚Äî that's actually something I should consider adding to RAG Enterprise.\n\nqwen3-coder:30b is impressive, though it needs serious VRAM. I've kept the default models smaller (7B-14B) to lower the entry barrier, but power users can definitely swap in larger models.\n\nCool to see others building in this space ‚Äî plenty of room for different approaches! ü§ù",
              "score": 2,
              "created_utc": "2026-01-26 07:45:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1s7eiw",
                  "author": "arxdit",
                  "text": "I wanted to have the capability of targeting enterprise level private deployments - like a law firm",
                  "score": 1,
                  "created_utc": "2026-01-26 08:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zgjsq",
              "author": "GP_103",
              "text": "Pretty cool. Did you actually Claude code that in 13 days. Props.",
              "score": 1,
              "created_utc": "2026-01-27 08:52:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ziacp",
                  "author": "arxdit",
                  "text": "Thank you!\n\nYes - first planned with gemini pro + opus, then passed to claude code. Still using this approach on major features / refactors.\n\nPeppered my codebase with MAP files and now my main problem is babysitting claude code... can't fully trust an AI with code but I'm getting used to its failure modes and I can smell the mistakes much faster",
                  "score": 1,
                  "created_utc": "2026-01-27 09:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zz39u",
          "author": "primoco",
          "text": "UPDATE: Benchmarks are live!\n\nAdded a complete benchmarking suite to the repo:\n\nBenchmark script: python benchmark/rag\\_benchmark.py ‚Äî run it on your hardware\n\nTest documents: Mueller Report, 9/11 Commission Report, Bitcoin Whitepaper, \"Attention Is All You Need\"\n\nReal metrics: Upload times, query latency (mean/median/p95), similarity scores\n\nResults from my setup (Ryzen 9 5950X, 64GB RAM, RTX 5070 Ti):\n\nQuery response: \\~4.3s mean, \\~3.6s median\n\nUpload: 0.6s - 24s depending on document size\n\nFull details in the README: Community Benchmarks section\n\nIf you run the benchmark on your hardware, I'd love to add your results to the comparison table. Open an issue or comment here!",
          "score": 1,
          "created_utc": "2026-01-27 11:37:05",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnplpb",
      "title": "Built a tool for visualizing text chunking strategies",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "author": "Crazy-Plan8697",
      "created_utc": "2026-01-26 19:09:49",
      "score": 26,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Hey :)\n\nSome time ago I wanted to learn the fundamentals of RAG, and I started with chunking. Greg Kamradt‚Äôs tool ([https://chunkviz.up.railway.app/](https://chunkviz.up.railway.app/)) helped me understand the basics, and it was a great starting point.\n\nWhile digging deeper, especially when reading papers like [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997), I noticed that **semantic** and **agentic** chunking are showing up more often and are getting adopted in new research. But I couldn‚Äôt find any visualizers that supported those methods, so I tried to build one myself.\n\nI put together **Chunking-Vis**, a small web tool for anyone who wants to explore and learn how different chunking strategies behave. I think it can be especially helpful if you‚Äôre new to RAG and want to see how chunks are formed before they‚Äôre sent to an embedding model or retrieval pipeline.\n\n# What Chunking-Vis supports\n\n* Character-level chunking\n* Word-level chunking\n* Token-level chunking (GPT-4o tokenizer)\n* Recursive chunking\n* Semantic chunking\n* Agentic chunking (Phi-3 powered, available locally on CPU)\n\nThere‚Äôs also a **Snapshot** feature that lets you save and compare different chunking configurations side-by-side, which can make experimentation easier.\n\n# Live Demo\n\n[https://chunkingvis-production.up.railway.app](https://chunkingvis-production.up.railway.app)  \n(Agentic chunking is disabled in the demo due to compute limits.)\n\n# Github Repository\n\n[https://github.com/MichalZnalezniak/Chunking-Vis](https://github.com/MichalZnalezniak/Chunking-Vis)\n\nHope some of you find it useful ‚Äî and if you have ideas, feedback, or suggestions, please let me know. Thank you!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qp9pmy",
      "title": "How to handle extremely large extracted document data in an agentic system? (RAG / alternatives?)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "author": "Complex-Time-4287",
      "created_utc": "2026-01-28 12:13:17",
      "score": 20,
      "num_comments": 27,
      "upvote_ratio": 0.96,
      "text": "I‚Äôm building an agentic system where users can upload documents. These documents can be *very* large ‚Äî for example, up to **15 documents at once**, where some are **\\~1500 pages** and others **300‚Äì400 pages**. Most of these are financial documents (e.g., tax forms), though not exclusively.\n\nWe have a document extraction service that works well and produces structured layout + document data.  \nHowever, the extracted data itself is also huge, so we **can‚Äôt fit it into the chat context**.\n\n  \n**Current approach**\n\n* The extracted structured data is stored as a **JSON file in cloud storage**\n* We store a **reference/ID in the DB**\n* Tools can fetch the data using this reference when needed\n\n  \n**The Problem**\n\nBecause the agent never directly ‚Äúsees‚Äù or understands the extracted data:\n\n* If a user asks questions about the document content,\n* The agent often can‚Äôt answer correctly, since the data is not in its context or memory\n\n  \n**What we‚Äôre considering**\n\nWe‚Äôre thinking about applying **RAG on the extracted data**, but we have a few concerns:\n\n* Agents run in a chat loop ‚Üí **creation + retrieval must be fast**\n* The data is deeply nested and very large\n* We want minimal latency and good accuracy\n\n**Questions**\n\n1. What are **practical solutions** to this problem?\n2. Which **RAG systems / architectures** would work best for this kind of use-case?\n3. Are there **alternative approaches** (non-RAG) that might work better for large documents?\n4. Any best practices for handling **very large documents** in agentic systems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o27bksd",
          "author": "Mishuri",
          "text": "You must do semantic chunking on the document. Split it up according to logical partitions. Ask LLM to enhance with descriptions metadata. Explicit relationships to other sections. Then embed those. 90% and the expensive part is LLM preprocessing of this. Then you gather context with vector rag and details with agentic rag + subagents to manage context.",
          "score": 5,
          "created_utc": "2026-01-28 12:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dy9y",
              "author": "rshah4",
              "text": "Yes, and then you can have a Table of Contents and make sure each of those sections understand their role in the hierarchical structure. This is what we do and it works well.  \nAlso a database can be useful here as well.",
              "score": 2,
              "created_utc": "2026-01-28 12:46:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27onir",
              "author": "Complex-Time-4287",
              "text": "this it totally possible, but I'm concerned about the time it is likely to take, in a chat it'll feel kind of blocking until the chuking and embedding is complete",
              "score": 1,
              "created_utc": "2026-01-28 13:47:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28i5j8",
              "author": "usernotfoundo",
              "text": "Are there any particular resources you would suggest that go into detail in this process? Currently I have been simply using an llm to process my large paragraph into a list of (observation,recommendation), embedding the observations and retrieving it based on similarity with a query. I feel this is too simplified, and breaking it down into multiple steps like you described could be the way to go, no idea how to start.",
              "score": 1,
              "created_utc": "2026-01-28 16:07:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d1jzt",
          "author": "aiprod",
          "text": "I think what most people are missing here are the strict latency requirements. The user uploads documents in a live chat session and wants to interact with them immediately, correct?\n\nThis rules out time intensive approaches like embeddings or generating summaries or metadata with LLMs.\n\nThere are a few things that could work:\n\nGive the agent a search tool that is based on BM25. Create page chunks from the data (usually a good semantic boundary too), index it into open search or elastic search and let the agent search the index. This is fast and context efficient.\n\nOn top of that, you could add the first one or two pages of each file to the context window of the agent. Usually, the first pages give an indication of what a doc is about. With that knowledge, the agent could make targeted searches inside a specific doc by using a filter with the search queries.\n\n\nAlternatively, you could use the file system based approach that coding agents like Claude code use. Give the agent tools to grep through the files and to read slices of the document. You don‚Äôt have to use an actual file system, it could just be simulated with tools. The agent will grep and slice through the docs to answer questions. RLM is an advanced version of this approach: https://arxiv.org/pdf/2512.24601v1",
          "score": 4,
          "created_utc": "2026-01-29 05:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d5vl6",
              "author": "Complex-Time-4287",
              "text": "That's right! Thanks for the suggestions, I'll try this",
              "score": 1,
              "created_utc": "2026-01-29 06:12:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27mwi8",
          "author": "KnightCodin",
          "text": "There are few gaps in the problem summary that might help  \n1. Operational flow :   \nWhen you say \"tools can fetch the data using this reference when needed\" and \"gent never directly ‚Äúsees‚Äù or understands the extracted data\",   \n\\- Where is data going to - directly to the user and not to the Agent/LLM?  \n\\- What is stopping you from presenting the \"summary\" data (Chain of Density compressed) to the Agent so follow up questions can be answered\n\n2. Do you need to answer questions on documents by other users? - Meaning is it a departmental segmentation with multiple users but same overall context or user/documents are isolated?   \n\\- This will provide type of KG and scale   \n\\- Summary \"Contextual Map\"",
          "score": 2,
          "created_utc": "2026-01-28 13:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27riay",
              "author": "Complex-Time-4287",
              "text": "In my agentic system, users can connect third-party MCP tools. If a tool requires access to the extracted data, the agent can pass that data to the specific tool the user has attached, but only when it‚Äôs actually needed.\n\nThe main issue with relying on summaries is that the extracted data itself is already very large and deeply nested JSON. Generating a meaningful summary from it is hard, and even a compressed (Chain-of-Density‚Äìstyle) summary would still fail to answer very specific questions‚Äîfor example, ‚ÄúWhat was the annual income in 2023?‚Äù\n\nRegarding document access and isolation: documents are scoped strictly to the current conversation. Conversations are not user-specific, and there can be multiple conversations, but within each conversation we only reference the documents uploaded in that same context.\n\nDocuments are uploaded dynamically as part of the conversation flow, and only those on-the-go uploads are considered when answering questions or invoking tools.",
              "score": 1,
              "created_utc": "2026-01-28 14:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27twm2",
                  "author": "KnightCodin",
                  "text": "Better :)  \nSimplistic and practical solution (not to be confused with simple) is   \nMulti-tier retrieval:   \n  \nEg : ‚ÄúSummary doc map‚Äù ‚Üí ‚ÄúTargeted Sub-Node‚Äù ‚Üí \"Drill-Down Deep fetch\"\n\nThis will be the most latency-effective for massive bundles.\n\n**SPECIFICITY :**   \n**Tier A: coarse index**\n\n* Embed¬†full **page summaries**,¬†**section headers**, and¬†**table captions**\n* Or **one chunk per page**¬†: Fully summarized (Will say normalized but that will open a whole new can of worms)\n* Path: identify *which pages/sections matter -> use deep fetch to grab that JSON*\n\n**Tier B: targeted extraction retrieval**\n\n* Once you know relevant pages/sections, fetch only that slice from cloud storage:\n   * e.g., pages 210‚Äì218 JSON\n   * or the section subtree for¬†`Income >` What was the annual income in 2023",
                  "score": 1,
                  "created_utc": "2026-01-28 14:14:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27bc02",
          "author": "patbhakta",
          "text": "For financial data, skip the traditional RAG, skip the vector databases, perhaps skip graph rag too. Go with trees, you'll incur more cost but at least your data will be sound.",
          "score": 1,
          "created_utc": "2026-01-28 12:28:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fxm7",
              "author": "yelling-at-clouds-40",
              "text": "Trees are just subset of graphs, but curious: what kind of trees do you suggest (as node hierarchy)?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27nv20",
              "author": "ajay-c",
              "text": "Interesting do you know any tree techniques?",
              "score": 1,
              "created_utc": "2026-01-28 13:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27oswt",
              "author": "Complex-Time-4287",
              "text": "can you please provide some details on this?",
              "score": 1,
              "created_utc": "2026-01-28 13:48:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bjew",
          "author": "arxdit",
          "text": "I‚Äôm handling this via knowledge trees and human supervised document ingestion (you supervise proper slicing and where the document belongs in the knowledge tree - though the AI does make suggestions)\n\nThe AI by itself is very bad at organizing information with no clear rules and will fail spectacularly \n\nSlowly learning through this\n\nYou can check out my solution [FRAKTAG on github](https://github.com/andreirx/FRAKTAG)",
          "score": 1,
          "created_utc": "2026-01-28 12:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27nxzr",
              "author": "ajay-c",
              "text": "Interesting",
              "score": 1,
              "created_utc": "2026-01-28 13:43:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27p3tn",
              "author": "Complex-Time-4287",
              "text": "Looks interesting, I'll check this  \nFor my use-case, we cannot really have a human in the loop, agents are completely autonomous and must proceed on their own",
              "score": 1,
              "created_utc": "2026-01-28 13:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27q1w8",
                  "author": "arxdit",
                  "text": "I want to get there too and I‚Äôm using my human decisions to hopefully ‚Äúteach‚Äù the ai how to do it by itself, and I am gathering data",
                  "score": 1,
                  "created_utc": "2026-01-28 13:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27btme",
          "author": "proxima_centauri05",
          "text": "You‚Äôre not doing anything ‚Äúwrong‚Äù. This is the natural failure mode when the agent only has a pointer to the data instead of an understanding of it. If the model never sees even a compressed view of the document, it‚Äôll confidently answer based on vibes.\n\nWhat‚Äôs worked for me is separating understanding from storage. On ingestion, I generate a thin semantic layer, section summaries, key entities, numbers, obligations, relationships. That layer is small, fast, and always available to the agent. The heavy JSON stays out of the loop unless the agent explicitly needs to verify something.\nTrying to RAG directly over deeply nested extracted data is usually a dead end. It‚Äôs slow, and the signal to noise ratio is awful. Hierarchical retrieval helps a lot, first decide where to look, then pull only that slice, then answr. Latency stays low because most questions never touch the raw data.\n\nFor financial or forms heavy docs, I often skip RAG entirely and just query normalized fields. It‚Äôs boring, but it‚Äôs correct. RAG is great for ‚Äúexplain‚Äù questions, terrible for ‚Äúcalculate‚Äù ones.\n\nI‚Äôm building something in this space too, and the big unlock was treating documents like evolving knowledge objects, not blobs you fetch. Once the agent has a map of the document, it stops hallucinating and starts reasoning.",
          "score": 1,
          "created_utc": "2026-01-28 12:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27tqal",
              "author": "Complex-Time-4287",
              "text": "In my case, the questions are much more likely to be *‚Äúfind‚Äù* questions rather than *‚Äúcalculate‚Äù* ones. For extremely large documents say a 1,500-page PDF containing multiple tax forms summaries or key-entity layers won‚Äôt realistically capture all the essential details.\n\nAlso, I‚Äôm not entirely sure what you mean by ‚Äújust query normalized fields‚Äù in this context.",
              "score": 1,
              "created_utc": "2026-01-28 14:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27oiym",
          "author": "ajay-c",
          "text": "I do have same issues",
          "score": 1,
          "created_utc": "2026-01-28 13:46:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27swp4",
          "author": "Crafty_Disk_7026",
          "text": "Try using a retrieval MCP https://github.com/imran31415/codemode-sqlite-mcp/tree/main. Here's one I made you can try.  This won't require embedding",
          "score": 1,
          "created_utc": "2026-01-28 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u005",
          "author": "Popular_Sand2773",
          "text": "Your instinct that you can't just shove these in the context window is correct and that you need some sort of RAG but what and why depend on the answers to these questions.\n\nWhat questions are your users asking?  \nGiven it is financial documents if it is mainly numbers and tables they care about then you should think about a SQL db and retrieval. Regular semantic embeddings are not very good at highly detailed math. If it's contract minutia then maybe a vector db and semantic embeddings. Likely you'll need both.\n\nHow much of this is noise?  \nYou mention huge documents and tax forms as an example. If a lot of this is stuff your users are never going to query you are paying both in quality and cost for things you won't use and don't need. Figure out what you can prune. \n\nIs there clear structure you can leverage?  \nJust because it's called unstructured text doesn't mean there is no structure at all. If you can narrow down where in the documents you are looking for a specific query based on the inherent structure like sections etc then you can narrow the search space and increase your top-k odds.\n\nAll this to say. It's not about what RAG is best etc it's what problem are you actually trying to solve and why. If you just want a flat quality bump without further thought try knowledge graph embeddings.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28dreo",
          "author": "Det-Nick-Valentine",
          "text": "I have the same problem.\n\nI'm working on an in-company solution like NotebookLM.\n\nIt works very well for small and medium-sized documents, but when the user uploads something large, like legal documents, it doesn't give good responses.\n\nI'm thinking of going for a summary by N chunks and working with re-ranking.\n\nWhat do you think of this approach?",
          "score": 1,
          "created_utc": "2026-01-28 15:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9g7o",
          "author": "pl201",
          "text": "Take a look of open source LightRAG. Per my research and trying, it has the best potential to be used for the requirements you have described in the post. I am working on enhancements so it can be used on a company setting (multi users, workspace, separate embedding LLM and chat LLM, speed the query for a larger knowledge base.  Etc). PM me if you are interested to make it work for your case.",
          "score": 1,
          "created_utc": "2026-01-28 20:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e7nmc",
          "author": "Infamous_Ad5702",
          "text": "We had a similar problem for a client.\n\nNo GPU needed\nThey Can‚Äôt use black box LLM.\nThey Can‚Äôt have hallucinations.\n\nDefence industry so needed to be offline.\n\nWe built a tool that builds an index first. Makes it efficient. Every new query it builds a new Knowledge Graph. \n\nDoes the trick.",
          "score": 1,
          "created_utc": "2026-01-29 11:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eeuh4",
          "author": "TechnicalGeologist99",
          "text": "Hierarchical RAG. The document structure is important. Detect section headers and use them to construct a data tree of each document. \n\nAt the same time extract tags that you will predefined (i.e. financial, design, technical) use those same tags at query time to prefilter. \n\nWhen a section gets many hits from semantic retrieval you will upgrade and retrieve more or all of that section (it's clearly relevant)\n\nEnsure you use query decomposition (fragmenting the users question into multiple questions for multiple retrieval) and rerank those. For large retrievals, group chunks by their section id and summarise them in context of the sub question that was used to retrieve them. And then inject those summaries as documents in the final call. \n\nCongrats you didn't really need an agentic system. \nBut you can always migrate to one if and when the time is right. But don't just go agentic because it's popular. Build your domain and solutions by proving the need (YAGNI)",
          "score": 1,
          "created_utc": "2026-01-29 12:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2mnabf",
          "author": "Ecstatic_Heron_7944",
          "text": "Chiming in to offer an alternative perspective: you're doing all this document extraction, table identification, json generating, heavy processing and time consuming work for pages the user hasn't even asked for. In a document with 300 pages, any given query could require a fraction (maybe 10 - 20 pages) for a suitable answer. Could a better approach be to do the search (fast) first and extraction (slow) later - especially after the user is happy to confirm the context? Well, I hope so because this is what I'm building with [ragextract.com](http://ragextract.com) !\n\nTo answer your questions:\n\n1. RAG would work as a way to try narrow the search space for the user query but for financial data, it's unlikely to be sufficient on its own. You'll still need to post-process the pages for accuracy - though you may sometime get away with just winging it with vision models.  \n2. Multimodal RAG works incredibly well if documents don't share a standardised layout ie different statements from different bank. You'd also might want to look at a more optimised retrieval system for pages.  \n3. In practical terms, not that I can think of. Search-first-parse-later is an alternative RAG approach I think is worth exploring in this scenario.  \n4. Best practices for large documents? You probably already know this but go (1) big, (2) async and (3) distributed!",
          "score": 1,
          "created_utc": "2026-01-30 16:32:47",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq8y1q",
      "title": "TextTools ‚Äì High-Level NLP Toolkit Built on LLMs (Translation, NER, Categorization & More)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "author": "Due_Place_6635",
      "created_utc": "2026-01-29 13:45:08",
      "score": 19,
      "num_comments": 1,
      "upvote_ratio": 0.93,
      "text": "Hey everyone! üëã\n\nI've been working on TextTools, an open-source NLP toolkit that wraps LLMs with ready-to-use utilities for common text processing tasks. Think of it as a high-level API that gives you structured outputs without the prompt engineering hassle.\n\nWhat it does:\n\nTranslation, summarization, and text augmentation\n\nQuestion detection and generation\n\nCategorization and keyword extraction\n\nNamed Entity Recognition (NER)\n\nCustom tools for almost anything\n\nWhat makes it different:\n\nBoth sync and async APIs (TheTool & AsyncTheTool)\n\nStructured outputs with validation\n\nProduction-ready tools (tested) + experimental features\n\nWorks with any OpenAI-compatible endpoint\n\nQuick example:\n\npython\nfrom texttools import TheTool\n\nthe_tool = TheTool(client=openai_client, model=\"your_model\")\nresult = the_tool.is_question(\"Is this a question?\")\nprint(result.to_json())\nCheck it out: https://github.com/mohamad-tohidi/texttools\n\nI'd love to hear your thoughts! If you find it useful, contributions and feedback are super welcome. What other NLP utilities would you like to see added?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2ewxt9",
          "author": "Popular_Sand2773",
          "text": "Hey like the idea of a one stop shop but a lot of these are tasks I would never give to an llm. Have you thought about unifying the BIC options for these different tasks? BERTopic is a good example of a unifying framework that takes the best from everyone.",
          "score": 2,
          "created_utc": "2026-01-29 14:17:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qohzz3",
      "title": "Multilingual RAG for Legal Documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "author": "mathrb",
      "created_utc": "2026-01-27 16:06:51",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.84,
      "text": "Hey all,\n\nWe're a small team (not many engineers) building a RAG system for legal documents(contracts, NDAs, terms of service, compliance docs, etc.).\n\nThe multilingual challenge:\n\nOur documents span multiple languages (EN, FR, DE, ES, IT, etc.).\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have docs in a single language (e.g., all French)\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have mixed-language corpora\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some individual documents are bilingual\n\n¬†\n\nFor legal docs, hybrid search (FT search and dense vectors with re rank) seems to be a good candidate for retrieval. One issue I saw is that most implementations relies on language dependent solutions for FT search.\n\nApproaches I've seen discussed:\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Per-language BM25 indexes: Detect language, route to the right index with proper stemmer. Seems correct but adds complexity. How do you handle bilingual documents?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Language-agnostic tokenization: Skip stemming, just split on whitespace. Loses morphological matching but works across languages.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† BGE-M3 sparse vectors: Supposedly handles 100+ languages natively for both dense and sparse. But does it require GPU? What's the cost/perf tradeoff vs traditional BM25?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Translate everything to English: Normalize the knowledge base. Feels wrong for legal where original wording matters and adds a translation failure mode.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Dense-only + reranker : Skip BM25 entirely, use strong multilingual embeddings (BGE-M3, multilingual-e5) and rerank. Loses exact keyword matching.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Qdrant's native BM25 : Qdrant now has built-in BM25 with language configs. Anyone using this for multilingual? How does it compare to dedicated solutions?\n\n¬†\n\nWe‚Äôd rather use managed services when available in the cloud provider we chose (scaleway).\n\nOur constraints:\n\n* Managed PostgreSQL for app data : only supports pgvector, not pg\\_search/ParadeDB. Would require to self-host a postgres for additional extensions.\n* Prefer simplicity: Leaning toward Qdrant over Milvus since it seems easier to operate.\n* Cost-conscious: GPU-heavy solutions for embeddings are a concern.\n* Multi-tenant: Each tenant's documents are typically in one consistent language, but not always.\n\nAnyone would like to share their experience or thoughts on this challenge?  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o21ijp5",
          "author": "patbhakta",
          "text": "1) translate everything to English is a bad solution for legal reasons as the verbage needs to stay in tact. Translation is another can of worms you don't want to go down.\n\n2) you could have multiple vector\\graph\\sql databases for each language, querying them all could be a nightmare though.\n\n3) embeddings, chunking, parsing, are all an issue that needs to be addressed with every RAG.\n\n4) legal documents are like a git repo, multiple versions and annotations, how are going to handle edits?\n\n5) cross referencing legal jurisdiction that periodically changes as well and in multiple countries you'll need a system for that knowledge base\n\n6) the larger your documents grow the worse your AI will get and will hallucinate beyond use. \n\n7) dedups and gardening can help but that's another wrench in the system\n\nPersonally I wouldn't use RAG on your use case, you need a truthful symantec search with citation. You really should seek consultation before going forward on ideas.",
          "score": 8,
          "created_utc": "2026-01-27 16:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nzh2",
              "author": "mathrb",
              "text": "Thanks for your reply\n\n1. Agree, feels wrong in this domain\n2. Agree with the nightmare\n3. As of today, only last version will be handled\n4. Not in scope\n\n\\`truthful symantec search with citation\\`, what kind of systems are you referring to?  \nEdit: I assume you meant semantic search.  \nEven though, we still face some of the same the challenges (minus the generative part at the end), right?",
              "score": 1,
              "created_utc": "2026-01-27 16:55:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21wv03",
                  "author": "patbhakta",
                  "text": "Yes semantic hybrid approach. \nWithout knowing your document size, document length, who the users are, what do they want done, etc it's hard to provide a solution. \n\nIf the scenario was like a legal SaaS where documents needed to be translated then AI is great at that or a summary to avoid reading hundreds of pages. A tuned AI with case logic can also research as a paralegal or even a college level lawyer.\n\nIf you're building an AI tailored to the firm based on its documents, procedures and cases then it's a bit more complex but doable.\n\nIf you're building an AI for automation even that's doable but a different approach. \n\nHave to break down the scope, what it's roadmap is like, budget, legal compliance, security access (who has access to what documents) etc...",
                  "score": 1,
                  "created_utc": "2026-01-27 17:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o269emw",
          "author": "jael_m",
          "text": "You can still do the hybrid search combining dense vector search and text match like BM25. There are some special tokenizers for multilingual text. For example, milvus supports language identifier to automatically detect and apply the proper tokenizer, and the multi-language analyzer for text retrieval.",
          "score": 2,
          "created_utc": "2026-01-28 07:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e9yg",
              "author": "explodedgiraffe",
              "text": "But sparse will still fail on cross language queries right? It would match a query ‚Äúv√©lo‚Äù (French) with ‚Äúbike‚Äù in English ?",
              "score": 1,
              "created_utc": "2026-01-28 07:47:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blzit",
          "author": "Repulsive-Memory-298",
          "text": "Interesting. I mean bm25 is beyond FT right? You could start with FT key word maybe? Where you could milk that by sorting by vector similarity? \n\nI wonder if term frequency would really struggle with combined languages. \n\nAnyways there are lots of future directions. Definitely start with more basic solutions imo, it‚Äôll give you a footing and then you can explore and test more complex solutions. BM25 is pretty advanced FT compared to meat and potato‚Äôs.",
          "score": 1,
          "created_utc": "2026-01-29 00:34:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ekocl",
          "author": "bumkey1101",
          "text": "Yeah I‚Äôve dealt with this a bit ‚Äî I wouldn‚Äôt overthink ‚Äúperfect multilingual BM25‚Äù.\n\nBM25/FT is awesome **when query + doc are same language**, but it‚Äôs basically not gonna do ‚Äúv√©lo‚Äù ‚Üí ‚Äúbike‚Äù unless you start doing synonym/translation stuff. So I‚Äôd just accept that and let **multilingual embeddings handle cross-lang**, and use BM25 for the ‚Äúsame language, exact wording‚Äù wins (which matters a ton in legal).\n\nAlso bilingual docs: don‚Äôt route by *document* language, route by **chunk**. Chunk it, detect lang per chunk, store `chunk_lang`. Then BM25 mostly hits same-lang chunks, dense can still pull cross-lang if needed.\n\nRe: translate everything to English‚Ä¶ yeah no thx for legal. If you ever need translation, do it only as a *query expansion* (translate the query to a couple langs) not the whole corpus.\n\nGiven your Scaleway constraints, honestly **OpenSearch** for the text side is a pretty sane pick. Qdrant BM25 is nice but once you want analyzers/highlighting/etc you‚Äôll end up in search-engine land anyway.\n\nKeep rerank small (top 20‚Äì50) and do embeddings async on ingestion so you‚Äôre not ‚ÄúGPU all day‚Äù üî•.",
          "score": 1,
          "created_utc": "2026-01-29 13:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2f4ib9",
          "author": "proxima_centauri05",
          "text": "We‚Äôve been dealing with something similar. What‚Äôs worked reasonably well for us is keeping raw text untouched and making embeddings fully configurable, so we can switch to multilingual models and re embed without changing the pipeline. That already handles cross language semantic recall better than expected, even for mixed language documents.\n\nHybrid is where it gets tricky for legal. Per language BM25 felt correct in theory but added a lot of operational complexity, especially with bilingual docs. Dense only was simpler but missed exact phrasing cases, so reranking became important.\n\nWe‚Äôve avoided translation entirely because original wording matters too much in legal. Curious to hear how others balance keyword precision vs complexity in production.",
          "score": 1,
          "created_utc": "2026-01-29 14:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2g2ri1",
          "author": "Dry_Substance_5124",
          "text": "Given your constraints (small team, cost-conscious, multi-tenant), I‚Äôd do: multilingual dense embeddings + reranker, and add a lightweight language-agnostic lexical layer (no stemming) for exact strings, citations, clause numbers, names. Per-language BM25 is ''correct'' but operationally annoying. For bilingual docs, index at the chunk level with language tags. I‚Äôve seen AI Lawyer-style systems win by keeping retrieval simple and spending effort on chunking + eval, not over-building the index zoo.",
          "score": 1,
          "created_utc": "2026-01-29 17:29:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qngww9",
      "title": "GraphRAG D√©j√† Vu: Freezing Edges = Graph DB Repeat? (Prod Trade-offs)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "author": "dqj1998",
      "created_utc": "2026-01-26 14:05:21",
      "score": 12,
      "num_comments": 10,
      "upvote_ratio": 0.93,
      "text": "**Update (Jan 27, 2026)**:  \nThanks for the great discussion here in r/RAG! Some highlights from related threads:\n- Determinism & reproducibility as key to relational DB win (echoing why Graph DBs struggled).\n- Real prod experiences: keep graphs deterministic/auditable (e.g., calls/imports/FKs), avoid LLM-guessed edges clutter.\n- Links shared: [DDG preprint](https://zenodo.org/records/18373053) and [RoslynIndexer repo](https://github.com/RusieckiRoland/RoslynIndexer) for deterministic code RAG.\n\n---\n\nr/RAG ‚Äî GraphRAG hype (explicit graphs over vector RAG) feels like 70s graph DBs (IMS/CODASYL): explicit relations won benchmarks but lost to relational cuz upfront assumptions brittle.\n\n**Hype vs Reality**\nLLM infers entities/relations ‚Üí persist edges ‚Üí query traversal. Cool for global search, but edges = ingestion-time guesses ‚Üí bias for new intents.\n\n**Core Brittleness**\nFrom my [r/programming post](https://www.reddit.com/r/programming/comments/1pz6pj3/graphrag_is_just_graph_databases_all_over_again/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): Nodes=facts, edges=guesses. Scoped query-time inference (BM25+vectors+rerank) often better for ambiguous RAG (no freeze).\n\n**Pushback & Predictability**\nComments nailed it: auditable edges > opaque LLMs (prod win). Dynamic rebuilds? Viable, but maintenance cost high vs simple hybrid RAG.\n\nShines: stable domains (regs/code deps). Fails: intent-shifting queries.\n\nMedium breakdown:[Medium friend link](https://medium.com/sisai/graphrags-deja-vu-why-are-we-repeating-the-same-mistakes-f6852f54bde0?sk=2692c642e7dfb19e9d552162462384c4)\n\n\nProd experiences? GraphRAG beat baseline RAG on your corpus (e.g., multi-hop QA, latency)? Hybrid + dynamic graphs? Or stick to rerank?\n\nShare benchmarks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1uoi8i",
          "author": "RolandRu",
          "text": "Really happy I found this Reddit btw ‚Äî the topics here are genuinely interesting and they kind of force you to think things through.\n\nAnd yeah, you‚Äôre right: it really depends.\n\nGraphs *can* be brittle when the edges are basically guessed during ingestion (LLM-inferred relations). You‚Äôre sort of freezing assumptions that may not match what people will ask later.\n\nBut it‚Äôs very use-case dependent. For code, I honestly think a dependency graph is pretty much **non-optional**. Calls/imports/inheritance aren‚Äôt opinions ‚Äî they‚Äôre real structure. Without graph expansion you often end up with random snippets, and vanilla RAG struggles badly with questions like ‚Äúwhere does this start?‚Äù or ‚Äúwhat does this change affect?‚Äù, because you‚Äôre missing the whole call chain.",
          "score": 4,
          "created_utc": "2026-01-26 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xa5pj",
              "author": "dqj1998",
              "text": "I totally agree! Seeing your comment about code dependencies resonated with me, I also believe that code is essentially a set of pre-defined dependency chains, and debugging is about constantly patching the gap between these pre-defined chains and reality.\n\nWhile my thinking is still quite rudimentary, that's precisely why I wanted to discuss it further here.\n\nThank you for sharing your real-world experience! In this era where AI is coding faster and faster, this kind of discussion is really interesting‚Äîdo you think AI might represent a spiral of dependency reduction?",
              "score": 2,
              "created_utc": "2026-01-27 00:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xg881",
                  "author": "RolandRu",
                  "text": "I see it like this: dependencies aren‚Äôt going away, because they‚Äôre basically a consequence of architecture (boundaries, responsibilities, contracts). But AI lowers the cost of doing things ‚Äúthe right way‚Äù ‚Äî it‚Äôs easier to add an adapter, an interface, a test, validation, or split a big chunk into smaller parts, without feeling like you‚Äôre wasting time on repetitive stuff. So you‚Äôre less tempted to cram everything into one file/method ‚Äújust because it‚Äôs faster.‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-27 00:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1upwsy",
          "author": "hhussain-",
          "text": "Determinism is what really makes this paradigm *shake*.  \nYour Reddit post and Medium article are excellent ‚Äî they pinpoint both the possibilities *and* the limitations very clearly.\n\nWhat ultimately made relational databases win wasn‚Äôt just performance, but **determinism and reproducibility enabled by a new computing model**.\n\nI had a similar discussion on [r/Rag post](https://www.reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) about deterministic graphs vs probabilistic vectors/embeddings. That thread, along with related discussions, helped isolate the issue to what seems like a missing graph category.\n\nInteresting to see this resurfacing now. I‚Äôve just published a timestamped preprint defining a *Deterministic Domain Graph (DDG)* category:  \n[https://zenodo.org/records/18373053](https://zenodo.org/records/18373053)\n\nI‚Äôm currently working on a framework to construct DDGs in practice, and early experiments suggest this is feasible even for large real-world codebases.",
          "score": 3,
          "created_utc": "2026-01-26 17:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vrjfq",
              "author": "RolandRu",
              "text": "Thanks for sharing the article ‚Äî honestly pretty interesting read.\n\nThis is actually close to where I ended up while building a **code RAG** system. I‚Äôm trying to keep the *edges* deterministic + auditable (calls/imports/inheritance, ReadsFrom/WritesTo, FKs etc.), and I‚Äôm really trying to avoid freezing ‚ÄúLLM-guessed‚Äù relations during ingestion.\n\nI kind of treat vectors as *ranking / fuzzy recall*, but the graph as the closed-world structure that should rebuild the same way every time. For example I force stable outputs (sorted nodes/edges) and I also add missing TABLE nodes so the SQL graph is actually closed (nodes + edges), not half implicit.\n\nOne thing I‚Äôd highlight though: **heuristics ‚â† inference**. I‚Äôm fine with fixed, testable heuristics (like inline SQL detection) ‚Äî even if it‚Äôs not perfect, it‚Äôs still deterministic and you can regression-test it. What I‚Äôm trying to avoid is context-dependent enrichment that changes depending on the model/prompt or whatever the ‚Äúbest guess‚Äù is this week.\n\nIf you‚Äôre curious, this repo is just the **indexing part** (Roslyn/.NET side). The actual RAG pipeline / retrieval is in a separate project:  \n[https://github.com/RusieckiRoland/RoslynIndexer](https://github.com/RusieckiRoland/RoslynIndexer)\n\nAlso curious how you want to handle schema evolution / versioning for DDGs on big real repos ‚Äî do you version the domain spec per build, kind of like a compiler?",
              "score": 3,
              "created_utc": "2026-01-26 20:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xcvpz",
                  "author": "dqj1998",
                  "text": "Wow, thank you both‚Äîthis thread is gold! \n\nu/hhussain, your DDG preprint looks fascinating‚Äîdeterminism + reproducibility as the key to why relational won makes total sense, and early experiments on large codebases sound promising. Will dive into it right away.\n\nu/RolandRu, super appreciate you sharing your setup and the RoslynIndexer repo! Exactly aligns with what I've been thinking: keep the graph as closed-world, deterministic structure (hard facts like calls/imports/inheritance/ReadsFrom/WritesTo/FKs), and treat vectors as fuzzy recall/ranking. Love the emphasis on heuristics (fixed, testable, regression-friendly) vs. context-dependent LLM guesses‚Äîthat's the brittleness killer.Your approach to forcing stable outputs (sorted nodes/edges) and adding missing nodes for closed structure is smart‚Äîavoids the \"half implicit\" mess. Curious on a couple things:\n\n\\* How do you handle schema evolution/versioning in big repos? Per-build domain spec like a compiler, or something else?\n\n\\* Have you seen measurable wins on those chain-tracing queries (e.g., impact analysis) vs. vanilla RAG?\n\n\\* Any thoughts on hybrid with dynamic rebuilds for evolving code, or is pure deterministic the way?\n\nThis ties perfectly into what I'm exploring next: code itself as frozen presuppositions/dependencies, and debug as closing the gap to reality. If you're open, I'd love to reference/link your repo/preprint in upcoming posts (with credit, of course).\n\nThanks again‚Äîthreads like this are why I love posting here!",
                  "score": 3,
                  "created_utc": "2026-01-27 00:30:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qpla70",
      "title": "A framework to evaluate RAG answers in production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "author": "esp_py",
      "created_utc": "2026-01-28 19:26:55",
      "score": 12,
      "num_comments": 8,
      "upvote_ratio": 0.94,
      "text": " How do you know your RAG system is sending correct answers to users? A\n\n\n\nFollowing a recent discussion[ i had here](https://www.reddit.com/r/datascience/comments/1o5n86i/in_production_how_do_you_evaluate_the_quality_of/), I went ahead and  developed a waterfall evaluation framework designed to fail safely and detect hallucinations. \n\nKey components: \n\n\\- Pre-generation retrieval checks \n\n\\- Answerability validation \n\n\\- Faithfulness scoring (NLI, RAGAS, LLM-as-judge) \n\n\\- Answer relevance checks \n\n\n\n[https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach](https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach)\n\nPlease have a read and let me your thoughs, I will share the results soon in the second part.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2a4j6q",
          "author": "No_Kick7086",
          "text": "Nice work, really good article that and covers an area I am looking to address with my own SME+ Rag system. Im curious about the answerability checks., was the classifier or llm more accurate? Im taking a guess at classifier. Look forward to part 2 on this with your results! Cheers",
          "score": 2,
          "created_utc": "2026-01-28 20:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a4tdw",
              "author": "esp_py",
              "text": "On the answerability check I just tried the LLM and it work fine",
              "score": 2,
              "created_utc": "2026-01-28 20:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ahg08",
                  "author": "seomonstar",
                  "text": "nice to know",
                  "score": 1,
                  "created_utc": "2026-01-28 21:18:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d01rd",
          "author": "Charming_Group_2950",
          "text": "TrustifAI also seems to solve the same problem with little different approach. It gives a trust score along with explanations for correct or hallucinated responses. Explore more here:¬†https://github.com/Aaryanverma/trustifai",
          "score": 2,
          "created_utc": "2026-01-29 05:27:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e7w6h",
              "author": "No_Kick7086",
              "text": "Looks good. How are the results from it?",
              "score": 1,
              "created_utc": "2026-01-29 11:45:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e9qhx",
                  "author": "Charming_Group_2950",
                  "text": "Benchmarking in progress. You can see the results here in readme of repo. So far results are promising.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bl4r2",
          "author": "Able-Let-1399",
          "text": "Thanks, good stuff when learnings from real projects are shared üëç. Looking forward to next part.\nI wonder: You must have been warned / read about not asking LLM as a judge to validate multiple aspects in the same prompt, so why do you do that?",
          "score": 1,
          "created_utc": "2026-01-29 00:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l23jr",
              "author": "esp_py",
              "text": "Yes, that is a good point! For the LLM as a judge, I didn't get the choice of the prompt, I used an internal package that have a predefined prompt.",
              "score": 1,
              "created_utc": "2026-01-30 11:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo9u4g",
      "title": "I'm looking for an OCR for my RAG.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "author": "AdministrationPure45",
      "created_utc": "2026-01-27 10:04:44",
      "score": 11,
      "num_comments": 30,
      "upvote_ratio": 1.0,
      "text": "Which one do you think is the best among:   \nMistral OCR, LightOnOCR-2, Chandra, OlmOCR 2, Dolphin-v2, LlamaParse, Reducto, Qwen2.5-VL-8B, or DeepSeekOCR?\n\nWhich one do you use? Thanks ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2009jp",
          "author": "CapitalShake3085",
          "text": "Qwen3 vl",
          "score": 4,
          "created_utc": "2026-01-27 11:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr0q1",
          "author": "zzriyansh",
          "text": "use pymupdf4llm + layout + pro",
          "score": 4,
          "created_utc": "2026-01-27 10:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o201f6p",
          "author": "roydotai",
          "text": "DeepSeekOCR v2 came out recently. I haven‚Äôt gotten around to test it myself yet, but it looked interesting",
          "score": 4,
          "created_utc": "2026-01-27 11:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zzn59",
          "author": "hashiromer",
          "text": "Go with LlamaParse or Reducto if budget allows. I benchmark various PDF parsing solutions for work and LlamaParse and Reducto are far ahead of everything else. However, i only tested at highest compute so ymmv.\n\nYou can also directly use Gemini 3 Flash to convert PDFs to markdown directly with a prompt but you will need to write some basic logic like splitting pages and convert each separately. Gemini on its own beats specialized pipeline based OCRs solutions easily on my internal benchmarks.\n\nIt also depends on complexity of layout by the way, if you are dealing with simple layouts, simpler pipeline based tools like docling,minerU,marker could also work very well.",
          "score": 3,
          "created_utc": "2026-01-27 11:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20aoiw",
              "author": "nofuture09",
              "text": "why reducto? isnt llamaparse better and cheaper?",
              "score": 1,
              "created_utc": "2026-01-27 12:58:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20r6yd",
                  "author": "hashiromer",
                  "text": "Reducto was cheaper last i tested and faster but LlamaParse was slightly more accurate.",
                  "score": 1,
                  "created_utc": "2026-01-27 14:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o202dir",
          "author": "SiebenZwerg",
          "text": "We use Mistral OCR due to high accuracy.",
          "score": 3,
          "created_utc": "2026-01-27 12:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20gus1",
          "author": "AloneSYD",
          "text": "I like a very underrated OCR model nanonets/Nanonets-OCR2-3B FP8 quantized",
          "score": 3,
          "created_utc": "2026-01-27 13:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr1pz",
          "author": "fabkosta",
          "text": "‚ÄúBest‚Äù depends on your data. You need to do a PoC to find out. Also look at Docling, it‚Äôs free.",
          "score": 2,
          "created_utc": "2026-01-27 10:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zt402",
          "author": "Live-Guitar-8661",
          "text": "We use Llama-4-17b with Poppler, probably not the popular choice out there but works for us. Give it a shot if you want to see how well it works: https://orchata.ai\n\nPS- not giving you the link as a plug, just saying if you want to see how they work, it‚Äôs free and you can get a sense of the output in the dashboard. Hope that helps.",
          "score": 2,
          "created_utc": "2026-01-27 10:47:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zbll",
          "author": "it_and_webdev",
          "text": "Docling",
          "score": 2,
          "created_utc": "2026-01-27 15:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o269e9z",
          "author": "sirebral",
          "text": "Smallest Qwen 3 VL variant is wonderful at this task, even with a four bit quant.",
          "score": 2,
          "created_utc": "2026-01-28 07:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zu2u4",
          "author": "instantlybanned",
          "text": "Depends a little on what you need it for (document ocr, read text in images etc.) but for just general purpose OCR on images, Paddle's PP-OCRv5 is probably the state-of-the-art model out there at the moment.",
          "score": 1,
          "created_utc": "2026-01-27 10:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o200kzs",
          "author": "teroknor92",
          "text": "ParseExtract works well for complex documents with tables, handwriting etc. The pricing is very friendly with good ocr and data extraction accuracy.",
          "score": 1,
          "created_utc": "2026-01-27 11:48:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21fnkx",
          "author": "Rich-Emu-1561",
          "text": "An OCR API could help that. I have been using qoest for developers platform for similar document processing. You can check their site to see if it fit your setup.",
          "score": 1,
          "created_utc": "2026-01-27 16:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o224zfc",
          "author": "maniac_runner",
          "text": "llmwhisperer if you want to parse complex tables in documents",
          "score": 1,
          "created_utc": "2026-01-27 18:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228kmn",
          "author": "zmanning",
          "text": "[https://99franklin.github.io/ocrbench\\_v2/](https://99franklin.github.io/ocrbench_v2/)",
          "score": 1,
          "created_utc": "2026-01-27 18:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23qx01",
          "author": "No_Thing8294",
          "text": "What do you want to achieve? Qwen-VL is totally different to Deepseek-OCR. Totally different approaches.",
          "score": 1,
          "created_utc": "2026-01-27 22:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23r5r4",
              "author": "No_Thing8294",
              "text": "‚Ä¶and it depends on the kind of input data. Text? Tables, complex PDFs etc.?",
              "score": 1,
              "created_utc": "2026-01-27 22:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25bgby",
          "author": "FrozenBuffalo25",
          "text": "What‚Äôs your hardware?",
          "score": 1,
          "created_utc": "2026-01-28 03:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264vwh",
          "author": "DressMetal",
          "text": "Do you need a local model? Otherwise, Gemini 2.5 flash lite is great at this and it costs next to nothing.",
          "score": 1,
          "created_utc": "2026-01-28 06:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jawc",
          "author": "Ch3mCat",
          "text": "Colqwen 2.5 ?",
          "score": 1,
          "created_utc": "2026-01-28 13:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vumt",
          "author": "Independent-Cost-971",
          "text": "Try using Kudra AI it is very good at complex doc extraction you can even pick what you want to extract",
          "score": 1,
          "created_utc": "2026-01-28 14:24:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsbnr",
          "author": "nnamfuak",
          "text": "Mistral OCR 3 (mistral-ocr-2512)",
          "score": 1,
          "created_utc": "2026-01-29 01:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d5rz5",
          "author": "Whole-Assignment6240",
          "text": "recently i got recommended on mineru. haven't tried yet.",
          "score": 1,
          "created_utc": "2026-01-29 06:11:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e27ll",
          "author": "patbhakta",
          "text": "OCR has been around for a very long time, way before LLM and AI. What exactly are you extracting? I use 3-4 models at any given time but measuring accuracy they all will fail.",
          "score": 1,
          "created_utc": "2026-01-29 11:00:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2ijm4a",
          "author": "Infamous_Ad5702",
          "text": "I made my own tool. Handles PDF‚Äôs well, called it Leonata. Inside I use Apache Tika to handle file conversion. I‚Äôll check what is doing the OCR but it works great.\n\nNo tokens\nNo hallucination \nNo GPU",
          "score": 1,
          "created_utc": "2026-01-30 00:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2295sc",
          "author": "Fresh_Refuse_4987",
          "text": "You can check Reseek. It's an AI tool that automatically extract text from images and PDFs for your RAG pipeline, and uses semantic search so you can find your information easily.",
          "score": 0,
          "created_utc": "2026-01-27 18:26:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq1hly",
      "title": "PDFstract now supports chunking inspection & evaluation for RAG document pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "author": "GritSar",
      "created_utc": "2026-01-29 06:55:11",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.93,
      "text": "I‚Äôve been experimenting with different chunking strategies for RAG pipelines, and one pain point I kept hitting was **not knowing whether a chosen strategy actually makes sense for a given document** before moving on to embeddings and indexing.\n\n\n\nSo I added a **chunking inspection & evaluation feature** to an open-source tool I‚Äôm building called **PDFstract**.\n\n\n\nHow it works:\n\n* You **choose a chunking strategy**\n* PDFstract applies it to your document\n* You can **inspect chunk boundaries, sizes, overlap, and structure**\n* Decide if it fits your use case *before* you spend time and tokens on embeddings\n\n\n\nIt sits as the **first layer in the pipeline**:\n\nExtract ‚Üí Chunk ‚Üí (Embedding coming next)\n\n\n\nI‚Äôm curious how others here validate chunking today:\n\n* Do you tune based on document structure?\n* Or rely on downstream retrieval metrics?\n\nWould love to hear what‚Äôs actually worked in production.\n\nRepo if anyone wants to try it:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qmoxj2",
      "title": "Looking for RAG Engineer / AI Partner ‚Äî Real Estate + SMB Automation (Paid Contract, Long-Term Potential)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "author": "TheGloomWalker",
      "created_utc": "2026-01-25 17:12:28",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Hey everyone, I‚Äôm building a small AI services company focused on deploying custom RAG-based systems and internal AI tools for small and mid-sized businesses (starting with real estate automation and an industrial services company).\n\nI currently have infrastructure running (servers, cloud resources, deployment environment) and guaranteed business interest, but I‚Äôm looking to bring on a technical partner or contractor who can help design and implement production-grade RAG pipelines.\n\nWhat I‚Äôm building initially:\n\nReal estate automation use cases:\n\n\t‚Ä¢\tScraping + ingesting foreclosure / distressed property listings\n\n\t‚Ä¢\tStructured document ingestion (county data, CSVs, PDFs)\n\n\t‚Ä¢\tSearch + semantic querying over listings and owner data\n\n\t‚Ä¢\tEmail / outreach workflow integration (CRM-style pipelines)\n\nEnterprise pilot project (industrial company):\n\n\t‚Ä¢\tInternal document RAG (finance, operations, SOPs, contracts)\n\n\t‚Ä¢\tSecure knowledge base assistant for staff\n\n\t‚Ä¢\tRole-based access control\n\n\t‚Ä¢\tData isolation + security-first architecture\n\nThis company i‚Äôm working initial contract with is real (50m valuation), has active operations, and is willing to deploy AI internally across departments. Their IT team is security-focused, so experience with data isolation, permissioning, private vector DBs, and secure API practices is important.\n\nWhat I‚Äôm looking for:\n\nSomeone with experience in:\n\n\t‚Ä¢\tRAG pipelines (LangChain, LlamaIndex, custom pipelines, etc.)\n\n\t‚Ä¢\tVector DBs (Pinecone, Weaviate, Qdrant, FAISS, Chroma)\n\n\t‚Ä¢\tEmbeddings + chunking strategies\n\n\t‚Ä¢\tAPI integration\n\n\t‚Ä¢\tAuth / security best practices\n\n\t‚Ä¢\tCloud deployment (Docker, VPS, AWS/GCP/Hetzner/etc.)\n\n\t‚Ä¢\tBonus: web scraping + ETL pipelines\n\nCompensation:\n\n\t‚Ä¢\tPaid contract work (budget available)\n\n\t‚Ä¢\tOpportunity for ongoing partnership if things go well\n\n\t‚Ä¢\tOpen to milestone-based payments\n\nWhat I‚Äôd like to see from you:\n\n\t‚Ä¢\tBrief background / experience\n\n\t‚Ä¢\tAny demos, repos, or projects you‚Äôve built\n\n\t‚Ä¢\tWhat stack you prefer working with\n\n\t‚Ä¢\tAvailability\n\nIf you‚Äôre interested, DM me or reply here and we can talk details.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1nnecp",
          "author": "DeadPukka",
          "text": "Have a look at [Graphlit](https://www.graphlit.com). We do this as a platform and can help with custom services. \n\nNo need to build this yourself in 2026.",
          "score": 2,
          "created_utc": "2026-01-25 17:49:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqnhe",
          "author": "ampancha",
          "text": "The enterprise pilot is where this gets interesting. Role-based access control in RAG isn't a UI toggle; it has to happen at retrieval time, or users can still surface documents they shouldn't see through indirect queries. Their IT team will ask how you verify that isolation actually holds under adversarial prompts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-01-25 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1odwl7",
          "author": "pk13055",
          "text": "Sounds interesting [pk13055.com](https://pk13055.com)",
          "score": 1,
          "created_utc": "2026-01-25 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwvna",
          "author": "radicalpeaceandlove",
          "text": "I am an AI/ML engineer leaving Optum, would be open to chatting",
          "score": 1,
          "created_utc": "2026-01-26 02:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rnp1d",
          "author": "Fear_ltself",
          "text": "I‚Äôd love to give it a shot, I‚Äôve been working exactly with these programs for over 2 years and think we could get something up and running with what you‚Äôre looking for very quickly.",
          "score": 1,
          "created_utc": "2026-01-26 05:26:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqkah",
          "author": "BallinwithPaint",
          "text": "Hey, saw your post. This is right up my alley. I specialize in building the exact kind of autonomous RAG and automation pipelines you're describing. My experience includes projects involving live web scraping for data ingestion and building secure, production-grade agentic systems.\n\n\n\nSending you a DM with my portfolio and more details now.",
          "score": 0,
          "created_utc": "2026-01-25 18:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpfzko",
      "title": "Convert Charts & Tables to Knowledge Graphs in Minutes | Vision RAG Tuto...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "author": "BitterHouse8234",
      "created_utc": "2026-01-28 16:24:54",
      "score": 10,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "Struggling to extract data from complex charts and tables? Stop relying on broken OCR. In this video, I reveal how to use Vision-Native RAG to turn messy PDFs into structured Knowledge Graphs using Llama 3.2 Vision.  \n  \nTraditional RAG pipelines fail when they meet complex tables or charts. Optical Character Recognition (OCR) just produces a mess of text. Today, we are exploring VeritasGraph, a powerful new tool that uses Multimodal AI to \"see\" documents exactly like a human does.  \n  \nWe will walk through the entire pipeline: ingesting a financial report, bypassing OCR, extracting hierarchical data, and visualizing the connections in a stunning Knowledge Graph.  \n  \nüëá Resources & Code mentioned in this video: üîó GitHub Repo (VeritasGraph): [https://github.com/bibinprathap/VeritasGraph](https://github.com/bibinprathap/VeritasGraph)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qoyyjm",
      "title": "We built a knowledge graph from code using AST extractors. Now we're drowning in edge cases. Roast our approach.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "author": "TraditionalDegree333",
      "created_utc": "2026-01-28 02:39:08",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "I'm building a code intelligence platform that answers questions like¬†*\"who owns this service?\"*¬†and¬†*\"what breaks if I change this event format?\"*¬†across 30+ repos.\n\nOur approach: Parse code with tree-sitter AST ‚Üí Extract nodes and relationships ‚Üí Populate Neo4j knowledge graph ‚Üí Query with natural language.\n\nHow It Works:\n\n    Code File\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ tree-sitter AST parse\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Extractors (per file type):\n        ‚îÇ   ‚îú‚îÄ‚îÄ CodeNodeExtractor     ‚Üí File, Class, Function nodes\n        ‚îÇ   ‚îú‚îÄ‚îÄ CommitNodeExtractor   ‚Üí Commit, Person nodes + TOUCHED relationships  \n        ‚îÇ   ‚îú‚îÄ‚îÄ DiExtractor           ‚Üí Spring  ‚Üí INJECTS relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ MessageBrokerExtractor‚Üí Kafka listeners ‚Üí CONSUMES_FROM relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ HttpClientExtractor   ‚Üí RestTemplate calls ‚Üí CALLS_SERVICE\n        ‚îÇ   ‚îî‚îÄ‚îÄ ... 15+ more extractors\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Enrichers (add context):\n        ‚îÇ   ‚îú‚îÄ‚îÄ JavaSemanticEnricher  ‚Üí Classify: Service? Controller? Repository?\n        ‚îÇ   ‚îî‚îÄ‚îÄ ConfigPropertyEnricher‚Üí Link (\"${prop}\") to config files\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ Neo4j batch write (MERGE nodes + relationships)\n\n**The graph we build:**\n\n    (:Person)-[:TOUCHED]->(:Commit)-[:TOUCHED]->(:File)\n    (:File)-[:CONTAINS_CLASS]->(:Class)-[:HAS_METHOD]->(:Function)\n    (:Class)-[:INJECTS]->(:Class)\n    (:Class)-[:PUBLISHES_TO]->(:EventChannel)\n    (:Class)-[:CONSUMES_FROM]->(:EventChannel)\n    (:ConfigFile)-[:DEFINES_PROPERTY]->(:ConfigProperty)\n    (:File)-[:USES_PROPERTY]->(:ConfigProperty)\n\n  \n**The problem we're hitting:**\n\nEvery new framework or pattern = new extractor.\n\n* Customer uses Feign clients? Write FeignExtractor.\n* Uses AWS SQS instead of Kafka? Write SqsExtractor.\n* Uses custom DI framework? Write another extractor.\n* Spring Boot 2 vs 3 annotations differ? Handle both.\n\nWe have 40+ node types and 60+ relationship types now. Each extractor is imperative pattern-matching on AST nodes. It works, but:\n\n1. Maintenance nightmare¬†- Every framework version bump can break extractors\n2. Doesn't generalize¬†- Works for our POC customer, but what about the next customer with different stack?\n3. No semantic understanding¬†- We can extract¬†\\`@KafkaListener\\`but can't answer¬†\"what's our messaging strategy?\"\n\n**Questions:**\n\n1. Anyone built something similar and found a better abstraction?\n2. How do you handle cross-repo relationships? (Config in repo A, code in repo B, deployment values in repo C)\n\n\n\n  \nHappy to share more details or jump on a call. DMs open.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26gra0",
          "author": "Sure_Host_4255",
          "text": "That's what I was exactly was thinking about for spring projects, because mostly you don't need general knowledge, but framework specific. Maybe another step would be to create detailed skill or rule for agent how to use this graph, because for LLM it is just graph, it doesn't know what to do with it and still reads the files to context for it's tasks.\nAs for me even better results was when I wrote spring documentation MCP, then even weak models become stronger.\n\nAnother though, you don't need to support All frameworks, java specific would be enough for commercial product, just choose this niche and keep going, enterprise clients will pay for it.\n\nFor patterns you need to ask LLM to write breaf reviews for graph chains, it could cost 20-100$, depending on repo and model. Also write custom wights algorithms, but it's rather doubtful, because it could work great for 1 project and can harm for another.\nI was participating in similar project for Cobol, and can say you are in the right direction and have similar problems ‚ò∫Ô∏è",
          "score": 1,
          "created_utc": "2026-01-28 08:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26inxl",
              "author": "Sure_Host_4255",
              "text": "So for conclusion:\n1. Write spring docs MCP, it can fetch documentation from GitHub md docs for each version upgrade \n2. Focus just on java frameworks\n3. Ask LLM to create summary for nodes relationship chain",
              "score": 1,
              "created_utc": "2026-01-28 08:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26lewq",
          "author": "TraditionalDegree333",
          "text": "Thank you for the advice, I will explore",
          "score": 1,
          "created_utc": "2026-01-28 08:52:06",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o28aaig",
          "author": "devopstoday",
          "text": "Sounds good approach. Is your project opensource?",
          "score": 1,
          "created_utc": "2026-01-28 15:33:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28fvkv",
              "author": "TraditionalDegree333",
              "text": "No, it‚Äôs not",
              "score": 1,
              "created_utc": "2026-01-28 15:57:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2liwxe",
          "author": "sp3d2orbit",
          "text": "Yeah, I spent years on this problem. You have a nice setup, and this is a valuable problem to solve.\n\n\nI ended up converging on a solution. I don't use standard graph databases anymore. I created a new type of graph database that can store the AST as well as the extracted rules together in one unified format. This helps the inference engine navigate back and forth between code and rules easily. It even helps with abstraction patterns.¬†\n\n\nFor me the atomic unit is a prototype. And a graph of prototypes is called a prototype graph.\n\n\nThen instead of using extractors, I created a general object to graph framework with circular dependency detection and resolution. That lets me call ToPrototype on any object and have it serialized to a prototype graph which can be stored inside the graph database along with the rules.¬†\n\n\nOne of the cool things of this approach is that the inference in learning engines can utilize graphs that contain natural language or code or data all in the same graph structure.",
          "score": 1,
          "created_utc": "2026-01-30 13:17:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlmttf",
      "title": "built a Local RAG System That Works Without API Keys - Is This Actually Useful?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qlmttf/built_a_local_rag_system_that_works_without_api/",
      "author": "DetectiveMindless652",
      "created_utc": "2026-01-24 13:21:25",
      "score": 9,
      "num_comments": 14,
      "upvote_ratio": 0.8,
      "text": "Spent a couple days putting together¬†this local RAG SDK¬†thing. Basically lets you do document search and context retrieval entirely offline, no cloud APIs required by default.\n\nIt¬†uses local embeddings so you don't have to pay OpenAI or Cohere, and it's built on this fast storage engine that does O(1) lookups. Performance seems decent - about 5x faster¬†than what I was getting with cloud alternatives when I tested it.\n\nCompared to cloud services like Pinecone that charge $70-200/month, this is completely¬†free. Plus¬†you¬†get better¬†speed since¬†everything runs locally¬†and your data never¬†leaves¬†your machine. + no cloud cost \n\nThe main issue I was¬†trying to solve is that¬†if you want privacy with your RAG setup, you usually end up with something¬†slow or complicated. This keeps everything local¬†but still performs pretty well.\n\nWould people¬†actually use something like this? I feel¬†like most RAG tutorials just assume¬†you'll use Pinecone + OpenAI, but¬†maybe there's demand for a local option¬†that doesn't suck.\n\nDownsides: local¬†embeddings aren't as good as the latest¬†OpenAI models, and it depends¬†on your hardware. But no monthly bills and your data¬†stays private.\n\nWhat do you think¬†- overkill or actually useful? Have you¬†run into the privacy vs¬†performance trade-off with RAG?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qlmttf/built_a_local_rag_system_that_works_without_api/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1fzyqo",
          "author": "hrishikamath",
          "text": "I use Postgres with pg vector, it takes milli seconds for retrieval over 100k documents. I spend like a handful dollars in a deployed version. It‚Äôs simple. You don‚Äôt need pinecone unless you have scale.",
          "score": 5,
          "created_utc": "2026-01-24 15:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k6mjx",
          "author": "voycey",
          "text": "Anyone can do it, it's literally how most RAG systems work, what you have to do is differentiate, having basic retrieval gives you no edge, vector search has been around much longer than the current AI hype, it's not a new concept.\n\nHell all the hyperscalers offer this out of the box, why would someone choose to use your system over that? \n\nI think the future is in private AI solutions but they have to be different",
          "score": 3,
          "created_utc": "2026-01-25 04:21:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kdnob",
              "author": "No-Consequence-1779",
              "text": "Can you list some downloadable consumer products please?¬†",
              "score": 1,
              "created_utc": "2026-01-25 05:05:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sf451",
              "author": "DetectiveMindless652",
              "text": "i see what you are saying, but in theory what we have built is different because it is not vector based, predictable performance, memory efficient works beyond ram, local, and acid,wal crash recovery etc.\n\nWould be cool to get your thoughts on that",
              "score": 0,
              "created_utc": "2026-01-26 09:11:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1zg74n",
                  "author": "voycey",
                  "text": "Any RAG system that doesnt in some way use Vectors or provide similar context so that it can be fetched by the LLM isnt going to perform well.  \n  \nTake Langextract by Google as an example, it essentially does away with vectors but instead you have to provide an example for NER, it does a great job of locating answers in the corpus where what you are searching for matches whats in the document but ultimately you are just pushing the pipeline to a different part, in production this puts much more strain on the query segment of Retrieval vs the pre-processing that RAG does.\n\nI could go into a bunch of different reasons why LangExtract isnt a good solution by itself for enterprise retrieval - but I will also say its an incredible product that when used in tandem with other methods can significantly increase accuracy and / or speed.\n\nhttps://preview.redd.it/des5fhkmuufg1.png?width=2484&format=png&auto=webp&s=92afd14d45037ea81ee8f6e68b43fc495dd4aa85\n\nOverall I think it's a good idea to go ahead and build - it sounds like you are focusing on robustness too (incredibly important as most of the options out there are incredibly brittle) but you really must get some production data. The \"non-vector\" retrieval options (Langextract, PageIndex etc), right now frequently fail on production data (complex tables are a good example of something thats still a very hard problem and something I have spent a non-trivial amount of time solving for my system).",
                  "score": 1,
                  "created_utc": "2026-01-27 08:49:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fma0m",
          "author": "Pale_Reputation_511",
          "text": "I've been working on this and refining it for the last two months. I built it from scratch, without a framework or anything. I want it to be simple and without external dependencies, if possible, just what's necessary for the job. It's a tailor-made solution for the things I do; the process has been difficult, but I've learned a lot.",
          "score": 2,
          "created_utc": "2026-01-24 14:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h1sv7",
              "author": "DetectiveMindless652",
              "text": "We‚Äôve built this bro production ready dm me",
              "score": -2,
              "created_utc": "2026-01-24 18:46:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1j7jsl",
          "author": "burntoutdev8291",
          "text": "Jina and bge is usually good enough. We use this for airgapped environments, so everything is local.",
          "score": 2,
          "created_utc": "2026-01-25 01:02:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yvfft",
          "author": "SkyFeistyLlama8",
          "text": "Don't reinvent the wheel. I've made local RAG setups using local embedding models and local LLMs, Postgres with pgvector or a big CSV file for vector and keyword search. Vector search isn't the bottleneck! You're limited by the speed of creating and maintaining vector indexes (it can take days to index a million chunks) and by local LLM inference.",
          "score": 1,
          "created_utc": "2026-01-27 05:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jk5no",
          "author": "wurzelbrunft",
          "text": "It's certainly useful if you don't want to upload private data to an AI. I'm currently developing such a concept myself.",
          "score": 0,
          "created_utc": "2026-01-25 02:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sf51u",
              "author": "DetectiveMindless652",
              "text": "would be cool to hear about it.",
              "score": 2,
              "created_utc": "2026-01-26 09:12:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qomak1",
      "title": "Chunking strategies for contracts",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "author": "cat47b",
      "created_utc": "2026-01-27 18:34:39",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all, after seeing a few posts I'm curious as to if anyone has a tried and true favourite method for chunking.\n\nIf I set the scene as we're making an agentic RAG for contracts, ingesting with OCR, carrying out hybrid search (vector+BM25) and a re-ranker. My main concern is the ingestion/chunking process (garbage in, garbage out).\n\nLooking at OCR results I can see that a paragraph may have 1 line that lands on the next page, or that a bullet list goes across 2 pages or that structurally few headings are used.\n\nFor no good reason semantic chunking has stood out as something that makes logical sense as I just want chunks that in isolation make sense, and hopefully side-step the need for overlap etc. \n\nOne downside to this I can imagine however is that a chunk can be semantically complete, but could otherwise refer to an exception clause. Without both you could end up with poor context to feed an LLM when answering a user query.\n\n  \nSo back to the original question, for the stated use case does anyone have a pattern they would use here?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o234f9p",
          "author": "ManufacturerIll6406",
          "text": "I just published an article today on this subject, also included lightweight framework for evaluating various strategies with statistical rigor\n\n[https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)",
          "score": 5,
          "created_utc": "2026-01-27 20:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23j8hk",
              "author": "cat47b",
              "text": "Your post was excellent and I did read it, would I be right in saying if you had to pick one it'd be sentence based chunking?\n\nIn a sense I don't really care about chunk size as accuracy is more important to me",
              "score": 1,
              "created_utc": "2026-01-27 21:50:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o23lt8e",
                  "author": "ManufacturerIll6406",
                  "text": "Thanks! And yes. If you just want max recall and don't care about chunk size, sentence chunking is a safe default. It naturally produces larger chunks, which is exactly why it wins.\n\nThe caveat is cost and latency. Sentence chunking at 1024 config gives you \\~3500 char chunks. At top-k=5, that's 17,500 chars in your LLM context before you've added the prompt. If that's fine for your use case, go for it.\n\nTL;DR: Sentence chunking works. Now you know \\*why\\* it works.",
                  "score": 1,
                  "created_utc": "2026-01-27 22:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o292824",
          "author": "ampancha",
          "text": "Chunking matters, but with an agentic RAG the higher-order risk is what happens when the agent acts on a hallucinated clause or when contract text contains embedded instructions that hijack the workflow. Contracts are untrusted input at scale. Before optimizing retrieval, I'd map the action boundary: what can this agent actually do, and what stops it from doing something wrong? Sent you a DM.",
          "score": 1,
          "created_utc": "2026-01-28 17:34:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qleohy",
      "title": "Data Mining Contract PDF",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qleohy/data_mining_contract_pdf/",
      "author": "bboysathish",
      "created_utc": "2026-01-24 05:40:24",
      "score": 8,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm hitting a wall with a project that feels like it should be a breeze, but the \"vibe coding\" is leading me into a regex nightmare.\n\n‚ÄãThe Setup\n\n‚ÄãI‚Äôm building a fully local system to process contracts from fulfillment companies. These docs are a mess: standard text, complex tables, and image-based PDFs that need OCR.\n\n‚ÄãMy Goals\n\n‚ÄãRAG: Convert PDFs to Markdown, perform PII cleanup (Presidio), strip out the legal fluff, and feed \"gold-level\" data into the vector store.\n‚ÄãData Mining: Extract specific rates and charges from both tables and prose for each contract.\n\n‚ÄãWhat I‚Äôve Tried\n\n‚ÄãParsing: Docling + Marker.\n‚ÄãPII: Microsoft Presidio.\n‚ÄãRAG: LlamaIndex Semantic Parser (still missing context).\n\n‚ÄãExtraction: Local Llama Vision models (painfully slow, often hangs/fails on 30-page files).\n\n‚ÄãThe Problem\n\n‚ÄãEven with Markdown as the \"gold standard,\" the tables are often parsed incorrectly, or the RAG loses the context of which rate applies to which service.\n\n‚ÄãMore importantly, when I try to automate this, the AI keeps generating document-specific code. It‚Äôs full of hardcoded regex and logic that only works for that specific vendor's layout. It won't scale.\n‚ÄãMy Questions\n\n1. ‚ÄãScalability: I see people talking about RAG for 100k+ documents. How? If I'm struggling with 15 docs of 30 pages each, what am I missing?\n\n2. ‚ÄãGeneralization: How do you handle table extraction and RAG context without writing custom logic for every new PDF layout?\n\n3. ‚ÄãLocal Vision: Is there a trick to getting local VLMs to handle 30-page PDFs without taking an eternity, or should I be chunking images differently?\n\n‚ÄãI want a generic pipeline, not a collection of 15 different scripts. Any advice on architectural shifts or better local-first tools?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qleohy/data_mining_contract_pdf/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1fxy9w",
          "author": "Popular_Sand2773",
          "text": "I am not going to sugar coat this what you want is hard. Most people are able to scale and generalize because they compromise somewhere like not using local models or maintaining a bank of scripts. That said it can be done.\n\nIf you really want to do this right and keep it local the first thing you need to be doing is tagging the document. Otherwise known as image pre-processing + layout detection. Before you even parse anything use a vision model to draw boxes. At a high level those boxes should be table, useful prose, useless legalese. Once you have that you can 1 cut the crap and 2 use the right tool for each job. \n\nNow you have categorized sections of pdf. The tables go through a model designed to interpret tables. Can be a slow local llama or a more standard table tuned OCR. The point is it is something that doesn't screw up tables and doesn't care about prose. Then the regular valuable prose can go through something designed for it. Each piece has a tool appropriately powered and designed for it. \n\nMake sure you track provenance because you decomposed: so which page which document etc. Now you have reasonably parsed data structured in a way that is useable for downstream processes. There is no something for nothing. To get what you want you need to first understand the layout then use the right tool for the job. There are pre-existing tools out there if you don't care about the useful vs useless prose filtering before OCR. \n\nHigh-level: You need to decompose with layout detection.",
          "score": 1,
          "created_utc": "2026-01-24 15:50:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1g95wl",
          "author": "Far_Statistician1479",
          "text": "Are you just tossing the entire 30 page doc into the vlm at once and wondering why it‚Äôs not working?",
          "score": 1,
          "created_utc": "2026-01-24 16:41:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1gi2ko",
              "author": "bboysathish",
              "text": "No page by page, that too splitting top/bottom section for each page.",
              "score": 1,
              "created_utc": "2026-01-24 17:21:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gk9y9",
                  "author": "Far_Statistician1479",
                  "text": "Then how does the page count make any difference whatsoever?",
                  "score": 1,
                  "created_utc": "2026-01-24 17:30:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gh14u",
          "author": "teroknor92",
          "text": "using local tools for extracting data from complex documents with tables need VLM which you can already using. For speed you can serve the VLM using vLLM but you should have enough free GPU RAM to run with vLLM. Otherwise you can look at external API solutions which do not store your data like ParseExtract, Llamaparse.",
          "score": 1,
          "created_utc": "2026-01-24 17:16:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l2ijf",
          "author": "ampancha",
          "text": "You nailed the core issue: AI-generated extraction code optimizes for the document in front of it, not the schema you actually need. The fix is inverting the approach. Define a contract-agnostic output schema first (service types, rate structures, effective dates), then use structured extraction with validation rather than regex. Tables become reliable when you treat them as data sources against a known schema, not text to parse. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-01-25 08:18:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e000o",
          "author": "OnyxProyectoUno",
          "text": "The regex nightmare you're describing is exactly what happens when you try to solve parsing problems at the wrong layer. You're fighting symptoms instead of fixing the root cause.\n\nYour core issue isn't the AI generating document-specific code. It's that your parsing pipeline is already broken before the AI even sees it. When Docling and Marker mangle those tables, you're asking your models to extract structured data from garbage. Even perfect extraction logic can't fix information that was lost during parsing.\n\nHere's what you need to test: grab one of those 30-page contracts and run it through your current pipeline. Look at the actual markdown output. I guarantee you'll find tables split across chunks, headers separated from their content, and rate information scattered in ways that make context impossible to maintain.\n\nThe scalability problem you're seeing isn't about document volume. It's about data quality. Teams processing 100k+ documents have solved the parsing layer first. They're not debugging extraction failures because their documents come out clean from the start.\n\nFor your contract processing, try switching to a table-aware chunking strategy that keeps related pricing information together. Standard recursive chunking will slice right through your rate tables. You also need to preserve document structure so the RAG system knows which rates belong to which services.\n\nI've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this kind of pipeline configuration problem. Instead of writing custom scripts for each layout, you configure the entire processing pipeline through conversation and see exactly what your documents look like after each transformation.\n\nWhat does your parsed output actually look like right now? Can you spot where the context is getting lost?",
          "score": 0,
          "created_utc": "2026-01-24 07:17:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qqfxko",
      "title": "Tried to Build a Personal AI Memory that Actually Remembers - Need Your Help!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "author": "Tough-Percentage-864",
      "created_utc": "2026-01-29 18:01:20",
      "score": 8,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "Hey everyone, I was inspired by the **Shark Tank NeoSapien concept**, so I built my own **Eternal Memory system** that doesn‚Äôt just store data - it *evolves with time*.([LinkedIn](https://www.linkedin.com/posts/abhaygupta53_ai-python-eternal-activity-7422690736359415808-PjWd?rcm=ACoAADg6WLoBZEyh7uuClgIZx3hLSD1IzZb81Nc&utm_medium=member_desktop&utm_source=share))\n\nRight now it can:  \n\\-Transcribe audio + remember context  \n\\-  Create **Daily / Weekly / Monthly summaries**  \n\\- Maintain short-term memory that fades into long-term  \n\\- Run **semantic + keyword search** over your entire history\n\nI‚Äôm also working on **GraphRAG for relationship mapping** and **speaker identification** so it knows *who said what*.\n\nI‚Äôm looking for **high-quality conversational / life-log / audio datasets** to stress-test the memory evolution logic.  \n**Does anyone have suggestions? Or example datasets (even just in DataFrame form) I could try?**\n\nExamples of questions I want to answer with a dataset:\n\n* ‚ÄúWhat did I do in **Feb 2024?**‚Äù\n* ‚ÄúWhy was I sad in **March 2024?**‚Äù\n* Anything where a system can actually recall patterns or context over time.\n\nDrop links, dataset names, or even Pandas DataFrame ideas anything helps! üôå\n\n  \n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2g9zjj",
          "author": "Tough-Percentage-864",
          "text": "Repo link --> [https://github.com/Abhay-404/Eternal-Memory](https://github.com/Abhay-404/Eternal-Memory)",
          "score": 2,
          "created_utc": "2026-01-29 18:01:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2iiim8",
          "author": "DetectiveMindless652",
          "text": "Dm me",
          "score": 1,
          "created_utc": "2026-01-30 00:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kqo5a",
              "author": "Tough-Percentage-864",
              "text": "ü§î",
              "score": 1,
              "created_utc": "2026-01-30 09:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ks73z",
                  "author": "DetectiveMindless652",
                  "text": "We‚Äôre working on something adjacent mate",
                  "score": 2,
                  "created_utc": "2026-01-30 09:57:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp7psa",
      "title": "Compared hallucination detection for RAG: LLM judges vs NLI",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "author": "meedameeda",
      "created_utc": "2026-01-28 10:25:07",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.9,
      "text": "I looked into different ways to detect hallucinations in RAG. Compared LLM judges, atomic claim verification, and encoder-based NLI.\n\nSome findings:\n\n* LLM judge: 100% accuracy, \\~1.3s latency\n* Atomic claim verification: 100% recall, \\~10.7s latency\n* Encoder-based NLI: \\~91% accuracy, \\~486ms latency (CPU-only)\n\nFor real-time systems, NLI seems like the most reasonable trade-off. \n\nWhat has been your experience with this?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26vl7l",
          "author": "meedameeda",
          "text": "full write up here if interesting: [https://agentset.ai/blog/how-to-detect-hallucinations-in-rag](https://agentset.ai/blog/how-to-detect-hallucinations-in-rag)",
          "score": 5,
          "created_utc": "2026-01-28 10:25:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2d30ki",
              "author": "aiprod",
              "text": "Reading the comment now and seeing you used RAGTruth. It‚Äôs a poor dataset full of errors. Try our modified version linked in my other comment.",
              "score": 1,
              "created_utc": "2026-01-29 05:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2785zr",
          "author": "Upset-Pop1136",
          "text": "we put nli as a fast filter, async llm judge on low-confidence hits, and cache verdicts per doc passage. reduced latency and cost by 70% while keeping recall. try thresholding confidence before invoking expensive checks.",
          "score": 2,
          "created_utc": "2026-01-28 12:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2hvp",
          "author": "aiprod",
          "text": "We tested NLI based detectors like azure groundedness on our Ragtruth++ dataset (https://www.blueguardrails.com/en/blog/ragtruth-plus-plus-enhanced-hallucination-detection-benchmark). And the results were very different. More like 0.35 f1 score.\n\nOur own hallucination detection (agentic verification) scores around 0.8 f1 on the same dataset.\n\nI think your high scores are an indication of a poor quality dataset or some mistakes in the benchmark setup.\n\nHere‚Äôs a video with some numbers for azure and a comparable approach to NLI from scratch (both at 0.35 - 0.45 f1): https://www.blueguardrails.com/en/videos/ragtruth-plus-plus-benchmark-creation",
          "score": 2,
          "created_utc": "2026-01-29 05:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27dxpi",
          "author": "youre__",
          "text": "Seems to have potential if tested for production and applied to certain applications (e.g., where information correctness is a nice-to-have, not a critical requirement).\n\nFrom the test, anything 100% seems fishy. How many samples and what are the error bars after running same test with different seeds? There‚Äôs a ‚Äú66.7%‚Äù precision number in the article, which is oddly clean (2/3), too. Was there a test/validation split with the dataset?\n\nFor hardware testing and comparison, the laptop vs gpt-5 is an interesting comparison. Network latency will be a factor as well as thinking level. So a good test might be to test the NLI over the network, even if on a cloudflare tunnel to simulate cloud. Also test thinking/non-thinking variants of smaller cloud models. This way you can see where the cutoff in performance is. E.g., Can gpt-4o-mini perform just as well as gpt-5 on the dataset? And/Or maybe another cloud hallucination detector?\n\nThis might help ground the comparison and highlight the true benefits against systems people are already using.",
          "score": 1,
          "created_utc": "2026-01-28 12:45:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o290pyn",
          "author": "Financial-Bank2756",
          "text": "Interesting breakdown. These are all post-generation detection, catching hallucinations after the model outputs them. I've been exploring the other side: pre-generation constraint with my project Acatalepsy. which uses\n\n* VIN (Vector Identification Number) ‚Äî constraint operators, not labels\n* ACU (Atomic Claim Unit) ‚Äî immutable identity, mutable confidence\n* Pulse-VIN cycle ‚Äî emission ‚Üí coalescence ‚Üí interrogation ‚Üí sedimentation\n* Confidence vectors ‚Äî multi-axis, decaying, never absolute\n\nhope this helps",
          "score": 1,
          "created_utc": "2026-01-28 17:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eb2pn",
          "author": "Charming_Group_2950",
          "text": "Try TrustifAI. It provides a trust score along with explanations for LLM responses. Explore here:¬†[https://github.com/Aaryanverma/trustifai](https://github.com/Aaryanverma/trustifai)",
          "score": 1,
          "created_utc": "2026-01-29 12:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wedu",
          "author": "ThrowAway516536",
          "text": "I'd take 100% accuracy any day. If you prefer 91% accuracy, I reckon your product and data, where the LLM is integrated, aren't worth much.",
          "score": 1,
          "created_utc": "2026-01-28 10:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26ydem",
              "author": "meedameeda",
              "text": "if latency and cost don‚Äôt matter, 100% accuracy is obviously the right choice (but also depending on your type of production)",
              "score": 2,
              "created_utc": "2026-01-28 10:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}