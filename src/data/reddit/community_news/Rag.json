{
  "metadata": {
    "last_updated": "2026-01-21 02:39:59",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 138,
    "file_size_bytes": 195053
  },
  "items": [
    {
      "id": "1qd2e2b",
      "title": "RAG at scale still underperforming for large policy/legal docs ‚Äì what actually works in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-01-14 23:00:50",
      "score": 58,
      "num_comments": 34,
      "upvote_ratio": 0.95,
      "text": "I‚Äôm running RAG fairly strong on-prem setup, but quality still degrades badly with large policy / regulatory documents and multi-document corpora. Looking for practical architectural advice, not beginner tips.\n\nCurrent stack:\n-Open WebUI (self-hosted)\n-Docling for parsing (structured output)\n-Token-based chunking\n-bge-m3 embeddings\n-bge-m3-v2 reranker\n-Milvus (COSINE + HNSW)\n-Hybrid retrieval (BM25 + vector)\n-LLM: gpt-oss-20B\n-Context window: 64k\n-Corpus: large policy / legal docs, 20+ documents\n-Infra: RTX 6000 ADA 48GB, 256GB DDR5 ECC\n\nObserved issues:\nCross-section and cross-document reasoning is weak\nIncreasing context window doesn‚Äôt materially help\nReranking helps slightly but doesn‚Äôt fix missed clauses\nWorks ‚Äúokay‚Äù for academic projects, but not enterprise-grade\n\nI‚Äôm thinking of trying:\nGraph RAG (Neo4j for clause/definition relationships)\nAgentic RAG (controlled, not free-form agents)\n\nQuestions for people running this in production:\nHave you moved beyond flat chunk-based retrieval in Open WebUI? If yes, how?\nHow are you handling definitions, exceptions, overrides in policy docs?\nDoes Graph RAG actually improve answer correctness, or mainly traceability?\nAny proven patterns for RAG specifically (pipelines, filters, custom retrievers)?\nAt what point did you stop relying purely on embeddings?\n\nI‚Äôm starting to feel that naive RAG has hit a ceiling, and the remaining gains are in retrieval logic, structure, and constraints‚Äînot models or hardware.\nWould really appreciate insights from anyone who has pushed RAG system beyond demos into real-world, compliance-heavy use cases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzqkpdf",
          "author": "Soft-Speaker6195",
          "text": "You‚Äôre not wrong - flat chunk RAG hits a wall fast on policy/legal corpora because ‚Äúcorrectness‚Äù is mostly about scope, definitions, exceptions, and precedence, not semantic similarity. The teams I‚Äôve seen get this working stop treating retrieval as ‚Äútop-k chunks‚Äù and start treating it as a constrained legal reasoning pipeline: definition expansion > scope filters > precedence rules > targeted clause fetch > answer with mandatory citations. Graph adds value when it‚Äôs used for precedence + definition binding, not as a generic knowledge graph. If you want a concrete example of how this looks in practice (especially definition/exception handling + audit-friendly outputs), AI Lawyer has some useful patterns you can mirror: strict citation gating, clause-level retrieval, and override/exception tracing.",
          "score": 14,
          "created_utc": "2026-01-15 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmphjq",
          "author": "Chemical_Orange_8963",
          "text": "Basically you are making GLEAN, research more on that on how it works",
          "score": 11,
          "created_utc": "2026-01-14 23:06:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolbyp",
          "author": "OnyxProyectoUno",
          "text": "Yeah, you've hit the ceiling that most people hit. Token-based chunking on legal docs is basically guaranteed to break cross-reference reasoning because it has no awareness of document structure.\n\nThe issue isn't your retrieval stack, it's what you're feeding it. Legal docs have explicit hierarchical relationships: definitions that apply to specific sections, exceptions that modify clauses, cross-references that span documents. Flat chunks destroy all of that before your embeddings ever see it. Doesn't matter how good your reranker is if the chunk boundaries cut through a definition-to-usage relationship.\n\nGraph RAG can help with traceability but it won't fix the upstream problem. You're still building the graph from chunks that already lost the structure. Same with agentic approaches, they're working with degraded inputs.\n\nWhat's actually worked in production for policy docs: semantic chunking that respects section boundaries, explicit metadata for document hierarchy (section > subsection > clause), and preserving cross-reference relationships as first-class data. You want chunks that know what section they belong to and what other sections they reference.\n\nDocling gives you structured output but are you actually using that structure for chunking decisions? Most people parse structured and then chunk flat anyway. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this kind of pipeline configuration, where you can see how your docs look after each transformation before committing.\n\nFor the cross-document reasoning specifically, you probably need document-level metadata propagation so chunks know which policy they came from and what other policies they relate to. That's not a retrieval problem, it's an enrichment problem that happens way before Milvus sees anything.",
          "score": 7,
          "created_utc": "2026-01-15 05:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrtoxv",
              "author": "stevevaius",
              "text": "Vectorflow still not available to test. Waiting for access",
              "score": 1,
              "created_utc": "2026-01-15 18:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrxwzu",
                  "author": "OnyxProyectoUno",
                  "text": "Apologies, the next limited early access will be in the very near future. You should receive an email a week or two before launch. \n\nIn the interim, is there something we can help you with, perhaps related to your setup or feature requests/pain points in your current setup, that you would like us to take into account as we refine the VectorFlow experience?",
                  "score": 2,
                  "created_utc": "2026-01-15 18:42:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00dcmg",
              "author": "bigshit123",
              "text": "Can you explain how you got structured output from docling? I‚Äôm parsing to markdown but docling seems to make every title a second-level heading (##).",
              "score": 1,
              "created_utc": "2026-01-16 22:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o026cwg",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, the default markdown export flattens everything to h2. You want to use the JSON output instead, which preserves the actual document structure. When you call docling, set the output format to JSON and you'll get the hierarchical layout with proper nesting levels, section types, and element relationships.\n\nThe JSON gives you stuff like section numbers, whether something is a definition block, table structure, footnote relationships. That's what you actually want to use for chunking decisions. I chunk based on the JSON structure first, then convert relevant parts to markdown only for the final chunk content. Most people do it backwards and lose all the structural info that docling worked to extract.",
                  "score": 2,
                  "created_utc": "2026-01-17 05:45:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzn3o33",
          "author": "ggone20",
          "text": "Graphs are the answer. You need to rethink chunking strategies though - semantic and section based chunking. All the questions you have are indeed things you need to work through. \n\nYou can (and should) approach retrieval from both ends: start with keyword and vector search, use the findings to traverse graph relationships - you‚Äôll need agentic search here so it can intelligently ‚Äòloop‚Äô and refine queries as needed. You can come from the graph side as well if you know certain nodes on specific edges you‚Äôre looking for to filter (keep an index). \n\nHave your agent use a ‚Äòscratchpad‚Äô during search and keep each search branch‚Äôs context clean and focused - what I‚Äôve found so far, what information I still need, search terms used).  There are a hundred more things but yea‚Ä¶\n\nI built an engineering assistant for the hydrogen mobility industry (think hydrogen fueling stations and generation facilities) that is used to validate plans and even day to day work packages against required standards, protocols, and regulations. It provides detailed report with citations of ‚Äòwhy‚Äô for go/no-go decisions.  So yes, to answer your final questions, this is the way. Prompting and intelligent search is more art than science though. Expanding user queries, prompting the user intelligently for search clarity, cacheing and vectorizing queries to store them with ‚Äòwhat worked‚Äô provenance so the agent can find like or similar answers later. \n\nA simple ranking system that users can ‚Äòthumbs up/down‚Äô responses (and potentially provide feedback) helps a lot for refining the system down the road. Good luck!",
          "score": 7,
          "created_utc": "2026-01-15 00:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzov0wj",
              "author": "cisspstupid",
              "text": "I agree to this approach. I'm myself starting to look into building knowledge graph and how to use them. If u/ggone20 have any good references for learning or tips. Those will be highly appreciated.",
              "score": 2,
              "created_utc": "2026-01-15 07:19:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpcwac",
          "author": "TechnicalGeologist99",
          "text": "Legal documents are not really semantic. \n\nThe semantics of the text help get us in the correct postcode...but it doesn't help us to reason or extract full threads of information. \n\nThis is because legal documents are actually hiding a great deal of latent structure. \n\nThis is why people use knowledge graphs for high stakes documents like this. \n\nYou need to hire someone with research expertise in processing legal text. \n\nBuilding a useful knowledge graph is very difficult.\n\nAnyone who says otherwise is a hype gremlin that's never had to evaluate something with genuinely high risk outputs.\n\nYou should also be aware that KGs usually run in memory and are  memory hungry. This will be a major consideration for deployment. Either you already own lots of RAM (you lucky boy) or you're about to find out how much AWS charge per GB",
          "score": 6,
          "created_utc": "2026-01-15 10:10:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4etw",
          "author": "FormalAd7367",
          "text": "in my experiences, law/statues/policy doc are hardest because there are so many variables that agentic ai can‚Äôt read like sec 7(x)(78) of law -> sec 7(x7(8) of law\n\nthen there are tables and long winded text.. and law that was superseded by another law like 59 times",
          "score": 3,
          "created_utc": "2026-01-15 00:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmrcb1",
          "author": "DeadPukka",
          "text": "Any links to the type of docs you‚Äôre working with? (If public)\n\nAnd what types of prompts are you using?  \n\nAre you doing prompt rewriting? Reranking?\n\nAre you locked into only on-prem?",
          "score": 2,
          "created_utc": "2026-01-14 23:16:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4sah",
          "author": "hrishikamath",
          "text": "I have a feeling you should use agentic retrieval. Also do more of metadata engineering than anything. From my personal experience building an agent for sec filings. You can look at the readme for inspiration: https://github.com/kamathhrishi/stratalens-ai/blob/main/agent/README.md I am writing an elaborate blogpost on this too.",
          "score": 2,
          "created_utc": "2026-01-15 00:28:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobqkz",
          "author": "tony10000",
          "text": "I would use a dense model rather than OSS20B.  I am not sure that a MoE model is up to the task.  If you must use MoE, try: [https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)",
          "score": 2,
          "created_utc": "2026-01-15 04:46:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzogl1v",
          "author": "Past-Grapefruit488",
          "text": "Consider : \n\n1. Full text search via Elastic Search or similar (in addition to vecros and graph store)  \n2. Agentic RAG taht uses all these and evaluates it in context of inputs (initial as well as subsequent)",
          "score": 2,
          "created_utc": "2026-01-15 05:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolkqm",
          "author": "Rokpiy",
          "text": "hierarchical chunking helps but doesn't solve cross-references. legal docs have section 7.2.3 referencing \"as defined in section 2.1\" which has exceptions in section 5.4. flat chunks break these chains, hierarchical chunks reduce it but don't eliminate it\n\nwhat worked for us: dual-layer retrieval. first pass gets relevant sections via embeddings, second pass explicitly searches for cross-reference patterns in those sections and fetches the referenced content. regex + heuristics after semantic search\n\nyou can't solve this with better embeddings because legal language encodes relationships through explicit references, not semantic similarity. \"section 7.2.3\" and \"section 2.1\" have zero semantic overlap but maximum logical dependency\n\ngraph rag helps if you pre-extract \"section X references section Y\" during ingestion. but that's a parsing problem, not retrieval. most teams skip it because the parsing is fragile and breaks on updates\n\nfor the 64k context issue: either accept incomplete context or implement multi-hop retrieval where the model asks for missing definitions when it hits a reference",
          "score": 2,
          "created_utc": "2026-01-15 05:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzom65c",
          "author": "RecommendationFit374",
          "text": "Have you tried papr.ai we have document ingestion  u can use reducto or other providers, define your custom schema and auto build graph we combine vector + graph + prediction models it works well at scale. See our docs at platform.papr.ai",
          "score": 2,
          "created_utc": "2026-01-15 06:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzometk",
              "author": "RecommendationFit374",
              "text": "Our open source repo here https://github.com/Papr-ai/memory-opensource \n\nWe will bring doc ingestion to open source soon",
              "score": 2,
              "created_utc": "2026-01-15 06:06:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzqkbvr",
          "author": "DistinctRide9884",
          "text": "Check out SurrealDB, which is multi-model and has support for graph, vectors, documents and can be updated in real time (vs. other graph DBs where you have to rebuild the cache each time time you update the graph).\n\nThen for the documenting parsing/extraction something like¬†[https://cocoindex.io/](https://cocoindex.io/)¬†might be worth exploring, their core value prop is real-time updates and full traceability from origin into source. A CocoIndex and SurrealDB integration is in the works.",
          "score": 2,
          "created_utc": "2026-01-15 14:58:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqvuzd",
              "author": "Early_Interest_5768",
              "text": "CocoIndex looks great thanks.",
              "score": 2,
              "created_utc": "2026-01-15 15:52:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo6491",
          "author": "Recursive_Boomerang",
          "text": "https://medium.com/enterprise-rag/deterministic-document-structure-based-retrieval-472682f9629a\n\nMight help you out. PS I'm not affiliated with them",
          "score": 1,
          "created_utc": "2026-01-15 04:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobiin",
          "author": "charlesthayer",
          "text": "Please tell us a little bit more about your current setup. I'm wondering how many agents or sub agents you are currently using? \n\nEg. Do you have a search assistant agent that focuses just on finding docs (without full context)? \nAre you using a memory system like mem0 or a skill system like ACE?\nDo your prompts include both negative and positive examples? \nDo you have a set of evals that provides precision and recall (relevance)\n\nMy first thought is that it would probably help to extract a fair amount of metadata for each document into a more structured database. So this would be a separate pipeline that understands key important things that you're looking for in these docs. Eg. Which compliance or audit standards are discussed, etc.\n\nAdding the graph DB should be a great help. Doing multiple levels of chunking so that you include whole paragraphs and sections will also be a help for ranking. I'm not familiar with law or legal documents, but I imagine there may be some models fine-tuned for your legal domain.\n\nSounds interesting!",
          "score": 1,
          "created_utc": "2026-01-15 04:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoc0kd",
          "author": "tony10000",
          "text": "You may also want to check out Anything LLM.  It has excellent RAG capabilities.",
          "score": 1,
          "created_utc": "2026-01-15 04:48:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzojqrq",
          "author": "AloneSYD",
          "text": "First you need to work on you chunking for splitting you need to optimize splitting for the your kind of documents and specially cross pages, section\n\nLook up contextual retrieval, basically you need to add metadata to each chunk. CR can help in two ways either like a first stage retrieval or it can embedded with each chunk to enhance relevant chunks\n\n If you don't have enough context length to take a full document, make a custom one that will take n #pages before and after to create the context for each chunk instead of the whole document.\n\nI would highly recommend is to use a reAct agent. Because the reflection step help it in many situations to requery until it reaches a satisfiying state and you can specify the criteria on the answer is complete",
          "score": 1,
          "created_utc": "2026-01-15 05:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp0q11",
          "author": "cat47b",
          "text": "Can you share an anonymised example of the exceptions and overrides text. As others have said a chunk may need metadata that refers to others which need to be retrieved as part of that overall context",
          "score": 1,
          "created_utc": "2026-01-15 08:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpp9nl",
          "author": "HonestoJago",
          "text": "Law firms are hard. As soon as there‚Äôs one slight error, or one response that isn‚Äôt ‚Äúcomplete‚Äù, everyone will stop using it. It‚Äôs probably malpractice just to rely on the RAG, anyway, and not review the full underlying docs, so I‚Äôm not really sure there‚Äôs a lot of value there. I think tools for email management and retrieval are easier to build and are more attractive to attorneys, who are constantly overwhelmed by emails and can‚Äôt keep track of project status, etc‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-15 11:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqciya",
          "author": "my_byte",
          "text": "Hard disagree with folks screaming graph. If you do A/B with GraphRAG vs agentic, multi turn search and normalize for tokens - the latter will have similar results, without the operational headache that comes from graph updates, disambiguation and such.\nI'm not sure if I like gpt-oss tbh. Have you tried other models?\nGenerally speaking - what's your approach to measure recall and accuracy in your system?",
          "score": 1,
          "created_utc": "2026-01-15 14:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzs2ta0",
          "author": "ClearApartment2627",
          "text": "20+ documents? Did you mean 20k+ or 20m+ docs? Just asking because 20 does not sound right, and 20m might need a different architecture than 20k.",
          "score": 1,
          "created_utc": "2026-01-15 19:04:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzttohd",
          "author": "MrTechnoScotty",
          "text": "Perhaps RLM is worth a look?",
          "score": 1,
          "created_utc": "2026-01-16 00:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuw0ip",
          "author": "One_Milk_7025",
          "text": "Your chunking strategy is not perfect i feel. token based basic chunking wont do much when you need interlinking cross document reference. Docling is already good choice..   \nPick a AST based chunker like markdown-it if you can convert you existing doc to a MD file , extensively extract metadata from the chunks and atttach back to it(Qdrant support it natively). Optionally use a NER model like Gliner to extract the entities from those chunk text and header , this gives a common Concept registry which can be very helpful to create the graphDB. Chunking is the most important part of this, you need to extract parent/neighbour chunk relation, line count, section header , optionally token count etc etc.. work on the chunking pipeline and find what is more suitable for you.  \nfor graphdb its not necessary to use Neo4j but it does have its perks.. but for start you can use postgres +qdrant . it gives both hard graph from postgres which contains the file structure and hierarchy and semantic graph from qdrant.. but to have actual graph like structure its the Concept registry where things get really connected..  \nnow for the retrieval part it will be now much more easy to hop around those concepts, expanding the neighbor..",
          "score": 1,
          "created_utc": "2026-01-16 03:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwv5t8",
          "author": "Alternative-Edge-149",
          "text": "I think you should try the graphiti mcp server with ollama and use qwen 8b embedding + qwen 32b vlm or qwen 32b llm or something similar. Graphiti works for this usecase precisely since it is a temporal graph. You can connect it with Neo4j or Falkordb. It might not work out of the box with ollama as it requires the new open ai \"/v1/responses\" endpoint instead of chat completions endpoint but it can be done by something like LiteLLM. This should be accurate enough",
          "score": 1,
          "created_utc": "2026-01-16 12:59:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0326ic",
          "author": "andreasdotorg",
          "text": "20+ documents? Is that a typo or the actual number?\n\nI'm working with corporate policy, about 100 documents, plus legal texts relevant to them. This is purely agentic, I use Claude Code with Opus 4.5, with a lot of subagents.\n\nAll data storage is on disk, using ordinary file tools, no RAG in sight. And I don't think it's needed. What's a policy document, 5k tokens? I can have 20 in context and still have 100k tokens headroom in context.\n\nHere's a high level overview of what I do.\n\nOne important subagent is source intake. There's an agent fetching the original source, extracting full text and images from it (for PDF, .docx, web sites, whatever), creating a summary in a standardized format, including source location, link to full text, ready made citation in Chicago Style Manual format, and relevance to us (there's a high level background on jurisdiction, legal status, company size etc. in the global context that all agents get). Subagent has some CLI prompts for processing, it knows how to call lynx -dump or pdftotext. \n\nAnother one is the legal researcher. It knows how to call another subagent doing research on our legal database for case law, cross references to other relevant laws, etc. It provides a list of sources (then to be processed by source intake) , and a preliminary answer to the legal question.\n\nThere's a subagent for actually writing the text answering a research question. It has an extensive style guide: every sentence either a premise or a conclusion. Every premise has a citation. Conclusions depending on interpretation need a citation too backing up this conclusion. \n\nThen, a subagent for validation. Does the cited document exist? Is the citation precise enough? Does the citation actually support our statements? Do conclusions follow? Any language imprecise? Any claim unsubstantiated? Anything we missed?\n\nWorks pretty well for me, got compliments from actual lawyers.",
          "score": 1,
          "created_utc": "2026-01-17 10:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e5kdn",
          "author": "IllFirefighter4079",
          "text": "How many docs? I am building a Rust with Rig RAG ai client. I have 30,000 emails to try it on. Already built a version that put them all in a flagged database and labels them with llm. Wanted to see if I could do better with rag!",
          "score": 1,
          "created_utc": "2026-01-19 00:56:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdy998",
      "title": "Job wants me to develop RAG search engine for internal documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qdy998/job_wants_me_to_develop_rag_search_engine_for/",
      "author": "Next-Self-184",
      "created_utc": "2026-01-15 22:39:17",
      "score": 53,
      "num_comments": 82,
      "upvote_ratio": 0.96,
      "text": "this would be the first time I develop a RAG tool that searches through 2-4 million documents (mainly PDFs and many of those needing OCR). I was wondering what sort of approach I should take with this and whether it makes more sense to develop a local or cloud tool. Also the information needs to be secured so that's why I was leading toward local. Have software exp in other things but not working with LLMs or RAG systems so looking for pointers. Also turnkey tools are out of the picture unless they're close to 100k.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qdy998/job_wants_me_to_develop_rag_search_engine_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nztftcm",
          "author": "GroundbreakingEmu450",
          "text": "My two cents, while open for more experts to chime in: 2/4 million is a lot of documents and pdf is the worst format to start with. I would start by identifying the most valuable subset of documents, define a conversion approach (pref. towards markdown files) and then experiment with different chunking strategies (esp. parsing) to test retrieval. Embedding and Reranking should be considered. Those models are usually tiny and can run locally. For the reasoning part (responding to the user query by formulating an answer starting from the retrieved chunks) you can think about using a cloud LLM to have more advanced reasoning and potentially less hallucinations.",
          "score": 26,
          "created_utc": "2026-01-15 22:53:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztpvvl",
              "author": "OnyxProyectoUno",
              "text": "Parsing tools will already output markdown. That's the whole job of parsing, converting unstructured data like PDFs to more structured data in an LLM friendly document format like Markdown (or worse but acceptable, JSON).",
              "score": 4,
              "created_utc": "2026-01-15 23:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztsn6m",
                  "author": "ggone20",
                  "text": "It isn‚Äôt this trivial‚Ä¶ parsing pdfs sucks still today in 2026. If it‚Äôs just words with one column, it‚Äôs awesome. Two columns? You need to babysit it or develop more robust processes. Tables and charts? Lmao yea‚Ä¶ LLM/vLM parsing (expensive and/or slow). \n\nYou can rapid parse any corpus‚Ä¶ is what you get out useful and retrievable afterwards? Not likely‚Ä¶",
                  "score": 6,
                  "created_utc": "2026-01-16 00:02:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztz1ep",
              "author": "MrTechnoScotty",
              "text": "I agree in principle:  Start with a bite size chunk that focused on some subset of docs that can offer biggest outcomes when transformed.  Learn, iterate, it will begin to get clearer how scaling should occur‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-16 00:36:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztifap",
          "author": "nicoracarlo",
          "text": "What u/GroundbreakingEmu450 said it true. When you start handling such large amount of data, a simple RAG is going to get very messy and starts hallucinating (or missing important facts). You should look at graph rags and drift techniques, but those are not things you learn in a day or a week, as they require solid foundations (and analysing 2-4m documents for a graph rag is going to be very expensive even using specific models with low CPM) and it will take a lot of time.  \nI build a graph rag with drift and it takes between 10 to 60 seconds (depending on size and complexity)  to properly analyse a document. do the math...  \nMaybe at this point an off the shelf solution as u/BigNoseEnergyRI suggested may be less painful",
          "score": 7,
          "created_utc": "2026-01-15 23:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvy5wz",
              "author": "TechnicalGeologist99",
              "text": "Not to mention 2-4 million docs is looking to be 100 million nodes atleast. That's gonna be one expensive bill in terms of memory/ hosting fees",
              "score": 3,
              "created_utc": "2026-01-16 08:28:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o085ylp",
                  "author": "wing-of-freak",
                  "text": "We recently got hit with a pretty significant Pinecone bill. After doing some research, we found that S3 Vector Buckets are roughly 90% cheaper, with the main tradeoff being higher latency. Given the scale of our data, that tradeoff is acceptable for us. So we‚Äôre moving to S3 Vectors, which seems to be a better fit for our needs.",
                  "score": 1,
                  "created_utc": "2026-01-18 03:07:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztuxlx",
              "author": "BigNoseEnergyRI",
              "text": "Great advice. I wasn‚Äôt trying to be glib. The request is so general. Everything needs to be secure? They are describing every business out there. What industry? Compliance? Search to do what? Do they need to index/extract/enrich 2-4M documents or is there any curation needed?",
              "score": 1,
              "created_utc": "2026-01-16 00:14:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxfqv1",
              "author": "Popular_Sand2773",
              "text": "For a graph at that scale have you considered knowledge graph embeddings. Sounds like it strikes the right balance you‚Äôd be looking for.",
              "score": 1,
              "created_utc": "2026-01-16 14:49:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztq8ui",
          "author": "exaknight21",
          "text": "What resources have they made available to you? I would suggest do it locally. Get your hands on a 32 GB+ GPU, 3 if you can, and here is why:\n\n1. Run an 8b model (i prefer qwen3 family, and personally use qwen3:4b for my construction documents, I do government contracts only). This is your main LLM so serve with vLLM. If budget is low, go Tesla V100 32 GB. If budget is good, go RTX Pro 6000 Blackwell.\n\n2. Get another GPU, this I think should be a V100 32GB and will process all documents for embedding. You don‚Äôt need vLLM for this, Hugging Face Transformers works perfectly fine because most requests are batch processed and you want it to be like that.\n\n3. Get another GPU for OCR and use PaddleOCR-VL - it is genuinely good. \n\n\nOnce you‚Äôre good, deploy your gateway, I use dokploy, so get a container going for your gateway, a web app, and you have an entire pipeline.\n\nFor RAG, use lanceDB + S3 to store vector data directly into the S3, saving on storage otherwise you‚Äôll need excessive amount of data/drive management. I use an older 0.17.0 version of lanceDB + Backblaze B2. Latency is 400-500 ms for me. S3 would be 200-300, I ain‚Äôt dying.\n\nQuality of OCR matters, but so does organization of documents, and knowledge graphs. Use dgraph for knowledge graph, and call it a day.\n\nIf you‚Äôre not dealing with Handwritten Text, then you can use something like OCRMyPDF (i wrapped an API around it, it‚Äôs at https://github.com/ikantkode/exaOCR - completely free and dockerized). This runs CPU only and is very effective for me since I don‚Äôt work with HTR (handwritten text recognition). I mainly deal with scanned/text recognizable PDF, and it works like a charm.\n\n\nYou‚Äôd wanna create a gateway",
          "score": 7,
          "created_utc": "2026-01-15 23:49:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03u4t6",
              "author": "NeoNix888",
              "text": "https://preview.redd.it/pthygm472xdg1.png?width=2146&format=png&auto=webp&s=6b78b2d26b288f22b9983872cd082bf7f6fbc4d1\n\nThat's kewl you are using Gwen3 because they are on the top ten on [hugginghugh.com](http://hugginghugh.com) AI model scores!\n\nI have not use it yet, still learning.",
              "score": 1,
              "created_utc": "2026-01-17 14:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o03vx1k",
              "author": "NeoNix888",
              "text": "https://preview.redd.it/nwqukah73xdg1.png?width=2130&format=png&auto=webp&s=66ee02aaee8d6d391a2206be92354c5934cb7753\n\none more thing u/exaknight21 \\- there is 1 vulnerability in the exaOCR in case you didn't know. I got the report from [sbomly.com](http://sbomly.com) for free basically, it took less than a minute to get the full report.\n\nyou would get the below in the report and can click to go directly to the GHSA and CVE. they even provide SBOMQS scores!\n\nCheers!\n\n# üìö python-multipart\n\nv0.0.9¬†purl: pkg:pypi/python-multipart@0.0.9\n\nLIBRARYAPACHE-2.01 VULN (HIGH)\n\n**üîê**Hashes:**‚ìò**0\n\n**üîó**References:**‚ìò**0\n\n**‚öôÔ∏è**Properties:**‚ìò**5\n\n**‚ö†Ô∏è**Vulnerabilities:**‚ìò**1\n\n‚ö†Ô∏è Vulnerabilities (1)\n\n**GHSA-59g5-xgcq-4qw3 CVE-2024-53981** \n\nHIGH\n\nDenial of service (DoS) via deformation \\`multipart/form-data\\` boundary\n\n**‚úÖ Fix Available:**¬†Upgrade to version¬†**0.0.18**\n\n**EPSS:**¬†**0.1%**¬†chance of exploitation¬†(Top 68.9% most likely)\n\n**Risk Score:**¬†**0.09**¬†(combines CVSS + EPSS + KEV)\n\nCVSS: 7.5CWE-770\n\n[üîó ](https://nvd.nist.gov/vuln/detail/CVE-2024-53981)NVD\n\n**Risk Score:**¬†62/100 (MEDIUM)\n\n**üéØ Recommendations:**\n\n* Update python-multipart to fix 1 vulnerability\n\nüìñ View Technical Details",
              "score": 1,
              "created_utc": "2026-01-17 14:14:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuvdcl",
          "author": "One_Milk_7025",
          "text": "everythign can be ran locally..  \nUse docling to parse those pdf with page number included..  \nonce you get the markdown then the actual play starts. Pick a AST based chunker like markdown-it , extensively extract metadata from the chunks and atttach back to it(Qdrant support it natively). Optionally use a NER model like Gliner to extract the entities from those chunk text and header , this gives a common Concept registry which can be very helpful to create the graphDB. Chunking is the most important part of this, you need to extract parent/neighbour chunk relation, line count, section header , optionally token count etc etc.. you can use local bge-m3 quantized model or even the embedding-gemma 300m model.. which works fine for me.  \nfor graphdb its not necessary to use Neo4j but it does have its perks.. but for start you can use postgres +qdrant . it gives both hard graph from postgres which contains the file structure and hierarchy and semantic graph from qdrant.. but to have actual graph like structure its the Concept registry where things get really connected..  \nnow for the retrieval part it will be now much more easy to hop around those concepts, expanding the neighbor.. i hope this helps..",
          "score": 4,
          "created_utc": "2026-01-16 03:37:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvsk2q",
          "author": "fabkosta",
          "text": "We built search engines for 600m docs. But even with 2 - 4m things are really complicated, you need a lot of infra.\n\nSteps:\nFirst, build an OCRing system that scales. This is its own application that runs independently from the entire ingestion.\n\nSecond, build scalable ingestion and search infrastructure. That‚Äôs complicated. You will probably need mass storage, Kafka, back propagation pressure, and Elasticsearch.\n\nThird, build the RAG system. Use hybrid search, it‚Äôs almost always superior than RAG alone. But you must take a measuring approach, each change must be applied to eg 100 sample docs, and you measure whether a change improves something or not.\n\nYou see, these are really 3 distinct projects. There is a lot more here, eg OCRing will yield bad results, and you‚Äôll need a language model to correct OCRing results. (Disclaimer: I am selling consulting for such stuff.)",
          "score": 6,
          "created_utc": "2026-01-16 07:37:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztjgx0",
          "author": "Hour-Entertainer-478",
          "text": "For response generation: \nYou need to find a nice local llm that would hallucinate less, findinfo buried in lots of chunks, handle follow up conversations, and offer good tool calling. \nI have been doing pretty much that for different companies so ill say got oss 20b qwen3:30ba3b and gemma3:27b are good options. \n\nFor parsing docs: docling with rapidocr is a good start. \n\nFor 2 million docs, id suggest qdrant. It is generally faster than weaviate. (Vector db)\n\nOpinions are my own, hope that helps",
          "score": 4,
          "created_utc": "2026-01-15 23:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztkmyt",
              "author": "Next-Self-184",
              "text": "Thanks for responding - I'm assuming you'd need a pretty decent hardware setup?",
              "score": 1,
              "created_utc": "2026-01-15 23:19:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzts37u",
              "author": "ggone20",
              "text": "I really like got-oss also. I would layer with Qwen-vl‚Ä¶\n\nDocling is OK but the chunk strategy needs to be a lot more robust, it‚Äôll never work out if the box for this many documents. \n\nStorage explodes since you‚Äôll definitely need blob/object/artifact storage (s3), vector store, traditional db (Postgres-like is preferable), and knowledge graph‚Ä¶ local is ideal to ‚Äòkeep costs down‚Äô, specifically ongoing costs, but the hardware here isn‚Äôt trivial if you want anything done in a reasonable amount of time and HA durable with backups. If you‚Äôve never done infra before, bad time to start‚Ä¶\n\nThis is not a learn on the job endeavor‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-15 23:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvrjia",
          "author": "Much-Researcher6135",
          "text": "For document prep, there are several solid libraries out there (`marker`, `docling`, `unstructured`, `pymupdf`), depending on what type of layout and information are in the PDFs, but really good results may only come with neural net layout detection *followed* by OCR, not just OCR alone.\n\nBy all means go the non-ML route first to see if something like `ocrmypdf` will be sufficient. If so, that's a big win, because the really good stuff requires GPU acceleration.\n\nSo yeah, experiment. I am building RAG for a lot of funky stuff like [math-heavy PDFs](https://linear.axler.net/). For that, [marker](https://github.com/datalab-to/marker) outclassed everything else I listed above, but it's also the slowest., Guess you get what you pay for.\n\nEven that isn't always good enough, so in my workflow, I typically take marker's JSON output and run it through a cleanup XGBoost model I hand-tuned to omit stuff that annoys me. In other words, I burn a lot of GPU cycles to let marker do most of the work, and the library comes with some absolutely brilliant pre-trained neural nets, but there's still a bit of cleanup to do.\n\nAnyway running marker to convert a folder of files is dead simple code-wise (python or CLI). The following is done serially because the box in question has only one GPU, an RTX 3090, and marker will saturate that GPU while using maybe 15/24 GB VRAM on certain files ([this two-column technical textbook was brutal](https://networksciencebook.com/)):\n\n    # Convert a folder of books (epub, pdf) to markdown + JSON\n    from pathlib import Path\n    import os\n    import json\n    from marker.converters.pdf import PdfConverter\n    from marker.models import create_model_dict\n    from marker.renderers.markdown import MarkdownRenderer\n    from marker.renderers.json import JSONRenderer\n    \n    path_input = Path(\"input\")\n    path_output = Path(\"output\")\n    \n    for fn in os.listdir(path_input):\n        fp_in = path_input / fn\n        stem = fp_in.stem\n        fp_out_md = path_output / f\"{stem}.md\"\n        fp_out_json = path_output / f\"{stem}.json\"\n    \n        print(f\"Converting {stem}\")\n        if fp_out_md.exists() or fp_out_json.exists():\n            print(\"   already exists, skipping.\")\n        else:\n            converter = PdfConverter(artifact_dict=create_model_dict())\n            doc = converter.build_document(str(fp_in))\n            md_out = MarkdownRenderer()(doc)      # pydantic model\n            js_out = JSONRenderer()(doc)          # pydantic model\n    \n            fp_out_md.write_text(md_out.markdown, encoding=\"utf-8\")\n            fp_out_json.write_text(json.dumps(js_out.model_dump(mode='json'), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n            print(f\"   {stem} converted\")",
          "score": 4,
          "created_utc": "2026-01-16 07:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuzn6y",
          "author": "GP_103",
          "text": "Missing from this conversation and really the starting point is what‚Äôs the use case?\n\nIf you need ground truth and linked citation, then none of the aforementioned solutions may work.",
          "score": 3,
          "created_utc": "2026-01-16 04:03:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy8ncx",
              "author": "Next-Self-184",
              "text": "when you say linked citations, do you mean like retrieving the documents with the keywords?",
              "score": 1,
              "created_utc": "2026-01-16 16:58:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz0yer",
                  "author": "GP_103",
                  "text": "I mean whatever natural language /text AI wraps around the answer, you include a link or citation to the actual content block in the document.\n\nThat and evaluation suite, validation and visibility/traceability across the pipeline and back to the actual source.\n\nBasic for Healthcare, finance, legal, engineering and now enterprise in general",
                  "score": 1,
                  "created_utc": "2026-01-16 19:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzx1lev",
          "author": "voycey",
          "text": "So I have just spent the last 6 months building exactly this, unfortunately you are in for a rough time.\nAs others have said, PDF parsing isn't a solved thing, it requires a LOT of steps to get right.\n\nUltimately you will have to contend with a few different things:\n\n1. Sovereignty issue - unlikely you can use public APIs with company / PII data \n2. Contention issues - local LLMs are literally the \"Fast, Cheap, Good - Pick 2\" paradox\n3. Machines reading documents is not the same as people reading documents.\n4. Infrastructure - you better be happy deploying some specific tooling\n5. Speed - Good RAG is S L O W - are your users going to be ok with a 20-40 second round trip?\n\nShortcuts for you:.\n1. Use Bedrock and Nova Pro 2 - this is probably the fastest way to get something that will work at scale up and running. The RAG isn't great but you can build a simple chatbot in Lex or use Open WebUI and hook it up to your Bedrock pipeline, you then basically just upload all of the docs to the Bedrock KB S3. Also keeps it somewhat sovereign (although watch out for CRIS on Bedrock models).  Other clouds have something similar for this!\n\n2. Buy something readymade that does this and focus instead on the data engineering challenges that surround this like the pre RAG and post RAG pipelines to actually derive value from those documents! I can help here!\n\nTons of trade-offs DIYing it, really comes down to what you want to get out of it at the end!",
          "score": 3,
          "created_utc": "2026-01-16 13:36:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyadvn",
              "author": "Next-Self-184",
              "text": "yea 20-40 second round trip is fine compared to hours of searching. been seeing a lot of feedback recommending to really focus on the OCR pipeline and then get elastic search to deal with the vectors.",
              "score": 1,
              "created_utc": "2026-01-16 17:06:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02r47w",
                  "author": "voycey",
                  "text": "I would personally avoid ElasticSearch - if you are deploying a service for this then just start with Postgres for everything. The only time you would want to use ES is if you are planning on indexing the full documents in there (reasons for and against this).\n\nThe OCR pipeline has many solutions - Comprehend is very good from AWS and you can pipeline that many documents easily, however, your chunking strategy / solution is the most important thing to get right and there are no easy / ready made solutions out there that solve this for you, it will be different on the document type, the document content and several other things!",
                  "score": 1,
                  "created_utc": "2026-01-17 08:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nztkc6a",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's a classic enterprise RAG problem. The scale isn't the hard part, it's getting consistent results across 2-4 million documents when you can't see what went wrong during processing.\n\nStart with your document processing pipeline first. OCR quality varies wildly between tools, and bad OCR creates garbage chunks that'll haunt your retrieval later. Test Tesseract vs cloud OCR on a sample of your PDFs to see what you're working with. For parsing, Unstructured handles PDFs reasonably well, but you'll want to preview what your documents actually look like after each transformation step.\n\nChunking strategy matters more than your vector store choice here. Recursive chunking works for most enterprise docs, but test different chunk sizes on your actual content. 500 tokens might work great for technical docs but fail on contracts or reports.\n\nLocal makes sense for security, but don't underestimate the infrastructure overhead. You'll need serious compute for embedding 2-4 million docs, plus vector storage that can handle the scale. Qdrant or Chroma can work locally, but plan your hardware accordingly.\n\nThe real killer is iteration speed. Every config change usually means reprocessing everything, which gets expensive fast at your scale. That's what I've been building around at vectorflow.dev, but honestly any approach that lets you test configurations before committing to the full pipeline will save you weeks.\n\nWhat type of PDFs are you dealing with? Scanned documents vs native PDFs change your whole preprocessing approach.",
          "score": 4,
          "created_utc": "2026-01-15 23:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqg5m",
              "author": "ggone20",
              "text": "This is good advice. The part to really pay attention here is iteration as mentioned. \n\nYou can‚Äôt test your pipe against 1000 docs and assume it‚Äôs good because your evals hit hard. If you‚Äôre not doing the full ingestion and testing against the full corpus, you have no idea what performance is actually going to be like. \n\nHow many users you‚Äôre serving, how many updates per day/week/month are needed (re-indexing is a pain in the ass) and what downtime is acceptable while that happens? Are you serving users in different time zones where you can only do re-indexes on weekends? Rolling indexes? How will you do change management? Lol the list goes on.",
              "score": 3,
              "created_utc": "2026-01-15 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01tx46",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, the rolling index problem is brutal at that scale. Most people don't think about it until they're staring at 8 hour reprocessing times for a single config change.\n\nChange management gets weird too because you can't really do proper A/B testing when your index is that large. You end up having to commit to architectural decisions based on smaller samples and hope they hold up. The timezone constraint is real, especially if you're dealing with global teams who need the system during your maintenance windows.\n\nHave you dealt with incremental indexing before? I'm wondering if OP could get away with treating document updates as deletes plus inserts rather than trying to build something more sophisticated for their first pass.",
                  "score": 3,
                  "created_utc": "2026-01-17 04:14:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztzoom",
              "author": "MrTechnoScotty",
              "text": "I think ocr should be a separate process from the ingesting.  OCR using known quality tool(s), likely NOT AI, then work on converting/ingesting the output docs that have been tested to equal = ‚Äúgood‚Äù‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-16 00:40:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01y75a",
                  "author": "OnyxProyectoUno",
                  "text": "That's the right call. OCR as a preprocessing step keeps your pipeline cleaner and lets you validate quality before anything hits your vector store. Tesseract with some postprocessing usually gets you there for most enterprise docs, though you might need to tune confidence thresholds per document type.\n\nThe tricky part becomes managing that two-stage workflow at scale. You'll want to track which source documents failed OCR, what your confidence scores look like across batches, and probably build in some manual review process for borderline cases. Once you've got clean text output though, the rest of your RAG pipeline becomes much more predictable since you're not debugging OCR artifacts mixed in with chunking or retrieval issues.\n\nAre you planning to reprocess the OCR if it fails quality checks, or just flag those documents for manual handling?",
                  "score": 1,
                  "created_utc": "2026-01-17 04:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvgpo9",
          "author": "Select-Spirit-6726",
          "text": "It‚Äôs great that your team recognizes the scale early ‚Äî millions of PDFs with OCR is a hard engineering problem on its own before you even get to an LLM.\n\nA few folks here are pointing you toward Elastic + pgvector or similar solutions, and that‚Äôs not accidental. Doing reliable vector search at that scale *first* will make whatever generative layer you add later much more accurate and cost-effective.\n\nAlso, starting with a strict chunking + OCR pipeline and validating extraction quality at smaller scale will save you from scaling bad vectors later. Even a well-tuned vector store alone can answer a lot of semantic queries without invoking a language model on every request.\n\nIf security and local deployment are priorities, open-source vector stores and on-prem OCR tools are totally reasonable to start with. Just be sure to invest as much in ingestion and indexing quality as you do in the LLM layer.",
          "score": 2,
          "created_utc": "2026-01-16 05:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxbfdq",
          "author": "Majinsei",
          "text": "2 million is quite a large number~\n\nYou need to do a classification analysis of exactly what you have...\n\nProcessing invoices is not the same as processing laws, processing minutes, processing a survey, etc.~\n\nIf necessary, each of these will surely have a custom extraction method, although creating about 20 different extraction methods is easier than creating several hundred.~\n\nThere are several ways to do it, but a popular strategy is to extract the metadata. You create a metadata table, and your agents search using that metadata as the first filtering step. Then you use the embedding of those filters to get the relevant data from there.\n\nThe metadata, for example, involves adding an Entity extractor step:\n\n{\"Type\": City, \"name\": Los Angeles, \"id\": 5}\n\nAnother for dates, and so on, depending on the data model you need.\n\nSo, when the user asks for the city of \"Los Angeles,\" an Entity extractor is used in the query, allowing you to filter by the city dimension (this has its pros and cons). And so on with each data point. You can optimize it later with a custom model that does everything at once.\n\nAnd these filters based on your metadata require filtering by ranking the N most similar.\n\nThis is a more or less effective method.\n\n\nRegarding OCR, you have to consider two different cases. One is pure images to be OCRed; there are many good cloud-based models for this, and you can even customize them yourself according to your needs. And you also have to consider when you have the explanatory context; a graph requires knowing which line goes up and which line goes down.\n\n\nAlthough this depends on how granular you want the information to be.",
          "score": 2,
          "created_utc": "2026-01-16 14:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz8nwg",
          "author": "patbhakta",
          "text": "2-4mill documents and mostly OCR... You're in for a treat. I wish you well. \n\nYou don't need RAG just yet you need an architect. Let's say you ignore my advice so you proceed...\n\nLet's say you find a YouTuber follow some clever selling n8n\\rag implementation and it might even work or so you think on the first document. Maybe even the 10th or 100th document. So you think. \n\nThen reality sets in your users who actually retrieve the stuff find the information garbage when researching a bit more. Or worse your AI hallucinates information based on your embeddings. \n\nThen you work on 2.0\nOnly to find chunking large documents suck\nOnly to find OCR sucks\nOnly to find charts, tables, diagrams, media is lost in ingestion\nYou come to find data cleansing is hard on just a few different documents let alone hundreds or even thousands.\n\nBut let's say you figured it all out now you need to work on 3.0\nThis involves scaling vector databases, graph data, traditional data, etc. \nCongrats you managed to embed 4 million documents! \nYou get a meeting with the CFO about why the database cost too much monthly, users are also complaining about it being slow at times, constantly getting unrelatable documents, and it's just clunky in general\n\nNow it's time for 4.0\nYou discover the databases are littered with duplicate entries from ingesting similar template documents, you discover dedup methods and implement that, potentially breaking your beautiful vectors. \nNow your CEO is happy about the speed optimization and reduced cost, however the users still think it's shit and unreliable. Congrats you spent months on something nobody can use or trust... \n\nThen you still have 5.0, 6.0, 7.0, 8.0 problems too such as  data security, everyone in the damn company has access to all 4 million documents and it's crappy info. CEO doesn't want proprietary data going to openAI or others. New documents are made constantly and it's mixing with old documents and confusing the shit about what's newer. Users find your system to be shit, they end up manually getting relevant docs uploading them to ChatGPT so it can give a seemingly intelligent answer to the dummy who knows well whatever you built is worse. \n\nCEO cans the project because AI can't be trusted, data is flawed, staff is lazy and inept now thanks to your RAG.",
          "score": 2,
          "created_utc": "2026-01-16 19:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztrd1y",
          "author": "Infamous_Ad5702",
          "text": "I did it.\nI do it via an index. And build a new KG on the fly for each new query. \nIt‚Äôs offline.\nDeterministic \nNot AI.\nI can show you.\nI do reddit webinars now apparently",
          "score": 2,
          "created_utc": "2026-01-15 23:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztpinp",
          "author": "TrevorHikes",
          "text": "Talk to https://aicamp.so/\n\nAnd no I‚Äôm not affiliated but I have used their platform and they have been doing on premise solutions for high compliance industries and aren‚Äôt so big that a small to medium biz customer wouldn‚Äôt be attractive. \n\nIf you want to see an ideal solution in my opinion for productivity check out Juma.ai",
          "score": 1,
          "created_utc": "2026-01-15 23:45:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzujwv2",
          "author": "zapaljeniulicar",
          "text": "A huge chunk of the advice lies in what are you actually going to talk to, like what is in those PDFs. What information are you going to deal with? I‚Äôve seen somebody here talking about RAG for CAD PDFs, like why? If you are searching for an image, RAG is not the way to do it.",
          "score": 1,
          "created_utc": "2026-01-16 02:33:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv3x1o",
          "author": "IdeaAffectionate945",
          "text": "2/4 million documents might be too much for our SQLite VSS database, but if you want to perform tests to check if you could get away with it, you can search for AINIRO Magic Cloud (open source!)\n\nIt can automatically convert PDF (with **text**) to text ...",
          "score": 1,
          "created_utc": "2026-01-16 04:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxhc1g",
          "author": "sippin-jesus-juice",
          "text": "They want to vectorize 4 million documents but only are prepared to spend 100k in total for the project?",
          "score": 1,
          "created_utc": "2026-01-16 14:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxvv3f",
          "author": "seomonstar",
          "text": "best approach get a new job",
          "score": 1,
          "created_utc": "2026-01-16 16:02:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyk425",
          "author": "kbash9",
          "text": "Just use a managed platform that offers agentic RAG: rather than a single lookup, an agentic RAG system uses a planner to find the answer. Much better accuracy",
          "score": 1,
          "created_utc": "2026-01-16 17:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzykwnl",
          "author": "Wise_Reward6165",
          "text": "IMHO you would need a good server to use or 9-series cpu/gpu for local at minimum. Servers are recommended for continuous duty. If money is the primary concern (and not time), consider using a CPU server with a lot of RAM. GPUs are great for acceleration but unnecessary for building the initial database. A standard PC with a 7900 or 5080 could serve the actual AI model once the database is fully assembled. Since security is an issue you will have to be technical btw. Use air gapped protection without wifi-bluetooth-eth0 initially then when the database is assembled serve it as a reverse proxy (with apache or nginx) to a second machine in a VM with monitoring.\n\nTools:\nChromadb transformers torch langchain\nFaiss and llama.cpp are optional. Custom binding a llama.cpp fork for security is extra work but necessary for security.\n\nFrom HF.io:\ndeepseek-ocr\nQwen3-embedding \nNomic-text-embedding (is highly rated)\n\nIt would be a multi-model setup to make the initial database but after that just serve it with apache or nginx and a reverse proxy to your intranet.\n\nAlso, you ‚Äòmight‚Äô be able to use traefik as a web scraper arranged locally on the initial data and then feed it to the Chromadb setup. (Custom programming for sure)\n\nAlso, Also, you could look into using vision models for the PDFs like llama scout but most I think are heavy on ram and vision models still struggle with setup and accuracy.",
          "score": 1,
          "created_utc": "2026-01-16 17:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzypb2t",
          "author": "Hungry-Amount-2730",
          "text": "I'm curious - Why building it from scratch, not just buying already existing software?",
          "score": 1,
          "created_utc": "2026-01-16 18:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyrcgu",
              "author": "Next-Self-184",
              "text": "yea this might be the way, focus on the OCR pipeline and get something like elastic search",
              "score": 1,
              "created_utc": "2026-01-16 18:21:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzys3pg",
                  "author": "Hungry-Amount-2730",
                  "text": "to be completely honest - we have built semantic code search tool and we have PoC for documentation (and gonna go further, its on ou roadmap) , that's why it was my business curiosity to have better understanding :)",
                  "score": 1,
                  "created_utc": "2026-01-16 18:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01mja4",
          "author": "Previous-Ad5318",
          "text": "we use Ziqara internally for document search. Its like google drive for Rag, we kinda love it as they outperformed glean and other rag solutions",
          "score": 1,
          "created_utc": "2026-01-17 03:24:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02xiuy",
          "author": "techie_boy69",
          "text": "IBM Docling project on GitHub",
          "score": 1,
          "created_utc": "2026-01-17 09:50:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03gfsv",
          "author": "jannemansonh",
          "text": "Worth checking out Needle. Production-ready RAG API, handles large document volumes including PDFs with OCR. Might save you from building everything from scratch.",
          "score": 1,
          "created_utc": "2026-01-17 12:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05348d",
          "author": "ImpossibleEnd8335",
          "text": "(stochastic thinking cap on)\n\n4M \"might\" (sprinkles fairy dust on quotation marks) actually mean < 5% of 4M.  \nWhat are people actually looking at?\n\nI think it would make sense to get stats on doc usage and tier it that way.",
          "score": 1,
          "created_utc": "2026-01-17 17:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bnf3",
          "author": "ampancha",
          "text": "\"Local\" vs \"cloud\" is actually a secondary decision. The real security question at your scale is: who can retrieve what, and can you prove it? With 2-4M documents, you need retrieval filtering (so users only see docs they're authorized for), audit trails, and rate limits to prevent bulk extraction.  \nOn the OCR side, your chunking strategy will make or break retrieval quality. Poor OCR output creates garbage embeddings, which means irrelevant results regardless of how good your vector DB is. Happy to point you toward specific patterns if useful.",
          "score": 1,
          "created_utc": "2026-01-17 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05d1lu",
              "author": "ampancha",
              "text": "Sent you a DM with more detail on the architecture side.",
              "score": 1,
              "created_utc": "2026-01-17 18:30:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0934yb",
          "author": "Neat_Cartographer864",
          "text": "One question... Isn't it better to convert PDFs to TOON (token-oriented object notation) instead of Markdown?",
          "score": 1,
          "created_utc": "2026-01-18 06:57:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztezp5",
          "author": "BigNoseEnergyRI",
          "text": "Every off the shelf enterprise search solution can do this without having to build it. Coveo, Elastic, Lucid, serachblox, lucid, Lucy, Glean, OpenText, Google, etc.",
          "score": 0,
          "created_utc": "2026-01-15 22:49:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztni1h",
              "author": "ggone20",
              "text": "Someone has never built anything before lol. Come back when you‚Äôre grown up, kid üòéüòÇ\n\nThere literally doesn‚Äôt exist an out of box solution that is reliable for several hundred thousand documents much less millions.\n\nSure, you COULD throw them in these services, you‚Äôd never get anything useful out.",
              "score": -2,
              "created_utc": "2026-01-15 23:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztvomo",
                  "author": "BigNoseEnergyRI",
                  "text": "We don‚Äôt know what they are even trying to get out of it and how many documents they actually need. \n\nBut you said I‚Äôm young, so muwah!!!",
                  "score": 2,
                  "created_utc": "2026-01-16 00:18:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu3dya",
          "author": "SerDetestable",
          "text": "u don‚Äôt have 4M pdfs. nobody does",
          "score": -1,
          "created_utc": "2026-01-16 01:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuzsoz",
          "author": "Responsible_Type_",
          "text": "Bro just use aws bedrock. U don't need to worry about anything other than cost. \n\nJust learn what is what thorugh youtube.",
          "score": -1,
          "created_utc": "2026-01-16 04:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvt9cr",
              "author": "Much-Researcher6135",
              "text": "\"just throw money at the problem\" lol",
              "score": 3,
              "created_utc": "2026-01-16 07:43:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztrbwn",
          "author": "abhi91",
          "text": "Just use contextual AI. This is a standard use case, they power enterprises with this scale and it's affordable. Most importantly you'll be able to have enterprise grade security. If needed it can be deployed on your own VPC",
          "score": -2,
          "created_utc": "2026-01-15 23:55:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu5lhy",
              "author": "Wikileaks_2412",
              "text": "ContextualAI is affordable ? How much do they charge for the parsing part ?",
              "score": 1,
              "created_utc": "2026-01-16 01:13:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztmnb9",
          "author": "ggone20",
          "text": "This will be extremely difficult. \n\nI‚Äôve discussed this in many other threads but if you‚Äôre here looking for advice to build a RAG system at this scale you‚Äôve already failed. \n\nHire someone. Hire ME, I can be available. #shameless-pitch\n\nFor real though‚Ä¶ hire someone who demonstrably has done this before. You‚Äôll never succeed. Not putting you down, it‚Äôs just that hard and there STILL doesn‚Äôt yet exist a service that you can just plug and play into. Further, even if you do everything RIGHT, it will be very expensive just to do the ingestion and processing of this many documents - you/your employer likely won‚Äôt expect actual costs. \n\nIf you do things wrong or need to trial/error, costs balloon. There isn‚Äôt really a way to test if things work until you scale the ingestion as well. Lots of processes work at smaller document numbers and fall apart in the tens or hundreds of thousands. Nevermind millions.  \n\nFor the record I‚Äôve led building systems for utility-scale capital construction projects where 5-10 million artifacts are produced over the course of the project. The utility portfolio has hundreds of millions. In addition to the artifacts created, the actual management of the project requires accurate retrieval to give go/no-go decisions as well as adherence to standards, regulations, etc where reporting decisions with provenance is critical. Good times. \n\nGood luck. \nDMs open ü´°üôÉü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": -3,
          "created_utc": "2026-01-15 23:29:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qed97y",
      "title": "A user shared to me this complete RAG guide",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/",
      "author": "Real-Turnover9685",
      "created_utc": "2026-01-16 11:12:01",
      "score": 33,
      "num_comments": 7,
      "upvote_ratio": 0.9,
      "text": "Someone juste shared to me this complete RAG guide with everything from parsing to reranking. Really easy to follow through.  \nLink :¬†[https://app.ailog.fr/en/blog](https://app.ailog.fr/en/blog)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzwsx76",
          "author": "Much-Researcher6135",
          "text": "Fancy seeing this at the top of /r/rag. I just asked Gemini for RAG knowledge hubs and it [recommended this sick repo](https://github.com/NirDiamant/RAG_Techniques). I just found out about it 2 minutes ago, but 24k github stars can't be wrong. It also recommended the llamaindex and langchain docs, plus this subreddit!",
          "score": 5,
          "created_utc": "2026-01-16 12:44:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwtwqi",
              "author": "Real-Turnover9685",
              "text": "Oh that's nice, I'll take a look at it, thank you.",
              "score": 2,
              "created_utc": "2026-01-16 12:51:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwl8mh",
          "author": "tchikss",
          "text": "Thanks for sharing !",
          "score": 1,
          "created_utc": "2026-01-16 11:51:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwnjsz",
              "author": "Real-Turnover9685",
              "text": "Glad you liked it :)",
              "score": 1,
              "created_utc": "2026-01-16 12:08:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02alm5",
          "author": "George_David_S",
          "text": "Thanks so much for sharing would love to connect with you",
          "score": 1,
          "created_utc": "2026-01-17 06:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bdfpj",
          "author": "Distinct-Land-5749",
          "text": "What's the use case you are using this for?",
          "score": 1,
          "created_utc": "2026-01-18 16:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx0ajk",
          "author": "atultrp",
          "text": "[fastrag.live](http://fastrag.live)",
          "score": 0,
          "created_utc": "2026-01-16 13:29:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdziof",
      "title": "New Chapter on \"Chunking Strategies\" - 21 RAG Strategies Book",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qdziof/new_chapter_on_chunking_strategies_21_rag/",
      "author": "blue-or-brown-keys",
      "created_utc": "2026-01-15 23:29:31",
      "score": 32,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "I have added a new Chapter on Chunking the \"21 RAG Strategies\" Book. I am looking for feedback, Which of these strategies do you use in production? Also do you use a strategy you like thats not mentioned here?\n\n[Download \"21 RAG Strategies\" Ebook here](https://www.twig.so/#DownloadEbookSection)\n\n* **Chapter 22 ‚Äî Chunking Strategies for Retrieval-Augmented Generation**\n      1. Chunking as a Core RAG Primitive\n   * 1.1 Definition of a Chunk\n   * 1.2 Chunking vs. Text Splitting\n   * 1.3 Chunking and Retrieval Semantics\n* 2. Why Chunking Determines RAG Accuracy\n   * 2.1 Context Window and Model Constraints\n   * 2.2 Retrieval Precision and Recall\n   * 2.3 Cost, Latency, and Token Efficiency\n   * 2.4 Chunking as an Information Architecture Problem\n* 3. Baseline Chunking Approaches\n   * 3.1 Fixed-Size Token Windowing\n   * 3.2 Sentence-Aligned Chunk Construction\n   * 3.3 Paragraph-Aligned Chunk Construction\n* 4. Structure-Driven Chunking\n   * 4.1 Section- and Heading-Scoped Chunking\n   * 4.2 Document Markup‚ÄìAware Chunking\n   * 4.3 Code- and Clause-Scoped Chunking\n* 5. Semantic Boundary Detection\n   * 5.1 Topic Shift‚ÄìBased Chunk Segmentation\n   * 5.2 Embedding Similarity Thresholding\n   * 5.3 Discourse-Level Chunk Formation\n* 6. Context Preservation Techniques\n   * 6.1 Controlled Overlap and Window Expansion\n   * 6.2 Sentence-Window Retrieval Models\n   * 6.3 Contextual Header Injection\n   * 6.4 Pre- and Post-Context Buffering\n* 7. Hierarchical and Multi-Resolution Chunking\n   * 7.1 Fine-Grained vs. Coarse-Grained Retrieval Units\n   * 7.2 Parent‚ÄìChild Chunk Hierarchies\n   * 7.3 Recursive and Outline-Derived Chunking\n* 8. Question-Centric Chunk Design\n   * 8.1 Generating Retrieval-Aligned Questions\n   * 8.2 Answer-Complete Chunk Construction\n   * 8.3 Context-Buffered Question Anchoring\n* 9. Dual-Index and Retrieval-First Architectures\n   * 9.1 Question-First Retrieval Models\n   * 9.2 Canonical Chunk Grounding\n   * 9.3 Deduplication, Reranking, and Stitching\n* 10. Domain-Aware Chunking Patterns\n   * 10.1 API and Reference Documentation\n   * 10.2 Support Tickets and Conversation Threads\n   * 10.3 Policy, Compliance, and Versioned Knowledge\n* 11. Evaluation-Driven Chunk Optimization\n   * 11.1 Measuring Chunk Quality\n   * 11.2 Retrieval Accuracy and Citation Fidelity\n   * 11.3 Iterative Chunking Refinement\n* 12. Practical Guidance and Trade-Offs\n   * 12.1 Choosing the Right Strategy per Data Source\n   * 12.2 Combining Multiple Chunking Strategies\n   * 12.3 Common Failure Modes and Anti-Patterns\n* 13. Summary: Chunking as the Foundation of RAG\n* 13.1 Why Models Fail When Chunking Fails\n* 13.2 Recommended Production Defaults",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qdziof/new_chapter_on_chunking_strategies_21_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzwjarp",
          "author": "GroundbreakingEmu450",
          "text": "I can already see it is missing anything related to AST based chunking for indexing codebases for example",
          "score": 3,
          "created_utc": "2026-01-16 11:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwjg41",
          "author": "GroundbreakingEmu450",
          "text": "Also the structure you posted here does not match the one of the book, wtf?",
          "score": 2,
          "created_utc": "2026-01-16 11:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxon9h",
              "author": "blue-or-brown-keys",
              "text": "Let me check, this is only one chapter of the book.",
              "score": 1,
              "created_utc": "2026-01-16 15:30:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpith",
          "author": "Able-Let-1399",
          "text": "Thanks. I will when I've read it üëç",
          "score": 2,
          "created_utc": "2026-01-16 12:22:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzumx9s",
          "author": "blue-or-brown-keys",
          "text": "Please share your feedback and how I can improve on the book, what strategies are missing and which ones you have used in production.",
          "score": 1,
          "created_utc": "2026-01-16 02:50:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhzs5i",
      "title": "DeepResearch is finally localized! The 8B on-device writing agent AgentCPM-Report is now open-sourced!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhzs5i/deepresearch_is_finally_localized_the_8b_ondevice/",
      "author": "Relevant_Abroad_6614",
      "created_utc": "2026-01-20 12:27:26",
      "score": 28,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "In an era where **Deep Research** is surging, we all long for a ‚Äúsuper writing assistant‚Äù capable of automatically producing **tens of thousands of words**.\n\nBut‚Äîwhen you‚Äôre holding **corporate strategic plans, unpublished financial reports, or core research data**, would you really dare to upload them to the cloud ‚òÅÔ∏è?\n\nToday, we bring a **game-changing solution**: **AgentCPM-Report** ‚Äî a **localized, private, yet top-tier** deep research agent.\n\nJointly developed by **Tsinghua University NLP Lab**, **Renmin University of China**, **ModelBest**, and the **OpenBMB open-source community**, it is now **open-sourced** on **GitHub, Hugging Face**, and more.\n\n**What does this mean?**\n\nNo expensive compute. No data uploads.\n\nYou can run an **expert-level research assistant entirely on your local machine** üîß\n\nüîç **Why choose AgentCPM-Report?**\n\n‚úÖ **Extreme efficiency ‚Äî doing more with less**\n\nWith only **8B parameters**, it achieves **40+ rounds of deep retrieval** and **nearly 100 steps of chain-of-thought reasoning**, generating logically rigorous, insight-rich **long-form reports** comparable to top closed-source systems.\n\n‚≠êÔ∏è DeepResearch Bench\n\n|Model|Overall|Comprehensiveness|Insight|Instruction Following|Readability|\n|:-|:-|:-|:-|:-|:-|\n|Doubao-research|44.34|44.84|40.56|47.95|44.69|\n|Claude-research|45.00|45.34|42.79|47.58|44.66|\n|OpenAI-deepresearch|46.45|46.46|43.73|49.39|47.22|\n|Gemini-2.5-Pro-deepresearch|49.71|49.51|49.45|50.12|50.00|\n|WebWeaver (Qwen3-30B-A3B)|46.77|45.15|45.78|49.21|47.34|\n|WebWeaver (Claude-Sonnet-4)|50.58|51.45|50.02|50.81|49.79|\n|Enterprise-DR (Gemini-2.5-Pro)|49.86|49.01|50.28|50.03|49.98|\n|RhinoInsigh (Gemini-2.5-Pro)|50.92|50.51|51.45|51.72|50.00|\n|**AgentCPM-Report**|**50.11**|**50.54**|**52.64**|**48.87**|**44.17**|\n\n‚≠êÔ∏è DeepResearch Gym\n\n|Model|Avg.|Clarity|Depth|Balance|Breadth|Support|Insightfulness|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|Doubao-research|84.46|68.85|93.12|83.96|93.33|84.38|83.12|\n|Claude-research|80.25|86.67|96.88|84.41|96.56|26.77|90.22|\n|OpenAI-deepresearch|91.27|84.90|98.10|89.80|97.40|88.40|89.00|\n|Gemini-2.5-pro-deepresearch|96.02|90.71|99.90|93.37|99.69|95.00|97.45|\n|WebWeaver (Qwen3-30b-a3b)|77.27|71.88|85.51|75.80|84.78|63.77|81.88|\n|WebWeaver (Claude-sonnet-4)|96.77|90.50|99.87|94.30|100.00|98.73|97.22|\n|**AgentCPM-Report**|**98.48**|**95.10**|**100.00**|**98.50**|**100.00**|**97.30**|**100.00**|\n\n‚≠êÔ∏è DeepConsult\n\n|Model|Avg.|Win|Tie|Lose|\n|:-|:-|:-|:-|:-|\n|Doubao-research|5.42|29.95|40.35|29.70|\n|Claude-research|4.60|25.00|38.89|36.11|\n|OpenAI-deepresearch|5.00|0.00|100.00|0.00|\n|Gemini-2.5-Pro-deepresearch|6.70|61.27|31.13|7.60|\n|WebWeaver (Qwen3-30B-A3B)|4.57|28.65|34.90|36.46|\n|WebWeaver (Claude-Sonnet-4)|6.96|66.86|10.47|22.67|\n|Enterprise-DR (Gemini-2.5-Pro)|6.82|71.57|19.12|9.31|\n|RhinoInsigh (Gemini-2.5-Pro)|6.82|68.51|11.02|20.47|\n|**AgentCPM-Report**|**6.60**|**57.60**|**13.73**|**28.68**|\n\n‚úÖ **Physical isolation, true local security**\n\nDesigned for **high-privacy scenarios**, it supports **fully offline deployment**, eliminating cloud data leakage risks.\n\nYou can mount **local knowledge bases**, ensuring sensitive data **never leaves your domain** while still producing professional-grade reports.\n\nüòé **Try it now: put DeepResearch on your hard drive**\n\n**AgentCPM-Report** is now available on **GitHub | Hugging Face | ModelScope | GitCode | Modelers**, and we warmly invite developers to try it out and co-build the ecosystem!\n\n**GitHubÔºöüîó**¬†[https://github.com/OpenBMB/AgentCPM](https://github.com/OpenBMB/AgentCPM)\n\n**HuggingFaceÔºö üîó**¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n\nIf you find our work helpful, please consider giving us a ‚≠ê **Star** & üíñ **Like**\\~",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qhzs5i/deepresearch_is_finally_localized_the_8b_ondevice/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0o2fw3",
          "author": "New_Wear4177",
          "text": "Fantastic Work! Put the DeepResearch in edges.",
          "score": 2,
          "created_utc": "2026-01-20 14:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r1vb7",
          "author": "Unique-Temperature17",
          "text": "This is really cool stuff, thanks for sharing! The privacy-first approach with full offline deployment is exactly what's needed for handling sensitive docs. Will definitely check out the GitHub repo soon. Would love to see this integrated into apps - having an 8B model that punches above its weight like this would be a game-changer for local document workflows.",
          "score": 2,
          "created_utc": "2026-01-20 22:30:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhxtt2",
      "title": "Chunking without document hierarchy breaks RAG quality",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhxtt2/chunking_without_document_hierarchy_breaks_rag/",
      "author": "Upset-Pop1136",
      "created_utc": "2026-01-20 10:41:45",
      "score": 22,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I tested a few AI agent builders (Dify, Langflow, n8n, LyZR). Most of them chunk documents by size, but they ignore document hierarchy (doc name, section titles, headings).  \n\n\nSo each chunk loses context and doesn‚Äôt ‚Äúknow‚Äù what topic it belongs to.  \n\n\nSimple fix: **Contextual Prefixing**\n\nBefore embedding, prepend hierarchy like this:\n\n`Document: Admin Guide`\n\n`Section: Security > SSL Configuration`\n\n`[chunk content]`  \n\n\nThis adds a few tokens but improves retrieval a lot.\n\nSurprised this isn‚Äôt common. Does anyone know a builder that already supports hierarchy-aware chunking?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qhxtt2/chunking_without_document_hierarchy_breaks_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0ng5eg",
          "author": "janus2527",
          "text": "Docling",
          "score": 3,
          "created_utc": "2026-01-20 11:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nit6k",
          "author": "Live-Guitar-8661",
          "text": "Totally agree. Also just breaking for chunks wherever.\n\nWe do hierarchy, smart chunks, expanded context, etc. I would love some beta users to test\n\nhttps://orchata.ai",
          "score": 3,
          "created_utc": "2026-01-20 12:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0njsv5",
              "author": "Upset-Pop1136",
              "text": "Thanks I would love to check it.",
              "score": 2,
              "created_utc": "2026-01-20 12:11:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0nnqsc",
                  "author": "Live-Guitar-8661",
                  "text": "Free to sign up, just go to https://app.orchata.ai/signup, let me know what you think!",
                  "score": 1,
                  "created_utc": "2026-01-20 12:39:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0no99r",
          "author": "SiebenZwerg",
          "text": "why before embedding?  \nI thought of this as well but I would have saved the document and section as metadata and provided it as additional context during retrieval so that i don't have 1000 chunks with similar lines at the start.",
          "score": 3,
          "created_utc": "2026-01-20 12:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ntdfy",
              "author": "DotPhysical1282",
              "text": "Agree, what are the benefits of running through it before embedding?",
              "score": 2,
              "created_utc": "2026-01-20 13:15:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0q29ld",
              "author": "Clay_Ferguson",
              "text": "But those similar lines of text at the start ARE part of the context of that chunk. What would be interesting is a hybrid approach where the 'category/metadata' for each 'chunk' is still linked (by relational DB field) to the chunk, but where the 'category/metadata' has IT'S OWN vector generated. So this means would be like having two semantic searches. First you identify things matching the high level category, and then once you narrow down you do semantic search on just the chunks (that don't have the metadata, or the duplicate lines at the start)\n\nI haven't yet done RAG myself, so I may be missing something.",
              "score": 1,
              "created_utc": "2026-01-20 19:44:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0naasd",
          "author": "One_Milk_7025",
          "text": "Yes this is not common but this does improve the retrieval quality a lot.. I use this but not sure any library support this or not.\nYou should keep this recursive depth limit so that the breadcumbs doesn't overflow.. good to see more people using this ‚úåÔ∏è",
          "score": 2,
          "created_utc": "2026-01-20 10:54:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nad4n",
          "author": "Final_Special_7457",
          "text": "I saw someone on YouTube talk about this",
          "score": 2,
          "created_utc": "2026-01-20 10:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ogzaa",
              "author": "rshah4",
              "text": "Maybe me, over at Contextual AI we do this and I have shared/shown this technique.",
              "score": 1,
              "created_utc": "2026-01-20 15:20:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ndpq5",
          "author": "Code-Axion",
          "text": "Https://hierarchychunker.codeaxion.com \n\nSee it in action\nhttps://youtu.be/czO39PaAERI?si=1t_J4NZYUcFU1m1E\n\nCheck this out",
          "score": 2,
          "created_utc": "2026-01-20 11:24:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0no4ae",
          "author": "seomonstar",
          "text": "any open source solutions . it sounds a good idea but a lot of promos in this thread.",
          "score": 2,
          "created_utc": "2026-01-20 12:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oqm4u",
          "author": "Ecstatic_Heron_7944",
          "text": "Yep, this is the same idea behind [https://www.anthropic.com/engineering/contextual-retrieval](https://www.anthropic.com/engineering/contextual-retrieval) (Sep 2024).  \nFrom the article:\n\n    original_chunk = \"The company's revenue grew by 3% over the previous quarter.\"\n    \n    contextualized_chunk = \"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\"",
          "score": 2,
          "created_utc": "2026-01-20 16:06:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg6iqv",
      "title": "RAG for excel/CSV",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg6iqv/rag_for_excelcsv/",
      "author": "user_rituraj",
      "created_utc": "2026-01-18 11:42:15",
      "score": 21,
      "num_comments": 23,
      "upvote_ratio": 0.97,
      "text": "I have been working on a personal project with AI. Majorly, it involves reading financial documents(more specifically, DCF models, MIS in Excel).\n\n\n\n\n\nI am using the Claude/GPT 5.1 models for my extraction agent (LLMS running in a Loop) and have in place chunking and indexing with Azure OCR and Azure Search (which provide indexing and searching).\n\n\n\nMeanwhile, PDF extraction is working better, but with Excel I am facing many issues where LLMs mix data, such as saying data is for FY21 when it is for FY22(after getting the chunk data) or not able to find the exact related chunks.\n\n\n\nThe problem is that, in Excel, it is very number-heavy (like a 100\\* 50 type table). Also, structurally, it is a finance document and is created by different individuals, so I really do not control the structures, so lots of spaces or themes, so it is really not like CSV, where columns and rows are well defined.\n\n\n\nMajor Problem:\n\n1. By chunking the data, it loses a lot of context, like headers or information is missing if a single table is divided into multiple chunks, and hence, the context is missing, like what that column is like, the year, and the type.\n\n2. If I keep the table big, it is not going to fit sometimes in context as well.\n\n\n\n3. Also, as these tables are mostly number-heavy, creating chunks really does not make sense much (based on my understanding, as in vector embedding, the number itself does not have much context with text).\n\n\n\n\n\nPlease suggest if someone has worked with Excel and what has helped them to get the data in the best possible way.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qg6iqv/rag_for_excelcsv/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0a0s9n",
          "author": "Sunchax",
          "text": "I usually make a code \"sub-agent\" that can use pandas or such libraries to interact with the excel/csv.\n\nThis means that it can generate exact queries to aggregate, look up, or otherwise extract and manipulate excel data that is even in large files.",
          "score": 9,
          "created_utc": "2026-01-18 12:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0arutp",
              "author": "babygrenade",
              "text": "Second this. Llms can understand text but not really data files.",
              "score": 5,
              "created_utc": "2026-01-18 14:57:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0b1fp7",
              "author": "SuedeBandit",
              "text": "Yup. Subagents.\nPlanner / prompted analyst\nFunction caller with mcp\nGuardians all around\nVL validator\nDeterministic inputs and outputs",
              "score": 2,
              "created_utc": "2026-01-18 15:45:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b8t6c",
                  "author": "Sunchax",
                  "text": "Agree, except I usually try to avoid MCP.",
                  "score": 1,
                  "created_utc": "2026-01-18 16:20:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0bchij",
                  "author": "user_rituraj",
                  "text": "I would love to do this but the challenge is to have the proper data in a structured way in excel. \n\n\nBut unfortunately, table and header detection is an endless rabbit hole of a problem. There are an infinite number of ways a human can muck up an Excel document to make it troublesome to ingest. Common examples are multi-row headers, merged cells within headers, multiple tables in a single sheet separated by blank rows/columns, etc....\n\n\nSo the issue with this is:\n\nI cannot make this directly into tabular data and use agent to.find it and at the same time creating chunks for indexing by ocr has its problem of context loss as data is number heavy so its embedding does not make sense in itself.\n\n\nProbably i will have to create a structured context out of it with proper headers (a processing layer to convert it but i am really not sure how hard this is going to be)so that we can pass headers to read the data using some excel agent.",
                  "score": 1,
                  "created_utc": "2026-01-18 16:38:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0b8d6g",
              "author": "Sunchax",
              "text": "Might add that during ingestion one can uses similar sub-agents to explore such data to write a description of what's in it. This makes it possible to fetch such descriptors during search and let the orchestrator know if the right call is to ask a sub-agent to further explore the file.",
              "score": 2,
              "created_utc": "2026-01-18 16:18:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0cwvgp",
              "author": "lupin-the-third",
              "text": "This is what I do do as well, but when indexing the files initially I have a data threshhold where if there are less then like 50 rows or something I will index it rather then include it in the code subnet.",
              "score": 2,
              "created_utc": "2026-01-18 21:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ebfbw",
                  "author": "Sunchax",
                  "text": "Smart! I like that idea, might test it out =)",
                  "score": 1,
                  "created_utc": "2026-01-19 01:29:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a1d9h",
          "author": "jesus_was_rasta",
          "text": "Worked a lot last year on the matter. Reading Excel is a fucking nightmare. Ones that are basically flat tables: you can arrange something that converts them as a database tables then implement text to SQL to get correct data. You have to enrich data with context, as many times column names doesn't say much. \n\nThen, you have to work on other Excels, those that are more fancy with graphics, informations boxes in many tables per sheet here and there. Those ones better to scan as a whole, like concerting them to PDFs than use Docling",
          "score": 3,
          "created_utc": "2026-01-18 12:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0begde",
              "author": "user_rituraj",
              "text": "Most of my exercises are going to be the second type with graphics, spaces and what not.\n\nScanning was my first step to go but as excel is very heavy and tables are also huge so after scanning, i am dividing it into chunks but i see lots of issues with this.\n\n1. Because of number embedding search or semantic search is really not working.\n2.Also with chunking, if we split, usually column names are at the top so that context gets lost in the table.",
              "score": 1,
              "created_utc": "2026-01-18 16:47:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0gan3o",
          "author": "lost_soul1995",
          "text": "I had similar project. \n- chunks per table\n- summary per table\n- embed summary\n- retrieve results based on summary. Feed the retrieved summary and table together.\n- context should mention about quarterly, yearly, monthly terminologies.\n- Use reranking model\n- Hybrid retrieval (B25 plus vector) introduced dirty context as B25 would bring in irrelevant chunks such as revenue repeated in multiple chunks.",
          "score": 2,
          "created_utc": "2026-01-19 10:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdnwt",
              "author": "user_rituraj",
              "text": "This is exactly what i'm doing as of now and i believe this is good at the early stage of the solution. Eventually, i will have to do the pre processing of data to make it like csv (with proper columns in a plane in a nice table) and then give the column structure to the agent to work on csv.\n\nAs these tables can be huge and passing all these numbers to agent context is going to worsen the agent itself.",
              "score": 1,
              "created_utc": "2026-01-20 06:02:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a68nj",
          "author": "Durovilla",
          "text": "How many CSVs do you have? is there an inherent structure to them? if so, you could use DuckDB for text2SQL across all your files without having to embed them",
          "score": 1,
          "created_utc": "2026-01-18 12:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bfwj5",
              "author": "user_rituraj",
              "text": "Usually 4-5 excels with multiple sheets into it.\n\nSo it's more like a MIS document.\n\nBut excels are graphic heavy and also have multiple columns name and there are an infinite number of ways a human can muck up an Excel document to make it troublesome to ingest. Common examples are multi-row headers, merged cells within headers, multiple tables in a single sheet separated by blank rows/columns, etc....",
              "score": 1,
              "created_utc": "2026-01-18 16:54:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a9v1f",
          "author": "IdeaAffectionate945",
          "text": "[Magic Cloud](https://ainiro.io) (my project) has strong support for both PDF files and CSV files. You might want to check it out ...",
          "score": 1,
          "created_utc": "2026-01-18 13:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0aqcw6",
          "author": "Anth-Virtus",
          "text": "Llama Cloud offers specifically an OCR/document parsing pipeline especially for spreadsheets.",
          "score": 1,
          "created_utc": "2026-01-18 14:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bf7sl",
              "author": "user_rituraj",
              "text": "Interesting, will check.\n\nAlso if you have already tried it out, do let me know your experience.",
              "score": 1,
              "created_utc": "2026-01-18 16:51:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0atu2q",
          "author": "Much-Researcher6135",
          "text": "1. A DCF model's output will be derived from raw financial report data which is highly structured. Import that, then rerun the DCF model in Python, inserting both into the database.\n\n2. You should NOT push tabular data into a tabular database as a chunked strings for vector search. You should push it *as tabular data*, then give his agent the schema (table layouts in the db) and let it author and run queries. LLMs are great at SQL once they understand your schema!",
          "score": 1,
          "created_utc": "2026-01-18 15:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ben1k",
              "author": "user_rituraj",
              "text": "https://www.reddit.com/r/Rag/s/VajxL7WS47",
              "score": 1,
              "created_utc": "2026-01-18 16:48:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cqnzh",
          "author": "College_student_444",
          "text": "Turn excel data into sentences. Then split and index.",
          "score": 1,
          "created_utc": "2026-01-18 20:35:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qga7m6",
      "title": "Claude RAG Skills : 4 open-source tools to optimize your RAG pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qga7m6/claude_rag_skills_4_opensource_tools_to_optimize/",
      "author": "Responsible-Radish65",
      "created_utc": "2026-01-18 14:38:44",
      "score": 21,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "I've been using these internally for 3 months while building our RAG platform. Just cleaned them up for public release.\n\n**The 4 skills:**\n\n* `/rag-audit`¬†‚Üí Scans your codebase, flags anti-patterns, gives you a score out of 100\n* `/rag-scaffold`¬†‚Üí Generates 800+ lines of production-ready boilerplate in seconds\n* `/chunking-advisor`¬†‚Üí Decision tree for optimal chunk size based on your document types\n* `/rag-eval`¬†‚Üí Retrieval metrics (recall, MRR, NDCG) + optional benchmark against our API\n\n**Concrete results:**\n\n* Debugging sessions cut from 2h to 30min (the audit catches recurring mistakes)\n* Scaffold saves \\~15k tokens per new project setup\n* Chunking advisor prevented me from using 512 tokens on legal documents (bad idea)\n\nMIT licensed, no signup required:¬†[https://github.com/floflo777/claude-rag-skills](https://github.com/floflo777/claude-rag-skills)\n\nFeedback welcome, especially if you spot missing anti-patterns.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qga7m6/claude_rag_skills_4_opensource_tools_to_optimize/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qh5kls",
      "title": "Very confused on the optimal approach for generating knowledge-graphs for use with RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qh5kls/very_confused_on_the_optimal_approach_for/",
      "author": "boombox_8",
      "created_utc": "2026-01-19 14:27:40",
      "score": 17,
      "num_comments": 16,
      "upvote_ratio": 0.95,
      "text": "Hey guys! I am new to the world of knowledge graphs and RAGs, and am very interested in exploring it!\n\n  \nI am currently looking at using property graphs (neo4j to be specific) as the 'knowledge base' for RAG implementations since I've read that they're more powerful than the alternative of RDFs\n\n  \nWhat confuses me is about how one should go about generating the knowledge graph in the first place. neo4j's own blog and various others propose using LLMs to extract the data for you, and construct a JSON/csv-esque format which is then ingested to create the knowledge graph\n\n  \nExcept it feels like I am poisoning the well here so to speak? If I have tons of text-based documents as my corpora, won't using LLMs to do the job of data extraction and graph generation have issues?\n\nOff the top of my head, I can think of the following issues:\n\n1) The LLM could generate duplicates of entities across documents/chunks (For example, the word \"White House\" is present in a bunch of various documents in various levels of described detail? The LLM could very well extract out multiple such 'White House' entities\n\nI did have an idea of pre-defining all entity types and relations and forcing he LLM to stick with that, as well do an NLP-based deduplication technique, though I am not sure if it'll work well\n\n  \n2) The LLM could just up and hallucinate up data. Bad for obvious reasons, since I don't want a garbage in = garbage out problem for the resultant rag\n\n  \n3) It could just generate wonky results with incorrect 'syntax'. Bad for obvious reasons\n\n  \n4) Manually extracting data and writing the appropriate CYPHER queries? Yeah, won't work out feasibly\n\n5) Using an NLP-based entity and relation extractor? Faster and cheaper compute-wise, but the duplication issue still remains. It does solve issue 3)\n\nWith all these issues comes the extra issue of validating the output graph. Feels like I'm biting off more than I can chew, since all of this is VERY hard to pack into a pipeline unless I make my own bespoke one for the domain I am focusing on. Is there a better way of doing things?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qh5kls/very_confused_on_the_optimal_approach_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0hkqew",
          "author": "Popular_Sand2773",
          "text": "Knowledge graphs definitely aren't for the faint of heart. The reason everyone recommends using llms to generate the graph is because the alternatives involve significantly more effort. Stanford maintains the CoreNLP library which you can use and is deterministic so no hallucination risk. It's very easy to try out and once you see it's default hot mess you'll see why people risk it with llms.  \n  \nNowadays the risk of improper extraction is fairly low with the llm but the cost is obviously very high. For many RAG setups retrieval corpus's are large and queries are sparse. This can mean you are paying to have a llm generate a mostly unused graph which can hurt at scale.  \n[](https://alb.reddit.com/cr?za=ierAezmqakjkPaVDWKzCA3tCEIV4aCCC0bCRBCF5670AiSndpd2SMNGrxPD7pIs6QF_4_YGz223SK1eDI9NQaF3Bknih3JiBtO6-hq5KNJJm-M01GGJkGTYSSFvK9a--Ar68SJTzwWNl4P85wKLFZdf16OlAlcuEW3bQ8uXOYDazxjJfRvbl32NUYNgzANollM-JxDrxA9w5Y12eNTUmO27Ryo87KWbIfQIfJGpcpepJNJAFeDTsF2g3zJADNX9DoiMdVapRNgCzoQJ7qVHCxGrTGszYBwTzOhewPPUg97EFuriNOiSlKB8mgqfSm7Y6VBddMqH2m3j8H5TviyJQqm9pkBWo1-bQzRQvzseTDrb640e94Gk5GLcfiwM2rq0HtTVmaCwYyVGZ3xCSnsAe9uGOTYk9_rGGRR-AacQb5Mt_CAbs4yf7K4tOwLzW0Gf54Qf2tO_sJQF39wH8Ze8qdBof95qtUiZTkvswn5SQCJ_mUXzAHEUmOh-XA2Sl598ax2cyd-oM1T3oPMMf7m85Q99cXlzhY_Fp23SrxgpqwblimxUIWyD84KxrrgRU2ZpYBN3gxww2U4FwY6Rij4hIWQ7s-xTHsBcaSUSThgeUcd4XuZj5OATt&zp=EhBBaNqSZnW-BP4gOIznz82pQlrRYlfvaCNcyBIhs7wTVfWQDkGmlXivkNxEMtVXgXrymn3Wp2-145A4MY8bevLzVp1T2C9RxQn5ZonalUJpLtqYJENT7XBECPofmqZoSYN6dRcQs05mbKpeONZg3gNhIuibc7o_A9zb46C_7iccXXMOH2yC0cwG6bnT-efJDN_tqMYjKd2etZfrO3Y8GCROkZrxPUnmC--M3iUkc79mq0cOkoRo2W8LVfPMGq6mhq1oAyQ_hbSV6sOcq946_PtUYjPmN3kr4meSycYg_NXUQQfmxr99tsjBBv25tnzoWhw3hJHyQf_N6XQMXqpOhRCRm55i8xSv3PhZYuijg7qfIfRZ5LPyr2gvl21sBitjPlEmA2UuICjDB7Kypxrts7lNzRc-eCtDEuPDpGBHV3KpsXB30tBvlbVpNyhx2uMiFpeS4c3pn-Dii9nkkcviACMwawKXj0mue16rI3_gKxMWYnnStQ&a=598359&b=565262&be=383828&c=382514&d=568613&e=378211&ea=379178&eb=353934&f=352953&r=10&g=1&i=1768835127842&t=1768835729084&o=1&q=1&h=212&w=732&sh=864&sw=1536)  \nYou also identified the other issue which is canonicalization. Assuming your many white houses are spelled the same then they are a shared node with multiple edges. This is desired behavior and they aren't true duplicates just shared entities. The problem is even a small typo without post processing will faultily create a new node.   \n  \nIf you really want knowledge graph like quality without all the extra hassle then knowledge graph embeddings are a great place to look.",
          "score": 5,
          "created_utc": "2026-01-19 15:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hwe31",
              "author": "callmedevilthebad",
              "text": "You are spot on about the cost. Interested to know if you found any low cost, good-quality solutions.",
              "score": 1,
              "created_utc": "2026-01-19 16:09:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ish77",
                  "author": "Popular_Sand2773",
                  "text": "I can probably point you in the right direction/to the right model family but would need to know a little more about the use case. For example if you have a reasonable closed world/taxonomy you can collapse the problem space. Knowledge graphs certainly have their place and purpose but it's more scalpel to embeddings machete. \n\nWithout more context for a case like this I would suggest knowledge graph embeddings as the sweet spot between cost quality and latency. DM me and I can get you access to an early model that's working well for me.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:33:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0j11s2",
              "author": "boombox_8",
              "text": ">If you really want knowledge graph like quality without all the extra hassle then knowledge graph embeddings are a great place to look\n\n  \nI am planning on using the KG for a problem with a very particular domain. Would it be right to assume that the constrained domain/problem space makes it more suitable to NOT go for a knowledge graph embedding and instead just go directly for an LLM-generated KG followed by manual validation and cleaning up from my side?\n\n  \nOr can the two methods be combined and used in a hybrid fashion?",
              "score": 1,
              "created_utc": "2026-01-19 19:11:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jaje0",
                  "author": "Popular_Sand2773",
                  "text": "That's actually a great question. There is no reason you can't do both as complements and if you are maintaining your own graph you can fine tune the embeddings on the graph for some really fun combo moves.\n\nFor a constrained domain/problem space you are in the best place knowledge graph wise because you can potentially handroll a cheaper and more reliable system than a pure llm with something like langextract. CIE closed information extraction models take a predefined schema + set of relationships and reliably pull what you need at much lower cost than an llm. For example Gliner2, UIE, REBEL(ish) can likely do the job cleanly without the coreNLP noise. You have some options.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0hrj27",
          "author": "ggone20",
          "text": "Depends on the data. Or if you build it right it dynamically creates new nodes, edges, and properties. Entities need to deduped so you‚Äôll manage an index. You also don‚Äôt want similar edges -> is_part_of, has_part‚Ä¶ same same. Feed the LLM your index of nodes and edges and properties‚Ä¶ tell her to use those when it makes sense or create new ones when it doesn‚Äôt. You‚Äôll also use the index as filter, which helps you search and traverse much faster as well.",
          "score": 1,
          "created_utc": "2026-01-19 15:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j1od4",
              "author": "boombox_8",
              "text": "By index, are you referring to having a pre-defined list of 'valid' relation names and entities, and feed it to the LLM, forcing it to stick to just those values unless it can't (in which case it creates new ones)?",
              "score": 2,
              "created_utc": "2026-01-19 19:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jbi7d",
                  "author": "Popular_Sand2773",
                  "text": "They are talking about canonicalization. You can constrain the llm but most people just map things back using heuristics like cosine or jaccard similarity as post processing cleanup before you move it into the graph.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:59:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhvzy7",
      "title": "Compiled a list of ùêöùê∞ùêûùê¨ùê®ùê¶ùêû ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhvzy7/compiled_a_list_of_ùêöùê∞ùêûùê¨ùê®ùê¶ùêû_ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨/",
      "author": "midamurat",
      "created_utc": "2026-01-20 08:52:51",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Been working on reranking for a while and kept finding info all over the place - different docs, papers, blog posts. Put together what I found in case it helps someone else.\n\n**What it includes:** \n\n* Code to get started quickly (both API and self-hosted)\n* Which models to use for different situations\n* About 20 papers - older foundational ones and recent stuff from 2024-2025\n* How to plug into LangChain, LlamaIndex, etc.\n* Benchmarks and how to measure performance\n* Live leaderboard for comparing models\n\nSome of the recent papers cover interesting approaches like test-time compute for reranking, KV-cache optimizations for throughput, and RL-based dynamic document selection.\n\nStill adding to it as I find more useful stuff. If you've come across resources I missed, feel free to contribute or drop suggestions.\n\n  \nGitHub: [https://github.com/agentset-ai/awesome-rerankers](https://github.com/agentset-ai/awesome-rerankers)\n\nHappy to answer questions about specific models or implementations if anyone's working on similar stuff!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qhvzy7/compiled_a_list_of_ùêöùê∞ùêûùê¨ùê®ùê¶ùêû_ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0n2y7y",
          "author": "Professional_Cup6629",
          "text": "thanks for the list!",
          "score": 1,
          "created_utc": "2026-01-20 09:47:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n32br",
              "author": "midamurat",
              "text": "glad if it helps :)",
              "score": 1,
              "created_utc": "2026-01-20 09:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0neqyl",
          "author": "No_Kick7086",
          "text": "Nice work",
          "score": 1,
          "created_utc": "2026-01-20 11:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nt2t8",
              "author": "midamurat",
              "text": "üôå üôå",
              "score": 1,
              "created_utc": "2026-01-20 13:13:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qibjv",
          "author": "McNiiby",
          "text": "How does this compare to MTEB/RTEB? Why are some of the more popular STOA models missing from the Agentset leaderboard like Llama, gemini, and Qwen3?",
          "score": 1,
          "created_utc": "2026-01-20 20:58:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg2hxo",
      "title": "PyTelos - Agentic RAG powered by Postgres pg_vector and pg_textsearch extensions",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg2hxo/pytelos_agentic_rag_powered_by_postgres_pg_vector/",
      "author": "Feisty-Assignment393",
      "created_utc": "2026-01-18 07:45:20",
      "score": 16,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I wanna introduce my side project \n\n[PyTelos](https://github.com/richinex/pytelos)\n\nIt is an Agentic RAG App with support for the major LLM providers. I built it as a side project to show a friend how Agentic RAG works. It uses the Postgres pg\\_vector and newly released pg\\_textsearch extensions. The pg\\_textsearch extension allows for BM25 relevance-ranked full-text search.\n\nIt also uses a durable execution library I wrote for distributed indexing.\n\nI'd like to hear your thoughts and feedback.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qg2hxo/pytelos_agentic_rag_powered_by_postgres_pg_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o09c14b",
          "author": "mtbMo",
          "text": "Nice one, might check it out. Any plans for a web-ui?",
          "score": 1,
          "created_utc": "2026-01-18 08:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09cgtw",
              "author": "Feisty-Assignment393",
              "text": "My web design skills are still sub-par so I can't say for now",
              "score": 1,
              "created_utc": "2026-01-18 08:21:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09cwph",
                  "author": "mtbMo",
                  "text": "Once my coding agents are up and running, i will give them a try to get something working",
                  "score": 2,
                  "created_utc": "2026-01-18 08:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qgbm8d",
      "title": "Which one is better for GraphRAG?: Cognee vs Graphiti vs Mem0",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qgbm8d/which_one_is_better_for_graphrag_cognee_vs/",
      "author": "Imaginary-Bee-8770",
      "created_utc": "2026-01-18 15:35:07",
      "score": 16,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "Hello everybody, appreciate any insights you may have on this\n\nIn my team we are trying to evolve from traditional RAG into a more comprehensive and robust approach: GraphRAG. We have a extensive corpus of deep technical documents such as  manuals and datasheets that we want to use to feed customer support agents.  \n  \nWe've seen there are a lot of OSS tools out there to work with, however, we don't know the limitations, ease-of-use, scalability and overall information about them. So, if you have a personal opinion about them and you've tried any of them before, we would be glad if you could share it with us.   \n  \nThanks a lot!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qgbm8d/which_one_is_better_for_graphrag_cognee_vs/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0bivda",
          "author": "astronomikal",
          "text": "Before jumping to tools, it might help to clarify what RAG is failing at for you. A few concrete questions that usually separate ‚Äúbetter RAG‚Äù from ‚Äúdifferent architecture‚Äù\n\nAre you mostly struggling with retrieval quality, or with reasoning across multiple documents?\n\nDo agents need to answer single-fact questions, or questions that require combining constraints across specs, revisions, and edge cases?\n\n When a customer issue is resolved, do you want the system to learn from that resolution, or is memory strictly static?\n\nDo answers need to be explainable/traceable beyond citations (e.g. why one constraint overrode another)?\n\nHow often do documents contradict or partially overlap (datasheet vs manual vs errata)?\n\nDo you need to model relationships (dependencies, incompatibilities, versions), or is chunk-level retrieval sufficient?\n\nIs latency predictability important (e.g. local-first, deterministic), or is cloud-scale recall the priority?",
          "score": 7,
          "created_utc": "2026-01-18 17:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b6cxn",
          "author": "ubiquae",
          "text": "Hey, looking for similar feedback.\n\nMy feedback regarding graphiti is that it is a good starting point but it still lacks core capabilities.\n\nI am waiting for a PR to be reviewed to be able to work with custom entities and relationships since it does not work out of the box. And of course quality depends a lot on how the graph is being modelled so I can't understand why they haven't solved it yet.\n\nAlso, there is no evaluation ready, so there is no way to test it and have at least a basic idea about how it performs.\n\nFinally, the MCP server is pretty basic. Useful for demos but not leveraging all their capabilities.\n\nCognee sounds nice but it seems even more half baked. They probably are pushing their cloud offering. So ideas are great, implementation is not still there.",
          "score": 2,
          "created_utc": "2026-01-18 16:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cz066",
              "author": "Unlucky_Comment",
              "text": "Also used graphiti and its a good starting point, but you grow out of it, which isn't a bad thing. Depending on your use case, you'll want to do something custom.",
              "score": 1,
              "created_utc": "2026-01-18 21:20:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hoesw",
          "author": "Popular_Sand2773",
          "text": "As others mentioned it's not always a flat upgrade just a different way of doing things that excels in certain use cases. One of the reasons I ended up moving away from these guys was latency. For example if your customer support use case is using voice with a dense graph the latency can spike aggressively leading to awkward pauses that make people scream \"human\". You'll see they all dance around the problem of graph latency at scale. \n\nIf you are looking for something with graph like quality/behavior but actual guaranteed low latency and ANN vector db speeds then I would recommend checking out knowledge graph embeddings.",
          "score": 2,
          "created_utc": "2026-01-19 15:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fvx7v",
          "author": "OnyxProyectoUno",
          "text": "Technical manuals and datasheets are tricky for graph approaches because the relationships you care about aren't always explicit in the text. Part numbers reference other parts, specs depend on operating conditions, procedures assume prior steps. Before picking a tool, worth mapping out what relationships actually matter for your support use case.\n\nCognee handles entity extraction and relationship building pretty well out of the box. Graphiti is more focused on temporal/conversational memory, less suited for static technical docs. Mem0 is really about user-level personalization, probably not what you need here.\n\nHave you validated that graph structure actually helps your retrieval? Sometimes dense technical docs benefit more from better chunking that preserves table structure and cross-references than from full knowledge graphs. What's failing with your current RAG setup that's pushing you toward graphs?",
          "score": 1,
          "created_utc": "2026-01-19 07:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mx9v7",
          "author": "334578theo",
          "text": "If you can‚Äôt make an objective decision on this yourself then you probably don‚Äôt need GraphRAG",
          "score": 1,
          "created_utc": "2026-01-20 08:53:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgh5nw",
      "title": "RAG Discovery Framework. It's a checklist of what to ask the client before writing any code",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qgh5nw/rag_discovery_framework_its_a_checklist_of_what/",
      "author": "not-so-boring",
      "created_utc": "2026-01-18 19:02:15",
      "score": 14,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "I've found this on Linkedin and it goes on about the importance of business understanding before building any project.\n\nIt's 42 items that you can map out to a decision matrix for the architecture.\n\nThe article with more details:¬†[https://thehyperplane.substack.com/p/why-the-hell-should-i-build-this](https://thehyperplane.substack.com/p/why-the-hell-should-i-build-this)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qgh5nw/rag_discovery_framework_its_a_checklist_of_what/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qcjvn2",
      "title": "RAG BUT WITHOUT LLM (RULE-BASED)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "author": "adrjan13",
      "created_utc": "2026-01-14 10:27:43",
      "score": 13,
      "num_comments": 15,
      "upvote_ratio": 0.88,
      "text": "Hello, has anyone here created a scripted chatbot (without using LLM)? \n\nI would like to implement such a solution in my company, e.g., for complaints, so that the chatbot guides the customer from A to Z. I don't see the need to use LLM here (unless you have a different opinion‚Äîfeel free to discuss). \n\nHas anyone built such rule-based chatbots? Do you have any useful links? Any advice? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzimms7",
          "author": "redditorialy_retard",
          "text": "that's building the retrieval but replace LLM with scripts. IE having relevant article 1, 2, 3 (retrieval results) and having the chat bot recommend those.¬†\n\n\nIe chatbot ask what is the problem, user input then is parsed to use as a query to the RAG.\n\n\nBut honestly it's just easier to put an LLM in the middle if you don't want to expose it to the end user.",
          "score": 5,
          "created_utc": "2026-01-14 10:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjegum",
          "author": "PrepperDisk",
          "text": "Tested this out with Haystack just doing retrieval, but then feeding into an LLM to rephrase the chunks.\n\nMy issue with this was user expectation.  With a simple search box, users know to search on keywords.  With a conversational interface, users increasingly expect to be able to phrase requests like they do with Gemini or ChatGPT  (with conversations and context from last request).\n\nMy solution was in a kind of \"uncanny valley\" where it was neither and users got stuck.",
          "score": 3,
          "created_utc": "2026-01-14 13:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzk7hvg",
          "author": "Elses_pels",
          "text": "Have you tried RASA ?",
          "score": 3,
          "created_utc": "2026-01-14 16:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjnesz",
          "author": "irodov4030",
          "text": "there are 100s of applications where you can use retrieved chunks in a workflow and do not need LLM to package the response. Your solution will be more deterministic than a typical RAG with LLM\n\nI have built a similar custom solution.\n\nYou are going in the right direction.\n\nLet me know if you have any specific questions.",
          "score": 2,
          "created_utc": "2026-01-14 14:40:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq2cf7",
              "author": "ghaaribkhurshid",
              "text": "Hello, I'm a fresher in CS, I want to build career in AI, could you please guide me more on this?",
              "score": 1,
              "created_utc": "2026-01-15 13:24:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzio6rc",
          "author": "Necessary-Dot-8101",
          "text": "compression-aware intelligence (CAI) is useful bc it treats hallucinations, identity drift, and reasoning collapse not as output errors but as structural consequences of compression strain within intermediate representations. it provides instrumentation to detect where representations are conflicting and routing strategies that stabilize reasoning rather than patch outputs",
          "score": 1,
          "created_utc": "2026-01-14 10:50:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzipyni",
          "author": "trollsmurf",
          "text": "How would you build consistent responses from chopped up content chunks otherwise?\n\nIf you consistently chunk on chapters/sections and provide that whole section as a response it would work, but it's not quite the same thing.\n\nInterested in knowing how commercial support-related chatbots handle this.",
          "score": 1,
          "created_utc": "2026-01-14 11:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjn5h4",
          "author": "vdharankar",
          "text": "So basically you just want to pull the chunks and show user ? Base idea of RAG is generation with augmentation",
          "score": 1,
          "created_utc": "2026-01-14 14:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjq12n",
          "author": "cubixy2k",
          "text": "So the standard intent based chat bots like Alexa skills?",
          "score": 1,
          "created_utc": "2026-01-14 14:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkj4cl",
          "author": "Alternative_Nose_874",
          "text": "You may consider [botpress.com](http://botpress.com) or similar open source platform as the backend for easy setup.",
          "score": 1,
          "created_utc": "2026-01-14 17:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw8el0",
          "author": "TechnicalGeologist99",
          "text": "This is a classification problem. \n\nIngest text to something like XLM Roberta. Fine tune to classify the failure modes (or modes of complaint) that you have identified in your taxonomy.\n\nAt run time, the model predicts the label and the label triggers whatever text is associated with that problem",
          "score": 1,
          "created_utc": "2026-01-16 10:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01bos7",
          "author": "HealthyCommunicat",
          "text": "I mean isnt this just classic route of ‚Äúassign keywords to docs, use query keywords to mass scan and find matching docs?‚Äù\n\nSo you mean a knowledgebase?\n\nYou must be thinking of RAG as the object that the ‚Äúknowledgebase/library‚Äù part that the LLM is attached to - but RAG actually means the process of taking information, usually a large number of separate docs, and then having the LLM read and use those tokens to generate an output. You‚Äôre overthinking it. You just want a knowledgebase.\n\nEasily put, RAG is a process not an object, the G for generation should tell you that much.\n\nHow are you going to make a chatbot that can possibly really detect every possible combination of keywords and make sure that it pulls up the right docs? Are you sure its not just the fact that you‚Äôre really really overthinking LLM‚Äôs and you would much rather not just download LM Studio + Anything LLM and drag and drop all your docs? Or is it because you can‚Äôt afford it? Solutions are usually made entirely to fix a problem, and noone can help solve your problem unless we know what  it even is.\n\n122 days ago you posted a thread related to RAGs. During that 122 days you have not still even understood what a RAG even is. It‚Äôs pretty easy to tell you just gave up when if you were to go and ask Gemini, it would give you exact step by step instructions on how to set this up, you can even really use like a 1-4b model on a 10 year old laptop, and Gemini can put the steps in a way that even a child can understand if you simply say ‚Äúexplain the steps like you‚Äôre explaining to a kid‚Äù.\n\nIt doesn‚Äôt matter what answer here someone gives you. You‚Äôve shown that you will pretend to care and want to know something but still won‚Äôt make any progress 122 days later. The biggest group of people I despise are those that say they ‚Äúwant to learn something‚Äù when they really really don‚Äôt care in the slightest - it just makes people who actually want to learn get taken unseriously.",
          "score": 1,
          "created_utc": "2026-01-17 02:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a5ds8",
          "author": "Gazorpazzor",
          "text": "Have you tried Rasa, Tok, ‚Ä¶ or other rule based frameworks ? You still need a classifier for intent detection tho",
          "score": 1,
          "created_utc": "2026-01-18 12:39:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mjs8m",
          "author": "rookie-bee",
          "text": "Without an LLM, the chunk maybe relevant but an LLM is needed to frame an anwer from the chunk. Just pulling the chunks is retrieval, the R of RAG, after retrieval there needs to be the AG, i.e Augmented Generation, which needs an LLM. Here is an easy way to create a [RAG based chatbot](https://predictabledialogs.com/learn/openai/website-chatbot-implementation)",
          "score": 1,
          "created_utc": "2026-01-20 06:52:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzin0qd",
          "author": "Bozo32",
          "text": "I used a combination of cosine similarity and bm25 to filter the obviously irrelevant -> reranker -> NLI and then presented results to the user...have them say yes or no. This could iterate where you use the 'no' answers as a filter to rerank results  \nover time you would accumulate an evidence base of   \n'query' 'rejected resources' 'chosen resource'  \nthat would be useful for future searches  \nthe work done by folks at Utrecht university on screening abstracts for systematic review may be helpful  \n[https://asreview.nl/install/](https://asreview.nl/install/)",
          "score": -1,
          "created_utc": "2026-01-14 10:40:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg17pf",
      "title": "I cut my Claude Code costs by ~70% by routing it through local & cheaper models",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg17pf/i_cut_my_claude_code_costs_by_70_by_routing_it/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-18 06:31:55",
      "score": 12,
      "num_comments": 6,
      "upvote_ratio": 0.88,
      "text": "I love Claude Code, but using it full-time was getting expensive.\n\nSo I built **Lynkr**, a proxy that lets me:\n\n* Route some prompts to local models\n* Fall back to stronger models only when needed\n* Cache repeated prompts automatically\n\nResult: \\~60‚Äì80% lower costs depending on workload.\n\nIt‚Äôs open source and self-hosted:\n\n[https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr?utm_source=chatgpt.com)  \nIf you‚Äôre juggling multiple LLM providers, this might be useful ‚Äî feedback welcome.\n\nIt also supports Codex cli, [continue.dev](http://continue.dev), cursor pro, Cline etc",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qg17pf/i_cut_my_claude_code_costs_by_70_by_routing_it/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0e09ms",
          "author": "Economy-Manager5556",
          "text": "I spend less on Claude code by using it less... \nI gotta try that",
          "score": 3,
          "created_utc": "2026-01-19 00:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0f389j",
              "author": "Dangerous-Dingo-5169",
              "text": "Lol üòÇ",
              "score": 1,
              "created_utc": "2026-01-19 04:05:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nhzuk",
          "author": "alokin_09",
          "text": "What local models are you running? And if I'm getting this right, you're still in the CC CLI but using local models?\n\nI've been using Kilo Code for most of my coding stuff (actually contributing to the project and working with their team), and it supports local models through Ollama and LM Studio. I use that sometimes when I wanna keep costs down. So how does your tool differ from that setup? Also curious if it could work with Kilo.\n\nCool idea btw :)",
          "score": 1,
          "created_utc": "2026-01-20 11:57:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p4mf2",
              "author": "Dangerous-Dingo-5169",
              "text": "Thanks alokin_09\nThis project also supports models hosted via ollama, llama.cpp and LM studio \nYes you would still be using cc cli or any other supoort ai tool ui itself\nAnd this project has a feature where the request automatically reroutes to a better model besides local llms based on the complexity of the question or if the local llms failed to answer it \nTo answer your question about kilocode \nI have not personally looked into it\n If its open ai compatible \nThen yes lynkr should support it otherwise we need to make the corresponding code changes \nCan you please raise a feature request issue on the repository \nThank you üòä",
              "score": 1,
              "created_utc": "2026-01-20 17:10:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0plw2n",
              "author": "Dangerous-Dingo-5169",
              "text": "I just checked the documentation of kilocode and from the looks of it its openai compatible so lynkr should be able to work with kilocode",
              "score": 1,
              "created_utc": "2026-01-20 18:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0rgq0z",
          "author": "Ok-Responsibility734",
          "text": "Have you tried Headroom? https://github.com/chopratejas/headroom ?",
          "score": 1,
          "created_utc": "2026-01-20 23:49:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qep0ap",
      "title": "Web pages are best performing sources in RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qep0ap/web_pages_are_best_performing_sources_in_rag/",
      "author": "pskd73",
      "created_utc": "2026-01-16 19:04:32",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I found that the web pages perform a lot better in RAG as a quality sources. The reason is, they are mostly already divided by topic, example, installation, api-fetch, api-update etc. In semantic search it is important for a chunk to be of a specific topic, if a chunk covers multiple topics, the chances that the chunk getting low scores is very high.\n\nBecause of the same reason, I have observed a very consistent pattern. The landing pages generally perform poor because they cover all the topics.\n\nSo chunking is a very an important process and web pages inherently have an advantage. Anybody has similar approach for files, pdfs etc?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qep0ap/web_pages_are_best_performing_sources_in_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzzk6d5",
          "author": "hrishikamath",
          "text": "Fortunately sec filings had hierarchical structure so I divided it into sections and did it for finance. Can check the details here: https://github.com/kamathhrishi/stratalens-ai/tree/main/agent",
          "score": 2,
          "created_utc": "2026-01-16 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01y2mn",
              "author": "Much-Researcher6135",
              "text": "Wait, you're scraping SEC filings? That's a freaking GREAT idea! People pay good money for those data. But with the advent of LLMs, this process should become open. Well done!",
              "score": 1,
              "created_utc": "2026-01-17 04:44:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08npjq",
                  "author": "hrishikamath",
                  "text": "Haha, yeah. Parsing isn't particularly challenging, datamule is a good library for that.",
                  "score": 1,
                  "created_utc": "2026-01-18 04:56:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02ry1t",
          "author": "Rokpiy",
          "text": "web pages win on single-topic constraint - most docs are multi-topic because they're reference material, not explanatory content. the closer your source chunks to 'one concept per document' the less you fight retrieval.",
          "score": 1,
          "created_utc": "2026-01-17 08:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07xr6r",
          "author": "maschayana",
          "text": "Lmao",
          "score": 1,
          "created_utc": "2026-01-18 02:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hemaf",
          "author": "IdeaAffectionate945",
          "text": "Interesting, but it actually makes sense. I've notice that high quality SEO produces better quality ...",
          "score": 1,
          "created_utc": "2026-01-19 14:45:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qf66yi",
      "title": "Need advice: Best RAG strategy for parsing RBI + bank credit-card documents?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qf66yi/need_advice_best_rag_strategy_for_parsing_rbi/",
      "author": "Infinite_Bat_7008",
      "created_utc": "2026-01-17 06:46:35",
      "score": 11,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm building a RAG-based chat agent that explains and validates credit-card terms (payment cycle, fees, interest, etc.) using **only RBI circulars + official bank T&C PDFs**.\n\nThese documents have messy formatting (tables, multi-column text, long clauses), so I‚Äôm struggling to choose the right **parsing, chunking, and embedding** approach.\n\nIf you‚Äôve built RAG for legal/compliance/financial docs, what worked best for you?  \nLooking for practical tips on:\n\n* PDF parsing tools\n* Chunking strategy that preserves clause meaning\n* Embedding models that handle regulatory text well\n* Retrieval tricks to reduce hallucination\n\nWould love any real-world advice or workflows you‚Äôve used.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qf66yi/need_advice_best_rag_strategy_for_parsing_rbi/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o05k1nz",
          "author": "CarefulDeer84",
          "text": "for financial docs with weird layouts, honestly the parsing tool makes a huge difference. i've found that using something like Apify's PDF parsers combined with custom scripts handles tables and multi-column stuff way better than generic tools. the trick is getting the structure right before you even start chunking.\n\nafter that, i'd suggest going with semantic chunking instead of fixed-size chunks because regulatory clauses need to stay intact. we worked with Lexis Solutions on a similar project where they used VoyageAI embeddings with ChromaDB for a financial platform, and their approach to keeping context within chunks actually reduced our hallucination issues significantly. they built custom preprocessing pipelines that preserved the clause relationships.\n\ntry to keep your chunks around meaningful sections like \"interest calculation methods\" or \"payment terms\" rather than arbitrary character limits. makes retrieval way more accurate.",
          "score": 5,
          "created_utc": "2026-01-17 19:03:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05l4mz",
              "author": "Infinite_Bat_7008",
              "text": "hm it seems good a idea to apply semantic chunking instead of fixed size in financial doc..ill keep that in mind ans ill also experiment with that Apify's PDF parser \nThanks!üòä",
              "score": 1,
              "created_utc": "2026-01-17 19:08:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0bv119",
              "author": "ArgumentNo2049",
              "text": "This is a shill bot comment.",
              "score": 1,
              "created_utc": "2026-01-18 18:05:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o049qwk",
          "author": "rv-ban",
          "text": "FWIW, my approach is to always parse the pdfs to markdown using olmocr. It will extract the tables from the pdf as html tables accurately including colspans/rowspans. As u/OnyxProyectoUno mentioned these documents were produced by humans for humans, so weird table layouts are the rule not the exception. It seems that tables are mostly used for layout and not for logically grouping data. For really important stuff I had to touch the tables manually and split them logically, regroup the information on some columns and so on. \n\nI also noticed that expressing logic in natural language can be difficult and incomplete - that is one of the problems solved by programming languages, BTW. So I had to help the LLM by embedding HTML comments in the document with more detailed explanations to nudge it in the right direction in how they interpret and apply them. Of course, this approach doesn't scale, but there you go. I'm looking forward to see how others have dealt with these issues. The big names in AI seem to mostly ignore these heavily structured documents from their training data.",
          "score": 4,
          "created_utc": "2026-01-17 15:26:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o04dog2",
              "author": "OnyxProyectoUno",
              "text": "How does Olmocr compare to docling and other parsers? What's the failure rate on tables? You're giving me something to research.",
              "score": 2,
              "created_utc": "2026-01-17 15:45:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o04rivp",
                  "author": "rv-ban",
                  "text": "I don't know, never used them, I'm still a newbie's newb in this field. olmocr never failed me with the documents I parsed but they aren't scans of old printed documents. It deals accurately with complex tables, math, even tries to parse flowcharts. It correctly rotated portrait mode tables. Give it a go, download is free if you can run it locally.",
                  "score": 2,
                  "created_utc": "2026-01-17 16:51:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o065jbi",
                  "author": "bravelogitex",
                  "text": "[https://x.com/VikParuchuri/status/1988985863607300516/photo/1](https://x.com/VikParuchuri/status/1988985863607300516/photo/1)",
                  "score": 1,
                  "created_utc": "2026-01-17 20:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02rlkk",
          "author": "Rokpiy",
          "text": "preserving clause boundaries matters more than semantic chunking for regulatory text - split mid-clause and you'll generate responses that cite half a rule.",
          "score": 2,
          "created_utc": "2026-01-17 08:54:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02ruhx",
              "author": "Infinite_Bat_7008",
              "text": "Will recursive text splitting do the work?",
              "score": 1,
              "created_utc": "2026-01-17 08:56:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o09b40v",
          "author": "ampancha",
          "text": "The parsing and chunking choices matter, but the harder problem with compliance RAG is verification. When your agent explains a fee structure or payment cycle incorrectly, the failure mode is legal exposure, not just a bad user experience.  \nI'd prioritize retrieval with citation (return the exact clause IDs alongside answers) and build a test harness that checks known question/answer pairs against your source docs before every deploy. Happy to share more on the verification layer if useful.",
          "score": 1,
          "created_utc": "2026-01-18 08:08:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09bkdg",
              "author": "ampancha",
              "text": "Sent you a DM with more detail on the verification layer.",
              "score": 1,
              "created_utc": "2026-01-18 08:12:54",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o09jcun",
              "author": "explodedgiraffe",
              "text": "Could you share more on the verification layer?",
              "score": 1,
              "created_utc": "2026-01-18 09:25:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o02s3hc",
          "author": "OnyxProyectoUno",
          "text": "The issue is usually that regulatory PDFs are designed for human reading, not machine parsing. Tables get scrambled, multi-column layouts break chunking logic, and you lose critical relationships between clauses and their context.\n\nFor RBI circulars specifically, you need a parser that handles complex layouts without mangling the content. Docling works well for financial documents, but the real test is whether you can see what your parsed content actually looks like before chunking. Most teams discover their parsing is broken only after they've embedded everything and wonder why retrieval is inconsistent.\n\nChunking strategy matters more than embedding model choice here. Recursive chunking with overlap usually works better than semantic chunking for regulatory text because you want to preserve exact clause boundaries. Watch out for splitting mid-sentence or separating related subsections.\n\nFor embeddings, try sentence-transformers/all-MiniLM-L6-v2 first. It handles formal language well and you can always upgrade later. The bigger issue is whether your chunks actually contain complete, coherent information.\n\nWhat does your parsed content look like right now? Are tables staying intact and is section hierarchy preserved? I built [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_c) partly because debugging RAG without seeing your processed docs is like coding blindfolded.",
          "score": 0,
          "created_utc": "2026-01-17 08:59:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o02t4rc",
              "author": "Infinite_Bat_7008",
              "text": "Yupp,, I've used recursive text splitter...and about the parsed text section hierarchy is preserved but im not sure about table...\nThe problem I‚Äôm facing is that when a document has a table with multiple columns, the parsed output comes column-wise instead of row-wise. So a 2-column table gets flattened like:\ncol1_item1\ncol1_item2\ncol1_item3\ncol2_item1\ncol2_item2\ncol2_item3\ninstead of preserving the actual row structure.",
              "score": 2,
              "created_utc": "2026-01-17 09:08:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02wvdw",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, that's the classic table parsing nightmare. Most PDF parsers just extract text in reading order, which means they go down column 1, then column 2, completely destroying the row relationships that actually matter in regulatory docs.\n\nYou need a parser that does layout detection first, identifies table boundaries, then extracts by rows instead of columns. Docling handles this better than basic text extraction, but you should also look at pdfplumber or camelot if you're dealing with a lot of structured tables. The key is getting something that can detect table structure and preserve row integrity.\n\nQuick test: grab one of your problem tables and see if the parser can output it as actual structured data (like CSV or markdown table format) before it goes into your chunking pipeline. If it can't maintain row structure there, your chunks will be garbage no matter how good your embedding model is.",
                  "score": 0,
                  "created_utc": "2026-01-17 09:44:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcgoo8",
      "title": "need help embedding 250M vectors / chunks at 1024 dims, should I self host embedder (BGE-M3) and self host Qdrant OR use voyage-3.5 or 4?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "author": "zriyansh",
      "created_utc": "2026-01-14 07:06:24",
      "score": 10,
      "num_comments": 18,
      "upvote_ratio": 0.86,
      "text": "hey redditors, I am building a legal research RAG tool for law firms, just research and nothing else.\n\n  \nI have around 1.5TB of legal precedence data, parsed them all using 64 core Azure VM, using PyMuPDF + Layout + Pro. Using custom scripts and getting around 30 - 150 files / second parse speed. \n\nVoyage-3-large surpassed voyage-law-2 and now gemini 001 embedder is ranked #2 (MTEB ranking).  Domain specific models are now overthrown by general embedders. \n\nI have around 250 million vectors to embed, and even using voyage-3.5 (0.06$/mill token), the cost is around $3k dollars. \n\n  \nUsing Qdrant cloud will be another $500.\n\n  \nQuestion I need help with:\n\n1. Should I self host embedder and vectorDB? (for chunking as well retrival later on)  \n2. Bear one time cost of it and be hastle free? \n\n\n\nFeel free to DM me for the parsing and chunking and embedding scripts. Using BM25 + RRF + Hybrid search + Rerank using voyage-rank2.5, CRAG + Web Search. \n\n  \nCurrent latency woth 2048 dims on test dataset of 400k legal text vectors is 5 seconds. \n\nChunking by characters and not token.\n\n|Metric|Value|\n|:-|:-|\n|**Avg parsed file size**|68.5 KB|\n|**Sample text length**|2,521 chars (small doc)|\n|**Total PDFs**|16,428,832|\n|**Chunk size**|4,096 chars (\\~1,024 tokens)|\n|**Chunk overlap**|512 chars (\\~128 tokens)|\n|**Min chunk size**|256 chars|\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzi2930",
          "author": "aiprod",
          "text": "Do you have an eval set that would allow you to test recall etc. on a subset of the corpus? Should help with selecting the right model. I‚Äôd also look into getting those 2048 dims down. Will save you a lot on vector db costs and reduces latency. Five seconds seems very slow. How did you test that? Was it pure embedding retrieval or your full retrieval pipeline?",
          "score": 4,
          "created_utc": "2026-01-14 07:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2p88",
              "author": "zriyansh",
              "text": "I dont have an eval set just yet, working on that. This is qdrant telling me about the latency. Okay it improved from yesterday lol. \n\nhttps://preview.redd.it/7ki00keho9dg1.png?width=2378&format=png&auto=webp&s=83bb39f165ed1da3c98c4b0af0d3d162ea4b5706",
              "score": 1,
              "created_utc": "2026-01-14 07:26:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzi4ay7",
                  "author": "aiprod",
                  "text": "That looks better although some of the queries are still a bit slow. How are you running the qdrant cluster? Is that through their cloud offering?",
                  "score": 1,
                  "created_utc": "2026-01-14 07:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzi2gda",
          "author": "bravelogitex",
          "text": "able to embed across a large number of gpu instances? havent heard of that done before",
          "score": 1,
          "created_utc": "2026-01-14 07:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2r62",
              "author": "zriyansh",
              "text": "CPU\\*",
              "score": 1,
              "created_utc": "2026-01-14 07:26:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi3k45",
          "author": "ai_hedge_fund",
          "text": "This is interesting because you use all different units than I usually think in terms of\n\nWhen you say self host, what GPUs do you have or are you backing into a budget?\n\nThen it becomes a question of turnaround time\n\nIf the time isn‚Äôt an issue then I think you can do better than the $3K\n\nIf you want it done fast then $3K to $4K sounds about right if you rent GPUs / have quota in Azure\n\nTo me the cloud DB question depends on how users will access the data and when. I‚Äôd think you could defer that $500 now if it‚Äôs an issue and just store the vectors yourself.",
          "score": 1,
          "created_utc": "2026-01-14 07:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi8rqo",
              "author": "zriyansh",
              "text": "expecting around 50 users in a month, and 10 queries per user each day.\n\nyeah not using token because character is what I understand well, so it works for me. \n\nI have a budget for $1K for now as we dont have any customers, using my savings for this.\n\nAs far as I understanding, embedding and hosting a vector DB is CPU intensive not GPU (can be wrong here), I have 1k$ credit from Azure as I registered my startup with them (and linked my LinkedIn with them as well).   \n  \nIf we break even, I will want to use cloud services and focus on what we do best.",
              "score": 1,
              "created_utc": "2026-01-14 08:23:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nziamtn",
          "author": "mwon",
          "text": "I have also a side project in the field of legal AI, but with considerable lower size. My biggest index has about 11M vectors, size 1024, generated form a fined tuned BGE-M3.   \nI use Milvus with index in disk in  a dedicated server fom Hetzner, and my latency is bellow 0.5s.  \nI think is a bit odd your latency is 5s for only 400k vectors. You should check if everything is ok, because is too much. I also think chunks of 1024 is too much. You will l likely loose a lot of recall.",
          "score": 1,
          "created_utc": "2026-01-14 08:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzib92e",
              "author": "zriyansh",
              "text": "so its self hosted embedder I suppose, what kind of machine are you using? and anything I need to take care of here?",
              "score": 1,
              "created_utc": "2026-01-14 08:47:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzic6mi",
                  "author": "mwon",
                  "text": "Yes, self embedder. I never use embedding services. Very expensive for what they do and no better than many OS solutions that you can instantiate locally.   \n  \nI'm using one of theirs 64GB ram dedicated servers. They are very cheap, like 40-50 EUR/month. \n\nYou need to be careful with your benchmarks estimation. A sample of 400k vector is very small compared with your final production setup. Recall values will be very diferent with 250M vectors.",
                  "score": 1,
                  "created_utc": "2026-01-14 08:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nziqy8d",
              "author": "explodedgiraffe",
              "text": "I am also working on a side project of that size. I was thinking of using qwen 4b embedding and rerank. How was your experience with BGE-M3? I like the architecture of it (sparse/dense/multi vector) but their benchmarks weren't that impressive compared to dense of the same size. Also curious how you fined tuned it for your usecase.\n\n  \n5s must be caused by network latency from external rerank and web search calls?",
              "score": 1,
              "created_utc": "2026-01-14 11:14:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nziseix",
                  "author": "mwon",
                  "text": "qwen-4b vs BGE-M3 both out of the box, qwen likely wins. After all it is a 4B model against vs 0.5B. But you can give a good boost to BGE-M3 by finetuning it, which will give you a small model that does not need GPU for inference. The sparse part is also nice because it allows you to do hybrid with a sparse search in one go. Note however that from my experiences with BGE-M3, BM25 is still better than its sparse.",
                  "score": 1,
                  "created_utc": "2026-01-14 11:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzicnrv",
          "author": "UseMoreBandwith",
          "text": "how long does it take to process 1.5TB ?",
          "score": 1,
          "created_utc": "2026-01-14 09:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzicuih",
              "author": "zriyansh",
              "text": "around 3 days with 64 core CPU, but there exist faster parsers which can parse 4-5k documents per second with such beast machine but I wasn't able to run that properly, its a C implementation of pymupdf4llm-c",
              "score": 2,
              "created_utc": "2026-01-14 09:03:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qg2h8f",
      "title": "Why is codebase awareness shifting toward vector embeddings instead of deterministic graph models?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/",
      "author": "hhussain-",
      "created_utc": "2026-01-18 07:44:08",
      "score": 10,
      "num_comments": 18,
      "upvote_ratio": 0.81,
      "text": "I‚Äôve been watching the recent wave of **‚Äúcode RAG‚Äù** and **‚ÄúAI code understanding‚Äù** systems, and something feels fundamentally misaligned.\n\nMost of the new tooling is heavily based on **embedding + vector database retrieval**, which is inherently **probabilistic**.\n\nBut **code is not probabilistic ‚Äî it‚Äôs deterministic**.\n\nA codebase is a formal system with:\n\n* Strict symbol resolution\n* Explicit dependencies\n* Precise call graphs\n* Exact type relationships\n* Well-defined inheritance and ownership models\n\nThese properties are *naturally represented as a graph*, not as semantic neighborhoods in vector space.\n\nUsing embeddings for code understanding feels like **using OCR to parse a compiler**.\n\nI‚Äôve been building a **Rust-based graph engine** that parses very large codebases (10M+ LOC) into a full relationship graph in seconds, with a REPL/MCP runtime query system.\n\nThe contrast between what this exposes **deterministically** versus what embedding-based retrieval exposes **probabilistically** is‚Ä¶ stark.\n\nSo I‚Äôm genuinely curious:\n\n**Why is the industry defaulting to probabilistic retrieval for code intelligence when deterministic graph models are both feasible and vastly more precise?**\n\nIs it:\n\n* Tooling convenience?\n* LLM compatibility?\n* Lack of awareness?\n* Or am I missing a real limitation of graph-based approaches at scale?\n\nI‚Äôd genuinely love to hear perspectives from people building or using these systems ‚Äî especially from those deep in code intelligence, AI tooling, or compiler/runtime design.\n\n  \nEDIT: I'm not referring to Knowledge Graph",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o09dn3s",
          "author": "munkymead",
          "text": "Its more to do with context management. When you give an vanilla LLM a task, it has to read files each time to understand context. Even with a graph engine (forgive me if my understanding of this is wrong, I haven't delved much into graph models) it will still need to retrieve information despite understanding the relationships etc.\n\nProper chunking can separate your code and docs logically  \nProperly chunked docs are then embedded and stored as vectors.\n\nIf an LLM needs to get a better understanding of things, it could query a subagent to query your vector store with semantic search. A query might return 20 documents, the subagent can scan through those, find a deterministic answer or say it can't find anything on that matter. This is usually done by a cheaper agent that doesn't require critical thinking, just evaluate the documents and answer the query.\n\nIt can then formulate a low token concise response to answer the main agents query and provide examples if needed.\n\nMain agents context window stays clean, performance is improved, more work can be done without hitting limits. This can be done over and over again in a single session. It's cheaper and more effective.\n\nCombine this with the MCP Proxy Pattern and your chat sessions have extended capabilities for both tool calling and knowledge retrieval without bloating the main context window. It's like dependency injection for AI.\n\nIt's one of the main reasons why NotebookLM is so popular, it offers zero hallucinations on queries against your sources. If the query cannot be answered it won't make things up if the available information is not in your sources.\n\nLong story short, cost, performance and accuracy combined.",
          "score": 11,
          "created_utc": "2026-01-18 08:32:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pz286",
              "author": "hhussain-",
              "text": "I may got you wrong at first reply.\n\nSo, let me ask this: if \"cost, performance and accuracy\" are somehow solved, do you think deterministic graph would be a better choice?",
              "score": 1,
              "created_utc": "2026-01-20 19:29:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0q27aw",
                  "author": "munkymead",
                  "text": "Hey, sorry for not getting back to you. Truthfully, I cannot answer your question, as my understanding of knowledge graphs is limited. Although it's definitely something I should look into more.\n\nOne other guy commented, talking about his project ChunkHound\n\nOn the main page, there's a small explanation there:\n\n|Approach|Base Capability|Orchestration|Monorepo Scale|Maintenance|\n|:-|:-|:-|:-|:-|\n|**Keyword Search**|Exact matching|None|‚úì Fast|None|\n|**Traditional RAG**|Semantic search|None|‚úì Scales|Re-index files|\n|**Knowledge Graphs**|Relationship queries|Pre-computed|‚úó Expensive|Continuous sync|\n|**ChunkHound**|Semantic + Regex|Code Research sub-agent|‚úì Automatic|Automatic (incremental + realtime)|\n\nI think RolandRu hit the nail on the head in that you could probably use a hybrid approach but until I understand knowledge graphs more, I won't be able to give a valid opinion as to whether or not that would be beneficial.\n\nTo my understanding so far, it seems like the work involved in maintaining the accuracy of a graph is a lot. I imagine it probably could achieve better results in the right context but not sure if it's a one solution fits all kind of thing. Whereas you can chunk, embed and search pretty much anything in a well architected RAG framework.",
                  "score": 2,
                  "created_utc": "2026-01-20 19:43:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o09hgyd",
              "author": "hhussain-",
              "text": "I agree with you on context management and usage of mini-LLM (cheap) to get context. My understanding of current AI Agents is they either bloat their context window with tool calls (codebase index, vector embeddings, semantic graph) to build some rational, or use a mini-LLM on top of (codebase index, vector embeddings, semantic graph) to get a deterministic answer.\n\nMy points is why leaning to vector embeddings if graph is cheaper and deterministic (zero hallucinations)?\n\nIt is new to me that NotebookLM is resulting in zero hallucinations. In codebase I've always sees vector embedding as minimizing hallucinations, but never reached zero.\n\nA semantic graph is similar to vector embeddings in sense of codebase/documents indexing and relations building with reference to original codebase/documents. The difference is data structure and retrieval. A graph (data structure) stores nodes and edges (relations) with line number start/end with column. The graph covers certain depth i.e. up to a function call, or up to 4 levels depth sub-titles. So it is similar to vector embedding in sense of not storing everything, but different in a way that it has facts up to that depth instead of probability. In my case I did not use any storage(database), it is all in ram (less than100MB for 10Mil LOC) and query is 50 milliseconds. Building the graph is always the challenge, rest are normal operations.\n\nI feel still I'm missing something! what areas vector embeddings are better than graph, or what in graph is preventing its usage?",
              "score": 0,
              "created_utc": "2026-01-18 09:07:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0b6qot",
          "author": "RolandRu",
          "text": "I think the mismatch you‚Äôre feeling is real, but it comes from mixing two different jobs under ‚Äúcode understanding‚Äù.\n\nFor compiler-style questions (symbol resolution, types, call graphs, inheritance), a deterministic graph is the right tool. It‚Äôs precise and explainable.\n\nBut most real-world queries aren‚Äôt that clean. People ask fuzzy stuff like ‚Äúwhere is the business rule?‚Äù, ‚Äúwhy does this happen sometimes?‚Äù, ‚Äúwhat code path sends the email?‚Äù In those cases the hard part isn‚Äôt ‚Äúprove the answer‚Äù, it‚Äôs ‚Äúfind a good starting point‚Äù. Embeddings are basically a cheap, surprisingly effective discovery layer across code, comments, tests, configs, strings, docs, etc. They also degrade gracefully when builds don‚Äôt load perfectly.\n\nAlso, ‚Äúgraph is deterministic‚Äù is true only *given a stable build reality*. In practice you have DI, runtime routing, reflection/plugins, generated code, conditional compilation, multiple targets‚Ä¶ so the graph is often ‚Äúdeterministic per configuration‚Äù, and keeping that fully correct across environments is work.\n\nSo the industry default is embeddings because they‚Äôre fast to ship, cross-language-ish, resilient, and they fit the LLM retrieve‚Üístuff‚Üíanswer pipeline.\n\nThe best systems usually end up hybrid anyway: embeddings to locate candidate entry points, graphs to expand/verify/ground the answer.",
          "score": 5,
          "created_utc": "2026-01-18 16:10:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j4bhb",
              "author": "hhussain-",
              "text": "You are totally right! I was assuming (vector db + embedding) is one thing while they are 2 distinct purposed differently.\n\nThis is insightful, thanks!",
              "score": 2,
              "created_utc": "2026-01-19 19:26:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o098uy3",
          "author": "stingraycharles",
          "text": "Because you want LLMs to see relations between different functions / components that are not in code yet. \n\nIt‚Äôs also not either / or, a coding assistant can use both vector embeddings and LSP just like most RAGs use vector embeddings and BM25.",
          "score": 1,
          "created_utc": "2026-01-18 07:48:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09f3ya",
              "author": "hhussain-",
              "text": "If my understanding is correct, you are referring to vector embedding being able to help LLM in case of relations between functions/components that are **not** in code yet? I don't see a difference between graph and vector embeddings in this area, unless I'm missing something.",
              "score": 1,
              "created_utc": "2026-01-18 08:45:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09gius",
                  "author": "stingraycharles",
                  "text": "The graph requires the dependencies / edges to already be implemented, the vector embeddings enable seeing potential relations that are not yet implemented, eg ‚ÄúI need to write a function that calculated the foo distance‚Äù -> ‚Äúoh we already have a function that does something similar in that other component‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-18 08:58:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o09tsb0",
          "author": "Funny-Anything-791",
          "text": "With [ChunkHound](https://chunkhound.github.io) we're doing a mixture of both approaches. We start with semantic search for scale and speed, augment it with AST parsing, symbols, constants and imports extraction, then combine it all using a deep research like orchestration layer on top of an LLM. This enables it to scale to tens of millions LoC while staying local first with a DB that's a few GB in size and a memory footprint that fits on a laptop",
          "score": 1,
          "created_utc": "2026-01-18 11:01:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hn1wz",
              "author": "UnionCounty22",
              "text": "I noticed chunkhound will populate 98% of my 64GB of system ram when it runs a chunkhound search ‚ÄúClassName‚Äù. The entire PC comes to a standstill for 15-20 seconds.\n\nDDR5 Ram, any idea why it does this? It‚Äôs with 4B qwen3 embed & rerank loaded into the 3090 GPU. I tried the 0.6B pairs and it‚Äôs somewhat faster although still pretty choppy.",
              "score": 3,
              "created_utc": "2026-01-19 15:27:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0hxszu",
                  "author": "Funny-Anything-791",
                  "text": "Yes, that's a known issue that's already fixed in the next version. Hang in there üôè",
                  "score": 2,
                  "created_utc": "2026-01-19 16:15:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0aigm1",
              "author": "hhussain-",
              "text": "I really appreciate this link! ChunkHound made me understand that I'm talking about something different than usual! and now it makes more sense to hear limitations about graphs (knowledge graphs).\n\nThe graph I was testing (semantic graph) is build based on full AST, so resulting graph is based on AST-driven semantics.\n\nI will do more research and test ChunkHound , it seems an excellent option based on my reading! My codebases usually are in millions, so it should shine in there.",
              "score": 1,
              "created_utc": "2026-01-18 14:05:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0citng",
                  "author": "FidgetyHerbalism",
                  "text": "Are you done jerking off your alt account yet?¬†",
                  "score": 3,
                  "created_utc": "2026-01-18 19:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcy7nw",
      "title": "Solo Building a Custom RAG Model for Financial Due Diligence - Need Help",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcy7nw/solo_building_a_custom_rag_model_for_financial/",
      "author": "PositionBoring9826",
      "created_utc": "2026-01-14 20:19:51",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.92,
      "text": "Hey everyone, \n\nI am new to this community and came here because I have been spinning my wheels for awhile. I am new to RAG and trying to build a RAG model for a private equity firm solo. I understand the concepts and have used LlamaIndex, openai-embeddings, and chromadb to build a \"working\" RAG system. \n\nThe problem I am running into is the type of documents we need to index are pitch deck pdfs (about 100 pages of marketing material, branding images, graphs and visuals, financial tables, and commentary with no whitespace). How do I chunk these documents? Is there any custom embedding model for financial purposes? What methods can I use to improve the accuracy and reduce hallucinations? Where should I even start? I also am curious to see how people metadata-tag these documents. Any advice would be appreciated. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcy7nw/solo_building_a_custom_rag_model_for_financial/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzp0a9q",
          "author": "aiprod",
          "text": "I‚Äôve seen success with slides by feeding them into a vision language model (smaller ones like haiku or gpt5-mini are good for this and don‚Äôt break the bank). Ask the VLM to extract the text and describe any tables, charts and images. This is what you index with a normal embedding model. For retrieval, you retrieve the text version of a full slide and then also fetch a screenshot of the slide in a second step. You feed both to the LLM and you will get good answers.",
          "score": 3,
          "created_utc": "2026-01-15 08:07:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nznf38f",
          "author": "Whole-Assignment6240",
          "text": "depends on what you do - if you have lots of images with text, i find it effective to split pages then do a colpali embedding for just search. and you can add metadata information on that page.\n\n[https://cocoindex.io/examples/multi\\_format\\_index](https://cocoindex.io/examples/multi_format_index)\n\n(i'm one of the maintainer for the framework, this is a tutorial captures the high level gist.)\n\ni see people building with OCR too, and extract  image and text elements with relations, depends on what you need to do with different metadata etc.",
          "score": 1,
          "created_utc": "2026-01-15 01:26:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztno18",
          "author": "blue-or-brown-keys",
          "text": "u/PositionBoring9826 I added a chapter on chunking strategies in the 21 RAG strategies Book. check it out [https://www.twig.so/#DownloadEbookSection](https://www.twig.so/#DownloadEbookSection)",
          "score": 1,
          "created_utc": "2026-01-15 23:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxeg12",
          "author": "Popular_Sand2773",
          "text": "I think one thing people struggle to realize when just starting out is what you search doesn‚Äôt have to be what you return. You are on the right track with metadata but with complex multimodal and long docs like this I think you would see a lot of success if you used a summarizer. \n\nIt keeps the search surface clean, does the heavy lifting off the hot path and lets you separate what you search from what you return. A cook doesn‚Äôt use raw eggs to make an omelette they whip them first. It‚Äôs no different than that.",
          "score": 1,
          "created_utc": "2026-01-16 14:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzm2tfs",
          "author": "OnyxProyectoUno",
          "text": "You've got tables that get mangled, images that carry meaning but no text, and dense financial commentary all mashed together. Standard chunking just flattens everything into text soup.\n\nThe issue is your parser, not your chunker. Most PDF parsers treat tables as lines of text, which destroys the structure. You need something that extracts tables as tables, like Docling or Unstructured with table extraction enabled. Once you have clean structured output, chunking becomes way easier.\n\nFor the financial tables specifically, consider keeping them as separate chunks with metadata indicating they're tabular data. Don't try to embed a P&L statement the same way you embed commentary paragraphs. Different content types need different treatment.\n\nOn embeddings, OpenAI's ada-002 is fine for financial text. The custom embedding model rabbit hole rarely pays off unless you're doing something very domain-specific like ticker symbol matching. Your retrieval problems are almost certainly upstream of the embedding step.\n\nMetadata tagging, attach document type, section headers, page numbers, and whether the chunk contains tables or narrative text. This lets you filter at query time instead of hoping the embedding similarity sorts it out.\n\nI've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) to let you preview what your docs look like after each transformation, which helps catch table mangling before it hits your vector store. What parser are you using right now?",
          "score": 1,
          "created_utc": "2026-01-14 21:19:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzltt44",
          "author": "Anth-Virtus",
          "text": "Try out superlinked embedding methods. For your use case it seems really well suited. Also, look up late-chunking, it's probably better for you.",
          "score": 0,
          "created_utc": "2026-01-14 20:38:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}