{
  "metadata": {
    "last_updated": "2026-02-04 02:59:57",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 74,
    "file_size_bytes": 110588
  },
  "items": [
    {
      "id": "1qu5zua",
      "title": "OpenClaw enterprise setup: MCP isn't enough, you need reranking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "author": "Queasy-Tomatillo8028",
      "created_utc": "2026-02-02 20:05:44",
      "score": 47,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "OpenClaw, 145k stars in 10 weeks. Everyone's talking about MCP - how agents dynamically discover tools, decide when to use them, etc.\n\nI connected a local RAG to OpenClaw via MCP. My agent now knows when to search my docs vs use its memory.\n\n**The problem:** it was searching at the right time, but bringing back garbage.\n\n**MCP solves the WHEN, not the WHAT**\n\nMCP is powerful for orchestration:\n\n* Agent discovers tools at runtime\n* Decides on its own when to invoke `query_documents` vs answer directly\n* Stateful session, shared context\n\nBut MCP doesn't care about the quality of what your tool returns. If your RAG brings back 10 chunks and 7 are noise, the agent will still use them.\n\n**MCP = intelligence on WHEN to search** **Context Engineering = intelligence on WHAT goes into the prompt**\n\nBoth need to work together.\n\n**The WHAT: reranking**\n\nMy initial setup: hybrid search (vector + BM25), top 10 chunks, straight into context.\n\nResult: agent found the right docs but cited wrong passages. Context was polluted.\n\nThe fix: **reranking**.\n\nAfter search, a model re-scores chunks by actual relevance. You keep only top 3-5.\n\nI use **ZeroEntropy**. On enterprise content (contracts, specs), it goes from \\~40% precision to \\~85%. Classic cross-encoders (ms-marco, BGE) work for generic stuff, but on technical jargon ZeroEntropy performs better.\n\n**The full flow**\n\n    User query via WhatsApp\n        ‚Üì\n    OpenClaw decides: \"I need to search the docs\" (MCP)\n        ‚Üì\n    My RAG tool receives the query\n        ‚Üì\n    Hybrid search ‚Üí 30 candidates\n        ‚Üì\n    ZeroEntropy reranking ‚Üí top 3\n        ‚Üì\n    Only these 3 chunks enter the context\n        ‚Üì\n    Precise answer with correct citations\n\nAgent is smart about WHEN to search (MCP). Reranking ensures what it brings back is relevant (Context Engineering).\n\n**Stack**\n\n* **MCP server:** custom, exposes `query_documents`\n* **Search:** hybrid vector + BM25, RRF fusion\n* **Reranking:** ZeroEntropy\n* **Vector store:** ChromaDB\n\n**Result**\n\nBefore: agent searched at the right time but answers were approximate.\n\nAfter: WhatsApp query \"gardening obligations in my lease\" ‚Üí 3 sec ‚Üí exact paragraph, page, quote. Accurate.\n\n**The point**\n\nMCP is one building block. Reranking is another.\n\nMost MCP + RAG setups forget reranking. The agent orchestrates well but brings back noise.\n\nContext Engineering = making sure every token entering the prompt deserves its place. Reranking is how you do that on the retrieval side.\n\nShootout to some smart folks i met on this discord server who helped me figuring out a lot of things: [Context Engineering](https://discord.gg/F9VNyJzb)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o38a4qg",
          "author": "-Cubie-",
          "text": "I like rerankers, but is this an ad?",
          "score": 6,
          "created_utc": "2026-02-02 21:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39d8if",
          "author": "Informal_Tangerine51",
          "text": "You solved retrieval quality but not retrieval proof. When the agent cites wrong passage despite reranking, can you replay what was actually in those top 3 chunks?\n\nWe use reranking too. Helps accuracy but doesn't solve the debugging problem. Agent extracts wrong data, we know reranking happened, but can't verify: were those top 3 chunks stale? Did reranking score change between dev and prod? What version of the docs were retrieved?\n\nMCP orchestration plus reranking gives better answers. Still can't answer \"prove what the agent saw at 2:47am on case #4521\" because logs show reranking executed, not what content passed through.\n\nFor WhatsApp queries this works great. For production agents where Legal asks for evidence, the gap is: can you capture and verify the actual retrieved content, not just that retrieval happened?\n\nDoes your setup store the reranked chunks with timestamps for replay, or just return them to the agent?",
          "score": 3,
          "created_utc": "2026-02-03 00:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3afsec",
              "author": "hncvj",
              "text": "I'm 100% with this person. We also do re-ranking in our projects having HIPAA compliance. We have to keep logs of every single thing, even the data that was sent to LLMs, PHI, De-id, Returned from LLMs, re-ranked, pulled from KG or Vector Database. Everything must be logged with timestamps.\n\nHowever, this depends on project to project basis. In other projects where there is no compliance and the final output is workable and is not required to be error free, it's ok to not have logs that deeper.",
              "score": 2,
              "created_utc": "2026-02-03 04:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37terf",
          "author": "Edcoopersound",
          "text": "What's your latency like end-to-end? From WhatsApp message to response.",
          "score": 2,
          "created_utc": "2026-02-02 20:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37tqnf",
              "author": "Queasy-Tomatillo8028",
              "text": "2-3 sec total",
              "score": 1,
              "created_utc": "2026-02-02 20:15:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3828ew",
          "author": "apirateiwasmeanttobe",
          "text": "I think what people often forget is that you can put anything behind an mcp tool definition. The good mcp tools behave like a person, with some sort of agency or reactivity, answering not with a wall of text but with curated and well trimmed context. You want to minimize the amount of output so that you don't deplete the context of the calling agent.",
          "score": 1,
          "created_utc": "2026-02-02 20:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39c8cy",
          "author": "blue-or-brown-keys",
          "text": "At Twig MCP handles  RAG noise via strategies, the Redwood(basic RAG strategy) does not do reranking but Cedar and Cypress do.",
          "score": 1,
          "created_utc": "2026-02-03 00:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b40my",
          "author": "primoco",
          "text": "I‚Äôve been banging my head against the same wall with enterprise RAG for months, and you're spot on. The \"toy\" setups like basic MCP or vanilla LangChain wrappers just fall apart the second you feed them high-density documents.\n\nIn my experience, if you aren't obsessing over the retrieval pipeline before the query even hits the LLM, you're just building a very expensive hallucination machine. A few things I‚Äôve learned the hard way:\n\n1. **Hybrid search is the only way out.** If you rely only on vector embeddings for factual stuff (like specific dates or IDs in a 500-page report), you‚Äôre going to get \"semantic blurring.\" You need BM25 keyword matching running alongside your vectors with a tunable alpha. It‚Äôs the only way to catch those \"needle in a haystack\" moments.\n2. **Rerankers are double-edged swords.** I‚Äôve seen Rerankers actually kill the correct result because the threshold was a hair too tight. Now I just pull a wider window (Top-K 20) and let the reranker sort the Top-5 without hard-filtering. It‚Äôs safer and much more consistent.\n3. **Small chunks > Big chunks.** We moved to 600-char chunks with a decent overlap and the \"contextual precision\" shot up. Big chunks just add too much noise and confuse the model.\n4. **Stop the \"vibe-checks.\"** You can‚Äôt tell if a RAG is good just because the answer \"sounds professional.\" I had to build a full eval pipeline to realize my \"best sounding\" model was actually making up half the citations.\n\nEnterprise RAG isn't about which LLM is smarter, it's about how much you can control the data flow.",
          "score": 1,
          "created_utc": "2026-02-03 08:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4673",
          "author": "Shekher_05",
          "text": "Ad Detected",
          "score": 1,
          "created_utc": "2026-02-03 08:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37utrg",
          "author": "Anth-Virtus",
          "text": "Hey, yeah, MCP alone isn't enough for a good RAG.\nThanks for sharing the discord link, I appreciate it",
          "score": 1,
          "created_utc": "2026-02-02 20:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ecml",
          "author": "LeadingFun1849",
          "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nHelp me improve it; you can find the link here to try it out:\n\nWebsite¬†https://dlovable.daveplanet.com\nCODE :¬†https://github.com/davidmonterocrespo24/DaveLovable",
          "score": 1,
          "created_utc": "2026-02-02 21:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpce5s",
      "title": "You can now train embedding models 1.8-3.3x faster!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:11:25",
      "score": 38,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "Hey RAG folks! We collaborated with Hugging Face to enable 1.8-3.3x faster embedding model training with 20% less VRAM, 2x longer context & no accuracy loss vs. FA2 setups.\n\nFull finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default! You can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp etc.\n\nFine-tuning embedding models can improve retrieval & RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.\n\nWe provided many free notebooks with 3 main use-cases to utilize. \n\n* Try the [EmbeddingGemma notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M).ipynb) in a free Colab T4 instance\n* We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!\n\n‚≠ê Guide + notebooks: [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nGitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n\nThanks so much guys! :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o287p7m",
          "author": "z0han4eg",
          "text": "Thanks mate, this a rly big news",
          "score": 3,
          "created_utc": "2026-01-28 15:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27wj9w",
          "author": "Popular_Sand2773",
          "text": "This is very cool. Quick question if we are using these encoders for the base of something else is this still valuable or is it only really for classic fine tuning? If I understand correctly the main speedup came from a new fused kernel correct?",
          "score": 1,
          "created_utc": "2026-01-28 14:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284gga",
              "author": "yoracale",
              "text": "Apologies could you elaborate your first question?\n\nOur main optimizations includes gradient checkpointing, kernels yes and more. You can see gradient checkpointing here: https://unsloth.ai/docs/new/500k-context-length-fine-tuning#unsloth-gradient-checkpointing-enhancements",
              "score": 1,
              "created_utc": "2026-01-28 15:06:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28gjpf",
          "author": "TechySpecky",
          "text": "Why do people fine tune models and when do these lead to superior performance than large well-known embedding models like gemini / qwen ones? For example if I am doing RAG for archeology would it make sense to have a custom embedding model?",
          "score": 1,
          "created_utc": "2026-01-28 16:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o294cnv",
              "author": "Financial-Bank2756",
              "text": "yes, a custom embedding model could help if you have enough domain text and evaluation pairs. Otherwise, a strong general model plus better chunking, metadata filters, hybrid search, and rerankers often beats premature fine-tuning.",
              "score": 3,
              "created_utc": "2026-01-28 17:44:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o294nzx",
                  "author": "TechySpecky",
                  "text": "Yea makes sense, what do you mean by evaluation pairs?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:45:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2dh7lf",
          "author": "Aggressive-Solid6730",
          "text": "With how cheap compute is and how small embedding models are (compared to LLMs) I wouldn't think that time and memory are at that much of a premium. I am curious to hear any push-back on this, but I am also curious if in your experiments you saw any additional benefits of using these fine-tuning variants such as LoRA. Did they behave as regularizers making training more stable or were the gains purely speed and memory? The other thing you mention is context length which is fair, but as Google published, the amount of information we are trying to fit into a single vector is already quite limiting.",
          "score": 1,
          "created_utc": "2026-01-29 07:48:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2suc38",
              "author": "yoracale",
              "text": "For embedding model training speed is probably the most important. VRAM less so. If you can save time training why the hell not? And it's not a little speed boost, 2x faster basically means 100% faster than before\nThe gains are only for speed, memory and context length at this time. We don't do any accuracy changes as of this moment üôè",
              "score": 1,
              "created_utc": "2026-01-31 15:19:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2eqvq6",
          "author": "Interesting-Town-433",
          "text": "Amazing! Will incorporate into embedding-adapters asap\n\nUniversal Embedding Translation Library\nOutput an embedding from any model into any other model's vector space\n\ngo\nminilm <-> openai\ngoogle <-> openai\ne5 <-> openai\nwith confidence scoring to tell you when it will work\n https://github.com/PotentiallyARobot/EmbeddingAdapters",
          "score": 1,
          "created_utc": "2026-01-29 13:45:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2su01y",
              "author": "yoracale",
              "text": "Interesting thanks for sharing!",
              "score": 1,
              "created_utc": "2026-01-31 15:17:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34ch32",
          "author": "Informal-Victory8655",
          "text": "A basic question: how do we prepare data for a embedding model training? \n\nIs it we have to prepare queries and the relevant text documents / text paragraphs that must be retrieved for the given query? \n\nAs I've french law data but no such pairs available.",
          "score": 1,
          "created_utc": "2026-02-02 07:47:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qul1mq",
      "title": "NotebookLM For Teams",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:37",
      "score": 38,
      "num_comments": 2,
      "upvote_ratio": 0.98,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ax6mb",
          "author": "Otherwise_Wave9374",
          "text": "Cool project. The combination of \"team chat\" + internal sources + an agent that can actually take actions is the sweet spot.\n\nIf you have not already, you might want to think about a permissions model for agent actions (read vs write, connector scopes) plus a way to show citations for every claim to keep trust high.\n\nMore agent design notes here if helpful: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-03 07:05:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq8y1q",
      "title": "TextTools ‚Äì High-Level NLP Toolkit Built on LLMs (Translation, NER, Categorization & More)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "author": "Due_Place_6635",
      "created_utc": "2026-01-29 13:45:08",
      "score": 28,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "Hey everyone! üëã\n\nI've been working on TextTools, an open-source NLP toolkit that wraps LLMs with ready-to-use utilities for common text processing tasks. Think of it as a high-level API that gives you structured outputs without the prompt engineering hassle.\n\nWhat it does:\n\nTranslation, summarization, and text augmentation\n\nQuestion detection and generation\n\nCategorization and keyword extraction\n\nNamed Entity Recognition (NER)\n\nCustom tools for almost anything\n\nWhat makes it different:\n\nBoth sync and async APIs (TheTool & AsyncTheTool)\n\nStructured outputs with validation\n\nProduction-ready tools (tested) + experimental features\n\nWorks with any OpenAI-compatible endpoint\n\nQuick example:\n\n```python\nfrom texttools import TheTool\n\nthe_tool = TheTool(client=openai_client, model=\"your_model\")\nresult = the_tool.is_question(\"Is this a question?\")\nprint(result.to_json())\n```\nCheck it out: https://github.com/mohamad-tohidi/texttools\n\nI'd love to hear your thoughts! If you find it useful, contributions and feedback are super welcome. What other NLP utilities would you like to see added?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2ewxt9",
          "author": "Popular_Sand2773",
          "text": "Hey like the idea of a one stop shop but a lot of these are tasks I would never give to an llm. Have you thought about unifying the BIC options for these different tasks? BERTopic is a good example of a unifying framework that takes the best from everyone.",
          "score": 3,
          "created_utc": "2026-01-29 14:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qos79",
              "author": "Due_Place_6635",
              "text": "Hello, thank you for the reply  \ni completly agree, there are a lot of tasks that an small finetuned BERT model would be far better at them, rather than a giant LLM!\n\nyes, as a matter of fact, it used to be part of the library, but we decided to remove that\n\nbecause: the llm's are open domain, they are kind of like a quick Demo of what is possible...",
              "score": 1,
              "created_utc": "2026-01-31 05:10:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31qx2m",
                  "author": "leppardfan",
                  "text": "Nice work! I think you should put the BERT models back in as another option for the user, depending on the problem.",
                  "score": 2,
                  "created_utc": "2026-02-01 22:01:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qs87ld",
      "title": "Best chunking + embedding strategy for mixed documents converted to Markdown (Docling, FAQs, web data)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "author": "Particular-Gur-1339",
      "created_utc": "2026-01-31 17:19:34",
      "score": 27,
      "num_comments": 12,
      "upvote_ratio": 0.95,
      "text": "Hey folks üëã\nI‚Äôm building a RAG pipeline and could use some advice on chunking and embedding strategies when everything is eventually normalized into Markdown.\n\nCurrent setup\n\nConverting different file types (PDFs, docs, etc.) into Markdown using Docling\nScraping website FAQ pages and storing those as Markdown as well\nEmbedding everything into a vector store for retrieval\n\nStructure of the data\nEach document/page usually has:\nA main heading\nSub-sections under that heading\nMultiple FAQs under each section\nWeb FAQs are often short Q&A pairs\n\nWhat I‚Äôm confused about\nChunking strategy\nShould I chunk by:\nPage\nHeading / sub-heading\nIndividual FAQ (Q + A as one chunk)\n\nHybrid approach (heading context + FAQ chunk)?\n\nChunk size\nFixed token size (for example 300 to 500 tokens)\nOr semantic chunks that vary in size?\nMetadata\n\n\nGoal\nHigh answer accuracy\nAvoid partial or out-of-context answers",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2xli7k",
          "author": "Dapper-Turn-3021",
          "text": "Yeah, you‚Äôre basically on the right track. There‚Äôs no single perfect chunking or embedding strategy, it really depends on your data and what you‚Äôre trying to achieve. Keeping metadata separate is correct you should embed only the actual text content and store the metadata alongside it so you can filter or rank with it later. Embedding metadata usually just adds noise.\nChunking isn‚Äôt fixed either. Sometimes smaller chunks work better, sometimes larger ones, and semantic chunking is often the best option. It‚Äôs normal to tweak your chunking strategy as you see how your retrieval performs.\n\n\nAnd you‚Äôre absolutely right about retrieval. Don‚Äôt rely only on embeddings. Combining embeddings with metadata filtering, keyword or BM25 search, and then adding a reranking step gives much better results in most cases.\n\n\nSo yes, what you described is basically how strong RAG systems are built. we are following same for Zynfo AI to build out chatbot",
          "score": 5,
          "created_utc": "2026-02-01 07:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tpw4u",
          "author": "Curious-Sample6113",
          "text": "Everything depends on your original source. You are on the right path and just have to do a lot of testing.",
          "score": 2,
          "created_utc": "2026-01-31 17:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfmn5",
              "author": "Particular-Gur-1339",
              "text": "What should be my chunking strategy?",
              "score": 1,
              "created_utc": "2026-02-01 02:18:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xc1r6",
                  "author": "Curious-Sample6113",
                  "text": "You optimize your strategy by testing. I don't know what your source looks like. For example: if you are building a legal agent then you will have a series of questions. The ones that fail will reveal the issues with your ingestion. \n\nThere isn't a universal solution. Everything is tailored to the source.",
                  "score": 1,
                  "created_utc": "2026-02-01 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ttf5n",
          "author": "jannemansonh",
          "text": "hit the same wall building doc search workflows... ended up using needle app since it handles the chunking/embedding/rag stuff automatically (has hybrid search built in). you just describe what you need and it builds it vs configuring all the pieces manually",
          "score": 1,
          "created_utc": "2026-01-31 18:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2utz79",
              "author": "Particular-Gur-1339",
              "text": "Didn't get this what is a needle app?",
              "score": 1,
              "created_utc": "2026-01-31 21:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2v26bj",
                  "author": "bwhitts66",
                  "text": "Needle is a tool that streamlines the process of building document search workflows. It automates chunking and embedding so you don‚Äôt have to set everything up manually. It‚Äôs pretty handy if you‚Äôre looking to simplify the RAG pipeline!",
                  "score": 3,
                  "created_utc": "2026-01-31 21:44:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30re8z",
          "author": "Higgs_AI",
          "text": "You‚Äôre asking the wrong question and I mean that in the most helpful way possible.\nThe chunking debate (semantic vs fixed size vs hierarchical) assumes your RAG architecture is correct. It‚Äôs not. You‚Äôre optimizing for retrieval when you should be optimizing for knowledge structure.\nHere‚Äôs the problem with your current setup. Docling to Markdown to Vectors to Retrieval. This pipeline loses the thing that makes FAQs useful, which is the relationships between questions. When you chunk Q+A as atomic units, you lose which questions are siblings under the same topic, which answers reference concepts explained elsewhere, and the hierarchy you already identified (heading to subheading to FAQ).\nWhat you actually want is to stop chunking for embedding and start structuring for reasoning.\nInstead of treating each FAQ as an isolated chunk, build a structure where each FAQ has an ID, knows what topic it belongs to, knows what other FAQs it relates to, and carries its prerequisites. Now your retrieval can find the relevant FAQ by semantic match, pull in related FAQs automatically so you don‚Äôt get out of context answers, and include parent topic context without re-embedding it every time.\nIf you‚Äôre committed to vector RAG, here‚Äôs the practical move. Chunk at FAQ level with Q and A together, you had this right. But prepend the heading hierarchy as metadata, not as embedded text. Store the path like ‚ÄúAccount Settings > Security > Password Recovery‚Äù and at retrieval time inject that context before the FAQ content. This gives you semantic search on the answer content while preserving structural context for the LLM.\nThe hybrid approach you‚Äôre circling around looks like this. Your chunk is the individual FAQ as a Q+A pair. Your metadata is the full heading path, related FAQ IDs, and section summary. Your embedding should be the question plus the first sentence of the answer, not the full text since answers tend to be verbose. At retrieval you grab your top-k FAQs plus their metadata plus the parent section summary.\nToken budget roughly 50 to 100 tokens per FAQ chunk, 20 to 30 tokens for heading context, pull 3 to 5 related FAQs and you‚Äôre at maybe 500 tokens total. That‚Äôs enough for high answer accuracy without blowing up your context window.\nTo hit your specific questions directly. Chunk by FAQ with Q and A together, yes this is correct. Use semantic size not fixed tokens since FAQs vary so let them. Metadata is your secret weapon here, the heading path, section ID, related IDs. And don‚Äôt embed the hierarchy, reference it. Embed the answer, retrieve the structure.\nIf you want to go deeper on this, look into knowledge graphs as a retrieval layer instead of pure vector search. Structure beats embedding for FAQ style content every time. And I do mean EVERY TIME! Just my opinion ü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-01 19:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30z7x9",
          "author": "Live-Guitar-8661",
          "text": "I have a thing and I‚Äôm looking for testers and willing to build a POC for free alongside. DM if you are interested",
          "score": 1,
          "created_utc": "2026-02-01 19:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o313cnn",
          "author": "ampancha",
          "text": "For FAQ-heavy Markdown, heading-anchored chunks with parent context (section title + sub-heading prepended to each chunk) consistently outperform fixed-token splits because the LLM gets retrieval results that carry their own scope. The deeper problem is that without retrieval evaluation and input validation on ingested content, any chunking strategy is an untestable guess, and scraped web pages become an injection surface the moment they land in your vector store. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 20:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xyuw",
          "author": "voycey",
          "text": "There is no single strategy that works, the reason most RAG systems fail is due to poor chunking methodologies.\n\nI will say that there is no real reason not to mix multiple chunking strategies into a single pipeline to ensure you are getting the best retrieval, especially if you are using re-ranking!",
          "score": 1,
          "created_utc": "2026-02-01 22:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32aak9",
          "author": "Odd-Affect236",
          "text": "What benefits does markdown provide when compared to simple plain text?",
          "score": 1,
          "created_utc": "2026-02-01 23:43:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs2uzw",
      "title": "Building a RAG-Based Chat Assistant using Elasticsearch as a Vector Database",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "author": "Leading-Grape-6659",
      "created_utc": "2026-01-31 13:50:21",
      "score": 22,
      "num_comments": 1,
      "upvote_ratio": 0.96,
      "text": "Hi everyone üëã\n\n\n\nI recently built a simple RAG (Retrieval-Augmented Generation) chat assistant using Elasticsearch as a vector database.\n\n\n\nThe blog covers:\n\n‚Ä¢ How vector embeddings are stored in Elasticsearch\n\n‚Ä¢ Semantic retrieval using vector search\n\n‚Ä¢ How retrieved context improves LLM responses\n\n‚Ä¢ Real-world use cases like internal knowledge bots\n\n\n\nFull technical walkthrough with code and architecture here:\n\nüëâ [https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends\\_link&sk=d2006b31e40e3c3ed714c18eabf8f271](https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends_link&sk=d2006b31e40e3c3ed714c18eabf8f271)\n\n\n\nHappy to hear feedback or suggestions from folks working with RAG and vector databases!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2sf6zr",
          "author": "cat47b",
          "text": "I did wonder about elastic but the hassle and cost of using that as a store has put me off a bit. Have you looked at Clickhouse?",
          "score": 2,
          "created_utc": "2026-01-31 13:55:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp9pmy",
      "title": "How to handle extremely large extracted document data in an agentic system? (RAG / alternatives?)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "author": "Complex-Time-4287",
      "created_utc": "2026-01-28 12:13:17",
      "score": 21,
      "num_comments": 28,
      "upvote_ratio": 0.97,
      "text": "I‚Äôm building an agentic system where users can upload documents. These documents can be *very* large ‚Äî for example, up to **15 documents at once**, where some are **\\~1500 pages** and others **300‚Äì400 pages**. Most of these are financial documents (e.g., tax forms), though not exclusively.\n\nWe have a document extraction service that works well and produces structured layout + document data.  \nHowever, the extracted data itself is also huge, so we **can‚Äôt fit it into the chat context**.\n\n  \n**Current approach**\n\n* The extracted structured data is stored as a **JSON file in cloud storage**\n* We store a **reference/ID in the DB**\n* Tools can fetch the data using this reference when needed\n\n  \n**The Problem**\n\nBecause the agent never directly ‚Äúsees‚Äù or understands the extracted data:\n\n* If a user asks questions about the document content,\n* The agent often can‚Äôt answer correctly, since the data is not in its context or memory\n\n  \n**What we‚Äôre considering**\n\nWe‚Äôre thinking about applying **RAG on the extracted data**, but we have a few concerns:\n\n* Agents run in a chat loop ‚Üí **creation + retrieval must be fast**\n* The data is deeply nested and very large\n* We want minimal latency and good accuracy\n\n**Questions**\n\n1. What are **practical solutions** to this problem?\n2. Which **RAG systems / architectures** would work best for this kind of use-case?\n3. Are there **alternative approaches** (non-RAG) that might work better for large documents?\n4. Any best practices for handling **very large documents** in agentic systems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o27bksd",
          "author": "Mishuri",
          "text": "You must do semantic chunking on the document. Split it up according to logical partitions. Ask LLM to enhance with descriptions metadata. Explicit relationships to other sections. Then embed those. 90% and the expensive part is LLM preprocessing of this. Then you gather context with vector rag and details with agentic rag + subagents to manage context.",
          "score": 6,
          "created_utc": "2026-01-28 12:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dy9y",
              "author": "rshah4",
              "text": "Yes, and then you can have a Table of Contents and make sure each of those sections understand their role in the hierarchical structure. This is what we do and it works well.  \nAlso a database can be useful here as well.",
              "score": 2,
              "created_utc": "2026-01-28 12:46:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27onir",
              "author": "Complex-Time-4287",
              "text": "this it totally possible, but I'm concerned about the time it is likely to take, in a chat it'll feel kind of blocking until the chuking and embedding is complete",
              "score": 1,
              "created_utc": "2026-01-28 13:47:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28i5j8",
              "author": "usernotfoundo",
              "text": "Are there any particular resources you would suggest that go into detail in this process? Currently I have been simply using an llm to process my large paragraph into a list of (observation,recommendation), embedding the observations and retrieving it based on similarity with a query. I feel this is too simplified, and breaking it down into multiple steps like you described could be the way to go, no idea how to start.",
              "score": 1,
              "created_utc": "2026-01-28 16:07:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2d1jzt",
          "author": "aiprod",
          "text": "I think what most people are missing here are the strict latency requirements. The user uploads documents in a live chat session and wants to interact with them immediately, correct?\n\nThis rules out time intensive approaches like embeddings or generating summaries or metadata with LLMs.\n\nThere are a few things that could work:\n\nGive the agent a search tool that is based on BM25. Create page chunks from the data (usually a good semantic boundary too), index it into open search or elastic search and let the agent search the index. This is fast and context efficient.\n\nOn top of that, you could add the first one or two pages of each file to the context window of the agent. Usually, the first pages give an indication of what a doc is about. With that knowledge, the agent could make targeted searches inside a specific doc by using a filter with the search queries.\n\n\nAlternatively, you could use the file system based approach that coding agents like Claude code use. Give the agent tools to grep through the files and to read slices of the document. You don‚Äôt have to use an actual file system, it could just be simulated with tools. The agent will grep and slice through the docs to answer questions. RLM is an advanced version of this approach: https://arxiv.org/pdf/2512.24601v1",
          "score": 4,
          "created_utc": "2026-01-29 05:38:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2d5vl6",
              "author": "Complex-Time-4287",
              "text": "That's right! Thanks for the suggestions, I'll try this",
              "score": 1,
              "created_utc": "2026-01-29 06:12:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2mnabf",
          "author": "Ecstatic_Heron_7944",
          "text": "Chiming in to offer an alternative perspective: you're doing all this document extraction, table identification, json generating, heavy processing and time consuming work for pages the user hasn't even asked for. In a document with 300 pages, any given query could require a fraction (maybe 10 - 20 pages) for a suitable answer. Could a better approach be to do the search (fast) first and extraction (slow) later - especially after the user is happy to confirm the context? Well, I hope so because this is what I'm building with [ragextract.com](http://ragextract.com) !\n\nTo answer your questions:\n\n1. RAG would work as a way to try narrow the search space for the user query but for financial data, it's unlikely to be sufficient on its own. You'll still need to post-process the pages for accuracy - though you may sometime get away with just winging it with vision models.  \n2. Multimodal RAG works incredibly well if documents don't share a standardised layout ie different statements from different bank. You'd also might want to look at a more optimised retrieval system for pages.  \n3. In practical terms, not that I can think of. Search-first-parse-later is an alternative RAG approach I think is worth exploring in this scenario.  \n4. Best practices for large documents? You probably already know this but go (1) big, (2) async and (3) distributed!",
          "score": 3,
          "created_utc": "2026-01-30 16:32:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27mwi8",
          "author": "KnightCodin",
          "text": "There are few gaps in the problem summary that might help  \n1. Operational flow :   \nWhen you say \"tools can fetch the data using this reference when needed\" and \"gent never directly ‚Äúsees‚Äù or understands the extracted data\",   \n\\- Where is data going to - directly to the user and not to the Agent/LLM?  \n\\- What is stopping you from presenting the \"summary\" data (Chain of Density compressed) to the Agent so follow up questions can be answered\n\n2. Do you need to answer questions on documents by other users? - Meaning is it a departmental segmentation with multiple users but same overall context or user/documents are isolated?   \n\\- This will provide type of KG and scale   \n\\- Summary \"Contextual Map\"",
          "score": 2,
          "created_utc": "2026-01-28 13:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27riay",
              "author": "Complex-Time-4287",
              "text": "In my agentic system, users can connect third-party MCP tools. If a tool requires access to the extracted data, the agent can pass that data to the specific tool the user has attached, but only when it‚Äôs actually needed.\n\nThe main issue with relying on summaries is that the extracted data itself is already very large and deeply nested JSON. Generating a meaningful summary from it is hard, and even a compressed (Chain-of-Density‚Äìstyle) summary would still fail to answer very specific questions‚Äîfor example, ‚ÄúWhat was the annual income in 2023?‚Äù\n\nRegarding document access and isolation: documents are scoped strictly to the current conversation. Conversations are not user-specific, and there can be multiple conversations, but within each conversation we only reference the documents uploaded in that same context.\n\nDocuments are uploaded dynamically as part of the conversation flow, and only those on-the-go uploads are considered when answering questions or invoking tools.",
              "score": 1,
              "created_utc": "2026-01-28 14:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27twm2",
                  "author": "KnightCodin",
                  "text": "Better :)  \nSimplistic and practical solution (not to be confused with simple) is   \nMulti-tier retrieval:   \n  \nEg : ‚ÄúSummary doc map‚Äù ‚Üí ‚ÄúTargeted Sub-Node‚Äù ‚Üí \"Drill-Down Deep fetch\"\n\nThis will be the most latency-effective for massive bundles.\n\n**SPECIFICITY :**   \n**Tier A: coarse index**\n\n* Embed¬†full **page summaries**,¬†**section headers**, and¬†**table captions**\n* Or **one chunk per page**¬†: Fully summarized (Will say normalized but that will open a whole new can of worms)\n* Path: identify *which pages/sections matter -> use deep fetch to grab that JSON*\n\n**Tier B: targeted extraction retrieval**\n\n* Once you know relevant pages/sections, fetch only that slice from cloud storage:\n   * e.g., pages 210‚Äì218 JSON\n   * or the section subtree for¬†`Income >` What was the annual income in 2023",
                  "score": 1,
                  "created_utc": "2026-01-28 14:14:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27bc02",
          "author": "patbhakta",
          "text": "For financial data, skip the traditional RAG, skip the vector databases, perhaps skip graph rag too. Go with trees, you'll incur more cost but at least your data will be sound.",
          "score": 1,
          "created_utc": "2026-01-28 12:28:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fxm7",
              "author": "yelling-at-clouds-40",
              "text": "Trees are just subset of graphs, but curious: what kind of trees do you suggest (as node hierarchy)?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27nv20",
              "author": "ajay-c",
              "text": "Interesting do you know any tree techniques?",
              "score": 1,
              "created_utc": "2026-01-28 13:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27oswt",
              "author": "Complex-Time-4287",
              "text": "can you please provide some details on this?",
              "score": 1,
              "created_utc": "2026-01-28 13:48:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bjew",
          "author": "arxdit",
          "text": "I‚Äôm handling this via knowledge trees and human supervised document ingestion (you supervise proper slicing and where the document belongs in the knowledge tree - though the AI does make suggestions)\n\nThe AI by itself is very bad at organizing information with no clear rules and will fail spectacularly \n\nSlowly learning through this\n\nYou can check out my solution [FRAKTAG on github](https://github.com/andreirx/FRAKTAG)",
          "score": 1,
          "created_utc": "2026-01-28 12:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27nxzr",
              "author": "ajay-c",
              "text": "Interesting",
              "score": 1,
              "created_utc": "2026-01-28 13:43:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27p3tn",
              "author": "Complex-Time-4287",
              "text": "Looks interesting, I'll check this  \nFor my use-case, we cannot really have a human in the loop, agents are completely autonomous and must proceed on their own",
              "score": 1,
              "created_utc": "2026-01-28 13:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27q1w8",
                  "author": "arxdit",
                  "text": "I want to get there too and I‚Äôm using my human decisions to hopefully ‚Äúteach‚Äù the ai how to do it by itself, and I am gathering data",
                  "score": 1,
                  "created_utc": "2026-01-28 13:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27btme",
          "author": "proxima_centauri05",
          "text": "You‚Äôre not doing anything ‚Äúwrong‚Äù. This is the natural failure mode when the agent only has a pointer to the data instead of an understanding of it. If the model never sees even a compressed view of the document, it‚Äôll confidently answer based on vibes.\n\nWhat‚Äôs worked for me is separating understanding from storage. On ingestion, I generate a thin semantic layer, section summaries, key entities, numbers, obligations, relationships. That layer is small, fast, and always available to the agent. The heavy JSON stays out of the loop unless the agent explicitly needs to verify something.\nTrying to RAG directly over deeply nested extracted data is usually a dead end. It‚Äôs slow, and the signal to noise ratio is awful. Hierarchical retrieval helps a lot, first decide where to look, then pull only that slice, then answr. Latency stays low because most questions never touch the raw data.\n\nFor financial or forms heavy docs, I often skip RAG entirely and just query normalized fields. It‚Äôs boring, but it‚Äôs correct. RAG is great for ‚Äúexplain‚Äù questions, terrible for ‚Äúcalculate‚Äù ones.\n\nI‚Äôm building something in this space too, and the big unlock was treating documents like evolving knowledge objects, not blobs you fetch. Once the agent has a map of the document, it stops hallucinating and starts reasoning.",
          "score": 1,
          "created_utc": "2026-01-28 12:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27tqal",
              "author": "Complex-Time-4287",
              "text": "In my case, the questions are much more likely to be *‚Äúfind‚Äù* questions rather than *‚Äúcalculate‚Äù* ones. For extremely large documents say a 1,500-page PDF containing multiple tax forms summaries or key-entity layers won‚Äôt realistically capture all the essential details.\n\nAlso, I‚Äôm not entirely sure what you mean by ‚Äújust query normalized fields‚Äù in this context.",
              "score": 1,
              "created_utc": "2026-01-28 14:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27oiym",
          "author": "ajay-c",
          "text": "I do have same issues",
          "score": 1,
          "created_utc": "2026-01-28 13:46:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27swp4",
          "author": "Crafty_Disk_7026",
          "text": "Try using a retrieval MCP https://github.com/imran31415/codemode-sqlite-mcp/tree/main. Here's one I made you can try.  This won't require embedding",
          "score": 1,
          "created_utc": "2026-01-28 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u005",
          "author": "Popular_Sand2773",
          "text": "Your instinct that you can't just shove these in the context window is correct and that you need some sort of RAG but what and why depend on the answers to these questions.\n\nWhat questions are your users asking?  \nGiven it is financial documents if it is mainly numbers and tables they care about then you should think about a SQL db and retrieval. Regular semantic embeddings are not very good at highly detailed math. If it's contract minutia then maybe a vector db and semantic embeddings. Likely you'll need both.\n\nHow much of this is noise?  \nYou mention huge documents and tax forms as an example. If a lot of this is stuff your users are never going to query you are paying both in quality and cost for things you won't use and don't need. Figure out what you can prune. \n\nIs there clear structure you can leverage?  \nJust because it's called unstructured text doesn't mean there is no structure at all. If you can narrow down where in the documents you are looking for a specific query based on the inherent structure like sections etc then you can narrow the search space and increase your top-k odds.\n\nAll this to say. It's not about what RAG is best etc it's what problem are you actually trying to solve and why. If you just want a flat quality bump without further thought try knowledge graph embeddings.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28dreo",
          "author": "Det-Nick-Valentine",
          "text": "I have the same problem.\n\nI'm working on an in-company solution like NotebookLM.\n\nIt works very well for small and medium-sized documents, but when the user uploads something large, like legal documents, it doesn't give good responses.\n\nI'm thinking of going for a summary by N chunks and working with re-ranking.\n\nWhat do you think of this approach?",
          "score": 1,
          "created_utc": "2026-01-28 15:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9g7o",
          "author": "pl201",
          "text": "Take a look of open source LightRAG. Per my research and trying, it has the best potential to be used for the requirements you have described in the post. I am working on enhancements so it can be used on a company setting (multi users, workspace, separate embedding LLM and chat LLM, speed the query for a larger knowledge base.  Etc). PM me if you are interested to make it work for your case.",
          "score": 1,
          "created_utc": "2026-01-28 20:43:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2e7nmc",
          "author": "Infamous_Ad5702",
          "text": "We had a similar problem for a client.\n\nNo GPU needed\nThey Can‚Äôt use black box LLM.\nThey Can‚Äôt have hallucinations.\n\nDefence industry so needed to be offline.\n\nWe built a tool that builds an index first. Makes it efficient. Every new query it builds a new Knowledge Graph. \n\nDoes the trick.",
          "score": 1,
          "created_utc": "2026-01-29 11:43:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eeuh4",
          "author": "TechnicalGeologist99",
          "text": "Hierarchical RAG. The document structure is important. Detect section headers and use them to construct a data tree of each document. \n\nAt the same time extract tags that you will predefined (i.e. financial, design, technical) use those same tags at query time to prefilter. \n\nWhen a section gets many hits from semantic retrieval you will upgrade and retrieve more or all of that section (it's clearly relevant)\n\nEnsure you use query decomposition (fragmenting the users question into multiple questions for multiple retrieval) and rerank those. For large retrievals, group chunks by their section id and summarise them in context of the sub question that was used to retrieve them. And then inject those summaries as documents in the final call. \n\nCongrats you didn't really need an agentic system. \nBut you can always migrate to one if and when the time is right. But don't just go agentic because it's popular. Build your domain and solutions by proving the need (YAGNI)",
          "score": 1,
          "created_utc": "2026-01-29 12:34:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31qku6",
          "author": "ampancha",
          "text": "The retrieval strategy matters, but the harder problem at scale is what happens after. Large document volumes feeding an agentic system compound fast: unbounded tool calls spiking costs with no attribution, extracted content leaking PII into the context window, and concurrency triggering retry cascades. Whatever architecture you pick, the production controls need to be designed in from day one. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 21:59:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qpla70",
      "title": "A framework to evaluate RAG answers in production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "author": "esp_py",
      "created_utc": "2026-01-28 19:26:55",
      "score": 19,
      "num_comments": 10,
      "upvote_ratio": 0.96,
      "text": " How do you know your RAG system is sending correct answers to users? A\n\n\n\nFollowing a recent discussion[ i had here](https://www.reddit.com/r/datascience/comments/1o5n86i/in_production_how_do_you_evaluate_the_quality_of/), I went ahead and  developed a waterfall evaluation framework designed to fail safely and detect hallucinations. \n\nKey components: \n\n\\- Pre-generation retrieval checks \n\n\\- Answerability validation \n\n\\- Faithfulness scoring (NLI, RAGAS, LLM-as-judge) \n\n\\- Answer relevance checks \n\n\n\n[https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach](https://www.murhabazi.com/designing-trustworthy-rag-systems-part-one-a-step-by-step-waterfall-evaluation-approach)\n\nPlease have a read and let me your thoughs, I will share the results soon in the second part.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpla70/a_framework_to_evaluate_rag_answers_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2a4j6q",
          "author": "No_Kick7086",
          "text": "Nice work, really good article that and covers an area I am looking to address with my own SME+ Rag system. Im curious about the answerability checks., was the classifier or llm more accurate? Im taking a guess at classifier. Look forward to part 2 on this with your results! Cheers",
          "score": 2,
          "created_utc": "2026-01-28 20:22:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2a4tdw",
              "author": "esp_py",
              "text": "On the answerability check I just tried the LLM and it work fine",
              "score": 2,
              "created_utc": "2026-01-28 20:23:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2so7ux",
                  "author": "Sweet_Access_9996",
                  "text": "Interesting! Did you find any specific limitations or advantages using the LLM for answerability checks compared to a traditional classifier? Would love to hear more about your experience.",
                  "score": 2,
                  "created_utc": "2026-01-31 14:46:55",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o2ahg08",
                  "author": "seomonstar",
                  "text": "nice to know",
                  "score": 1,
                  "created_utc": "2026-01-28 21:18:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2d01rd",
          "author": "Charming_Group_2950",
          "text": "TrustifAI also seems to solve the same problem with little different approach. It gives a trust score along with explanations for correct or hallucinated responses. Explore more here:¬†https://github.com/Aaryanverma/trustifai",
          "score": 2,
          "created_utc": "2026-01-29 05:27:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2e7w6h",
              "author": "No_Kick7086",
              "text": "Looks good. How are the results from it?",
              "score": 1,
              "created_utc": "2026-01-29 11:45:29",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o2e9qhx",
                  "author": "Charming_Group_2950",
                  "text": "Benchmarking in progress. You can see the results here in readme of repo. So far results are promising.",
                  "score": 1,
                  "created_utc": "2026-01-29 11:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2bl4r2",
          "author": "Able-Let-1399",
          "text": "Thanks, good stuff when learnings from real projects are shared üëç. Looking forward to next part.\nI wonder: You must have been warned / read about not asking LLM as a judge to validate multiple aspects in the same prompt, so why do you do that?",
          "score": 1,
          "created_utc": "2026-01-29 00:30:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2l23jr",
              "author": "esp_py",
              "text": "Yes, that is a good point! For the LLM as a judge, I didn't get the choice of the prompt, I used an internal package that have a predefined prompt.",
              "score": 1,
              "created_utc": "2026-01-30 11:23:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qq1hly",
      "title": "PDFstract now supports chunking inspection & evaluation for RAG document pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "author": "GritSar",
      "created_utc": "2026-01-29 06:55:11",
      "score": 16,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I‚Äôve been experimenting with different chunking strategies for RAG pipelines, and one pain point I kept hitting was **not knowing whether a chosen strategy actually makes sense for a given document** before moving on to embeddings and indexing.\n\n\n\nSo I added a **chunking inspection & evaluation feature** to an open-source tool I‚Äôm building called **PDFstract**.\n\n\n\nHow it works:\n\n* You **choose a chunking strategy**\n* PDFstract applies it to your document\n* You can **inspect chunk boundaries, sizes, overlap, and structure**\n* Decide if it fits your use case *before* you spend time and tokens on embeddings\n\n\n\nIt sits as the **first layer in the pipeline**:\n\nExtract ‚Üí Chunk ‚Üí (Embedding coming next)\n\n\n\nI‚Äôm curious how others here validate chunking today:\n\n* Do you tune based on document structure?\n* Or rely on downstream retrieval metrics?\n\nWould love to hear what‚Äôs actually worked in production.\n\nRepo if anyone wants to try it:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qsenyn",
      "title": "MiRAGE: A Multi-Agent Framework for Generating Multimodal, Multihop Evaluation Datasets (Paper + Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "author": "Socaplaya21",
      "created_utc": "2026-01-31 21:21:26",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "TL;DR**:** We developed a multi-agent framework that generates multimodal, multihop QA pairs from technical documents (PDFs containing text, tables, charts). Unlike existing pipelines that often generate shallow questions, MiRAGE uses an adversarial verifier and expert persona injection to create complex reasoning chains (avg 2.3+ hops).\n\n**Paper:** [https://arxiv.org/abs/2601.15487](https://arxiv.org/abs/2601.15487)\n\n**Code:** [https://github.com/ChandanKSahu/MiRAGE](https://github.com/ChandanKSahu/MiRAGE)\n\nHi everyone,\n\nWe've been working on evaluating RAG systems for industrial/enterprise use cases (technical manuals, financial reports, regulations), and (as many have) we hit a recurring problem: standard benchmarks like Natural Questions or MS MARCO don't reflect the complexity of our data.\n\nMost existing eval datasets are single-hop and purely textual. In the real world, our documents are multimodal (*especially* heavy on tables/charts in our use cases) and require reasoning across disjoint sections (multi-hop).\n\nWe built and open-sourced MiRAGE, a multi-agent framework designed to automate the creation of \"Gold Standard\" evaluation datasets from your arbitrary corpora.\n\nInstead of a linear generation pipeline (which often leads to hallucinations or shallow questions), we use a swarm of specialized agents.\n\nInstead of immediate generation, we use a retrieval agent that recursively builds a semantic context window. This agent gathers scattered evidence to support complex inquiries *before* a question-answer pair is formulated, allowing the system to generate multi-hop queries (averaging >2.3 hops) rather than simple keyword lookups.\n\nWe address the reliability of synthetic data through an adversarial verification phase. A dedicated verifier agent fact-checks the generated answer against the source context to ensure factual grounding and verifies that the question does not rely on implicit context (e.g., rejecting questions like \"In the table below...\").\n\nWhile the system handles text and tables well, visual grounding remains a frontier. Our ablation studies revealed that current VLMs still rely significantly on dense textual descriptions to bridge the visual reasoning gap, when descriptions were removed, faithfulness dropped significantly.\n\nThe repo supports local and cloud API model calls. We're hoping this helps others stress test their pipelines.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qpfzko",
      "title": "Convert Charts & Tables to Knowledge Graphs in Minutes | Vision RAG Tuto...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "author": "BitterHouse8234",
      "created_utc": "2026-01-28 16:24:54",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "Struggling to extract data from complex charts and tables? Stop relying on broken OCR. In this video, I reveal how to use Vision-Native RAG to turn messy PDFs into structured Knowledge Graphs using Llama 3.2 Vision.  \n  \nTraditional RAG pipelines fail when they meet complex tables or charts. Optical Character Recognition (OCR) just produces a mess of text. Today, we are exploring VeritasGraph, a powerful new tool that uses Multimodal AI to \"see\" documents exactly like a human does.  \n  \nWe will walk through the entire pipeline: ingesting a financial report, bypassing OCR, extracting hierarchical data, and visualizing the connections in a stunning Knowledge Graph.  \n  \nüëá Resources & Code mentioned in this video: üîó GitHub Repo (VeritasGraph): [https://github.com/bibinprathap/VeritasGraph](https://github.com/bibinprathap/VeritasGraph)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qpfzko/convert_charts_tables_to_knowledge_graphs_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qv17hv",
      "title": "We open-sourced our code that outperforms RAPTOR on multi-hop retrieval",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "author": "captainPigggy",
      "created_utc": "2026-02-03 19:14:03",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.8,
      "text": "We recently open-sourced a RAG system we built for internal use and figured it might be useful to others working on retrieval-heavy applications.\n\nThere‚Äôs no novel algorithm or research contribution here. The system is built by carefully combining existing techniques:\n\n* RAPTOR-style hierarchical trees\n* Knowledge graphs\n* HyDE query expansion\n* BM25 + dense hybrid search\n* Cohere reranker (this alone gave \\~+9%)\n\nOn benchmarks, it slightly outperforms RAPTOR on multi-hop retrieval (72.89% on MultiHop-RAG) and gets \\~99% retrieval accuracy on SQuAD.\n\nWe focused on making this something you can actually install, run, and modify without stitching together a dozen repos.\n\nWe built this for IncidentFox, where we use it to store and retrieve company and team knowledge. Since retrieval isn‚Äôt our product differentiator, we decided to open-source the RAG layer.\n\nRepo: [https://github.com/incidentfox/OpenRag](https://github.com/incidentfox/OpenRag)  \nWrite-up with details and benchmarks: [https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html](https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html)\n\nHappy to answer questions or hear feedback from folks building RAG systems.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ecql6",
          "author": "DashboardNight",
          "text": "Yeah, the Cohere reranker is really good. Unfortunately it remains a catastrophe with their privacy policy, where they can use anything that you provide. A local reranker may be preferable, or even a LLM-reranker using a local model or a GDPR-compliant provider. Other than that, good stuff!",
          "score": 4,
          "created_utc": "2026-02-03 19:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3elfp7",
              "author": "captainPigggy",
              "text": "good point, let it make this clear in readme",
              "score": 1,
              "created_utc": "2026-02-03 20:24:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ee7ua",
          "author": "Oshden",
          "text": "Amazing OP! Thank you for sharing this with the world at large. I‚Äôm definitely gonna star this repo!",
          "score": 3,
          "created_utc": "2026-02-03 19:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3el9hx",
              "author": "captainPigggy",
              "text": "of course thanks!",
              "score": 2,
              "created_utc": "2026-02-03 20:23:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3exmnn",
          "author": "iLoveSeiko",
          "text": "This is really cool compilation of techniques. Thanks for sharing pal",
          "score": 1,
          "created_utc": "2026-02-03 21:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1hgp",
          "author": "Regular-Forever5876",
          "text": "Thank you sir, will have a look into your implementation üôè",
          "score": 1,
          "created_utc": "2026-02-04 00:45:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr1suf",
      "title": "Build n8n Automation with RAG and AI Agents ‚Äì Real Story from the Trenches",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "author": "According-Site9848",
      "created_utc": "2026-01-30 10:24:49",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "One of the hardest lessons I learned while building n8n automations with RAG (Retrieval-Augmented Generation) and AI agents is that the problem isn‚Äôt writing workflows its handling real-world chaos. I was helping a mid-sized e-commerce client who sold across Shopify, eBay, and YouTube and the volume of incoming customer questions, order updates and content requests was overwhelming their small team. The breakthrough came when we layered RAG on top of n8n: every new message or order triggers a workflow that first retrieves relevant historical context (past orders, previous customer messages, product FAQs) and then passes it to an AI agent that drafts a response or generates a content snippet. This reduced manual errors drastically and allowed staff to focus on exceptions instead of repetitive tasks. For example, a new Shopify order automatically pulled product specs, checked inventory, created a draft invoice in QuickBooks and even generated a YouTube short highlighting the new product without human intervention. The key insight: start with the simplest reliable automation backbone (parsing inputs ‚Üí enriching via RAG ‚Üí action via AI agents), then expand iteratively. If anyone wants to map their messy multi-platform workflows into a clean, intelligent n8n + RAG setup, I‚Äôm happy to guide and  to help get it running efficiently in real operations.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2nhhpu",
          "author": "Whole-Board4430",
          "text": "Hi i read your post, i am new to RAG. for one of my employers where i'm helping with marketing and implementing basic AI, i am building a RAG agent. For now i just built a simple agent on n8n, 1 workflow to vector the documents into a database, the agent to retrieve it.\n\nWhat i want this agent to be able to do (if possible) is using our business documents + certain books, methods and other data we find important, and transform this working together with a seperate LLM to generate content, help our customers with questions, help our team with their work, onboard new people into the team when nessecarry. Do you mind if i ask you some questions, you now have a basic view of what i am trying to get working",
          "score": 1,
          "created_utc": "2026-01-30 18:46:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qp7psa",
      "title": "Compared hallucination detection for RAG: LLM judges vs NLI",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "author": "meedameeda",
      "created_utc": "2026-01-28 10:25:07",
      "score": 9,
      "num_comments": 9,
      "upvote_ratio": 0.86,
      "text": "I looked into different ways to detect hallucinations in RAG. Compared LLM judges, atomic claim verification, and encoder-based NLI.\n\nSome findings:\n\n* LLM judge: 100% accuracy, \\~1.3s latency\n* Atomic claim verification: 100% recall, \\~10.7s latency\n* Encoder-based NLI: \\~91% accuracy, \\~486ms latency (CPU-only)\n\nFor real-time systems, NLI seems like the most reasonable trade-off. \n\nWhat has been your experience with this?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp7psa/compared_hallucination_detection_for_rag_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26vl7l",
          "author": "meedameeda",
          "text": "full write up here if interesting: [https://agentset.ai/blog/how-to-detect-hallucinations-in-rag](https://agentset.ai/blog/how-to-detect-hallucinations-in-rag)",
          "score": 6,
          "created_utc": "2026-01-28 10:25:31",
          "is_submitter": true,
          "replies": [
            {
              "id": "o2d30ki",
              "author": "aiprod",
              "text": "Reading the comment now and seeing you used RAGTruth. It‚Äôs a poor dataset full of errors. Try our modified version linked in my other comment.",
              "score": 1,
              "created_utc": "2026-01-29 05:49:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2785zr",
          "author": "Upset-Pop1136",
          "text": "we put nli as a fast filter, async llm judge on low-confidence hits, and cache verdicts per doc passage. reduced latency and cost by 70% while keeping recall. try thresholding confidence before invoking expensive checks.",
          "score": 2,
          "created_utc": "2026-01-28 12:06:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2d2hvp",
          "author": "aiprod",
          "text": "We tested NLI based detectors like azure groundedness on our Ragtruth++ dataset (https://www.blueguardrails.com/en/blog/ragtruth-plus-plus-enhanced-hallucination-detection-benchmark). And the results were very different. More like 0.35 f1 score.\n\nOur own hallucination detection (agentic verification) scores around 0.8 f1 on the same dataset.\n\nI think your high scores are an indication of a poor quality dataset or some mistakes in the benchmark setup.\n\nHere‚Äôs a video with some numbers for azure and a comparable approach to NLI from scratch (both at 0.35 - 0.45 f1): https://www.blueguardrails.com/en/videos/ragtruth-plus-plus-benchmark-creation",
          "score": 2,
          "created_utc": "2026-01-29 05:46:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27dxpi",
          "author": "youre__",
          "text": "Seems to have potential if tested for production and applied to certain applications (e.g., where information correctness is a nice-to-have, not a critical requirement).\n\nFrom the test, anything 100% seems fishy. How many samples and what are the error bars after running same test with different seeds? There‚Äôs a ‚Äú66.7%‚Äù precision number in the article, which is oddly clean (2/3), too. Was there a test/validation split with the dataset?\n\nFor hardware testing and comparison, the laptop vs gpt-5 is an interesting comparison. Network latency will be a factor as well as thinking level. So a good test might be to test the NLI over the network, even if on a cloudflare tunnel to simulate cloud. Also test thinking/non-thinking variants of smaller cloud models. This way you can see where the cutoff in performance is. E.g., Can gpt-4o-mini perform just as well as gpt-5 on the dataset? And/Or maybe another cloud hallucination detector?\n\nThis might help ground the comparison and highlight the true benefits against systems people are already using.",
          "score": 1,
          "created_utc": "2026-01-28 12:45:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o290pyn",
          "author": "Financial-Bank2756",
          "text": "Interesting breakdown. These are all post-generation detection, catching hallucinations after the model outputs them. I've been exploring the other side: pre-generation constraint with my project Acatalepsy. which uses\n\n* VIN (Vector Identification Number) ‚Äî constraint operators, not labels\n* ACU (Atomic Claim Unit) ‚Äî immutable identity, mutable confidence\n* Pulse-VIN cycle ‚Äî emission ‚Üí coalescence ‚Üí interrogation ‚Üí sedimentation\n* Confidence vectors ‚Äî multi-axis, decaying, never absolute\n\nhope this helps",
          "score": 1,
          "created_utc": "2026-01-28 17:28:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2eb2pn",
          "author": "Charming_Group_2950",
          "text": "Try TrustifAI. It provides a trust score along with explanations for LLM responses. Explore here:¬†[https://github.com/Aaryanverma/trustifai](https://github.com/Aaryanverma/trustifai)",
          "score": 1,
          "created_utc": "2026-01-29 12:08:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o26wedu",
          "author": "ThrowAway516536",
          "text": "I'd take 100% accuracy any day. If you prefer 91% accuracy, I reckon your product and data, where the LLM is integrated, aren't worth much.",
          "score": 1,
          "created_utc": "2026-01-28 10:32:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26ydem",
              "author": "meedameeda",
              "text": "if latency and cost don‚Äôt matter, 100% accuracy is obviously the right choice (but also depending on your type of production)",
              "score": 2,
              "created_utc": "2026-01-28 10:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qrdj5s",
      "title": "Reranker Strategy: Switching from MiniLM to Jina v2 or BGE m3 for larger chunks?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-01-30 18:24:03",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\nI'm upgrading the reranker in my RAG setup. I'm moving off ms-marco-MiniLM-L12-v2 because its 512-token limit is truncating my 500-word chunks.\n\nI need something with at least a 1k token context window that offers a good balance of modern accuracy and decent latency on a GPU.\n\nI'm currently torn between:\n\n1. jinaai/jina-reranker-v2-base-multilingual\n\n2. BAAI/bge-reranker-v2-m3\n\nIs the Jina model actually faster in practice? Is BGE's accuracy worth the extra compute? If anyone is using these for chunks of similar size, I'd love to hear your experience.\n\nOpen to other suggestions as well!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o31hlfn",
          "author": "ampancha",
          "text": "Switching to a longer-context reranker solves truncation, but it also means more tokens per retrieval hit flowing into your LLM. If you don't have per-query token caps and cost attribution already in place, the accuracy upgrade can quietly double your inference spend. Worth locking that down before you benchmark the new model. Sent you a DM.",
          "score": 2,
          "created_utc": "2026-02-01 21:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qlkx0",
          "author": "Horror-Turnover6198",
          "text": "Theoretically you could chunk again post-retrieval to sub 512, then rerank, and if one of the sub-chunks makes the cut you splice it back together. Or discard the other sub-chunk. Just depends on your chunking strategy I guess. Not sure if that‚Äôs appealing but it may be an option if bge or jina performance isn‚Äôt great.",
          "score": 1,
          "created_utc": "2026-01-31 04:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlyhg",
              "author": "CourtAdventurous_1",
              "text": "Bit if i truncate and then rerank it, wouldn‚Äôt it decrease the accuracy or the actual meaning of the chunk pr something like that?",
              "score": 1,
              "created_utc": "2026-01-31 04:49:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qmz8g",
                  "author": "Horror-Turnover6198",
                  "text": "So let‚Äôs say you retrieved 10 candidates of 500 words each. If you took each candidate and split it into 3 or 4 chunks, you‚Äôd end up with 30 or 40 sub-512 token chunks, that would then get reranked individually. After rerank, let‚Äôs say chunks 7, 14, and 16 ended up in your top_k. You would then lookup which chunk 7, 14 and 16 came from, and just use those. So the meaning of each chunk would make it into your reranker, just not all at once. Some semantic meaning might be lost by breaking up the chunks for the reranker, but as I understand it, it‚Äôs hard to keep tight meaning across much more than 512 tokens in the first place. I‚Äôm totally open to being called out as wrong here though.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpl86",
          "author": "jannemansonh",
          "text": "spent way too much time optimizing rerankers and chunk sizes for my doc workflows... ended up moving those to needle app since the rag stack is built in. still run custom rag for specific use cases though. for your question though - bge m3 accuracy is solid if you've got the compute, jina v2 is faster but check the multilingual overhead if you're english-only",
          "score": 1,
          "created_utc": "2026-01-31 10:39:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qth7ek",
      "title": "RAG, Medical Models <20B, guardrails, and sVLMs for medical scans ?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "author": "jiii95",
      "created_utc": "2026-02-02 01:26:49",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.85,
      "text": "[](https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22Resources%22)\n\nSo, I am in the cardiovascular area, and I am looking for small models < 20B params, that can work for my rag that is dealing with structured JSON data. Do you have any suggestions ? I also suffer from some hallucinations, and I want also to imlement guardrails for my application to answer only medical questions about cardiovascular & data that is present and cited in the docs, will LLM be efficient with some prompts for guardrails or do you have something specific to offer. I am open only for open-source solutions, not enterprise paid software.  \nI am also looking for any sVLMs (Small Vision Language Models) that can take scans of the chest region or aorta and interpret them, or at least do segmentation or classification, any suggestions? If not a complete answer you have, any resources to look into?\n\nThank you very much (If you think I can cross-post in some other subreddit, please, any answer you can give and be beneficial, please)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o32vlim",
          "author": "Yablan",
          "text": "Literally guardrails then?\n\nhttps://github.com/guardrails-ai/guardrails",
          "score": 2,
          "created_utc": "2026-02-02 01:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32wh3m",
              "author": "jiii95",
              "text": "Well I want the output to be two things: output aigh cited data or only medical content about cardiovascular. Anything else such as medical advice or any otherput must marked as Off-topic and nothing as ouput",
              "score": 1,
              "created_utc": "2026-02-02 01:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35jeqi",
          "author": "sp3d2orbit",
          "text": "Have you considered a different approach? When I try to apply RAG directly to medical Data, the problem is always in the vector embeddings. No matter how I create them (frontier models or specialized models).\n\n  \nSince you already have everything in json format that means you're dealing with structured data. I would build an ontology on top of it and do ontology guiding search‚Äã. That means you're not dealing with hallucinations. And still use the llm in the parts of the pipeline where it makes sense, just not for the parts that require no hallucination.",
          "score": 1,
          "created_utc": "2026-02-02 13:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35jkrr",
              "author": "jiii95",
              "text": "Can you please elaborate more ? Very interesting",
              "score": 1,
              "created_utc": "2026-02-02 13:43:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35qm22",
                  "author": "sp3d2orbit",
                  "text": "So I don't know your exact use case. But you mentioned cardiology. If look at the ICD-10 ontology there's something like 1200 codes that are applicable to cardiology. If we look at SNOMED-CT there might be 10 times that number if you consider all the different structures disorders etc. \n\n  \nThose codes exist because there's some sort of medical workflow or billing logic or treatment that depends on them being distinct. When you try to apply RAG to this problem, your in essence hoping that you can fragment the documents into those 1200 codes or 12,000 codes depending on your scenario. \n\n  \nEven the very best models are going to have a hard time doing that with Fidelity. I find it's better to invert the problem, use the ontology first, and then then we use the llm to do generative tasks.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:22:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qq847v",
      "title": "RAG SDK: would this benefit anyone?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq847v/rag_sdk_would_this_benefit_anyone/",
      "author": "DetectiveMindless652",
      "created_utc": "2026-01-29 13:09:32",
      "score": 8,
      "num_comments": 11,
      "upvote_ratio": 0.9,
      "text": "Hey everyone,\n\n\n\nI've been working on a local RAG SDK that runs entirely on your machine - no cloud, no API keys needed. It's built on top of a persistent knowledge graph engine and I'm looking for developers to test it and give honest feedback.\n\n\n\nWe'd really love people's feedback on this. We've had about 10 testers so far and they love it - but we want to make sure it works well for more use cases before we call it production-ready. If you're building RAG applications or working with LLMs, we'd appreciate you giving it a try.\n\n\n\nWhat it does:\n\n\\- Local embeddings using sentence-transformers (works offline)\n\n\\- Semantic search with 10-20ms latency (vs 50-150ms for cloud solutions)\n\n\\- Document storage with automatic chunking\n\n\\- Context retrieval ready for LLMs\n\n\\- ACID guarantees (data never lost)\n\n\n\nBenefits:\n\n\\- 2-5x faster than cloud alternatives (no network latency)\n\n\\- Complete privacy (data never leaves your machine)\n\n\\- Works offline (no internet required after setup)\n\n\\- One-click installer (5 minutes to get started)\n\n\\- Free to test (beer money - just looking for feedback)\n\n\n\nWhy I'm posting:\n\nI want to know if this actually works well in real use cases. It's completely free to test - I just need honest feedback:\n\n\\- Does it work as advertised?\n\n\\- Is the performance better than what you're using?\n\n\\- What features are missing?\n\n\\- Would you actually use this?\n\n\n\nIf you're interested, DM me and I'll send you the full package with examples and documentation. Happy to answer questions here too!\n\n\n\nThanks for reading - really appreciate any feedback you can give.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qq847v/rag_sdk_would_this_benefit_anyone/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2kt1t8",
          "author": "Orihara-Izaya",
          "text": "Github????",
          "score": 1,
          "created_utc": "2026-01-30 10:05:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kwf7d",
              "author": "DetectiveMindless652",
              "text": "24 hours bro",
              "score": 1,
              "created_utc": "2026-01-30 10:35:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o703k",
          "author": "Diablo_Rigs",
          "text": "I‚Äôd love to give a try, was planning on building my own but this is wonderful.",
          "score": 1,
          "created_utc": "2026-01-30 20:43:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s3uep",
              "author": "DetectiveMindless652",
              "text": "Dm me",
              "score": 1,
              "created_utc": "2026-01-31 12:41:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z7z5g",
          "author": "Expensive_Culture_46",
          "text": "If you had an interface that was user friendly this would ruin a lot of people‚Äôs scams. \n\nGo for it.",
          "score": 1,
          "created_utc": "2026-02-01 14:53:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o309p3x",
              "author": "DetectiveMindless652",
              "text": "Please dm me, this is a brilliant idea",
              "score": 1,
              "created_utc": "2026-02-01 17:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o30lfg7",
                  "author": "Expensive_Culture_46",
                  "text": "Streamlit and then flask for file uploads",
                  "score": 1,
                  "created_utc": "2026-02-01 18:42:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34a0rj",
          "author": "relcyoj",
          "text": "‚úãüôÇ‚Äç‚ÜîÔ∏èüëç",
          "score": 1,
          "created_utc": "2026-02-02 07:25:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o355nhj",
              "author": "DetectiveMindless652",
              "text": "dm me",
              "score": 1,
              "created_utc": "2026-02-02 12:13:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34wfpn",
          "author": "FancyAd4519",
          "text": "as long as its not dependant on torch or numpy",
          "score": 1,
          "created_utc": "2026-02-02 10:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o356222",
              "author": "DetectiveMindless652",
              "text": "sure isnt!",
              "score": 1,
              "created_utc": "2026-02-02 12:16:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qqfxko",
      "title": "Tried to Build a Personal AI Memory that Actually Remembers - Need Your Help!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "author": "Tough-Percentage-864",
      "created_utc": "2026-01-29 18:01:20",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "Hey everyone, I was inspired by the **Shark Tank NeoSapien concept**, so I built my own **Eternal Memory system** that doesn‚Äôt just store data - it *evolves with time*.([LinkedIn](https://www.linkedin.com/posts/abhaygupta53_ai-python-eternal-activity-7422690736359415808-PjWd?rcm=ACoAADg6WLoBZEyh7uuClgIZx3hLSD1IzZb81Nc&utm_medium=member_desktop&utm_source=share))\n\nRight now it can:  \n\\-Transcribe audio + remember context  \n\\-  Create **Daily / Weekly / Monthly summaries**  \n\\- Maintain short-term memory that fades into long-term  \n\\- Run **semantic + keyword search** over your entire history\n\nI‚Äôm also working on **GraphRAG for relationship mapping** and **speaker identification** so it knows *who said what*.\n\nI‚Äôm looking for **high-quality conversational / life-log / audio datasets** to stress-test the memory evolution logic.  \n**Does anyone have suggestions? Or example datasets (even just in DataFrame form) I could try?**\n\nExamples of questions I want to answer with a dataset:\n\n* ‚ÄúWhat did I do in **Feb 2024?**‚Äù\n* ‚ÄúWhy was I sad in **March 2024?**‚Äù\n* Anything where a system can actually recall patterns or context over time.\n\nDrop links, dataset names, or even Pandas DataFrame ideas anything helps! üôå\n\n  \n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2g9zjj",
          "author": "Tough-Percentage-864",
          "text": "Repo link --> [https://github.com/Abhay-404/Eternal-Memory](https://github.com/Abhay-404/Eternal-Memory)",
          "score": 2,
          "created_utc": "2026-01-29 18:01:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2rt5dr",
          "author": "vikasprogrammer",
          "text": "I am also interested in this, I have started working on the hardware part using this chip Seeed Studio XIAO Nordic nRF52840 Sense Module (chip -> BLE audio -> app -> AI/memory -> output). I want to combine efforts.",
          "score": 2,
          "created_utc": "2026-01-31 11:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2sgx12",
              "author": "Tough-Percentage-864",
              "text": "Cool Keep buliding üöÄ",
              "score": 1,
              "created_utc": "2026-01-31 14:05:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2iiim8",
          "author": "DetectiveMindless652",
          "text": "Dm me",
          "score": 1,
          "created_utc": "2026-01-30 00:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kqo5a",
              "author": "Tough-Percentage-864",
              "text": "ü§î",
              "score": 1,
              "created_utc": "2026-01-30 09:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ks73z",
                  "author": "DetectiveMindless652",
                  "text": "We‚Äôre working on something adjacent mate",
                  "score": 2,
                  "created_utc": "2026-01-30 09:57:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtnjhr",
      "title": "Is this \"Probe + NLI Verification\" logic overkill for accurate GraphRAG? (Replacing standard rerankers)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qtnjhr/is_this_probe_nli_verification_logic_overkill_for/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-02-02 06:30:49",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI'm building a RAG pipeline that relies on graph-based connections between large chunks (\\~500 words). I previously used a standard reranker (BGE-M3) to establish edges like \"Supports\" or \"Contradicts,\" but I ran into a major semantic collision problem:\n\nThe Problem:\n\nRelevance models don't understand logic. To BGE-M3, Chunk A (\"AI is safe\") and Chunk B (\"AI is NOT safe\") are 95% similar. My graph ended up with edges saying Chunk A both SUPPORTS and CONTRADICTS Chunk B.\n\nThe Proposed Fix (My \"Probe Graph\" Logic):\n\nI'm shifting to a new architecture and want to know if this is a solid approach or if I'm over-engineering it.\n\n1. Intent Probing (Vector Search): Instead of one generic search, I run 5 parallel searches with specific query templates (e.g., Query for Contradicts: \"Criticism and counter-arguments to {Chunk\\_Summary}\").\n\n2. Logic Gating (Zero-Shot): I pass the candidates to ModernBERT-large-zeroshot with specific labels (supports, contradicts, example of).\n\n3. Strict Filtering: I only create the edge if the NLI model predicts the specific relationship and rejects the others (e.g., if I'm probing for \"Supports,\" I reject the edge if the model detects \"Contradiction\").\n\nMy Question:\n\nHas anyone successfully used Zero-Shot classifiers (like ModernBERT) as a \"Logic Gate\" for graph edges in production?\n\n‚Ä¢ Does the latency hit (running NLI on top-k pairs) justify the accuracy gain?\n\n‚Ä¢ Are there lighter-weight ways to stop \"Supports/Contradicts\" collisions without running a full cross-encoder?\n\nStack: Infinity (Rust) for Embeddings + ModernBERT (Bfloat16) for Logic.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qtnjhr/is_this_probe_nli_verification_logic_overkill_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o35fbzw",
          "author": "saiprasad04",
          "text": "The semantic collision problem you're describing is a real pain point with embedding models - they capture topic similarity but not logical relationships. Your approach is solid.\n\nA few thoughts on your questions:\n\nFor lighter alternatives to full NLI, you might consider using negation-aware embeddings (like InstructorXL with explicit instructions about stance) or adding a simple negation detection layer before your classifier. Some teams also use contrastive fine-tuning on their specific domain to make the embedding space more logic-aware.\n\nOn latency - if you're running NLI on top-k pairs (say k=20), that's manageable in most cases. The bigger question is whether you batch these calls or run them sequentially. Batching on GPU makes a huge difference.\n\nYour strict filtering approach (rejecting edges when NLI detects conflicting signals) is a good safeguard. One refinement: consider adding a confidence threshold rather than binary accept/reject - edges with ambiguous NLI scores might warrant a different edge type like \"related\" rather than being dropped entirely.",
          "score": 1,
          "created_utc": "2026-02-02 13:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35n2f3",
              "author": "CourtAdventurous_1",
              "text": "Thanks for the suggestion but i am using multiple probes like similar,contradict,elaborate,depends on and example of,\nSo will a nli model handle all this probes or is there any alternative (and for my pipeline if the probe graph creation takes some time its not that much of a problem as it is not directly related to output of the user‚Äôs query)",
              "score": 1,
              "created_utc": "2026-02-02 14:03:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35iowx",
          "author": "jannemansonh",
          "text": "the semantic collision problem you're hitting is brutal... spent way too long wiring custom reranker + nli logic for graph-based rag before. ended up using needle app for those workflows since it handles the embedding/relationship logic internally.... way simpler than managing the bge-m3 + modernbert pipeline yourself, especially for the supports/contradicts edge cases",
          "score": 1,
          "created_utc": "2026-02-02 13:38:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35zynq",
          "author": "coderarun",
          "text": "How does this approach compare to extracting a KG and have concepts/arguments as nodes and \"SUPPORTS/CONTRADICTS\" as edges?",
          "score": 1,
          "created_utc": "2026-02-02 15:10:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qti5up",
      "title": "FAQ content formatting advice for RAG chatbot",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qti5up/faq_content_formatting_advice_for_rag_chatbot/",
      "author": "Odd-Affect236",
      "created_utc": "2026-02-02 02:09:25",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm building a RAG‚Äëbased chatbot for FAQ content. Currently, the FAQ data is stored in HTML tags as JSON within our CMS, but it contains many extra fields that aren‚Äôt needed for this use case. I‚Äôm trying to decide on the best format for storing the content. Should I use plain text (`.txt`), Markdown (`.md`), or something else?\n\nAdditionally, should all FAQs be placed in a single file or grouped logically into multiple files?\n\nI‚Äôm considering using a structure like this:\n\n    Q1\n    A1\n    \n    Q2\n    A2\n    \n    ...\n    ...\n\nDoes this approach make sense?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qti5up/faq_content_formatting_advice_for_rag_chatbot/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o37f38b",
          "author": "blue-or-brown-keys",
          "text": "Depending on how many FAQ you have, if there are too few all of it in a single context window will be useful if you have too many then you need to invest each one of them as a separate input into your vector database",
          "score": 1,
          "created_utc": "2026-02-02 19:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ghvt9",
          "author": "Suspicious-Juice3897",
          "text": "maybe, you should store them in a vector database and do a similarity search on that or bm25 and similarity search",
          "score": 1,
          "created_utc": "2026-02-04 02:18:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}