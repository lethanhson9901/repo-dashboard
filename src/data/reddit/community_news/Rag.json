{
  "metadata": {
    "last_updated": "2026-01-20 02:29:03",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 134,
    "file_size_bytes": 180687
  },
  "items": [
    {
      "id": "1qd2e2b",
      "title": "RAG at scale still underperforming for large policy/legal docs ‚Äì what actually works in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-01-14 23:00:50",
      "score": 57,
      "num_comments": 34,
      "upvote_ratio": 0.95,
      "text": "I‚Äôm running RAG fairly strong on-prem setup, but quality still degrades badly with large policy / regulatory documents and multi-document corpora. Looking for practical architectural advice, not beginner tips.\n\nCurrent stack:\n-Open WebUI (self-hosted)\n-Docling for parsing (structured output)\n-Token-based chunking\n-bge-m3 embeddings\n-bge-m3-v2 reranker\n-Milvus (COSINE + HNSW)\n-Hybrid retrieval (BM25 + vector)\n-LLM: gpt-oss-20B\n-Context window: 64k\n-Corpus: large policy / legal docs, 20+ documents\n-Infra: RTX 6000 ADA 48GB, 256GB DDR5 ECC\n\nObserved issues:\nCross-section and cross-document reasoning is weak\nIncreasing context window doesn‚Äôt materially help\nReranking helps slightly but doesn‚Äôt fix missed clauses\nWorks ‚Äúokay‚Äù for academic projects, but not enterprise-grade\n\nI‚Äôm thinking of trying:\nGraph RAG (Neo4j for clause/definition relationships)\nAgentic RAG (controlled, not free-form agents)\n\nQuestions for people running this in production:\nHave you moved beyond flat chunk-based retrieval in Open WebUI? If yes, how?\nHow are you handling definitions, exceptions, overrides in policy docs?\nDoes Graph RAG actually improve answer correctness, or mainly traceability?\nAny proven patterns for RAG specifically (pipelines, filters, custom retrievers)?\nAt what point did you stop relying purely on embeddings?\n\nI‚Äôm starting to feel that naive RAG has hit a ceiling, and the remaining gains are in retrieval logic, structure, and constraints‚Äînot models or hardware.\nWould really appreciate insights from anyone who has pushed RAG system beyond demos into real-world, compliance-heavy use cases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzqkpdf",
          "author": "Soft-Speaker6195",
          "text": "You‚Äôre not wrong - flat chunk RAG hits a wall fast on policy/legal corpora because ‚Äúcorrectness‚Äù is mostly about scope, definitions, exceptions, and precedence, not semantic similarity. The teams I‚Äôve seen get this working stop treating retrieval as ‚Äútop-k chunks‚Äù and start treating it as a constrained legal reasoning pipeline: definition expansion > scope filters > precedence rules > targeted clause fetch > answer with mandatory citations. Graph adds value when it‚Äôs used for precedence + definition binding, not as a generic knowledge graph. If you want a concrete example of how this looks in practice (especially definition/exception handling + audit-friendly outputs), AI Lawyer has some useful patterns you can mirror: strict citation gating, clause-level retrieval, and override/exception tracing.",
          "score": 14,
          "created_utc": "2026-01-15 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmphjq",
          "author": "Chemical_Orange_8963",
          "text": "Basically you are making GLEAN, research more on that on how it works",
          "score": 10,
          "created_utc": "2026-01-14 23:06:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolbyp",
          "author": "OnyxProyectoUno",
          "text": "Yeah, you've hit the ceiling that most people hit. Token-based chunking on legal docs is basically guaranteed to break cross-reference reasoning because it has no awareness of document structure.\n\nThe issue isn't your retrieval stack, it's what you're feeding it. Legal docs have explicit hierarchical relationships: definitions that apply to specific sections, exceptions that modify clauses, cross-references that span documents. Flat chunks destroy all of that before your embeddings ever see it. Doesn't matter how good your reranker is if the chunk boundaries cut through a definition-to-usage relationship.\n\nGraph RAG can help with traceability but it won't fix the upstream problem. You're still building the graph from chunks that already lost the structure. Same with agentic approaches, they're working with degraded inputs.\n\nWhat's actually worked in production for policy docs: semantic chunking that respects section boundaries, explicit metadata for document hierarchy (section > subsection > clause), and preserving cross-reference relationships as first-class data. You want chunks that know what section they belong to and what other sections they reference.\n\nDocling gives you structured output but are you actually using that structure for chunking decisions? Most people parse structured and then chunk flat anyway. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this kind of pipeline configuration, where you can see how your docs look after each transformation before committing.\n\nFor the cross-document reasoning specifically, you probably need document-level metadata propagation so chunks know which policy they came from and what other policies they relate to. That's not a retrieval problem, it's an enrichment problem that happens way before Milvus sees anything.",
          "score": 7,
          "created_utc": "2026-01-15 05:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrtoxv",
              "author": "stevevaius",
              "text": "Vectorflow still not available to test. Waiting for access",
              "score": 1,
              "created_utc": "2026-01-15 18:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrxwzu",
                  "author": "OnyxProyectoUno",
                  "text": "Apologies, the next limited early access will be in the very near future. You should receive an email a week or two before launch. \n\nIn the interim, is there something we can help you with, perhaps related to your setup or feature requests/pain points in your current setup, that you would like us to take into account as we refine the VectorFlow experience?",
                  "score": 2,
                  "created_utc": "2026-01-15 18:42:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00dcmg",
              "author": "bigshit123",
              "text": "Can you explain how you got structured output from docling? I‚Äôm parsing to markdown but docling seems to make every title a second-level heading (##).",
              "score": 1,
              "created_utc": "2026-01-16 22:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o026cwg",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, the default markdown export flattens everything to h2. You want to use the JSON output instead, which preserves the actual document structure. When you call docling, set the output format to JSON and you'll get the hierarchical layout with proper nesting levels, section types, and element relationships.\n\nThe JSON gives you stuff like section numbers, whether something is a definition block, table structure, footnote relationships. That's what you actually want to use for chunking decisions. I chunk based on the JSON structure first, then convert relevant parts to markdown only for the final chunk content. Most people do it backwards and lose all the structural info that docling worked to extract.",
                  "score": 2,
                  "created_utc": "2026-01-17 05:45:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzn3o33",
          "author": "ggone20",
          "text": "Graphs are the answer. You need to rethink chunking strategies though - semantic and section based chunking. All the questions you have are indeed things you need to work through. \n\nYou can (and should) approach retrieval from both ends: start with keyword and vector search, use the findings to traverse graph relationships - you‚Äôll need agentic search here so it can intelligently ‚Äòloop‚Äô and refine queries as needed. You can come from the graph side as well if you know certain nodes on specific edges you‚Äôre looking for to filter (keep an index). \n\nHave your agent use a ‚Äòscratchpad‚Äô during search and keep each search branch‚Äôs context clean and focused - what I‚Äôve found so far, what information I still need, search terms used).  There are a hundred more things but yea‚Ä¶\n\nI built an engineering assistant for the hydrogen mobility industry (think hydrogen fueling stations and generation facilities) that is used to validate plans and even day to day work packages against required standards, protocols, and regulations. It provides detailed report with citations of ‚Äòwhy‚Äô for go/no-go decisions.  So yes, to answer your final questions, this is the way. Prompting and intelligent search is more art than science though. Expanding user queries, prompting the user intelligently for search clarity, cacheing and vectorizing queries to store them with ‚Äòwhat worked‚Äô provenance so the agent can find like or similar answers later. \n\nA simple ranking system that users can ‚Äòthumbs up/down‚Äô responses (and potentially provide feedback) helps a lot for refining the system down the road. Good luck!",
          "score": 6,
          "created_utc": "2026-01-15 00:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzov0wj",
              "author": "cisspstupid",
              "text": "I agree to this approach. I'm myself starting to look into building knowledge graph and how to use them. If u/ggone20 have any good references for learning or tips. Those will be highly appreciated.",
              "score": 2,
              "created_utc": "2026-01-15 07:19:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpcwac",
          "author": "TechnicalGeologist99",
          "text": "Legal documents are not really semantic. \n\nThe semantics of the text help get us in the correct postcode...but it doesn't help us to reason or extract full threads of information. \n\nThis is because legal documents are actually hiding a great deal of latent structure. \n\nThis is why people use knowledge graphs for high stakes documents like this. \n\nYou need to hire someone with research expertise in processing legal text. \n\nBuilding a useful knowledge graph is very difficult.\n\nAnyone who says otherwise is a hype gremlin that's never had to evaluate something with genuinely high risk outputs.\n\nYou should also be aware that KGs usually run in memory and are  memory hungry. This will be a major consideration for deployment. Either you already own lots of RAM (you lucky boy) or you're about to find out how much AWS charge per GB",
          "score": 5,
          "created_utc": "2026-01-15 10:10:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4etw",
          "author": "FormalAd7367",
          "text": "in my experiences, law/statues/policy doc are hardest because there are so many variables that agentic ai can‚Äôt read like sec 7(x)(78) of law -> sec 7(x7(8) of law\n\nthen there are tables and long winded text.. and law that was superseded by another law like 59 times",
          "score": 3,
          "created_utc": "2026-01-15 00:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmrcb1",
          "author": "DeadPukka",
          "text": "Any links to the type of docs you‚Äôre working with? (If public)\n\nAnd what types of prompts are you using?  \n\nAre you doing prompt rewriting? Reranking?\n\nAre you locked into only on-prem?",
          "score": 2,
          "created_utc": "2026-01-14 23:16:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4sah",
          "author": "hrishikamath",
          "text": "I have a feeling you should use agentic retrieval. Also do more of metadata engineering than anything. From my personal experience building an agent for sec filings. You can look at the readme for inspiration: https://github.com/kamathhrishi/stratalens-ai/blob/main/agent/README.md I am writing an elaborate blogpost on this too.",
          "score": 2,
          "created_utc": "2026-01-15 00:28:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobqkz",
          "author": "tony10000",
          "text": "I would use a dense model rather than OSS20B.  I am not sure that a MoE model is up to the task.  If you must use MoE, try: [https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)",
          "score": 2,
          "created_utc": "2026-01-15 04:46:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzogl1v",
          "author": "Past-Grapefruit488",
          "text": "Consider : \n\n1. Full text search via Elastic Search or similar (in addition to vecros and graph store)  \n2. Agentic RAG taht uses all these and evaluates it in context of inputs (initial as well as subsequent)",
          "score": 2,
          "created_utc": "2026-01-15 05:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolkqm",
          "author": "Rokpiy",
          "text": "hierarchical chunking helps but doesn't solve cross-references. legal docs have section 7.2.3 referencing \"as defined in section 2.1\" which has exceptions in section 5.4. flat chunks break these chains, hierarchical chunks reduce it but don't eliminate it\n\nwhat worked for us: dual-layer retrieval. first pass gets relevant sections via embeddings, second pass explicitly searches for cross-reference patterns in those sections and fetches the referenced content. regex + heuristics after semantic search\n\nyou can't solve this with better embeddings because legal language encodes relationships through explicit references, not semantic similarity. \"section 7.2.3\" and \"section 2.1\" have zero semantic overlap but maximum logical dependency\n\ngraph rag helps if you pre-extract \"section X references section Y\" during ingestion. but that's a parsing problem, not retrieval. most teams skip it because the parsing is fragile and breaks on updates\n\nfor the 64k context issue: either accept incomplete context or implement multi-hop retrieval where the model asks for missing definitions when it hits a reference",
          "score": 2,
          "created_utc": "2026-01-15 05:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzom65c",
          "author": "RecommendationFit374",
          "text": "Have you tried papr.ai we have document ingestion  u can use reducto or other providers, define your custom schema and auto build graph we combine vector + graph + prediction models it works well at scale. See our docs at platform.papr.ai",
          "score": 2,
          "created_utc": "2026-01-15 06:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzometk",
              "author": "RecommendationFit374",
              "text": "Our open source repo here https://github.com/Papr-ai/memory-opensource \n\nWe will bring doc ingestion to open source soon",
              "score": 2,
              "created_utc": "2026-01-15 06:06:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzqkbvr",
          "author": "DistinctRide9884",
          "text": "Check out SurrealDB, which is multi-model and has support for graph, vectors, documents and can be updated in real time (vs. other graph DBs where you have to rebuild the cache each time time you update the graph).\n\nThen for the documenting parsing/extraction something like¬†[https://cocoindex.io/](https://cocoindex.io/)¬†might be worth exploring, their core value prop is real-time updates and full traceability from origin into source. A CocoIndex and SurrealDB integration is in the works.",
          "score": 2,
          "created_utc": "2026-01-15 14:58:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqvuzd",
              "author": "Early_Interest_5768",
              "text": "CocoIndex looks great thanks.",
              "score": 2,
              "created_utc": "2026-01-15 15:52:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo6491",
          "author": "Recursive_Boomerang",
          "text": "https://medium.com/enterprise-rag/deterministic-document-structure-based-retrieval-472682f9629a\n\nMight help you out. PS I'm not affiliated with them",
          "score": 1,
          "created_utc": "2026-01-15 04:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobiin",
          "author": "charlesthayer",
          "text": "Please tell us a little bit more about your current setup. I'm wondering how many agents or sub agents you are currently using? \n\nEg. Do you have a search assistant agent that focuses just on finding docs (without full context)? \nAre you using a memory system like mem0 or a skill system like ACE?\nDo your prompts include both negative and positive examples? \nDo you have a set of evals that provides precision and recall (relevance)\n\nMy first thought is that it would probably help to extract a fair amount of metadata for each document into a more structured database. So this would be a separate pipeline that understands key important things that you're looking for in these docs. Eg. Which compliance or audit standards are discussed, etc.\n\nAdding the graph DB should be a great help. Doing multiple levels of chunking so that you include whole paragraphs and sections will also be a help for ranking. I'm not familiar with law or legal documents, but I imagine there may be some models fine-tuned for your legal domain.\n\nSounds interesting!",
          "score": 1,
          "created_utc": "2026-01-15 04:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoc0kd",
          "author": "tony10000",
          "text": "You may also want to check out Anything LLM.  It has excellent RAG capabilities.",
          "score": 1,
          "created_utc": "2026-01-15 04:48:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzojqrq",
          "author": "AloneSYD",
          "text": "First you need to work on you chunking for splitting you need to optimize splitting for the your kind of documents and specially cross pages, section\n\nLook up contextual retrieval, basically you need to add metadata to each chunk. CR can help in two ways either like a first stage retrieval or it can embedded with each chunk to enhance relevant chunks\n\n If you don't have enough context length to take a full document, make a custom one that will take n #pages before and after to create the context for each chunk instead of the whole document.\n\nI would highly recommend is to use a reAct agent. Because the reflection step help it in many situations to requery until it reaches a satisfiying state and you can specify the criteria on the answer is complete",
          "score": 1,
          "created_utc": "2026-01-15 05:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp0q11",
          "author": "cat47b",
          "text": "Can you share an anonymised example of the exceptions and overrides text. As others have said a chunk may need metadata that refers to others which need to be retrieved as part of that overall context",
          "score": 1,
          "created_utc": "2026-01-15 08:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpp9nl",
          "author": "HonestoJago",
          "text": "Law firms are hard. As soon as there‚Äôs one slight error, or one response that isn‚Äôt ‚Äúcomplete‚Äù, everyone will stop using it. It‚Äôs probably malpractice just to rely on the RAG, anyway, and not review the full underlying docs, so I‚Äôm not really sure there‚Äôs a lot of value there. I think tools for email management and retrieval are easier to build and are more attractive to attorneys, who are constantly overwhelmed by emails and can‚Äôt keep track of project status, etc‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-15 11:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqciya",
          "author": "my_byte",
          "text": "Hard disagree with folks screaming graph. If you do A/B with GraphRAG vs agentic, multi turn search and normalize for tokens - the latter will have similar results, without the operational headache that comes from graph updates, disambiguation and such.\nI'm not sure if I like gpt-oss tbh. Have you tried other models?\nGenerally speaking - what's your approach to measure recall and accuracy in your system?",
          "score": 1,
          "created_utc": "2026-01-15 14:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzs2ta0",
          "author": "ClearApartment2627",
          "text": "20+ documents? Did you mean 20k+ or 20m+ docs? Just asking because 20 does not sound right, and 20m might need a different architecture than 20k.",
          "score": 1,
          "created_utc": "2026-01-15 19:04:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzttohd",
          "author": "MrTechnoScotty",
          "text": "Perhaps RLM is worth a look?",
          "score": 1,
          "created_utc": "2026-01-16 00:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuw0ip",
          "author": "One_Milk_7025",
          "text": "Your chunking strategy is not perfect i feel. token based basic chunking wont do much when you need interlinking cross document reference. Docling is already good choice..   \nPick a AST based chunker like markdown-it if you can convert you existing doc to a MD file , extensively extract metadata from the chunks and atttach back to it(Qdrant support it natively). Optionally use a NER model like Gliner to extract the entities from those chunk text and header , this gives a common Concept registry which can be very helpful to create the graphDB. Chunking is the most important part of this, you need to extract parent/neighbour chunk relation, line count, section header , optionally token count etc etc.. work on the chunking pipeline and find what is more suitable for you.  \nfor graphdb its not necessary to use Neo4j but it does have its perks.. but for start you can use postgres +qdrant . it gives both hard graph from postgres which contains the file structure and hierarchy and semantic graph from qdrant.. but to have actual graph like structure its the Concept registry where things get really connected..  \nnow for the retrieval part it will be now much more easy to hop around those concepts, expanding the neighbor..",
          "score": 1,
          "created_utc": "2026-01-16 03:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwv5t8",
          "author": "Alternative-Edge-149",
          "text": "I think you should try the graphiti mcp server with ollama and use qwen 8b embedding + qwen 32b vlm or qwen 32b llm or something similar. Graphiti works for this usecase precisely since it is a temporal graph. You can connect it with Neo4j or Falkordb. It might not work out of the box with ollama as it requires the new open ai \"/v1/responses\" endpoint instead of chat completions endpoint but it can be done by something like LiteLLM. This should be accurate enough",
          "score": 1,
          "created_utc": "2026-01-16 12:59:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0326ic",
          "author": "andreasdotorg",
          "text": "20+ documents? Is that a typo or the actual number?\n\nI'm working with corporate policy, about 100 documents, plus legal texts relevant to them. This is purely agentic, I use Claude Code with Opus 4.5, with a lot of subagents.\n\nAll data storage is on disk, using ordinary file tools, no RAG in sight. And I don't think it's needed. What's a policy document, 5k tokens? I can have 20 in context and still have 100k tokens headroom in context.\n\nHere's a high level overview of what I do.\n\nOne important subagent is source intake. There's an agent fetching the original source, extracting full text and images from it (for PDF, .docx, web sites, whatever), creating a summary in a standardized format, including source location, link to full text, ready made citation in Chicago Style Manual format, and relevance to us (there's a high level background on jurisdiction, legal status, company size etc. in the global context that all agents get). Subagent has some CLI prompts for processing, it knows how to call lynx -dump or pdftotext. \n\nAnother one is the legal researcher. It knows how to call another subagent doing research on our legal database for case law, cross references to other relevant laws, etc. It provides a list of sources (then to be processed by source intake) , and a preliminary answer to the legal question.\n\nThere's a subagent for actually writing the text answering a research question. It has an extensive style guide: every sentence either a premise or a conclusion. Every premise has a citation. Conclusions depending on interpretation need a citation too backing up this conclusion. \n\nThen, a subagent for validation. Does the cited document exist? Is the citation precise enough? Does the citation actually support our statements? Do conclusions follow? Any language imprecise? Any claim unsubstantiated? Anything we missed?\n\nWorks pretty well for me, got compliments from actual lawyers.",
          "score": 1,
          "created_utc": "2026-01-17 10:34:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0e5kdn",
          "author": "IllFirefighter4079",
          "text": "How many docs? I am building a Rust with Rig RAG ai client. I have 30,000 emails to try it on. Already built a version that put them all in a flagged database and labels them with llm. Wanted to see if I could do better with rag!",
          "score": 1,
          "created_utc": "2026-01-19 00:56:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdy998",
      "title": "Job wants me to develop RAG search engine for internal documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qdy998/job_wants_me_to_develop_rag_search_engine_for/",
      "author": "Next-Self-184",
      "created_utc": "2026-01-15 22:39:17",
      "score": 50,
      "num_comments": 82,
      "upvote_ratio": 0.96,
      "text": "this would be the first time I develop a RAG tool that searches through 2-4 million documents (mainly PDFs and many of those needing OCR). I was wondering what sort of approach I should take with this and whether it makes more sense to develop a local or cloud tool. Also the information needs to be secured so that's why I was leading toward local. Have software exp in other things but not working with LLMs or RAG systems so looking for pointers. Also turnkey tools are out of the picture unless they're close to 100k.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qdy998/job_wants_me_to_develop_rag_search_engine_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nztftcm",
          "author": "GroundbreakingEmu450",
          "text": "My two cents, while open for more experts to chime in: 2/4 million is a lot of documents and pdf is the worst format to start with. I would start by identifying the most valuable subset of documents, define a conversion approach (pref. towards markdown files) and then experiment with different chunking strategies (esp. parsing) to test retrieval. Embedding and Reranking should be considered. Those models are usually tiny and can run locally. For the reasoning part (responding to the user query by formulating an answer starting from the retrieved chunks) you can think about using a cloud LLM to have more advanced reasoning and potentially less hallucinations.",
          "score": 28,
          "created_utc": "2026-01-15 22:53:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztpvvl",
              "author": "OnyxProyectoUno",
              "text": "Parsing tools will already output markdown. That's the whole job of parsing, converting unstructured data like PDFs to more structured data in an LLM friendly document format like Markdown (or worse but acceptable, JSON).",
              "score": 4,
              "created_utc": "2026-01-15 23:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztsn6m",
                  "author": "ggone20",
                  "text": "It isn‚Äôt this trivial‚Ä¶ parsing pdfs sucks still today in 2026. If it‚Äôs just words with one column, it‚Äôs awesome. Two columns? You need to babysit it or develop more robust processes. Tables and charts? Lmao yea‚Ä¶ LLM/vLM parsing (expensive and/or slow). \n\nYou can rapid parse any corpus‚Ä¶ is what you get out useful and retrievable afterwards? Not likely‚Ä¶",
                  "score": 8,
                  "created_utc": "2026-01-16 00:02:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztz1ep",
              "author": "MrTechnoScotty",
              "text": "I agree in principle:  Start with a bite size chunk that focused on some subset of docs that can offer biggest outcomes when transformed.  Learn, iterate, it will begin to get clearer how scaling should occur‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-16 00:36:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztifap",
          "author": "nicoracarlo",
          "text": "What u/GroundbreakingEmu450 said it true. When you start handling such large amount of data, a simple RAG is going to get very messy and starts hallucinating (or missing important facts). You should look at graph rags and drift techniques, but those are not things you learn in a day or a week, as they require solid foundations (and analysing 2-4m documents for a graph rag is going to be very expensive even using specific models with low CPM) and it will take a lot of time.  \nI build a graph rag with drift and it takes between 10 to 60 seconds (depending on size and complexity)  to properly analyse a document. do the math...  \nMaybe at this point an off the shelf solution as u/BigNoseEnergyRI suggested may be less painful",
          "score": 7,
          "created_utc": "2026-01-15 23:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvy5wz",
              "author": "TechnicalGeologist99",
              "text": "Not to mention 2-4 million docs is looking to be 100 million nodes atleast. That's gonna be one expensive bill in terms of memory/ hosting fees",
              "score": 3,
              "created_utc": "2026-01-16 08:28:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o085ylp",
                  "author": "wing-of-freak",
                  "text": "We recently got hit with a pretty significant Pinecone bill. After doing some research, we found that S3 Vector Buckets are roughly 90% cheaper, with the main tradeoff being higher latency. Given the scale of our data, that tradeoff is acceptable for us. So we‚Äôre moving to S3 Vectors, which seems to be a better fit for our needs.",
                  "score": 1,
                  "created_utc": "2026-01-18 03:07:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztuxlx",
              "author": "BigNoseEnergyRI",
              "text": "Great advice. I wasn‚Äôt trying to be glib. The request is so general. Everything needs to be secure? They are describing every business out there. What industry? Compliance? Search to do what? Do they need to index/extract/enrich 2-4M documents or is there any curation needed?",
              "score": 1,
              "created_utc": "2026-01-16 00:14:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxfqv1",
              "author": "Popular_Sand2773",
              "text": "For a graph at that scale have you considered knowledge graph embeddings. Sounds like it strikes the right balance you‚Äôd be looking for.",
              "score": 1,
              "created_utc": "2026-01-16 14:49:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztq8ui",
          "author": "exaknight21",
          "text": "What resources have they made available to you? I would suggest do it locally. Get your hands on a 32 GB+ GPU, 3 if you can, and here is why:\n\n1. Run an 8b model (i prefer qwen3 family, and personally use qwen3:4b for my construction documents, I do government contracts only). This is your main LLM so serve with vLLM. If budget is low, go Tesla V100 32 GB. If budget is good, go RTX Pro 6000 Blackwell.\n\n2. Get another GPU, this I think should be a V100 32GB and will process all documents for embedding. You don‚Äôt need vLLM for this, Hugging Face Transformers works perfectly fine because most requests are batch processed and you want it to be like that.\n\n3. Get another GPU for OCR and use PaddleOCR-VL - it is genuinely good. \n\n\nOnce you‚Äôre good, deploy your gateway, I use dokploy, so get a container going for your gateway, a web app, and you have an entire pipeline.\n\nFor RAG, use lanceDB + S3 to store vector data directly into the S3, saving on storage otherwise you‚Äôll need excessive amount of data/drive management. I use an older 0.17.0 version of lanceDB + Backblaze B2. Latency is 400-500 ms for me. S3 would be 200-300, I ain‚Äôt dying.\n\nQuality of OCR matters, but so does organization of documents, and knowledge graphs. Use dgraph for knowledge graph, and call it a day.\n\nIf you‚Äôre not dealing with Handwritten Text, then you can use something like OCRMyPDF (i wrapped an API around it, it‚Äôs at https://github.com/ikantkode/exaOCR - completely free and dockerized). This runs CPU only and is very effective for me since I don‚Äôt work with HTR (handwritten text recognition). I mainly deal with scanned/text recognizable PDF, and it works like a charm.\n\n\nYou‚Äôd wanna create a gateway",
          "score": 6,
          "created_utc": "2026-01-15 23:49:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03u4t6",
              "author": "NeoNix888",
              "text": "https://preview.redd.it/pthygm472xdg1.png?width=2146&format=png&auto=webp&s=6b78b2d26b288f22b9983872cd082bf7f6fbc4d1\n\nThat's kewl you are using Gwen3 because they are on the top ten on [hugginghugh.com](http://hugginghugh.com) AI model scores!\n\nI have not use it yet, still learning.",
              "score": 1,
              "created_utc": "2026-01-17 14:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o03vx1k",
              "author": "NeoNix888",
              "text": "https://preview.redd.it/nwqukah73xdg1.png?width=2130&format=png&auto=webp&s=66ee02aaee8d6d391a2206be92354c5934cb7753\n\none more thing u/exaknight21 \\- there is 1 vulnerability in the exaOCR in case you didn't know. I got the report from [sbomly.com](http://sbomly.com) for free basically, it took less than a minute to get the full report.\n\nyou would get the below in the report and can click to go directly to the GHSA and CVE. they even provide SBOMQS scores!\n\nCheers!\n\n# üìö python-multipart\n\nv0.0.9¬†purl: pkg:pypi/python-multipart@0.0.9\n\nLIBRARYAPACHE-2.01 VULN (HIGH)\n\n**üîê**Hashes:**‚ìò**0\n\n**üîó**References:**‚ìò**0\n\n**‚öôÔ∏è**Properties:**‚ìò**5\n\n**‚ö†Ô∏è**Vulnerabilities:**‚ìò**1\n\n‚ö†Ô∏è Vulnerabilities (1)\n\n**GHSA-59g5-xgcq-4qw3 CVE-2024-53981** \n\nHIGH\n\nDenial of service (DoS) via deformation \\`multipart/form-data\\` boundary\n\n**‚úÖ Fix Available:**¬†Upgrade to version¬†**0.0.18**\n\n**EPSS:**¬†**0.1%**¬†chance of exploitation¬†(Top 68.9% most likely)\n\n**Risk Score:**¬†**0.09**¬†(combines CVSS + EPSS + KEV)\n\nCVSS: 7.5CWE-770\n\n[üîó ](https://nvd.nist.gov/vuln/detail/CVE-2024-53981)NVD\n\n**Risk Score:**¬†62/100 (MEDIUM)\n\n**üéØ Recommendations:**\n\n* Update python-multipart to fix 1 vulnerability\n\nüìñ View Technical Details",
              "score": 1,
              "created_utc": "2026-01-17 14:14:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuvdcl",
          "author": "One_Milk_7025",
          "text": "everythign can be ran locally..  \nUse docling to parse those pdf with page number included..  \nonce you get the markdown then the actual play starts. Pick a AST based chunker like markdown-it , extensively extract metadata from the chunks and atttach back to it(Qdrant support it natively). Optionally use a NER model like Gliner to extract the entities from those chunk text and header , this gives a common Concept registry which can be very helpful to create the graphDB. Chunking is the most important part of this, you need to extract parent/neighbour chunk relation, line count, section header , optionally token count etc etc.. you can use local bge-m3 quantized model or even the embedding-gemma 300m model.. which works fine for me.  \nfor graphdb its not necessary to use Neo4j but it does have its perks.. but for start you can use postgres +qdrant . it gives both hard graph from postgres which contains the file structure and hierarchy and semantic graph from qdrant.. but to have actual graph like structure its the Concept registry where things get really connected..  \nnow for the retrieval part it will be now much more easy to hop around those concepts, expanding the neighbor.. i hope this helps..",
          "score": 4,
          "created_utc": "2026-01-16 03:37:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvsk2q",
          "author": "fabkosta",
          "text": "We built search engines for 600m docs. But even with 2 - 4m things are really complicated, you need a lot of infra.\n\nSteps:\nFirst, build an OCRing system that scales. This is its own application that runs independently from the entire ingestion.\n\nSecond, build scalable ingestion and search infrastructure. That‚Äôs complicated. You will probably need mass storage, Kafka, back propagation pressure, and Elasticsearch.\n\nThird, build the RAG system. Use hybrid search, it‚Äôs almost always superior than RAG alone. But you must take a measuring approach, each change must be applied to eg 100 sample docs, and you measure whether a change improves something or not.\n\nYou see, these are really 3 distinct projects. There is a lot more here, eg OCRing will yield bad results, and you‚Äôll need a language model to correct OCRing results. (Disclaimer: I am selling consulting for such stuff.)",
          "score": 5,
          "created_utc": "2026-01-16 07:37:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztjgx0",
          "author": "Hour-Entertainer-478",
          "text": "For response generation: \nYou need to find a nice local llm that would hallucinate less, findinfo buried in lots of chunks, handle follow up conversations, and offer good tool calling. \nI have been doing pretty much that for different companies so ill say got oss 20b qwen3:30ba3b and gemma3:27b are good options. \n\nFor parsing docs: docling with rapidocr is a good start. \n\nFor 2 million docs, id suggest qdrant. It is generally faster than weaviate. (Vector db)\n\nOpinions are my own, hope that helps",
          "score": 4,
          "created_utc": "2026-01-15 23:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztkmyt",
              "author": "Next-Self-184",
              "text": "Thanks for responding - I'm assuming you'd need a pretty decent hardware setup?",
              "score": 1,
              "created_utc": "2026-01-15 23:19:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzts37u",
              "author": "ggone20",
              "text": "I really like got-oss also. I would layer with Qwen-vl‚Ä¶\n\nDocling is OK but the chunk strategy needs to be a lot more robust, it‚Äôll never work out if the box for this many documents. \n\nStorage explodes since you‚Äôll definitely need blob/object/artifact storage (s3), vector store, traditional db (Postgres-like is preferable), and knowledge graph‚Ä¶ local is ideal to ‚Äòkeep costs down‚Äô, specifically ongoing costs, but the hardware here isn‚Äôt trivial if you want anything done in a reasonable amount of time and HA durable with backups. If you‚Äôve never done infra before, bad time to start‚Ä¶\n\nThis is not a learn on the job endeavor‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-15 23:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzvrjia",
          "author": "Much-Researcher6135",
          "text": "For document prep, there are several solid libraries out there (`marker`, `docling`, `unstructured`, `pymupdf`), depending on what type of layout and information are in the PDFs, but really good results may only come with neural net layout detection *followed* by OCR, not just OCR alone.\n\nBy all means go the non-ML route first to see if something like `ocrmypdf` will be sufficient. If so, that's a big win, because the really good stuff requires GPU acceleration.\n\nSo yeah, experiment. I am building RAG for a lot of funky stuff like [math-heavy PDFs](https://linear.axler.net/). For that, [marker](https://github.com/datalab-to/marker) outclassed everything else I listed above, but it's also the slowest., Guess you get what you pay for.\n\nEven that isn't always good enough, so in my workflow, I typically take marker's JSON output and run it through a cleanup XGBoost model I hand-tuned to omit stuff that annoys me. In other words, I burn a lot of GPU cycles to let marker do most of the work, and the library comes with some absolutely brilliant pre-trained neural nets, but there's still a bit of cleanup to do.\n\nAnyway running marker to convert a folder of files is dead simple code-wise (python or CLI). The following is done serially because the box in question has only one GPU, an RTX 3090, and marker will saturate that GPU while using maybe 15/24 GB VRAM on certain files ([this two-column technical textbook was brutal](https://networksciencebook.com/)):\n\n    # Convert a folder of books (epub, pdf) to markdown + JSON\n    from pathlib import Path\n    import os\n    import json\n    from marker.converters.pdf import PdfConverter\n    from marker.models import create_model_dict\n    from marker.renderers.markdown import MarkdownRenderer\n    from marker.renderers.json import JSONRenderer\n    \n    path_input = Path(\"input\")\n    path_output = Path(\"output\")\n    \n    for fn in os.listdir(path_input):\n        fp_in = path_input / fn\n        stem = fp_in.stem\n        fp_out_md = path_output / f\"{stem}.md\"\n        fp_out_json = path_output / f\"{stem}.json\"\n    \n        print(f\"Converting {stem}\")\n        if fp_out_md.exists() or fp_out_json.exists():\n            print(\"   already exists, skipping.\")\n        else:\n            converter = PdfConverter(artifact_dict=create_model_dict())\n            doc = converter.build_document(str(fp_in))\n            md_out = MarkdownRenderer()(doc)      # pydantic model\n            js_out = JSONRenderer()(doc)          # pydantic model\n    \n            fp_out_md.write_text(md_out.markdown, encoding=\"utf-8\")\n            fp_out_json.write_text(json.dumps(js_out.model_dump(mode='json'), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n            print(f\"   {stem} converted\")",
          "score": 4,
          "created_utc": "2026-01-16 07:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuzn6y",
          "author": "GP_103",
          "text": "Missing from this conversation and really the starting point is what‚Äôs the use case?\n\nIf you need ground truth and linked citation, then none of the aforementioned solutions may work.",
          "score": 3,
          "created_utc": "2026-01-16 04:03:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy8ncx",
              "author": "Next-Self-184",
              "text": "when you say linked citations, do you mean like retrieving the documents with the keywords?",
              "score": 1,
              "created_utc": "2026-01-16 16:58:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz0yer",
                  "author": "GP_103",
                  "text": "I mean whatever natural language /text AI wraps around the answer, you include a link or citation to the actual content block in the document.\n\nThat and evaluation suite, validation and visibility/traceability across the pipeline and back to the actual source.\n\nBasic for Healthcare, finance, legal, engineering and now enterprise in general",
                  "score": 1,
                  "created_utc": "2026-01-16 19:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzx1lev",
          "author": "voycey",
          "text": "So I have just spent the last 6 months building exactly this, unfortunately you are in for a rough time.\nAs others have said, PDF parsing isn't a solved thing, it requires a LOT of steps to get right.\n\nUltimately you will have to contend with a few different things:\n\n1. Sovereignty issue - unlikely you can use public APIs with company / PII data \n2. Contention issues - local LLMs are literally the \"Fast, Cheap, Good - Pick 2\" paradox\n3. Machines reading documents is not the same as people reading documents.\n4. Infrastructure - you better be happy deploying some specific tooling\n5. Speed - Good RAG is S L O W - are your users going to be ok with a 20-40 second round trip?\n\nShortcuts for you:.\n1. Use Bedrock and Nova Pro 2 - this is probably the fastest way to get something that will work at scale up and running. The RAG isn't great but you can build a simple chatbot in Lex or use Open WebUI and hook it up to your Bedrock pipeline, you then basically just upload all of the docs to the Bedrock KB S3. Also keeps it somewhat sovereign (although watch out for CRIS on Bedrock models).  Other clouds have something similar for this!\n\n2. Buy something readymade that does this and focus instead on the data engineering challenges that surround this like the pre RAG and post RAG pipelines to actually derive value from those documents! I can help here!\n\nTons of trade-offs DIYing it, really comes down to what you want to get out of it at the end!",
          "score": 3,
          "created_utc": "2026-01-16 13:36:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyadvn",
              "author": "Next-Self-184",
              "text": "yea 20-40 second round trip is fine compared to hours of searching. been seeing a lot of feedback recommending to really focus on the OCR pipeline and then get elastic search to deal with the vectors.",
              "score": 1,
              "created_utc": "2026-01-16 17:06:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02r47w",
                  "author": "voycey",
                  "text": "I would personally avoid ElasticSearch - if you are deploying a service for this then just start with Postgres for everything. The only time you would want to use ES is if you are planning on indexing the full documents in there (reasons for and against this).\n\nThe OCR pipeline has many solutions - Comprehend is very good from AWS and you can pipeline that many documents easily, however, your chunking strategy / solution is the most important thing to get right and there are no easy / ready made solutions out there that solve this for you, it will be different on the document type, the document content and several other things!",
                  "score": 1,
                  "created_utc": "2026-01-17 08:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nztkc6a",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's a classic enterprise RAG problem. The scale isn't the hard part, it's getting consistent results across 2-4 million documents when you can't see what went wrong during processing.\n\nStart with your document processing pipeline first. OCR quality varies wildly between tools, and bad OCR creates garbage chunks that'll haunt your retrieval later. Test Tesseract vs cloud OCR on a sample of your PDFs to see what you're working with. For parsing, Unstructured handles PDFs reasonably well, but you'll want to preview what your documents actually look like after each transformation step.\n\nChunking strategy matters more than your vector store choice here. Recursive chunking works for most enterprise docs, but test different chunk sizes on your actual content. 500 tokens might work great for technical docs but fail on contracts or reports.\n\nLocal makes sense for security, but don't underestimate the infrastructure overhead. You'll need serious compute for embedding 2-4 million docs, plus vector storage that can handle the scale. Qdrant or Chroma can work locally, but plan your hardware accordingly.\n\nThe real killer is iteration speed. Every config change usually means reprocessing everything, which gets expensive fast at your scale. That's what I've been building around at vectorflow.dev, but honestly any approach that lets you test configurations before committing to the full pipeline will save you weeks.\n\nWhat type of PDFs are you dealing with? Scanned documents vs native PDFs change your whole preprocessing approach.",
          "score": 4,
          "created_utc": "2026-01-15 23:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqg5m",
              "author": "ggone20",
              "text": "This is good advice. The part to really pay attention here is iteration as mentioned. \n\nYou can‚Äôt test your pipe against 1000 docs and assume it‚Äôs good because your evals hit hard. If you‚Äôre not doing the full ingestion and testing against the full corpus, you have no idea what performance is actually going to be like. \n\nHow many users you‚Äôre serving, how many updates per day/week/month are needed (re-indexing is a pain in the ass) and what downtime is acceptable while that happens? Are you serving users in different time zones where you can only do re-indexes on weekends? Rolling indexes? How will you do change management? Lol the list goes on.",
              "score": 3,
              "created_utc": "2026-01-15 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01tx46",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, the rolling index problem is brutal at that scale. Most people don't think about it until they're staring at 8 hour reprocessing times for a single config change.\n\nChange management gets weird too because you can't really do proper A/B testing when your index is that large. You end up having to commit to architectural decisions based on smaller samples and hope they hold up. The timezone constraint is real, especially if you're dealing with global teams who need the system during your maintenance windows.\n\nHave you dealt with incremental indexing before? I'm wondering if OP could get away with treating document updates as deletes plus inserts rather than trying to build something more sophisticated for their first pass.",
                  "score": 3,
                  "created_utc": "2026-01-17 04:14:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztzoom",
              "author": "MrTechnoScotty",
              "text": "I think ocr should be a separate process from the ingesting.  OCR using known quality tool(s), likely NOT AI, then work on converting/ingesting the output docs that have been tested to equal = ‚Äúgood‚Äù‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-16 00:40:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01y75a",
                  "author": "OnyxProyectoUno",
                  "text": "That's the right call. OCR as a preprocessing step keeps your pipeline cleaner and lets you validate quality before anything hits your vector store. Tesseract with some postprocessing usually gets you there for most enterprise docs, though you might need to tune confidence thresholds per document type.\n\nThe tricky part becomes managing that two-stage workflow at scale. You'll want to track which source documents failed OCR, what your confidence scores look like across batches, and probably build in some manual review process for borderline cases. Once you've got clean text output though, the rest of your RAG pipeline becomes much more predictable since you're not debugging OCR artifacts mixed in with chunking or retrieval issues.\n\nAre you planning to reprocess the OCR if it fails quality checks, or just flag those documents for manual handling?",
                  "score": 1,
                  "created_utc": "2026-01-17 04:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvgpo9",
          "author": "Select-Spirit-6726",
          "text": "It‚Äôs great that your team recognizes the scale early ‚Äî millions of PDFs with OCR is a hard engineering problem on its own before you even get to an LLM.\n\nA few folks here are pointing you toward Elastic + pgvector or similar solutions, and that‚Äôs not accidental. Doing reliable vector search at that scale *first* will make whatever generative layer you add later much more accurate and cost-effective.\n\nAlso, starting with a strict chunking + OCR pipeline and validating extraction quality at smaller scale will save you from scaling bad vectors later. Even a well-tuned vector store alone can answer a lot of semantic queries without invoking a language model on every request.\n\nIf security and local deployment are priorities, open-source vector stores and on-prem OCR tools are totally reasonable to start with. Just be sure to invest as much in ingestion and indexing quality as you do in the LLM layer.",
          "score": 2,
          "created_utc": "2026-01-16 05:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxbfdq",
          "author": "Majinsei",
          "text": "2 million is quite a large number~\n\nYou need to do a classification analysis of exactly what you have...\n\nProcessing invoices is not the same as processing laws, processing minutes, processing a survey, etc.~\n\nIf necessary, each of these will surely have a custom extraction method, although creating about 20 different extraction methods is easier than creating several hundred.~\n\nThere are several ways to do it, but a popular strategy is to extract the metadata. You create a metadata table, and your agents search using that metadata as the first filtering step. Then you use the embedding of those filters to get the relevant data from there.\n\nThe metadata, for example, involves adding an Entity extractor step:\n\n{\"Type\": City, \"name\": Los Angeles, \"id\": 5}\n\nAnother for dates, and so on, depending on the data model you need.\n\nSo, when the user asks for the city of \"Los Angeles,\" an Entity extractor is used in the query, allowing you to filter by the city dimension (this has its pros and cons). And so on with each data point. You can optimize it later with a custom model that does everything at once.\n\nAnd these filters based on your metadata require filtering by ranking the N most similar.\n\nThis is a more or less effective method.\n\n\nRegarding OCR, you have to consider two different cases. One is pure images to be OCRed; there are many good cloud-based models for this, and you can even customize them yourself according to your needs. And you also have to consider when you have the explanatory context; a graph requires knowing which line goes up and which line goes down.\n\n\nAlthough this depends on how granular you want the information to be.",
          "score": 2,
          "created_utc": "2026-01-16 14:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz8nwg",
          "author": "patbhakta",
          "text": "2-4mill documents and mostly OCR... You're in for a treat. I wish you well. \n\nYou don't need RAG just yet you need an architect. Let's say you ignore my advice so you proceed...\n\nLet's say you find a YouTuber follow some clever selling n8n\\rag implementation and it might even work or so you think on the first document. Maybe even the 10th or 100th document. So you think. \n\nThen reality sets in your users who actually retrieve the stuff find the information garbage when researching a bit more. Or worse your AI hallucinates information based on your embeddings. \n\nThen you work on 2.0\nOnly to find chunking large documents suck\nOnly to find OCR sucks\nOnly to find charts, tables, diagrams, media is lost in ingestion\nYou come to find data cleansing is hard on just a few different documents let alone hundreds or even thousands.\n\nBut let's say you figured it all out now you need to work on 3.0\nThis involves scaling vector databases, graph data, traditional data, etc. \nCongrats you managed to embed 4 million documents! \nYou get a meeting with the CFO about why the database cost too much monthly, users are also complaining about it being slow at times, constantly getting unrelatable documents, and it's just clunky in general\n\nNow it's time for 4.0\nYou discover the databases are littered with duplicate entries from ingesting similar template documents, you discover dedup methods and implement that, potentially breaking your beautiful vectors. \nNow your CEO is happy about the speed optimization and reduced cost, however the users still think it's shit and unreliable. Congrats you spent months on something nobody can use or trust... \n\nThen you still have 5.0, 6.0, 7.0, 8.0 problems too such as  data security, everyone in the damn company has access to all 4 million documents and it's crappy info. CEO doesn't want proprietary data going to openAI or others. New documents are made constantly and it's mixing with old documents and confusing the shit about what's newer. Users find your system to be shit, they end up manually getting relevant docs uploading them to ChatGPT so it can give a seemingly intelligent answer to the dummy who knows well whatever you built is worse. \n\nCEO cans the project because AI can't be trusted, data is flawed, staff is lazy and inept now thanks to your RAG.",
          "score": 2,
          "created_utc": "2026-01-16 19:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztrd1y",
          "author": "Infamous_Ad5702",
          "text": "I did it.\nI do it via an index. And build a new KG on the fly for each new query. \nIt‚Äôs offline.\nDeterministic \nNot AI.\nI can show you.\nI do reddit webinars now apparently",
          "score": 2,
          "created_utc": "2026-01-15 23:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztpinp",
          "author": "TrevorHikes",
          "text": "Talk to https://aicamp.so/\n\nAnd no I‚Äôm not affiliated but I have used their platform and they have been doing on premise solutions for high compliance industries and aren‚Äôt so big that a small to medium biz customer wouldn‚Äôt be attractive. \n\nIf you want to see an ideal solution in my opinion for productivity check out Juma.ai",
          "score": 1,
          "created_utc": "2026-01-15 23:45:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzujwv2",
          "author": "zapaljeniulicar",
          "text": "A huge chunk of the advice lies in what are you actually going to talk to, like what is in those PDFs. What information are you going to deal with? I‚Äôve seen somebody here talking about RAG for CAD PDFs, like why? If you are searching for an image, RAG is not the way to do it.",
          "score": 1,
          "created_utc": "2026-01-16 02:33:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv3x1o",
          "author": "IdeaAffectionate945",
          "text": "2/4 million documents might be too much for our SQLite VSS database, but if you want to perform tests to check if you could get away with it, you can search for AINIRO Magic Cloud (open source!)\n\nIt can automatically convert PDF (with **text**) to text ...",
          "score": 1,
          "created_utc": "2026-01-16 04:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxhc1g",
          "author": "sippin-jesus-juice",
          "text": "They want to vectorize 4 million documents but only are prepared to spend 100k in total for the project?",
          "score": 1,
          "created_utc": "2026-01-16 14:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxvv3f",
          "author": "seomonstar",
          "text": "best approach get a new job",
          "score": 1,
          "created_utc": "2026-01-16 16:02:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyk425",
          "author": "kbash9",
          "text": "Just use a managed platform that offers agentic RAG: rather than a single lookup, an agentic RAG system uses a planner to find the answer. Much better accuracy",
          "score": 1,
          "created_utc": "2026-01-16 17:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzykwnl",
          "author": "Wise_Reward6165",
          "text": "IMHO you would need a good server to use or 9-series cpu/gpu for local at minimum. Servers are recommended for continuous duty. If money is the primary concern (and not time), consider using a CPU server with a lot of RAM. GPUs are great for acceleration but unnecessary for building the initial database. A standard PC with a 7900 or 5080 could serve the actual AI model once the database is fully assembled. Since security is an issue you will have to be technical btw. Use air gapped protection without wifi-bluetooth-eth0 initially then when the database is assembled serve it as a reverse proxy (with apache or nginx) to a second machine in a VM with monitoring.\n\nTools:\nChromadb transformers torch langchain\nFaiss and llama.cpp are optional. Custom binding a llama.cpp fork for security is extra work but necessary for security.\n\nFrom HF.io:\ndeepseek-ocr\nQwen3-embedding \nNomic-text-embedding (is highly rated)\n\nIt would be a multi-model setup to make the initial database but after that just serve it with apache or nginx and a reverse proxy to your intranet.\n\nAlso, you ‚Äòmight‚Äô be able to use traefik as a web scraper arranged locally on the initial data and then feed it to the Chromadb setup. (Custom programming for sure)\n\nAlso, Also, you could look into using vision models for the PDFs like llama scout but most I think are heavy on ram and vision models still struggle with setup and accuracy.",
          "score": 1,
          "created_utc": "2026-01-16 17:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzypb2t",
          "author": "Hungry-Amount-2730",
          "text": "I'm curious - Why building it from scratch, not just buying already existing software?",
          "score": 1,
          "created_utc": "2026-01-16 18:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyrcgu",
              "author": "Next-Self-184",
              "text": "yea this might be the way, focus on the OCR pipeline and get something like elastic search",
              "score": 1,
              "created_utc": "2026-01-16 18:21:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzys3pg",
                  "author": "Hungry-Amount-2730",
                  "text": "to be completely honest - we have built semantic code search tool and we have PoC for documentation (and gonna go further, its on ou roadmap) , that's why it was my business curiosity to have better understanding :)",
                  "score": 1,
                  "created_utc": "2026-01-16 18:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01mja4",
          "author": "Previous-Ad5318",
          "text": "we use Ziqara internally for document search. Its like google drive for Rag, we kinda love it as they outperformed glean and other rag solutions",
          "score": 1,
          "created_utc": "2026-01-17 03:24:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02xiuy",
          "author": "techie_boy69",
          "text": "IBM Docling project on GitHub",
          "score": 1,
          "created_utc": "2026-01-17 09:50:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03gfsv",
          "author": "jannemansonh",
          "text": "Worth checking out Needle. Production-ready RAG API, handles large document volumes including PDFs with OCR. Might save you from building everything from scratch.",
          "score": 1,
          "created_utc": "2026-01-17 12:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05348d",
          "author": "ImpossibleEnd8335",
          "text": "(stochastic thinking cap on)\n\n4M \"might\" (sprinkles fairy dust on quotation marks) actually mean < 5% of 4M.  \nWhat are people actually looking at?\n\nI think it would make sense to get stats on doc usage and tier it that way.",
          "score": 1,
          "created_utc": "2026-01-17 17:45:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o05bnf3",
          "author": "ampancha",
          "text": "\"Local\" vs \"cloud\" is actually a secondary decision. The real security question at your scale is: who can retrieve what, and can you prove it? With 2-4M documents, you need retrieval filtering (so users only see docs they're authorized for), audit trails, and rate limits to prevent bulk extraction.  \nOn the OCR side, your chunking strategy will make or break retrieval quality. Poor OCR output creates garbage embeddings, which means irrelevant results regardless of how good your vector DB is. Happy to point you toward specific patterns if useful.",
          "score": 1,
          "created_utc": "2026-01-17 18:24:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o05d1lu",
              "author": "ampancha",
              "text": "Sent you a DM with more detail on the architecture side.",
              "score": 1,
              "created_utc": "2026-01-17 18:30:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0934yb",
          "author": "Neat_Cartographer864",
          "text": "One question... Isn't it better to convert PDFs to TOON (token-oriented object notation) instead of Markdown?",
          "score": 1,
          "created_utc": "2026-01-18 06:57:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztezp5",
          "author": "BigNoseEnergyRI",
          "text": "Every off the shelf enterprise search solution can do this without having to build it. Coveo, Elastic, Lucid, serachblox, lucid, Lucy, Glean, OpenText, Google, etc.",
          "score": 0,
          "created_utc": "2026-01-15 22:49:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztni1h",
              "author": "ggone20",
              "text": "Someone has never built anything before lol. Come back when you‚Äôre grown up, kid üòéüòÇ\n\nThere literally doesn‚Äôt exist an out of box solution that is reliable for several hundred thousand documents much less millions.\n\nSure, you COULD throw them in these services, you‚Äôd never get anything useful out.",
              "score": -3,
              "created_utc": "2026-01-15 23:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztvomo",
                  "author": "BigNoseEnergyRI",
                  "text": "We don‚Äôt know what they are even trying to get out of it and how many documents they actually need. \n\nBut you said I‚Äôm young, so muwah!!!",
                  "score": 2,
                  "created_utc": "2026-01-16 00:18:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu3dya",
          "author": "SerDetestable",
          "text": "u don‚Äôt have 4M pdfs. nobody does",
          "score": -1,
          "created_utc": "2026-01-16 01:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuzsoz",
          "author": "Responsible_Type_",
          "text": "Bro just use aws bedrock. U don't need to worry about anything other than cost. \n\nJust learn what is what thorugh youtube.",
          "score": -1,
          "created_utc": "2026-01-16 04:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvt9cr",
              "author": "Much-Researcher6135",
              "text": "\"just throw money at the problem\" lol",
              "score": 3,
              "created_utc": "2026-01-16 07:43:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztrbwn",
          "author": "abhi91",
          "text": "Just use contextual AI. This is a standard use case, they power enterprises with this scale and it's affordable. Most importantly you'll be able to have enterprise grade security. If needed it can be deployed on your own VPC",
          "score": -2,
          "created_utc": "2026-01-15 23:55:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu5lhy",
              "author": "Wikileaks_2412",
              "text": "ContextualAI is affordable ? How much do they charge for the parsing part ?",
              "score": 1,
              "created_utc": "2026-01-16 01:13:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztmnb9",
          "author": "ggone20",
          "text": "This will be extremely difficult. \n\nI‚Äôve discussed this in many other threads but if you‚Äôre here looking for advice to build a RAG system at this scale you‚Äôve already failed. \n\nHire someone. Hire ME, I can be available. #shameless-pitch\n\nFor real though‚Ä¶ hire someone who demonstrably has done this before. You‚Äôll never succeed. Not putting you down, it‚Äôs just that hard and there STILL doesn‚Äôt yet exist a service that you can just plug and play into. Further, even if you do everything RIGHT, it will be very expensive just to do the ingestion and processing of this many documents - you/your employer likely won‚Äôt expect actual costs. \n\nIf you do things wrong or need to trial/error, costs balloon. There isn‚Äôt really a way to test if things work until you scale the ingestion as well. Lots of processes work at smaller document numbers and fall apart in the tens or hundreds of thousands. Nevermind millions.  \n\nFor the record I‚Äôve led building systems for utility-scale capital construction projects where 5-10 million artifacts are produced over the course of the project. The utility portfolio has hundreds of millions. In addition to the artifacts created, the actual management of the project requires accurate retrieval to give go/no-go decisions as well as adherence to standards, regulations, etc where reporting decisions with provenance is critical. Good times. \n\nGood luck. \nDMs open ü´°üôÉü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": -2,
          "created_utc": "2026-01-15 23:29:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbjr5n",
      "title": "We built a semantic highlighting model for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
      "author": "ethanchen20250322",
      "created_utc": "2026-01-13 06:09:28",
      "score": 49,
      "num_comments": 16,
      "upvote_ratio": 0.98,
      "text": "We kept running into this problem: when we retrieve documents in our RAG system,¬†**users can't find where the relevant info actually is**. Keyword highlighting is useless ‚Äì if someone searches \"iPhone performance\" and the text says \"A15 Bionic chip, smooth with no lag,\" nothing gets highlighted.\n\nWe looked at existing semantic highlighting models:\n\n* OpenSearch's model: 512 token limit, too small for real docs\n* Provence: English-only\n* XProvence: supports Chinese but performance isn't great + NC license\n* Open Provence: solid but English/Japanese only\n\nNone fit our needs,¬†**so we trained our own bilingual (EN/CH) model (Hugging Face:¬†https://huggingface.co/zilliz/semantic-highlight-bilingual-v1)**. Used LLMs to generate 5M training samples where they explain their reasoning before labeling highlights. This made the data way more consistent.\n\n**Quick example of why it matters:**\n\nQuery: \"Who wrote the film The Killing of a Sacred Deer?\"\n\nContext mentions:\n\n1. The screenplay writers (correct)\n2. Euripides who wrote the Greek play it's based on (trap)\n\nOur model: 0.915 for #1, 0.719 for #2 ‚Üí correct\n\nXProvence: 0.133 for #1, 0.947 for #2 ‚Üí wrong, fooled by keyword \"wrote\"\n\nWe're using it in Milvus and open-sourced it (MIT license), covers EN/CH right now.\n\nWould be interested to hear if this solves similar problems for others or if we're missing something obvious.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzb4gde",
          "author": "ethanchen20250322",
          "text": "Know more: [https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md](https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md)",
          "score": 2,
          "created_utc": "2026-01-13 06:12:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nzb83es",
          "author": "Rokpiy",
          "text": "the keyword vs semantic highlighting gap is real. especially when the answer is conceptually there but zero keyword overlap\n\nthe training approach is interesting, having LLMs explain reasoning before labeling probably helps with edge cases where multiple spans could be \"correct\" but with different confidence levels\n\ncurious about the 512 token limit you mentioned with opensearch. did you end up with a higher context window for your model or just better semantic understanding within similar limits?",
          "score": 2,
          "created_utc": "2026-01-13 06:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbif1j",
              "author": "ProfessionalLaugh354",
              "text": "Thanks. Data labeling with reasoning leads to higher-quality data.\n\nWe‚Äôve moved to a larger 8k context window model‚Äîit‚Äôs much more aligned with real-world use cases in RAG/Agent scenarios.",
              "score": 1,
              "created_utc": "2026-01-13 08:16:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcfuog",
                  "author": "DeliciousWalk9535",
                  "text": "An 8k context window is definitely a game changer! It should really help with capturing the nuances in longer documents. Have you noticed any specific improvements in user satisfaction since making that switch?",
                  "score": 0,
                  "created_utc": "2026-01-13 13:02:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o09g6nf",
          "author": "stevevaius",
          "text": "Just wondering if someone lets say they want to train their own bilingual model with same results, how to develop training data? Good progress btw",
          "score": 2,
          "created_utc": "2026-01-18 08:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbcb9k",
          "author": "-Cubie-",
          "text": "Cool work!",
          "score": 1,
          "created_utc": "2026-01-13 07:19:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbs9tt",
          "author": "jerrysyw",
          "text": "THE problem want to solve was how to give the right reference when llm generated answers?",
          "score": 1,
          "created_utc": "2026-01-13 09:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgt4o2",
              "author": "ProfessionalLaugh354",
              "text": "Split the context by sentences, then assign a serial number to each sentence, and let the LLM select the number of the sentence that should be highlighted. And use a thinking model to enable the thinking mode.",
              "score": 1,
              "created_utc": "2026-01-14 02:10:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzccj7k",
          "author": "Wimiam1",
          "text": "This is great! Excuse my ignorance, but how does this compare to using something like ColBERTv2 as a reranker and pooling token vector scores into sentence scores?",
          "score": 1,
          "created_utc": "2026-01-13 12:40:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgwd3o",
              "author": "ProfessionalLaugh354",
              "text": "A good point. As far as I know, ColBERT‚Äôs training objective is to use the average of the maximum similarity scores across the entire context as the overall context score. While it can also output token-level scores, its training objective may not be perfectly aligned with tasks like semantic highlighting or context pruning. We haven‚Äôt conducted an evaluation yet, but I suspect there might be a slight mismatch. We welcome more tests and insights from the community.",
              "score": 1,
              "created_utc": "2026-01-14 02:28:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh0pc2",
                  "author": "Wimiam1",
                  "text": "Interesting! I only thought of it because one of my first introductions to ColBERT was [this website](https://colbert.aiserv.cloud) where you could run little demos in browser and it would highlight the relevant parts of the document. I tried it with some of the demos on your GitHub, but it didn‚Äôt perform as well. I suspect this is v1 using BERT, which would explain its poor performance on specific technical jargon like in the iPhone example I tried.",
                  "score": 1,
                  "created_utc": "2026-01-14 02:53:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdkdjq",
          "author": "OrbMan99",
          "text": "This looks great. It's unclear to me, though, how to relate the sentence output back to the original text (e.g, how do I locate \"Sentence13\"). I do not know how the text is being split. What is the algorithm you recommend for locating the sentences for highlighting?",
          "score": 1,
          "created_utc": "2026-01-13 16:28:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgzj4b",
              "author": "ProfessionalLaugh354",
              "text": "This \\`process()\\` function can directly return the sentences that need to be highlighted.\n\nThe sentence splitting logic is right here in this code. [https://huggingface.co/zilliz/semantic-highlight-bilingual-v1/blob/main/modeling\\_open\\_provence\\_standalone.py](https://huggingface.co/zilliz/semantic-highlight-bilingual-v1/blob/main/modeling_open_provence_standalone.py)   \n It works a little differently for each language, and you can also override to customize it.",
              "score": 1,
              "created_utc": "2026-01-14 02:46:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhbvyf",
                  "author": "OrbMan99",
                  "text": "Thanks! I thought the model was splitting the sentences for some reason.",
                  "score": 1,
                  "created_utc": "2026-01-14 04:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzp5gfb",
          "author": "jerrysyw",
          "text": "which means when recall chunks filters the most related sentence with this model ,and then give it to llm for summary?",
          "score": 1,
          "created_utc": "2026-01-15 08:57:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qed97y",
      "title": "A user shared to me this complete RAG guide",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/",
      "author": "Real-Turnover9685",
      "created_utc": "2026-01-16 11:12:01",
      "score": 32,
      "num_comments": 7,
      "upvote_ratio": 0.88,
      "text": "Someone juste shared to me this complete RAG guide with everything from parsing to reranking. Really easy to follow through.  \nLink :¬†[https://app.ailog.fr/en/blog](https://app.ailog.fr/en/blog)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzwsx76",
          "author": "Much-Researcher6135",
          "text": "Fancy seeing this at the top of /r/rag. I just asked Gemini for RAG knowledge hubs and it [recommended this sick repo](https://github.com/NirDiamant/RAG_Techniques). I just found out about it 2 minutes ago, but 24k github stars can't be wrong. It also recommended the llamaindex and langchain docs, plus this subreddit!",
          "score": 3,
          "created_utc": "2026-01-16 12:44:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwtwqi",
              "author": "Real-Turnover9685",
              "text": "Oh that's nice, I'll take a look at it, thank you.",
              "score": 2,
              "created_utc": "2026-01-16 12:51:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwl8mh",
          "author": "tchikss",
          "text": "Thanks for sharing !",
          "score": 1,
          "created_utc": "2026-01-16 11:51:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwnjsz",
              "author": "Real-Turnover9685",
              "text": "Glad you liked it :)",
              "score": 1,
              "created_utc": "2026-01-16 12:08:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02alm5",
          "author": "George_David_S",
          "text": "Thanks so much for sharing would love to connect with you",
          "score": 1,
          "created_utc": "2026-01-17 06:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0bdfpj",
          "author": "Distinct-Land-5749",
          "text": "What's the use case you are using this for?",
          "score": 1,
          "created_utc": "2026-01-18 16:42:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx0ajk",
          "author": "atultrp",
          "text": "[fastrag.live](http://fastrag.live)",
          "score": 0,
          "created_utc": "2026-01-16 13:29:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdziof",
      "title": "New Chapter on \"Chunking Strategies\" - 21 RAG Strategies Book",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qdziof/new_chapter_on_chunking_strategies_21_rag/",
      "author": "blue-or-brown-keys",
      "created_utc": "2026-01-15 23:29:31",
      "score": 31,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "I have added a new Chapter on Chunking the \"21 RAG Strategies\" Book. I am looking for feedback, Which of these strategies do you use in production? Also do you use a strategy you like thats not mentioned here?\n\n[Download \"21 RAG Strategies\" Ebook here](https://www.twig.so/#DownloadEbookSection)\n\n* **Chapter 22 ‚Äî Chunking Strategies for Retrieval-Augmented Generation**\n      1. Chunking as a Core RAG Primitive\n   * 1.1 Definition of a Chunk\n   * 1.2 Chunking vs. Text Splitting\n   * 1.3 Chunking and Retrieval Semantics\n* 2. Why Chunking Determines RAG Accuracy\n   * 2.1 Context Window and Model Constraints\n   * 2.2 Retrieval Precision and Recall\n   * 2.3 Cost, Latency, and Token Efficiency\n   * 2.4 Chunking as an Information Architecture Problem\n* 3. Baseline Chunking Approaches\n   * 3.1 Fixed-Size Token Windowing\n   * 3.2 Sentence-Aligned Chunk Construction\n   * 3.3 Paragraph-Aligned Chunk Construction\n* 4. Structure-Driven Chunking\n   * 4.1 Section- and Heading-Scoped Chunking\n   * 4.2 Document Markup‚ÄìAware Chunking\n   * 4.3 Code- and Clause-Scoped Chunking\n* 5. Semantic Boundary Detection\n   * 5.1 Topic Shift‚ÄìBased Chunk Segmentation\n   * 5.2 Embedding Similarity Thresholding\n   * 5.3 Discourse-Level Chunk Formation\n* 6. Context Preservation Techniques\n   * 6.1 Controlled Overlap and Window Expansion\n   * 6.2 Sentence-Window Retrieval Models\n   * 6.3 Contextual Header Injection\n   * 6.4 Pre- and Post-Context Buffering\n* 7. Hierarchical and Multi-Resolution Chunking\n   * 7.1 Fine-Grained vs. Coarse-Grained Retrieval Units\n   * 7.2 Parent‚ÄìChild Chunk Hierarchies\n   * 7.3 Recursive and Outline-Derived Chunking\n* 8. Question-Centric Chunk Design\n   * 8.1 Generating Retrieval-Aligned Questions\n   * 8.2 Answer-Complete Chunk Construction\n   * 8.3 Context-Buffered Question Anchoring\n* 9. Dual-Index and Retrieval-First Architectures\n   * 9.1 Question-First Retrieval Models\n   * 9.2 Canonical Chunk Grounding\n   * 9.3 Deduplication, Reranking, and Stitching\n* 10. Domain-Aware Chunking Patterns\n   * 10.1 API and Reference Documentation\n   * 10.2 Support Tickets and Conversation Threads\n   * 10.3 Policy, Compliance, and Versioned Knowledge\n* 11. Evaluation-Driven Chunk Optimization\n   * 11.1 Measuring Chunk Quality\n   * 11.2 Retrieval Accuracy and Citation Fidelity\n   * 11.3 Iterative Chunking Refinement\n* 12. Practical Guidance and Trade-Offs\n   * 12.1 Choosing the Right Strategy per Data Source\n   * 12.2 Combining Multiple Chunking Strategies\n   * 12.3 Common Failure Modes and Anti-Patterns\n* 13. Summary: Chunking as the Foundation of RAG\n* 13.1 Why Models Fail When Chunking Fails\n* 13.2 Recommended Production Defaults",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qdziof/new_chapter_on_chunking_strategies_21_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzwjarp",
          "author": "GroundbreakingEmu450",
          "text": "I can already see it is missing anything related to AST based chunking for indexing codebases for example",
          "score": 3,
          "created_utc": "2026-01-16 11:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwjg41",
          "author": "GroundbreakingEmu450",
          "text": "Also the structure you posted here does not match the one of the book, wtf?",
          "score": 2,
          "created_utc": "2026-01-16 11:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxon9h",
              "author": "blue-or-brown-keys",
              "text": "Let me check, this is only one chapter of the book.",
              "score": 1,
              "created_utc": "2026-01-16 15:30:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpith",
          "author": "Able-Let-1399",
          "text": "Thanks. I will when I've read it üëç",
          "score": 2,
          "created_utc": "2026-01-16 12:22:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzumx9s",
          "author": "blue-or-brown-keys",
          "text": "Please share your feedback and how I can improve on the book, what strategies are missing and which ones you have used in production.",
          "score": 1,
          "created_utc": "2026-01-16 02:50:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg6iqv",
      "title": "RAG for excel/CSV",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg6iqv/rag_for_excelcsv/",
      "author": "user_rituraj",
      "created_utc": "2026-01-18 11:42:15",
      "score": 22,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "I have been working on a personal project with AI. Majorly, it involves reading financial documents(more specifically, DCF models, MIS in Excel).\n\n\n\n\n\nI am using the Claude/GPT 5.1 models for my extraction agent (LLMS running in a Loop) and have in place chunking and indexing with Azure OCR and Azure Search (which provide indexing and searching).\n\n\n\nMeanwhile, PDF extraction is working better, but with Excel I am facing many issues where LLMs mix data, such as saying data is for FY21 when it is for FY22(after getting the chunk data) or not able to find the exact related chunks.\n\n\n\nThe problem is that, in Excel, it is very number-heavy (like a 100\\* 50 type table). Also, structurally, it is a finance document and is created by different individuals, so I really do not control the structures, so lots of spaces or themes, so it is really not like CSV, where columns and rows are well defined.\n\n\n\nMajor Problem:\n\n1. By chunking the data, it loses a lot of context, like headers or information is missing if a single table is divided into multiple chunks, and hence, the context is missing, like what that column is like, the year, and the type.\n\n2. If I keep the table big, it is not going to fit sometimes in context as well.\n\n\n\n3. Also, as these tables are mostly number-heavy, creating chunks really does not make sense much (based on my understanding, as in vector embedding, the number itself does not have much context with text).\n\n\n\n\n\nPlease suggest if someone has worked with Excel and what has helped them to get the data in the best possible way.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qg6iqv/rag_for_excelcsv/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0a0s9n",
          "author": "Sunchax",
          "text": "I usually make a code \"sub-agent\" that can use pandas or such libraries to interact with the excel/csv.\n\nThis means that it can generate exact queries to aggregate, look up, or otherwise extract and manipulate excel data that is even in large files.",
          "score": 9,
          "created_utc": "2026-01-18 12:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0arutp",
              "author": "babygrenade",
              "text": "Second this. Llms can understand text but not really data files.",
              "score": 7,
              "created_utc": "2026-01-18 14:57:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0b1fp7",
              "author": "SuedeBandit",
              "text": "Yup. Subagents.\nPlanner / prompted analyst\nFunction caller with mcp\nGuardians all around\nVL validator\nDeterministic inputs and outputs",
              "score": 2,
              "created_utc": "2026-01-18 15:45:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b8t6c",
                  "author": "Sunchax",
                  "text": "Agree, except I usually try to avoid MCP.",
                  "score": 1,
                  "created_utc": "2026-01-18 16:20:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0bchij",
                  "author": "user_rituraj",
                  "text": "I would love to do this but the challenge is to have the proper data in a structured way in excel. \n\n\nBut unfortunately, table and header detection is an endless rabbit hole of a problem. There are an infinite number of ways a human can muck up an Excel document to make it troublesome to ingest. Common examples are multi-row headers, merged cells within headers, multiple tables in a single sheet separated by blank rows/columns, etc....\n\n\nSo the issue with this is:\n\nI cannot make this directly into tabular data and use agent to.find it and at the same time creating chunks for indexing by ocr has its problem of context loss as data is number heavy so its embedding does not make sense in itself.\n\n\nProbably i will have to create a structured context out of it with proper headers (a processing layer to convert it but i am really not sure how hard this is going to be)so that we can pass headers to read the data using some excel agent.",
                  "score": 1,
                  "created_utc": "2026-01-18 16:38:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0b8d6g",
              "author": "Sunchax",
              "text": "Might add that during ingestion one can uses similar sub-agents to explore such data to write a description of what's in it. This makes it possible to fetch such descriptors during search and let the orchestrator know if the right call is to ask a sub-agent to further explore the file.",
              "score": 2,
              "created_utc": "2026-01-18 16:18:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0cwvgp",
              "author": "lupin-the-third",
              "text": "This is what I do do as well, but when indexing the files initially I have a data threshhold where if there are less then like 50 rows or something I will index it rather then include it in the code subnet.",
              "score": 2,
              "created_utc": "2026-01-18 21:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ebfbw",
                  "author": "Sunchax",
                  "text": "Smart! I like that idea, might test it out =)",
                  "score": 1,
                  "created_utc": "2026-01-19 01:29:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a1d9h",
          "author": "jesus_was_rasta",
          "text": "Worked a lot last year on the matter. Reading Excel is a fucking nightmare. Ones that are basically flat tables: you can arrange something that converts them as a database tables then implement text to SQL to get correct data. You have to enrich data with context, as many times column names doesn't say much. \n\nThen, you have to work on other Excels, those that are more fancy with graphics, informations boxes in many tables per sheet here and there. Those ones better to scan as a whole, like concerting them to PDFs than use Docling",
          "score": 3,
          "created_utc": "2026-01-18 12:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0begde",
              "author": "user_rituraj",
              "text": "Most of my exercises are going to be the second type with graphics, spaces and what not.\n\nScanning was my first step to go but as excel is very heavy and tables are also huge so after scanning, i am dividing it into chunks but i see lots of issues with this.\n\n1. Because of number embedding search or semantic search is really not working.\n2.Also with chunking, if we split, usually column names are at the top so that context gets lost in the table.",
              "score": 1,
              "created_utc": "2026-01-18 16:47:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a68nj",
          "author": "Durovilla",
          "text": "How many CSVs do you have? is there an inherent structure to them? if so, you could use DuckDB for text2SQL across all your files without having to embed them",
          "score": 1,
          "created_utc": "2026-01-18 12:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bfwj5",
              "author": "user_rituraj",
              "text": "Usually 4-5 excels with multiple sheets into it.\n\nSo it's more like a MIS document.\n\nBut excels are graphic heavy and also have multiple columns name and there are an infinite number of ways a human can muck up an Excel document to make it troublesome to ingest. Common examples are multi-row headers, merged cells within headers, multiple tables in a single sheet separated by blank rows/columns, etc....",
              "score": 1,
              "created_utc": "2026-01-18 16:54:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a9v1f",
          "author": "IdeaAffectionate945",
          "text": "[Magic Cloud](https://ainiro.io) (my project) has strong support for both PDF files and CSV files. You might want to check it out ...",
          "score": 1,
          "created_utc": "2026-01-18 13:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0aqcw6",
          "author": "Anth-Virtus",
          "text": "Llama Cloud offers specifically an OCR/document parsing pipeline especially for spreadsheets.",
          "score": 1,
          "created_utc": "2026-01-18 14:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bf7sl",
              "author": "user_rituraj",
              "text": "Interesting, will check.\n\nAlso if you have already tried it out, do let me know your experience.",
              "score": 1,
              "created_utc": "2026-01-18 16:51:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0atu2q",
          "author": "Much-Researcher6135",
          "text": "1. A DCF model's output will be derived from raw financial report data which is highly structured. Import that, then rerun the DCF model in Python, inserting both into the database.\n\n2. You should NOT push tabular data into a tabular database as a chunked strings for vector search. You should push it *as tabular data*, then give his agent the schema (table layouts in the db) and let it author and run queries. LLMs are great at SQL once they understand your schema!",
          "score": 1,
          "created_utc": "2026-01-18 15:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ben1k",
              "author": "user_rituraj",
              "text": "https://www.reddit.com/r/Rag/s/VajxL7WS47",
              "score": 1,
              "created_utc": "2026-01-18 16:48:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cqnzh",
          "author": "College_student_444",
          "text": "Turn excel data into sentences. Then split and index.",
          "score": 1,
          "created_utc": "2026-01-18 20:35:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0gan3o",
          "author": "lost_soul1995",
          "text": "I had similar project. \n- chunks per table\n- summary per table\n- embed summary\n- retrieve results based on summary. Feed the retrieved summary and table together.\n- context should mention about quarterly, yearly, monthly terminologies.\n- Use reranking model\n- Hybrid retrieval (B25 plus vector) introduced dirty context as B25 would bring in irrelevant chunks such as revenue repeated in multiple chunks.",
          "score": 1,
          "created_utc": "2026-01-19 10:03:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgbm8d",
      "title": "Which one is better for GraphRAG?: Cognee vs Graphiti vs Mem0",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qgbm8d/which_one_is_better_for_graphrag_cognee_vs/",
      "author": "Imaginary-Bee-8770",
      "created_utc": "2026-01-18 15:35:07",
      "score": 18,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Hello everybody, appreciate any insights you may have on this\n\nIn my team we are trying to evolve from traditional RAG into a more comprehensive and robust approach: GraphRAG. We have a extensive corpus of deep technical documents such as  manuals and datasheets that we want to use to feed customer support agents.  \n  \nWe've seen there are a lot of OSS tools out there to work with, however, we don't know the limitations, ease-of-use, scalability and overall information about them. So, if you have a personal opinion about them and you've tried any of them before, we would be glad if you could share it with us.   \n  \nThanks a lot!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qgbm8d/which_one_is_better_for_graphrag_cognee_vs/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0bivda",
          "author": "astronomikal",
          "text": "Before jumping to tools, it might help to clarify what RAG is failing at for you. A few concrete questions that usually separate ‚Äúbetter RAG‚Äù from ‚Äúdifferent architecture‚Äù\n\nAre you mostly struggling with retrieval quality, or with reasoning across multiple documents?\n\nDo agents need to answer single-fact questions, or questions that require combining constraints across specs, revisions, and edge cases?\n\n When a customer issue is resolved, do you want the system to learn from that resolution, or is memory strictly static?\n\nDo answers need to be explainable/traceable beyond citations (e.g. why one constraint overrode another)?\n\nHow often do documents contradict or partially overlap (datasheet vs manual vs errata)?\n\nDo you need to model relationships (dependencies, incompatibilities, versions), or is chunk-level retrieval sufficient?\n\nIs latency predictability important (e.g. local-first, deterministic), or is cloud-scale recall the priority?",
          "score": 8,
          "created_utc": "2026-01-18 17:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b6cxn",
          "author": "ubiquae",
          "text": "Hey, looking for similar feedback.\n\nMy feedback regarding graphiti is that it is a good starting point but it still lacks core capabilities.\n\nI am waiting for a PR to be reviewed to be able to work with custom entities and relationships since it does not work out of the box. And of course quality depends a lot on how the graph is being modelled so I can't understand why they haven't solved it yet.\n\nAlso, there is no evaluation ready, so there is no way to test it and have at least a basic idea about how it performs.\n\nFinally, the MCP server is pretty basic. Useful for demos but not leveraging all their capabilities.\n\nCognee sounds nice but it seems even more half baked. They probably are pushing their cloud offering. So ideas are great, implementation is not still there.",
          "score": 2,
          "created_utc": "2026-01-18 16:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cz066",
              "author": "Unlucky_Comment",
              "text": "Also used graphiti and its a good starting point, but you grow out of it, which isn't a bad thing. Depending on your use case, you'll want to do something custom.",
              "score": 1,
              "created_utc": "2026-01-18 21:20:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hoesw",
          "author": "Popular_Sand2773",
          "text": "As others mentioned it's not always a flat upgrade just a different way of doing things that excels in certain use cases. One of the reasons I ended up moving away from these guys was latency. For example if your customer support use case is using voice with a dense graph the latency can spike aggressively leading to awkward pauses that make people scream \"human\". You'll see they all dance around the problem of graph latency at scale. \n\nIf you are looking for something with graph like quality/behavior but actual guaranteed low latency and ANN vector db speeds then I would recommend checking out knowledge graph embeddings.",
          "score": 2,
          "created_utc": "2026-01-19 15:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fvx7v",
          "author": "OnyxProyectoUno",
          "text": "Technical manuals and datasheets are tricky for graph approaches because the relationships you care about aren't always explicit in the text. Part numbers reference other parts, specs depend on operating conditions, procedures assume prior steps. Before picking a tool, worth mapping out what relationships actually matter for your support use case.\n\nCognee handles entity extraction and relationship building pretty well out of the box. Graphiti is more focused on temporal/conversational memory, less suited for static technical docs. Mem0 is really about user-level personalization, probably not what you need here.\n\nHave you validated that graph structure actually helps your retrieval? Sometimes dense technical docs benefit more from better chunking that preserves table structure and cross-references than from full knowledge graphs. What's failing with your current RAG setup that's pushing you toward graphs?",
          "score": 1,
          "created_utc": "2026-01-19 07:45:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qga7m6",
      "title": "Claude RAG Skills : 4 open-source tools to optimize your RAG pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qga7m6/claude_rag_skills_4_opensource_tools_to_optimize/",
      "author": "Responsible-Radish65",
      "created_utc": "2026-01-18 14:38:44",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I've been using these internally for 3 months while building our RAG platform. Just cleaned them up for public release.\n\n**The 4 skills:**\n\n* `/rag-audit`¬†‚Üí Scans your codebase, flags anti-patterns, gives you a score out of 100\n* `/rag-scaffold`¬†‚Üí Generates 800+ lines of production-ready boilerplate in seconds\n* `/chunking-advisor`¬†‚Üí Decision tree for optimal chunk size based on your document types\n* `/rag-eval`¬†‚Üí Retrieval metrics (recall, MRR, NDCG) + optional benchmark against our API\n\n**Concrete results:**\n\n* Debugging sessions cut from 2h to 30min (the audit catches recurring mistakes)\n* Scaffold saves \\~15k tokens per new project setup\n* Chunking advisor prevented me from using 512 tokens on legal documents (bad idea)\n\nMIT licensed, no signup required:¬†[https://github.com/floflo777/claude-rag-skills](https://github.com/floflo777/claude-rag-skills)\n\nFeedback welcome, especially if you spot missing anti-patterns.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qga7m6/claude_rag_skills_4_opensource_tools_to_optimize/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qg2hxo",
      "title": "PyTelos - Agentic RAG powered by Postgres pg_vector and pg_textsearch extensions",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg2hxo/pytelos_agentic_rag_powered_by_postgres_pg_vector/",
      "author": "Feisty-Assignment393",
      "created_utc": "2026-01-18 07:45:20",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I wanna introduce my side project \n\n[PyTelos](https://github.com/richinex/pytelos)\n\nIt is an Agentic RAG App with support for the major LLM providers. I built it as a side project to show a friend how Agentic RAG works. It uses the Postgres pg\\_vector and newly released pg\\_textsearch extensions. The pg\\_textsearch extension allows for BM25 relevance-ranked full-text search.\n\nIt also uses a durable execution library I wrote for distributed indexing.\n\nI'd like to hear your thoughts and feedback.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qg2hxo/pytelos_agentic_rag_powered_by_postgres_pg_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o09c14b",
          "author": "mtbMo",
          "text": "Nice one, might check it out. Any plans for a web-ui?",
          "score": 1,
          "created_utc": "2026-01-18 08:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09cgtw",
              "author": "Feisty-Assignment393",
              "text": "My web design skills are still sub-par so I can't say for now",
              "score": 1,
              "created_utc": "2026-01-18 08:21:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09cwph",
                  "author": "mtbMo",
                  "text": "Once my coding agents are up and running, i will give them a try to get something working",
                  "score": 2,
                  "created_utc": "2026-01-18 08:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcjvn2",
      "title": "RAG BUT WITHOUT LLM (RULE-BASED)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "author": "adrjan13",
      "created_utc": "2026-01-14 10:27:43",
      "score": 13,
      "num_comments": 14,
      "upvote_ratio": 0.88,
      "text": "Hello, has anyone here created a scripted chatbot (without using LLM)? \n\nI would like to implement such a solution in my company, e.g., for complaints, so that the chatbot guides the customer from A to Z. I don't see the need to use LLM here (unless you have a different opinion‚Äîfeel free to discuss). \n\nHas anyone built such rule-based chatbots? Do you have any useful links? Any advice? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzimms7",
          "author": "redditorialy_retard",
          "text": "that's building the retrieval but replace LLM with scripts. IE having relevant article 1, 2, 3 (retrieval results) and having the chat bot recommend those.¬†\n\n\nIe chatbot ask what is the problem, user input then is parsed to use as a query to the RAG.\n\n\nBut honestly it's just easier to put an LLM in the middle if you don't want to expose it to the end user.",
          "score": 5,
          "created_utc": "2026-01-14 10:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjegum",
          "author": "PrepperDisk",
          "text": "Tested this out with Haystack just doing retrieval, but then feeding into an LLM to rephrase the chunks.\n\nMy issue with this was user expectation.  With a simple search box, users know to search on keywords.  With a conversational interface, users increasingly expect to be able to phrase requests like they do with Gemini or ChatGPT  (with conversations and context from last request).\n\nMy solution was in a kind of \"uncanny valley\" where it was neither and users got stuck.",
          "score": 3,
          "created_utc": "2026-01-14 13:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzk7hvg",
          "author": "Elses_pels",
          "text": "Have you tried RASA ?",
          "score": 3,
          "created_utc": "2026-01-14 16:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjnesz",
          "author": "irodov4030",
          "text": "there are 100s of applications where you can use retrieved chunks in a workflow and do not need LLM to package the response. Your solution will be more deterministic than a typical RAG with LLM\n\nI have built a similar custom solution.\n\nYou are going in the right direction.\n\nLet me know if you have any specific questions.",
          "score": 2,
          "created_utc": "2026-01-14 14:40:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq2cf7",
              "author": "ghaaribkhurshid",
              "text": "Hello, I'm a fresher in CS, I want to build career in AI, could you please guide me more on this?",
              "score": 1,
              "created_utc": "2026-01-15 13:24:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzio6rc",
          "author": "Necessary-Dot-8101",
          "text": "compression-aware intelligence (CAI) is useful bc it treats hallucinations, identity drift, and reasoning collapse not as output errors but as structural consequences of compression strain within intermediate representations. it provides instrumentation to detect where representations are conflicting and routing strategies that stabilize reasoning rather than patch outputs",
          "score": 1,
          "created_utc": "2026-01-14 10:50:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzipyni",
          "author": "trollsmurf",
          "text": "How would you build consistent responses from chopped up content chunks otherwise?\n\nIf you consistently chunk on chapters/sections and provide that whole section as a response it would work, but it's not quite the same thing.\n\nInterested in knowing how commercial support-related chatbots handle this.",
          "score": 1,
          "created_utc": "2026-01-14 11:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjn5h4",
          "author": "vdharankar",
          "text": "So basically you just want to pull the chunks and show user ? Base idea of RAG is generation with augmentation",
          "score": 1,
          "created_utc": "2026-01-14 14:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjq12n",
          "author": "cubixy2k",
          "text": "So the standard intent based chat bots like Alexa skills?",
          "score": 1,
          "created_utc": "2026-01-14 14:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkj4cl",
          "author": "Alternative_Nose_874",
          "text": "You may consider [botpress.com](http://botpress.com) or similar open source platform as the backend for easy setup.",
          "score": 1,
          "created_utc": "2026-01-14 17:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw8el0",
          "author": "TechnicalGeologist99",
          "text": "This is a classification problem. \n\nIngest text to something like XLM Roberta. Fine tune to classify the failure modes (or modes of complaint) that you have identified in your taxonomy.\n\nAt run time, the model predicts the label and the label triggers whatever text is associated with that problem",
          "score": 1,
          "created_utc": "2026-01-16 10:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01bos7",
          "author": "HealthyCommunicat",
          "text": "I mean isnt this just classic route of ‚Äúassign keywords to docs, use query keywords to mass scan and find matching docs?‚Äù\n\nSo you mean a knowledgebase?\n\nYou must be thinking of RAG as the object that the ‚Äúknowledgebase/library‚Äù part that the LLM is attached to - but RAG actually means the process of taking information, usually a large number of separate docs, and then having the LLM read and use those tokens to generate an output. You‚Äôre overthinking it. You just want a knowledgebase.\n\nEasily put, RAG is a process not an object, the G for generation should tell you that much.\n\nHow are you going to make a chatbot that can possibly really detect every possible combination of keywords and make sure that it pulls up the right docs? Are you sure its not just the fact that you‚Äôre really really overthinking LLM‚Äôs and you would much rather not just download LM Studio + Anything LLM and drag and drop all your docs? Or is it because you can‚Äôt afford it? Solutions are usually made entirely to fix a problem, and noone can help solve your problem unless we know what  it even is.\n\n122 days ago you posted a thread related to RAGs. During that 122 days you have not still even understood what a RAG even is. It‚Äôs pretty easy to tell you just gave up when if you were to go and ask Gemini, it would give you exact step by step instructions on how to set this up, you can even really use like a 1-4b model on a 10 year old laptop, and Gemini can put the steps in a way that even a child can understand if you simply say ‚Äúexplain the steps like you‚Äôre explaining to a kid‚Äù.\n\nIt doesn‚Äôt matter what answer here someone gives you. You‚Äôve shown that you will pretend to care and want to know something but still won‚Äôt make any progress 122 days later. The biggest group of people I despise are those that say they ‚Äúwant to learn something‚Äù when they really really don‚Äôt care in the slightest - it just makes people who actually want to learn get taken unseriously.",
          "score": 1,
          "created_utc": "2026-01-17 02:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0a5ds8",
          "author": "Gazorpazzor",
          "text": "Have you tried Rasa, Tok, ‚Ä¶ or other rule based frameworks ? You still need a classifier for intent detection tho",
          "score": 1,
          "created_utc": "2026-01-18 12:39:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzin0qd",
          "author": "Bozo32",
          "text": "I used a combination of cosine similarity and bm25 to filter the obviously irrelevant -> reranker -> NLI and then presented results to the user...have them say yes or no. This could iterate where you use the 'no' answers as a filter to rerank results  \nover time you would accumulate an evidence base of   \n'query' 'rejected resources' 'chosen resource'  \nthat would be useful for future searches  \nthe work done by folks at Utrecht university on screening abstracts for systematic review may be helpful  \n[https://asreview.nl/install/](https://asreview.nl/install/)",
          "score": -1,
          "created_utc": "2026-01-14 10:40:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgh5nw",
      "title": "RAG Discovery Framework. It's a checklist of what to ask the client before writing any code",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qgh5nw/rag_discovery_framework_its_a_checklist_of_what/",
      "author": "not-so-boring",
      "created_utc": "2026-01-18 19:02:15",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "I've found this on Linkedin and it goes on about the importance of business understanding before building any project.\n\nIt's 42 items that you can map out to a decision matrix for the architecture.\n\nThe article with more details:¬†[https://thehyperplane.substack.com/p/why-the-hell-should-i-build-this](https://thehyperplane.substack.com/p/why-the-hell-should-i-build-this)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qgh5nw/rag_discovery_framework_its_a_checklist_of_what/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qg17pf",
      "title": "I cut my Claude Code costs by ~70% by routing it through local & cheaper models",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg17pf/i_cut_my_claude_code_costs_by_70_by_routing_it/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-18 06:31:55",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 0.87,
      "text": "I love Claude Code, but using it full-time was getting expensive.\n\nSo I built **Lynkr**, a proxy that lets me:\n\n* Route some prompts to local models\n* Fall back to stronger models only when needed\n* Cache repeated prompts automatically\n\nResult: \\~60‚Äì80% lower costs depending on workload.\n\nIt‚Äôs open source and self-hosted:\n\n[https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr?utm_source=chatgpt.com)  \nIf you‚Äôre juggling multiple LLM providers, this might be useful ‚Äî feedback welcome.\n\nIt also supports Codex cli, [continue.dev](http://continue.dev), cursor pro, Cline etc",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qg17pf/i_cut_my_claude_code_costs_by_70_by_routing_it/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0e09ms",
          "author": "Economy-Manager5556",
          "text": "I spend less on Claude code by using it less... \nI gotta try that",
          "score": 3,
          "created_utc": "2026-01-19 00:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0f389j",
              "author": "Dangerous-Dingo-5169",
              "text": "Lol üòÇ",
              "score": 1,
              "created_utc": "2026-01-19 04:05:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qh5kls",
      "title": "Very confused on the optimal approach for generating knowledge-graphs for use with RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qh5kls/very_confused_on_the_optimal_approach_for/",
      "author": "boombox_8",
      "created_utc": "2026-01-19 14:27:40",
      "score": 12,
      "num_comments": 14,
      "upvote_ratio": 0.93,
      "text": "Hey guys! I am new to the world of knowledge graphs and RAGs, and am very interested in exploring it!\n\n  \nI am currently looking at using property graphs (neo4j to be specific) as the 'knowledge base' for RAG implementations since I've read that they're more powerful than the alternative of RDFs\n\n  \nWhat confuses me is about how one should go about generating the knowledge graph in the first place. neo4j's own blog and various others propose using LLMs to extract the data for you, and construct a JSON/csv-esque format which is then ingested to create the knowledge graph\n\n  \nExcept it feels like I am poisoning the well here so to speak? If I have tons of text-based documents as my corpora, won't using LLMs to do the job of data extraction and graph generation have issues?\n\nOff the top of my head, I can think of the following issues:\n\n1) The LLM could generate duplicates of entities across documents/chunks (For example, the word \"White House\" is present in a bunch of various documents in various levels of described detail? The LLM could very well extract out multiple such 'White House' entities\n\nI did have an idea of pre-defining all entity types and relations and forcing he LLM to stick with that, as well do an NLP-based deduplication technique, though I am not sure if it'll work well\n\n  \n2) The LLM could just up and hallucinate up data. Bad for obvious reasons, since I don't want a garbage in = garbage out problem for the resultant rag\n\n  \n3) It could just generate wonky results with incorrect 'syntax'. Bad for obvious reasons\n\n  \n4) Manually extracting data and writing the appropriate CYPHER queries? Yeah, won't work out feasibly\n\n5) Using an NLP-based entity and relation extractor? Faster and cheaper compute-wise, but the duplication issue still remains. It does solve issue 3)\n\nWith all these issues comes the extra issue of validating the output graph. Feels like I'm biting off more than I can chew, since all of this is VERY hard to pack into a pipeline unless I make my own bespoke one for the domain I am focusing on. Is there a better way of doing things?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qh5kls/very_confused_on_the_optimal_approach_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0hkqew",
          "author": "Popular_Sand2773",
          "text": "Knowledge graphs definitely aren't for the faint of heart. The reason everyone recommends using llms to generate the graph is because the alternatives involve significantly more effort. Stanford maintains the CoreNLP library which you can use and is deterministic so no hallucination risk. It's very easy to try out and once you see it's default hot mess you'll see why people risk it with llms.  \n  \nNowadays the risk of improper extraction is fairly low with the llm but the cost is obviously very high. For many RAG setups retrieval corpus's are large and queries are sparse. This can mean you are paying to have a llm generate a mostly unused graph which can hurt at scale.  \n[](https://alb.reddit.com/cr?za=ierAezmqakjkPaVDWKzCA3tCEIV4aCCC0bCRBCF5670AiSndpd2SMNGrxPD7pIs6QF_4_YGz223SK1eDI9NQaF3Bknih3JiBtO6-hq5KNJJm-M01GGJkGTYSSFvK9a--Ar68SJTzwWNl4P85wKLFZdf16OlAlcuEW3bQ8uXOYDazxjJfRvbl32NUYNgzANollM-JxDrxA9w5Y12eNTUmO27Ryo87KWbIfQIfJGpcpepJNJAFeDTsF2g3zJADNX9DoiMdVapRNgCzoQJ7qVHCxGrTGszYBwTzOhewPPUg97EFuriNOiSlKB8mgqfSm7Y6VBddMqH2m3j8H5TviyJQqm9pkBWo1-bQzRQvzseTDrb640e94Gk5GLcfiwM2rq0HtTVmaCwYyVGZ3xCSnsAe9uGOTYk9_rGGRR-AacQb5Mt_CAbs4yf7K4tOwLzW0Gf54Qf2tO_sJQF39wH8Ze8qdBof95qtUiZTkvswn5SQCJ_mUXzAHEUmOh-XA2Sl598ax2cyd-oM1T3oPMMf7m85Q99cXlzhY_Fp23SrxgpqwblimxUIWyD84KxrrgRU2ZpYBN3gxww2U4FwY6Rij4hIWQ7s-xTHsBcaSUSThgeUcd4XuZj5OATt&zp=EhBBaNqSZnW-BP4gOIznz82pQlrRYlfvaCNcyBIhs7wTVfWQDkGmlXivkNxEMtVXgXrymn3Wp2-145A4MY8bevLzVp1T2C9RxQn5ZonalUJpLtqYJENT7XBECPofmqZoSYN6dRcQs05mbKpeONZg3gNhIuibc7o_A9zb46C_7iccXXMOH2yC0cwG6bnT-efJDN_tqMYjKd2etZfrO3Y8GCROkZrxPUnmC--M3iUkc79mq0cOkoRo2W8LVfPMGq6mhq1oAyQ_hbSV6sOcq946_PtUYjPmN3kr4meSycYg_NXUQQfmxr99tsjBBv25tnzoWhw3hJHyQf_N6XQMXqpOhRCRm55i8xSv3PhZYuijg7qfIfRZ5LPyr2gvl21sBitjPlEmA2UuICjDB7Kypxrts7lNzRc-eCtDEuPDpGBHV3KpsXB30tBvlbVpNyhx2uMiFpeS4c3pn-Dii9nkkcviACMwawKXj0mue16rI3_gKxMWYnnStQ&a=598359&b=565262&be=383828&c=382514&d=568613&e=378211&ea=379178&eb=353934&f=352953&r=10&g=1&i=1768835127842&t=1768835729084&o=1&q=1&h=212&w=732&sh=864&sw=1536)  \nYou also identified the other issue which is canonicalization. Assuming your many white houses are spelled the same then they are a shared node with multiple edges. This is desired behavior and they aren't true duplicates just shared entities. The problem is even a small typo without post processing will faultily create a new node.   \n  \nIf you really want knowledge graph like quality without all the extra hassle then knowledge graph embeddings are a great place to look.",
          "score": 5,
          "created_utc": "2026-01-19 15:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hwe31",
              "author": "callmedevilthebad",
              "text": "You are spot on about the cost. Interested to know if you found any low cost, good-quality solutions.",
              "score": 1,
              "created_utc": "2026-01-19 16:09:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ish77",
                  "author": "Popular_Sand2773",
                  "text": "I can probably point you in the right direction/to the right model family but would need to know a little more about the use case. For example if you have a reasonable closed world/taxonomy you can collapse the problem space. Knowledge graphs certainly have their place and purpose but it's more scalpel to embeddings machete. \n\nWithout more context for a case like this I would suggest knowledge graph embeddings as the sweet spot between cost quality and latency. DM me and I can get you access to an early model that's working well for me.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:33:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0j11s2",
              "author": "boombox_8",
              "text": ">If you really want knowledge graph like quality without all the extra hassle then knowledge graph embeddings are a great place to look\n\n  \nI am planning on using the KG for a problem with a very particular domain. Would it be right to assume that the constrained domain/problem space makes it more suitable to NOT go for a knowledge graph embedding and instead just go directly for an LLM-generated KG followed by manual validation and cleaning up from my side?\n\n  \nOr can the two methods be combined and used in a hybrid fashion?",
              "score": 1,
              "created_utc": "2026-01-19 19:11:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jaje0",
                  "author": "Popular_Sand2773",
                  "text": "That's actually a great question. There is no reason you can't do both as complements and if you are maintaining your own graph you can fine tune the embeddings on the graph for some really fun combo moves.\n\nFor a constrained domain/problem space you are in the best place knowledge graph wise because you can potentially handroll a cheaper and more reliable system than a pure llm with something like langextract. CIE closed information extraction models take a predefined schema + set of relationships and reliably pull what you need at much lower cost than an llm. For example Gliner2, UIE, REBEL(ish) can likely do the job cleanly without the coreNLP noise. You have some options.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0hrj27",
          "author": "ggone20",
          "text": "Depends on the data. Or if you build it right it dynamically creates new nodes, edges, and properties. Entities need to deduped so you‚Äôll manage an index. You also don‚Äôt want similar edges -> is_part_of, has_part‚Ä¶ same same. Feed the LLM your index of nodes and edges and properties‚Ä¶ tell her to use those when it makes sense or create new ones when it doesn‚Äôt. You‚Äôll also use the index as filter, which helps you search and traverse much faster as well.",
          "score": 1,
          "created_utc": "2026-01-19 15:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j1od4",
              "author": "boombox_8",
              "text": "By index, are you referring to having a pre-defined list of 'valid' relation names and entities, and feed it to the LLM, forcing it to stick to just those values unless it can't (in which case it creates new ones)?",
              "score": 1,
              "created_utc": "2026-01-19 19:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jbi7d",
                  "author": "Popular_Sand2773",
                  "text": "They are talking about canonicalization. You can constrain the llm but most people just map things back using heuristics like cosine or jaccard similarity as post processing cleanup before you move it into the graph.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:59:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qbgeap",
      "title": "OSS Alternative to Glean",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbgeap/oss_alternative_to_glean/",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-13 03:21:44",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, Connect any LLM to your internal knowledge sources (Search Engines, Drive, Calendar, Notion and 15+ other connectors) and chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams)\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Multi Collaborative Chats\n* Multi Collaborative Documents\n* Real Time Features\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qbgeap/oss_alternative_to_glean/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzaftxf",
          "author": "Oshden",
          "text": "Hey OP, how would one get involved in testing out the system and giving you some feedback? This looks perfect for something I‚Äôm trying to implement",
          "score": 1,
          "created_utc": "2026-01-13 03:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzai3cy",
              "author": "Uiqueblhats",
              "text": "Hey, we are actively looking for feedback. The easiest way to test the product is to try our cloud version. If you find any bugs, please create an issue at [https://github.com/MODSetter/SurfSense/issues](https://github.com/MODSetter/SurfSense/issues).",
              "score": 1,
              "created_utc": "2026-01-13 03:42:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzd046k",
                  "author": "Oshden",
                  "text": "This looks awesome!! I‚Äôll have to do a bit of research on how this could work but I‚Äôm excited to try it. Maybe even run it locally! Do you guys have a way to set up custom agent instructions to further refine the desired behavior for the chat agent?",
                  "score": 1,
                  "created_utc": "2026-01-13 14:53:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qc5qua",
      "title": "Is RAG the right approach for exhaustive searches over a corpus of complex documents?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qc5qua/is_rag_the_right_approach_for_exhaustive_searches/",
      "author": "cleinias",
      "created_utc": "2026-01-13 22:35:27",
      "score": 11,
      "num_comments": 28,
      "upvote_ratio": 0.92,
      "text": "Disclaimer: I am completely new to RAG systems and I am trying to determine whether they are the right approach to my use cases.  I just spent the last few hours reading various material and watching videos on the subject, but still can't  figure out the answer.\n\nConsider this use case (more of a toy problem than a real use case, but close enough in spirit):\n\nYou have a collection of cookbooks, each one being a PDF file several hundred pages long. Let's say you have a few hundreds of them. That is your knowledge base\n\nYou want to be able to query *exclusively* and *exhaustively* this knowledge base with question that may be as simple as:\n\n\"List *all* the recipes using kale in the knowledge base providing the source title, author, and page number.\"\n\nto more complex one such as, for instance, \n\n\"Provide a list of *all* recipes suitable as a main course that include a green vegetable similar to kale as one of the main ingredients, providing the source title, author, and page number.\"\n\n  \nIn short: I have a corpus of documents that are semantically fairly homogeneous and therefore all more or less relevant to the possible queries and I need to the answers to be exhaustive.\n\nThe resources I have read and watched, on the other hand, seem to focus on a different set of use cases, where they are confronted with a vast collection of potentially heterogeneous documents (e.g., all the internal policy documents of a large company) and are keen to extract the very few items relevant to the query at hand in order to integrate the LLM processing step.\n\nWelcoming all suggestions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qc5qua/is_rag_the_right_approach_for_exhaustive_searches/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzge88w",
          "author": "OnyxProyectoUno",
          "text": "Your instinct that standard RAG might not fit here is worth taking seriously. The typical RAG setup optimizes for finding the most relevant handful of chunks, not exhaustive recall across a corpus. When you need every recipe with kale, similarity search with a top-k cutoff is fundamentally the wrong tool.\n\nWhat you're describing sounds more like a structured extraction problem. You'd want to parse each cookbook, extract recipe entities with their ingredients, metadata, and page references, then store that in something queryable like a database or search index. The LLM piece comes in for the fuzzy matching (\"green vegetable similar to kale\") but the exhaustive listing part needs deterministic retrieval.\n\nThe preprocessing step is where this gets tricky. PDF cookbooks are a nightmare because recipes span pages, ingredient lists get mangled, and section headers don't always parse cleanly. I work on document processing tooling at vectorflow.dev and this exact pattern comes up a lot. You need to see what your parser is actually extracting before you can trust any downstream queries.\n\nFor your use case, I'd explore a hybrid approach: structured extraction for the exhaustive search, semantic matching for the similarity queries. What format are these PDFs in? Scanned images or native text?",
          "score": 5,
          "created_utc": "2026-01-14 00:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgo3yx",
              "author": "ai_hedge_fund",
              "text": "Agree 100%\n\nI see this as going into some structured database first, possibly NoSQL, and then maybe using an LLM-as-a-Judge or Classifier to decide how well the search results meet the goal of the query, filtering, etc. \n\nDepending on the real details of the use case the end user may or may not want further generation using those chunks. \n\nSo, yeah, vector search and generation may not be the ultimate configuration.",
              "score": 2,
              "created_utc": "2026-01-14 01:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh0j52",
                  "author": "sqm_prout",
                  "text": "Totally agree, structured databases can really help here. Using something like a NoSQL setup to store the extracted data will make those complex queries way easier. Plus, integrating an LLM to refine the results can add that extra layer of intelligence without losing the exhaustiveness you‚Äôre after. Just make sure your extraction process is solid to handle those tricky PDF formats!",
                  "score": 2,
                  "created_utc": "2026-01-14 02:52:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzfs4k9",
          "author": "Far_Statistician1479",
          "text": "I‚Äôm not 100% sold that semantic search is really an improvement over tokenized search, but if you want ai refined search, really the only way to do that is to give an LLM a few search tools. And semantic search could be the underlying search mechanism if it works well for you.",
          "score": 2,
          "created_utc": "2026-01-13 22:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzg5tdl",
              "author": "Infamous_Ad5702",
              "text": "I love semantic search. Accurate. Low cost. No gpu for me. I can‚Äôt do hallucinations for my client‚Ä¶must be offline also.",
              "score": 2,
              "created_utc": "2026-01-14 00:00:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg6ztr",
                  "author": "Far_Statistician1479",
                  "text": "All of these are features of traditional tokenized search as well?",
                  "score": 2,
                  "created_utc": "2026-01-14 00:07:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzglc55",
          "author": "jordan_yeo",
          "text": "I‚Äôve taken an approach for exhaustive search where we first grab all unique document ids that contain a matching chunk, then for each of those docs, do the hybrid search, run through reranker. You can then take all the cross document results and rerank again. It‚Äôs expensive, and not instant, but has done really well ensuring we get comprehensive results across the corpus",
          "score": 2,
          "created_utc": "2026-01-14 01:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgq470",
          "author": "BusinessMindedAI",
          "text": "RAG is built for top-K relevance, not exhaustive discovery, so it will miss recipes in your case.\nYou need to extract every recipe into a structured index (ingredients, pages, categories) using LLMs once.\nThen run database or graph queries for 100% recall, using LLMs only to interpret and explain results.",
          "score": 2,
          "created_utc": "2026-01-14 01:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzi92ry",
          "author": "GP_103",
          "text": "Hybrid Search - Graph + BM25 with lightweight intent classifier.\n\nMake no mistake though, you‚Äôll have to pre-process those docs in a schema that adds some hierarchy to the content blocks. And there‚Äôs work to get BM25 weighted right.",
          "score": 1,
          "created_utc": "2026-01-14 08:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkxa1r",
          "author": "Curious-Sample6113",
          "text": "Only way to find out is to start",
          "score": 1,
          "created_utc": "2026-01-14 18:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o026vpr",
          "author": "blue-or-brown-keys",
          "text": "This problem requires aggregations across an entire corpus‚Äîpotentially hundreds of pages. An LLM can only answer accurately if it effectively ‚Äúsees‚Äù the whole corpus, which exceeds the context window.\n\nTo solve this, I‚Äôd precompute structured aggregations ahead of time and enrich documents with extracted tags or attributes. You need an extraction + aggregation layer on top of the raw documents; otherwise, the data simply won‚Äôt fit into context.\n\nBasic metadata like title or author is easy‚Äîretrieval already handles citations. But queries like ‚Äúfind all recipes that contain a given ingredient‚Äù are hard unless those ingredients have been explicitly extracted and aggregated in advance.",
          "score": 1,
          "created_utc": "2026-01-17 05:49:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbn7fc",
      "title": "A good way to reduce cost of your RAG system",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbn7fc/a_good_way_to_reduce_cost_of_your_rag_system/",
      "author": "ProfessionalLaugh354",
      "created_utc": "2026-01-13 09:43:42",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I've been working on RAG systems and kept running into the same frustrating pattern: I'd retrieve 10 documents per query, each a few thousand tokens long, but only a handful of sentences actually answered the question. The LLM would get distracted by all the noise, and my token costs were spiraling.\n\nI tried a few existing context pruning models, but they either only had tiny context windows (512 tokens), or weren't commercially usable. Nothing fit what I needed.\n\nSo I trained my own model to do semantic highlighting - basically, it scans through your retrieved context and identifies which sentences are actually relevant to the query. It's a small encoder-only model (0.6B params) that's fast to run and supports both English and Chinese.\n\nHere's how it works in practice:\n\n    from transformers import AutoModel\n    \n    model = AutoModel.from_pretrained(\n        \"zilliz/semantic-highlight-bilingual-v1\",\n        trust_remote_code=True\n    )\n    \n    question = \"What are the symptoms of dehydration?\"\n    context = \"\"\"\n    Dehydration occurs when your body loses more fluid than you take in.\n    Common signs include feeling thirsty and having a dry mouth.\n    The human body is composed of about 60% water.\n    Dark yellow urine and infrequent urination are warning signs.\n    Water is essential for many bodily functions.\n    Dizziness, fatigue, and headaches can indicate severe dehydration.\n    Drinking 8 glasses of water daily is often recommended.\n    \"\"\"\n    \n    result = model.process(\n        question=question,\n        context=context,\n        threshold=0.5,\n        # language=\"en\",  # Language can be auto-detected, or explicitly specified\n        return_sentence_metrics=True,  # Enable sentence probabilities\n    )\n    \n    highlighted = result[\"highlighted_sentences\"]\n    print(f\"Highlighted {len(highlighted)} sentences:\")\n    for i, sent in enumerate(highlighted, 1):\n        print(f\"  {i}. {sent}\")\n    print(f\"\\nTotal sentences in context: {len(context.strip().split('.')) - 1}\")\n    \n    # Print sentence probabilities if available\n    if \"sentence_probabilities\" in result:\n        probs = result[\"sentence_probabilities\"]\n        print(f\"\\nSentence probabilities: {probs}\")\n\nOutput:\n\n    Highlighted 3 sentences:\n      1. Common signs include feeling thirsty and having a dry mouth.\n      2. Dark yellow urine and infrequent urination are warning signs.\n      3. Dizziness, fatigue, and headaches can indicate severe dehydration.\n    \n    Total sentences in context: 7\n    \n    Sentence probabilities: [0.017, 0.990, 0.002, 0.947, 0.001, 0.972, 0.001]\n\nOut of 7 sentences, it correctly picked the 3 that actually answer the question. The token reduction is huge - I'm seeing 70-80% savings in production use cases.\n\nThe model is based on the Provence architecture (encoder-only, token-level scoring) and trained on 5M+ bilingual samples. I used BGE-M3 Reranker v2 as the base model since it already handles long contexts (8192 tokens) and supports multiple languages well.\n\nReleased everything under MIT license if anyone wants to try it out.\n\nCurious if others have been tackling similar problems with RAG context management. What approaches have worked for you?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qbn7fc/a_good_way_to_reduce_cost_of_your_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzcqz6s",
          "author": "shitmoji",
          "text": "nice. I'll try it on an Arabic corpus.",
          "score": 2,
          "created_utc": "2026-01-13 14:05:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd7sbh",
          "author": "mrnoirblack",
          "text": "Download where pls",
          "score": 1,
          "created_utc": "2026-01-13 15:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdjx4g",
          "author": "IdeaAffectionate945",
          "text": "Interesting idea. I've built something completely different, that kind of solves the same problem, since it allows you much more control over your RAG data - Especially during scraping. You can check out the general idea here ==> [https://ainiro.io/natural-language-api](https://ainiro.io/natural-language-api)",
          "score": 1,
          "created_utc": "2026-01-13 16:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze33jz",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's the usual story with RAG systems. You pull back way more context than you need and the LLM gets lost in the noise. Smart move training your own model for this.\n\nThe semantic highlighting approach is solid. I've seen similar patterns where people try to solve this with better retrieval (hybrid search, reranking) but miss that the real issue is context bloat after retrieval. Your model catches the stuff that made it through retrieval but shouldn't have.\n\nFew things to watch out for with this approach. First, the threshold tuning can be tricky. 0.5 might work great for some domains but be way off for others. I'd test across different question types because technical queries vs conversational ones can have very different score distributions.\n\nAlso, sentence-level splitting can miss context that spans multiple sentences. Like if you have \"The company reported losses. This was due to supply chain issues.\" Your model might only highlight the second sentence but lose the connection to what \"this\" refers to. Might be worth experimenting with paragraph-level or sliding window approaches for certain document types.\n\nThe bilingual support is nice though. Most context pruning solutions are English-only which is a pain if you're dealing with mixed-language documents.\n\nWhat kind of domains are you testing this on? Technical docs vs conversational content can behave pretty differently with semantic scoring. And are you seeing any cases where it's too aggressive with the pruning?",
          "score": 1,
          "created_utc": "2026-01-13 18:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfr0f8",
          "author": "Technical-Will-2862",
          "text": "Is this not the same thing?¬†https://www.reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
          "score": 1,
          "created_utc": "2026-01-13 22:42:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcgoo8",
      "title": "need help embedding 250M vectors / chunks at 1024 dims, should I self host embedder (BGE-M3) and self host Qdrant OR use voyage-3.5 or 4?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "author": "zriyansh",
      "created_utc": "2026-01-14 07:06:24",
      "score": 11,
      "num_comments": 18,
      "upvote_ratio": 0.92,
      "text": "hey redditors, I am building a legal research RAG tool for law firms, just research and nothing else.\n\n  \nI have around 1.5TB of legal precedence data, parsed them all using 64 core Azure VM, using PyMuPDF + Layout + Pro. Using custom scripts and getting around 30 - 150 files / second parse speed. \n\nVoyage-3-large surpassed voyage-law-2 and now gemini 001 embedder is ranked #2 (MTEB ranking).  Domain specific models are now overthrown by general embedders. \n\nI have around 250 million vectors to embed, and even using voyage-3.5 (0.06$/mill token), the cost is around $3k dollars. \n\n  \nUsing Qdrant cloud will be another $500.\n\n  \nQuestion I need help with:\n\n1. Should I self host embedder and vectorDB? (for chunking as well retrival later on)  \n2. Bear one time cost of it and be hastle free? \n\n\n\nFeel free to DM me for the parsing and chunking and embedding scripts. Using BM25 + RRF + Hybrid search + Rerank using voyage-rank2.5, CRAG + Web Search. \n\n  \nCurrent latency woth 2048 dims on test dataset of 400k legal text vectors is 5 seconds. \n\nChunking by characters and not token.\n\n|Metric|Value|\n|:-|:-|\n|**Avg parsed file size**|68.5 KB|\n|**Sample text length**|2,521 chars (small doc)|\n|**Total PDFs**|16,428,832|\n|**Chunk size**|4,096 chars (\\~1,024 tokens)|\n|**Chunk overlap**|512 chars (\\~128 tokens)|\n|**Min chunk size**|256 chars|\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzi2930",
          "author": "aiprod",
          "text": "Do you have an eval set that would allow you to test recall etc. on a subset of the corpus? Should help with selecting the right model. I‚Äôd also look into getting those 2048 dims down. Will save you a lot on vector db costs and reduces latency. Five seconds seems very slow. How did you test that? Was it pure embedding retrieval or your full retrieval pipeline?",
          "score": 4,
          "created_utc": "2026-01-14 07:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2p88",
              "author": "zriyansh",
              "text": "I dont have an eval set just yet, working on that. This is qdrant telling me about the latency. Okay it improved from yesterday lol. \n\nhttps://preview.redd.it/7ki00keho9dg1.png?width=2378&format=png&auto=webp&s=83bb39f165ed1da3c98c4b0af0d3d162ea4b5706",
              "score": 1,
              "created_utc": "2026-01-14 07:26:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzi4ay7",
                  "author": "aiprod",
                  "text": "That looks better although some of the queries are still a bit slow. How are you running the qdrant cluster? Is that through their cloud offering?",
                  "score": 1,
                  "created_utc": "2026-01-14 07:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzi2gda",
          "author": "bravelogitex",
          "text": "able to embed across a large number of gpu instances? havent heard of that done before",
          "score": 1,
          "created_utc": "2026-01-14 07:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2r62",
              "author": "zriyansh",
              "text": "CPU\\*",
              "score": 1,
              "created_utc": "2026-01-14 07:26:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi3k45",
          "author": "ai_hedge_fund",
          "text": "This is interesting because you use all different units than I usually think in terms of\n\nWhen you say self host, what GPUs do you have or are you backing into a budget?\n\nThen it becomes a question of turnaround time\n\nIf the time isn‚Äôt an issue then I think you can do better than the $3K\n\nIf you want it done fast then $3K to $4K sounds about right if you rent GPUs / have quota in Azure\n\nTo me the cloud DB question depends on how users will access the data and when. I‚Äôd think you could defer that $500 now if it‚Äôs an issue and just store the vectors yourself.",
          "score": 1,
          "created_utc": "2026-01-14 07:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi8rqo",
              "author": "zriyansh",
              "text": "expecting around 50 users in a month, and 10 queries per user each day.\n\nyeah not using token because character is what I understand well, so it works for me. \n\nI have a budget for $1K for now as we dont have any customers, using my savings for this.\n\nAs far as I understanding, embedding and hosting a vector DB is CPU intensive not GPU (can be wrong here), I have 1k$ credit from Azure as I registered my startup with them (and linked my LinkedIn with them as well).   \n  \nIf we break even, I will want to use cloud services and focus on what we do best.",
              "score": 1,
              "created_utc": "2026-01-14 08:23:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nziamtn",
          "author": "mwon",
          "text": "I have also a side project in the field of legal AI, but with considerable lower size. My biggest index has about 11M vectors, size 1024, generated form a fined tuned BGE-M3.   \nI use Milvus with index in disk in  a dedicated server fom Hetzner, and my latency is bellow 0.5s.  \nI think is a bit odd your latency is 5s for only 400k vectors. You should check if everything is ok, because is too much. I also think chunks of 1024 is too much. You will l likely loose a lot of recall.",
          "score": 1,
          "created_utc": "2026-01-14 08:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzib92e",
              "author": "zriyansh",
              "text": "so its self hosted embedder I suppose, what kind of machine are you using? and anything I need to take care of here?",
              "score": 1,
              "created_utc": "2026-01-14 08:47:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzic6mi",
                  "author": "mwon",
                  "text": "Yes, self embedder. I never use embedding services. Very expensive for what they do and no better than many OS solutions that you can instantiate locally.   \n  \nI'm using one of theirs 64GB ram dedicated servers. They are very cheap, like 40-50 EUR/month. \n\nYou need to be careful with your benchmarks estimation. A sample of 400k vector is very small compared with your final production setup. Recall values will be very diferent with 250M vectors.",
                  "score": 1,
                  "created_utc": "2026-01-14 08:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nziqy8d",
              "author": "explodedgiraffe",
              "text": "I am also working on a side project of that size. I was thinking of using qwen 4b embedding and rerank. How was your experience with BGE-M3? I like the architecture of it (sparse/dense/multi vector) but their benchmarks weren't that impressive compared to dense of the same size. Also curious how you fined tuned it for your usecase.\n\n  \n5s must be caused by network latency from external rerank and web search calls?",
              "score": 1,
              "created_utc": "2026-01-14 11:14:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nziseix",
                  "author": "mwon",
                  "text": "qwen-4b vs BGE-M3 both out of the box, qwen likely wins. After all it is a 4B model against vs 0.5B. But you can give a good boost to BGE-M3 by finetuning it, which will give you a small model that does not need GPU for inference. The sparse part is also nice because it allows you to do hybrid with a sparse search in one go. Note however that from my experiences with BGE-M3, BM25 is still better than its sparse.",
                  "score": 1,
                  "created_utc": "2026-01-14 11:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzicnrv",
          "author": "UseMoreBandwith",
          "text": "how long does it take to process 1.5TB ?",
          "score": 1,
          "created_utc": "2026-01-14 09:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzicuih",
              "author": "zriyansh",
              "text": "around 3 days with 64 core CPU, but there exist faster parsers which can parse 4-5k documents per second with such beast machine but I wasn't able to run that properly, its a C implementation of pymupdf4llm-c",
              "score": 2,
              "created_utc": "2026-01-14 09:03:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qep0ap",
      "title": "Web pages are best performing sources in RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qep0ap/web_pages_are_best_performing_sources_in_rag/",
      "author": "pskd73",
      "created_utc": "2026-01-16 19:04:32",
      "score": 11,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I found that the web pages perform a lot better in RAG as a quality sources. The reason is, they are mostly already divided by topic, example, installation, api-fetch, api-update etc. In semantic search it is important for a chunk to be of a specific topic, if a chunk covers multiple topics, the chances that the chunk getting low scores is very high.\n\nBecause of the same reason, I have observed a very consistent pattern. The landing pages generally perform poor because they cover all the topics.\n\nSo chunking is a very an important process and web pages inherently have an advantage. Anybody has similar approach for files, pdfs etc?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qep0ap/web_pages_are_best_performing_sources_in_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzzk6d5",
          "author": "hrishikamath",
          "text": "Fortunately sec filings had hierarchical structure so I divided it into sections and did it for finance. Can check the details here: https://github.com/kamathhrishi/stratalens-ai/tree/main/agent",
          "score": 2,
          "created_utc": "2026-01-16 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01y2mn",
              "author": "Much-Researcher6135",
              "text": "Wait, you're scraping SEC filings? That's a freaking GREAT idea! People pay good money for those data. But with the advent of LLMs, this process should become open. Well done!",
              "score": 1,
              "created_utc": "2026-01-17 04:44:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o08npjq",
                  "author": "hrishikamath",
                  "text": "Haha, yeah. Parsing isn't particularly challenging, datamule is a good library for that.",
                  "score": 1,
                  "created_utc": "2026-01-18 04:56:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o02ry1t",
          "author": "Rokpiy",
          "text": "web pages win on single-topic constraint - most docs are multi-topic because they're reference material, not explanatory content. the closer your source chunks to 'one concept per document' the less you fight retrieval.",
          "score": 1,
          "created_utc": "2026-01-17 08:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o07xr6r",
          "author": "maschayana",
          "text": "Lmao",
          "score": 1,
          "created_utc": "2026-01-18 02:22:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0hemaf",
          "author": "IdeaAffectionate945",
          "text": "Interesting, but it actually makes sense. I've notice that high quality SEO produces better quality ...",
          "score": 1,
          "created_utc": "2026-01-19 14:45:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qblw1k",
      "title": "Best knowledge graph graph view?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qblw1k/best_knowledge_graph_graph_view/",
      "author": "PutridPut7225",
      "created_utc": "2026-01-13 08:19:05",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.92,
      "text": "What is the most advanced graph view out there currently I do find them all pretty limited especially for very high node count. But I also don't know a lot of knowledge graph software. So maybe you guys know something I don't ",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qblw1k/best_knowledge_graph_graph_view/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzbmslo",
          "author": "Rokpiy",
          "text": "neo4j bloom handles large graphs better than most but starts choking around 50k+ nodes. for really high counts, check out graphistry or yworks yed - they use gpu rendering which makes a difference. \n\ngephi is free and handles 100k+ nodes but the ui is dated. if you're dealing with millions of nodes, you'll need something like tigergraph's graph studio or memgraph lab which are built for scale.\n\ndepends on your use case though. what node count are you working with?",
          "score": 2,
          "created_utc": "2026-01-13 08:58:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbw757",
              "author": "PutridPut7225",
              "text": "10 k is enough for me. But I want the visuality as big as possible. So my problem is less how well it can handle many nodes performance speaking wise, but how fast can I find a specific node. So it's more about space management and so on. Anyways thanks for your suggested tools. Appreciate it",
              "score": 1,
              "created_utc": "2026-01-13 10:28:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbnp6d",
          "author": "Striking-Bluejay6155",
          "text": "Consider looking at [FalkorDB's browser](https://browser.falkordb.com/). You can also look at g.v() if you've got an underlying graph database. How many nodes are we talking?",
          "score": 2,
          "created_utc": "2026-01-13 09:07:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbwci9",
              "author": "PutridPut7225",
              "text": "Thanks will look into it. Normally not more than 10k nodes. However I need to find the node I am looking for the fastest way possible in the graph",
              "score": 2,
              "created_utc": "2026-01-13 10:30:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzclk5d",
                  "author": "Striking-Bluejay6155",
                  "text": "You're welcome. Has the issue so far been writing the cypher query? What I shared comes with built-in filters so you could probably narrow down 10k to a few. Plus you can control the sizes of the nodes according to your needs, so the ones you frequently look at can be larger and different color",
                  "score": 2,
                  "created_utc": "2026-01-13 13:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdybh4",
          "author": "Whole-Assignment6240",
          "text": "all kg comes with a browser. comes down to which kg you pick . neo4j is pretty decent, i used in many projects",
          "score": 1,
          "created_utc": "2026-01-13 17:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg10yt",
          "author": "TrustGraph",
          "text": "I've never found graph viewers useful from a data analysis perspective. I've had some folks from Neo4j tell me it's about 50/50, people that love graph viewers vs. people that never use them. I happen to be a never uses them.\n\nIf you want pretty, there's [Graphistry](https://www.graphistry.com/). I can't argue with it's aesthetics. Most all GraphDB systems have graph viewers. In fact, Neo4j's graph viewer is what really gained them their fame.\n\nTrustGraph uses [3D Force Graph](https://github.com/vasturiano/3d-force-graph) to show 3D graphs that could be stored in Cassandra, Neo4j, Memgraph, or FalkorDB. It has a ton of customization available in it.\n\nIf you'd like the ability to build context graphs and view them in 3D in a single platform with zero coding, TrustGraph is free and open source:\n\n[https://github.com/trustgraph-ai/trustgraph](https://github.com/trustgraph-ai/trustgraph)",
          "score": 1,
          "created_utc": "2026-01-13 23:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkmavb",
          "author": "coderarun",
          "text": "Store it in r/DuckDB and query it via r/LadybugDB\n\n[https://adsharma.github.io/explainable-ai/](https://adsharma.github.io/explainable-ai/)\n\nVisualization: you can probably ask your favorite terminal based coding agent to spit out one. Just need a mcp-server to talk to the data source.\n\nI've tested wikidata (90 million nodes) and am currently testing something 3x its size.",
          "score": 1,
          "created_utc": "2026-01-14 17:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkmrpo",
              "author": "coderarun",
              "text": "Also see: [https://adsharma.github.io/duckdb-wikidata-compression/](https://adsharma.github.io/duckdb-wikidata-compression/)",
              "score": 1,
              "created_utc": "2026-01-14 17:25:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv6yjs",
          "author": "One_Milk_7025",
          "text": "i am using basic postgres and writing things from scratch.. and i am getting quite good result till now .. almost 30k nodes now.. its stable with react-force-graph-2d",
          "score": 1,
          "created_utc": "2026-01-16 04:50:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qct663",
      "title": "Designing inverted indexes in a KV-store on object storage",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qct663/designing_inverted_indexes_in_a_kvstore_on_object/",
      "author": "itty-bitty-birdy-tb",
      "created_utc": "2026-01-14 17:17:49",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "my colleague morgan has been working on redesigning turbopuffer's inverted index structure for full-text search and attribute filtering, and he wrote about it: [https://turbopuffer.com/blog/fts-v2-postings](https://turbopuffer.com/blog/fts-v2-postings)   \n  \nThe main takeaways are that the index structure is designed using fixed-sized posting blocks (as opposed to our prior approach which set posting list partition boundaries at existing vector cluster boundaries) which minimizes KV overhead and improves compression to reduce the physical size of the index by up to 10x. combined with our [vectorized MAXSCORE algorithm](https://turbopuffer.com/blog/fts-v2-maxscore) this has sped up some full-text search queries by up to 20x.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qct663/designing_inverted_indexes_in_a_kvstore_on_object/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzkv3f2",
          "author": "Necessary-Dot-8101",
          "text": "compression-aware intelligence is a fundamentally different design layer than prompting or RAG and meta only just started using it over the past few days. \n\nsuper useful for this bc it treats hallucinations, identity drift, and reasoning collapse not as output errors but as structural consequences of compression strain within intermediate representations. it provides instrumentation to detect where representations are conflicting and routing strategies that stabilize reasoning rather than patch outputs",
          "score": 1,
          "created_utc": "2026-01-14 18:02:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}