{
  "metadata": {
    "last_updated": "2026-01-15 16:59:59",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 50,
    "total_comments": 369,
    "file_size_bytes": 522819
  },
  "items": [
    {
      "id": "1pzwot0",
      "title": "I made a fast, structured PDF extractor for RAG; 300 pages a second",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzwot0/i_made_a_fast_structured_pdf_extractor_for_rag/",
      "author": "absqroot",
      "created_utc": "2025-12-30 23:06:55",
      "score": 141,
      "num_comments": 55,
      "upvote_ratio": 0.97,
      "text": "Hi all,\n\nI hope you're doing well. I'd like to share (what I believe) may be a useful tool I've made.\n\nI was recently helping develop a cybersecurity RAG program with my dad (I'm 15). He probably didn't care about speed, but I did. In fact, I got annoyed. I couldn't find a single lightning fast PDF parser for RAG, with quality intact. I had this weird itch to scratch.. I wanted to change my chunking pipeline and see results INSTANTLY.\n\nAnd so, I ended up porting (kind of, though it has a different output format) `pymupdf4llm` to C, then binding it back to Python. And the results were insane.. not even as much as I thought.\n\n~300 pages a second. 30x faster than `pymupdf4llm`.\n\n\n## what exactly is it?\nA fast PDF extractor for Python. I used most of `pymupdf4llm`'s features and heuristics for detection and parsing as a reference, then wrote it in C for speed. However, unlike `pymupdf4llm` and many others, for RAG, I chose to output structured JSON with **heaps** of data: geometry, typography, document structure, etc.\n\n**speed:** ~300 pages/second on CPU. no GPU needed. 1 million pages in ~55 minutes.\n\n## the problem\nMost PDF extractors give you either raw text (fast but unusable) or *full on* OCR and ML kinda stuff. Anyway, for RAG, I believe you'd want structured data you can control; you want to build smart chunks based on document layout, not just word count. you'd want this fast, especially when processing large volumes.\n\nAlso, chunking matters more than people think. I'm serious here.. not even related to my tool, but I used to have 200 word slivers of text.. and bigger embedding models were NOT helping, lol.\n\n## what you get\n\nJSON output with metadata for every element:\n\n```json\n{\n  \"type\": \"heading\",\n  \"text\": \"Step 1. Gather threat intelligence\",\n  \"bbox\": [64.00, 173.74, 491.11, 218.00],\n  \"font_size\": 21.64,\n  \"font_weight\": \"bold\"\n}\n```\n\nFor example, instead of splitting on word counts and overlaps, you can, now:\n- use bounding boxes to find semantic boundaries (*where does this chunk probably end*, **literally**.. instead of guessing for each document)\n- filter out headers and footers from the top & bottom of pages\n- and lots more. you've got ALL the data!\n\n## comparison table\n\n| Tool            | Speed (pps) | Tables | Images (Figures)                                  | OCR (Y/N)     | JSON Output      | Best For              |\n| --------------- | ----------- | ------ | ------------------------------------------------- | ------------- | ---------------- | --------------------- |\n| pymupdf4llm-C   | ~300        | Yes    | No (WIP)                                          | N             | Yes (structured) | RAG, high volume      |\n| pymupdf4llm     | ~10         | Yes    | Yes (but not ML to get contents)                  | N             | Markdown         | General extraction    |\n| pymupdf (alone) | ~250        | No     | No, not by itself, requires more effort I believe | N             | No (text only)   | basic text extraction |\n| marker          | ~0.5-1      | Yes    | Yes (contents with ML?)                           | Y (optional?) | Markdown         | Maximum fidelity      |\n| docling         | ~2-5        | Yes    | Yes                                               | Y             | JSON             | Document intelligence |\n| PaddleOCR       | ~20-50      | Yes    | Yes                                               | Y             | Text             | Scanned documents     |\n\n**the tradeoff:** speed and control over automatic extraction. marker and docling give higher fidelity if you have time; this is built for when you don't.\n\n## what it handles well\n\n- high volume PDF ingestion (millions of pages)\n- RAG pipelines where document structure matters for chunking\n- custom downstream processing; you own the logic\n- cost sensitive deployments; CPU only, no expensive inference\n- iteration speed; refine your chunking strategy in minutes\n\n## what it doesn't handle\n- scanned or image heavy PDFs (no OCR)\n- 99%+ accuracy on complex edge cases; this trades some precision for speed\n- figues or image extraction\n\n## why i built this\ni used this in my own RAG project and the difference was great. i got to see results instantly, and my chunks were better! (i used bounding boxes to find where one paragraph ends, not \"2000 chars, 500 overlap\").\n\n## links\nrepo: [https://github.com/intercepted16/pymupdf4llm-C](https://github.com/intercepted16/pymupdf4llm-C)\n\npip: `pip install pymupdf4llm-C` ([https://pypi.org/project/pymupdf4llm-C](https://pypi.org/project/pymupdf4llm-C))\n\nnote: prebuilt wheels from 3.9 -> 3.14 (inclusive) (macOS ARM, macOS x64, Linux (glibc > 2011)). no Windows. pain to build for.\n\ndocs and examples in the repo. would love feedback from anyone!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1pzwot0/i_made_a_fast_structured_pdf_extractor_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwtimj3",
          "author": "Synyster328",
          "text": "What trade offs were made to get this performance?",
          "score": 4,
          "created_utc": "2025-12-30 23:17:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtjyl9",
              "author": "absqroot",
              "text": "If we're talking trade-offs in comparison to PyMuPDF4LLM:\n\nNot as much as you'd think.\n\nThe reason for PyMuPDF4LLM being so slow wasn't due to its quality. It was a terrible codebase. Inefficient O(n\\^2), looping, raw numbers in Python, pretty much just bad code and a bad language for lots of maths.\n\nThis isn't a trade-off of the project itself, but there may still be minor cases where I haven't 100% copied the heuristics.\n\nIf we're talking about trade-offs in comparison to tools like Paddle, Marker & Docling:\n\nIt does not do any fancy ML. It's just some basic geometric math. Therefore it won't handle:\n\n\\- scanned pages; no OCR  \n\\- complex tables or tables without some form of edges  \n\\- stuff like that; etc.",
              "score": 10,
              "created_utc": "2025-12-30 23:24:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu3ir6",
                  "author": "Synyster328",
                  "text": "Gotcha, thanks for explaining it. Parsing PDFs is the bane of my existence, or at least was at one point. I certainly don't want any AI magic sitting between the original source doc and my app, since my app is the one adding the AI bs I want the source to be as pure as possible. Unfortunately with PDFs, pure doesn't mean good quality. But I digress.\n\nI would def be comparing this to something like PyMuPDF which is my usual go-to.",
                  "score": 2,
                  "created_utc": "2025-12-31 01:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvtvin",
          "author": "Guboken",
          "text": "This kind of thread brings in the professionals so I have to ask, what is the most accurate pipeline to read pdfs? I rather get 1%+ accuracy and it takes a minute longer than go for speed. I‚Äôm thinking vision VL secondary step to validate perhaps?",
          "score": 5,
          "created_utc": "2025-12-31 08:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtjlil",
          "author": "Repulsive-Memory-298",
          "text": "How do people use bounding boxes to find semantic boundaries",
          "score": 3,
          "created_utc": "2025-12-30 23:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtka9x",
              "author": "absqroot",
              "text": "good question. basically, layout reveals structure.\n\nlarge gaps in y-coordinates often mean topic breaks. if one element ends at y=150 and the next starts at y=200, that's probably a section boundary worth chunking on.\n\nfont size changes are obvious; heading at 21pt followed by body text at 12pt usually means new section. indentation changes (x-coordinate) show hierarchy; bullet points belong with their parent, not split arbitrarily.\n\nwidth changes can indicate sidebars or different content streams. tables have consistent cell structure, so you chunk the whole table as one unit instead of breaking it apart.\n\nthe key difference from word count splitting: word count just breaks whenever you hit N words, regardless of meaning. you might split mid-sentence. bounding boxes let you chunk at natural document boundaries.",
              "score": 4,
              "created_utc": "2025-12-30 23:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtqtb9",
                  "author": "Repulsive-Memory-298",
                  "text": "interesting. And what are your thoughts on something like GROBID, assuming you have suitable models for your document domains",
                  "score": 0,
                  "created_utc": "2025-12-31 00:02:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwbp9q",
          "author": "my_byte",
          "text": "Neat. The caveat is that it's only going to work on simple PDFs. The reason why we need slow stuff with OCR or VLMs is that PDF is a horrible abomination allowing for maximum contol over layout - it was designed for print. I had to write a tool to patch PDF contents once and I kid you not - whatever software they used to create these docs positioned each character on the page individually thru X/Y coords.\nThis makes document extraction at scale - where your solution would be extremely useful - very tricky. Arguably at scale, you don't really know what kind of docs you're going to encounter. And with information retrieval, people kinda care about 1%. Would be neat if some of the libraries with adaptive parsing took what you've built and used it for the first pass. That might result in much better overall throughput",
          "score": 2,
          "created_utc": "2025-12-31 11:17:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwehw9",
          "author": "EveYogaTech",
          "text": "Sounds awesome, but we cannot use it üò≠ because of GNU AFFERO General license (we don't want to force others to open-source every project or SaaS).\n\nMIT, BSD or Apache2 would fix this for us at /r/Nyno",
          "score": 2,
          "created_utc": "2025-12-31 11:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6rdje",
              "author": "absqroot",
              "text": "yeah, i agree, it's not great.\n\nnot saying you should for my tool, but this might help you in the future:\n\nif you really want to, you can host the GPL licensed project separately and expose the source code of that, then call it via API.\n\nor, you can just pay the company in question.",
              "score": 1,
              "created_utc": "2026-01-02 02:44:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtiif0",
          "author": "JDubbsTheDev",
          "text": "Damn those speeds are crazy, definitely checking this out",
          "score": 1,
          "created_utc": "2025-12-30 23:16:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtkccl",
              "author": "absqroot",
              "text": "thanks!",
              "score": 1,
              "created_utc": "2025-12-30 23:26:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtkupr",
          "author": "One-Claim8561",
          "text": "I am building a RAG for comparing companies annual reports. Do you think will suit my goal? For me having a structured file that also recognizes footnote is crucial",
          "score": 1,
          "created_utc": "2025-12-30 23:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtl9zf",
              "author": "absqroot",
              "text": "Honestly, if you‚Äôre not processing that much documents and it‚Äôs not like a chatbot where you want live responses, no. I‚Äôd recommend the higher fidelity but slower options.\n\nMy tool isn‚Äôt as good as those with stuff like footnotes, for example.",
              "score": 1,
              "created_utc": "2025-12-30 23:32:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtltxt",
          "author": "One-Claim8561",
          "text": "Thank you for the clarification!",
          "score": 1,
          "created_utc": "2025-12-30 23:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui8cd",
              "author": "absqroot",
              "text": "no problem",
              "score": 1,
              "created_utc": "2025-12-31 02:38:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmw8w",
          "author": "polandtown",
          "text": "very cool - the major concern for me personally is no-ocr but if I was confident all my pdfs were properly scanned/generated beforehand I'd love to use this option. \n\nGreat work!",
          "score": 1,
          "created_utc": "2025-12-30 23:41:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui5xx",
              "author": "absqroot",
              "text": "thanks! as for OCR, it's a design choice. i decided to keep it simple, ocr will ruin the speed benefits and so it'd just be the same as the others.",
              "score": 1,
              "created_utc": "2025-12-31 02:38:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuwwoe",
                  "author": "AffectionateCap539",
                  "text": "very rational decision. i have been obsessed with ocr for a while  and i must say that this is totally a brand new domain to evolve. complex",
                  "score": 1,
                  "created_utc": "2025-12-31 04:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtnymk",
          "author": "mysterymanOO7",
          "text": "Definitely excellent work, congratulations! This would have been really useful only if it had OCR support. Is there a way to use it with RapidOCR?",
          "score": 1,
          "created_utc": "2025-12-30 23:47:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuhze5",
              "author": "absqroot",
              "text": "Yeah, that could be implemented. However, that would probably compromise most of the speed benefit. it is a design choice that I did not add it. I think if you want OCR, just go with the heavier but slow libraries; they're designed for it.",
              "score": 1,
              "created_utc": "2025-12-31 02:36:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtuyb3",
          "author": "OnyxProyectoUno",
          "text": "You built exactly what's missing. Most people are stuck with either pymupdf's raw text or waiting forever for marker to crawl through documents. 300 pages per second with structured output changes the game completely.\n\nThe JSON metadata approach is spot on. Bounding boxes let you chunk on actual document structure instead of arbitrary word counts. I see too many RAG systems failing because they're splitting mid-paragraph or breaking up tables. Your bbox-based chunking fixes that fundamental problem.\n\nOne thing I'd test is how well the font weight detection works across different PDF generators. Some tools embed fonts weirdly and \"bold\" doesn't always mean what you think. But for high-volume processing where you need good-enough extraction fast, this hits the sweet spot.\n\nThe comparison table tells the story. Marker gives you perfection at 0.5 pages per second. You're giving 80% of that quality at 600x the speed. For most RAG use cases, that math works out perfectly. When you're processing millions of pages, waiting 55 minutes instead of 55 hours matters more than catching every edge case.\n\nTesting different chunking strategies on the structured output, something vectorflow.dev handles well, would help you dial in the optimal approach for your specific documents. The metadata gives you enough control to experiment with semantic boundaries versus fixed sizes.",
          "score": 1,
          "created_utc": "2025-12-31 00:25:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuj28q",
              "author": "absqroot",
              "text": "thanks, really appreciate this. you're right on the font weight thing, some PDFs are weird about it. it's something i've noticed but haven't fully tested across different generators yet.\n\nthe math on marker vs this (80% quality at 600x speed) is exactly the trade-off i was going for. for most RAG pipelines processing millions of pages, that's the right call.\n\nas for [vectorflow.dev](http://vectorflow.dev), i've seen it in other threads, but I don't understand what exactly it is?",
              "score": 1,
              "created_utc": "2025-12-31 02:43:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuqieh",
                  "author": "OnyxProyectoUno",
                  "text": "VectorFlow is a RAG pipeline platform that handles the chunking, embedding, and ingestion side after you extract the structured data. Think of it as the layer between your PDF extractor and your vector database.\n\nThe font weight issue is trickier than most people realize. Adobe's PDF spec allows fonts to be embedded in about six different ways, and \"bold\" can be a font name, a weight property, or just how the renderer decides to display it. I've seen PDFs where the same visual boldness gets tagged three different ways in the metadata.\n\nYour 80/20 approach is dead right for most cases. The only time I'd reach for marker over your tool is when I'm dealing with academic papers with complex equations or heavily formatted financial documents where every table cell matters. For standard business documents, technical manuals, reports - your speed advantage wins every time.\n\nThe bbox chunking strategy you're enabling is underrated. Most people chunk on token counts and wonder why their retrieval sucks. When you can say \"everything between y-coordinates 150 and 400 is one logical section\" you get much cleaner semantic boundaries. Tables stay intact, headers don't get orphaned from their content.\n\nHave you tested how it handles PDFs with multiple column layouts? That's usually where bbox-based approaches either shine or fall apart completely.",
                  "score": 2,
                  "created_utc": "2025-12-31 03:27:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuofcx",
          "author": "Ok-Attention2882",
          "text": "I like how you tried to Un-ChatGPT-ify the post not by changing the language, but by making everything lowercase.",
          "score": 1,
          "created_utc": "2025-12-31 03:14:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwurdca",
              "author": "absqroot",
              "text": "it‚Äôs so hard NOT to get blamed for using ai man.\nBut I‚Äôd like to know how I can improve my writing then if it‚Äôs so AI like?",
              "score": 1,
              "created_utc": "2025-12-31 03:32:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwjhz0",
                  "author": "Ok-Attention2882",
                  "text": "There's actually no way you thought that reverse psychology crap would work here.",
                  "score": 1,
                  "created_utc": "2025-12-31 12:23:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuxda5",
          "author": "AffectionateCap539",
          "text": "can this deal with pdf with diverse layouts? like cv, research paper etc...",
          "score": 1,
          "created_utc": "2025-12-31 04:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvjhyj",
          "author": "Lanky-Cobbler-3349",
          "text": "No OCR? Docling excellent? lol",
          "score": 1,
          "created_utc": "2025-12-31 06:54:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvm301",
              "author": "absqroot",
              "text": "I didn‚Äôt fully test docling. I heard it was as good as the others I might be wrong.\n\nNo OCR is a design choice I chose for this tool.",
              "score": 0,
              "created_utc": "2025-12-31 07:16:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvosms",
                  "author": "Lanky-Cobbler-3349",
                  "text": "Sorry read my comment again. It sounds kind of mean. It should be easy to fix the OCR issue. In general the whole topic is just a little frustrating as all these libraries are not satisfying. It feels wrong that LLMs are better at extracting text from pdf, docx etc. while preserving structure if you provide a decent prompt than those libraries.",
                  "score": 1,
                  "created_utc": "2025-12-31 07:41:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvkat7",
          "author": "Think-Draw6411",
          "text": "Did you run any benchmarks testing the performance ? If docling is excellent it‚Äôs worrying",
          "score": 1,
          "created_utc": "2025-12-31 07:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvlyod",
              "author": "absqroot",
              "text": "I didn‚Äôt fully test docling to be honest I was judging off what I heard.",
              "score": 1,
              "created_utc": "2025-12-31 07:15:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwx0spr",
                  "author": "Think-Draw6411",
                  "text": "Thanks for the reply ! Which systems did you test and which benchmark scores did you achieve with your fast system ?",
                  "score": 1,
                  "created_utc": "2025-12-31 14:17:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx28ztn",
              "author": "TaurusBlack16",
              "text": "I am currently using docling and I didn't really have any issue with it. If you don't mind can you explain why docling being good is worrying. Cuz it does a pretty good job with table extraction, and header-body detection. What other features are usually considered when testing pdf2markdown tools or similar stuff. I am sorry if this sounded rude but I am just trying to learn.",
              "score": 1,
              "created_utc": "2026-01-01 10:35:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvno6o",
          "author": "Cladser",
          "text": "I have a project that am working on using mxbai locally for semantic splitting but it essentially prevents parallelisation.  This looks pretty interesting - is there a way to grab the preceding title as meta data?  For me I want to know if this chunck is from the introduction, rationale or somewhere else.",
          "score": 1,
          "created_utc": "2025-12-31 07:31:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6rky4",
              "author": "absqroot",
              "text": "it doesn't automatically provide that, but it's pretty easy to parse that yourself; all the data is in your hands.",
              "score": 2,
              "created_utc": "2026-01-02 02:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxfdbs6",
                  "author": "Cladser",
                  "text": "So I‚Äôve been testing this and for my use case (scientific reports with specific section) it fabulous. It‚Äôs is also very fast. 2700-ish 30+page pdf in 6-7 minutes with semantic sections. I said previously I was using a local model to infer sections and it was slow and less accurate than this. \nI take my hat off to you sir or ma‚Äôam (hard to tell from your username) ‚Ä¶ and honestly I‚Äôm not even a hat person",
                  "score": 1,
                  "created_utc": "2026-01-03 12:11:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvs1rs",
          "author": "AsparagusKlutzy1817",
          "text": "PyMuPDF is by far the best PDF library out there but the underlying c-implementation mupdf has a tricky license for companies. I remember conversations with legal pushing us to replace it because the license was problematic. I wish someone would build a full apache or MIT license mupdf :)\n\nDid you have an actual throughput challenge with PDF extraction or why did you build it ?",
          "score": 1,
          "created_utc": "2025-12-31 08:12:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvu1pv",
              "author": "absqroot",
              "text": "I'll be honest here.  \nI didn't have an actual throughput challenge.  \ni was just helping my dad out, he wanted to make a cybersecurity agent for companies using RAG.  he didn't know programming but he had relations. and i was just annoyed that everything took forever.  \npymupdf was too raw, pymupdf4llm annoyed me, others were DEAD slow.\n\nthen i looked at its code and then realised this is absolute garbage code in terms of optimization.",
              "score": 1,
              "created_utc": "2025-12-31 08:31:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwv8tz",
          "author": "drink_with_me_to_day",
          "text": "Would be nice as a duckdb extension\n\nJust grab the https://github.com/duckdb/extension-template, add your C source in a subfolder and ask Copilot to create a scalar function and register a replacement for .pdf files\n\n    TableFunction table_func(\"mupdf4llm\",\n        {LogicalType::VARCHAR},\n        extract_func,\n        extract_bind,\n        extract_init\n    );\n    ExtensionUtil::RegisterFunction(instance, table_func);\n\n    // Register the replacement scan for .pdf files\n    instance.config.replacement_scans.emplace_back(replacement_func);\n\nThen use as `SELECT * FROM mupdf4llm('s3://some-butcket/somefile.pdf')`",
          "score": 1,
          "created_utc": "2025-12-31 13:44:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy45al",
          "author": "Straight-Gazelle-597",
          "text": "interesting. 300 pps is amazing.",
          "score": 1,
          "created_utc": "2025-12-31 17:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx287jb",
          "author": "PeaAccurate5302",
          "text": "This sounds amazing - I will pitch it in our team to try in a huge RAG project where we are struggling with extraction cost‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-01 10:27:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyx7bd",
              "author": "absqroot",
              "text": "Wow, thanks for the feedback!",
              "score": 1,
              "created_utc": "2026-01-06 07:26:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7fomt",
          "author": "Ash_It_98",
          "text": "Great work.",
          "score": 1,
          "created_utc": "2026-01-02 05:22:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyx66k",
              "author": "absqroot",
              "text": "Thank you!",
              "score": 1,
              "created_utc": "2026-01-06 07:25:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxaxhwp",
          "author": "No-Neighborhood-7229",
          "text": "Sounds cool. Would be great if you also added transformation to md",
          "score": 1,
          "created_utc": "2026-01-02 19:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbreaj",
              "author": "absqroot",
              "text": "Yeah, I‚Äôll work on what, but should not be too difficult to convert to other formats, only like 50 lines.",
              "score": 1,
              "created_utc": "2026-01-02 21:36:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxea60r",
          "author": "stevevaius",
          "text": "Windows version could be a game changer for me at least. Congratulations",
          "score": 1,
          "created_utc": "2026-01-03 06:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf27xw",
          "author": "aieatstheworld",
          "text": "This is solid. The focus on layout-level signals (bbox, font size/weight, coordinates) is exactly what most RAG pipelines miss when they jump straight to word-count chunking.\n\nThe speed numbers check out for the design choices, and CPU-only ingestion is a big deal when you‚Äôre iterating or processing at scale. Having structured, deterministic output that lets you own the chunking logic is way more useful than opinionated ‚Äúsmart‚Äù extractors.\n\nI can see this being useful to many in edge cases.\n\nNice work!",
          "score": 1,
          "created_utc": "2026-01-03 10:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhc5x1",
          "author": "kacisse",
          "text": "Man that looks nice, I myself had to make a crazy gas factory to use rag on pdf. I had to use huggingface models + LLM serializing.\nDoes yours handle all types of table (including nested elements) ? That was the most challenging.\nIm super interested in using your project.\nGood job !",
          "score": 1,
          "created_utc": "2026-01-03 18:27:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdbss",
              "author": "kacisse",
              "text": "Ah unfortunately your project won't fit my C# stack I think. But I'm curious to see how it performs",
              "score": 1,
              "created_utc": "2026-01-03 18:32:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q11yc5",
      "title": "I rebuilt my entire RAG infrastructure to be 100% EU-hosted and open-source, here's everything I changed",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q11yc5/i_rebuilt_my_entire_rag_infrastructure_to_be_100/",
      "author": "ahmadalmayahi",
      "created_utc": "2026-01-01 11:15:01",
      "score": 64,
      "num_comments": 30,
      "upvote_ratio": 0.88,
      "text": "Wanted to share my journey rebuilding a RAG-based AI chatbot platform ([chatvia.ai](https://chatvia.ai)) from scratch to be fully EU-hosted with zero US data processing. This turned out to be a much bigger undertaking than I expected, so I thought I'd document what I learned.\n\n**The catalyst**\n\nTwo separate conversations killed my original approach. A guy at a networking event asked \"where is the data stored?\" I proudly said \"OpenAI, Claude, you can pick!\" He walked away. A week later, a lawyer told me straight up: \"We will never feed client cases to ChatGPT or any US company due to privacy concerns\".\n\nThat was my wake-up call. The EU market REALLY cares about data sovereignty, and it's only getting stronger.\n\n**The full migration**\n\nHere's what I had to replace:\n\n\n\n|Component|Before|After|\n|:-|:-|:-|\n|LLMs|GPT-4, Claude, Gemini, etc...|Llama 3.3 70B, Qwen3 235B, DeepSeek R1, Mistral Nemo, Gemma 3, Holo2|\n|Embeddings|Cohere|Qwen-embedding (seriously impressed by this)|\n|Re-ranking|Cohere Rerank|RRF (Reciprocal Rank Fusion)|\n|OCR|LlamaParse|Mistral OCR|\n|Object Storage|AWS S3|Scaleway (French)|\n|Hosting|AWS|Hetzner (German)|\n|Vector DB|\\-|VectorChord (self-hosted on Hetzner)|\n|Analytics|Google Analytics|Plausible (EU)|\n|Email|Sender|Scaleway|\n\n**On ditching Cohere Rerank for RRF**\n\nThis was the hardest trade-off. Cohere's reranker is really good, but I couldn't find an EU-hosted alternative that didn't require running my own inference setup. So I went with RRF instead.\n\nFor those unfamiliar: RRF (Reciprocal Rank Fusion) merges multiple ranked lists (e.g., BM25 + vector search) into a unified ranking based on position rather than raw scores. It's not as sophisticated as a neural reranker (such as Cohere Re-reanker), but it's surprisingly effective when you're already doing hybrid search.\n\n**Embedding quality**\n\nSwitching from Cohere to Qwen-embedding was actually a pleasant surprise. The retrieval quality is comparable, and having it run on EU infrastructure without vendor lock-in is a huge win. I'm using the 8B parameter version.\n\n**What I'm still figuring out**\n\n* Better chunking strategies, currently experimenting with semantic chunking using LLMs to maintain context (I already do this with website crawling).\n* Whether to add a lightweight reranker back (maybe a distilled model I can self-host?)\n* Agentic document parsing for complex PDFs with tables/images\n\n**Try it out**\n\nIf you want to see the RAG in action:\n\n* **ChatGPT-style knowledge base:** [help.chatvia.ai](https://help.chatvia.ai)  this is our docs trained as a chatbot\n* **Embeddable widget:** [chatvia.ai](https://chatvia.ai)  check the bottom-right corner\n\n**Future plans**\n\nI'm planning to gradually open-source the entire stack:\n\n* Document parsing pipeline\n* Chat widget\n* RAG orchestration layer\n\nThe goal is to make it available for on-premise hosting.\n\nAnyone else running a fully EU-hosted RAG stack? Would love to compare notes on what's working for you.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q11yc5/i_rebuilt_my_entire_rag_infrastructure_to_be_100/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx2g6na",
          "author": "iamkucuk",
          "text": "Rrf is not a reranker. It‚Äôs a method for fusing different retrievals. Completely different things.",
          "score": 8,
          "created_utc": "2026-01-01 11:50:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbw3d7",
              "author": "TechnicalGeologist99",
              "text": "Thank you came here to say this",
              "score": 1,
              "created_utc": "2026-01-02 21:59:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2f0zu",
          "author": "Nshx-",
          "text": "its not open source",
          "score": 5,
          "created_utc": "2026-01-01 11:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2g0hp",
              "author": "automata_n8n",
              "text": "Using open source models, doesn't mean it's open source.",
              "score": 11,
              "created_utc": "2026-01-01 11:48:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxf0mak",
                  "author": "LilPsychoPanda",
                  "text": "*open weights",
                  "score": 1,
                  "created_utc": "2026-01-03 10:26:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2fuly",
              "author": "ahmadalmayahi",
              "text": "Open-source models :-)",
              "score": -2,
              "created_utc": "2026-01-01 11:46:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2kng2",
          "author": "_xXKaiserXx_",
          "text": "Have you seen a difference on the price?",
          "score": 2,
          "created_utc": "2026-01-01 12:32:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx35gyg",
          "author": "HatEducational9965",
          "text": "Thank you for sharing! Where do you host the models, which inference engine?",
          "score": 2,
          "created_utc": "2026-01-01 15:04:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3704f",
              "author": "ahmadalmayahi",
              "text": "I use scaleway (french cloud provider).",
              "score": 2,
              "created_utc": "2026-01-01 15:13:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx379zb",
          "author": "skibare87",
          "text": "AWS has the EU Sovereign cloud now, not really necessary to replace it",
          "score": 2,
          "created_utc": "2026-01-01 15:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxafhqg",
              "author": "GeroldM972",
              "text": "Physical location isn't the real problem here. Legal location is. No matter if AWS hosts in the EU or not, AWS is an US company, headquartered in the US so subject to US law. And there are laws over there that allow the US to retrieve EU data from EU-based locations without AWS needing to inform the EU company that they did.\n\nActually, that US company is forbidden to do so, else this US company will be subject to severe (administrative) punishment in the US.\n\nSo yes, replacement is becoming a necessity.\n\nGiven that Trump has shown to swivel his stance on almost anything in a moments notice in 2025, all other countries in the world deem the US government to be unstable. Dealing with an unstable US government is a headache for every and any non-US company and country. And it is deemed to be 'not worth the headache anymore\" in the EU. And I'm quite sure that many other countries on other continents feel the same way. They may not (yet) officially proclaim it, but they sure do. That is the true force behind what is called 'soft power'. \n\nResults from this EU stance will not hit the US in 2026 or 2027, but in 2028/2029 the VS will start to notice. Because the US was making a significant sum of money from cloud-services in the EU. And because levels of trust between the US and EU have been on a steady decline since Trump 1 and freefall since Trump 2 the tech industry in the US will start to hurt financially.   \n  \nIsn't industrial/political inertia a nice thing...",
              "score": 1,
              "created_utc": "2026-01-02 17:48:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxej29f",
                  "author": "Aggressive_Bed2609",
                  "text": "Legal location is definitely key. Even with EU cloud options, a lot of companies are still wary about data sovereignty due to potential US influence. Better safe than sorry when handling sensitive data!",
                  "score": 1,
                  "created_utc": "2026-01-03 07:54:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxaw20m",
                  "author": "skibare87",
                  "text": "That is not how the EU sovereign cloud works. You should look it up. All operations happen in the EU and is not connected to the US. It is completely separate and launches Jan 15. Im not talking FRA, but THF eusc-de-east-1 which is the first sovereign cloud for AWS. There are already have large partners starting to move to that partition due to the protections and EU only control.",
                  "score": 0,
                  "created_utc": "2026-01-02 19:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5sqgq",
          "author": "StartX007",
          "text": "OP - you are confusing people by calling what you built as Open Source. It is not open source and may be using Open Source model (open sourced by someone else). That does not make your solution open source.",
          "score": 2,
          "created_utc": "2026-01-01 23:21:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf0vu3",
              "author": "LilPsychoPanda",
              "text": "OpenAI? ü§îü§ì",
              "score": 1,
              "created_utc": "2026-01-03 10:28:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrlun",
                  "author": "StartX007",
                  "text": "Your call - what I see is that you really built an EU based service using local (EU) hosted LLM which could be Open AI compatible models.",
                  "score": 1,
                  "created_utc": "2026-01-03 13:48:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2jozq",
          "author": "Gondor14",
          "text": "Mistral OCR is hosted on Azure I beleive : /\n\nWhy no OVHcloud ?",
          "score": 3,
          "created_utc": "2026-01-01 12:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2wfkm",
              "author": "sleepydevs",
              "text": "Yeah I'm a huge ovh fan for my personal servers. We're contemplating moving some of our backend infra to ovh (from AWS) for similar reasons to the ops.",
              "score": 1,
              "created_utc": "2026-01-01 14:04:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ji5j",
          "author": "macthom",
          "text": "Fascinating, ty for sharing üëç",
          "score": 1,
          "created_utc": "2026-01-01 16:22:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3lix4",
          "author": "cryptoviksant",
          "text": "You RAG it's solely based on doc chunks? Or has something else that improves the response's quality?",
          "score": 1,
          "created_utc": "2026-01-01 16:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5f0j0",
          "author": "KvAk_AKPlaysYT",
          "text": "GitHub?",
          "score": 1,
          "created_utc": "2026-01-01 22:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6tw0v",
          "author": "fustercluck6000",
          "text": "Was there anything in particular about switching geographies that was different from running things  locally in an air-gapped environment?",
          "score": 1,
          "created_utc": "2026-01-02 02:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6vqf9",
          "author": "OnyxProyectoUno",
          "text": "Most people underestimate how much work goes into replacing the entire preprocessing stack when they migrate infrastructure. Your embedding switch working out is huge because that's where most quality degradation happens during migrations.\n\nThe chunking experiments you mentioned are where I'd focus next. Semantic chunking with LLMs can work well but gets expensive fast when you're processing volume. What I've seen work better is hybrid approaches where you do initial structural chunking based on document hierarchy, then use the LLM selectively for edge cases like tables or complex sections. Keeps costs manageable while handling the tricky parts.\n\nFor the reranker question, you might want to look at BGE or similar models you can self-host. They're not Cohere-level but significantly better than pure RRF, especially when your retrieval is pulling from heterogeneous document types. The performance hit from RRF usually shows up when you have mixed content formats where position-based fusion breaks down.\n\nYour agentic parsing mention caught my attention because that's exactly the kind of pipeline complexity that becomes a nightmare to debug when something goes wrong. Document processing pipelines fail silently and you don't discover the issues until retrieval starts returning garbage. I work on tooling for this exact problem at vectorflow.dev, specifically around making document preprocessing visible before it hits your vector store.\n\nWhat's your current approach for validating that parsed content actually looks right before embedding? Most people discover their PDF tables got mangled only after users complain about weird responses.",
          "score": 1,
          "created_utc": "2026-01-02 03:11:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx74zdo",
          "author": "Flamenverfer",
          "text": "Github where?",
          "score": 1,
          "created_utc": "2026-01-02 04:10:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8to1p",
          "author": "ggone20",
          "text": "You can still use OpenAI with Azure. No reason to give up the best intelligence provider. I work with several EU clients and was pleasantly surprised how easy of a switch this was.",
          "score": 1,
          "created_utc": "2026-01-02 12:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9ynxq",
          "author": "AloneSYD",
          "text": "You can try bge m3 reranker or gte multilingual base",
          "score": 1,
          "created_utc": "2026-01-02 16:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2wvci",
          "author": "Straight-Gazelle-597",
          "text": "And not sure that self-hosted Chinese models will be compliant with EU authorities. Don't get me wrong, we are all for open-source models but the compliance is the most tricky thing in the world. üò≠",
          "score": 1,
          "created_utc": "2026-01-01 14:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx36wg3",
              "author": "ahmadalmayahi",
              "text": "You can easily choose the model you want .. however I don‚Äôt really understand the issue here.",
              "score": 2,
              "created_utc": "2026-01-01 15:12:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3di1m",
      "title": "Why RAG is hitting a wall‚Äîand how Apple's \"CLaRa\" architecture fixes it",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q3di1m/why_rag_is_hitting_a_walland_how_apples_clara/",
      "author": "SKD_Sumit",
      "created_utc": "2026-01-04 02:15:41",
      "score": 56,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "Hey everyone,\n\nI‚Äôve been tracking the shift from \"Vanilla RAG\" to more integrated architectures, and Apple‚Äôs recent **CLaRa** paper is a significant milestone that I haven't seen discussed much here yet.\n\nStandard RAG treats retrieval and generation as a \"hand-off\" process, which often leads to the \"lost in the middle\" phenomenon or high latency in long-context tasks.\n\n**What makes CLaRa different?**\n\n* **Salient Compressor:** It doesn't just retrieve chunks; it compresses relevant information into \"Memory Tokens\" in the latent space.\n* **Differentiable Pipeline:** The retriever and generator are optimized together, meaning the system \"learns\" what is actually salient for the specific reasoning task.\n* **The 16x Speedup:** By avoiding the need to process massive raw text blocks in the prompt, it handles long-context reasoning with significantly lower compute.\n\nI put together a technical breakdown of the **Salient Compressor** and how the **two-stage pre-training** works to align the memory tokens with the reasoning model.\n\n**For those interested in the architecture diagrams and math:** [https://yt.openinapp.co/o942t](https://yt.openinapp.co/o942t)\n\nI'd love to discuss: Does anyone here think latent-space retrieval like this will replace standard vector database lookups in production LangChain apps, or is the complexity too high for most use cases?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q3di1m/why_rag_is_hitting_a_walland_how_apples_clara/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxlua1w",
          "author": "Much-Researcher6135",
          "text": "I don't think its weakness will be in its architectural complexity. In fact, this should make RAG simpler to deploy: You pick a model family, and the encoding/retrieval choice has already been made for you.\n\nI think one issue will be a lack of customization. If you've got an embedding setup you're happy with, perhaps even fine-tuning an embedder model to do a better job differentiating your production corpus, kiss all that goodbye because the model architecture will making all those choices for you now, except perhaps compression level.\n\nAlso there's migration / lock-in risk. Unless these new compressed representations become standardized somehow (doubt it), you might have to re-embed your entire corpus every time you want to switch model families. Maybe even for upgrades, if those include major improvements in the latent embeddings. That could cost a lot of money. Still, that kind of lock-in problem is expected with any integration, be it choice of cloud or Apple's software/hardware integration. You end up in an ecosystem with a higher exit cost.\n\nInteresting post, hadn't heard of this. I'm new to this LLM stuff, just starting to tinker.\n\n---\n\nEdit: On the face of it, this seems so useful that I've changed my mind. Surely one or more standards for these \"compact continuous memory-token\" vectors will emerge so everyone can encode to that standard, then hot-plug models into it. Doesn't mean model-specific proprietary schemes won't emerge too, but I'd expect to see some standards emerge if this architecture becomes popular.",
          "score": 7,
          "created_utc": "2026-01-04 10:53:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxk3fzk",
          "author": "PrepperDisk",
          "text": "First hearing about this but anything that promises RAG features with improved quality will get my attention. ¬†When do you think we will see implementations of this? ¬†",
          "score": 4,
          "created_utc": "2026-01-04 02:52:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxkcu1e",
              "author": "coloradical5280",
              "text": "notice how OPs link doens't actually go to CLaRa ? [https://github.com/apple/ml-clara](https://github.com/apple/ml-clara)\n\nyou won't see an \"implementation\" of it, it's a compression thing, you have to pre-train the compressor, you have to fine tune the compressor.\n\nif you want compression and still want to do a lot of work but like 90% less than use deepseek-ocr, which is far more clever in it's approach, and is a fully implemented e2e tool on it's own.\n\nedit to add: oh wow that released an e2e thing, interesting: [https://huggingface.co/apple/CLaRa-7B-E2E](https://huggingface.co/apple/CLaRa-7B-E2E)",
              "score": 16,
              "created_utc": "2026-01-04 03:46:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxkgzih",
                  "author": "SKD_Sumit",
                  "text": "Compressor makes it faster but most important how feedback loop is binding both retreiver and generator",
                  "score": 2,
                  "created_utc": "2026-01-04 04:11:13",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nxlm84e",
                  "author": "LavoP",
                  "text": "Silicon Valley (the show) is becoming real before our eyes",
                  "score": 1,
                  "created_utc": "2026-01-04 09:41:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxke2xb",
              "author": "SKD_Sumit",
              "text": "It not focusing on improving RAG's retreiver or generation separately.  Its upgrading both being in a feedback loop that too making it faster upto 100x .That what makes it different from other RAG strategies like Self-RAG CRAG type.... I believe it will absolutely grab the attention.",
              "score": 1,
              "created_utc": "2026-01-04 03:53:21",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxkwaz8",
              "author": "ChapterEquivalent188",
              "text": "you miught wann see this then ;) https://github.com/2dogsandanerd/RAG_enterprise_core",
              "score": -3,
              "created_utc": "2026-01-04 05:56:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxra4tj",
          "author": "damhack",
          "text": "Just another method that will fall foul of the weakness of vector embeddings based on word similarity.\n\nThere are better methods like compressing an entire query‚Äôs latent space representation down to a single vector embedding and then similarity matching against the latent space vectors of other chunks.  Much better semantic matching and overall relevance without blowing through k=100.",
          "score": 1,
          "created_utc": "2026-01-05 03:56:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9srmh",
      "title": "Announcing Kreuzberg v4",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q9srmh/announcing_kreuzberg_v4/",
      "author": "Goldziher",
      "created_utc": "2026-01-11 07:04:47",
      "score": 55,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Hi Peeps,\n\nI'm excited to announce [Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg) v4.0.0. \n\n## What is Kreuzberg:\n\nKreuzberg is a document intelligence library that extracts structured data from 56+ formats, including PDFs, Office docs, HTML, emails, images and many more. Built for RAG/LLM pipelines with OCR, semantic chunking, embeddings, and metadata extraction. \n\nThe new v4 is a ground-up rewrite in Rust with a bindings for 9 other languages! \n\n## What changed:\n\n- **Rust core**: Significantly faster extraction and lower memory usage. No more Python GIL bottlenecks.\n- **Pandoc is gone**: Native Rust parsers for all formats. One less system dependency to manage.\n- **10 language bindings**: Python, TypeScript/Node.js, Java, Go, C#, Ruby, PHP, Elixir, Rust, and WASM for browsers. Same API, same behavior, pick your stack.\n- **Plugin system**: Register custom document extractors, swap OCR backends (Tesseract, EasyOCR, PaddleOCR), add post-processors for cleaning/normalization, and hook in validators for content verification.\n- **Production-ready**: REST API, MCP server, Docker images, async-first throughout.\n- **ML pipeline features**: ONNX embeddings on CPU (requires ONNX Runtime 1.22.x), streaming parsers for large docs, batch processing, byte-accurate offsets for chunking.\n\n## Why polyglot matters:\n\nDocument processing shouldn't force your language choice. Your Python ML pipeline, Go microservice, and TypeScript frontend can all use the same extraction engine with identical results. The Rust core is the single source of truth; bindings are thin wrappers that expose idiomatic APIs for each language.\n\n## Why the Rust rewrite:\n\nThe Python implementation hit a ceiling, and it also prevented us from offering the library in other languages. Rust gives us predictable performance, lower memory, and a clean path to multi-language support through FFI.\n\n## Is Kreuzberg Open-Source?:\n\nYes! Kreuzberg is MIT-licensed and will stay that way. \n\n## Links\n\n- [Star us on GitHub](https://github.com/kreuzberg-dev/kreuzberg)\n- [Read the Docs](https://kreuzberg.dev/)\n- [Join our Discord Server](https://discord.gg/38pF6qGpYD)\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q9srmh/announcing_kreuzberg_v4/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyxoxt2",
          "author": "butwhol",
          "text": "This is amazing. Thanks. Will definitely try out. I wonder how it compares to docling  esp on complex pdf‚Äôs. I am using docling and looking to replace it as its too slow and huge. Is this with one of ocr plugin comparable to docling?",
          "score": 5,
          "created_utc": "2026-01-11 07:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxpxmq",
              "author": "Goldziher",
              "text": "its x50 times faster than docling on a CPU (not suprising, since docling is GPU orientated), and it works very well. Docling is superior in terms of complex layout extraction. So, test and see how it works for your use case.",
              "score": 7,
              "created_utc": "2026-01-11 07:33:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyxpr9z",
              "author": "Anth-Virtus",
              "text": "Would love to know this, too",
              "score": 2,
              "created_utc": "2026-01-11 07:31:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz0jkqv",
          "author": "delapria",
          "text": "Are there any plans to support labeled bounding boxes in the future? That probably requires a layout/reading order model.¬†\n\nFor me, that is the missing piece and gets you to the next level of performance for two main reasons: the labels make it possible to treat tables, figures etc differently. Second, the bounding boxes itself are useful for a lot of downstream tasks",
          "score": 2,
          "created_utc": "2026-01-11 18:32:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz0vxa",
          "author": "CMPUTX486",
          "text": "Thx for sharing.  I want to try it with C#, but the installation guide page returns 404.. could you help me with that please?",
          "score": 1,
          "created_utc": "2026-01-11 14:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyz2daj",
              "author": "Goldziher",
              "text": "I'll check",
              "score": 1,
              "created_utc": "2026-01-11 14:13:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyz7qf5",
          "author": "silenceimpaired",
          "text": "My gut feel is I‚Äôm not technically literate enough to integrate this into KOBOLDCPP, Text Gen UI (by Oobabooga) or Silly Tavern. \n\nIs it in any platform like those?",
          "score": 1,
          "created_utc": "2026-01-11 14:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzwzfw",
              "author": "Goldziher",
              "text": "Well, its not for the text generation side, its for getting text out of different documents and preparing it for processing in ML or RAG, or other such use cases. I would guess you could take docs you want to feed into any of these tools, and use Kreuzberg, yes. It shouldnt be difficult to do - you can ask an AI agent tool such as Claude Code or Gemini to install Kreuzberg as a CLI tool or using docker and even manually extract text from documents. Its very easy to do and the agent will do this quickly. Point it at the kreuzberg docs.",
              "score": 1,
              "created_utc": "2026-01-11 16:47:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyzz5qg",
                  "author": "silenceimpaired",
                  "text": "I get this ends up in the prompt sent to the LLM‚Ä¶ good idea asking AI for integration.",
                  "score": 2,
                  "created_utc": "2026-01-11 16:57:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz17a0g",
          "author": "Lanky-Cobbler-3349",
          "text": "Why do people still use text parsers?",
          "score": 1,
          "created_utc": "2026-01-11 20:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxxx23",
          "author": "drm00",
          "text": "Looks great! Will it be possible to use docling as an OCR plugin in the future?",
          "score": 0,
          "created_utc": "2026-01-11 08:46:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4svz6",
      "title": "We built a chunker that chunks 20GB of text in 120ms",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q4svz6/we_built_a_chunker_that_chunks_20gb_of_text_in/",
      "author": "shreyash_chonkie",
      "created_utc": "2026-01-05 18:00:20",
      "score": 51,
      "num_comments": 18,
      "upvote_ratio": 0.95,
      "text": "Chunking is one of those \"solved problems\" that nobody thinks about until you're processing millions of documents and your pipeline is bottlenecked on text splitting.\n\nWe ran into this building Chonkie (our chunking library) and decided to see how fast we could actually go. The result is memchunk ‚Äî a SIMD-accelerated chunker hitting ~1 TB/s.\n\nWhy chunking speed matters:\n\nFor a single document? It doesn't. Even slow chunkers are \"fast enough.\"\n\nBut when you're:\n\n- Indexing a knowledge base with 100k+ documents\n- Reprocessing your corpus after changing chunk sizes\n- Running experiments with different chunking strategies\n- Building a pipeline that ingests documents continuously\n\nchunking becomes a real bottleneck. We were spending more time chunking than embedding on large corpora.\n\nThe problem with most chunkers:\n\n1. Token-based chunkers call the tokenizer for every chunk boundary decision. Tokenizers are slow (relatively).\n2. Character splitters are fast but dumb ‚Äî they cut sentences in half, destroying semantic coherence.\n3. Sentence splitters use NLP models or regex, adding overhead.\n\nOur approach:\n\nSplit at delimiters (., ?, \\n, etc.) using SIMD-accelerated byte search. You get semantically meaningful boundaries without the tokenizer overhead.\n\nThe key insight: search backwards from your target size. Forward search requires scanning the whole window and tracking the last delimiter. Backward search? One lookup.\n\nBenchmarks:\n\n| Approach | Throughput |\n| --- | --- |\n| memchunk | ~1 TB/s |\n| Other Rust chunkers | ~1 GB/s |\n| Typical Python chunker | ~3 MB/s |\n\nThe trade-off:\n\nmemchunk operates on bytes, not tokens. Your chunks won't be exactly 512 tokens ‚Äî they'll be approximately N bytes, split at sentence boundaries.\n\nFor most RAG use cases, this is fine. Embedding models handle variable-length inputs, and the semantic coherence from proper sentence boundaries matters more than exact token counts.\n\nIf you absolutely need token-precise chunks (e.g., filling context windows exactly), use a tokenizer-based chunker. But for ingestion pipelines? Byte-based is 1000x faster.\n\nHow to use it:\n\nStandalone:\n\nInstall: `pip install memchunk`\n\n```\nfrom memchunk import chunk\nfor c in chunk(text, size=4096, delimiters=\".?\\n\"):\nprocess(c)\n```\n\nWith Chonkie:\nInstall: `pip install chonkie[fast]`\n\n```\nfrom chonkie import FastChunker\nchunker = FastChunker(chunk_size=4096, delimiters=\"\\n.?\")\nchunks = chunker(corpus)\n```\n\nFeatures for RAG:\n\n- delimiters=\".?!\\n\" ‚Äî split at sentence/paragraph boundaries\n- pattern=\"\\n\\n\" ‚Äî split at paragraph breaks (double newlines)\n- consecutive=True ‚Äî handle multiple newlines cleanly\n- Returns start/end indices so you can track provenance\n\n\nCheck us out on Github! https://github.com/chonkie-inc/memchunk\n\nRead more about how memchunk works: https://minha.sh/posts/so,-you-want-to-chunk-really-fast",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q4svz6/we_built_a_chunker_that_chunks_20gb_of_text_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxuyunu",
          "author": "m0j0m0j",
          "text": "Chunky chunk McChunkie McChunkerton chu chu chunk",
          "score": 10,
          "created_utc": "2026-01-05 18:23:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxuz0l2",
          "author": "ChapterEquivalent188",
          "text": "Retrieval will be a  disaster / hallucinating LLM .... proof me wrong",
          "score": 8,
          "created_utc": "2026-01-05 18:23:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv02pd",
              "author": "shreyash_chonkie",
              "text": "Why? We're still chunking on sentence delimiters",
              "score": 2,
              "created_utc": "2026-01-05 18:28:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxv54d5",
                  "author": "TechnicalGeologist99",
                  "text": "Making chunks like this is right for smaller datasets. But I'd say over a given limit your top-k will be nonsense. \n\nDepends on the use case tbf. If you need stable retrieval then you need to extract structure from the documents.\n\nIf you're happy to retrieve random though somewhat relevant docs each time then fine.",
                  "score": 7,
                  "created_utc": "2026-01-05 18:51:10",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxvfzpw",
                  "author": "ChapterEquivalent188",
                  "text": "may i see a example of teh files you ingest  ? i can provide example of realworld finalboss files we use to ingest in enterprise Environment... dont get me wrong, depends on use case but a nother pdf-chat isnt what the world needs ;) your aproche is good and the right track but far away from beeing a solution for the real world proplem-.----- https://github.com/2dogsandanerd/smart-ingest-kit --simple one",
                  "score": 1,
                  "created_utc": "2026-01-05 19:40:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxv1b0b",
          "author": "aiprod",
          "text": "Does it handle abbreviations, or email addresses, or dates? (basically all those tricky instances where a . does not mark a sentence boundary)",
          "score": 4,
          "created_utc": "2026-01-05 18:34:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv4syp",
              "author": "shreyash_chonkie",
              "text": "That is one of the tradeoffs of this approach, delimiter logic has to be really simple. Few approaches that I have seen to get around this are: \n\n\\- Setting a large chunk size, so you are guaranteed to capture some sentences\n\n\\- Creating chunk overlaps so in case you split at an abbreviation, the overlap ensures its captured whole \n\n\\- Adding spaces after the delimiters - so splitting at '. ' (note the space after .)",
              "score": 1,
              "created_utc": "2026-01-05 18:49:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxvbccr",
                  "author": "aiprod",
                  "text": "Understood. What do you see as primary use cases? Most RAG cases that I‚Äòve built (up to ~150-200M chunks), I was perfectly happy to pay the latency price of a slightly more sophisticated chunker (still small compared to embedding latency or pdf conversion/extraction).\n\nI could see this being applied for preparing massive datasets for pre training or similar.\n\nHave you seen cases where this would be beneficial for actual RAG?",
                  "score": 1,
                  "created_utc": "2026-01-05 19:19:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxvu81a",
          "author": "Hegemonikon138",
          "text": "Lost me at the first sentence about a solved problem.\n\nAnyone who thinks chunking is a solved problem doesn't know what they are doing",
          "score": 2,
          "created_utc": "2026-01-05 20:46:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxvdm1t",
          "author": "Overall-Somewhere760",
          "text": "Im on the chonkie train ! I m using every single one of your chunkers! ü•≥",
          "score": 1,
          "created_utc": "2026-01-05 19:29:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxvi8nb",
              "author": "ChapterEquivalent188",
              "text": "How is your LLM doing ?",
              "score": 2,
              "created_utc": "2026-01-05 19:50:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxmg3m",
          "author": "CommercialPianist468",
          "text": "Chu(n)k norris!",
          "score": 1,
          "created_utc": "2026-01-06 02:11:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycbmqq",
          "author": "badgerbadgerbadgerWI",
          "text": "Nice work on the benchmarks. Chunking is one of those \"boring\" problems that becomes critical at scale. What's the memory footprint look like? Curious if this could run on edge devices processing local document corpuses.",
          "score": 1,
          "created_utc": "2026-01-08 04:40:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbjr5n",
      "title": "We built a semantic highlighting model for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
      "author": "ethanchen20250322",
      "created_utc": "2026-01-13 06:09:28",
      "score": 48,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "We kept running into this problem: when we retrieve documents in our RAG system,¬†**users can't find where the relevant info actually is**. Keyword highlighting is useless ‚Äì if someone searches \"iPhone performance\" and the text says \"A15 Bionic chip, smooth with no lag,\" nothing gets highlighted.\n\nWe looked at existing semantic highlighting models:\n\n* OpenSearch's model: 512 token limit, too small for real docs\n* Provence: English-only\n* XProvence: supports Chinese but performance isn't great + NC license\n* Open Provence: solid but English/Japanese only\n\nNone fit our needs,¬†**so we trained our own bilingual (EN/CH) model (Hugging Face:¬†https://huggingface.co/zilliz/semantic-highlight-bilingual-v1)**. Used LLMs to generate 5M training samples where they explain their reasoning before labeling highlights. This made the data way more consistent.\n\n**Quick example of why it matters:**\n\nQuery: \"Who wrote the film The Killing of a Sacred Deer?\"\n\nContext mentions:\n\n1. The screenplay writers (correct)\n2. Euripides who wrote the Greek play it's based on (trap)\n\nOur model: 0.915 for #1, 0.719 for #2 ‚Üí correct\n\nXProvence: 0.133 for #1, 0.947 for #2 ‚Üí wrong, fooled by keyword \"wrote\"\n\nWe're using it in Milvus and open-sourced it (MIT license), covers EN/CH right now.\n\nWould be interested to hear if this solves similar problems for others or if we're missing something obvious.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzb4gde",
          "author": "ethanchen20250322",
          "text": "Know more: [https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md](https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md)",
          "score": 2,
          "created_utc": "2026-01-13 06:12:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nzb83es",
          "author": "Rokpiy",
          "text": "the keyword vs semantic highlighting gap is real. especially when the answer is conceptually there but zero keyword overlap\n\nthe training approach is interesting, having LLMs explain reasoning before labeling probably helps with edge cases where multiple spans could be \"correct\" but with different confidence levels\n\ncurious about the 512 token limit you mentioned with opensearch. did you end up with a higher context window for your model or just better semantic understanding within similar limits?",
          "score": 2,
          "created_utc": "2026-01-13 06:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbif1j",
              "author": "ProfessionalLaugh354",
              "text": "Thanks. Data labeling with reasoning leads to higher-quality data.\n\nWe‚Äôve moved to a larger 8k context window model‚Äîit‚Äôs much more aligned with real-world use cases in RAG/Agent scenarios.",
              "score": 1,
              "created_utc": "2026-01-13 08:16:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcfuog",
                  "author": "DeliciousWalk9535",
                  "text": "An 8k context window is definitely a game changer! It should really help with capturing the nuances in longer documents. Have you noticed any specific improvements in user satisfaction since making that switch?",
                  "score": 0,
                  "created_utc": "2026-01-13 13:02:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzbcb9k",
          "author": "-Cubie-",
          "text": "Cool work!",
          "score": 1,
          "created_utc": "2026-01-13 07:19:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbs9tt",
          "author": "jerrysyw",
          "text": "THE problem want to solve was how to give the right reference when llm generated answers?",
          "score": 1,
          "created_utc": "2026-01-13 09:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgt4o2",
              "author": "ProfessionalLaugh354",
              "text": "Split the context by sentences, then assign a serial number to each sentence, and let the LLM select the number of the sentence that should be highlighted. And use a thinking model to enable the thinking mode.",
              "score": 1,
              "created_utc": "2026-01-14 02:10:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzccj7k",
          "author": "Wimiam1",
          "text": "This is great! Excuse my ignorance, but how does this compare to using something like ColBERTv2 as a reranker and pooling token vector scores into sentence scores?",
          "score": 1,
          "created_utc": "2026-01-13 12:40:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgwd3o",
              "author": "ProfessionalLaugh354",
              "text": "A good point. As far as I know, ColBERT‚Äôs training objective is to use the average of the maximum similarity scores across the entire context as the overall context score. While it can also output token-level scores, its training objective may not be perfectly aligned with tasks like semantic highlighting or context pruning. We haven‚Äôt conducted an evaluation yet, but I suspect there might be a slight mismatch. We welcome more tests and insights from the community.",
              "score": 1,
              "created_utc": "2026-01-14 02:28:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh0pc2",
                  "author": "Wimiam1",
                  "text": "Interesting! I only thought of it because one of my first introductions to ColBERT was [this website](https://colbert.aiserv.cloud) where you could run little demos in browser and it would highlight the relevant parts of the document. I tried it with some of the demos on your GitHub, but it didn‚Äôt perform as well. I suspect this is v1 using BERT, which would explain its poor performance on specific technical jargon like in the iPhone example I tried.",
                  "score": 1,
                  "created_utc": "2026-01-14 02:53:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdkdjq",
          "author": "OrbMan99",
          "text": "This looks great. It's unclear to me, though, how to relate the sentence output back to the original text (e.g, how do I locate \"Sentence13\"). I do not know how the text is being split. What is the algorithm you recommend for locating the sentences for highlighting?",
          "score": 1,
          "created_utc": "2026-01-13 16:28:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgzj4b",
              "author": "ProfessionalLaugh354",
              "text": "This \\`process()\\` function can directly return the sentences that need to be highlighted.\n\nThe sentence splitting logic is right here in this code. [https://huggingface.co/zilliz/semantic-highlight-bilingual-v1/blob/main/modeling\\_open\\_provence\\_standalone.py](https://huggingface.co/zilliz/semantic-highlight-bilingual-v1/blob/main/modeling_open_provence_standalone.py)   \n It works a little differently for each language, and you can also override to customize it.",
              "score": 1,
              "created_utc": "2026-01-14 02:46:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhbvyf",
                  "author": "OrbMan99",
                  "text": "Thanks! I thought the model was splitting the sentences for some reason.",
                  "score": 1,
                  "created_utc": "2026-01-14 04:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzp5gfb",
          "author": "jerrysyw",
          "text": "which means when recall chunks filters the most related sentence with this model ,and then give it to llm for summary?",
          "score": 1,
          "created_utc": "2026-01-15 08:57:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9brpp",
      "title": "üöÄ Master RAG from Zero to Production: I‚Äôm building a curated \"Ultimate RAG Roadmap\" playlist. What are your \"must-watch\" tutorials?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q9brpp/master_rag_from_zero_to_production_im_building_a/",
      "author": "Dev-it-with-me",
      "created_utc": "2026-01-10 18:37:10",
      "score": 41,
      "num_comments": 6,
      "upvote_ratio": 0.94,
      "text": "Hey everyone,\n\nRetrieval-Augmented Generation (RAG) is moving at light speed. While there are a million \"Chat with PDF\" tutorials, it's becoming harder to find deep dives into the advanced stuff that actually makes RAG work in production (Evaluation, Agentic flows, GraphRAG, etc.).\n\nI‚Äôve started a curated YouTube playlist: [**RAG - How To / All You Need To Know / Tutorials**](https://www.youtube.com/watch?v=0kVT1B1yrMc&list=PLIjrFhaJBqA4PYmO_16489unuYLfHY-CQ&index=2).\n\nMy goal is to build a playlist that goes from the basic \"What is RAG?\" to advanced enterprise-grade architectures.\n\n**Current topics covered:**\n\n* **Foundations:** High-level conceptual overviews.\n* **GraphRAG:** Visual guides and comparisons vs. traditional RAG.\n* **Local RAG:** Private setups using Ollama & local models.\n* **Frameworks:** LangChain Masterclasses & Hybrid Search strategies.\n\n**I‚Äôm the creator of the GraphRAG and Local RAG videos in the list,** but I know I can't cover everything alone. I want this to be a \"best-of-the-best\" resource featuring creators who actually explain the why behind the code.\n\n**I‚Äôm looking for your recommendations!** Specifically, do you know of high-quality videos on:\n\n1. **Evaluation:** RAGAS, TruLens, or DeepEval deep dives?\n2. **Chunking:** Beyond just recursive splitting - semantic or agentic chunking?\n3. **Agentic RAG:** Self-RAG, Corrective RAG (CRAG), or Adaptive RAG tutorials?\n4. **Production:** Real-world deployment, latency optimization, or CI/CD for RAG?\n5. **Multimodal RAG:** Tutorials on handling images, complex PDF tables, or charts using vision models?\n\nIf there‚Äôs a creator you think is underrated or a specific video that gave you an \"Aha!\" moment, please drop the link below. I'll be updating the playlist regularly.\n\nThanks for helping build a better roadmap for the community! üõ†Ô∏è",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q9brpp/master_rag_from_zero_to_production_im_building_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyvlmsp",
          "author": "Fear_ltself",
          "text": "Embeddings are in my opinion the most important aspect of a great RAG. Research stuff like EmbeddingGemma:300m or the new qwenVL embeddor (I think it embeds photos for example)‚Ä¶ they have all kinds of neat tricks. Also be aware the model has to be the same lineage as the embedding (qwen VL- qwen 14b, embeddingGemma:300m - Gemma3 27B) or the embedding will be gibberish to the model",
          "score": 3,
          "created_utc": "2026-01-10 23:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nywl0sj",
              "author": "sqm_prout",
              "text": "Great point about embeddings! They really are the backbone of RAG. Have you found any specific tutorials that dive deep into those new models like qwenVL or embeddingGemma? I'd love to add some solid resources on that.",
              "score": 1,
              "created_utc": "2026-01-11 02:55:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nywlcyf",
                  "author": "Fear_ltself",
                  "text": "https://youtu.be/Xu1X-J-r5Xk\n\nI just watch the release vids and read the documentation for the capabilities, then try to code an example to get the feature working locally",
                  "score": 1,
                  "created_utc": "2026-01-11 02:57:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyyiwip",
                  "author": "Dev-it-with-me",
                  "text": "You can checkout my guide on Naive RAG and it is already in the playlist - I used embeddingGemma for embeddings and Gemma3 for inference.  \n  \n[https://www.youtube.com/watch?v=TqeOznAcXXU&list=PLIjrFhaJBqA4PYmO\\_16489unuYLfHY-CQ&index=3](https://www.youtube.com/watch?v=TqeOznAcXXU&list=PLIjrFhaJBqA4PYmO_16489unuYLfHY-CQ&index=3)",
                  "score": 1,
                  "created_utc": "2026-01-11 11:58:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nza58z0",
          "author": "Drj_dev411",
          "text": "Would love to explore Evaluation of RAGs and what each evaluation library has to offer and how it can be helpful",
          "score": 1,
          "created_utc": "2026-01-13 02:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytyika",
          "author": "macromind",
          "text": "This playlist idea is great, the gap between \"chat with PDF\" and actual production RAG is huge.\n\nIf youre adding agentic RAG content, Id vote for resources that show: retrieval eval, self critique loops, and how they stop tool calls from spiraling (timeouts, budgets, fallbacks). Those bits matter way more than the happy path demo.\n\nAlso, Ive bookmarked this as a quick reference for agentic AI + automation patterns and guardrails: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-01-10 18:50:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd2e2b",
      "title": "RAG at scale still underperforming for large policy/legal docs ‚Äì what actually works in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-01-14 23:00:50",
      "score": 37,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm running RAG fairly strong on-prem setup, but quality still degrades badly with large policy / regulatory documents and multi-document corpora. Looking for practical architectural advice, not beginner tips.\n\nCurrent stack:\n-Open WebUI (self-hosted)\n-Docling for parsing (structured output)\n-Token-based chunking\n-bge-m3 embeddings\n-bge-m3-v2 reranker\n-Milvus (COSINE + HNSW)\n-Hybrid retrieval (BM25 + vector)\n-LLM: gpt-oss-20B\n-Context window: 64k\n-Corpus: large policy / legal docs, 20+ documents\n-Infra: RTX 6000 ADA 48GB, 256GB DDR5 ECC\n\nObserved issues:\nCross-section and cross-document reasoning is weak\nIncreasing context window doesn‚Äôt materially help\nReranking helps slightly but doesn‚Äôt fix missed clauses\nWorks ‚Äúokay‚Äù for academic projects, but not enterprise-grade\n\nI‚Äôm thinking of trying:\nGraph RAG (Neo4j for clause/definition relationships)\nAgentic RAG (controlled, not free-form agents)\n\nQuestions for people running this in production:\nHave you moved beyond flat chunk-based retrieval in Open WebUI? If yes, how?\nHow are you handling definitions, exceptions, overrides in policy docs?\nDoes Graph RAG actually improve answer correctness, or mainly traceability?\nAny proven patterns for RAG specifically (pipelines, filters, custom retrievers)?\nAt what point did you stop relying purely on embeddings?\n\nI‚Äôm starting to feel that naive RAG has hit a ceiling, and the remaining gains are in retrieval logic, structure, and constraints‚Äînot models or hardware.\nWould really appreciate insights from anyone who has pushed RAG system beyond demos into real-world, compliance-heavy use cases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzqkpdf",
          "author": "Soft-Speaker6195",
          "text": "You‚Äôre not wrong - flat chunk RAG hits a wall fast on policy/legal corpora because ‚Äúcorrectness‚Äù is mostly about scope, definitions, exceptions, and precedence, not semantic similarity. The teams I‚Äôve seen get this working stop treating retrieval as ‚Äútop-k chunks‚Äù and start treating it as a constrained legal reasoning pipeline: definition expansion > scope filters > precedence rules > targeted clause fetch > answer with mandatory citations. Graph adds value when it‚Äôs used for precedence + definition binding, not as a generic knowledge graph. If you want a concrete example of how this looks in practice (especially definition/exception handling + audit-friendly outputs), AI Lawyer has some useful patterns you can mirror: strict citation gating, clause-level retrieval, and override/exception tracing.",
          "score": 11,
          "created_utc": "2026-01-15 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmphjq",
          "author": "Chemical_Orange_8963",
          "text": "Basically you are making GLEAN, research more on that on how it works",
          "score": 9,
          "created_utc": "2026-01-14 23:06:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolbyp",
          "author": "OnyxProyectoUno",
          "text": "Yeah, you've hit the ceiling that most people hit. Token-based chunking on legal docs is basically guaranteed to break cross-reference reasoning because it has no awareness of document structure.\n\nThe issue isn't your retrieval stack, it's what you're feeding it. Legal docs have explicit hierarchical relationships: definitions that apply to specific sections, exceptions that modify clauses, cross-references that span documents. Flat chunks destroy all of that before your embeddings ever see it. Doesn't matter how good your reranker is if the chunk boundaries cut through a definition-to-usage relationship.\n\nGraph RAG can help with traceability but it won't fix the upstream problem. You're still building the graph from chunks that already lost the structure. Same with agentic approaches, they're working with degraded inputs.\n\nWhat's actually worked in production for policy docs: semantic chunking that respects section boundaries, explicit metadata for document hierarchy (section > subsection > clause), and preserving cross-reference relationships as first-class data. You want chunks that know what section they belong to and what other sections they reference.\n\nDocling gives you structured output but are you actually using that structure for chunking decisions? Most people parse structured and then chunk flat anyway. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this kind of pipeline configuration, where you can see how your docs look after each transformation before committing.\n\nFor the cross-document reasoning specifically, you probably need document-level metadata propagation so chunks know which policy they came from and what other policies they relate to. That's not a retrieval problem, it's an enrichment problem that happens way before Milvus sees anything.",
          "score": 7,
          "created_utc": "2026-01-15 05:57:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn3o33",
          "author": "ggone20",
          "text": "Graphs are the answer. You need to rethink chunking strategies though - semantic and section based chunking. All the questions you have are indeed things you need to work through. \n\nYou can (and should) approach retrieval from both ends: start with keyword and vector search, use the findings to traverse graph relationships - you‚Äôll need agentic search here so it can intelligently ‚Äòloop‚Äô and refine queries as needed. You can come from the graph side as well if you know certain nodes on specific edges you‚Äôre looking for to filter (keep an index). \n\nHave your agent use a ‚Äòscratchpad‚Äô during search and keep each search branch‚Äôs context clean and focused - what I‚Äôve found so far, what information I still need, search terms used).  There are a hundred more things but yea‚Ä¶\n\nI built an engineering assistant for the hydrogen mobility industry (think hydrogen fueling stations and generation facilities) that is used to validate plans and even day to day work packages against required standards, protocols, and regulations. It provides detailed report with citations of ‚Äòwhy‚Äô for go/no-go decisions.  So yes, to answer your final questions, this is the way. Prompting and intelligent search is more art than science though. Expanding user queries, prompting the user intelligently for search clarity, cacheing and vectorizing queries to store them with ‚Äòwhat worked‚Äô provenance so the agent can find like or similar answers later. \n\nA simple ranking system that users can ‚Äòthumbs up/down‚Äô responses (and potentially provide feedback) helps a lot for refining the system down the road. Good luck!",
          "score": 4,
          "created_utc": "2026-01-15 00:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzov0wj",
              "author": "cisspstupid",
              "text": "I agree to this approach. I'm myself starting to look into building knowledge graph and how to use them. If u/ggone20 have any good references for learning or tips. Those will be highly appreciated.",
              "score": 1,
              "created_utc": "2026-01-15 07:19:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpcwac",
          "author": "TechnicalGeologist99",
          "text": "Legal documents are not really semantic. \n\nThe semantics of the text help get us in the correct postcode...but it doesn't help us to reason or extract full threads of information. \n\nThis is because legal documents are actually hiding a great deal of latent structure. \n\nThis is why people use knowledge graphs for high stakes documents like this. \n\nYou need to hire someone with research expertise in processing legal text. \n\nBuilding a useful knowledge graph is very difficult.\n\nAnyone who says otherwise is a hype gremlin that's never had to evaluate something with genuinely high risk outputs.\n\nYou should also be aware that KGs usually run in memory and are  memory hungry. This will be a major consideration for deployment. Either you already own lots of RAM (you lucky boy) or you're about to find out how much AWS charge per GB",
          "score": 4,
          "created_utc": "2026-01-15 10:10:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmrcb1",
          "author": "DeadPukka",
          "text": "Any links to the type of docs you‚Äôre working with? (If public)\n\nAnd what types of prompts are you using?  \n\nAre you doing prompt rewriting? Reranking?\n\nAre you locked into only on-prem?",
          "score": 2,
          "created_utc": "2026-01-14 23:16:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4etw",
          "author": "FormalAd7367",
          "text": "in my experiences, law/statues/policy doc are hardest because there are so many variables that agentic ai can‚Äôt read like sec 7(x)(78) of law -> sec 7(x7(8) of law\n\nthen there are tables and long winded text.. and law that was superseded by another law like 59 times",
          "score": 2,
          "created_utc": "2026-01-15 00:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4sah",
          "author": "hrishikamath",
          "text": "I have a feeling you should use agentic retrieval. Also do more of metadata engineering than anything. From my personal experience building an agent for sec filings. You can look at the readme for inspiration: https://github.com/kamathhrishi/stratalens-ai/blob/main/agent/README.md I am writing an elaborate blogpost on this too.",
          "score": 2,
          "created_utc": "2026-01-15 00:28:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobqkz",
          "author": "tony10000",
          "text": "I would use a dense model rather than OSS20B.  I am not sure that a MoE model is up to the task.  If you must use MoE, try: [https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)",
          "score": 2,
          "created_utc": "2026-01-15 04:46:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzogl1v",
          "author": "Past-Grapefruit488",
          "text": "Consider : \n\n1. Full text search via Elastic Search or similar (in addition to vecros and graph store)  \n2. Agentic RAG taht uses all these and evaluates it in context of inputs (initial as well as subsequent)",
          "score": 2,
          "created_utc": "2026-01-15 05:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolkqm",
          "author": "Rokpiy",
          "text": "hierarchical chunking helps but doesn't solve cross-references. legal docs have section 7.2.3 referencing \"as defined in section 2.1\" which has exceptions in section 5.4. flat chunks break these chains, hierarchical chunks reduce it but don't eliminate it\n\nwhat worked for us: dual-layer retrieval. first pass gets relevant sections via embeddings, second pass explicitly searches for cross-reference patterns in those sections and fetches the referenced content. regex + heuristics after semantic search\n\nyou can't solve this with better embeddings because legal language encodes relationships through explicit references, not semantic similarity. \"section 7.2.3\" and \"section 2.1\" have zero semantic overlap but maximum logical dependency\n\ngraph rag helps if you pre-extract \"section X references section Y\" during ingestion. but that's a parsing problem, not retrieval. most teams skip it because the parsing is fragile and breaks on updates\n\nfor the 64k context issue: either accept incomplete context or implement multi-hop retrieval where the model asks for missing definitions when it hits a reference",
          "score": 2,
          "created_utc": "2026-01-15 05:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqkbvr",
          "author": "DistinctRide9884",
          "text": "Check out SurrealDB, which is multi-model and has support for graph, vectors, documents and can be updated in real time (vs. other graph DBs where you have to rebuild the cache each time time you update the graph).\n\nThen for the documenting parsing/extraction something like¬†[https://cocoindex.io/](https://cocoindex.io/)¬†might be worth exploring, their core value prop is real-time updates and full traceability from origin into source. A CocoIndex and SurrealDB integration is in the works.",
          "score": 2,
          "created_utc": "2026-01-15 14:58:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqvuzd",
              "author": "Early_Interest_5768",
              "text": "CocoIndex looks great thanks.",
              "score": 2,
              "created_utc": "2026-01-15 15:52:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo6491",
          "author": "Recursive_Boomerang",
          "text": "https://medium.com/enterprise-rag/deterministic-document-structure-based-retrieval-472682f9629a\n\nMight help you out. PS I'm not affiliated with them",
          "score": 1,
          "created_utc": "2026-01-15 04:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobiin",
          "author": "charlesthayer",
          "text": "Please tell us a little bit more about your current setup. I'm wondering how many agents or sub agents you are currently using? \n\nEg. Do you have a search assistant agent that focuses just on finding docs (without full context)? \nAre you using a memory system like mem0 or a skill system like ACE?\nDo your prompts include both negative and positive examples? \nDo you have a set of evals that provides precision and recall (relevance)\n\nMy first thought is that it would probably help to extract a fair amount of metadata for each document into a more structured database. So this would be a separate pipeline that understands key important things that you're looking for in these docs. Eg. Which compliance or audit standards are discussed, etc.\n\nAdding the graph DB should be a great help. Doing multiple levels of chunking so that you include whole paragraphs and sections will also be a help for ranking. I'm not familiar with law or legal documents, but I imagine there may be some models fine-tuned for your legal domain.\n\nSounds interesting!",
          "score": 1,
          "created_utc": "2026-01-15 04:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoc0kd",
          "author": "tony10000",
          "text": "You may also want to check out Anything LLM.  It has excellent RAG capabilities.",
          "score": 1,
          "created_utc": "2026-01-15 04:48:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzojqrq",
          "author": "AloneSYD",
          "text": "First you need to work on you chunking for splitting you need to optimize splitting for the your kind of documents and specially cross pages, section\n\nLook up contextual retrieval, basically you need to add metadata to each chunk. CR can help in two ways either like a first stage retrieval or it can embedded with each chunk to enhance relevant chunks\n\n If you don't have enough context length to take a full document, make a custom one that will take n #pages before and after to create the context for each chunk instead of the whole document.\n\nI would highly recommend is to use a reAct agent. Because the reflection step help it in many situations to requery until it reaches a satisfiying state and you can specify the criteria on the answer is complete",
          "score": 1,
          "created_utc": "2026-01-15 05:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzom65c",
          "author": "RecommendationFit374",
          "text": "Have you tried papr.ai we have document ingestion  u can use reducto or other providers, define your custom schema and auto build graph we combine vector + graph + prediction models it works well at scale. See our docs at platform.papr.ai",
          "score": 1,
          "created_utc": "2026-01-15 06:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzometk",
              "author": "RecommendationFit374",
              "text": "Our open source repo here https://github.com/Papr-ai/memory-opensource \n\nWe will bring doc ingestion to open source soon",
              "score": 1,
              "created_utc": "2026-01-15 06:06:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzp0q11",
          "author": "cat47b",
          "text": "Can you share an anonymised example of the exceptions and overrides text. As others have said a chunk may need metadata that refers to others which need to be retrieved as part of that overall context",
          "score": 1,
          "created_utc": "2026-01-15 08:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpp9nl",
          "author": "HonestoJago",
          "text": "Law firms are hard. As soon as there‚Äôs one slight error, or one response that isn‚Äôt ‚Äúcomplete‚Äù, everyone will stop using it. It‚Äôs probably malpractice just to rely on the RAG, anyway, and not review the full underlying docs, so I‚Äôm not really sure there‚Äôs a lot of value there. I think tools for email management and retrieval are easier to build and are more attractive to attorneys, who are constantly overwhelmed by emails and can‚Äôt keep track of project status, etc‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-15 11:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqciya",
          "author": "my_byte",
          "text": "Hard disagree with folks screaming graph. If you do A/B with GraphRAG vs agentic, multi turn search and normalize for tokens - the latter will have similar results, without the operational headache that comes from graph updates, disambiguation and such.\nI'm not sure if I like gpt-oss tbh. Have you tried other models?\nGenerally speaking - what's your approach to measure recall and accuracy in your system?",
          "score": 1,
          "created_utc": "2026-01-15 14:19:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxom6i",
      "title": "[OpenSource | pip ] Built a unified PDF extraction & benchmarking tool for RAG ‚Äî PDFstract (Web UI ‚Ä¢ CLI ‚Ä¢ API)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pxom6i/opensource_pip_built_a_unified_pdf_extraction/",
      "author": "GritSar",
      "created_utc": "2025-12-28 10:52:22",
      "score": 31,
      "num_comments": 16,
      "upvote_ratio": 0.97,
      "text": "I‚Äôve been experimenting with different PDF ‚Üí text/markdown extraction libraries for RAG pipelines, and I found myself repeatedly setting up environments, testing outputs, and validating quality across tools.\n\nSo I built **PDFstract** ‚Äî a small unified toolkit that lets you:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract) \n\n* upload a PDF and run it through multiple extraction / OCR libraries\n* compare outputs side-by-side\n* benchmark quality before choosing a pipeline\n* use it via **Web UI, CLI, or API** depending on your workflow\n\n\n\nRight now it supports libraries like \n\n\\- Unstructured\n\n\\- Marker\n\n\\- Docling\n\n\\- PyMuPDF4LLM\n\n\\- Markitdown, etc., and I‚Äôm adding more over time.\n\n\n\nThe goal isn‚Äôt to ‚Äúreplace‚Äù these libraries ‚Äî but to make evaluation easier when you‚Äôre deciding which one fits your dataset or RAG use-case.\n\nIf this is useful, I‚Äôd love feedback, suggestions, or thoughts on what would make it more practical for real-world workflows.\n\nCurrently working on adding a Chunking strategies into PDFstract post conversion so that it can directly be used in your pipelines .\n",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1pxom6i/opensource_pip_built_a_unified_pdf_extraction/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwd5955",
          "author": "bonsaisushi",
          "text": "Looking great! Have you done any testing in heavy pdfs (1000+ pages) by any chance?",
          "score": 2,
          "created_utc": "2025-12-28 14:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdft5t",
              "author": "anashel",
              "text": "I will test it and let you know, but I previously helped a client convert a library of 20,000 books, each 200 to 300 pages. I battle tested just about everything you can imagine, from Python OCR pipelines to Grok, GPT, Claude, and others. Because the output was for audiobooks, precision and quality were non negotiable, which made this especially difficult.\n\nIn the end, no one came close to Mistral OCR. And I mean no one, by a thousand miles.\n\nPricing was $0.003 to $0.007 per page, with a batch API that can process an entire PDF library. But the real kicker is the JSON schema support.\n\nYou define exactly not only how the content is returned, but also the meta analysis you want run on each page. Meta data like; is this page part of a table of contents? Does the text continue on the next page, or is the last sentence complete?  There are no prompt gimmicks to force valid JSON. It is native, structured, and reliable. It is also massively strong in multiple languages, which mattered for us since the content was in French.\n\nWhat used to be a complex, fragile pipeline is now simple. An R2 bucket, a PDF upload, a queue event triggered on upload, a 20 line Cloudflare Worker invoking Mistral, and all content is saved back into R2 and automatically indexed by Cloudflare RAG. That is it. Nothing else. You could build the same thing on AWS in less than a day.\n\nRight now I am working on a much more challenging project: converting an archive of 500,000 printed architectural blueprints, mostly phone and fiber cable connections inside buildings, so we have a powerful search engine with an agent that can generate a first pass analysis and inventory for technicians planning a repair at a location.",
              "score": 4,
              "created_utc": "2025-12-28 15:11:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdllkq",
                  "author": "bonsaisushi",
                  "text": "That is actually impressive, I'll surely give it a try, thanks!",
                  "score": 3,
                  "created_utc": "2025-12-28 15:42:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf0rvw",
                  "author": "getarbiter",
                  "text": "That use case is actually a very clean fit.\n\nWhere keyword search breaks on technical drawings is terminology drift over time and roles. \n\nThe people querying (‚Äúfiber issue third floor east wing‚Äù) rarely use the same language as the original drafters (‚Äúoptical distribution frame,‚Äù ‚ÄúFJB,‚Äù ‚Äútelecom riser,‚Äù etc.).\n\n Zero keyword overlap, same physical thing.\n\nARBITER isn‚Äôt an OCR or extraction layer ‚Äî it sits after that. \n\nIt scores semantic coherence between a technician‚Äôs description and candidate documents.\n\nFor a blueprint archive like yours, that enables a few concrete things:\n\nCross-document retrieval\nNatural-language queries surface relevant drawings even when terminology differs completely from the query.\n\nInventory sanity checks\nScore extracted equipment lists against expected semantic patterns. Low coherence is a signal for likely extraction errors or anomalous drawings worth review.\n\nRepair triage\nA technician describes an issue in their own words; ARBITER narrows 500k documents down to a small, semantically coherent candidate set.\n\nIt‚Äôs deterministic, CPU-only, ~26MB, and runs air-gapped ‚Äî which tends to matter once building data becomes sensitive.\n\nNot competing with docling/unstructured ‚Äî complementary. Those get text out; this helps determine which documents actually mean what the technician is asking about.\n\nOut of curiosity, what are you using today to search or route those blueprints once they‚Äôre extracted?",
                  "score": 1,
                  "created_utc": "2025-12-28 19:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwd5qd8",
              "author": "GritSar",
              "text": "I have tried 100 pages and since pdfstract is a wrapper on top of libraries like unstructured, miner, docling, tessaract etc \n\nThe performance is subjective to the document and the system capacity \n\nBut it can be done",
              "score": 2,
              "created_utc": "2025-12-28 14:11:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdbc7i",
          "author": "silvrrwulf",
          "text": "This sounds really great.  Have you found any to be better at one type vs another?  Say, Docling is great at unstructured legal but can't parse medical, or any generalities?  Curious if you found some do better in certain industries than others due to formatting, vocabulary, etc.  \n\nThis sounds really cool.",
          "score": 2,
          "created_utc": "2025-12-28 14:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdh8nu",
              "author": "GritSar",
              "text": "It is subjective to  usecases and this is what I have found in general.\n\nhttps://preview.redd.it/mlfobhrdpy9g1.png?width=1086&format=png&auto=webp&s=03b70fcda171180b1fc246b5b670a8a2652f6b54",
              "score": 3,
              "created_utc": "2025-12-28 15:19:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwe77ts",
                  "author": "silvrrwulf",
                  "text": "This is very helpful!  Thanks!!",
                  "score": 2,
                  "created_utc": "2025-12-28 17:31:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhtz4f",
                  "author": "Snoo-85117",
                  "text": "You should test out docstrange, Nanonets ocr",
                  "score": 2,
                  "created_utc": "2025-12-29 05:02:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfmplw",
          "author": "OnyxProyectoUno",
          "text": "The side-by-side comparison saves so much time over setting up each library separately.\n\nOne thing that bit me was that parser comparison is only half the story. Even when you find the best parser for your docs, chunking strategy can completely change what your RAG system actually sees. I ended up building something similar at vectorflow.dev but focused on the full preprocessing pipeline, not just extraction.\n\nThe chunking addition you mentioned sounds like the right direction. Being able to see how different chunking strategies affect the same parsed content would be huge. What's your plan for the chunking comparison UI?",
          "score": 2,
          "created_utc": "2025-12-28 21:38:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhxivr",
              "author": "GritSar",
              "text": "In next release chunking strategies would come - it‚Äôs being added",
              "score": 1,
              "created_utc": "2025-12-29 05:27:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwi0ua4",
                  "author": "OnyxProyectoUno",
                  "text": "I love it. We're both addressing the same problem from different angles. Good luck to you sir!",
                  "score": 2,
                  "created_utc": "2025-12-29 05:53:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdqjr9",
          "author": "Vegetable-Second3998",
          "text": "What makes this different from or better than https://www.docling.ai?",
          "score": 1,
          "created_utc": "2025-12-28 16:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdtemo",
              "author": "GritSar",
              "text": "It‚Äôs just a wrapper for validating and using libraries like docling, unstructured etc and benchmark results and use multiple ocr libraries in your data engineering pipeline",
              "score": 2,
              "created_utc": "2025-12-28 16:22:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q5wxb3",
      "title": "ChatEpstein - Epstein Files RAG Search",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q5wxb3/chatepstein_epstein_files_rag_search/",
      "author": "br3nn21",
      "created_utc": "2026-01-06 22:14:52",
      "score": 30,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "While there‚Äôs been a lot of information about Epstein released, much of it is very unorganized. There have been platforms like¬†[jmail.world](http://jmail.world), but it still contains a wide array of information that is difficult to search through quickly.\n\nTo solve these issues, I created¬†[ChatEpstein](https://chat-epstein.vercel.app/), a chatbot with access to the Epstein files to provide a more targeted search. Right now, it only has a subset of text from the documents, but I was planning on adding more if people were more interested. This would include more advanced data types (audio, object recognition, video) while also including more of the files.\n\nHere‚Äôs the data I‚Äôm using:\n\n**Epstein Files Transparency Act (H.R.4405)**¬†\\-> I extracted all pdf text\n\n**Oversight Committee Releases Epstein Records Provided by the Department of Justice**¬†\\-> I extracted all image text\n\n**Oversight Committee Releases Additional Epstein Estate Documents**¬†\\-> I extracted all image text and text files\n\nOverall, this leads to about 300k documents total.\n\nWith all queries, results will be quoted and a link to the source provided. This will be to prevent the dangers of hallucinations, which can lead to more misinformation that can be very harmful. Additionally, proper nouns are strongly highlighted with searches. This helps to analyze specific information about people and groups. My hope with this is to increase accountability while also minimizing misinformation.\n\n\n\n**Here‚Äôs the tech I used:**\n\nFor initial storage, I put all the files in an AWS S3 bucket. Then, I used Pinecone as a vector database for the documents. For my chunking strategy, I initially used a character count of 1024 for each chunk, which worked well for long, multipage documents. However, since many of the documents are single-page and have a lot of continuous context, I have been experimenting with a page-based chunking strategy. Additionally, I am using spAcy to find people, places, and geopolitical entities. \n\n\n\nDuring the retrieval phase, I am fetching both using traditional methods and using entity-based matching. Doing both of these gives me more accurate but diverse results. I am also having it keep track of the last 2 2 exchanges (4 messages: 2 user + 2 assistant). Overall, this gives me a token usage of 2k-5k. Because I‚Äôm semi-broke, I‚Äôm using Groq‚Äôs cheap llama-3.1-8b-instant API.\n\n\n\nOne of the most important parts of this phase is accuracy. Hallucinations from an LLM are an inherent certainty in some instances. As a result, I have ensured that I am not only providing information, but also quotes, sources, and links to every piece of information. I also prompted the LLM to try to avoid making assumptions not directly stated in the text.\n\n\n\nWith that being said, I‚Äôm certain that there will be issues, given the non-deterministic nature of AI models and the large amount of data being fed. If anyone finds any issues, please let me know! I‚Äôd love to fix them to make this a more usable tool.\n\n\n\n\n\n[https://chat-epstein.vercel.app/](https://chat-epstein.vercel.app/)\n\n\n\n[](https://www.reddit.com/submit/?source_id=t3_1q5mjzy)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q5wxb3/chatepstein_epstein_files_rag_search/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny3ozgd",
          "author": "Far_Statistician1479",
          "text": "Maybe a cool idea but ‚ÄúChatEpstein‚Äù is probably the worst name I‚Äôve ever heard",
          "score": 5,
          "created_utc": "2026-01-06 23:35:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5j6ib",
              "author": "CallOfBurger",
              "text": "ChatGePsTein would have been goated",
              "score": 2,
              "created_utc": "2026-01-07 05:57:42",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny40cty",
              "author": "Adorable_Speed_6137",
              "text": "lmao",
              "score": 1,
              "created_utc": "2026-01-07 00:34:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny4933i",
              "author": "br3nn21",
              "text": "Ngl, I don't disagree with you. Open to suggestions lol",
              "score": 1,
              "created_utc": "2026-01-07 01:20:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nybb1k6",
                  "author": "br3nn21",
                  "text": "Late, but renamed to JSearch... I did have something like ChatGePsTein in mind intially ngl",
                  "score": 1,
                  "created_utc": "2026-01-08 01:19:01",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny3hbx9",
          "author": "OnyxProyectoUno",
          "text": "Your page-based chunking strategy makes sense for single-page documents, but you're hitting the classic tradeoff between context preservation and retrieval precision. With 300k documents, that character-based approach probably fragments important relationships across chunk boundaries.\n\nThe entity extraction with spaCy is smart. You're already doing hybrid retrieval with traditional similarity plus entity matching, which catches things that pure semantic search misses. But watch out for entity disambiguation - \"Clinton\" could refer to Bill or Hillary, and without proper linking, you might surface the wrong context.\n\nOne gotcha with legal documents like these: they're full of references and citations that lose meaning when chunked. \"As stated in paragraph 3\" becomes useless if paragraph 3 is in a different chunk. I've been building document processing tooling at vectorflow.dev specifically for these preprocessing visibility issues.\n\nYour hallucination mitigation with quotes and sources is the right approach. But the real accuracy killer is usually upstream in chunking, not the LLM itself. If a key piece of evidence gets split across chunks or loses its document context, no amount of prompting fixes that.\n\nHave you tested how your chunking handles multi-column layouts or embedded tables? Court documents are notorious for weird formatting that breaks standard text extraction. Also curious about your metadata strategy - are you preserving document dates, case numbers, or filing types at the chunk level?",
          "score": 5,
          "created_utc": "2026-01-06 22:56:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4b3zv",
              "author": "br3nn21",
              "text": "Third paragraph is a great point, and it's something I thought about, but had no real solution for at the time other than prompting it to ensure valid context.  It actually was an issue I faced, as there were times it would attribute quotes to people who were around the quote, not who it was actually referencing, due to page cut-offs (e.g. A quote about someone who was an inspiration to black scientists was claimed to be referring to Epstein.... this is not the case).\n\nI have not tested to the tool on multi-column layouts, but have attempted to mitigate incorrect context, as mentioned above. Many of the documents, especially the later emails and .txt files, do not have many of these issues. I did not get to the CSV's yet. If you have any strategies you would recommend, I would love to hear.\n\nA lot of thse docuements are simply thrown into a google drive/zip file, so there was not an easy way to extract dates or numbers. There were solutions I considered with using LLM's to extract these, but I decided against to maintain simplicity for now.\n\n  \nThat's an awesome tool! I absolutely see the use, especially as RAG is applied to large corporate/professional documents so frequently.",
              "score": 2,
              "created_utc": "2026-01-07 01:31:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nz50mna",
              "author": "stevevaius",
              "text": "Legal text search and citation is very importat for my project. Can you elaborate more on this with your solution?",
              "score": 1,
              "created_utc": "2026-01-12 10:18:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny3ex9i",
          "author": "Ok_Mirror7112",
          "text": "Hahaha, I guess someone had to do it",
          "score": 2,
          "created_utc": "2026-01-06 22:44:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4cj6z",
          "author": "zapaljeniulicar",
          "text": "https://preview.redd.it/s5gl8dbnztbg1.jpeg?width=2187&format=pjpg&auto=webp&s=e99ce287669ef57a1c377f1803c78218c187a63c\n\nDid it months ago on my local machine using LangChain, Chroma (and faiss), Ollama, Llama 3.2 and UI in Gradio, obviously.",
          "score": 2,
          "created_utc": "2026-01-07 01:39:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4gp5k",
              "author": "br3nn21",
              "text": "This is awesome, I had a hard time believing no one had done this yet. You should deploy it!",
              "score": 2,
              "created_utc": "2026-01-07 02:02:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny4s6w8",
                  "author": "zapaljeniulicar",
                  "text": "It was done for learning purposes only, and I don‚Äôt have enough money to run LLM for sheets and giggles. What I‚Äôve found hilarious is, if you check the image, it shows the Epstein‚Äôs email address clearly (Gmail), but if I asked for Epstein‚Äôs email address, safeguards baked into Llama 3.2 would return ‚Äúcan‚Äôt do that Dave‚Äù, but if I asked ‚Äúwhat email address were people using to communicate with Epstein?‚Äù It would return the address :) Love it.",
                  "score": 2,
                  "created_utc": "2026-01-07 03:04:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny6j1o4",
          "author": "Snoo-85117",
          "text": "What did you use to extract the text data from the 300k odd documents?",
          "score": 2,
          "created_utc": "2026-01-07 11:14:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nybb95s",
              "author": "br3nn21",
              "text": "I used pytesseract for OCR and pdfplumber for pdf documents",
              "score": 1,
              "created_utc": "2026-01-08 01:20:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny4i2xf",
          "author": "StackOwOFlow",
          "text": " Responds just like the DOJ\n\nhttps://preview.redd.it/6ub5y62j5ubg1.png?width=1658&format=png&auto=webp&s=40d4b6d60ec5787e6ccf2a3cc28f45d07f452fcb",
          "score": 1,
          "created_utc": "2026-01-07 02:09:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4khda",
              "author": "br3nn21",
              "text": "It struggles with impartial/exact data. It works by attempting to query a limited number of data chunks, so I think opinionated/very exact statistics wouldn't do well with it.",
              "score": 1,
              "created_utc": "2026-01-07 02:22:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q8zx27",
      "title": "Grantflow.AI codebase is now public",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q8zx27/grantflowai_codebase_is_now_public/",
      "author": "Goldziher",
      "created_utc": "2026-01-10 09:33:32",
      "score": 28,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "Hi peeps,\n\nAs I wrote in the title. I and my cofounders decided to open https://grantflow.ai as source-available (BSL) and make the repo public. Why? well, we didn't manage to get sufficient traction in our former strategy, so we decided to pivot. Additionally, I had some of my mentees helping with the development (junior devs), and its good for their GitHub profiles to have this available. \n\nYou can see the codebase here: https://github.com/grantflow-ai/grantflow -- I worked on this extensively for the better part of a year. This features a complex and high performance RAG system with the following components:\n\n1. An `indexer` service, which uses [kreuzberg](https://github.com/kreuzberg-dev/kreuzberg) for text extraction.\n2. A `crawler` service, which does the same but for URLs.\n3. A `rag` service, which uses pgvector and a bunch of ML to perform sophisticated RAG.\n4. A `backend` service, which is the backend for the frontend.\n5. Several frontend app components, including a NextJS app and an editor based on TipTap. \n\nI am proud of this codebase - I wrote most of it, and while we did use AI agents, it started out by being hand-written and its still mostly human written. It show cases various things that can bring value to you guys:\n\n1. how to integrate SQLAlchemy with pgvector for effective RAG\n2. how to create evaluation layers and feedback loops\n3. usage of various Python libraries with correct async patterns (also ML in async context)\n4. usage of the Litestar framework in production\n5. how to create an effective uv + pnpm monorepo\n6. advanced GitHub workflows and integration with terraform\n\nI'm glad to answer questions. \n\nP.S. if you wanna chat with me on discord, I am on the [Kreuzberg discord server](https://discord.gg/D5ZR83W5KM)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q8zx27/grantflowai_codebase_is_now_public/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyshij4",
          "author": "bsenftner",
          "text": "I've been watching your repo for over a year. I could just be biased, but with the current administration canceling grants left and right, the general perception I'm seeing is that grants are not really grants anymore, more like nepotistic and political operative gifts. This shift in cultural perception may be why a grant supporting startup has difficulty. Curious your thoughts, but due to the nature of the topic, you may want to respond in a private message.",
          "score": 3,
          "created_utc": "2026-01-10 14:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nysufvd",
              "author": "Goldziher",
              "text": "Absolutely - it's very much a shit show at the moment. But we approached this as a real world use case for AI that is a net positive for humans.",
              "score": 3,
              "created_utc": "2026-01-10 15:40:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyt5rny",
                  "author": "bsenftner",
                  "text": "I was considering something similar, and it was market research that led me to Grantflow.AI originally. I was curious if such a clear application of LLMs were already available. Good luck in your future projects. \"It's a jungle out there.\"",
                  "score": 1,
                  "created_utc": "2026-01-10 16:35:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nysoy1q",
          "author": "RolandRu",
          "text": "Thanks for open-sourcing this ‚Äî looks like a solid real-world RAG stack. I‚Äôll take a look. Any docs on the high-level architecture + deployment path?",
          "score": 1,
          "created_utc": "2026-01-10 15:12:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nystzx0",
              "author": "Goldziher",
              "text": "There is a docs folder. But I'll add more",
              "score": 2,
              "created_utc": "2026-01-10 15:38:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyss48a",
          "author": "Powerful-Ad-7237",
          "text": "Under your current BSL 1.1 license, would hosting a modified version of GrantFlow as a multi-tenant managed service for paying customers be permitted, or would that require a commercial license?",
          "score": 1,
          "created_utc": "2026-01-10 15:29:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nysu3rn",
              "author": "Goldziher",
              "text": "That would require a commercial license.",
              "score": 2,
              "created_utc": "2026-01-10 15:39:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nysua01",
          "author": "ai_hedge_fund",
          "text": "Would be very interested in learning, from the business aspect, what you feel is the cause of the lack of traction, level of effort expended/techniques used in trying to gain traction, suggestions for other AI businesses. Thanks for sharing your work.",
          "score": 1,
          "created_utc": "2026-01-10 15:40:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyt8m69",
              "author": "Goldziher",
              "text": "Oh that's the big question. I'd say in our case it's 30% us (founders), and the rest the market and reality. \n\nThe biggest problem is lack of trust - AI, privacy concerns, and of our product.",
              "score": 1,
              "created_utc": "2026-01-10 16:48:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyt3d26",
          "author": "Accomplished_Life416",
          "text": "Where did you get start to build this kind of Ai application?",
          "score": 1,
          "created_utc": "2026-01-10 16:24:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyt6tn7",
              "author": "Goldziher",
              "text": "The tech? It evolved over time. If started smaller and grew as the domain complexity grew. \n\nPersonally - I started building AI products in 2022. I learnt over time. I came from being a fullstack dev and OSS maintainer and just learnt.",
              "score": 1,
              "created_utc": "2026-01-10 16:40:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyt9uiv",
                  "author": "Accomplished_Life416",
                  "text": "Yes of course the tech part, indeed a quick response from your side says how much sincere you are (Sorry for bad english)\n\nThe logic for asking this question to you is only that to get same kind of motivation in work for Ai development",
                  "score": 2,
                  "created_utc": "2026-01-10 16:54:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyuvn3p",
          "author": "Oshden",
          "text": "OP, this is really amazing stuff!!! Thank you for open sourcing it! Do you think I might be able to leverage this for non-grant writing purposes? I have a repository of legal documents that I need a custom built AI agent to be able to process to help in claims and appeals for a vulnerable population and it seems like many parts of this project could help! I just don‚Äôt have a cs background; I‚Äôve had to teach myself everything that I know about code (and it‚Äôs not much lol)",
          "score": 1,
          "created_utc": "2026-01-10 21:34:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxe1qf",
              "author": "Goldziher",
              "text": "Probably.",
              "score": 1,
              "created_utc": "2026-01-11 05:54:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qaxwi5",
      "title": "We tested Vector RAG on a real production codebase (~1,300 files), and it didn‚Äôt work",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qaxwi5/we_tested_vector_rag_on_a_real_production/",
      "author": "Julianna_Faddy",
      "created_utc": "2026-01-12 15:30:12",
      "score": 28,
      "num_comments": 29,
      "upvote_ratio": 0.76,
      "text": "Vector RAG has become the default pattern for coding agents: embed the code, store it in a vector DB, retrieve top-k chunks. It feels obvious.\n\nWe tested this on a real production codebase (\\~1,300 files) and it mostly‚Ä¶ didn‚Äôt work.\n\nThe issue isn‚Äôt embeddings or models. It‚Äôs that similarity is a bad proxy for relevance in code.\n\nIn practice, vector RAG kept pulling:\n\n* test files instead of implementations\n* deprecated backups alongside the current code\n* unrelated files that just happened to share keywords\n\nSo, the agent‚Äôs context window filled up with noise. Reasoning got worse, not better.\n\nWe compared this against an agentic search approach using context trees (structured, intent-aware navigation instead of similarity search). We won‚Äôt dump all the numbers here, but a few highlights:\n\n* Orders of magnitude fewer tokens per query\n* Much higher precision on ‚Äúwhere is X implemented?‚Äù questions\n* More consistent answers for refactors and feature changes\n\nVector RAG did slightly better on recall in some cases, but that mostly came from dumping more files into context, which turned out to be actively harmful for reasoning.\n\nThe takeaway for me:\n\nCode isn‚Äôt documentation. It‚Äôs a graph with structure, boundaries, and dependencies. Treating it like a bag of words breaks down fast once the repo gets large.\n\nI wrote a [detailed breakdown](https://github.com/RyanNg1403/agentic-search-vs-rag?tab=readme-ov-file) of the experiment, failure modes, and why context trees work better for code (with full setup and metrics) here if you want the full take.\n\nCurious if others here have hit similar issues with vector RAG for code, or if you‚Äôve found ways to make it behave at scale.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qaxwi5/we_tested_vector_rag_on_a_real_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz6orvk",
          "author": "hrishikamath",
          "text": "You are not supposed to chunk codebase directly! Rather take functions/classes and then have a LLM summarize them and do RAG on that! Read Greptile engineering blog they describe this well",
          "score": 23,
          "created_utc": "2026-01-12 16:26:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9hnu6",
              "author": "technicalpickles",
              "text": "https://www.greptile.com/blog/semantic-codebase-search",
              "score": 2,
              "created_utc": "2026-01-13 00:25:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzaqp1m",
                  "author": "hrishikamath",
                  "text": "Thanks for the exact link",
                  "score": 1,
                  "created_utc": "2026-01-13 04:32:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzbbhoz",
              "author": "Diligent-Builder7762",
              "text": "*separate LLM does rag, summarizes them to the main one. and ripgrep. Yummy.",
              "score": 1,
              "created_utc": "2026-01-13 07:12:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzl2fbg",
              "author": "Julianna_Faddy",
              "text": "Hey, thanks for the Greptile tip, summarizing functions/classes first is smart. We tried method-level chunking, but vectors still missed code deps like imports, leading to noisy context. Worth benchmarking their approach  tho!",
              "score": 1,
              "created_utc": "2026-01-14 18:34:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzl8yyv",
                  "author": "hrishikamath",
                  "text": "I mean it‚Äôs not a complete solution by itself. You will have to make changes around that. I was telling more about the chunking part.",
                  "score": 1,
                  "created_utc": "2026-01-14 19:03:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6no0j",
          "author": "ggone20",
          "text": "Why downvoted to hell?\n\nLarge codebases need knowledge graphs, at the very least. Advanced RAG is a major driver of consulting business. On one hand I‚Äôm surprised there aren‚Äôt more write-ups but on the other‚Ä¶ I don‚Äôt want to give my secrets when commercially intelligent retrieval and inference over data stores is by far the biggest market right now in AI since every company can use it in some degree. \n\nYou mentioned in the larger piece the ‚Äòmuseum problem‚Äô - this is a function of poor architecture AND another area where graphs would help‚Ä¶ so long as you keep them updated. For large codebases, change management/hooks are critical to keep things up to date. It‚Äôs easy enough to automate but work does need to be done to this end. Just setup on-change scripts and reprocess files. üôÉ\n\nSeems like overall you‚Äôre happy with the system you ended up with, any other major headaches you haven‚Äôt figured out a solution for? Are the results good enough for your use case? I feel like the numbers, while dramatically better, are still rough‚Ä¶ no?",
          "score": 6,
          "created_utc": "2026-01-12 16:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl2t88",
              "author": "Julianna_Faddy",
              "text": "Haha, no clue on the downvotes, Reddit gonna Reddit. Totally agree on knowledge graphs for big codebases; that's why agentic search via context trees felt like a win because it basically builds a dynamic graph on the fly, respecting deps without the full KG overhead. On the museum problem, yeah, poor arch is part of it, but graphs + hooks for updates are key, we're automating re-indexing with git hooks now, keeps it fresh.\n\nWe're stoked with the setup; results are solid for our prod use (e.g., quick impl lookups), but yeah, numbers aren't perfect, hallucinations creep in on super ambiguous queries.",
              "score": 1,
              "created_utc": "2026-01-14 18:36:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzn7uk2",
                  "author": "ggone20",
                  "text": "Nice. It‚Äôs fun when there‚Äôs no ‚Äòright‚Äô answer I development. In prod it‚Äôs a bit different through haha. Keep us updated!",
                  "score": 2,
                  "created_utc": "2026-01-15 00:45:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6npz3",
          "author": "dyeusyt",
          "text": "we ran into pretty much the same problem when we got into this codebase-knowledge bog\n\nbut in our case we wanted to create a system that could evaluate GitHub repos. i.e only index the codebase once; not after every update.\n\nas we started, we didn‚Äôt find any resources on this other than videos showing graphical implementation of semantic search using Euclidian distance and so after looking at a bunch of open-source projects and VS Code‚Äôs way of managing things, we made our own Python library for it. our main goal was actually to build semantic search functionality out of it (note we had never done this kind of stuff earlier)\n\nso after some trial and error, we built the whole system. it chunks the codebase with the help of tree-sitter and builds a parent‚Äìchild relationship between chunks to reduce extra chunks and noise.\n\nalthough the semantic search started working, we soon realized the same problem as you did: ‚Äúsimilarity is a bad proxy for relevance in code.‚Äù\n\nLater on, we realized how some CLI-based coding agents like AWS Q work:\n\n\\* they read the project structure a lot, they do a lot of \\`cat\\` and \\`grep\\` instead of semantic searches, and still outperform agents that rely heavily on semantic search.\n\n\\* this gave us the idea to build more tools like \\`grep\\`, \\`cat\\`, \\`folder\\_structure\\`, etc (thanks to \\`chromadb\\` for these though)\n\n\\* so instead of solely relying on semantic search, we distributed the load.\n\n\\* now the agent gets the repo folder structure and is easily able to get a gist of the codebase from the structure itself\n\n\\* for files it considers important, it automatically \\`cat\\`s them; for patterns, it calls the pattern-matching tool.\n\n\\* and only for more generalized queries like ‚Äúauthentication implementation‚Äù does the agent currently do semantic tool calling.\n\n\\* this way, we were able to still use semantic tool calling, but in a much more efficient way.\n\na few days ago, I also posted a thread on this sub regarding too much noise in semantic search results. one better fix for that turned out to be using a dual-vector index mechanism and analyzing the intent of the semantic query each time, then redirecting it accordingly.\n\nwe‚Äôre still learning about these things, but this is basically how we‚Äôve monkey-patched our way till here.\n\nhere‚Äôs the thread mentioned above: [https://www.reddit.com/r/Rag/comments/1q3ksvy/how\\_do\\_you\\_tackle\\_semantic\\_search\\_ranking\\_issues/](https://www.reddit.com/r/Rag/comments/1q3ksvy/how_do_you_tackle_semantic_search_ranking_issues/)\n\nEdit: can you share your repo link (that 1300 files one), will probably see how our semantic search performs lol",
          "score": 4,
          "created_utc": "2026-01-12 16:21:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6owd9",
              "author": "Julianna_Faddy",
              "text": "the repo link [Experimental validation: Agentic Search (context trees) vs traditional RAG for code retrieval. Agentic wins with 99% fewer tokens and 2√ó better accuracy. Fully reproducible with automated pipelines.](https://github.com/RyanNg1403/agentic-search-vs-rag) this repo link is also included in the detailed breakdown blog",
              "score": 1,
              "created_utc": "2026-01-12 16:27:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7bur8",
          "author": "OnyxProyectoUno",
          "text": "Yeah, code as a bag of words is fundamentally broken. You're spot on about similarity being a terrible proxy for relevance in structured codebases.\n\nThe test files vs implementation thing is classic. Embeddings see similar function signatures and think \"this looks relevant\" when it's actually noise. Same with deprecated code that shares naming patterns with current implementations. Your context window fills up with garbage before you even get to reasoning.\n\nContext trees make way more sense for code. You're working with actual relationships instead of word soup. Function calls, imports, class hierarchies. That's the real structure that matters for understanding how code works.\n\nI've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) for document processing pipelines, and while that's different from code search, the core insight is the same. Structure matters more than similarity. With documents, I see people losing hierarchy, metadata, and relationships because they're chunking without considering document structure. Same problem, different domain.\n\nFor code specifically, have you looked at tree-sitter for parsing? It gives you the AST structure you need for proper context trees. Combined with dependency graphs from static analysis, you can build much smarter traversal than vector similarity.\n\nThe token efficiency gain you're seeing makes sense too. When you're pulling relevant code based on actual relationships instead of keyword matching, you need way fewer examples to give the model proper context.\n\nWhat does your context tree traversal look like? Are you doing breadth-first from entry points or something more sophisticated based on the query type?",
          "score": 2,
          "created_utc": "2026-01-12 18:11:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl3jms",
              "author": "Julianna_Faddy",
              "text": "Dude, this is spot on, we hit the exact same wall with pure semantic search being noisy AF for code. Love how you monkey-patched it with tree-sitter chunking, parent-child relations, and tools like grep/cat/folder\\_structure to offload the heavy lifting. That's basically agentic search: let the agent decide when to grep patterns or cat files vs semantic for broader stuff. In our tests, context trees did that graph traversal dynamically. \n\nWe're still tweaking too\n\nSure, here's the repo (no surprise, it's gemini-cli repo, a really big prod-ready repo):https://github.com/google-gemini/gemini-cli",
              "score": 1,
              "created_utc": "2026-01-14 18:39:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7fued",
          "author": "notAllBits",
          "text": "Code is structured. Use to operationalize your data. You turn structure into poison using just vector indexing",
          "score": 2,
          "created_utc": "2026-01-12 18:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl429y",
              "author": "Julianna_Faddy",
              "text": "That's right code's all structure, and straight vector indexing turns that gold into garbage by ignoring deps and hierarchies.",
              "score": 1,
              "created_utc": "2026-01-14 18:42:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6eeep",
          "author": "Creative-Chance514",
          "text": "I did not read your full post but the first half suggests that its a problem of indexing, if you will just index everything in the vector db you will get chunks from everything. In the very first class of computers when I was a kid I was taught about a concept called GIGO",
          "score": 4,
          "created_utc": "2026-01-12 15:38:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6ihcf",
              "author": "Technical-Will-2862",
              "text": "Your comment didn‚Äôt even answer their question so try reading more. Indexing is great but it doesn‚Äôt solve contextual relevance.¬†",
              "score": 4,
              "created_utc": "2026-01-12 15:57:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6msmt",
          "author": "irodov4030",
          "text": "which specific embedding model did you use to embed codebase?",
          "score": 1,
          "created_utc": "2026-01-12 16:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl18l7",
              "author": "Julianna_Faddy",
              "text": "I used text-embedding-3-small from OpenAI",
              "score": 1,
              "created_utc": "2026-01-14 18:29:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzl99jx",
                  "author": "irodov4030",
                  "text": "This is a model for embedding texts and more for general purpose tasks\n\n  \nIf you want to embed codebase try using specific models like CodeBERT. There are models available specific to your coding language too.\n\n [https://github.com/microsoft/CodeBERT](https://github.com/microsoft/CodeBERT)",
                  "score": 1,
                  "created_utc": "2026-01-14 19:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6o9gt",
          "author": "Medium_Chemist_4032",
          "text": "If you have access to a model with long context (like the unsloth extended nemotron 1M), try using it instead of RAG for filtering out relevant parts in the first stage of processing.  \nEDIT: I'd love to see a comparision to RAG of that",
          "score": 1,
          "created_utc": "2026-01-12 16:24:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6vju3",
          "author": "CallOfBurger",
          "text": "Have you added metadata on your embeddings after the chunking ? If you only get deprecated stuff and test file it might retrieve the wrong stuff. You could add title, tags, dates etc. you could filter out of your search",
          "score": 1,
          "created_utc": "2026-01-12 16:57:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdmm3d",
          "author": "TechnicalGeologist99",
          "text": "Code isn't semantic!\n\nIt's already structured and highly queriable/navigable\n\nWhy would we want to search for semantically similar text? \n\n\nUnless you have fine-tuned some embedding models to align with the semantic meaning of discrete pieces of code, and you are trying to find similar code that isn't syntactically similar but is semantically similar \n\nI.e. two separate implementations of one algorithm.\n\nThen I don't really see any need to be embedding code.",
          "score": 1,
          "created_utc": "2026-01-13 16:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdz2ao",
          "author": "Whole-Assignment6240",
          "text": "It really comes down to what metadata you collect and how you handle the codebase‚Äîfor example, respecting semantic breaks at code boundaries by tree-sitter.  \ntake a look at [https://cocoindex.io/examples/code\\_index](https://cocoindex.io/examples/code_index) (i'm the maintainer) we also plan to do graph version for this as well! lmk what you think!",
          "score": 1,
          "created_utc": "2026-01-13 17:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh0jzl",
          "author": "Academic_Track_2765",
          "text": "lol what are you doing!!!! This should be a case study on poor implementation. Code by design is not semantic! And this should be enough to understand why this failed.",
          "score": 1,
          "created_utc": "2026-01-14 02:52:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl5cuc",
              "author": "Julianna_Faddy",
              "text": "Agreed, that‚Äôs exactly why treating it as ‚Äújust embed + top-k‚Äù fails in practice. This wasn‚Äôt a toy setup: clean chunking, good embeddings, metadata, the usual best practices ‚Äî and it still pulled irrelevant tests/backups because similarity ‚â† intent",
              "score": 1,
              "created_utc": "2026-01-14 18:47:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmo161",
                  "author": "Academic_Track_2765",
                  "text": "You can solve this issue, we have done it, but it adds latency which is expected.",
                  "score": 1,
                  "created_utc": "2026-01-14 22:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz722md",
          "author": "jba1224a",
          "text": "‚ÄúWe used the wrong approach and got a bad result‚Äù\n\n*scrolls to bottom*\n\nYep someone else trying to sell something.\n\nIt would be so much more effective if you just stated the problem up front, offer up your solution - put a link to the raw data so people can comment and make a conclusion.\n\nAt the risk of sounding rude - no want wants to read your long winded pedantic Reddit post which is light on facts and heavy on generalizations.\n\nPs - code isn‚Äôt semantic.  Literally anyone in this space could explain in less than a paragraph why just straight embedding code using a non specialized model and then running pure semantic search against it would fail miserably.",
          "score": 0,
          "created_utc": "2026-01-12 17:27:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6ld9p",
          "author": "nineelevglen",
          "text": "I see zero reasoning on chunking strategy like AST using something like tree sitter. or picking an embedding model for code like Jina, Voyage etc. and performance between those. did you spend any time on that?",
          "score": 0,
          "created_utc": "2026-01-12 16:11:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1d00i",
      "title": "Reaching my wit‚Äôs end with PDF ingestion",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q1d00i/reaching_my_wits_end_with_pdf_ingestion/",
      "author": "fustercluck6000",
      "created_utc": "2026-01-01 19:41:33",
      "score": 27,
      "num_comments": 62,
      "upvote_ratio": 0.88,
      "text": "Recently had a client ask me at the last minute to ingest a large corpus of highly structured PDFs into the db for this application I‚Äôm building them. Some of these docs are several hundred pages long, and this was one of those frustrating examples of needless heartache because the PDFs were clearly exported from Word, they just couldn‚Äôt track down the original docx files.\n\nRight off the jump, the existing ingestion pipeline I‚Äôd built with Docling failed miserably (up until now it‚Äôs been structured file formats with the occasional small PDF, and ingestion has been pretty flawless). I spent way too much time trying to tweak things until I resorted to parsing pages with qwen3-vl and correcting all the formatting/parsing errors manually to meet an external deadline.\n\nAfter the number of different open-source tools/libraries I‚Äôve tried at this point (including some of unstructured‚Äôs open-source pdf tools, would consider paid options if I knew they‚Äôd work *much* better), I‚Äôm having trouble comprehending how something as dumb as getting reliably correct, structured text from a PDF (that‚Äôs visually identical to a word doc, no less) can be this much of a damn headache. Even just a single missing bullet point or incorrect section index in the right spot can completely throw off chunking and create total nightmares with retrieval later on.\n\nLike am I missing something, here? I feel confident in saying I‚Äôm good at what I do, but I think the client would have second thoughts about my competency if I told them I just spent all this time manually preprocessing documents to build them an application to literally automate preprocessing documents (not billing by the hour btw). I don‚Äôt usually work with PDFs much, especially not of this size (where structural components like chapters, lists, appendixes, etc become SUPER important), so if anyone here does and has some pro tips, please please please do share üôè",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q1d00i/reaching_my_wits_end_with_pdf_ingestion/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx4paox",
          "author": "fabkosta",
          "text": "You're not missing anything - the problem is just freakingly difficult. Sure, maybe a tool like Azure Document Intelligence might be slightly better than Docling, but only marginally so.\n\nIf your documents are all structured the same way, then you could use a template-based approach, but not all OCRing tools support that. (Not sure Docling does, but Azure Document Intelligence apparently does.)\n\nGenerally speaking your client does not give you a small last minute change, but a proper change request would be in place. OCRing docs is among the most challenging and time-consuming tasks to complete when it comes to building RAG solutions.",
          "score": 14,
          "created_utc": "2026-01-01 19:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4s3xq",
              "author": "fustercluck6000",
              "text": "Thanks for the reply! It‚Äôs good to know I‚Äôm not alone in this, although part of me almost wishes I was since that would mean there was a good solution out there. I didn‚Äôt mention this in the post, but everything does have to run locally on their air-gapped server for security reasons, so cloud-based tools are out of the question.\n\nAs an interim solution, I‚Äôm looking at creating a template and using more regex than I already am since the docs do follow a uniform structure, or at least they‚Äôre *supposed to* (tbd how I‚Äôll parse complex tables, which Docling is actually super good at). The frustrating thing is that there‚Äôs all these little human inconsistencies that any hard-coded preprocessing logic has to account for, e.g. in one document, subsection indexes would look like this: ‚Äò(a)‚Äô, while another one has ‚Äòa)‚Äô or ‚Äòa:‚Äô. Just a massive pain more than anything else honestly, idk why the hell corporate America got in this habit of using PDFs for literally everything in the first place haha",
              "score": 1,
              "created_utc": "2026-01-01 20:08:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx53lhv",
                  "author": "fabkosta",
                  "text": "Typically, for any project that intends to process documents, my rule-of-thumb is 80% of time to prepare docs and 20% to build the system itself. Maybe this is a bit exaggerated, but definitely 60:40 at least.\n\nPeople continuously underestimate the complexity of handling text documents - and PDFs are the worst ones, yet everyone is using them. Some PDFs are also intentionally created in a way that makes every OCRing tool fail, particularly when it comes to financial reports.",
                  "score": 5,
                  "created_utc": "2026-01-01 21:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx86nbo",
                  "author": "ai-mera",
                  "text": "\\> Azure Document Intelligence (DI) might be slightly better than Docling.\n\nWhile Azure Document Intelligence (DI) is better than nothing, it feels overpriced and doesn't deliver the expected level of performance.  \nParticularly, despite being a Microsoft product, its recognition of Word, PowerPoint, and Excel files is poor, requiring conversion to PDF before feeding them into DI.  \nUsing DI creates a black box. I believe tuning Docling according to specific use cases ultimately yields higher accuracy and lower costs.\n\nI was using DI, but I plan to start evaluating Docling after the new year.  \nI apologize for not being able to offer any specific hints. That's all for now.",
                  "score": 1,
                  "created_utc": "2026-01-02 09:20:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "ny403uh",
              "author": "Knight7561",
              "text": "I have just started using azure doc Intel for my pdf(which is highly structured with tables) , azure  returns and saves tables of the pdf in html format inside markdown. Do you think that‚Äôs the best format for tables or anything different in your opinion?",
              "score": 1,
              "created_utc": "2026-01-07 00:33:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny5qfh7",
                  "author": "fabkosta",
                  "text": "HTML or Markdown are both good formats, there's really not so much difference here. More important is the quality of extraction.",
                  "score": 1,
                  "created_utc": "2026-01-07 06:56:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx57re1",
          "author": "Porespellar",
          "text": "Apache Tika or Granite Docling. Tika is super easy to get going, just a simple Docker run.",
          "score": 5,
          "created_utc": "2026-01-01 21:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5tnes",
              "author": "fustercluck6000",
              "text": "Awesome, always love discovering new open source tools, reading up on Tika now!",
              "score": 1,
              "created_utc": "2026-01-01 23:26:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx515ha",
          "author": "Synyster328",
          "text": "It's because PDFs are for rendering graphics, not storing data. Like using a picture taken from your phone of a computer screen to represent JSON.",
          "score": 3,
          "created_utc": "2026-01-01 20:55:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx581du",
              "author": "fustercluck6000",
              "text": "And PDFs do make sense for a lot of things like hand-signed forms, scanned receipts, etc, but why people then insist on using it as the default standard for anything else that has structured text in it is completely beyond me. Even using acrobat for basic stuff imo is a total pain",
              "score": 2,
              "created_utc": "2026-01-01 21:31:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5denx",
                  "author": "Synyster328",
                  "text": "Yeah, it's a disaster, I spent like a year building a startup dependent on good PDF parsing and never came to a satisfying conclusion or solution, it just sucked ass forever.\n\nI ended up just treating them as two separate files - There's the text content you can extract programmatically, and there's the image content from rendering it.\n\nThe extracted text content often was a complete cluster fuck, the image content was often hard to decipher with custom fonts and weird layouts, but by treating them as two sides of the same document, and giving both to the AI at all times, with some rigid prompting, was usually able to make sense of it well enough.\n\nHowever, there were always some edge cases that would break the app. I ended up just diverting my focus to make those guardrails better, making it easy for users to flag bad outputs, RLHF pipelines, auditing, etc.",
                  "score": 2,
                  "created_utc": "2026-01-01 21:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8ynb8",
          "author": "TechnicalGeologist99",
          "text": "A decent bet here is to do hierarchical RAG. \n\nYou can use layout detection and parse headings to create a document hierarchy. (This is good for when you know documents are structured but don't necessarily know how, or it's too complex to fully apply true structure to all relevant docs. It will occasionally miss headings leading to an imperfect hierarchy) \n\nIt won't be 100%, you'll need to live with that.\n\nFor super structured docs like legal docs...it's really needed to build knowledge graph because it can capture more complex relationships that are needed in that domain. (You likely do not have time for this)\n\nThe lesson here is that you need to learn to say no to client. \n\nDid you agree to build such a complex system? Are they gonna pay your time to make the change? Do they understand how much of a unilateral change it is?",
          "score": 3,
          "created_utc": "2026-01-02 13:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxeglww",
              "author": "fustercluck6000",
              "text": "It's actually funny you mention legal docs because I've been working on a project in that area on the side for a little while now. With primary sources like statutes or case law, the structure itself is integral to how even the lawyers themselves read/interpret things (because of all the citations, definitions, precedents/priors, etc...), so I actually chose to hardcode hierarchical schemas (I guess technically hardcoding the dataclass factories but you get the idea) for chunking and adding nodes/edges to the knowledge graph before making any model calls, just because we didn't want to leave any margin for error when indexing really important, canonical materials like the U.S. Code or something (court documents like evidence and stuff are another story, though).\n\nThis definitely added a degree of complexity to the project that I didn't plan on signing up for before signing a new contract I'll be honest. And no, they really don't understand how much of a unilateral change it is, but to be fair I think a lot if not most people who aren't in aren't clued into the space would, either. I think we got CEOs promising the next model's gonna replace researchers with PhDs to thank for that lmao",
              "score": 1,
              "created_utc": "2026-01-03 07:33:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxfftnt",
                  "author": "TechnicalGeologist99",
                  "text": "I'll say that this is the biggest challenge in consulting in tech. The client doesn't see the work they see the result. \n\nMany companies agreed that they will produce a certain result for a certain price. This seems fair. But in reality many tech projects over run in costs and time by three times the expected amount.\n\nFor future projects you should aim to create a detailed PRD (product requirement document). It should capture the kind of documents you will be ingesting and what techniques will be used. It should state explicitly that changes to document type ingested will be added complexity and would require recosting of the work. \n\nThe PRD is what you point to to help you say no to the client (or rather \"yes, but more work = more cost\")\n\nEffectively we are just trying to protect ourselves and the client from scope creep mid-project. As this harms your business and creates tech debt for the client. Nobody really wins when scope creeps",
                  "score": 1,
                  "created_utc": "2026-01-03 12:30:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx581gt",
          "author": "ChapterEquivalent188",
          "text": "  \n**No you are not incompetent. You are fighting the \"Digital Paper\" fallacy**\n\n  \nFirst off: Take a deep breath. The fact that you resorted to manual fixing proves you care about data integrity. Most devs would have just ingested the garbage, delivered a hallucinating bot, and blamed the LLM.¬†**You did the job.**\n\n**Why this happens (The \"Word Export\" Trap):**  \nPDFs exported from Word are deceptively evil. Visually, they look structured  \nUnder the hood, a \"bullet point list\" in Word often gets exported as:\n\n1. A floating symbol (‚Ä¢) at coordinates X,Y.\n2. A text block at coordinates X+10, Y. Standard parsers (even Docling sometimes) read this as two separate, unrelated blocks. The semantic connection (\"This is a list item\") is lost.\n\n**The Solution: The Multi-Lane Approach**  \nI spent the last 2 years building an Enterprise RAG Architecture (V3 Core) exactly because of this headache. I found that¬†**no single tool**¬†works for 100% of pages.\n\n**My architecture uses a \"Consensus Engine\":**\n\n1. **Lane A (Text Layer):**¬†Since it's a Word export, the text layer is usually clean. Use¬†PyMuPDF¬†to extract raw text fast.\n2. **Lane B (Vision):**¬†Use a VLM (like¬†Qwen2-VL¬†or¬†Llama 3.2 Vision) ‚Äì¬†**BUT**¬†not to read everything. Use it to extract the¬†**Skeleton**¬†(Headers, List structures, Table boundaries).\n3. **Merge:**¬†Map the clean text from Lane A into the visual skeleton from Lane B.\n\n\n\n**Immediate Advice for you:**  \nSince you are on a deadline: Don't try to find the \"perfect tool\". It doesn't exist.  \nIf the docs are critical,¬†**Hybrid Parsing**¬†is the only way.  \nUse the Text Layer for content accuracy (0 hallucinations) and Vision for structural boundaries (Chunking)\n\nDon't let the client make you feel bad. Converting \"Print Layout\" back to \"Semantic Structure\" is one of the hardest problems in NLP right now",
          "score": 8,
          "created_utc": "2026-01-01 21:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5jcdl",
              "author": "fustercluck6000",
              "text": "I deeply appreciate this comment, super thoughtful and insightful and the kind of thing that makes me love Reddit tbh\n\nPart of the reason I stick to a fixed price policy vs. hourly is that I‚Äôll obsess over getting stuff like this right because I know full well how much boring shit like this will seriously matter in the end, so I‚Äôm happy to recoup the overtime spent on the data pipeline later. I know devs who just ignore known problems with data pipelines like that and it kinda makes me scratch my head. I think ingestion‚Äôs the single component of a RAG system with the most disproportionate impact on everything else (and that goes for accuracy and maintainability, both). Of all the things to half ass, ingestion sits at the bottom for me. And I‚Äôve half assed a few front ends in my day haha. But like c‚Äômon guys, this is literally why they say garbage in garbage out. \n\nAnd I never knew that about how Word exports PDFs but it makes total sense now (thinking about all the times sizing/scaling/page layouts get all mangled in a PDF export, which I always just assumed was *one of those things*). \n\nI actually tried working out something with PyMuPDF precisely because it extracts all the raw text correctly, but put a pin in it because I didn‚Äôt know how to combine that with structural/tabular information. Can you elaborate on how you‚Äôre merging clean text with the skeleton? I‚Äôm trying to think through the logic behind selecting text lines/paragraphs and allocating them to corresponding regions, and suddenly it makes a lot more sense how you could spend two years on this problem!\n\nAnd next meeting I have with the client‚Äôs upper management to give them a status update, I‚Äôm making sure *digital paper fallacy* is inserted at least 3 times in the conversation",
              "score": 2,
              "created_utc": "2026-01-01 22:30:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx79jwm",
                  "author": "ChapterEquivalent188",
                  "text": "I think you are already ready to understand my V3 ;) \nYou may have a look for a deeper understanding wahts awaiting you on day 3, 4 and 5........Yesterday I found Day 6 and it feels it never ends .....\n\nGlad the \"Digital Paper\" concept resonated! Use that term with your management. It shifts the blame from \"The dev is slow\" to \"The source material is flawed,\" which is the truth.\n\n\nLane B (Vision) acts as the Architect:\nIt looks at the page image and draws bounding boxes around logical sections. It says: \"There is a Header at [0, 0, 100, 50] and a Table at [0, 60, 500, 300].\" It creates the empty buckets.\n\nLane A (PyMuPDF) acts as the Miner:\nIt extracts the raw text words, but crucially, it also extracts their coordinates (fitz.Rect). It gives you the sand\n\nThe Merge (Spatial Join):\nYou write a function that checks: \"Which words from Lane A fall physically inside the box defined by Lane B?\"\n\nIf a word's center point is inside the \"Table Box\", it belongs to the table chunk.\n\nIf a word is inside the \"Header Box\", it becomes metadata\n\nHope it helpes and always enjoy what we do..... Its basic but necessary\n;)",
                  "score": 2,
                  "created_utc": "2026-01-02 04:41:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8z0as",
              "author": "TechnicalGeologist99",
              "text": "Yep, this approach is king. Especially when you can't go to the effort to understand each specific document structure and the many edge cases.",
              "score": 2,
              "created_utc": "2026-01-02 13:20:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbpxn6",
                  "author": "bravelogitex",
                  "text": "How do you merge both layers?",
                  "score": 2,
                  "created_utc": "2026-01-02 21:29:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxtgk9j",
              "author": "HonestoJago",
              "text": "I do something similar but with DeepSeek OCR for the second step. Definitely not a solved problem, but I do like the DeepSeek model.",
              "score": 1,
              "created_utc": "2026-01-05 14:01:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4p3o7",
          "author": "Snoo-85117",
          "text": "Use docstrange by Nanonets. \nYou can get the output in text format.",
          "score": 2,
          "created_utc": "2026-01-01 19:53:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4swdc",
              "author": "fustercluck6000",
              "text": "Thanks, I‚Äôll check it out!",
              "score": 1,
              "created_utc": "2026-01-01 20:12:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx7xqe3",
              "author": "bravelogitex",
              "text": "they are slow as hell though, like 10x compared to mistral in my exp",
              "score": 1,
              "created_utc": "2026-01-02 07:55:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4s20a",
          "author": "DeadPukka",
          "text": "The paid options have trials so you can see if it works for you. \n\nAzure Doc Intelligence has served us well, but look at the newcomers like Reducto. This shouldn‚Äôt be so painful; we support both in our platform and get reliable output.",
          "score": 2,
          "created_utc": "2026-01-01 20:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx52e6f",
              "author": "fustercluck6000",
              "text": "You‚Äôve just reminded me of all these Azure startup credits I have! (Marketed to ai startups but basically impossible to use for GPU time lmao) I‚Äôm going to look into both of these and running containers offline (necessary for security reasons). \n\nJust out of curiosity, how complex are the typical PDFs you‚Äôre working with? How substantially better are these than the open-source libraries out there (like if you had to estimate something like an indexing error rate)?",
              "score": 1,
              "created_utc": "2026-01-01 21:02:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5y26t",
          "author": "youre__",
          "text": "If the docs are highly structured and consistent across pages, an alternative approach is to brute-force it by cropping page images and OCRing the text. \n\nDocling and other systems can preserve page sections, but sometimes they have issues with relative placement when doing everything at once. If you do the segmentation yourself, you eliminate that as a variable. It will definitely take longer, but you only have to do it once.\n\nUse instructor or JSON response formats with qwen3-vl to ensure you grab the right properties. If it fails too many times, flag the section for human review.",
          "score": 2,
          "created_utc": "2026-01-01 23:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx68y1t",
              "author": "fustercluck6000",
              "text": "Cropping pages was actually one of the first steps I added in the pdf pipeline, since headers/footers are irrelevant anyways and always at the same heights. The issue I‚Äôve had with qwen3-vl (4b and 8b) is that by going page by page and exporting to markdown, without having all the previous context (like the last section‚Äôs index the previous header/indent levels), the model assumes whatever‚Äôs at the top of the page is the top-level header, and when numbered sections on that page start at, say, 3 (because section 2 was 6 pages ago), it assumes 3 is supposed to be a 1, then resets section 4 to 2, and so on‚Ä¶ Also, when sentences are broken up across pages (haven‚Äôt even gotten into multi-page tables yet), joining them back together properly is very error-prone, too. \n\nHaven‚Äôt had much luck tuning the prompt to prevent altering section indices, either, but to be fair there‚Äôs still a lot of room to experiment with that side of things. Also haven‚Äôt tried it with json output format, but now I‚Äôm wondering if there‚Äôs an elegant-ish way to enforce a pydantic output schema so I can pipe qwen3 outputs directly into a Docling document model‚Ä¶?",
              "score": 1,
              "created_utc": "2026-01-02 00:52:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6nr3a",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's the usual story with complex PDFs. The problem isn't your competency, it's that most parsing tools treat PDFs like they're just text with some formatting when they're actually structured documents with hierarchies, relationships, and semantic meaning that gets completely destroyed during extraction.\n\nWhat's killing you is the invisibility problem. You can't see what went wrong until you're deep into chunking and retrieval, discovering that bullet points got merged into paragraphs or section headers disappeared entirely. By then you're debugging symptoms three steps removed from the actual parsing failure.\n\nFor large structured PDFs, you need to preserve document hierarchy during extraction. Most tools flatten everything into linear text, which destroys the relationships between sections, subsections, and list items that your chunking strategy depends on. The fact that these were exported from Word makes it worse because they have implicit structure that PDFs don't preserve well.\n\nA few things to try: First, use OCR even on text-based PDFs. Sometimes the text extraction is garbage but the visual layout is clean. Second, chunk by logical sections rather than arbitrary token counts. If you can identify headers and preserve that hierarchy, your retrieval will be much more accurate. Third, validate your parsed output before chunking. Most people skip this step and wonder why their chunks are nonsensical.\n\nThe real issue is you're flying blind during document processing. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this problem because you need to see what your docs actually look like after parsing, before you commit to chunking strategies.\n\nWhat does your current output look like when Docling processes one of these PDFs? Are you losing the structural elements or is the text itself getting mangled?",
          "score": 2,
          "created_utc": "2026-01-02 02:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9pa0d",
              "author": "fustercluck6000",
              "text": "That invisibility problem is SO real, it‚Äôs these silent bugs that are a complete nightmare to even identify, let alone test for and fix. For now I‚Äôm just caching the parsed outputs in a test folder in my environment where I can easily look through them to see what‚Äôs going on during development.\n\nQuite a few people have suggested hybrid strategies and using OCR models to just detect the PDF‚Äôs layout and worry about text separately. Still thinking about how I want to implement that, but I‚Äôm all but certain at this point that‚Äôs how I‚Äôm going to design the pdf pipeline. Out of curiosity‚Äîwhen you say validate the parsed output, am I correct in assuming you mean Pydantic/something similar? I have a basic base model I‚Äôm using to validate simple markdown formatting syntax, but I do want to write more sophisticated checks for section indices at different depths and other structural stuff like that (which is uniform across all docs in this particular corpus).",
              "score": 1,
              "created_utc": "2026-01-02 15:45:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4pfl9",
          "author": "AsparagusKlutzy1817",
          "text": "PDF are not structured. It‚Äôs a canvas where you put lines, boxes and letters. Some pdfs have a style guide like clear spacing between paragraphs. Orderly distance of separate elements, etc., but not necessarily.\n\nIf you have a hard requirement of keeping bullet points together this will likely not work reliably. Some solutions may put more effort in keeping structure together than others but the task is per-se non deterministic. \nWhat works for one document may not work for another second one (style varies etc)\n\nWhy does it break when you miss a bulletpoint ? Create chunks with 500 token and generously let them overlap - even if a line slips up or down this should not matter.\nCan you elaborate what you are trying to do ? Maybe it‚Äôs easier to help then but PDF parsing is a mess - if the original word is gone you just lost so much structure and information. All you can do now is to make a sincere attempt in parsing but expect no magic",
          "score": 2,
          "created_utc": "2026-01-01 19:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4w4te",
              "author": "fustercluck6000",
              "text": "The only structural component of a PDF is the pages, which are completely useless 99% of the time anyway haha. But yeah to answer your question, this client has a few specific use cases for this but a good example is retrieving detailed compliance info they might receive from one of their own clients inside of an RFP. For reference, the compliance section of an RFP in their industry can be 300-400+ pages long and it‚Äôs all hyper-detailed (think specific codes, names, numbers, etc for various rules and regulations). This application helps the people who‚Äôd write the proposal in response to that RFP to get all the miscellaneous, granular details they need without having to spend half their time hunting for them.\n\nIt‚Äôs important for the db (neo4j) to map the structural hierarchy of the document so that when a cosine similarity or whatever retrieval method turns up a specific chunk, you can walk up and down the ‚Äòtree‚Äô to add additional context based on the location. For example, what chapter/section does this chunk belong to (since those typically name specific rules/regulations explicitly, while the chunk itself might only refer to it implicitly). It‚Äôs also often useful to add the previous/next chunks as context, but only if they‚Äôre at the same anatomical ‚Äòlevel‚Äô as the original chunk (so you don‚Äôt add irrelevant stuff from the next chapter because the original chunk happened to be the very last one in its respective chapter).\n\n Also knowing exactly where chunks came from so the model can cite sources in its responses is **critical** for fact-checking and safeguarding against hallucinations, (especially in this industry where a simple clerical mistake can cost thousands in fines), not to mention just plain useful for them.",
              "score": 2,
              "created_utc": "2026-01-01 20:28:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx4q2fp",
              "author": "AsparagusKlutzy1817",
              "text": "Oh why not load the full text document text based on a partial match on a chunk and then pipe the full document in blocks through an LLm if the full document is too large ?",
              "score": 1,
              "created_utc": "2026-01-01 19:57:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx59hm8",
          "author": "trollsmurf",
          "text": "PDF is formatted for presentation not editing. E.g. Adobe Acrobat can convert back to editable formats, and my experience is that it does it well.",
          "score": 1,
          "created_utc": "2026-01-01 21:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5bzyp",
              "author": "fustercluck6000",
              "text": "Acrobat is so obvious that I never actually thought about using it, I guess I‚Äôve been living under a rock because I never realized until just now that they actually have a Python API for this sort of stuff, is that what you use?",
              "score": 1,
              "created_utc": "2026-01-01 21:51:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5ptvn",
                  "author": "trollsmurf",
                  "text": "No, I convert manually. Not that many documents (yet).\n\nI also use this: [https://pypdf2.readthedocs.io/en/3.x/](https://pypdf2.readthedocs.io/en/3.x/)\n\nThis one converts to markdown, but I haven't tried it yet: [https://pypi.org/project/marker-pdf/](https://pypi.org/project/marker-pdf/)",
                  "score": 1,
                  "created_utc": "2026-01-01 23:05:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5mc05",
          "author": "davelargent",
          "text": "Have you tried https://www.docetl.org/ or LlamaParse? I‚Äôve had some success with those when others fell apart.",
          "score": 1,
          "created_utc": "2026-01-01 22:46:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5u51m",
              "author": "fustercluck6000",
              "text": "This is the first I‚Äôm hearing of DocETL, but the fact that the first thing on their landing page is an arxiv link tells me it‚Äôs definitely worth looking into in more depth, thanks for the rec!",
              "score": 1,
              "created_utc": "2026-01-01 23:29:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5x5cl",
          "author": "Lanky-Cobbler-3349",
          "text": "Just use gemini. Ask it to return a structured markfown format that preserves the original structure of the document if possible. Do it with low temperature and then apply a reasonable chunking strategy. Or use an OCR API that directly provides the structure without prompting. Docling etc. are shit. You need like 20 lines of code, you catch all the edge cases and dont have to implement some rule-based bullshit",
          "score": 1,
          "created_utc": "2026-01-01 23:46:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7xxts",
              "author": "bravelogitex",
              "text": "gemini flash 2.5 preview has worked great in my limited testing across 2 pdf's, one 18 page and another a 1 page table.\n\nsomeone said \"I work in fintech and we replaced an OCR vendor with Gemini at work for ingesting some PDFs\":¬†[https://news.ycombinator.com/item?id=42953665](https://news.ycombinator.com/item?id=42953665)",
              "score": 1,
              "created_utc": "2026-01-02 07:57:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx64llt",
          "author": "Far_Statistician1479",
          "text": "Why can‚Äôt you just convert the pages to markdown with a VLM and do whatever is needed with that?",
          "score": 1,
          "created_utc": "2026-01-02 00:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx69xfk",
              "author": "fustercluck6000",
              "text": "Accuracy, for smaller/less complex PDFs VLMs have worked totally fine but here, any structural parsing errors related to section indices and stuff, even minor ones effectively compound. \nI basically did what you‚Äôre describing with qwen3-vl-8b, and besides being super slow, the markdown wasn‚Äôt accurate enough on its own to chunk without making corrections, first.",
              "score": 1,
              "created_utc": "2026-01-02 00:58:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6f6th",
                  "author": "Far_Statistician1479",
                  "text": "Accuracy? I used smaller models than qwen3 and the accuracy is extremely high?",
                  "score": 1,
                  "created_utc": "2026-01-02 01:30:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6eubl",
          "author": "Flimsy_Vermicelli117",
          "text": "Did you try using Word to open these pdf files? If these are from Word, they open usually nearly flawlessly back into Word. Then save as more sane format. If you are on macOS, ChatGPT can whip out AppleScript which will convert folder of these using just Word. Word is AppleScriptable.",
          "score": 1,
          "created_utc": "2026-01-02 01:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6k8yp",
              "author": "fustercluck6000",
              "text": "I don‚Äôt even know how many years it‚Äôs been since I last had a Word license haha, but I‚Äôm definitely going to try and get my hands on one now because your suggestion sounds like it could just be a perfect, simple long term solution for this client in particular (who almost exclusively uses Microsoft enterprise stuff).\n\nAnd I‚Äôve never thought of doing it, but now I‚Äôm super interested in this idea of automating word tasks. My personal laptop is a Mac, but the project itself is running on a headless Ubuntu server that belongs to the client. It would probably be easy enough to work out something with WSL, I imagine. In the meantime, I think I‚Äôm going to set up some tests locally on my Mac to see if that‚Äôs worth pursuing, got any specific noteworthy tips/tricks for generating the AppleScript (never written any code in it before)? \n\nThanks for the advice",
              "score": 1,
              "created_utc": "2026-01-02 02:01:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6mlhw",
                  "author": "Flimsy_Vermicelli117",
                  "text": "I was never able to even phantom AppleScript as I could never find readable manual. Then I asked ChatGPT and turns out, it knows how to write AppleScripts well enough. Rarely on first attempt, but few iterations later and it's done. I asked it to help me to add right click function to convert Office documents to pdf and it wrote it fine and told me how to add it to macOS.\n\nI just asked ChatGPT and it generated macOS AppleScript as well as Windows PowerShell COM automation scripts.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6s15n",
          "author": "Flashy-Sprinkles1624",
          "text": "I don't mean to self-promote but I built a tool to specifically tackle this that I have since taken offline due to lack of uptake. Here's a like to the typescript SDK [https://github.com/Raptor-Data/ts-sdk](https://github.com/Raptor-Data/ts-sdk) . I intended to make a python SDK but haven't got that far. If it's something you think could be genuinely helpful then I'm more than happy to put it back online and chat with you to see how I can help. Please note that the current raptor data public site is for a different product so if you visit it just keep in mind the that these are two different tools.",
          "score": 1,
          "created_utc": "2026-01-02 02:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhmjr9",
          "author": "RolandRu",
          "text": "Yep ‚Äî you‚Äôve hit the ‚Äúlooks like Word, therefore it should parse like Word‚Äù trap. PDF is basically a rendering snapshot, so the semantic stuff you care about (lists, headings, reading order) is often implicit or just gone, even when it was exported from Word.\n\nA few things that have saved me pain in production:\n\nFirst, classify what you‚Äôre dealing with before you try to be clever. Born-digital PDFs with a clean text layer behave very differently than scanned/mixed ones, and even within born-digital you‚Äôll see ‚Äúnice text‚Äù but broken reading order because of columns, headers/footers, floating text boxes, etc. If it‚Äôs not stable, route it through a different path early instead of burning hours trying to tweak one tool.\n\nTreat layout as first-class signal, not just the text stream. Font size/weight, indentation, line spacing, and bounding boxes are what usually let you reconstruct headings and lists. Bullets and numbering are notoriously fragile if you rely on plain text output.\n\nChunk by structure rather than fixed token windows. Even an imperfect hierarchy based on detected headings/section titles beats 1k-token chunks when appendices/chapters matter, because you‚Äôll preserve intent and reduce ‚Äúwrong neighbors‚Äù in retrieval.\n\nKeep provenance for everything you store: page number and offsets at minimum, and bounding boxes when you can. It makes debugging and ‚Äúwhy did the model say this?‚Äù conversations a lot less painful.\n\nAdd a quick QC pass and a fallback. Simple checks like ‚Äúdid list item counts change?‚Äù, ‚Äúdid numbering skip?‚Äù, ‚Äúdid section headings disappear?‚Äù catch the cases that will later destroy chunking. When it fails, automatically re-run just those pages/sections with a more expensive extractor (layout-aware or vision-based) instead of paying that cost everywhere.\n\nAnd if the client can‚Äôt find the DOCX, ask them to re-export a tagged/accessible PDF if they possibly can. When headings and lists actually exist in the document structure tree, life gets dramatically easier ‚Äî it‚Äôs the closest you‚Äôll get to ‚ÄúDOCX semantics‚Äù coming out of a PDF.",
          "score": 1,
          "created_utc": "2026-01-03 19:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxteqz2",
          "author": "Early_Carrot_6972",
          "text": "Hey! First off, props for doing the manual work, it really shows that you care about the quality and you want to make sure everything is 100% accurate. Like @[ChapterEquivalent18](https://www.reddit.com/user/ChapterEquivalent188/)¬†said as well, Document Processing isn't a solved field yet, even though it's just word exports, there can be a lot of nuances around formatting, tables and such which even we might miss out on if we're not paying attention.\n\nIf you think about it, you ended up automating 70-80% of the work and manually had to correct around 20% of it max, which in perspective is 800 pages looking great and 200 needing some manual help (at worst?), which is something great that we tend to underappreciate.\n\nI work with Unstructured, and since you mentioned trying out open-source tool and being open to paid options, have you checked out our¬†[platform](https://unstructured.io/?modal=try-for-free)? We've been making a lot of improvements here, including parsing strategies with newer VLMs like GPT-5.1. There's a 15k page free trial as well, so it might be worth trying a few pages first to see how it handles them, especially since you're working against deadlines and need something reliable at scale. I can also chat more if you've got more questions. \n\nYou‚Äôre doing amazing. I hope your client sees it too!!",
          "score": 1,
          "created_utc": "2026-01-05 13:50:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxzfzm5",
          "author": "sullaugh",
          "text": "structured pdf extraction breaks fast once semantic layers are missing or inconsistent especially when sections and lists aren‚Äôt properly defined one workaround is using pdfelement early in the flow to normalize these documents by restoring readable structure and correcting visual breaks before ingesting",
          "score": 1,
          "created_utc": "2026-01-06 10:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4okoo",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-01 19:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4oyzx",
              "author": "fustercluck6000",
              "text": "Thanks for the reply! And you definitely have my attention haha, is there a repo I can check out? I didn‚Äôt mention this in the OP, but list of the docs are technical and everything has to be on-prem too for security reasons, so what you‚Äôre describing sounds highly relevant",
              "score": 1,
              "created_utc": "2026-01-01 19:52:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx4pqyd",
                  "author": "Impossible-Table6121",
                  "text": "I'm also interested in anything you are happy to share",
                  "score": 1,
                  "created_utc": "2026-01-01 19:56:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5uums",
          "author": "Both-Number-7319",
          "text": "So interesting \nI can see with you what approach can do",
          "score": 0,
          "created_utc": "2026-01-01 23:33:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4vzhf",
          "author": "ampancha",
          "text": "PDFs are the final boss of RAG. Standard libraries like Docling usually fail on 100+ page docs because they treat the file as a stream of text and ignore the spatial layout. If you lose the document hierarchy, your retrieval precision will never recover. I sent a DM with the specific architecture I use to handle these enterprise-scale PDFs without manual correction.",
          "score": -1,
          "created_utc": "2026-01-01 20:28:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4h062",
      "title": "Is there any comprehensive guide about RAG?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q4h062/is_there_any_comprehensive_guide_about_rag/",
      "author": "Obvious-Search-5569",
      "created_utc": "2026-01-05 09:12:33",
      "score": 26,
      "num_comments": 14,
      "upvote_ratio": 0.88,
      "text": "So a few days back, I came across a blog about RAG: [https://thinkpalm.com/blogs/what-is-retrieval-augmented-generation-rag/](https://thinkpalm.com/blogs/what-is-retrieval-augmented-generation-rag/) This blog offers a clear perspective on what RAG is, the types of RAG and the major new updates in the field. Could you please let me know if this is a good one for understanding or is there anything more that I should focus on?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q4h062/is_there_any_comprehensive_guide_about_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxstkgl",
          "author": "Fear_ltself",
          "text": "Been building my local llama setup. Had a decent RAG just dropping documents in a simple html solution that worked. Then I discovered embeddinggemma300m and once I got the embedding figured out it seems to be able to handle a lot more data submitted for retrieval. Spent a few weeks learning about embeddings and how they match the models. (So qwen uses qwen embedding, Gemma uses Gemma embedding, etc). Definitely research embedding and try to understand how it works if you don‚Äôt. I also think having a second program just for visualizing your RAG embeddings is great to have. I set it up so I can even see where the query is being connected to in latent space of the visualization for trouble shooting if a failure happened. You can see how the model query found an answer in the RAG according to its position in latent space relative to the RAG documents. Also looks like a brain when you get enough data points on various interconnected topics",
          "score": 7,
          "created_utc": "2026-01-05 11:23:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxtghsj",
              "author": "KVT_BK",
              "text": "I am thinking in the same lines and learning about embeddings. mind sharing visualization component ?",
              "score": 1,
              "created_utc": "2026-01-05 14:00:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxtgw9v",
                  "author": "Fear_ltself",
                  "text": "If you click my profile it‚Äôs my most recent post",
                  "score": 1,
                  "created_utc": "2026-01-05 14:03:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxsidnt",
          "author": "Green_Ad6024",
          "text": "https://medium.com/gopenai/rag-complete-tutorial-part-04-f89f5c076744",
          "score": 4,
          "created_utc": "2026-01-05 09:45:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxswnxg",
              "author": "Obvious-Search-5569",
              "text": "Thanks for sharing this!",
              "score": 2,
              "created_utc": "2026-01-05 11:49:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxswxht",
          "author": "joey2scoops",
          "text": "The IBM YouTube channel has some decent videos about RAG. High level stuff.",
          "score": 2,
          "created_utc": "2026-01-05 11:51:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzh95m",
              "author": "Obvious-Search-5569",
              "text": "Yes, saw their video on RAG. Good source!",
              "score": 2,
              "created_utc": "2026-01-06 10:34:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxslgdo",
          "author": "phizero2",
          "text": "Open llamaindex or langchain and apply the documentation there",
          "score": 1,
          "created_utc": "2026-01-05 10:13:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxsm12i",
          "author": "reddit-newbie-2023",
          "text": "Try https://ragyfied.com",
          "score": 1,
          "created_utc": "2026-01-05 10:18:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxt1j71",
          "author": "XertonOne",
          "text": "A RAG for what? You can use this tool for training a local weight or just create a precise dataset. https://github.com/ConardLi/easy-dataset?tab=readme-ov-file",
          "score": 1,
          "created_utc": "2026-01-05 12:25:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxx93l",
          "author": "marvindiazjr",
          "text": "No, that blog is not a good source. They don't discuss hybrid search or reranking at all.",
          "score": 1,
          "created_utc": "2026-01-06 03:11:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzh6k2",
              "author": "Obvious-Search-5569",
              "text": "Can you share anything that shares this topic?",
              "score": 1,
              "created_utc": "2026-01-06 10:34:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsfbkd",
          "author": "WiseAfternoon1554",
          "text": "I just skimmed through this blog by ThinkPalm, and I think this is a whole lot about RAG. However, if you want to know more you can definitely check out other resources as well.",
          "score": 0,
          "created_utc": "2026-01-05 09:15:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8q33s",
      "title": "Scaling RAG from MVP to 15M Legal Docs ‚Äì Cost & Stack Advice",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q8q33s/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/",
      "author": "Additional-Oven4640",
      "created_utc": "2026-01-10 01:01:05",
      "score": 26,
      "num_comments": 27,
      "upvote_ratio": 0.86,
      "text": "Hi all;\n\nWe are seeking investment for a LegalTech RAG project and need a realistic budget estimation for scaling.\n\n**The Context:**\n\n* **Target Scale:**¬†\\~15 million text files (avg. 120k chars/file). Total \\~1.8 TB raw text.\n* **Requirement:**¬†High precision. Must support¬†**continuous data updates**.\n* **MVP Status:**¬†We achieved successful results on a small scale using¬†`gemini-embedding-001`¬†**+**¬†`ChromaDB`.\n\n**Questions:**\n\n1. Moving from MVP to 15 million docs: What is a realistic OpEx range (Embedding + Storage + Inference) to present to investors?\n2. Is our MVP stack scalable/cost-efficient at this magnitude?\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q8q33s/scaling_rag_from_mvp_to_15m_legal_docs_cost_stack/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nypn7hv",
          "author": "Low-Efficiency-9756",
          "text": "I‚Äôm not sure I‚Äôm even qualified to go near this question however I‚Äôll bite. \n\nYou‚Äôre looking at roughly 450 billion tokens to embed. \ngemini-embedding-001 pricing: ~$0.00025/1k tokens\nInitial embed: ~$112,500 one-time. With 768-dim embeddings, that‚Äôs manageable storage\n\nChunking strategy: you‚Äôre probably chunking at 512-1024 tokens per chunk at 15 million docs you‚Äôre looking at 200 million chunks or so. Chroma DB will probably break at this scale. \n\nVector storage is probably gonna run $2k or more a month at that scale. Query inference maybe $5k on a low guess. \n\nContinuous updates complicate this situation heavily. \n\nOpex maybe $7k-21k or more a month. \n\nYou could try smarter filtering upstream and keep less documents hot loaded?",
          "score": 18,
          "created_utc": "2026-01-10 01:56:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyqoc7l",
              "author": "ChapterEquivalent188",
              "text": "this is why my platform is based on pure ollama and local llm. makes no sense to me processing legal docomunts throu internet",
              "score": 9,
              "created_utc": "2026-01-10 05:45:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nys0djv",
                  "author": "Straight-Gazelle-597",
                  "text": "agreed, it's a classic self-host use-case.",
                  "score": 1,
                  "created_utc": "2026-01-10 12:45:21",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyxbp8a",
                  "author": "vdharankar",
                  "text": "Even in that case OP is asking for capex , running your own LLM at that scale still incur lot of cost",
                  "score": 1,
                  "created_utc": "2026-01-11 05:36:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nypirzz",
          "author": "ggone20",
          "text": "I don‚Äôt want to be a downer here‚Ä¶ but if you‚Äôre on Reddit looking for advice to scale like this, you‚Äôve already failed and you should hire someone or a constancy that‚Äôs done this. Happy to help, for money.",
          "score": 19,
          "created_utc": "2026-01-10 01:31:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyrotyh",
              "author": "LilPsychoPanda",
              "text": "This! Because it‚Äôs a serious work and requires solid knowledge and you can‚Äôt (well you shouldn‚Äôt) whip it up with a quick ‚Äúresearch‚Äù. That is why professionals exist and get paid a lot to do the job properly ü§ì",
              "score": 3,
              "created_utc": "2026-01-10 11:10:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrrosz",
                  "author": "ggone20",
                  "text": "Indeed. It‚Äôs also not even JUST about the number of documents in this case (üôÉ) but also about required accuracy/performance. Playing with fire if not done right.",
                  "score": 2,
                  "created_utc": "2026-01-10 11:35:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyyejwt",
              "author": "SkyLordOmega",
              "text": "Bro was going to write the same thing!",
              "score": 2,
              "created_utc": "2026-01-11 11:19:56",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nysdcwj",
              "author": "kbash9",
              "text": "Yes probably best to use scalable platforms rather than building it yourself (see Contextual AI or Cohere)",
              "score": 1,
              "created_utc": "2026-01-10 14:07:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nysr1cf",
                  "author": "ggone20",
                  "text": "Unfortunately for a system of this scale there are exactly zero solutions. Everyone built the tools (AWS, Azure, GCS, etc) to build the solution; there doesn‚Äôt exist an oob solution for this problem.\n\nWhat they need is real architecture planning followed by hundreds of hours of engineering work ironing out provenance and decision traceability, among other things. Never mind evals and the ability to tweak all sorts of elements of each storage solution and the data flows between them. \n\nSounds like a fun project, but would they pay for a real solution? ü§∑üèΩ‚Äç‚ôÇÔ∏è.",
                  "score": 3,
                  "created_utc": "2026-01-10 15:23:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyrd77g",
          "author": "fabkosta",
          "text": "For a database at this scale use Elasticsearch, it‚Äòs one of the best product to scale horizontally. Use Kafka to load data from source to ES and transform (eg chunk) on the way. Consider using something like Apache Akka or similar to parallelize processing. Use a cheap mass storage as the source, like S3. Be prepared that all things break during ingestion, so use error queues, processing logs, processing timestamps that anllow you to retry only failed docs etc. Also, put financial alerts and monitors in place.",
          "score": 4,
          "created_utc": "2026-01-10 09:23:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nysevm4",
              "author": "ShardsOfHolism",
              "text": "If using S3 for storage, consider using S3 Vectors to store the embeddings and metadata. It's relatively inexpensive and scales to billions of vectors.",
              "score": 3,
              "created_utc": "2026-01-10 14:16:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nysv9i4",
                  "author": "fabkosta",
                  "text": "Oh, awesome, I did not even know about this feature yet! Thanks for pointing this out!",
                  "score": 2,
                  "created_utc": "2026-01-10 15:45:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyqpv1x",
          "author": "ChapterEquivalent188",
          "text": "The cost isn't storage ($5k/mo), the cost is¬†**liability**\n\nScaling garbage is easy  \nIf you process 15M legal docs with 'CPU-only OCR' to save money, you are building a¬†**Liability Engine**, not a Search Engine\n\nMy architecture ('The Blackbox') costs 10x more per document but I know the data in the RAG is 100% pure Quality\n\nANd to be clear if your architecture relies on an internet connection to process confidential files, you aren't building a legal platform, you are building a data leak",
          "score": 3,
          "created_utc": "2026-01-10 05:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyri0li",
              "author": "nineelevglen",
              "text": "Doesn't legora and harvey use OpenAI / Anthropic?",
              "score": 1,
              "created_utc": "2026-01-10 10:08:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyrxl6v",
                  "author": "ChapterEquivalent188",
                  "text": "Yes, they do\n\nHarvey has raised $200M+ to buy dedicated, private instances on Azure with special 'Zero Data Retention' contracts. They essentially bought their own private island within OpenAI\n\nIf you are a startup or an SME law firm using the standard Public API, you are not Harvey. You are feeding the beast",
                  "score": 1,
                  "created_utc": "2026-01-10 12:24:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyrezqi",
          "author": "cat47b",
          "text": "What scale was your POC, what frameworks, chunking strategies etc did you use? And have you evaluated other storage systems/providers? AgentSet have a good comparison - https://agentset.ai/vector-databases\n\nTurbopuffer which claims scale operation have a calculator on their homepage.",
          "score": 1,
          "created_utc": "2026-01-10 09:40:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytusy2",
          "author": "deejay217",
          "text": "Pure RAG will fail on 15 million files, you need a hybrid approach with bm25 pr something similar baked in.",
          "score": 1,
          "created_utc": "2026-01-10 18:33:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0bzyg",
      "title": "Those running RAG in production, what's your document parsing pipeline?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q0bzyg/those_running_rag_in_production_whats_your/",
      "author": "Hour-Entertainer-478",
      "created_utc": "2025-12-31 12:34:22",
      "score": 23,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "Following up on my previous post about hardware specs for RAG. Now I'm trying to nail down the document parsing side of things.\n\n**Background:**¬†I'm working on a fully self hosted RAG system.\n\nCurrently I'm using docling for parsing PDFs, docx files and images, combined with rapidocr for scanned pdfs. I have my custom chunking algorithm that chunks the parsed content in the way i want. It works pretty well for the most part, but I get the occasional hiccup with messy scanned documents or weird layouts. I just wanna make sure that I haven't made the wrong call, since there are lots of tools out there.\n\nMy use case involves handling a mix of everything really. Clean digital PDFs, scanned documents, Word files, the lot. Users upload whatever they have and expect it to just work.\n\nFor those of you running document parsing in production for your RAG systems:\n\n* What are you using for your parsing pipeline?\n* How do you handle the scanned vs native digital document split?\n* Any specific tools or combinations that have proven reliable at scale ?\n\nI've looked into things like¬†unstructured, pypdf, marker, etc but there's so many options and I'd rather hear from people who've¬†**actually**¬†battle tested these in real deployments rather than just going off benchmarks.\n\nWould be great to hear what's actually working for people in the wild.\n\nI've already looked into deepseekocr after i saw people hyping it, but it's too memory intensive for my use case and kinda slow.\n\nI understand that i'm looking for a self hosted solution, but even if you have something that works pretty well tho it's not self hosted, please feel free to share. I plan on connecting cloud apis for potential customers that wont care if its self hosted.\n\nBig thanks in advance for you help ‚ù§Ô∏è. The last post here, gave me some really good insights.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q0bzyg/those_running_rag_in_production_whats_your/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwxf8go",
          "author": "funkspiel56",
          "text": "Im using Mistral to get the job done. Its not perfect but its pretty damn cheap and quick. It fumbles on really tough handwriting as well as documents with a ton of diagrams in it but other than that it gets forms correct and roughly maintains the documents structure.",
          "score": 4,
          "created_utc": "2025-12-31 15:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxo0mp",
              "author": "dmx007",
              "text": "It's amazing until it isn't. Crazy fast. Inexpensive. Very accurate ocr. But sometimes, it drops content even on clear doc scans because of slight changes in font style or size in the middle of documents. It's hard to recover when the content is just gone at the ocr stage with post processing.",
              "score": 1,
              "created_utc": "2025-12-31 16:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzpiip",
                  "author": "funkspiel56",
                  "text": "yeah the sad part to is that I've also sent them a bit of samples and they are like yeah nothing we can do currently. Which I get but I know its possible. \n\nLlamaparse which in my experience reigns king on the leaderboard of ocr tools nails everything I feed it. Only the most obtuse items trip it up.   \n  \nIssue is llamaparse is really really pricey. Not affordable at all for a POC project where mistrals accuracy is good enough.  \n\nI tried the homegrown solutions where you run models on rtx cards but these are finicky and almost all of them require a bit of gluing together.",
                  "score": 2,
                  "created_utc": "2025-12-31 22:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwscdb",
          "author": "Snoo-85117",
          "text": "Using docstrange by Nanonets for my extraction and parsing pipeline.\n\nWorks phenomenally well.",
          "score": 2,
          "created_utc": "2025-12-31 13:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwu9z3",
              "author": "Hour-Entertainer-478",
              "text": "Thank you. Ill try that out.",
              "score": 1,
              "created_utc": "2025-12-31 13:38:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx0zy9t",
              "author": "bravelogitex",
              "text": "how's it compare to others?",
              "score": 1,
              "created_utc": "2026-01-01 03:39:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxty4r",
          "author": "ValueOk4740",
          "text": "deformable-detr-doclaynet for layout understanding + paddleocr for text extraction. Here's our library that implements these things together if you're trying to run self-hosted: [sycamore](https://sycamore.readthedocs.io/en/stable/sycamore/transforms/partition.html) but we also have a cloud api (which tends to be faster and has a better layout model that the open source one - free for the first 10k pages) [aryn docparse](https://docs.aryn.ai/docparse/introduction)\n\nThe cloud offering can handle to-pdf conversion; on your own you're gonna need headless libreoffice or something.",
          "score": 2,
          "created_utc": "2025-12-31 16:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwyrwlh",
          "author": "drfritz2",
          "text": "You may need more than one pipeline. One for text, another another kind of text, other for OCR, another for images, another for conversions. \n\nMineru , paddle OCR, docling, and many others.",
          "score": 2,
          "created_utc": "2025-12-31 19:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx65o0x",
          "author": "fustercluck6000",
          "text": "The more you build things around a particular library/tool, the harder it gets to change your mind later, so it‚Äôs really good that you‚Äôre thinking carefully about this now instead of just winging it.\n\nJust a word of caution from my recent experience with Docling‚Äîit‚Äôs a really great tool until it isn‚Äôt. I had such great luck initially using it for html, xml, docx, and other structured files that I assumed I could expect the same with PDFs if the need ever arose, then made the mistake of building a lot of my data pipeline around the DoclingDocument class. I‚Äôll just say I was very disappointed when I needed it to process large, complex PDFs. \n\nThe whole DoclingDocument ‚Äòecosystem‚Äô with Pydantic is super tempting as a general purpose solution for your project, but imho the documentation‚Äôs pretty bad (and there are quite a few different bugs, though to their credit they were very quick to push an update to one open issue I was asking about on GitHub). That becomes a major hassle when you need to tweak/tune the pipeline for your data instead of rewriting it, but can‚Äôt easily determine the scope of your options in the first place without taking the time to dig through source code. Idk I‚Äôm always left with this feeling there‚Äôs probably way more Docling can do than I‚Äôm aware of‚Äîbut I‚Äôll only find out about those things by getting lucky and reading the right Reddit post or spending more time reading through the source code and model json files to familiarize myself.\n\nI‚Äôm still torn about how much/whether to use Docling going forward because certain constructs/methods are incredibly useful, but without being confident I can quickly work through bugs in the future or knowing more conclusively how much I can scale my project with it gives me serious pause. Just make sure to keep things very modular and dependencies loosely coupled.",
          "score": 2,
          "created_utc": "2026-01-02 00:34:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7iu5f",
          "author": "ChapterEquivalent188",
          "text": "you might wanna sneak peak here https://github.com/2dogsandanerd/RAG_enterprise_core \nno ad but it might give you some idea for the future ;)",
          "score": 2,
          "created_utc": "2026-01-02 05:46:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf8t8o",
              "author": "Think-Draw6411",
              "text": "What‚Äôs the benchmark results ?",
              "score": 1,
              "created_utc": "2026-01-03 11:35:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfgqkj",
                  "author": "ChapterEquivalent188",
                  "text": "Im pretty sure you understand my platform ;) what would you measure for a benchmark ?",
                  "score": 1,
                  "created_utc": "2026-01-03 12:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx3265t",
          "author": "Hungry-Style-2158",
          "text": "I just use Wetrocloud‚Äôs end to end data extraction API.\n\nI spent a lot of time trying to create my own pipeline. That took so much effort from my team and the infrastructure was costing us financially. \n\nWe just decided to go with existing built solutions like Wetrocloud. It was our preferred choice cause we didn‚Äôt have to build and manage the infrastructure ourselves.",
          "score": 1,
          "created_utc": "2026-01-01 14:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7i3pl",
          "author": "beallg",
          "text": "MinerU 2.5",
          "score": 1,
          "created_utc": "2026-01-02 05:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgfrsm",
          "author": "franzel_ka",
          "text": "I made a combined workflow that is working well. ImageMagick to extract, Qwen3 VL model to recognise, even 2b is doing well and GPT 20b OSS for processing. I let CC write an orchestration combining the steps. The prompt for OCR is just ‚Äúconvert image to text‚Äù, GPT 20b prompt to extract invoice information must be somehow tailored to your use case. Medium reasoning is often required to get good results.\n\nFor best OCR I can recommend Chandra. Also make sure that your document really need OCR. Almost all computer-generated PDFs have perfectly extractable text embedded.\n\nHardware: MB Air M3 24 GB -> some patience required, up to 5 minutes per document",
          "score": 1,
          "created_utc": "2026-01-03 15:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwreed",
          "author": "patbhakta",
          "text": "You're looking for a silver bullet that doesn't exist.\n\n1) self hosted on limited hardware or free tier SaaS\n2) user can upload whatever and expect it to just work\n\nTrust me those are wonderful things but not achievable with what you're asking. \n\ntake a lesson from the GOAT chatGPT, a free tier limits to only a handful of documents and under a certain size too. it works ok as long as the documents are clean. Clean meaning all computer text, not scanned, no handwriting, charts, tables, pics, diagrams, etc. otherwise your results goes down severely.\n\nBest free tier IMO is Google's notebook llm, this does fine for most users.\n\nDocing is good in principle but take a hard look at the number of github issues...and ask yourself would your users be satisfied with the 2nd criteria.\n\nIf you want something production kinda and cheap you have to do it yourself along with open source projects such as docling. But your 2 requirements are out of scope, you need more constraints or pay up.",
          "score": 1,
          "created_utc": "2025-12-31 13:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwws6jw",
              "author": "Hour-Entertainer-478",
              "text": "thanks for the answer. I'm willing to upgrade the hardware. im sure you can understand that i had to ask just be sure that i'm not missing anything.   \nfor 2 i may have been rather vague with my description, the idea was to upload documents, and get a descent parsing (not perfect). \n\nDocling's been working good so far, but i was curious incase someone else has been using something that's better. \n\ni would reckon that you worked on something similar, may i ask what did you use ?",
              "score": 3,
              "created_utc": "2025-12-31 13:25:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwx1od4",
                  "author": "patbhakta",
                  "text": "Before you upgrade hardware you need a clear scope before architecture. \nWhat's your current use case and projection for 2026 and 2027, etc. If the users want everything to be on premise then good luck to you. \n\nBut odds are the staff at the company already uses chatGPT, perplexity, gemini, etc. So information is already being potentially compromised.\n\nI used docling for a bit but it just has too many issues, sure it's fine for testing, even hundreds of documents but it's not for me. I still have it running for testing also use notebookllm and chatGPT for testing (this is what your users see or do)\nIn general they're fine, most people don't even read the whole document much less understand it. So it's \"better than nothing\" which is great from a user standpoint. Problems arise when the information isn't accurate. \n\nMy use case is in fintech, so I don't need good, I need precise,  My architecture is hybrid, some stuff is in house, some stuff doesn't matter like SEC filings that are public knowledge. I use gemini, openai, openrouter, etc for various things like most people. They all have hiccups that's the nature of using LLMs. To reduce these hiccups you need quite a few things depending on your use case. \n\nI'm assuming you need an internal knowledge base that's why you're looking at rag, but knowledge bases have been around even before the internet, searching came soon after. So I'm assuming you need to incorporate LLMs for some reason because searching isn't good enough. This is where things get complicated because exact searches aren't good enough now you need semantic searches and that's where LLMs shine and that's also where things go wrong. \n\nI can go on and on and do deep dives but point is you need to scope first before you architect this...\n1) hardware / budget constraints\n2) privacy vs acceptable sharing\n3) personalized vs company-wide or both\n4) how many users and what speed is acceptable\n5) and so on...\n\nThen you can worry about the fun stuff architecture...\nIngestion, parsers, dedups, embeddings, vectordb, graphdb, tabulardb, agents, tools, orchestration, and which llm to use for what purpose and at what cost/time or and I almost forgot security and privileges...",
                  "score": 1,
                  "created_utc": "2025-12-31 14:22:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwxn4ff",
          "author": "ejpusa",
          "text": "My Parsing Pipeline Plan: GPT-5.2 writes the Python code. Nano Banana + Midjourney for infographics. VEO 3 for animations. Bare metal Linux box on Liquid Web to host it all.\n\nSTEP BY STEP. Do not go onto the next step until the first is done. My Agentic Agenct project, first Agent up this week. AI first drug discovery. \n\nSTEP 1: What is the Mission Statement\n\nThe Parkinson‚Äôs Evidence Atlas is an open, agentic AI research platform designed to continuously ingest, organize, and audit biomedical evidence at scale. By preserving raw data, exposing uncertainty and failure, and systematically surfacing neglected biological signals‚Äîespecially those overlooked for economic rather than scientific reasons‚Äîthe Atlas addresses structural blind spots in drug discovery. By treating AI as transparent research infrastructure rather than a black-box predictor, the project has a real opportunity to reshape how early-stage discoveries are identified, evaluated, and trusted, creating a new, more rigorous pathway for tackling complex neurodegenerative diseases such as Parkinson‚Äôs and Alzheimer‚Äôs.\n\nAnd so it begins.\n\n:-)\n\nThis is a family of Agents, we call them \"Harvesters.\" You can spin these up in minutes. Send them out in the wilds. Sample code:\n\n```\nimport requests\nimport psycopg2\nimport json\nfrom datetime import date\nfrom config import DATABASE_URL, DISEASE_QUERIES\nimport openai\n\nopenai.api_key = None  # set via env\n\nEUROPE_PMC_API = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n\ndef fetch_updates(query):\n    params = {\n        \"query\": query,\n        \"format\": \"json\",\n        \"pageSize\": 20,\n        \"sort\": \"P_PDATE_D\"\n    }\n    r = requests.get(EUROPE_PMC_API, params=params)\n    r.raise_for_status()\n    return r.json().get(\"resultList\", {}).get(\"result\", [])\n\ndef store_updates(conn, disease, records):\n    cur = conn.cursor()\n    for r in records:\n        cur.execute(\"\"\"\n            INSERT INTO research_updates\n            (source, disease, title, abstract, url, published_date, raw_json)\n            VALUES (%s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT DO NOTHING\n        \"\"\", (\n            \"EuropePMC\",\n            disease,\n            r.get(\"title\"),\n            r.get(\"abstractText\"),\n            r.get(\"fullTextUrlList\", {}).get(\"fullTextUrl\", [{}])[0].get(\"url\"),\n            r.get(\"firstPublicationDate\"),\n            json.dumps(r)\n        ))\n    conn.commit()\n\ndef generate_daily_summary(conn):\n    cur = conn.cursor()\n    cur.execute(\"\"\"\n        SELECT title, abstract\n        FROM research_updates\n        WHERE fetched_at::date = CURRENT_DATE\n    \"\"\")\n    rows = cur.fetchall()\n\n    if not rows:\n        return None\n\n    text_block = \"\\n\\n\".join(\n        f\"Title: {t}\\nAbstract: {a}\" for t, a in rows if a\n    )\n\n    prompt = f\"\"\"\nSummarize the following Parkinson‚Äôs and Alzheimer‚Äôs research updates.\nBe descriptive, not evaluative.\nDo not speculate on efficacy or importance.\nGroup by general topic if possible.\n\n{text_block}\n\"\"\"\n\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.2\n    )\n\n    summary = response.choices[0].message.content\n\n    cur.execute(\"\"\"\n        INSERT INTO daily_summaries (summary_date, summary_text)\n        VALUES (%s, %s)\n        ON CONFLICT (summary_date) DO NOTHING\n    \"\"\", (date.today(), summary))\n    conn.commit()\n\ndef run():\n    conn = psycopg2.connect(DATABASE_URL)\n\n    for disease, query in DISEASE_QUERIES.items():\n        records = fetch_updates(query)\n        store_updates(conn, disease, records)\n\n    generate_daily_summary(conn)\n    conn.close()\n\nif __name__ == \"__main__\":\n    run()\n```",
          "score": -1,
          "created_utc": "2025-12-31 16:14:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q02chq",
      "title": "Looking for someone to collaborate on an ML + RAG + Agentic LLM side project",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q02chq/looking_for_someone_to_collaborate_on_an_ml_rag/",
      "author": "Far-Palpitation4482",
      "created_utc": "2025-12-31 03:17:47",
      "score": 20,
      "num_comments": 24,
      "upvote_ratio": 0.92,
      "text": "Hey! Is anyone here interested in building a side project together involving RAG + LLMs (agentic workflows) + ML?\n\nI‚Äôm not looking for anything commercial right now, just learning + building with someone who‚Äôs serious and consistent.\nIf interested, drop a comment or DM,happy to discuss ideas and skill sets",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q02chq/looking_for_someone_to_collaborate_on_an_ml_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwwda59",
          "author": "lundrog",
          "text": "Discord server for everyone ? Im interested in the concept.",
          "score": 3,
          "created_utc": "2025-12-31 11:31:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwurfr3",
          "author": "liy8",
          "text": "Yep, I'm interested.",
          "score": 2,
          "created_utc": "2025-12-31 03:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuu89g",
          "author": "Nivedh2004",
          "text": "Interested",
          "score": 2,
          "created_utc": "2025-12-31 03:50:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuw999",
          "author": "Maleficent_Repair359",
          "text": "interested",
          "score": 2,
          "created_utc": "2025-12-31 04:03:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuwa8h",
          "author": "remoteinspace",
          "text": "Up for contributing to open source?",
          "score": 2,
          "created_utc": "2025-12-31 04:04:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv988i",
          "author": "rkpandey20",
          "text": "I am game.¬†",
          "score": 2,
          "created_utc": "2025-12-31 05:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpdck",
          "author": "Powerful-Teacher-188",
          "text": "Yup, interested",
          "score": 2,
          "created_utc": "2025-12-31 07:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvu70p",
          "author": "Ok-Development-9420",
          "text": "Let‚Äôs go!",
          "score": 2,
          "created_utc": "2025-12-31 08:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww7faf",
          "author": "glitch1080",
          "text": "interested",
          "score": 2,
          "created_utc": "2025-12-31 10:37:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwnhe2",
          "author": "Critical-Set1190",
          "text": "Instrested right now building a legaltech product that includes agentic rag",
          "score": 2,
          "created_utc": "2025-12-31 12:53:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwri3n",
          "author": "Ai_dl_folks",
          "text": "I'm interested",
          "score": 2,
          "created_utc": "2025-12-31 13:20:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxncks",
          "author": "Radio-Time",
          "text": "need an idea and teammates",
          "score": 2,
          "created_utc": "2025-12-31 16:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysuxx",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2025-12-31 19:44:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxuv2h",
          "author": "Tricky_Progress_3606",
          "text": "Hey man im interesterd currently building aswell",
          "score": 2,
          "created_utc": "2025-12-31 16:52:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysw8n",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2025-12-31 19:44:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ss5i",
          "author": "kiran-187",
          "text": "Yes I'm interested",
          "score": 1,
          "created_utc": "2026-01-01 17:11:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4ho10",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2026-01-01 19:16:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx96khz",
          "author": "legaltextai",
          "text": "i'd be interested if it involves legal area",
          "score": 1,
          "created_utc": "2026-01-02 14:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9b4j6",
          "author": "Altruistic_Leek6283",
          "text": " I'm down.",
          "score": 1,
          "created_utc": "2026-01-02 14:32:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcbrd8",
          "author": "SubtlyOnTheNose",
          "text": "DOWN, lets get a discord going ya?",
          "score": 1,
          "created_utc": "2026-01-02 23:20:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxld704",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2026-01-04 08:19:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxhcbro",
          "author": "hlama26",
          "text": "Why not to use the fine tuning concept instead of ML ?",
          "score": 1,
          "created_utc": "2026-01-03 18:28:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxld8wa",
              "author": "Far-Palpitation4482",
              "text": "Yeah we can ,never thought tho ..DM me",
              "score": 1,
              "created_utc": "2026-01-04 08:19:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxjex0k",
          "author": "distspace",
          "text": "Dm me I‚Äôm current working on one right now\n\nI don‚Äôt mind sharing some ideas",
          "score": 1,
          "created_utc": "2026-01-04 00:37:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qafa53",
      "title": "Best practices for running a CPU-only RAG chatbot in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qafa53/best_practices_for_running_a_cpuonly_rag_chatbot/",
      "author": "Acceptable_Young_167",
      "created_utc": "2026-01-11 23:55:37",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "Hi r/LocalLLaMA üëã\n\nMy company is planning to deploy a **production RAG-based chatbot that must run entirely on CPU** (no GPUs available in deployment). I‚Äôm looking for **general guidance and best practices** from people who‚Äôve done this in real-world setups.\n\n# What we‚Äôre trying to solve\n\n* Question-answering chatbot over internal documents\n* Retrieval-Augmented Generation (RAG) pipeline\n* Focus on **reliability, grounded answers, and reasonable latency**\n\n\n\n# Key questions\n\n**1Ô∏è‚É£ LLM inference on CPU**\n\n* What size range tends to be the sweet spot for CPU-only inference?\n* Is aggressive quantization (int8 / int4) generally enough for production use?\n* Any tips to balance latency vs answer quality?\n\n**2Ô∏è‚É£ Embeddings for retrieval**\n\n* What characteristics matter most for CPU-based semantic search?\n   * Model size vs embedding dimension\n   * Throughput vs recall\n* Any advice on multilingual setups (English + another language)?\n\n**3Ô∏è‚É£ Reranking on CPU**\n\n* In practice, is cross-encoder reranking worth the extra latency on CPU?\n* Do people prefer:\n   * Strong embeddings + higher `top_k`, or\n   * Lightweight reranking with small candidate sets?\n\n**4Ô∏è‚É£ System-level optimizations**\n\n* Chunk sizes and overlap that work well on CPU\n* Caching strategies (embeddings, reranker outputs, answers)\n* Threading / batch size tricks for Transformers on CPU\n\n\n\n# Constraints\n\n* CPU-only deployment (cloud VM)\n* Python + Hugging Face stack\n* Latency matters, but correctness matters more than speed\n\nWould love to hear **real deployment stories, lessons learned, or pitfalls to avoid**.  \nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qafa53/best_practices_for_running_a_cpuonly_rag_chatbot/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz2jqca",
          "author": "Altruistic_Leek6283",
          "text": "First:  Decide what are you want to delivery.   \nIf you don't mind latency go with the LLM with CPU. God have mercy in your soul. \n\nPlease, understand that the stack you will use, is defined by the data, not the hardware. Never the hardware, if you have issue with hardware you need to upgrade it. \n\nAll AI Systems you need to think first in the product, the architecture comes second, the third is the data. You need to see the corpus to understand the first stack and guess what? Will change. You will change the stack, because you don't know how the data will behave with the chuck and embedding process. \n\nDeploy a MVP of your system, and update here.",
          "score": 5,
          "created_utc": "2026-01-12 00:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8fb2c",
          "author": "Ok_Pomelo_5761",
          "text": "CPU only RAG can work if you keep the pipeline tight.\n\nLLM on CPU: pick a small model (3B to 8B) and quantize hard (int4). llama.cpp or GGUF usually gives better latency than raw HF on CPU.\n\nEmbeddings: use a small embedder, precompute everything, and keep top\\_k modest.\n\nReranking on CPU: yes it is worth it if you rerank a small set. Grab top 30 to 50 from embeddings, then rerank down to 5 to 8. This is where I would plug zeroentropy reranker, it is a clean drop in to boost precision without needing GPU if you keep candidates low.\n\nOps stuff: cache query embeddings + reranker results, keep chunks around 300 to 800 tokens with a bit of overlap, and force the bot to say I dont know when sources do not support the answer.",
          "score": 3,
          "created_utc": "2026-01-12 21:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz37cv7",
          "author": "raiffuvar",
          "text": "Metrics. Everything is solved by metrics. \nI've tried reranker on game specific slang and quality went down. \nSo, just try and search for your performance.",
          "score": 1,
          "created_utc": "2026-01-12 02:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3esia",
          "author": "Ok_Mirror7112",
          "text": "With your current requirements use pymupdf4llm for parsing. \n\nEmbeddings you will have to check which one has free tier.\n\nQuantisation works only if use over fetch 3-5x with RRF\n\nTop-k = 8-12 or 20\n\nChunk size 520 or 1028 depending on your goal.",
          "score": 1,
          "created_utc": "2026-01-12 02:54:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3g3z6",
          "author": "hrishikamath",
          "text": "I run completely on CPU, my embedding is ~300 dimension and latency is fine. I even use re ranker in prod on cpu. No quantization nothing. I think you are asking this question too early. First make a setup, benchmark it and then think of making it production ready.",
          "score": 1,
          "created_utc": "2026-01-12 03:01:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3zq4w",
          "author": "Giedi-Prime",
          "text": "need to something similar for our company, want to learn more, can anyone recommend a good starting point?",
          "score": 1,
          "created_utc": "2026-01-12 04:56:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4ytj1",
          "author": "tony10000",
          "text": "4B-8B models.  Look at AnythingLLM coupled with LM Studio or Ollama as the LLM server.",
          "score": 1,
          "created_utc": "2026-01-12 10:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz539zj",
          "author": "ConcertTechnical25",
          "text": "Since you are on a Python + HF stack, look into Intel Extension for PyTorch (IPEX) or OpenVINO‚Äîthe speedup on CPU is non-trivial compared to vanilla Transformers.\nThe real performance killer in CPU-RAG isn't the inference, it's the context bloat. If your chunks are too large, the self-attention mechanism will eat your CPU cycles for breakfast. Instead of long chunks, try \"Small-to-Big\" retrieval: index small 256-token chunks for better recall, but only feed the parent context to the LLM. This keeps your KV-cache small and your CPU latency predictable.",
          "score": 1,
          "created_utc": "2026-01-12 10:43:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5glzk",
          "author": "Rokpiy",
          "text": "the reranking tradeoff seems like the key question here. if correctness matters more than speed you probably want it, but cross-encoder on CPU adds up fast\n\nbetter embeddings + higher top\\_k might be the move? avoids the extra model call entirely. also curious what your retrieval recall looks like without reranking - might not even need it depending on your data",
          "score": 1,
          "created_utc": "2026-01-12 12:30:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6cmpb",
          "author": "Fun-Purple-7737",
          "text": "Best practices for running a CPU-only RAG chatbot:\n\ndon't",
          "score": 1,
          "created_utc": "2026-01-12 15:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6zpo3",
          "author": "OnyxProyectoUno",
          "text": "The chunking and caching side is where you'll probably get the biggest wins on CPU.\n\nFor chunking, smaller is usually better on CPU since you're already latency-constrained. I'd start around 256-512 tokens with 50-100 token overlap. Larger chunks mean more tokens to process during reranking, which hurts when you can't parallelize well. The tradeoff is you might need higher top_k to catch relevant info spread across multiple small chunks.\n\nAggressive embedding caching is crucial. Cache at the chunk level, not just query level. If your docs don't change much, precompute all chunk embeddings and store them. For queries, implement semantic similarity caching so similar questions hit cached results. Even fuzzy matching on query embeddings can save you inference cycles.\n\nOn the reranking question, cross-encoders are usually worth it but keep the candidate set small. Retrieve maybe 20-30 chunks, rerank to 5-8. The quality jump is significant enough to justify the latency hit, especially when correctness matters more than speed.\n\nOne gotcha: watch your memory usage with quantized models. int4 can be unstable under load, and int8 is often the better production choice even if it's slightly slower. Also, if you're doing multilingual, make sure your chunking strategy doesn't break on non-English text boundaries.\n\nWhat's your target response time looking like? That'll change the chunking math quite a bit.",
          "score": 1,
          "created_utc": "2026-01-12 17:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz77ylt",
          "author": "vinoonovino26",
          "text": "Try hyperlink.ai . It has done miracles for my rag setup",
          "score": 1,
          "created_utc": "2026-01-12 17:54:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze2kkp",
          "author": "Whole-Assignment6240",
          "text": "in most of the cases, in production, if you don't have enough resources and wants decent performance, Gemini embedding is pretty cost effective. a lot of our users use it.",
          "score": 1,
          "created_utc": "2026-01-13 18:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4qdjz",
          "author": "Both-Number-7319",
          "text": "Up",
          "score": 1,
          "created_utc": "2026-01-12 08:40:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzuerp",
      "title": "Lessons learned from building hybrid search in production (Weaviate, Qdrant, Postgres + pgvector) [OC]",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzuerp/lessons_learned_from_building_hybrid_search_in/",
      "author": "ElBargainout",
      "created_utc": "2025-12-30 21:32:09",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "After shipping hybrid search into multiple production systems (RAG/chatbots, product search, and support search) over the last 18 months, here's a practical playbook of what actually mattered. Full disclosure: we build retrieval/RAG systems for customers, so these are lessons we learned on real traffic, not toy benchmarks.\n\n**Why hybrid search**\n\nVector search finds semantics but misses exact matches (SKUs, IDs, proper nouns). BM25/TF-IDF finds exact tokens but misses paraphrases. Hybrid = pragmatic: combine both and tune for your user needs.\n\n**Quick decision flow (how to pick an approach)**\n\n\\- Need fastest time-to-market + minimal ops? Try a vector DB with built-in hybrid (Weaviate-style) if it fits your scale.  \n\\- Need tight control over scoring, advanced reranking, or best-effort accuracy? Use vector DB (Qdrant/FAISS) + a separate BM25 engine (Postgres full-text or Elasticsearch) and fuse results.  \n\\- Need transactional consistency, joins, or want a single source of truth for metadata and embeddings? Use Postgres + pgvector.\n\n**Patterns & code snippets**  \n  \nBuilt-in hybrid (example: Weaviate-style)  \nPros: simple API, single service, alpha knob for weighting. Cons: less control, black-box internals, possible limits on scale/tuning.\n\nPython pseudo-example:\n\n\\`\\`\\`python  \n\\# high-level example (client API varies by vendor)  \nresults = client.query.get(\"Document\", \\[\"content\"\\]) \\\\  \n.with\\_hybrid(query=\"how to cancel subscription\", alpha=0.7) \\\\  \n.with\\_limit(10) \\\\  \n.do()  \n\\`\\`\\`text\n\nTuning knobs: alpha (0..1), limit, semantic model version, chunking strategy.\n\nWhen to pick: small team, want fewer moving parts, need quick prototype, acceptable to trade some control for speed.\n\n\\---\n\n2) Multi-engine: Qdrant (vectors) + BM25 (Postgres/Elasticsearch)\n\nPattern A: Fuse scores from two full-retrievals  \n\\- Vector DB: get top-N semantic candidates  \n\\- BM25: get top-N lexical candidates  \n\\- Normalize scores and combine (alpha weighting or RRF)\n\nPattern B: Two-stage rerank (fast, often better tail quality)  \n\\- Stage 1: vector search to get \\~100 candidates  \n\\- Stage 2: BM25 (or cross-encoder) reranks those candidates\n\nExample normalization + fusion (Python sketch):\n\n\\`\\`\\`python  \n\\# vector\\_results = \\[{'id':id, 'score':v\\_score}, ...\\]  \n\\# bm25\\_scores = {doc\\_id: raw\\_score}\n\ndef normalize(scores):  \nvals = list(scores.values())  \nmx, mn = max(vals), min(vals)  \nif mx == mn: return {k: 1.0 for k in scores}  \nreturn {k: (v - mn) / (mx - mn) for k, v in scores.items()}\n\nvec = {r\\['id'\\]: r\\['score'\\] for r in vector\\_results}  \nvec\\_n = normalize(vec)  \nbm25\\_n = normalize(bm25\\_scores)\n\nalpha = 0.7  \ncombined = {}  \nfor doc in set(vec\\_n) | set(bm25\\_n):  \ncombined\\[doc\\] = alpha \\* vec\\_n.get(doc, 0) + (1 - alpha) \\* bm25\\_n.get(doc, 0)\n\nranked = sorted(combined.items(), key=lambda x: x\\[1\\], reverse=True)  \n\\`\\`\\`text\n\nTrade-offs: more infra and operational complexity, but more control over scoring, reranking, and caching. Two-stage rerank gives best cost/quality trade-off in many cases.\n\n\\---\n\n3) Postgres + pgvector (single-system hybrid)\n\nWhy choose this: transactional writes, rich joins (user/profile metadata), ability to keep embeddings in the same DB as your authoritative rows.\n\nExample schema and query (Postgres 14+ with pgvector extension):\n\n\\`\\`\\`sql  \n\\-- table: documents(id serial, content text, embedding vector(1536), ts tsvector)  \n\\-- create index on vector  \nCREATE INDEX ON documents USING ivfflat (embedding vector\\_cosine\\_ops) WITH (lists = 100);  \n\\-- create full-text index  \nCREATE INDEX documents\\_ts\\_idx ON documents USING GIN (ts);\n\n\\-- hybrid query: weight vector distance and text rank  \nSELECT id, content,  \n(1 - (embedding <#> :query\\_embedding)) AS vec\\_sim,    -- cosine distance -> similarity  \nts\\_rank\\_cd(ts, plainto\\_tsquery(:q)) AS ft\\_rank,  \n0.7 \\* (1 - (embedding <#> :query\\_embedding)) + 0.3 \\* ts\\_rank\\_cd(ts, plainto\\_tsquery(:q)) AS hybrid\\_score  \nFROM documents  \nWHERE ts @@ plainto\\_tsquery(:q)  \nORDER BY hybrid\\_score DESC  \nLIMIT 20;  \n\\`\\`\\`text\n\nNotes:  \n\\- pgvector uses \\`<->\\` or \\`<#>\\` operators depending on metric; check your version.  \n\\- You can include rows that don't match ts query by using LEFT JOIN or removing the WHERE clause and controlling ft\\_rank nulls.  \n\\- Use \\`lists\\` (IVF lists) and \\`probes\\` tuning for ivfflat; \\`lists\\` affects index size and build time.\n\nTrade-offs: single system simplicity and ACID guarantees vs scaling limits (need to shard or read-replicate for very high throughput). Maintenance (VACUUM, ANALYZE) matters.\n\n**Tuning knobs that actually moved metrics for us**\n\n\\- Chunking: semantic-aware chunks (paragraph boundaries) beat fixed token windows for recall.  \n\\- Alpha (vector vs BM25): tune per use-case. FAQ/support: favor vector (\\~0.7). SKU/product-id exact-match: lower alpha.  \n\\- Candidate set size for rerank: retrieving 100-500 candidates and reranking often beats smaller sets.  \n\\- Normalization: min-max per-query or rank-based fusion (RRF) is safer than naive raw-score mixing.  \n\\- Embedding model: pick one and be consistent; differences matter less than chunking and reranking.  \n\\- Index params: nlist/nprobe (FAISS), ef/search\\_k (HNSW), \\`lists\\`/\\`probes\\` (pgvector ivfflat) ‚Äî tune for your latency/recall curve.\n\n**Evaluation checklist (offline + online)**\n\nOffline:  \n\\- recall@k (k = 10, 50)  \n\\- MRR (mean reciprocal rank)  \n\\- Precision@k if relevance labels exist  \n\\- Diversity / redundancy checks\n\nOnline:  \n\\- latency p50/p95 (target SLA)  \n\\- synthetic query coverage (tokenized vs paraphrase)  \n\\- task success (e.g., issue resolved, product clicked)  \n\\- cost per Q (compute + index storage)\n\nExperimentation:  \n\\- A/B test different alphas and reranker thresholds  \n\\- Log and sample failures for manual review\n\n**Ops & deployment tips**\n\n\\- Version embeddings: store model name + version to allow reindexing safely.  \n\\- Incremental reindex: prefer small, transactional updates rather than bulk rebuilds where possible.  \n\\- Cache hot queries and pre-warm frequently used embeddings.  \n\\- Monitor drift: embeddings/models change over time; schedule periodic re-evaluation.  \n\\- Fallbacks: if vector DB fails, use BM25-only fallback instead of returning an error.  \n\\- Attribution: always include source IDs/snippets in generated responses to avoid hallucination.\n\n**Common failure modes**\n\n\\- Mixing raw scores without normalization ‚Äî one engine dominates.  \n\\- Using too-small candidate set and missing correct docs.  \n\\- Not accounting for metadata (date, user region) in ranking ‚Äî causes irrelevant hits.  \n\\- Treating hybrid as a silver bullet: some queries need exact filters before retrieval (e.g., rate-limited or region-restricted docs).\n\n\\---\n\n**Example test queries to validate hybrid behavior**\n\n\\- \"how to cancel subscription\"  \n\\- \"SKU-12345 warranty\"  \n\\- \"refund policy for order 9876\"  \n\\- \"best GPU for training transformer models\"\n\nFor each query, inspect: top-10 results, source IDs, whether exact-match tokens rank up, and whether paraphrase matches appear.\n\n\\---\n\n**TL;DR**\n\n\\- Hybrid = vectors + lexical. Pick the approach based on control vs speed-to-market vs transactional needs.  \n\\- Weaviate-style built-in hybrid is fastest to ship; multi-engine (Qdrant + BM25) gives most control and best quality with reranking; Postgres+pgvector gives transactional simplicity and joins.  \n\\- Chunking, candidate set size, and normalization/reranking matter more than small differences in embedding models.  \n\\- Always evaluate with recall@k, MRR, and online KPIs; version embeddings and plan for incremental reindexing.\n\nI'd love to hear how others fuse scores in production: do you prefer normalization, rank fusion (RRF), or two-stage rerank? What failure modes surprised you?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1pzuerp/lessons_learned_from_building_hybrid_search_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx231el",
          "author": "getarbiter",
          "text": "Solid playbook. One thing we've been experimenting with that complements hybrid retrieval: adding a coherence scoring stage after fusion.\n\nThe problem with alpha tuning is you're still combining two \"how close is this?\" signals. Neither actually answers \"does this candidate resolve the query under its constraints?\"\n\nExample: \"refund policy for order 9876\" ‚Äî hybrid might rank a general refund FAQ highly (good semantic match, maybe BM25 hit on \"refund\"). But it doesn't satisfy the constraint field (specific order). A coherence check catches that.\n\nWe run it as a lightweight post-retrieval filter: retrieve 50-100 candidates with hybrid ‚Üí score coherence against query constraints ‚Üí pass top-k to generation.\n\nThe nice part is it's orthogonal to your retrieval stack. Works with Weaviate, Qdrant, pgvector, whatever. 26MB engine, deterministic, runs locally.\n\nCurious if you've experimented with anything similar ‚Äî rerankers partially address this but they're still similarity-based.",
          "score": 1,
          "created_utc": "2026-01-01 09:32:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww6ojl",
          "author": "ElBargainout",
          "text": "To kick things off, I'm really curious about everyone's experience with RRF (Reciprocal Rank Fusion) vs. Score Normalization.\n\nWe settled on normalization because it felt more predictable for our specific dataset, but I hear a lot of praise for RRF being more robust out-of-the-box. Has anyone here switched from one to the other and noticed a significant jump in MRR?",
          "score": 1,
          "created_utc": "2025-12-31 10:31:01",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwtm3p6",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -1,
          "created_utc": "2025-12-30 23:36:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6rq9",
              "author": "ElBargainout",
              "text": "This is a fantastic addition. The point about checksumming reference embeddings to catch silent API updates is something I hadn‚Äôt implemented, but I‚Äôm definitely stealing that idea. It‚Äôs a silent killer for long-running systems.\n\nRegarding temporal relevance, it‚Äôs tricky because you don't want recency to override semantic relevance entirely (e.g., a relevant doc from 2022 vs. an irrelevant one from today).\n\nHere is how we usually handle it in production:\n\n\\- Decay Functions (The \"Soft\" Way): In systems like Elasticsearch or Weaviate, we apply a Gaussian decay function based on the timestamp. It acts as a multiplier on the hybrid score. The key is tuning the scale so the decay isn't too aggressive. Usually, we just want to break ties or give a slight edge to newer content, not bury older foundational docs.\n\n\\- Hard Filters (The \"Hard\" Way): For things like news or urgent support issues, we sometimes enforce a hard filter (e.g., date > now - 1 year) before the vector search, but this hurts recall for \"legacy\" problems.\n\n\\- Reranking Signal: If we use a cross-encoder (Stage 2), we sometimes inject the date into the text passed to the reranker (e.g., \\[Date: 2024-01-01\\] Content...) so the model explicitly sees the freshness, though this relies heavily on the model's training.\n\nDo you use the decay approach, or have you found a way to incorporate recency directly into the RRF logic?",
              "score": 1,
              "created_utc": "2025-12-31 10:31:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q4ahf9",
      "title": "Do we need LangChain?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q4ahf9/do_we_need_langchain/",
      "author": "Dear-Enthusiasm-9766",
      "created_utc": "2026-01-05 03:18:07",
      "score": 17,
      "num_comments": 36,
      "upvote_ratio": 0.85,
      "text": "Yesterday, I created a RAG project using Python without LangChain. So why do we even need LangChain? Is it just hype?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q4ahf9/do_we_need_langchain/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxr5fzs",
          "author": "Ok-Pause6148",
          "text": "In my opinion it's a wildly overbloated hunk of junk that was put out asap in order to facilitate development capture. I don't use it either",
          "score": 24,
          "created_utc": "2026-01-05 03:29:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxr7kzl",
              "author": "bravelogitex",
              "text": "they got so much vc money and still failed...",
              "score": 6,
              "created_utc": "2026-01-05 03:41:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxv27ix",
              "author": "abazabaaaa",
              "text": "This. It‚Äôs a heap of leaky abstraction. \n\nJust use the LLM api.",
              "score": 3,
              "created_utc": "2026-01-05 18:38:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxs0zl8",
          "author": "Challseus",
          "text": "It all depends on the scale and type of software you‚Äôre creating. If you‚Äôre building a RAG SaaS, and you want to support qdrant, pgvector, chromadb, and pinecone, and simultaneously support N number of file loaders, that‚Äôs where Langchain shines, as it gives you one interface for the vector stores and loaders/document.\n\nRight tool for the job and all ü§∑üèæ‚Äç‚ôÇÔ∏è",
          "score": 11,
          "created_utc": "2026-01-05 07:02:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxux5mu",
              "author": "UseMoreBandwith",
              "text": "wth would you use 4 different vector databases?",
              "score": 2,
              "created_utc": "2026-01-05 18:15:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxuzeyj",
                  "author": "Challseus",
                  "text": "If you're making a RAG product, sometimes you want to give the customer the option of what vector database to use. Hell, maybe for \"you\", you want to defer to `chromadb` in development, `pinecone` in production.\n\nMaybe you want to support someone coming from another system, who had all their shit in `pgvector`?\n\nAnother thing that happens with me a lot is that I will switch vector databases sometimes to test out certain functionality, it's much easier to quickly do that when you it's all under the same interface, and usually a configuration change more than anything.\n\nThat's where Langchain / Llama Index come in handy.\n\nTL;DR Useful when creating RAG frameworks and platforms.",
                  "score": 2,
                  "created_utc": "2026-01-05 18:25:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxxblwu",
              "author": "laurentbourrelly",
              "text": "Agreed\n\nThere is a place for Langchain.\n\nIt's not my favorite personal choice, but what matters is the job.",
              "score": 1,
              "created_utc": "2026-01-06 01:13:12",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxrbpyz",
          "author": "nangu22",
          "text": "You don't even need python nowadays, let alone langchain, so yes, you're right.",
          "score": 6,
          "created_utc": "2026-01-05 04:05:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxv8ucf",
              "author": "ShellofaHasBeen",
              "text": "You don't even need a computer. It's all done by the clouds you see up in the sky.",
              "score": 5,
              "created_utc": "2026-01-05 19:07:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxved2q",
                  "author": "nangu22",
                  "text": "You're right, but there are people out there who don't want to store data in the cloud, so not an option really.",
                  "score": 1,
                  "created_utc": "2026-01-05 19:33:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxrlnoh",
          "author": "halationfox",
          "text": "RAG is basically\n\n1. Take your corpus and try to find \"hits\" related to the user prompt\n2. Prepend the user prompt with the hits before passing the result to the server, so that the LLM uses the retrieved information rather than its own \"expertise\" and \"lived experiences\"\n\nSo the RA- part can be, in principle, whatever you want. A third of the time, particularly when the corpus is well organized and has lots of \"proper nouns/terms of art\", I just use some regex and skip modern RAG-industrial complex, like langchain and chromadb and bm25.",
          "score": 3,
          "created_utc": "2026-01-05 05:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs5ndm",
          "author": "Upset-Pop1136",
          "text": "Your first RAG demo works. Then a customer asks why an answer was wrong last Tuesday, and you can‚Äôt reproduce it.\n\nLangChain can help with tracing and evals (via LangSmith), but it can also hide prompts and retrieved chunks if you‚Äôre not careful.  \nPlain Python gives you full visibility, but then you have to build tracing, evals, and versioning yourself.\n\nEither way, debugging and reproducibility are the hard parts.",
          "score": 3,
          "created_utc": "2026-01-05 07:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs73mm",
          "author": "csharp-agent",
          "text": "no",
          "score": 2,
          "created_utc": "2026-01-05 07:57:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycalfd",
          "author": "badgerbadgerbadgerWI",
          "text": "Honestly? For most RAG projects, no. It adds abstraction that makes debugging harder. Native Python + your vector DB client is usually cleaner. LangChain shines when you need to rapidly prototype across multiple providers, but for prod systems the extra layer often isn't worth it.",
          "score": 2,
          "created_utc": "2026-01-08 04:34:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs2zkc",
          "author": "IdeaAffectionate945",
          "text": "LangChain is Python. Python doesn't scale for anything beyond *\"5 concurrent users\"*. Since your alternative is manually written Python though, you're kind of screwed anyways - But LangChain is fundamentally broken for the above reasons.\n\nMy own stuff is 19 times faster (and more scalable) than FastAPI for instance. My stuff is C# ...",
          "score": 5,
          "created_utc": "2026-01-05 07:20:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxstpg0",
              "author": "LogSlow1623",
              "text": "I gonna do it in rust",
              "score": 3,
              "created_utc": "2026-01-05 11:25:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxt12gz",
                  "author": "IdeaAffectionate945",
                  "text": "Well, just don't do it in Python. I just saw a friend of me measuring C# with SIMD versus Python. 380 times faster. Without SIMD it was 80 times faster ...",
                  "score": 3,
                  "created_utc": "2026-01-05 12:22:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nxuz1hq",
              "author": "UseMoreBandwith",
              "text": "that is nonsense.  \none can write slow code in any language.  \nand under the hood Python is usually highly optimized C (pandas, numpy), rust (polars, tokenizers) or something else.",
              "score": 4,
              "created_utc": "2026-01-05 18:24:00",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxzk9br",
                  "author": "IdeaAffectionate945",
                  "text": "Python's CLI prevents multiple threads from executing code at the same time. Python is broken. Google tried to fix it for 20 years, but had to give up years ago. This is why they created GoLang.\n\nPsst ==> [https://ainiro.io/blog/hyperlambda-is-20-times-faster-than-fast-api-and-python](https://ainiro.io/blog/hyperlambda-is-20-times-faster-than-fast-api-and-python)\n\nThe above is Fast API, but I've got tons of similar comparisons. \"Vedran B\" measured Python versus C# with SIMD instructions. C# was 380 times faster for calculating PI. That's not \"optimising\", that's the difference between **\\*fundamentally broken software\\*** and working software ...\n\nIf you use Python for anything else than as a *\"bash script alternative\"*, you're not creating software, you're creating junkware ...",
                  "score": 0,
                  "created_utc": "2026-01-06 11:01:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxrrqfp",
          "author": "lavangamm",
          "text": "It's just dependent on the usecase...like if you build arag ai agent it would be better if you have some framework for tools and the stuff so you don't need to rewrite everything by your own....langchain is just one of its framework thoo",
          "score": 1,
          "created_utc": "2026-01-05 05:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxtu2bf",
          "author": "Fit-Presentation-591",
          "text": "Langchain is great for doing some PoC work and maybe even what i‚Äôd call ‚Äúlight production‚Äù but pretty much everyone I know quickly pivots into ‚Äúrolling their own‚Äù interfaces quickly for better stability support and reliability.",
          "score": 1,
          "created_utc": "2026-01-05 15:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxudzzk",
          "author": "ninadpathak",
          "text": "LangChain works for certain use cases but you're asking the right question. In 2-3 years AI frameworks will handle a lot of this orchestration automatically. Right now you have to choose between flexibility (custom code) and convenience (LangChain). As AI becomes commodity, the frameworks that matter will be the ones designed from day one to work WITH AI agents, not against them.",
          "score": 1,
          "created_utc": "2026-01-05 16:47:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxwjqe1",
          "author": "Lanky-Cobbler-3349",
          "text": "No",
          "score": 1,
          "created_utc": "2026-01-05 22:47:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxyoja",
          "author": "New_Advance5606",
          "text": "Apparently, they are combining all maths into a standard model.¬† Like Russel.¬† Or physicist.¬† I think this one will taste like skittles.¬†¬†",
          "score": 1,
          "created_utc": "2026-01-06 03:19:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4617v",
          "author": "jackshec",
          "text": "nope",
          "score": 1,
          "created_utc": "2026-01-07 01:04:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5jypo",
          "author": "Free-Internet1981",
          "text": "It's junk",
          "score": 1,
          "created_utc": "2026-01-07 06:03:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyimsqa",
          "author": "BusinessMindedAI",
          "text": "LangChain is not required; RAG works perfectly in plain Python.\nIt exists to reduce boilerplate and help people prototype faster.\nIf you understand the pipeline, LangChain adds convenience, not capability.",
          "score": 1,
          "created_utc": "2026-01-09 01:52:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyj9g1g",
          "author": "astro_abhi",
          "text": "You don‚Äôt need LangChain to build RAG.\n\nFor many setups, a few well-written functions around ingestion, embeddings, retrieval, and generation are enough.\n\nThe harder part shows up over time, not at the beginning. As the pipeline evolves, questions start popping up:\n* how ingestion and chunking decisions affect retrieval\n* how to experiment with retrieval strategies without rewriting everything\n* how to see what‚Äôs actually happening when results get worse\n* how to change models or vector DBs safely\n\nFrameworks like LangChain try to help with this, but they can feel heavy if abstractions hide too much. \nThat's why I built [VectraSDK](https://vectra.thenxtgenagents.com) - Open Source Provider Agnostic RAG SDK.",
          "score": 1,
          "created_utc": "2026-01-09 03:55:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxc917",
          "author": "vdharankar",
          "text": "You can build anything using bare code but have you heard something called frameworks ? By definition they eastout on lot of things plus they provide structured way to achieve something . Are you sure your code is scalable also done when best practices across the industry ? Also is it optimized considering all the situations ? I am sure answer to a lot of these questions will be no.",
          "score": 1,
          "created_utc": "2026-01-11 05:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrfk60",
          "author": "portugese_fruit",
          "text": "BAML",
          "score": 1,
          "created_utc": "2026-01-05 04:28:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrqkhs",
          "author": "ai_hedge_fund",
          "text": "No, you dont need it. It‚Äôs framework that offers tools to do certain things. Many ways to approach but depends on how much time you want to soend reinventing the wheel. I dont use it.",
          "score": 1,
          "created_utc": "2026-01-05 05:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxytu7e",
          "author": "futurespacetraveler",
          "text": "No",
          "score": 0,
          "created_utc": "2026-01-06 06:56:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q4y21e",
      "title": "Starting with Docling",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q4y21e/starting_with_docling/",
      "author": "DespoticLlama",
      "created_utc": "2026-01-05 21:05:08",
      "score": 17,
      "num_comments": 13,
      "upvote_ratio": 0.95,
      "text": "We are looking to update our existing \"aging\" POC token based RAG platform. We currently extract text from PDFs and break them into 1000 chars + an overlap. It's good enough that the project is continuing but we feel we could do better with additional structure. \n\nDocling seems a perfect next step but a little overwhelmed on where to start. Any recommendations on blogs, repositories that will help us get started and hopefully avoid the basic mistakes or at least weigh the pros and cons of various approaches? Thanks \n\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q4y21e/starting_with_docling/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxwnt4l",
          "author": "bravelogitex",
          "text": "Try [https://ragflow.io/](https://ragflow.io/)\n\nAlso gemini flash will work great for OCR. One guy did it for the fintech company he works for here: [https://news.ycombinator.com/item?id=42953665](https://news.ycombinator.com/item?id=42953665)",
          "score": 3,
          "created_utc": "2026-01-05 23:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzac9v",
              "author": "DespoticLlama",
              "text": "Thankyou",
              "score": 1,
              "created_utc": "2026-01-06 09:30:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "ny05wk7",
              "author": "MonBabbie",
              "text": "How‚Äôd he deal with data privacy concerns?",
              "score": 1,
              "created_utc": "2026-01-06 13:34:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxwrcra",
          "author": "davernow",
          "text": "If you are willing to try various extraction methods and aren‚Äôt tied to Docling, try Kiln: https://docs.kiln.tech/docs/documents-and-search-rag\n\nYou can try various configs (extraction, chunking, embedding, vector search), and use evals to find the best for your use case.",
          "score": 2,
          "created_utc": "2026-01-05 23:26:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxza90w",
              "author": "DespoticLlama",
              "text": "Thankyou",
              "score": 1,
              "created_utc": "2026-01-06 09:29:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxxgw40",
          "author": "bjl218",
          "text": "I've been using Docling for a couple of weeks and have only scratched the surface. I'm probably not using it in the same way you intend though. I'm using it to parse and chunk documents of various types. I then convert the chunks into documents according a specific schema which I store in an index in OpenSearch. I provide a query tool to the model which uses OpenSearch's MCP server to query the index.\n\nSo far, Docling appears to be parsing and chunking the content well. I haven't quite gotten the model to call the search tool correctly at this point, but you probably don't care about that part.",
          "score": 2,
          "created_utc": "2026-01-06 01:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxx9v74",
          "author": "fustercluck6000",
          "text": "Fwiw, I‚Äôve been using Docling for a little bit now and still find it overwhelming. Imho the docs are pretty lacking, which makes it tough to fully leverage what‚Äôs under the hood in your pipeline. Plus it‚Äôs still relatively new, so the community is pretty small.\n\nIngesting and converting to markdown/other markup languages is super straightforward out of the box. If the conversion process works for your docs (I‚Äôve found it‚Äôs really hit or miss) and you don‚Äôt need to define more complex chunking strategy, then just using the document converter and ‚Äòexport_to_markdown()‚Äô methods will get you most of the way there. \n\nI‚Äôve found things get a lot trickier when you need to debug or want to interact with the Docling Document data model (to correct indexing errors or take advantage of the tree structure for better hierarchical indexing). Seems like a shame because the data model to my mind is maybe the most useful thing for RAG, but at least for now, I‚Äôve only found fragile, superficial ways of integrating that part into my pipeline. \n\nI just started using Pandoc and I‚Äôm loving it. It‚Äôs kind of the same idea‚Äîsupported documents are all mapped to a ‚Äòunified‚Äô data model that you can export to all kinds of markup languages. It‚Äôs well documented and you can customize things a ton, e.g. setting custom example docs for it to use as a layout template. It doesn‚Äôt use any deep learning and can‚Äôt read from PDF, but I like having a hard-coded tool that behaves consistently and adding the LLM/VLM logic myself.",
          "score": 1,
          "created_utc": "2026-01-06 01:03:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxybu7t",
              "author": "stevevaius",
              "text": "I have law texts in PDFs which are basically in same structure such as title of the law, article number, sub letter and text. Feeding LLM with formatted markdowns or JSON is important for me. Pandoc or Docling is the way to try?",
              "score": 1,
              "created_utc": "2026-01-06 04:39:21",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxz6jtb",
                  "author": "fustercluck6000",
                  "text": "I say test out Docling and go through the results with a fine-tooth comb to see if it can do what you need it to. Legal is especially tricky because of all the structuring/citations, idk how well Docling‚Äôs going to pick that up before introducing parsing errors, but definitely give it a shot.\n\nWhat I‚Äôm working on atm is using a separate pipeline altogether to convert PDFs to markdown format with VLMs, load that into Pandoc, then iterate over the document tree to get the markdown-formatted chunks (nodes)/define edges. You can do the same thing with Docling, I just got tired of trying to fix the parsing errors i kept getting with tougher PDFs.",
                  "score": 2,
                  "created_utc": "2026-01-06 08:53:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxybywo",
          "author": "Serious-Barber-2829",
          "text": "\\>> Docling seems a perfect next step\n\nHow did you arrive at this decision?",
          "score": 1,
          "created_utc": "2026-01-06 04:40:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxzap1g",
              "author": "DespoticLlama",
              "text": "Well there are many options to choose from and we have to start somewhere with one of them and from conversations I've had before Xmas at a conference docling came up a lot.",
              "score": 3,
              "created_utc": "2026-01-06 09:34:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny1eu7i",
                  "author": "Serious-Barber-2829",
                  "text": "I see.  Thanks for your reply.",
                  "score": 1,
                  "created_utc": "2026-01-06 17:14:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nykivbw",
          "author": "astro_abhi",
          "text": "Try [VectraSDK](https://vectra.thenxtgenagents.com)\n\nIt's an open Source Provider Agnostic RAG SDK for Production AI, full configurable pipeline",
          "score": 1,
          "created_utc": "2026-01-09 09:50:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q8100n",
      "title": "RAG systems get hard fast once you move beyond demos",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q8100n/rag_systems_get_hard_fast_once_you_move_beyond/",
      "author": "FoundSomeLogic",
      "created_utc": "2026-01-09 06:33:47",
      "score": 17,
      "num_comments": 16,
      "upvote_ratio": 1.0,
      "text": "Lately I‚Äôve been revisiting how RAG systems behave once they‚Äôre pushed past simple Q&A demos. What keeps coming up is that retrieval itself isn‚Äôt the hardest part, it‚Äôs everything around it.\n\nI‚Äôve been reading material that goes deeper into things like:\n\n* how RAG interacts with memory over time\n* using more structured or graph-based retrieval instead of flat similarity search\n* how agents actually consume and reuse retrieved context across steps\n\nIt really reinforced a pattern I‚Äôm seeing in practice: as systems scale, the challenges shift toward context lifecycle, relevance decay, and how retrieved knowledge is structured and controlled, not just how fast you can fetch embeddings.\n\nCurious how others here are approaching this:\n\n* Are you separating short-term context from longer-term memory?\n* Anyone experimenting with graph-based or multi-stage RAG in production?\n* What tends to break first when RAG systems grow more complex?\n\nIf this resonates and anyone wants more details, I can share the book I‚Äôve been reading that explores these ideas further.\n\nFor anyone who wants the book details, I have shared it in the comments section with few people who asked about it. You can check it out. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q8100n/rag_systems_get_hard_fast_once_you_move_beyond/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyk51iq",
          "author": "Sona_diaries",
          "text": "Sounds good_ please share",
          "score": 2,
          "created_utc": "2026-01-09 07:45:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl08u2",
              "author": "FoundSomeLogic",
              "text": "here you go: [Amazon.com: Unlocking Data with Generative AI and RAG: Learn AI agent fundamentals with RAG-powered memory, graph-based RAG, and intelligent recall: 9781806381654: Keith Bourne: Books](https://www.amazon.com/dp/1806381656)\n\nThis was super helpful for me in real world",
              "score": 1,
              "created_utc": "2026-01-09 12:14:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nykxo3u",
          "author": "Hegemonikon138",
          "text": "Yes, I mean this is obvious is it not?\n\nAll data needs to be managed and a semantic search engine is no different.\n\nSystems need to evolve as they grow often including hybrid options like graph, better classification and other maintenance and relevance revision workflows.",
          "score": 2,
          "created_utc": "2026-01-09 11:55:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nykb6zu",
          "author": "Better_Ad_3004",
          "text": "Interested please share",
          "score": 1,
          "created_utc": "2026-01-09 08:40:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl09th",
              "author": "FoundSomeLogic",
              "text": "here you go: [Amazon.com: Unlocking Data with Generative AI and RAG: Learn AI agent fundamentals with RAG-powered memory, graph-based RAG, and intelligent recall: 9781806381654: Keith Bourne: Books](https://www.amazon.com/dp/1806381656)\n\nThis was super helpful for me in real world",
              "score": 1,
              "created_utc": "2026-01-09 12:14:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyke16n",
          "author": "an596",
          "text": "Please share",
          "score": 1,
          "created_utc": "2026-01-09 09:05:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl0aa7",
              "author": "FoundSomeLogic",
              "text": "here you go: [Amazon.com: Unlocking Data with Generative AI and RAG: Learn AI agent fundamentals with RAG-powered memory, graph-based RAG, and intelligent recall: 9781806381654: Keith Bourne: Books](https://www.amazon.com/dp/1806381656)\n\nThis was super helpful for me in real world",
              "score": 2,
              "created_utc": "2026-01-09 12:14:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyl0mmv",
          "author": "Opposite_Toe_3443",
          "text": "Interested!",
          "score": 1,
          "created_utc": "2026-01-09 12:17:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyl0pyr",
          "author": "Sona_diaries",
          "text": "Oh wow! I have read its first edition. Didn‚Äôt know second edition is out as well.",
          "score": 1,
          "created_utc": "2026-01-09 12:17:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nymyoij",
          "author": "Character_Meeting877",
          "text": "You‚Äôre right, RAG gets hard not because retrieval is slow, but because context has history, structure, and consequences. Once systems scale, success depends less on embeddings and more on how deliberately you manage memory, authority, and decay.",
          "score": 1,
          "created_utc": "2026-01-09 18:04:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyk2whh",
          "author": "FellowshipOfTheBook",
          "text": "I'd be interested in the book.",
          "score": 1,
          "created_utc": "2026-01-09 07:26:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl08ec",
              "author": "FoundSomeLogic",
              "text": "here you go: [Amazon.com: Unlocking Data with Generative AI and RAG: Learn AI agent fundamentals with RAG-powered memory, graph-based RAG, and intelligent recall: 9781806381654: Keith Bourne: Books](https://www.amazon.com/dp/1806381656)\n\nThis was super helpful for me in real world",
              "score": 1,
              "created_utc": "2026-01-09 12:14:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyl5knz",
                  "author": "FellowshipOfTheBook",
                  "text": "Thank you!",
                  "score": 1,
                  "created_utc": "2026-01-09 12:50:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyk6iz4",
          "author": "patbhakta",
          "text": "I'm interested in the book",
          "score": 0,
          "created_utc": "2026-01-09 07:58:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyl09az",
              "author": "FoundSomeLogic",
              "text": "here you go: [Amazon.com: Unlocking Data with Generative AI and RAG: Learn AI agent fundamentals with RAG-powered memory, graph-based RAG, and intelligent recall: 9781806381654: Keith Bourne: Books](https://www.amazon.com/dp/1806381656)\n\nThis was super helpful for me in real world",
              "score": 1,
              "created_utc": "2026-01-09 12:14:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3dvh1",
      "title": "Lessons Learned from Building Vector Database for AI Apps: Use Cheap S3 and Treat RAM as a Cache",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q3dvh1/lessons_learned_from_building_vector_database_for/",
      "author": "ethanchen20250322",
      "created_utc": "2026-01-04 02:32:24",
      "score": 16,
      "num_comments": 1,
      "upvote_ratio": 0.91,
      "text": "As the team behind the Milvus vector database, and we keep hearing the same complaint from AI app developers:\n\n‚ÄúWhy are my vector costs so high? It doesn‚Äôt make sense to pay so much for inactive users‚Äô data.‚Äù\n\nAfter digging into this problem, we studied real-world search patterns and discovered a common behavior. For a consumer app, developers load 100% of their users‚Äô embeddings into RAM, but nearly 80% of those vectors are barely ever queried. They sit idle while consuming the most expensive resource.\n\nThat felt wasteful.\n\nSo our team spent several months rebuilding the scheduling strategy with a simple idea in mind: treat local storage like a cache, not a database.\n\n**Here's what we built:**\n\nüî∑¬†**Lazy loading**¬†\\- We only load metadata at startup. Collection becomes queryable in seconds, not 25 minutes. Actual data gets fetched when queries need it.\n\nüî∑¬†**Partial loading**¬†\\- Why load tenant B's data when you're querying tenant A? Now we only fetch the specific segments/columns each query actually needs.\n\nüî∑¬†**LRU eviction**¬†\\- We track which items are actually queried. When the cache fills up, cold data is automatically evicted to make space for active data.\n\nWe tested this with 100M vectors (768-dim) and saw memory drop from 430GB to about 85GB at steady state, hot queries stayed basically the same speed (under 7% increase), and load time went from 25 minutes down to 45 seconds. The trade-off felt worth it.\n\nWe wrote up the full technical details here if you want to dig deeper:¬†[https://milvus.io/blog/milvus-tiered-storage-80-less-vector-search-cost-with-on-demand-hot%E2%80%93cold-data-loading.md?utm\\_source=reddit](https://milvus.io/blog/milvus-tiered-storage-80-less-vector-search-cost-with-on-demand-hot%E2%80%93cold-data-loading.md?utm_source=reddit)\n\nCurious if others are exploring similar cache-based approaches for large-scale RAG?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q3dvh1/lessons_learned_from_building_vector_database_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1py8l8f",
      "title": "I built a Python library that translates embeddings from MiniLM to OpenAI ‚Äî and it actually works!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
      "author": "Interesting-Town-433",
      "created_utc": "2025-12-29 01:23:16",
      "score": 16,
      "num_comments": 3,
      "upvote_ratio": 0.91,
      "text": "*I built a Python library called* ***EmbeddingAdapters*** *that* ***provides multiple pre-trained adapters for translating embeddings from one model space into another***:\n\n[https://github.com/PotentiallyARobot/EmbeddingAdapters/](https://github.com/PotentiallyARobot/EmbeddingAdapters/)\n\n\\`\\`\\`  \n`pip install embedding-adapters`\n\n`embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text \"Where can I get a hamburger near me?\"`  \n\\`\\`\\`\n\n*This works because* ***each adapter is trained on a restrictive domain*** allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity.¬† ***A quality endpoint then lets you determine how well the adapter will perform*** *on a given input.*\n\nThis has been super useful to me, and I'm quickly iterating on it.\n\nUses for ***EmbeddingAdapters*** so far:\n\n1. You want to **use an existing vector index built with one embedding model and query it with another** \\- if it's expensive or problematic to re-embed your entire corpus, this is the package for you.\n2. You can also **operate mixed vector indexes** and map to the embedding space that works best for different questions.\n3. You can **save cost on questions that are easily adapted**, \"What's the nearest restaurant that has a Hamburger?\" no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.\n\nIt also lets you experiment with provider embeddings you may not have access to.¬† By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.\n\nThis makes it practical to:  \n\\- **sample providers you don't have direct access to**  \n\\- **migrate or experiment with embedding models gradually** instead of re-embedding everything at once,  \n\\- ***evaluate multiple providers side by side*** in a consistent retrieval setup,  \n\\- ***handle provider outages or rate limits*** without breaking retrieval,  \n\\- ***run RAG in air-gapped or restricted environments*** with no outbound embedding calls,  \n\\- ***keep a stable ‚Äúcanonical‚Äù embedding space*** while changing what runs at the edge.\n\nThe adapters aren't perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 98% of the openai embedding and dramatically outperforms minilm -> minilm RAG setups\n\nIt's still early¬†in this project. I‚Äôm actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the¬†models and improving evaluation and quality tooling.\n\nI‚Äôd love feedback from anyone who might be interested in using this:  \n*- What data would you like to see these adapters trained on?*  \n*- What domains would be most helpful to target?*  \n*- Which model pairs would you like me¬†to add next?*  \n*- How could¬†I make this more useful for you to use?*\n\nSo far the¬†library supports:  \n*minilm <-> openai*¬†  \n*openai <-> gemini*  \n*e5 <-> minilm*  \n*e5 <-> openai*  \n*e5 <-> gemini*  \n*minilm <-> gemini*\n\nHappy to answer questions and if anyone has any ideas please let me know.  \nI could use any support you can give, especially if anyone wants to chip in to help cover the training cost.\n\nPlease upvote if you can, thanks!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwii84p",
          "author": "-Cubie-",
          "text": "Nice work! This is cool. How does it train the adapter, and what is the network for the adapter? A single Linear with the correct input/output dimensionality trained with distillation?\n\nIt reminds me a bit of model distillation to finetune a small local embedding model to match a bigger one: https://sbert.net/examples/sentence_transformer/training/distillation/README.html Such s fascinating strategy.",
          "score": 2,
          "created_utc": "2025-12-29 08:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjv4le",
              "author": "Mysterious_Robot_476",
              "text": "Thanks! Really appreciate the support. Yeah, I'm super excited to share!\n\nRe architecture, I tried purely linear ( there are a couple in the registry ) but actually the mapping between embedding spaces is only mostly linear, not entirely, so higher accuracy models do benefit from non linearity. The balance is really how much size you want to add to the model. Still trying to figure out the best architecture for capturing, right now I'm leaning more toward a MOE of MLPs with residual connections, but still very flexible here.  The v2 models I'm training are more aligned with this.\n\nIt does seem to point to something interesting though, how can a small llm like minilm capture so much of a massive provider model ( even in a restricted domain ). Something deeper going on here perhaps?\n\nI'm training the v2 models atm expanding the training set size and providers, next version of embedding-adapters I plan to add fine tuning scripts so people can experiment here and upload their own adapters easier.  You can add to the registry as is if you have a working adapter as well, use the cli add  functionality or make a pr to the embedding-adapters registry\n\nhttps://github.com/PotentiallyARobot/embedding-adapters-registry",
              "score": 2,
              "created_utc": "2025-12-29 14:43:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ht22",
          "author": "Last-Application-558",
          "text": "Wow cool! I will check in details",
          "score": 1,
          "created_utc": "2026-01-01 16:13:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzo7t",
      "title": "Vector DBs for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pxzo7t/vector_dbs_for_rag/",
      "author": "hackdev001",
      "created_utc": "2025-12-28 19:13:14",
      "score": 16,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\n  \nI am working on a rag application and was confused on which vector db should I go ahead with? I have currently integrated Qdrant as it is open source I can deploy it to my own servers.\n\n  \nHowever, I dont really know how to judge the accuracy of the application. Does different vector dbs give different results in terms of accuracy?  \nIf yes, then which ones are the most accurate and SOTA?\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pxzo7t/vector_dbs_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nweunin",
          "author": "Spursdy",
          "text": "Accuracy won't change by changing the database. The embedding model , number of dimensions and distance algorithm will change the accuracy.\n\nI use postgres with ph_vector.",
          "score": 11,
          "created_utc": "2025-12-28 19:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwff3cw",
              "author": "EveYogaTech",
              "text": "üíØ We also use pg_vector, for most use-cases it seems to be enough.",
              "score": 5,
              "created_utc": "2025-12-28 21:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwntgmb",
                  "author": "344lancherway",
                  "text": "Have you experimented with different distance metrics in pg_vector? Sometimes tweaking those can make a big difference in how well your model performs.",
                  "score": 1,
                  "created_utc": "2025-12-30 02:38:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf199r",
              "author": "hackdev001",
              "text": "I am using latest google embedding model. So that should be good. Distance algorithm is something with which I can compare and verify",
              "score": 1,
              "created_utc": "2025-12-28 19:53:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfw5vl",
          "author": "Valeria_Xenakis",
          "text": "Yes, different vector databases can return different results ‚Äî but it‚Äôs usually not because one is ‚Äúsmarter.‚Äù It‚Äôs because most production vector search is approximate and each system (and even each config) trades speed vs retrieval recall (how often you actually get the true nearest neighbours).\n\nPeople also mix up two kinds of ‚Äúaccuracy‚Äù:\n\n1) Retrieval accuracy (are you fetching the right chunks?)\n\nMost vector databases use Approximate Nearest Neighbour search instead of exact nearest neighbour search, because exact is too slow once you have lots of vectors.\n\nSo results can differ due to:\n\nA. Index type (for example, Hierarchical Navigable Small World graphs)\n\nB. Different defaults (some default ‚Äúfast but lower recall‚Äù)\n\nC. Different tuning knobs (how hard the search works)\n\nSo yes: two databases can produce different top-k chunks if one is more aggressively approximate than the other.\n\n2) End-to-end Retrieval-Augmented Generation accuracy (is the final answer good?)\n\nIn practice, the bigger drivers for this are:\n\nA. Embedding model quality\n\nB. Chunking strategy\n\nC. Query rewriting\n\nD. Reranking (second stage re-ordering with a stronger model)\n\nDatabase choice matters, but it‚Äôs rarely the main reason your Retrieval-Augmented Generation answers are good/bad ‚Äî unless you‚Äôre under-tuned or using heavy compression.\n\nHow to test it:\n\nDo it in two layers:\n\nStep 1: Measure retrieval recall\n\nMake a small eval set (like 50‚Äì200 real queries).\n\nThen:\n\n1. Run exact search (full scan) to get ground truth neighbours\n\n2. Run your normal approximate search\n\n3. Compute Recall at K: ‚ÄúHow many of the true top-K did the approximate search retrieve?‚Äù\n\nThis tells you if you‚Äôre losing too much recall for speed.\n\nStep 2: Measure answer quality\n\nOn the same queries, score final answers using:\n\nA. Ground truth answers (if you have them)\n\nOR\n\nB. a mix of human review + automated judging\n\nThis tells you whether retrieval differences actually matter for your use case.\n\nFor the question which vector database is most accurate / state of the art?‚Äù\n\nIf you mean ‚Äúmost accurate possible‚Äù: exact search is the most accurate, but expensive.\n\nPS: The answer has been formatted and structured using AI. Protect our community against a dead internet due to overuse of AI",
          "score": 5,
          "created_utc": "2025-12-28 22:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwexmst",
          "author": "bzImage",
          "text": "want accuracy ? .. use qdrant metadata and save the chunk of data \"keywords\".. .. now.. you can filter and search by keywords.. \n\nkeywords > vector search for accuracy",
          "score": 5,
          "created_utc": "2025-12-28 19:35:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf0vw2",
              "author": "hackdev001",
              "text": "Thats interesting but in a long document, a few keywords can sometimes give inaccurate results. Maybe hybrid (keyword + vector) is the way to go here",
              "score": 1,
              "created_utc": "2025-12-28 19:51:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwji395",
                  "author": "bzImage",
                  "text": "Use llm to generate good size chunks and extract keywords..",
                  "score": 1,
                  "created_utc": "2025-12-29 13:27:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf1zrt",
              "author": "websinthe",
              "text": "Yeah, keywords/metadata, chunking strategy, and graph storage are all orders of magnitude more important but they're kinda treated like afterthoughts.",
              "score": 1,
              "created_utc": "2025-12-28 19:56:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nweuo0m",
          "author": "DressMetal",
          "text": "I use Chroma for now, works fine.",
          "score": 2,
          "created_utc": "2025-12-28 19:21:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwezg1z",
          "author": "RolandRu",
          "text": "In my case I‚Äôm doing RAG for code, so I evaluate retrieval directly. I run a fixed set of questions against a test repo and check whether the expected files/functions/fragments show up in the top-k retrieved chunks. Only after that I look at the final LLM answer. This keeps the evaluation focused on retrieval quality instead of being fooled by a fluent answer that isn‚Äôt grounded in the right context.",
          "score": 2,
          "created_utc": "2025-12-28 19:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1kck",
          "author": "websinthe",
          "text": "The best results I've had came from just using Polars. It's a little more work but not a huge amount, it's significantly more performant, and vector DBs are such a small part of a RAG that it's not worth taking on the constant breakages the named VDBs have on updates.\n\nOtherwise I'd just use chromadb.\n\nSpend 5% of your time choosing the vector database and 95% perfecting your chunking strategy. That's the important part.\n\nSorry if this is preachy.",
          "score": 1,
          "created_utc": "2025-12-28 19:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf2z9l",
              "author": "hackdev001",
              "text": "How are you using Polars for a Rag application?\nAnd how is it a substitute for a vector db?",
              "score": 2,
              "created_utc": "2025-12-28 20:01:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx48xeh",
                  "author": "websinthe",
                  "text": "A vector db is just a high-dimensional store of vector embeddings. Polars is kinda like Numpy and can have arbitrarily-dimensional storage of vectors, and if you have code that takes tokens and turns them into embeddings, there's nit a lot that a vector db does that isn't just a big multi-dimensional array.",
                  "score": 2,
                  "created_utc": "2026-01-01 18:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfen1a",
          "author": "hrishikamath",
          "text": "Unless you really have huge amount or data, pg vector w Postgres is more than enough",
          "score": 1,
          "created_utc": "2025-12-28 20:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlbk68",
          "author": "Potential-Buy-4267",
          "text": "Chromadb, pgvector,...",
          "score": 1,
          "created_utc": "2025-12-29 18:54:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q683tf",
      "title": "Hybrid search + reranking in prod, what's actually worth the complexity?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q683tf/hybrid_search_reranking_in_prod_whats_actually/",
      "author": "helentch",
      "created_utc": "2026-01-07 06:33:02",
      "score": 15,
      "num_comments": 15,
      "upvote_ratio": 0.86,
      "text": "Building a RAG system for internal docs (50k+ documents, multi tenant, sub 2s latency requirement) and I'm going in circles on whether hybrid search + reranking is worth it vs just dense embeddings.  \nEveryone says \"use both\" but rerankers add latency and cost. Tried Cohere rerank but it's eating our budget. BM25 + vector seems overkill for some queries but necessary for others?  \nAlso chunking strategy is all over the place. 512 tokens with overlap vs semantic chunking, no idea what actually moves the needle.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q683tf/hybrid_search_reranking_in_prod_whats_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny5y29d",
          "author": "Harotsa",
          "text": "A lot of the small open source rerankers are much faster and cheaper than proprietary rerankers like voyager or cohere. I really like the qwen3-reranker-8b, in our testing it was nearly as good as the voyager reranker and was a fraction of the cost and latency.\n\nIt might take a bit of effort to deploy on your infra if you haven‚Äôt deployed any open source LLMs before, but we saw significant improvements using a reranker vs just RRF with hybrid search.\n\nhttps://huggingface.co/Qwen/Qwen3-Reranker-8B\n\nIf you are already committed to using a dense vector search then you might as well do a hybrid search with RRF at the very least. BM25 is much faster and cheaper than dense vector search so it is effectively free from a cost and latency perspective, and it is a higher-precision lower-recall search so it complements the dense vector search well.",
          "score": 9,
          "created_utc": "2026-01-07 08:03:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5yjt8",
              "author": "-Cubie-",
              "text": "P.s. Qwen also has smaller ones if the 8b one is still out of budget: https://huggingface.co/Qwen/Qwen3-Reranker-0.6B",
              "score": 5,
              "created_utc": "2026-01-07 08:08:05",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny5ojg8",
          "author": "Hot_Substance_9432",
          "text": "There is a solution:)\n\nCombining BM25 and vector search (hybrid search) and applying them based on the specific query type is an effective strategy for maximizing search accuracy and performance. This approach, often managed by a¬†**query router**, leverages the strengths of each method while mitigating their weaknesses.¬†\n\nWhen to use BM25 vs. Vector Search\n\nA query router can analyze the incoming query and determine the most appropriate retrieval method:¬†\n\n|**Query Characteristic**¬†|**Best Method**|**Why?**|\n|:-|:-|:-|\n|**Specific IDs, error codes, product names, or jargon**|BM25 only|Excels at exact keyword matches and rare terms which vector search might miss.|\n|**Long, descriptive, or natural language questions**|Vector only (or Hybrid)|Captures the semantic meaning and context of the query, even if specific words don't match the documents exactly.|\n|**Statutory phrases or specific legal terms**|BM25 only|Ensures precise legal or technical phrases are found rather than semantically similar but incorrect paraphrases.|\n|**Short, vague, or conceptual queries**|Hybrid (BM25 + Vector)|A hybrid approach provides a comprehensive result set, capturing both exact matches and related concepts.|",
          "score": 4,
          "created_utc": "2026-01-07 06:40:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny5yqmu",
              "author": "-Cubie-",
              "text": "But wouldn't the query router itself add unnecessary overhead? Or is it just a few if-else statements to check for common cases like error codes/IDs?",
              "score": 3,
              "created_utc": "2026-01-07 08:09:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny61ik6",
                  "author": "Hot_Substance_9432",
                  "text": "Few common cases would be sufficient with an if else",
                  "score": 3,
                  "created_utc": "2026-01-07 08:35:28",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny641cs",
                  "author": "334578theo",
                  "text": "Use a small model, or even better - fine tune a tiny model which solely does query routing",
                  "score": 2,
                  "created_utc": "2026-01-07 08:58:58",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "ny6ju21",
                  "author": "dash_bro",
                  "text": "Use a small onprem model. I built one for my usecase that was 40MB, loaded and served efficiently\n\nThe default strategy was bm25+vector search when the router couldn't disambiguate meaningfully. Works for us in prod with a rolling average count of how many defaults get triggered (works as observability)",
                  "score": 2,
                  "created_utc": "2026-01-07 11:21:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny77wiw",
          "author": "davernow",
          "text": "Hybrid is mostly worth it. Not too hard to implement, fast, solves exact match issues. \n\nRe-ranking: wait and see if you need it. Ideally run some evals.",
          "score": 2,
          "created_utc": "2026-01-07 13:58:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny5rlvv",
          "author": "hrishikamath",
          "text": "For me the sentence transformers re ranking worked just okay",
          "score": 1,
          "created_utc": "2026-01-07 07:06:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6i6f0",
          "author": "Straight-Gazelle-597",
          "text": "what's the range of the budget? you can self-host several models in 1 machine to share the costs.",
          "score": 1,
          "created_utc": "2026-01-07 11:07:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6jzkr",
          "author": "dash_bro",
          "text": "A 4B qwen reranker worked for us very well. Check it out!\n\nAlso the jina AI reranker is pretty good.",
          "score": 1,
          "created_utc": "2026-01-07 11:22:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny846jq",
          "author": "OnyxProyectoUno",
          "text": "The chunking strategy matters way more than the retrieval setup here. You're optimizing the wrong layer.\n\nWith 50k docs and multi-tenant, your chunking approach is probably creating more noise than BM25 vs pure vector will ever solve. 512 tokens with overlap is a shotgun approach that works sometimes but creates tons of redundant, low-signal chunks. Semantic chunking sounds fancy but often splits at weird boundaries that hurt context.\n\nFor your scale and latency requirements, I'd nail down chunking first. Most retrieval problems trace back to bad chunks, not bad search algorithms. You want chunks that actually contain complete thoughts and preserve document structure. The reranker is trying to fix what got broken upstream.\n\nBeen building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_g) to tackle exactly this, where you can see what your docs look like after different chunking strategies before committing to one. The preview shows you which chunks would actually get retrieved for your queries.\n\nSkip Cohere rerank for now. Try a lightweight approach like reciprocal rank fusion between BM25 and vector results. It's fast, cheap, and often performs surprisingly well. You can always add a reranker later once your chunking is dialed in.\n\nWhat does your document structure actually look like? Are these structured docs with clear sections or more free-form content?",
          "score": 1,
          "created_utc": "2026-01-07 16:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny8or47",
          "author": "Hour-Entertainer-478",
          "text": "If you have fewer set of files, you dont need a  reranker. \n\nYou only need a reranker id youve got large set of files. Let me give you an example. Say your search query is \"what's the difference between Python 3 and Python 2\", and your documents mention these keywords hundreds of times. An embedding model will return chunks that are semantically similar to your query, but that doesn't mean they're the most relevant. You'll get 50 chunks that vaguely mention Python versions, and the one that actually explains the differences gets buried. That's where a reranker comes in, it takes those initial results and re scores them based on actual relevance to your query, pushing the useful chunks to the top.\n\nYou can use self hosted rerankers they work pretty well. Qwen3 and jina ai are good contenders",
          "score": 1,
          "created_utc": "2026-01-07 18:07:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nycanpz",
          "author": "badgerbadgerbadgerWI",
          "text": "For 50k docs with sub-2s latency, I'd start with just dense embeddings + a fast ANN index. Add BM25 hybrid only if you see clear retrieval misses on keyword-heavy queries. Reranking is powerful but you can defer it - do it async or only on top-10 results to stay under latency budget. Premature optimization here will kill your velocity.",
          "score": 1,
          "created_utc": "2026-01-08 04:34:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q67rjt",
      "title": "We built a shared RAG memory layer so every agent answers with our team‚Äôs real context",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q67rjt/we_built_a_shared_rag_memory_layer_so_every_agent/",
      "author": "Ok_Soup6298",
      "created_utc": "2026-01-07 06:14:12",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "Our team works with a lot of AI tools (ChatGPT, Claude, Cursor, Gemini, Perplexity), and we wanted every answer to reflect our¬†**team‚Äôs actual context**¬†instead of a fresh, stateless chat every time.‚Äã\n\nSo we built an internal¬†**shared memory + RAG layer**¬†for our own workflow:\n\n* All agent conversations go through a¬†**central memory service** ‚Üí we store important messages, decisions, summaries as structured context\n* The service ingests our existing knowledge bases ‚Üí Notion for specs/docs, Slack for discussions, Obsidian for personal notes\n* When any agent answers, it first queries this shared memory ‚Üí so replies are grounded in our latest team context, not just the current prompt\n\nIn practice this changed things a lot:\n\n* New hires get much better answers from agents because the system ‚Äúknows‚Äù our past decisions\n* We do less ‚Äúexplaining the same project for the 20th time‚Äù and more reviewing/iterating\n* Different tools (e.g., Cursor vs ChatGPT) still feel like one coherent assistant because they see the same memory\n\nWe originally prototyped this as a¬†**Universal Memory MCP**¬†for the Claude Code Hackathon and got 2nd place, then kept refining it for internal use. Now we are turning this into a service called¬†**Membase**¬†so others can plug their agents into a similar shared memory/RAG layer.‚Äã\n\nWe are preparing a small private beta with a demo and waitlist at¬†[**membase.so**](https://membase.so/?utm_source=reddit&utm_medium=Rag)¬†for anyone who wants to experiment with this kind of setup in their own stack. Happy to share more architectural details (indexing, schemas, routing logic) if people here are interested.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q67rjt/we_built_a_shared_rag_memory_layer_so_every_agent/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny5ym5e",
          "author": "Diablo_Rigs",
          "text": "I love what you‚Äôve built. Could you elaborate a bit more on how you achieved such an impressive feat?",
          "score": 1,
          "created_utc": "2026-01-07 08:08:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny6pn52",
          "author": "Accomplished_Life416",
          "text": "Please elaborate this bit more so we can digest this , can improve our system",
          "score": 1,
          "created_utc": "2026-01-07 12:05:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0v8jp",
      "title": "GraphQLite - Embedded graph database for building GraphRAG with SQLite",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q0v8jp/graphqlite_embedded_graph_database_for_building/",
      "author": "Fit-Presentation-591",
      "created_utc": "2026-01-01 04:03:30",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "For anyone building GraphRAG systems who doesn't want to run Neo4j just to store a knowledge graph, I've been working on something that might help.\n\nGraphQLite is an SQLite extension that adds Cypher query support. The idea is that you can store your extracted entities and relationships in a graph structure, then use Cypher to traverse and expand context during retrieval. Combined with sqlite-vec for the vector search component, you get a fully embedded RAG stack in a single database file.\n\nIt includes graph algorithms like PageRank and community detection, which are useful for identifying important entities or clustering related concepts. There's an example in the repo using the HotpotQA multi-hop reasoning dataset if you want to see how the pieces fit together.\n\n\\`pip install graphqlite\\`\n\nHope someone finds this useful.\n\nGitHub: [https://github.com/colliery-io/graphqlite](https://github.com/colliery-io/graphqlite)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q0v8jp/graphqlite_embedded_graph_database_for_building/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx2lxa4",
          "author": "ubiquae",
          "text": "This is great news, specially after kudu being discontinue.\n\nI will try it out asap.",
          "score": 1,
          "created_utc": "2026-01-01 12:44:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q47pgl",
      "title": "How do you organize your LLM embedding datasets? mine are a mess",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q47pgl/how_do_you_organize_your_llm_embedding_datasets/",
      "author": "MarketPredator",
      "created_utc": "2026-01-05 01:14:24",
      "score": 14,
      "num_comments": 7,
      "upvote_ratio": 0.95,
      "text": "I am an indie developer, building a few rag apps and the embedding situation is getting out of hand\n\nI have:\n\n*   embeddings from different models (bge, e5, nomic)\n    \n*   different chunk sizes\n    \n*   different source documents\n    \n*   some for prod, some experimental\n    \n\nall just sitting in folders with bad names. last week i accidentally used old embeddings for a demo and the results were garbage. took me an hour to figure out what went wrong.\n\nHow do you guys organize this stuff? just good folder structure? some kind of tracking system?\n\nSaw that apache gravitino added a Lance rest service in their 1.1.0 release last week. its a data catalog that exposes lance datasets over http with proper metadata. might be overkill for personal projects but honestly after wasting another hour debugging which embeddings i was using im considering it\n\nHas anyone tried it? or have simpler alternatives that aren't just folder or git structure",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q47pgl/how_do_you_organize_your_llm_embedding_datasets/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxr05sj",
          "author": "Hefty-Citron2066",
          "text": "i just checked their github: [https://github.com/apache/gravitino](https://github.com/apache/gravitino), its legit.",
          "score": 3,
          "created_utc": "2026-01-05 02:59:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqwtpz",
          "author": "Ready-Interest-1024",
          "text": "What type of documents? I feel like it varies based on what you‚Äôre trying to organize.",
          "score": 1,
          "created_utc": "2026-01-05 02:42:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxs6cdh",
          "author": "CheatCodesOfLife",
          "text": "!remind me 40 hours",
          "score": 1,
          "created_utc": "2026-01-05 07:50:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxs6f03",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-06 23:50:58 UTC**](http://www.wolframalpha.com/input/?i=2026-01-06%2023:50:58%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Rag/comments/1q47pgl/how_do_you_organize_your_llm_embedding_datasets/nxs6cdh/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FRag%2Fcomments%2F1q47pgl%2Fhow_do_you_organize_your_llm_embedding_datasets%2Fnxs6cdh%2F%5D%0A%0ARemindMe%21%202026-01-06%2023%3A50%3A58%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q47pgl)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-05 07:51:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxsdggq",
          "author": "toothpastespiders",
          "text": ">all just sitting in folders with bad names\n\nI'm at least glad to know I'm not the only one.",
          "score": 1,
          "created_utc": "2026-01-05 08:57:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny4l2bo",
          "author": "Much-Researcher6135",
          "text": "Maybe I'm missing something, but why not just put all this in a postgres+pgvector database? It's free and easy to set up, especially with docker. Depending on how private/solo/local the setup is, you could simply do one db installation and inside that, one logical database per project. Then in a chunk table (one row per chunk) just add metadata columns to do things like which embedding model was used, embedding prompts if you do that, etc.",
          "score": 1,
          "created_utc": "2026-01-07 02:25:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxqpqdj",
          "author": "Low-Efficiency-9756",
          "text": "I built a tool that gives llm agents the ability to build rag chatbots through a relatively deterministic process. It‚Äôs an mcp server and Claude desktop can use it to create a final product that \n\n1. Is hosted locally with an OpenAI API key in the .env\n2. Published to GitHub and uses docker files, railway.toml to host a live version with pages as the front end and the rag service with a /chat endpoint.",
          "score": 0,
          "created_utc": "2026-01-05 02:04:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2y8ni",
      "title": "PDF to md, table challenges, Docling chunks AND Marker chunks into the vector db? Bonkers?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q2y8ni/pdf_to_md_table_challenges_docling_chunks_and/",
      "author": "AvadaKK",
      "created_utc": "2026-01-03 16:03:22",
      "score": 14,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "PDF extraction is hard, and tables are absurdly difficult to deal with. This, especially if one goes the local computation way and doesn't use competent (and large) vision models.\n\nTable example: [https://i.imgur.com/HpNdn3g.png](https://i.imgur.com/HpNdn3g.png)\n\nIn my testing, docling and marker-pdf have given the best success. HOWEVER: docling might dominate one pdf and marker performs poorly, but in the next pdf the roles have been switched.\n\nOne idea of mine was to go page by page, and give the pdf page screenshot to qwen3-vl alongside the two md files and let it choose. This, especially if there were tables on the page.\n\nAnother method would be to just let marker and docling produce the md files, and chunk both versions into the same db. The retriever will fetch doubles, but the ranker will (hopefully) do its job and give us the \"better\" one of the doubles. We would semantically compare the chunks and find doubles, even if they are not perfectly aligned. Then go through the ranker's result list, and if that chunk's other double has already been picked, discard this so the LLM doesnt get the chunk twice. \n\nBasically the embedding will retrieve the bunch, and the re-ranker will go through the pile and give us the best matches. Choose the chunks that will be returned to the LLM and make sure no double chunks are given. \n\nDoes this help with the inconsistent pdf processing, or is the idea complete waste?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q2y8ni/pdf_to_md_table_challenges_docling_chunks_and/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxgu1f2",
          "author": "OnyxProyectoUno",
          "text": "Your approach is backwards. Instead of throwing multiple parsers at the wall and hoping the reranker sorts it out, you need to see what each parser actually produces before committing to the vector store. The problem isn't choosing between Docling and Marker outputs after embedding. It's that you can't tell which one mangled your tables until retrieval fails.\n\nMost people skip the preview step and end up with garbage chunks they never see. You're essentially building a complex deduplication system to fix a visibility problem. What you need is to compare the actual markdown outputs side by side, page by page, before any chunking happens.\n\nYour Qwen3-VL idea has merit but it's expensive and slow. Better approach is to run both parsers, preview the outputs, and pick the winner manually for each document type. Once you know Marker handles your financial reports better and Docling wins on research papers, you can automate that decision.\n\nThe double-chunking strategy will work but creates unnecessary complexity. Your retrieval becomes unpredictable because you never know which version will surface. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this preview problem because debugging parser outputs after they're embedded is painful.\n\nWhat does your table actually look like in the markdown after each parser runs?",
          "score": 1,
          "created_utc": "2026-01-03 17:04:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxh72wb",
              "author": "AvadaKK",
              "text": "Thanks for the thoughtful answer!\n\nI indeed was taking a look at the md files, and in some pdfs (all of technical style), Docling ruled the world, and in some, Marker was the king. If one doesn't manually go throuhg the pre-chunk checks, there is no way to know which tool to use on pdf X.\n\nCurrently trying to find ways to evaluate which parser to use to which document.",
              "score": 1,
              "created_utc": "2026-01-03 18:04:38",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxiksep",
              "author": "fabkosta",
              "text": "Uhm, okay. How do you deal with a situation where you have to process e.g. 10'000'000 docs?",
              "score": 1,
              "created_utc": "2026-01-03 22:01:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxj64pk",
          "author": "patbhakta",
          "text": "Your problem is you're using MD files for tables with merged cells. Might as well use CSV at that point both are useless at merged cells along with other issues with your example form. It's as arbitrary as saving a photo as text you're going to lose something. \n\nI moved all my parsing to html, html preserves it pretty well, I can do merged cells, sub tables, etc. Plus recall is super easy and can pretty it up with a simple css if needed. \n\nIf using python try pdf2htmlEX, token amount is less with MD and even less with TOON but for my needs I need accuracy not speed. Html isn't that much larger than MD for my use case.",
          "score": 1,
          "created_utc": "2026-01-03 23:50:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxlqgk3",
              "author": "AvadaKK",
              "text": "Doesnt pdf2htmlEX just produce visual glitter rather than chunkable precise html? The relationships for text / cell elements only exists in css, which is used to force the visuals imitate the original document.",
              "score": 1,
              "created_utc": "2026-01-04 10:19:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxmxbya",
                  "author": "patbhakta",
                  "text": "For my use case it works better than most. Give it a try, https://pdf2htmlex.github.io/pdf2htmlEX/\n\nEverything has limitations, it can do 10-20 pages into 1 html, you can chunk by page, or build your own parser to do it sections by sections\n\nArbitrarily you can save a PDF as html in MS word, but even that doesn't do well.",
                  "score": 1,
                  "created_utc": "2026-01-04 15:18:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxjxv90",
          "author": "Knight7561",
          "text": "Why don‚Äôt you try out azure document intelligence",
          "score": 1,
          "created_utc": "2026-01-04 02:21:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyn4hj",
      "title": "Why is there no opinionated all in one RAG platform?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pyn4hj/why_is_there_no_opinionated_all_in_one_rag/",
      "author": "Pl8tinium",
      "created_utc": "2025-12-29 14:05:40",
      "score": 14,
      "num_comments": 26,
      "upvote_ratio": 0.85,
      "text": "Im skimming through the web and unfortunately cannot find a SOTA maintained FOSS platform for RAG. I identified some platforms like\n\nQuivr but they dont seem to be maintained anymore.\n\n[https://github.com/QuivrHQ/quivr](https://github.com/QuivrHQ/quivr)\n\nI also identified a lot of frameworks that make it easier to build RAG apps like llamaindex, RAGflow, dify etc., but they dont provide the opinionated blackbox experience im searching for.\n\nSure there are also those \"all in one\" platforms like openwebui or localGPT that provide RAG capabilities and have an opinionated pipeline. But often times their primary focus is not just RAG and they thereby often do not incorporate SOTA techniques into their products. Also they are often built so that you could use them in conjunction with the rest of the package, not to just deliver the RAG results to another frontend.\n\n[https://github.com/PromtEngineer/localGPT](https://github.com/PromtEngineer/localGPT)\n\n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nThat being said i do think that it most definetly makes sense that there would be one giant FOSS project always keeping track of the latest and greatest techniques and would just provide a set of valves to tweak functionality and individualize the experience. Other proprietary vendors also try to provide this like Microsoft 365 Copilot Agent or Snowflake cortex. In these cases there is often a sophisticated RAG pipeline in place that does things like\n\n\\- broad chunk search, then narrow down when it identified focus on a specific document\n\n\\- expanding the context of found chunks\n\n\\- intermediate summarizations of docs when working with a large amount of docs simultaneously\n\n\\- ....\n\nAll of those things help to provide a great experience, but for me as a \"one engineer in the ai team\" cannot build, maintain and keep a self built RAG solution up to date to the latest and greatest additions to the space.\n\nJust to note one could say that an \"one size fits all\" solution is not possible, especially because data is so different from system to system, but i'd argue that many proprietary platforms like Microsoft 365 Copilot have perfected this already and can easily be plugged in to any arbitrary form of data and  work relatively well (atleast if the data is in one of the basic formats of data like txt, pdf, pptx, docx ...)\n\nIdeally i would want a RAG platform that always is relatively close behind SOTA, there would be (community) created adapters for enterprise data stores like sharepoint, SAP etc and allows for simple integration into other systems. I'd also pay for this, its not like i just want FOSS, but I would think that the community would also have identified a need for this..\n\nIs what im thinking about valid or is my department just too small to do any meaningful RAG and i should upgrade to more personal so i have the capability to build and maintain RAG pipelines from the ground up or am i just not noticing some development in the space?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pyn4hj/why_is_there_no_opinionated_all_in_one_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwjwuld",
          "author": "fabkosta",
          "text": "Why is there no single database system but many? Same reasons apply to RAG.",
          "score": 6,
          "created_utc": "2025-12-29 14:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4ar6a",
              "author": "hhussain-",
              "text": "One difference worth mentioning here: databases have solid definitions (DBRM) and well searched mechanism to achieve those definitions in reality. RAG, like the rest of AI arena, is well defined but mechanisms are still and R&D race. This is why all those $ billions are burned into it without really reaching a solid optimized mechanisms.\n\nCompanies are racing, cash is burnt, we enjoy what we can enjoy!",
              "score": 1,
              "created_utc": "2026-01-01 18:41:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwjxjsu",
              "author": "Pl8tinium",
              "text": "theres opinionated systems for many things though, eg docusaurus is an opinionated markdown documentation platform with the purpose to make it easy for people to bootstrap a general docu platform without glueing all things together themselves. Why not for RAG?",
              "score": -1,
              "created_utc": "2025-12-29 14:56:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwjzabk",
                  "author": "stingraycharles",
                  "text": "Because RAG is always just a component in a larger system rather than a system itself.",
                  "score": 4,
                  "created_utc": "2025-12-29 15:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjy42i",
          "author": "ampancha",
          "text": "The reason a \"One Size Fits All\" FOSS platform doesn't exist is that SOTA retrieval techniques (like Parent-Document or Late Interaction) are highly dependent on your specific data topology. A \"Black Box\" that works for PDFs will fail on SQL tables.\n\nThe closest you get to \"maintained SOTA\" without building from scratch is adopting an **Opinionated Reference Architecture** rather than a framework. I maintain a **Standard RAG** repo that implements the exact pipeline you described (Broad Search -> Narrow Focus -> Context Expansion) as a deployable microservice, specifically for teams that don't have the bandwidth to reinvent the wheel. I sent you a DM",
          "score": 3,
          "created_utc": "2025-12-29 14:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk1doi",
              "author": "Pl8tinium",
              "text": "sure that would be something i could use but i want to have that maintained not by a single individual but some kind of company or FOSS contributors, otherwise i dont see the steady maintenance and incorporation of new ideas.\n\n\nAlso, sure you could say the described techniques quickly fail on eg sql tables but tbh the majority of rag use cases is standard txt/ pdf or something that is converted to pdf, thereby allowing some assumptions on the data formats",
              "score": 1,
              "created_utc": "2025-12-29 15:16:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwks3a7",
          "author": "TrustGraph",
          "text": "TrustGraph is not only open source, but is maintained (1.7 was just released and 1.8 is already in testing) and in production with users. TrustGraph has been pioneering the concept of context graphs (funny that term is just now catching on) for nearly 2 years. Best I can tell, we were also the first to use the term \"TemporalRAG\" as well. Our temporal features likely won't but coming until 2.0 though.\n\nStill open source & in active development: [https://github.com/trustgraph-ai/trustgraph](https://github.com/trustgraph-ai/trustgraph)",
          "score": 3,
          "created_utc": "2025-12-29 17:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjyc4l",
          "author": "OnyxProyectoUno",
          "text": "Every all-in-one platform I've seen makes the same mistake. They focus on retrieval orchestration while the real problems happen upstream during document processing.\n\nYou can have the most sophisticated reranking and context expansion in the world, but if your PDFs got mangled during parsing or your chunking strategy is splitting sentences mid-thought, you're building on quicksand. Most teams discover their chunking is broken only after they've already embedded everything and are debugging weird responses three conversations deep.\n\nThe platforms you mentioned treat document preprocessing as solved when it's actually where most RAG systems break. Tables get scrambled, metadata gets dropped, section hierarchy gets flattened. By the time you're looking at similarity scores you're three steps removed from the root cause.\n\nWhat you need isn't another orchestration layer. You need visibility into what your documents actually look like after parsing and chunking, before they hit the vector store. That's the angle I've been taking with [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_c), focusing specifically on the preprocessing pipeline rather than trying to be everything to everyone.\n\nThe enterprise platforms work because they control the entire stack and can see their preprocessing output. You can't debug what you can't see.",
          "score": 4,
          "created_utc": "2025-12-29 15:00:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk0pg3",
              "author": "Pl8tinium",
              "text": "I dont need to debug these platforms because they just work. if i would build something myself, i agree, a monitoring platform for the input data may be helpful.\n\n\nBut the core of my requirement is not monitoring because i dont want to build myself and there is no inherit rule that says that what the enterprise platforms do is not feasible for a FOSS project. They all start their pipelines with the same input eg a pdf",
              "score": 2,
              "created_utc": "2025-12-29 15:13:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkvchk",
          "author": "ChapterEquivalent188",
          "text": "Everyone is building \"Chat with PDF\" wrappers, but nobody is solving the deep ingestion engineering required for enterprise-grade reliability.\n\nI'm a solo dev and I just released the architecture manifest for RAG Enterprise Core V3.0. It‚Äôs an opinionated, \"batteries-included\" platform designed to solve the exact problems you mentioned (complex docs, tables, hallucinations).\n\nMy approach is different: Instead of trusting one ingestion method, I built a \"Multi-Lane Consensus Engine\" (Solomon).\n\nIt runs parallel extraction lanes:\n\nFast Lane: PyMuPDF (Text) \n\nSmart Lane: Docling (Structure/Markdown) \n\nVision Lane: VLM/Ollama (Charts & Images)\n\nso\n\nThe engine then votes on the \"Ground Truth\" and reconciles conflicts before indexing into a Neo4j Graph + ChromaDB Vector store. It also includes a \"Surgical HITL\" UI where you only verify the specific tokens the AI disagreed on.\n\nI haven't open-sourced the full code yet (IP reasons), but I just published the V3 Architecture Manifest and a Live Demo Video showing the consensus engine in action. It might give you some ideas for your own stack, or at least validate that you aren't crazy for wanting an \"all-in-one\" solution.\n\nCheck the Manifest & Demo here:\n\n[https://github.com/2dogsandanerd/RAG\\_enterprise\\_core](https://github.com/2dogsandanerd/RAG_enterprise_core)",
          "score": 2,
          "created_utc": "2025-12-29 17:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp32fh",
              "author": "Pl8tinium",
              "text": "damn that atleast sounds pretty cool, ill check it out!",
              "score": 2,
              "created_utc": "2025-12-30 07:56:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwk67sh",
          "author": "Horror-Turnover6198",
          "text": "I agree with OP and think it‚Äôs odd that there isn‚Äôt a single plug and play RAG box that would handle PDFs, text and markdown, with a standard ingestion and retrieval API. I know it‚Äôs not prohibitively hard to roll that but I know that I sure had to do a ton of reading and trial and error to even get started. \n\nMy guess is there just hasn‚Äôt been enough time for a standard framework to develop. For example, before Symfony, and later Laravel, PHP was a mishmash solving the same problems on every project. I don‚Äôt love every design decision of those frameworks but holy crap do they do a lot of heavy lifting to get new devs started. Why not have that sort of thing for RAG?",
          "score": 1,
          "created_utc": "2025-12-29 15:40:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl5j6i",
          "author": "remoteinspace",
          "text": "Have you tried mem0 or papr?",
          "score": 1,
          "created_utc": "2025-12-29 18:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp3n4l",
              "author": "Pl8tinium",
              "text": "mem0 sounds cool and i may consider it in addition to my RAG capabilities i wanna provide, thx!",
              "score": 1,
              "created_utc": "2025-12-30 08:02:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwn4iu5",
          "author": "GP_103",
          "text": "Proprietary platforms like MS 365 Copilot certainly does not work well with dense PDFs.",
          "score": 1,
          "created_utc": "2025-12-30 00:21:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp49he",
              "author": "Pl8tinium",
              "text": "ive had good experiences with docs that had atleast 200 pages, what are your experiences about its limits?",
              "score": 1,
              "created_utc": "2025-12-30 08:07:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqj9ni",
          "author": "RolandRu",
          "text": "There‚Äôs a real demand for an ‚Äúopinionated RAG black box,‚Äù but the reason it rarely exists (and stays SOTA) is that RAG is *mostly integration + evaluation*, not just a pipeline recipe.\n\nA platform has to pick defaults for: parsing, chunking, embedding model, hybrid retrieval, reranking, query routing, caching, ACLs, connectors, observability, and (hardest) **how you measure ‚Äúgood‚Äù** across wildly different corpora. The moment it‚Äôs opinionated, it breaks for someone‚Äôs data shape, compliance rules, latency budget, or cost ceiling ‚Äî and now the maintainer is on the hook.\n\nWhat tends to work in practice is ‚Äúopinionated core + pluggable edges‚Äù:  \na solid ingestion + ACL story\n\nhybrid retrieval + reranking as a default\n\nstrong eval harness (golden Q/A sets, regression tests, drift monitoring)\n\nconnectors as community modules\n\nan API-first design so you can pipe results into any frontend.\n\nIf you‚Äôre a small team, I‚Äôd aim for a maintained base stack + a thin layer of your own opinions (connectors + eval + guardrails). The eval layer is the part that keeps you ‚Äúnear SOTA‚Äù longer than chasing the newest chunking trick every month.",
          "score": 1,
          "created_utc": "2025-12-30 14:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvz2sf",
              "author": "Pl8tinium",
              "text": "this is the best reply so far IMO, thank you very much! Do you have oncrete examples what a maintained base layer may be/ look like? can you drop names?",
              "score": 2,
              "created_utc": "2025-12-31 09:19:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww0qqf",
                  "author": "RolandRu",
                  "text": "Maintained base layer = the ‚Äúboring but critical‚Äù foundations you don‚Äôt want to hand-roll: storage/search, ingestion/parsing, and eval/observability.\n\nConcrete names people commonly use:  \nQdrant / Weaviate / Milvus (vector DB)  \nHaystack / LlamaIndex / LangChain (RAG framework)  \nUnstructured (document parsing/ingestion)  \nLangfuse or Arize Phoenix (tracing/observability)  \nRagas / TruLens / DeepEval + promptfoo in CI (evaluation/regression)\n\nA typical maintained stack looks like:  \nUnstructured ‚Üí Qdrant (or Weaviate/Milvus) ‚Üí Haystack (or LlamaIndex) + Langfuse + Ragas/promptfoo.",
                  "score": 1,
                  "created_utc": "2025-12-31 09:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrl7mc",
          "author": "coderarun",
          "text": "This is a data centric view. The first step towards such an opinionated RAG is to have one database that does keyword/vector/graph searches well, runs embedded (no server to run) and gives you 80% of what you need. The focus in r/RAG tends to be on the python/TS package that runs on top of the database and these packages support 5-10 databases with vast differences in capability.\n\nWith coding models becoming more capable, developers can generate their own python/TS package to suit their needs. They're nowhere near writing their own database, indexing or query optimizations though.",
          "score": 1,
          "created_utc": "2025-12-30 17:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrxf1s",
          "author": "digital_legacy",
          "text": "What could we add to make our solution what you need? We have a local Docker setup, UI and plugable models and it's open source. [https://www.reddit.com/r/eMediaLibrary/](https://www.reddit.com/r/eMediaLibrary/)\n\nWe currently have drivers for two RAG approaches. LlamaIndex and [ThoughtFrame.ai](http://ThoughtFrame.ai)",
          "score": 1,
          "created_utc": "2025-12-30 18:40:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwekbz",
          "author": "Powerful-Ad-7237",
          "text": "Check out Piragi: https://github.com/hemanth/piragi\n\nFeatures\n\nZero Config - Works with free local models out of the box\nAll Formats - PDF, Word, Excel, Markdown, Code, URLs, Images, Audio\nRemote Storage - Read from S3, GCS, Azure, HDFS, SFTP with glob patterns\nWeb Crawling - Recursively crawl websites with /** syntax\nAuto-Updates - Background refresh, queries never blocked\nSmart Citations - Every answer includes sources\nPluggable Stores - LanceDB, PostgreSQL, Pinecone, Supabase, or custom\nAdvanced Retrieval - HyDE, hybrid search, cross-encoder reranking\nSemantic Chunking - Context-aware and hierarchical chunking\nKnowledge Graph - Entity/relationship extraction for better answers\nAsync Support - Non-blocking API for web frameworks",
          "score": 1,
          "created_utc": "2025-12-31 11:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4b7sw",
          "author": "hhussain-",
          "text": "I believe this is because of maturity level in RAG and AI in general. There are many missing keys, and community don't have that leverage of putting the R&D time without ROI which is main concern in FOSS. For sure FOSS have the required skillset, but you can imagine what would happen if a key is released as FOSS. Motivation is hard.",
          "score": 1,
          "created_utc": "2026-01-01 18:44:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q71dio",
      "title": "RAG retrieval debugging is a nightmare. So I trained a model to fix it",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q71dio/rag_retrieval_debugging_is_a_nightmare_so_i/",
      "author": "ProfessionalLaugh354",
      "created_utc": "2026-01-08 03:49:14",
      "score": 13,
      "num_comments": 13,
      "upvote_ratio": 0.84,
      "text": "**TL;DR:** Manually verifying RAG retrieval quality is painful. What if we could automatically highlight which sentences actually answer the query? Sharing my approach using semantic highlighting. Looking for feedback on better solutions.\n\n# The actual problem I face every day\n\nHere's my workflow when debugging RAG:\n\n1. Query retrieves top-10 documents\n2. I need to verify if they're actually relevant\n3. Read document 1... document 2... document 3...\n4. Realize document 7 is complete garbage but my retriever ranked it high\n5. Cry\n\n**If I don't manually verify, those irrelevant chunks become context pollution for my LLM. The model gets distracted, answer quality drops,** and I have no idea why.\n\nBut manual verification doesn't scale. I'm not reading through 10 documents for every test query.\n\n# What if we could automatically see which sentences actually answer the query?\n\nHere's what I need: a model that can highlight exactly which sentences in each retrieved document are relevant to my query. Not keyword matching‚Äîactual semantic understanding.\n\nThis would enable:\n\n**1. Explainability**: Instantly see WHY a document was retrieved. Which sentences actually match my query? Is it relevant or did the retriever mess up?\n\n**2. Debugging**: When RAG fails, trace it back. \"Oh, the right document was found but the relevant sentence is buried at the end. Maybe I need better chunking.\"\n\n**3. Context pruning**: Send only highlighted sentences to the LLM instead of entire documents. Reduces context pollution and token costs.\n\n**4. Automated evaluation**: Score retrieval quality based on highlight coverage, or even auto-rerank results without manual review.\n\nThis is what semantic highlighting does. It understands meaning, not just literal text matches.\n\nTraditional highlighting (like Elasticsearch) can't do this. It only matches keywords. Search \"how to optimize database queries\" and it highlights \"database\" and \"queries\" everywhere, completely missing sentences like \"add an index on frequently joined columns\"‚Äîthe actual answer.\n\n# My attempt at solving this\n\nSo I tried training a semantic highlighting model. The idea: understand meaning, not just keywords.\n\nThe approach:\n\n* Generated 5M+ training samples using LLMs (with reasoning chains for better quality)\n* Fine-tuned BGE-M3 Reranker v2 (0.6B params, 8K context window)\n* Took \\~9 hours on 8x A100s\n\nNot sure if this is the best approach, but it's been working for my use cases.\n\nI put the model weights on HuggingFace:¬†[https://huggingface.co/zilliz/semantic-highlight-bilingual-v1](https://huggingface.co/zilliz/semantic-highlight-bilingual-v1)\n\n# How it works in practice\n\nHere's a real example of what this enables:\n\n**Query**: \"How to reduce memory usage in Python?\"\n\n**Top 3 retrieved documents**:\n\n**Doc 1** (Python optimization guide): \"Python's garbage collector automatically manages memory. *Use generators instead of lists for large datasets‚Äîthey compute values on-the-fly without storing everything in memory.* Global variables persist throughout program execution. *The* `del` *keyword can explicitly remove references to free up memory.*\"\n\n**Doc 2** (Data structures tutorial): \"Lists are the most common data structure in Python. They support append, insert, and remove operations. *For memory-intensive applications, consider using* `__slots__` *in classes to reduce per-instance memory overhead.* Lists can contain mixed types.\"\n\n**Doc 3** (Debugging guide): \"Use print statements to debug your code. The `pdb` module provides interactive debugging. Check variable values at breakpoints to find issues.\"\n\n**Highlighted sentences** (shown in *italics* above):\n\n* Doc 1: 2 relevant sentences ‚Üí High relevance ‚úì\n* Doc 2: 1 relevant sentence ‚Üí Partially relevant ‚úì\n* Doc 3: No highlights ‚Üí **Not relevant, retriever error** ‚úó\n\nWith semantic highlighting, I can quickly spot:\n\n* Doc 1 and 2 have useful information (generators, `del`, `__slots__`)\n* Doc 3 is off-topic‚Äîretriever mistake\n* Can extract just the highlighted parts (150 words ‚Üí 50 words) for the LLM\n\nTakes maybe 5 seconds vs reading 3 full docs. Not perfect, but way better than my old workflow.\n\n# Initial results look promising\n\nOn benchmarks, it's performing better than existing solutions (OpenSearch, Provence variants), but I'm more interested in real-world feedback.\n\n# What I'm curious about\n\n1. **How do you currently debug RAG retrieval?** Manual inspection? Automated metrics? Something else?\n2. **Would this actually be useful in your workflow?** Or is there a better approach I'm missing?\n3. **For context pruning**: Do you send full documents to your LLM, or do you already filter somehow?\n\nThere's a preview model on HF that people have been testing. But honestly just want to hear if this resonates with others or if I'm solving a problem that doesn't exist.\n\nAnyone working on similar RAG observability challenges?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q71dio/rag_retrieval_debugging_is_a_nightmare_so_i/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyca2vi",
          "author": "bravelogitex",
          "text": "Can't you use LLM as a judge or human in the loop with a eval framework like langfuse?",
          "score": 2,
          "created_utc": "2026-01-08 04:30:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyct70w",
              "author": "jael_m",
              "text": "That would be slow and expensive for your RAG in production.",
              "score": 1,
              "created_utc": "2026-01-08 06:45:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nycx6ok",
                  "author": "bravelogitex",
                  "text": "it's just for the evals portoin",
                  "score": 5,
                  "created_utc": "2026-01-08 07:18:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyfomfy",
                  "author": "Altruistic_Leek6283",
                  "text": "this is why RAG is expensive.",
                  "score": 1,
                  "created_utc": "2026-01-08 17:38:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nyfohd1",
          "author": "Altruistic_Leek6283",
          "text": "OMG!! YOU CAN'T DO THAT! \n\nIts not valid! You can't use a LLM for that. Please... \n\nhttps://preview.redd.it/wwiz15y1w5cg1.png?width=911&format=png&auto=webp&s=9057269f1bb87b413e8337eddded418f5edc7ce2",
          "score": 1,
          "created_utc": "2026-01-08 17:37:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyd1her",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2026-01-08 07:56:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyds5n6",
              "author": "Tough-Percentage-864",
              "text": "Why not create contextual¬†chunks? RAG fails most of the time because chunks lose context I tried¬†to fix it--> [https://github.com/Abhay-404/Eternal-Contextual-RAG](https://github.com/Abhay-404/Eternal-Contextual-RAG)",
              "score": 1,
              "created_utc": "2026-01-08 11:49:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nydxlid",
                  "author": "ProfessionalLaugh354",
                  "text": "Yes, chunking with contextual information is a good way to improve the retrieval performance",
                  "score": 1,
                  "created_utc": "2026-01-08 12:28:17",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "nyfv0i7",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, contextual chunks are the right direction. I looked at your approach and the idea of maintaining document hierarchy and surrounding context makes sense. That's exactly what I mean about upstream processing being critical.\n\nThe issue I see with most contextual chunking implementations is they still treat it as a preprocessing step that happens once and gets forgotten. Your approach with the hierarchical structure is better, but I'm wondering how you handle cases where the same content needs different context depending on the query type. Like a code example that's relevant for both performance questions and debugging questions, but needs different surrounding context highlighted for each.\n\nThe other thing is validation. How do you actually verify that your contextual chunks are working better than standard chunking? Most people just assume context helps without measuring whether their specific context strategy actually improves retrieval for their domain.",
                  "score": 1,
                  "created_utc": "2026-01-08 18:05:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyd6j2j",
              "author": "ProfessionalLaugh354",
              "text": "You are right. Chunking is indeed another critical area that needs our focus. The truth is, there are countless optimization details to nail down before RAG can be smoothly deployed to production",
              "score": 0,
              "created_utc": "2026-01-08 08:41:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3od2c",
      "title": "Email threads broke every RAG approach I tried. Here‚Äôs what finally worked",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q3od2c/email_threads_broke_every_rag_approach_i_tried/",
      "author": "EnoughNinja",
      "created_utc": "2026-01-04 12:06:17",
      "score": 13,
      "num_comments": 14,
      "upvote_ratio": 0.81,
      "text": "Ok so i've been building RAG pipelines for about a year.\n\nDocuments? fine.  \nNotion dumps? manageable.  \nEmail threads? absolute nightmare. Genuinely the worst data source i've worked with.\n\nHere‚Äôs what i tried and why each one failed:\n\n* **chunking by message** ‚Üí garbage. you lose conversation state. the LLM has no idea msg #7 is a reply to msg #3, not msg #6.  \n* **embedding whole threads** ‚Üí hits token limits instantly on anything real. also the model gets distracted because half the content is signatures + legal disclaimers.  \n* **strip signatures then chunk** ‚Üí better, but then quoted text kills you. people reply inline, edit quotes, forward with additions. my dedupe either removed important context or kept duplicate garbage.  \n\nBreaking point: a 25-message reply-all chain from a client. Retrieval kept returning the wrong messages because semantically every email looked identical because the same company footer was dominating the embedding.\n\n**What actually helped**:\n\nI stopped treating email like a document retrieval problem and started treating it as **graph reconstruction**.\n\nThe pipeline now:\n\n1. sync newest ‚Üí oldest (important: you need the final state first)  \n2. map In-Reply-To headers to build the actual conversation tree  \n3. dedupe quoted text **but preserve inline edits** (harder than expected)  \n4. extract structured metadata (decisions / tasks / owners) and embed *that* alongside cleaned text  \n\nI validated this on \\~200 threads i manually labeled for ‚Äúdid retrieval surface the correct part of the thread‚Äù.\n\n**Results on my set:**\n\n* naive chunking: \\~47%  \n* graph reconstruction + extraction: \\~91%  \n\nStill not perfect. the remaining failures are mostly:\n\n* forwarded threads where headers get stripped  \n* people replying to old messages mid-thread  \n* chains that fork + merge  \n\nif anyone else is doing RAG on comms data: what edge cases are killing you? would love to compare notes.\n\n(context: i‚Äôm building this at [iGPT](https://www.igpt.ai/), which is why i‚Äôm obsessed with it. if people want to poke holes in the approach, i can share more details / examples.)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q3od2c/email_threads_broke_every_rag_approach_i_tried/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxmadu2",
          "author": "RolandRu",
          "text": "I‚Äôm doing RAG for codebases (.NET), and your ‚Äústop treating it like documents ‚Üí treat it like graph reconstruction‚Äù matches what I‚Äôve seen.\n\nIn code, chunk-only retrieval fails because the real context is the dependency structure, not nearby text. My approach:\n\nembed chunks to find an entry point (semantic or BM25), then expand via a dependency graph (call graph / type refs / module deps) to pull the connected context, and if that expansion blows the token budget, I run a query-focused summarizer over the retrieved evidence to fit a fixed limit.\n\nFeels analogous to email threads: the challenge isn‚Äôt ‚Äúsimilarity search‚Äù, it‚Äôs reconstructing the right slice/path of the thread/code.\n\nCurious: for threads with stripped headers, have you tried inferring edges from quoted blocks / inline replies (kind of like building edges from weak signals)?",
          "score": 3,
          "created_utc": "2026-01-04 13:04:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxn3d0r",
          "author": "Altruistic_Leek6283",
          "text": "I never thought about E-mail RAG, but you nailed when you decided to treat as a graph reconstruction. \n\nThanks for sharing.",
          "score": 3,
          "created_utc": "2026-01-04 15:48:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpk6vv",
              "author": "bravelogitex",
              "text": "email RAG seems like a huge use case. some industries run off email",
              "score": 3,
              "created_utc": "2026-01-04 22:33:54",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxnpac9",
          "author": "OnyxProyectoUno",
          "text": "The graph reconstruction approach is the right call. Most people never get past the \"chunk by message\" phase because they're treating email like it's just another document format when it's actually structured communication with explicit relationships baked into the headers.\n\nYour remaining failures point to a harder problem though. Forwarded threads with stripped headers and mid-thread replies to old messages are both cases where the metadata you're relying on doesn't exist or lies to you. The fork-and-merge chains are even worse because you're trying to reconstruct a DAG from data that was never designed to preserve that structure.\n\nOne thing that helped me on similar comms data: extract the quoted text blocks and use fuzzy matching against earlier messages in the thread to rebuild relationships when headers fail. It's slow and imperfect but catches maybe 60% of the forwarded thread cases. For the inline edits you mentioned, diffing the quoted block against the original message and only embedding the delta can clean up a lot of noise.\n\nThe metadata extraction step is where I'd focus next. Decisions and tasks are high signal, but you might be missing action items that are implicit in the thread structure itself. Someone replying \"sounds good\" to a proposal is a decision even if it doesn't contain decision keywords. That's the kind of thing you can iterate on quickly if you can preview what your extraction looks like before embedding, which is what I built vectorflow.dev to handle. Curious whether you're seeing false positives on your task extraction or mostly false negatives.",
          "score": 3,
          "created_utc": "2026-01-04 17:30:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxm88p1",
          "author": "ChapterEquivalent188",
          "text": "garbage in, garbage out",
          "score": 4,
          "created_utc": "2026-01-04 12:49:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxm47z3",
          "author": "Mindless_Copy_7487",
          "text": "How can documents be no problem?\nDo you really support arbitrary company knowledge bases with arbitrary pdf, office files, slides, diagrams etc.?\n\nIn case you outsourced that to docling: how do you handle the bad performance on local hardware, given even smaller SMEs have 100.000+ documents?\n\nI would say emails are the much easier task and yes, treating it as a graph problem is the way to go",
          "score": 1,
          "created_utc": "2026-01-04 12:18:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxm50rs",
              "author": "EnoughNinja",
              "text": "I‚Äôm not claiming documents are ‚Äúeasy‚Äù in the abstract,  I mean they‚Äôre engineering-solved enough compared to email. \n\nMost company files (PDFs, Office, slides) can be normalized into a structured text representation and retrieved reliably at scale with a sane ingestion pipeline.\n\nEmail, by contrast, isn‚Äôt a document problem at all, it‚Äôs mutable state plus a conversation graph: replies to old messages, inline edits, forwards that strip headers, fork-and-merge threads, and duplicated footers that dominate embeddings. Treating email like static text makes retrieval confidently wrong. That‚Äôs why graph reconstruction plus metadata extraction works better, even if it still has edge cases.",
              "score": 1,
              "created_utc": "2026-01-04 12:24:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxm5n5u",
                  "author": "Mindless_Copy_7487",
                  "text": "Did you ever see a real document base from an actual company? What size was it?",
                  "score": 1,
                  "created_utc": "2026-01-04 12:29:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxx9wrh",
          "author": "Academic_Track_2765",
          "text": "build a knowledge graph. the email metadata will be extremely helpful for KG for email use case. Yes I also use graph for emails, but also for interrelated documents, which are similar in nature.",
          "score": 1,
          "created_utc": "2026-01-06 01:04:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny89y8p",
          "author": "Accomplished_Life416",
          "text": "Op , I want to know about your email Rag work basically if possible please share what you do in email because I have same quotation setup in email and I automated a lot ,so I am also going with rag later of using email data , \n\nSo can you give a little bit idea that what are you doing",
          "score": 1,
          "created_utc": "2026-01-07 17:01:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nycx7hd",
              "author": "EnoughNinja",
              "text": "Essentially, we treat email as a graph problem rather than a document problem. The key things that make a difference are thread reconstruction using header metadata and cleaning quoted text while preserving inline edits (this is trickier than it sounds because people quote-reply in inconsistent ways)\n\nIf you want to see how we're approaching the structured extraction piece, check out [https://docs.igpt.ai/](https://docs.igpt.ai/) there's a section on how we think about \"email intelligence\" vs. just email retrieval.",
              "score": 1,
              "created_utc": "2026-01-08 07:19:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyczzxv",
                  "author": "Accomplished_Life416",
                  "text": "Igpt is Api that you build or it is open-source that you are integrating in email with Rag ?\n\nMy main question is that what kind of conversation do you handle on email one by one",
                  "score": 1,
                  "created_utc": "2026-01-08 07:43:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q5hyl2",
      "title": "Recommended tech stack for RAG?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q5hyl2/recommended_tech_stack_for_rag/",
      "author": "ProtectedPlastic-006",
      "created_utc": "2026-01-06 13:01:45",
      "score": 13,
      "num_comments": 17,
      "upvote_ratio": 0.93,
      "text": "Trying to build out a retrieval-augmented generation (RAG) system without much of an idea of the different tools and tech out there to accomplish this. Would love to know what you recommend in terms of DB, language to make the calls and what LLM to use?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q5hyl2/recommended_tech_stack_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny01m84",
          "author": "fabkosta",
          "text": "Without context this is a rather meaningless question. For example, if I recommend you to use Elasticsearch running on Kubernetes - do you have the experience and the team to maintain that?\n\nIn any case, here's a solid choice for self-hosting:\n\n1. Use PostgreSQL with pgvector module installed as a vector database. Prefer cloud-hosting? Use Pinecone instead. Have a gigantic amount of data (I hope not)? Use Elasticsearch running on Kubernetes.\n2. Make sure to use hybrid search always (text + vector, then combine with RRF)\n3. For the backend you may want to write your own code given it's so simple (no need to pick e.g. Langchain or Langgraph or others, keep it as simple as possible)\n4. As a frontend you may want to look into e.g. Librechat or OpenWebUI.\n5. Use Docling for document OCRing and text extraction.\n6. Use a cloud-based SaaS (OpenAI's GPT models) to create both embedding vectors and result summarization.",
          "score": 13,
          "created_utc": "2026-01-06 13:09:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny03ubc",
              "author": "notAllBits",
              "text": "This, and many alternatives. Also: what is your use case? What type of data will you process? What requirements do you have for consent tracking, data product isolation, intent guardrailing, etc... and which criteria would you evaluate retrieval against?",
              "score": 3,
              "created_utc": "2026-01-06 13:22:32",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "ny03x0c",
              "author": "ProtectedPlastic-006",
              "text": "Some context: not too much data, I would say maybe about 6K max pages of PDF (is that a lot?). Essentially want to upload a bunch of construction code docs and create a RAG around them. Have experience as an SWE and in AWS. Will be doing this project completely on my own. Once the data is uploaded don‚Äôt see it changing for quite some time so it isn‚Äôt a continuously added to knowledge base.",
              "score": 1,
              "created_utc": "2026-01-06 13:22:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny6cawb",
                  "author": "DesignerTerrible5058",
                  "text": "for 6k pages you could expect 10k-20k chunks, 10k-20k embeddings and Vector data base size of a few hundred MB. I would guess 3-10 chunks retrieved per query. This is hoping your PDFs are properly OCR'd and easily chunkable.",
                  "score": 1,
                  "created_utc": "2026-01-07 10:16:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny03mni",
          "author": "bzImage",
          "text": "Docling + llm chunking/shaping/keyword extraction  + Langgraph + react + qdrant with keyword/metadata/dense/sparse/hybrid vector search",
          "score": 2,
          "created_utc": "2026-01-06 13:21:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny04psw",
              "author": "phizero2",
              "text": "This, but imo do 2 level retrieval, chunks for looking up information while pages for retrieving information.\n\nAlso, docling is very expensive and not very accurate, try API tools since they are cheap",
              "score": 1,
              "created_utc": "2026-01-06 13:27:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "ny080pl",
                  "author": "bzImage",
                  "text": "Docling running locally it's expensive? How ?",
                  "score": 1,
                  "created_utc": "2026-01-06 13:46:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "ny0bgrv",
          "author": "lucido_dio",
          "text": "Start as simple as possible and add complexity only when needed. Frameworks like Langchain will only clutter your understanding, keep it as lean as possible. Get the basic version running with bare tools: typescript, OpenAI api (or any other LLM provider you wanna use). I recommend pgvector since it's so easy to work with but you can go easier with Needle's RAG API: [https://docs.needle.app/](https://docs.needle.app/)",
          "score": 2,
          "created_utc": "2026-01-06 14:05:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny01k51",
          "author": "Interesting-Gap-1868",
          "text": "!RemindMe 3days",
          "score": 1,
          "created_utc": "2026-01-06 13:08:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny01nl9",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 3 days on [**2026-01-09 13:08:51 UTC**](http://www.wolframalpha.com/input/?i=2026-01-09%2013:08:51%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Rag/comments/1q5hyl2/recommended_tech_stack_for_rag/ny01k51/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FRag%2Fcomments%2F1q5hyl2%2Frecommended_tech_stack_for_rag%2Fny01k51%2F%5D%0A%0ARemindMe%21%202026-01-09%2013%3A08%3A51%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201q5hyl2)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-06 13:09:26",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny0rul7",
          "author": "digital_legacy",
          "text": "We created a UI and use Docker with LlamaIndex. Check out our channel: [https://www.reddit.com/r/eMediaLibrary/](https://www.reddit.com/r/eMediaLibrary/)",
          "score": 1,
          "created_utc": "2026-01-06 15:29:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny0s666",
              "author": "digital_legacy",
              "text": "eMedia (DAM/RAG/AI) stack is all inclusive, totally open source and self hosted",
              "score": 1,
              "created_utc": "2026-01-06 15:30:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "ny16j9c",
          "author": "ChapterEquivalent188",
          "text": "how about starting with basic knowledge ? sorry but this is most effortless approach i ever read...",
          "score": 1,
          "created_utc": "2026-01-06 16:36:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nydkwam",
          "author": "valerione",
          "text": "For PHP folks I suggest to take a look at the Neuron AI RAG component: https://docs.neuron-ai.dev/rag/rag",
          "score": 1,
          "created_utc": "2026-01-08 10:51:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny093bs",
          "author": "RunAlvinRun69",
          "text": "Educate yourself on the subject. Watch several hours (per day)of YouTube tutorials on RAG. You'll get out of it what you put into it. Bty, the customer acquisition part of your endeavor will be the most, shall I see, interesting",
          "score": 0,
          "created_utc": "2026-01-06 13:52:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q86y9d",
      "title": "New to RAG ‚Äî what does a production-ready stack look like (multi-user, concurrency, Graph RAG)?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q86y9d/new_to_rag_what_does_a_productionready_stack_look/",
      "author": "tchikss",
      "created_utc": "2026-01-09 12:26:17",
      "score": 13,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôm new to Retrieval-Augmented Generation (RAG) and will soon be starting a project focused on designing a production-ready RAG system. I‚Äôm currently learning the basics and trying to understand what real-world RAG architectures look like beyond toy examples.\n\nI‚Äôm especially interested in:\n\t‚Ä¢\tWhat a production-grade RAG stack typically includes\n(LLMs, embedding models, vector DBs, orchestration, serving)\n\t‚Ä¢\tHow production systems handle multiple users at once\n\t‚Ä¢\tConcurrency & scaling concerns (async pipelines, batching, caching, etc.)\n\t‚Ä¢\tCommon bottlenecks in practice (retrieval vs LLM vs embeddings)\n\nBeyond na√Øve RAG, I‚Äôm also planning to explore:\n\t‚Ä¢\tGraph RAG / knowledge-graph-augmented RAG\n\t‚Ä¢\tHybrid approaches (BM25 + dense retrieval)\n\t‚Ä¢\tOther alternatives or improvements over standard chunk-and-retrieve pipelines\n\nSince I‚Äôm early in my learning, I‚Äôd really appreciate:\n\t‚Ä¢\tArchitectural advice\n\t‚Ä¢\tThings you wish you had known early\n\t‚Ä¢\tOpen-source projects, blog posts, or papers worth studying\n\t‚Ä¢\tPitfalls to avoid when moving from prototypes to production\n\nThanks in advance ‚Äî happy to learn from your experience!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q86y9d/new_to_rag_what_does_a_productionready_stack_look/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nym9pex",
          "author": "remoteinspace",
          "text": "have you checked out mem0, papr, graphiti? all open source",
          "score": 6,
          "created_utc": "2026-01-09 16:12:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyn07fp",
              "author": "tchikss",
              "text": "I will do that, many thanks !",
              "score": 1,
              "created_utc": "2026-01-09 18:11:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nymf48k",
          "author": "herzo175",
          "text": "Something that gets overlooked a lot is access control. You'll want to make sure users from one group, such as \"marketing\", can't ask \"where does xxx live?\" while still providing access to other groups like \"adjusters\". Contrived example but you get the point.",
          "score": 3,
          "created_utc": "2026-01-09 16:37:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nymysa4",
              "author": "iandavidbrearley",
              "text": "Definitely, access control is crucial. Consider implementing role-based access control (RBAC) to manage permissions effectively across different user groups. Also, think about logging queries for auditing and to identify potential misuse.",
              "score": 2,
              "created_utc": "2026-01-09 18:05:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyn0wty",
                  "author": "tchikss",
                  "text": "Will check that out! Thank u !",
                  "score": 1,
                  "created_utc": "2026-01-09 18:14:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "nyn0t2h",
              "author": "tchikss",
              "text": "I guess that it is a must thing to see, I thank u about mentioning it ! But in my case, I guess I will be having only one kind of users, so I‚Äôm not gonna face this problem ( but I would like some ressources on that haha ! )",
              "score": 1,
              "created_utc": "2026-01-09 18:14:20",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyq0qyw",
              "author": "OnyxProyectoUno",
              "text": "I literally wrote about this same issue [here](https://nickrichu.me/posts/why-your-rag-will-fail-enterprise-security-review)\n\n1. Who can access what documents through this system?\n2. If someone‚Äôs permissions change in SharePoint, how long until the RAG reflects that?\n3. What stops User A from seeing documents they‚Äôre not authorized to?",
              "score": 1,
              "created_utc": "2026-01-10 03:11:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nymmjrl",
          "author": "hrishikamath",
          "text": "Project on production ready RAG? Production ready is very subjective and vague. It totally depends on end user and requirements right ? You can have 10 users or a million users. In the former case everything is closer to a toy.",
          "score": 3,
          "created_utc": "2026-01-09 17:10:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyn05wg",
              "author": "tchikss",
              "text": "Haha maybe my question was vague a lil, but I wanted to know what can be done more than what we see in demos, all tutos talk about some simple and easy things and everywhere I see other people talking about many problems in real production systems, so I want to know what is the minimal and maybe other advanced things ( like million of users as u ve mentioned) which I should see before beginning my journey",
              "score": 1,
              "created_utc": "2026-01-09 18:11:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nynyubm",
          "author": "ChapterEquivalent188",
          "text": "yeah, pretty much buzzword bingo ;) new to rag but sounds like a allready existing platform ;)\n\n  \ndont know but sounds pretty much like this here...[RAG\\_enterprise\\_core](https://github.com/2dogsandanerd/RAG_enterprise_core). good luck with your enterprise level rag",
          "score": 1,
          "created_utc": "2026-01-09 20:48:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyo0n1n",
              "author": "tchikss",
              "text": "Haha thanks for sharing !",
              "score": 1,
              "created_utc": "2026-01-09 20:56:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyuhwyf",
          "author": "Dev-it-with-me",
          "text": "You can checkout my two projects and YouTube explaining how they are build and how it works from theory to deployment for both Graph RAG and Classic Naive RAG: https://youtu.be/0kVT1B1yrMc?si=qoLlufV458WL2bDF & https://youtu.be/TqeOznAcXXU?si=JzuB6RB0lSILsnjD",
          "score": 1,
          "created_utc": "2026-01-10 20:25:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2gq6z",
      "title": "RAG, Knowledge Graphs, and LLMs in Knowledge-Heavy Industries - Open Questions from an Insurance Practitioner",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q2gq6z/rag_knowledge_graphs_and_llms_in_knowledgeheavy/",
      "author": "PlanktonPika",
      "created_utc": "2026-01-03 01:15:28",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 0.8,
      "text": "RAG, knowledge graphs (KG), LLMs, and \"AI\" more broadly are increasingly being applied in knowledge-heavy industries such as healthcare, law, insurance, and banking.\n\nI‚Äôve worked in the insurance domain since the mainframe era, and I‚Äôve been deep-diving into modern approaches: RAG systems, knowledge graphs, LLM fine-tuning, knowledge extraction pipelines, and LLM-assisted underwriting workflows. I‚Äôve built and tested a number of prototypes across these areas.\n\nWhat I‚Äôm still grappling with is this: **from an enterprise, production-grade perspective, how do these systems realistically earn trust and adoption from the business?**\n\nTwo concrete scenarios I keep coming back to:\n\n# Scenario 1: Knowledge Management\n\nInsurance organisations sit on enormous volumes of internal and external documents - guidelines, standards, regulatory texts, technical papers, and market materials.\n\nMuch of this ‚Äúknowledge‚Äù is:\n\n* High-level and ambiguous\n* Not formalised enough to live in a traditional rules engine\n* Hard to search reliably with keyword systems\n\nThe goal here isn‚Äôt just faster search, but **answers the business can trust,** answers that are accurate, grounded, and defensible.\n\nQuestions I‚Äôm wrestling with:\n\n* Is a pure RAG approach sufficient, or should it be combined with explicit structure such as ontologies or knowledge graphs?\n* How can fluent but subtly incorrect answers be detected and prevented from undermining trust?\n* From an enterprise perspective, what constitutes ‚Äúgood enough‚Äù performance for adoption and sustained use?\n\n# Scenario 2: Underwriting\n\nMany insurance products are non-standardised or only loosely standardised.\n\nUnderwriting in these cases is:\n\n* Highly manual\n* Knowledge- and experience-heavy\n* Inconsistent across underwriters\n* Slow and expensive\n\nThe goal is not full automation, but to **shorten the underwriting cycle** while producing outputs that are:\n\n* Reliable\n* Reasonable\n* Consistent\n* Traceable\n\nHere, the questions include:\n\n* Where should LLMs sit in the underwriting workflow?\n* How can consistency and correctness be assured across cases?\n* What level of risk control should be incorporated?\n\nI‚Äôm interested in hearing from others who are building, deploying, or evaluating RAG/KG/LLM systems in regulated or knowledge-intensive domains:\n\n* What has worked in practice?\n* Where have things broken down?\n* What do you see as the real blockers to enterprise adoption?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q2gq6z/rag_knowledge_graphs_and_llms_in_knowledgeheavy/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxrpz6i",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-05 05:34:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxxip7p",
          "author": "fustercluck6000",
          "text": "The bulk of my work in the last year has been on precisely these sorts of projects where 1) the client‚Äôs in a ‚Äòknowledge-heavy‚Äô industry where AI stands to make a major difference in terms of efficiency, and 2) accuracy isn‚Äôt just desirable, it‚Äôs a matter of liability.\n\nDomain knowledge is EVERYTHING. One of the most helpful things I‚Äôve found is taking the time to pick people‚Äôs brains about their work. Sometimes I‚Äôve even sat behind someone to literally be a fly on the wall and take notes on how they do their job because I want to know how they‚Äôre *thinking*. \n\nUsually, that ends up completely changing how I break down what I‚Äôm trying to solve with RAG, and you can make systems much more reliable/accurate by designing pipelines that reflect domain logic. Chunking‚Äôs a great example‚Äîhow you define a ‚Äòminimum logical unit‚Äô has a huge impact on retrieval accuracy, and almost always requires some intuition about what the data *means*.\n\nI also find hardcoding wherever possible makes things much more predictable and stable. If you can identify industry ‚Äòheuristics‚Äô, ‚Äònorms‚Äô, ‚Äòbest practices‚Äô, etc‚Ä¶, take that logic and apply it to the relevant part of the system (could be retrieval logic, node/edge types, etc). Also knowledge graphs are a total game changer because they provide another dimension for you to express domain logic with system design.",
          "score": 1,
          "created_utc": "2026-01-06 01:51:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxyopl6",
              "author": "PlanktonPika",
              "text": "Thank you sharing.\nCan you share what scenarios your AI projects worked on and what outputs they produce? How did you create and maintain knowledge graphs and what queries did they provide answers to, e.g., knowledge text or reasoning/decision-making using the knowledge?",
              "score": 1,
              "created_utc": "2026-01-06 06:13:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q3j5wb",
      "title": "Fully Offline Terminal RAG for Document Chat",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q3j5wb/fully_offline_terminal_rag_for_document_chat/",
      "author": "Apprehensive_Cell_48",
      "created_utc": "2026-01-04 06:55:37",
      "score": 12,
      "num_comments": 8,
      "upvote_ratio": 0.94,
      "text": "Hi, I want to build my own RAG system, which will be fully offline, where I can chat with my PDFs and documents. The files aren‚Äôt super large in number or size. It will be terminal-based: I will run it on my machine via my IDE and chat with it. The sole purpose is to use it as my personal assistant.\n\nPlease suggest me some very good resources so that i can build on my own. Also which Ollama LLM will be the best in this case or any alternatives?\nüôè",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q3j5wb/fully_offline_terminal_rag_for_document_chat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxlaf3z",
          "author": "GP_103",
          "text": "There is a FOSS version of NotebookLM,  see if that solves for it",
          "score": 3,
          "created_utc": "2026-01-04 07:54:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrh5wl",
              "author": "bravelogitex",
              "text": "I assume you mean [https://github.com/lfnovo/open-notebook](https://github.com/lfnovo/open-notebook)",
              "score": 2,
              "created_utc": "2026-01-05 04:36:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxpfna4",
          "author": "OnyxProyectoUno",
          "text": "The document processing side matters more than the LLM choice for personal RAG. You'll hit quality issues with chunking and parsing long before model limitations bite you.\n\nFor offline terminal RAG, start with LangChain or LlamaIndex as your base framework. Both have solid PDF handling and work well with Ollama. For the LLM, Llama 3.1 8B or Qwen2.5 7B are good starting points for document QA. They're fast enough for local use and handle reasoning reasonably well.\n\nThe real work is in document processing. PDFs are tricky - tables get mangled, headers disappear, and you lose document structure during parsing. I've been building tooling around this at vectorflow.dev because most RAG problems trace back to bad preprocessing, not retrieval issues.\n\nStart simple: basic recursive chunking with 1000 token chunks and 200 token overlap. Use sentence-transformers for embeddings (all-MiniLM-L6-v2 works offline). ChromaDB for your vector store since it's lightweight and doesn't need a server.\n\nWatch out for PDF parsing destroying your document hierarchy. You'll lose section context and get weird retrieval results. Test your chunking output manually before building the chat interface. Most people skip this step and wonder why their assistant gives confusing answers later.\n\nWhat kind of documents are you planning to work with? Technical docs behave differently than general text.",
          "score": 2,
          "created_utc": "2026-01-04 22:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxrmtt3",
              "author": "Apprehensive_Cell_48",
              "text": "Thanks a lot for your well described answer. I will definitely utilise your suggestions ‚ô•Ô∏è",
              "score": 1,
              "created_utc": "2026-01-05 05:12:35",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxrnoxx",
              "author": "Apprehensive_Cell_48",
              "text": "I was kind of playing around things like with documentations for products, manuals of various things etc.",
              "score": 1,
              "created_utc": "2026-01-05 05:18:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxmdayy",
          "author": "LiaVKane",
          "text": "If you intent to use it not for large scale (as corporation) but for small teams - feel free to request elDoc community version https://eldoc.online/blog/llm-rag-for-secure-on-premise-file-management/ it has it all for chatting with your documents fully offline, in secured way.",
          "score": 1,
          "created_utc": "2026-01-04 13:23:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxmk6z8",
          "author": "RobfromHB",
          "text": "Pick a light weight open source model. Chunk and store your embeddings in an SQLite db. Do similarly search to return some chunks. If you‚Äôre doing this small scale don‚Äôt even worry about a vector db. Your look up time will be linear, but for personal use and not much data it will still work plenty fast on a laptop.\n\nYou don‚Äôt need to worry about optimizing for the best model with something like this. Just get a basic set up and you‚Äôll be fine.",
          "score": 1,
          "created_utc": "2026-01-04 14:06:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxrrkwu",
          "author": "vinoonovino26",
          "text": "Try nexa.ai hyperlink",
          "score": 1,
          "created_utc": "2026-01-05 05:46:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q5vn4g",
      "title": "RAG tip: stop ‚Äúfixing hallucinations‚Äù until the system can ASK / UNKNOWN",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q5vn4g/rag_tip_stop_fixing_hallucinations_until_the/",
      "author": "coolandy00",
      "created_utc": "2026-01-06 21:27:30",
      "score": 12,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve seen a common RAG failure pattern:\n\nUser says: ‚ÄúMy RAG is hallucinating.‚Äù  \nSystem immediately suggests: ‚Äúincrease top-k, change chunking, add reranker‚Ä¶‚Äù\n\nBut we don‚Äôt even know:\n\n* what retriever they use\n* how they chunk\n* whether they require citations / quote grounding\n* what ‚Äúhallucination‚Äù means for their task (wrong facts vs wrong synthesis)\n\nSo the first ‚ÄúRAG fix‚Äù is often not retrieval tuning, it‚Äôs **escalation rules**.\n\nEscalation contract for RAG assistants\n\n* **ASK**: when missing pipeline details block diagnosis (retriever/embeddings/chunking/top-k/citation requirement)\n* **UNKNOWN:** when you can‚Äôt verify the answer with retrieved evidence\n* **PROCEED:** when you have enough context + evidence to make a grounded recommendation\n\nPractical use:\n\n* add a small ‚Äúrouter‚Äù step before answering:\n   * Do I have enough info to diagnose?\n   * Do I have enough evidence to answer?\n   * If not, ASK or UNKNOWN.\n\nThis makes your ‚ÄúRAG advice‚Äù less random and more reproducible.\n\nQuestion for the RAG folks: what‚Äôs your default when retrieval is weak, ask for more context, broaden retrieval, or abstain?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q5vn4g/rag_tip_stop_fixing_hallucinations_until_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny3hd34",
          "author": "RolandRu",
          "text": "Totally agree. My default when retrieval is weak: **ASK ‚Üí BROADEN (bounded) ‚Üí ABSTAIN**.  \nIf I can‚Äôt support claims with retrieved evidence (quotes/citations), I return **UNKNOWN** \\+ a reason code (‚Äúno supporting snippets / conflicting sources‚Äù). Makes RAG advice reproducible instead of ‚Äútry top-k=50‚Äù roulette.",
          "score": 2,
          "created_utc": "2026-01-06 22:56:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4jjo6",
              "author": "coolandy00",
              "text": "Yup, it worked well on our agent instructions",
              "score": 1,
              "created_utc": "2026-01-07 02:17:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny6hlgu",
          "author": "IdeaAffectionate945",
          "text": "From my system instruction;\n\n    ### Adhere to the context\n    * YOU ARE UNDER NO CIRCUMSTANCES ALLOWED TO ANSWER QUESTIONS YOU CANNOT FIND THE ANSWER TO IN THE CONTEXT.\n    * If the user asks you a question and you cannot find the answer to it in the context, or the question is irrelevant to the provided context, then inform the user that you don't know the answer, and encourage the user to provide some relevant keywords or stay on subject.",
          "score": 2,
          "created_utc": "2026-01-07 11:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "ny7x476",
          "author": "getarbiter",
          "text": "The most common misunderstanding is treating RAG failure as a retrieval problem instead of a validation problem.\n\nPeople assume: better chunking, bigger top-k, or another router will fix hallucinations. But the real issue is that the system has no way to score whether the retrieved context actually supports the claim being made.\nRetrieval answers ‚Äúwhat could be relevant?‚Äù\nValidation answers ‚Äúis this answer coherent with the evidence and intent?‚Äù\n\nWithout an explicit coherence/grounding check, you‚Äôre just increasing surface area for plausible nonsense.\n\nThat‚Äôs why systems look fine in evals and fail in production.",
          "score": 2,
          "created_utc": "2026-01-07 16:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyetcor",
              "author": "coolandy00",
              "text": "Agree. In addition, a structured prompt design adds on creating a production grade output.",
              "score": 1,
              "created_utc": "2026-01-08 15:19:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "ny4cheg",
          "author": "vaisnav",
          "text": "ai slop",
          "score": 1,
          "created_utc": "2026-01-07 01:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny4jg56",
              "author": "coolandy00",
              "text": "Totally agree.. üòÅ",
              "score": 1,
              "created_utc": "2026-01-07 02:16:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "ny4mgf6",
                  "author": "344lancherway",
                  "text": "What do you think is the most common misunderstanding when it comes to diagnosing RAG issues? Seems like it varies a lot depending on the use case.",
                  "score": 1,
                  "created_utc": "2026-01-07 02:33:16",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nybb866",
                  "author": "vaisnav",
                  "text": "I mean your post buddy, dont you have an original thought that doesnt rely on copy pasting a chat output?",
                  "score": 1,
                  "created_utc": "2026-01-08 01:19:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qc5qua",
      "title": "Is RAG the right approach for exhaustive searches over a corpus of complex documents?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qc5qua/is_rag_the_right_approach_for_exhaustive_searches/",
      "author": "cleinias",
      "created_utc": "2026-01-13 22:35:27",
      "score": 12,
      "num_comments": 11,
      "upvote_ratio": 0.93,
      "text": "Disclaimer: I am completely new to RAG systems and I am trying to determine whether they are the right approach to my use cases.  I just spent the last few hours reading various material and watching videos on the subject, but still can't  figure out the answer.\n\nConsider this use case (more of a toy problem than a real use case, but close enough in spirit):\n\nYou have a collection of cookbooks, each one being a PDF file several hundred pages long. Let's say you have a few hundreds of them. That is your knowledge base\n\nYou want to be able to query *exclusively* and *exhaustively* this knowledge base with question that may be as simple as:\n\n\"List *all* the recipes using kale in the knowledge base providing the source title, author, and page number.\"\n\nto more complex one such as, for instance, \n\n\"Provide a list of *all* recipes suitable as a main course that include a green vegetable similar to kale as one of the main ingredients, providing the source title, author, and page number.\"\n\n  \nIn short: I have a corpus of documents that are semantically fairly homogeneous and therefore all more or less relevant to the possible queries and I need to the answers to be exhaustive.\n\nThe resources I have read and watched, on the other hand, seem to focus on a different set of use cases, where they are confronted with a vast collection of potentially heterogeneous documents (e.g., all the internal policy documents of a large company) and are keen to extract the very few items relevant to the query at hand in order to integrate the LLM processing step.\n\nWelcoming all suggestions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qc5qua/is_rag_the_right_approach_for_exhaustive_searches/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzge88w",
          "author": "OnyxProyectoUno",
          "text": "Your instinct that standard RAG might not fit here is worth taking seriously. The typical RAG setup optimizes for finding the most relevant handful of chunks, not exhaustive recall across a corpus. When you need every recipe with kale, similarity search with a top-k cutoff is fundamentally the wrong tool.\n\nWhat you're describing sounds more like a structured extraction problem. You'd want to parse each cookbook, extract recipe entities with their ingredients, metadata, and page references, then store that in something queryable like a database or search index. The LLM piece comes in for the fuzzy matching (\"green vegetable similar to kale\") but the exhaustive listing part needs deterministic retrieval.\n\nThe preprocessing step is where this gets tricky. PDF cookbooks are a nightmare because recipes span pages, ingredient lists get mangled, and section headers don't always parse cleanly. I work on document processing tooling at vectorflow.dev and this exact pattern comes up a lot. You need to see what your parser is actually extracting before you can trust any downstream queries.\n\nFor your use case, I'd explore a hybrid approach: structured extraction for the exhaustive search, semantic matching for the similarity queries. What format are these PDFs in? Scanned images or native text?",
          "score": 5,
          "created_utc": "2026-01-14 00:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgo3yx",
              "author": "ai_hedge_fund",
              "text": "Agree 100%\n\nI see this as going into some structured database first, possibly NoSQL, and then maybe using an LLM-as-a-Judge or Classifier to decide how well the search results meet the goal of the query, filtering, etc. \n\nDepending on the real details of the use case the end user may or may not want further generation using those chunks. \n\nSo, yeah, vector search and generation may not be the ultimate configuration.",
              "score": 2,
              "created_utc": "2026-01-14 01:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh0j52",
                  "author": "sqm_prout",
                  "text": "Totally agree, structured databases can really help here. Using something like a NoSQL setup to store the extracted data will make those complex queries way easier. Plus, integrating an LLM to refine the results can add that extra layer of intelligence without losing the exhaustiveness you‚Äôre after. Just make sure your extraction process is solid to handle those tricky PDF formats!",
                  "score": 2,
                  "created_utc": "2026-01-14 02:52:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzfs4k9",
          "author": "Far_Statistician1479",
          "text": "I‚Äôm not 100% sold that semantic search is really an improvement over tokenized search, but if you want ai refined search, really the only way to do that is to give an LLM a few search tools. And semantic search could be the underlying search mechanism if it works well for you.",
          "score": 2,
          "created_utc": "2026-01-13 22:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzg5tdl",
              "author": "Infamous_Ad5702",
              "text": "I love semantic search. Accurate. Low cost. No gpu for me. I can‚Äôt do hallucinations for my client‚Ä¶must be offline also.",
              "score": 2,
              "created_utc": "2026-01-14 00:00:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg6ztr",
                  "author": "Far_Statistician1479",
                  "text": "All of these are features of traditional tokenized search as well?",
                  "score": 2,
                  "created_utc": "2026-01-14 00:07:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzglc55",
          "author": "jordan_yeo",
          "text": "I‚Äôve taken an approach for exhaustive search where we first grab all unique document ids that contain a matching chunk, then for each of those docs, do the hybrid search, run through reranker. You can then take all the cross document results and rerank again. It‚Äôs expensive, and not instant, but has done really well ensuring we get comprehensive results across the corpus",
          "score": 2,
          "created_utc": "2026-01-14 01:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgq470",
          "author": "BusinessMindedAI",
          "text": "RAG is built for top-K relevance, not exhaustive discovery, so it will miss recipes in your case.\nYou need to extract every recipe into a structured index (ingredients, pages, categories) using LLMs once.\nThen run database or graph queries for 100% recall, using LLMs only to interpret and explain results.",
          "score": 2,
          "created_utc": "2026-01-14 01:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzi92ry",
          "author": "GP_103",
          "text": "Hybrid Search - Graph + BM25 with lightweight intent classifier.\n\nMake no mistake though, you‚Äôll have to pre-process those docs in a schema that adds some hierarchy to the content blocks. And there‚Äôs work to get BM25 weighted right.",
          "score": 1,
          "created_utc": "2026-01-14 08:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkxa1r",
          "author": "Curious-Sample6113",
          "text": "Only way to find out is to start",
          "score": 1,
          "created_utc": "2026-01-14 18:12:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcjvn2",
      "title": "RAG BUT WITHOUT LLM (RULE-BASED)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "author": "adrjan13",
      "created_utc": "2026-01-14 10:27:43",
      "score": 12,
      "num_comments": 11,
      "upvote_ratio": 0.84,
      "text": "Hello, has anyone here created a scripted chatbot (without using LLM)? \n\nI would like to implement such a solution in my company, e.g., for complaints, so that the chatbot guides the customer from A to Z. I don't see the need to use LLM here (unless you have a different opinion‚Äîfeel free to discuss). \n\nHas anyone built such rule-based chatbots? Do you have any useful links? Any advice? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzimms7",
          "author": "redditorialy_retard",
          "text": "that's building the retrieval but replace LLM with scripts. IE having relevant article 1, 2, 3 (retrieval results) and having the chat bot recommend those.¬†\n\n\nIe chatbot ask what is the problem, user input then is parsed to use as a query to the RAG.\n\n\nBut honestly it's just easier to put an LLM in the middle if you don't want to expose it to the end user.",
          "score": 6,
          "created_utc": "2026-01-14 10:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjegum",
          "author": "PrepperDisk",
          "text": "Tested this out with Haystack just doing retrieval, but then feeding into an LLM to rephrase the chunks.\n\nMy issue with this was user expectation.  With a simple search box, users know to search on keywords.  With a conversational interface, users increasingly expect to be able to phrase requests like they do with Gemini or ChatGPT  (with conversations and context from last request).\n\nMy solution was in a kind of \"uncanny valley\" where it was neither and users got stuck.",
          "score": 3,
          "created_utc": "2026-01-14 13:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjnesz",
          "author": "irodov4030",
          "text": "there are 100s of applications where you can use retrieved chunks in a workflow and do not need LLM to package the response. Your solution will be more deterministic than a typical RAG with LLM\n\nI have built a similar custom solution.\n\nYou are going in the right direction.\n\nLet me know if you have any specific questions.",
          "score": 2,
          "created_utc": "2026-01-14 14:40:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq2cf7",
              "author": "ghaaribkhurshid",
              "text": "Hello, I'm a fresher in CS, I want to build career in AI, could you please guide me more on this?",
              "score": 1,
              "created_utc": "2026-01-15 13:24:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzk7hvg",
          "author": "Elses_pels",
          "text": "Have you tried RASA ?",
          "score": 2,
          "created_utc": "2026-01-14 16:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzio6rc",
          "author": "Necessary-Dot-8101",
          "text": "compression-aware intelligence (CAI) is useful bc it treats hallucinations, identity drift, and reasoning collapse not as output errors but as structural consequences of compression strain within intermediate representations. it provides instrumentation to detect where representations are conflicting and routing strategies that stabilize reasoning rather than patch outputs",
          "score": 1,
          "created_utc": "2026-01-14 10:50:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzipyni",
          "author": "trollsmurf",
          "text": "How would you build consistent responses from chopped up content chunks otherwise?\n\nIf you consistently chunk on chapters/sections and provide that whole section as a response it would work, but it's not quite the same thing.\n\nInterested in knowing how commercial support-related chatbots handle this.",
          "score": 1,
          "created_utc": "2026-01-14 11:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjn5h4",
          "author": "vdharankar",
          "text": "So basically you just want to pull the chunks and show user ? Base idea of RAG is generation with augmentation",
          "score": 1,
          "created_utc": "2026-01-14 14:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjq12n",
          "author": "cubixy2k",
          "text": "So the standard intent based chat bots like Alexa skills?",
          "score": 1,
          "created_utc": "2026-01-14 14:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkj4cl",
          "author": "Alternative_Nose_874",
          "text": "You may consider [botpress.com](http://botpress.com) or similar open source platform as the backend for easy setup.",
          "score": 1,
          "created_utc": "2026-01-14 17:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzin0qd",
          "author": "Bozo32",
          "text": "I used a combination of cosine similarity and bm25 to filter the obviously irrelevant -> reranker -> NLI and then presented results to the user...have them say yes or no. This could iterate where you use the 'no' answers as a filter to rerank results  \nover time you would accumulate an evidence base of   \n'query' 'rejected resources' 'chosen resource'  \nthat would be useful for future searches  \nthe work done by folks at Utrecht university on screening abstracts for systematic review may be helpful  \n[https://asreview.nl/install/](https://asreview.nl/install/)",
          "score": -1,
          "created_utc": "2026-01-14 10:40:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9x0gk",
      "title": "RAG without a Python pipeline: A Go-embeddable Vector+Graph database with an internal RAG pipeline",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q9x0gk/rag_without_a_python_pipeline_a_goembeddable/",
      "author": "sd_cips",
      "created_utc": "2026-01-11 11:22:51",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "Hi everyone,\n\n(English is not my first language, so please excuse any errors).\n\nFor the past few months, I've been working on **KektorDB**, an in-memory, embeddable vector database.\n\nInitially, it was just a storage engine. However, I wanted to run RAG locally on my documents, but I admit I'm lazy and I didn't love the idea of manually managing the whole pipeline with Python/LangChain just to chat with a few docs. So, I decided to move the retrieval logic directly inside the database binary.\n\n**How it works**\n\nIt acts as an OpenAI-compatible **middleware between your client** (like Open WebUI) **and your LLM** (Ollama/LocalAI). You configure it via two YAML files:\n\n*  **vectorizers.yaml:** Defines folders to watch. It handles ingestion, chunking, and uses a local LLM to extract entities and link documents (Graph).\n* **proxy.yaml:** Defines the inference pipeline settings (models for rewriting, generation, and search thresholds).\n\n\n\n**The Retrieval Logic (v0.4)**\n\nI implemented a specific pipeline and I‚Äôd love your feedback on it:\n\n* **CQR (Contextual Query Rewriting):** It intercepts chat messages and rewrites the last query based on history to fix missing context.\n* **Grounded HyDe:** Instead of standard HyDe (which can hallucinate), it performs a preliminary lookup to find real context snippets, generates a hypothetical answer based on that context, and finally embeds that answer for the search.\n* **Hybrid Search (Vector + BM25):** The final search combines dense vector similarity with sparse keyword matching (BM25) to ensure specific terms aren't lost.\n* **Graph Traversal:** It fetches the context window by traversing prev/next chunks and mentions links (entities) found during ingestion.\n\n**Note:** All pipeline steps are configurable via YAML, so you can toggle HyDe/Hybrid search and other on or off.\n\n\n\n**My questions for you**\n\nSince you folks build RAG pipelines daily:\n\nIs this \"Grounded HyDe + Hybrid\" approach robust enough for general purpose use cases?\n\nDo you find Entity Linking (Graph) actually useful for reducing hallucinations in local setups compared to standard window retrieval?\n\nShould I make more use of graph capabilities during ingestion and retrieval?Should I make more use of graph capabilities during ingestion and retrieval?\n\n**Disclaimer:** The goal isn't to replace manual pipelines for complex enterprise needs. **The goal is to provide a solid baseline for generic situations where you want RAG quickly without spinning up complex infrastructure.**\n\n\n\n**Current Limitations (That I'm aware of):**\n\n* **PDF Parsing:** It handles images via Vision models decently, but table interpretation needs improvement.\n* **Splitting:** Currently uses basic strategies; I need to dive deeper into semantic chunking.\n* **Storage:** It is currently RAM-bound. A hybrid disk-storage engine is already on the roadmap for v0.5.0.\n\n\n\nThe project compiles to a single binary and supports OpenAI/Ollama \"out of the box\".\n\nRepo: [https://github.com/sanonone/kektordb](https://github.com/sanonone/kektordb)\n\nGuide: [https://github.com/sanonone/kektordb/blob/main/docs/guides/zero\\_code\\_rag.md](https://github.com/sanonone/kektordb/blob/main/docs/guides/zero_code_rag.md)\n\nAny feedback or roasting is appreciated!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q9x0gk/rag_without_a_python_pipeline_a_goembeddable/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz2eblc",
          "author": "OnyxProyectoUno",
          "text": "Your retrieval pipeline looks solid, but I'd focus on those parsing limitations you mentioned. PDF table interpretation and basic chunking strategies will bite you more than retrieval tweaks. When tables get mangled or chunks lose semantic boundaries, even perfect retrieval can't save you. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) because I kept hitting these upstream issues - needed to see what documents actually looked like after parsing before they hit the vector store.\n\nEntity linking can be useful, but it depends on your documents. If you're dealing with contracts or technical docs where entities reference each other across sections, graph traversal helps. For general knowledge bases, the complexity might not be worth it. The prev/next chunk window you're doing is probably more reliable than entity links for most use cases.\n\nFor semantic chunking, look at Chonkie or the newer approaches in Docling. Basic sentence splitting loses too much context, especially with structured documents. Your YAML config approach is smart though - makes it easy to experiment with different strategies without touching code.\n\nThe single binary deployment is appealing for local setups. How are you handling memory usage with larger document sets? RAM-bound works for prototyping but becomes a constraint quickly. Your hybrid storage plan for v0.5 should help there.\n\nWhat's your chunking strategy looking like right now? Fixed size, sentence boundaries, or something else?",
          "score": 3,
          "created_utc": "2026-01-11 23:44:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4wi37",
              "author": "sd_cips",
              "text": "Thanks for the great feedback and the tool suggestions! You are absolutely right, if the parsing/chunking is bad, the retrieval doesn't matter.\n\nTo answer your question on chunking: Right now, I'm primarily using a Recursive Character Splitter with configurable overlap via the YAML config that tries to respect document structure (splitting on paragraphs \\\\n\\\\n, then sentences \\\\n, etc.) to preserve context. I also have specialized splitters for Markdown and Code to handle those formats more intelligently. It's basic, but since the pipeline is modular I plan to improve it later.\n\nMemory: Currently, it relies on Int8 quantization (reducing footprint by \\~75%) and Float16 compression to fit larger datasets in RAM. It works well for local/mid-sized workloads, but the hybrid storage (planned for v0.5) will be the real fix for scaling beyond RAM limits.",
              "score": 1,
              "created_utc": "2026-01-12 09:39:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz77jzg",
                  "author": "OnyxProyectoUno",
                  "text": "The recursive character splitter approach makes sense for getting started. The structure-aware splitting you're doing (paragraphs then sentences) is better than naive fixed-size chunks, but you'll still hit issues with things like lists, tables, or code blocks that span your chunk boundaries. When you do upgrade the chunking, test it against some messy real-world docs first. Clean markdown examples always work great, but PDFs with mixed formatting will show you where it breaks.\n\nInt8 quantization at 75% reduction is pretty good for the memory constraint. Are you quantizing just the embeddings or the whole pipeline? If you're running the embedding model in the same process, that's probably your bigger memory hog. The hybrid storage approach will help, but you might want to consider lazy loading embeddings from disk even before v0.5 if memory becomes a blocker for testing with larger datasets.",
                  "score": 2,
                  "created_utc": "2026-01-12 17:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q5e6i1",
      "title": "Building a hybrid OCR/LLM engine led to a \"DOM\" for PDFs (find(\".table\"))",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q5e6i1/building_a_hybrid_ocrllm_engine_led_to_a_dom_for/",
      "author": "Less_Influence8515",
      "created_utc": "2026-01-06 09:36:12",
      "score": 11,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "After having my share of pain in extracting 300-page financial reports, I've spent the last three months testing out different PDF extraction solutions before deciding to build one\n\n**Why hybrid?**\n\nReferences below show combining OCR and LLM yields improvements across document processing phases. This motivated me to converge different parsing sources as \"Layers\" in both Chat and in the Review pages. Two UX benefits so far:\n\n1. User can click on a table bounding box as context reference for Chat.\n2. I can ask the agent to verify the LLM-extracted text against OCR for hallucinations.\n\nLastly, I am experimenting with a \"DOM inspector\" on the Review page. Since I have entity coordinates in all pages, I can rebuild the PDF like a DOM and query it like one:\n\n        find(\".table[confidence>0.9]\") # high-confidence tables only\n        find(\".table, .figure\") # both\n        find(\".table\", pageRange=[30, 50]) # pages 30-50 only\n\nI think this would be a cool CLI for the AI Agent to help users move through the document faster and more effectively.\n\n**Demo**\n\n[OkraPDF Chat and Review page demo](https://www.youtube.com/watch?v=6kV2Rhx06rI)\n\nCurrently, VLM generates entity content, so parsing is slow. I've sped up some parts of the video to get the demo across.\n\nChat page\n\n* [0:00](https://www.youtube.com/watch?v=6kV2Rhx06rI) \\- [0:18](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=18s) Upload a 10-K filing with browser extension\n* [0:18](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=18s) \\- [0:56](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=56s) Search for a table to export to Excel using the Okra Agent\n* [0:56](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=56s) \\- [1:36](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=96s) Side-by-side comparison\n\nReview page\n\n* [1:36](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=96s) \\- [2:45](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=165s) Marking pages as verified\n* [2:45](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=165s) \\- [3:21](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=201s) Fixing error in-place and marking page as verified\n* [3:21](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=201s) \\- [3:41](https://www.youtube.com/watch?v=6kV2Rhx06rI&t=221s) Show document review history\n\nPublic pages for parsed documents\n\n* It's fun to be able to search images of \"speedometer\" [https://publicusercontent.okrapdf.com/acura/2008-tl](https://publicusercontent.okrapdf.com/acura/2008-tl)\n* Standard financial report [https://publicusercontent.okrapdf.com/10k/3m-2018](https://publicusercontent.okrapdf.com/10k/3m-2018)\n\n**References**\n\n\\- LLM identifies table regions, while a rule-based parser extracts the content from [\"Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task\"](https://arxiv.org/pdf/2510.10138)  \n\\- LLM to correct OCR hallucinations from [\"Correction of OCR results using LLM\"](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/13690/136901G/Correction-of-OCR-results-using-large-language-models/10.1117/12.3068186.short#:~:text=This%20paper%20proposes%20a%20large,and%20GLM%2D4%2DFlash)\n\nIt's in open beta and free to use: [https://okrapdf.com/](https://okrapdf.com/). I'd love to hear your feedback!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q5e6i1/building_a_hybrid_ocrllm_engine_led_to_a_dom_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny1x2f8",
          "author": "OnyxProyectoUno",
          "text": "The DOM approach for PDFs is clever. Having coordinates for every entity opens up possibilities that traditional text extraction just can't match. Your query syntax reminds me of how much easier web scraping got once we could target specific elements instead of parsing raw HTML.\n\nThe hybrid OCR/LLM verification step is smart too. I've seen too many cases where LLM parsing looks perfect until you compare it against the actual visual layout and realize it hallucinated table structures or missed embedded text in images.\n\nOne thing to watch out for as you scale this, the VLM processing speed you mentioned is going to be the bottleneck. Financial docs especially tend to have complex nested tables and multi-column layouts that take forever to process properly. I've been building similar document processing tooling at vectorflow.dev and speed vs accuracy is always the tradeoff.\n\nThe review interface looks solid for catching errors before they propagate downstream. Most people don't realize how much garbage gets through until it's already embedded and causing retrieval issues. Being able to fix extraction errors in place and track verification history is huge for maintaining quality.\n\nHow are you handling the coordinate mapping when documents have rotated pages or mixed orientations? That usually breaks a lot of extraction pipelines.",
          "score": 1,
          "created_utc": "2026-01-06 18:36:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qav580",
      "title": "Project ideas!!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qav580/project_ideas/",
      "author": "rayanskrrr",
      "created_utc": "2026-01-12 13:40:06",
      "score": 11,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "Can anyone recommend some begginer friendly rag project idea to someone who's new to generative ai something which is unique and not npc which would standout while being begginer friendly as well",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qav580/project_ideas/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz8l391",
          "author": "Stock-Cucumber6406",
          "text": "I would start with the context engineers discord server. Infinite resources :)\n\nhttps://discord.gg/EDwCHfPn9",
          "score": 5,
          "created_utc": "2026-01-12 21:40:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5vt1f",
          "author": "anashel",
          "text": "I think a quiz game is a good test, as it show your rag capacity to retrieve knwoeldge correctly for context to make the question and with precision for calidating the answer‚Ä¶\n\nLike, instead of building yet another ‚Äúchat with PDF‚Äù, you build a little game that forces grounding:\n- You ingest a single book (public domain novel, short story collection, or even a cookbook).\n- Your system generates questions only from retrieved passages (characters, places, plot events, relationships, timelines).\n-Then when the player answers, the system validates using retrieval again and shows citations from the exact chunks that justify the correct answer.\n- Bonus: show a side panel that compares ‚ÄúLLM with RAG‚Äù vs ‚ÄúLLM without RAG‚Äù so people instantly see hallucinations disappear.\n\nConcrete book example:\n- Pick something fun like Sherlock Holmes, Dracula, or Alice in Wonderland\n-Mode 1: ‚ÄúBook review mode‚Äù where the LLM writes a review, but every claim must be backed by a cited snippet (themes, character arcs, key scenes).\n- Mode 2: ‚ÄúQuiz mode‚Äù where it asks stuff like ‚ÄúWho said this line?‚Äù, ‚ÄúWhere were they when X happened?‚Äù, ‚ÄúWhat happened right before Y?‚Äù, ‚ÄúHow does character A relate to B?‚Äù‚Ä¶\n\nFor the dev, i suggest cloudflare.\n-simple worker typescript, very simple and one line to deploy (wrangler)\n- easy to upload your doc (r2 bucket)\n- you can build your own rag indexing (supabase postgres with pgvector) and use cloudflare hyperdrive (basically one line to integrate in your worker)\n- or you can use their RAG indexer to index your r2 bucket and chunk it automatically\n- ai gateway gives you nice visibility (logs and traceability) of all llm interactions\n\nYou can spin it in a nice simple react app to play with it and have a side by side answer; with and without rag to see the actual quality",
          "score": 4,
          "created_utc": "2026-01-12 14:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfxvoc",
              "author": "CompellingBytes",
              "text": "I was actually thinking of doing this in Jeopardy format.",
              "score": 2,
              "created_utc": "2026-01-13 23:18:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzggech",
                  "author": "anashel",
                  "text": "Thats a great mvp! Let me know if you do it, i‚Äôll love to see that!",
                  "score": 1,
                  "created_utc": "2026-01-14 00:58:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz93a82",
              "author": "FormalAd7367",
              "text": "Notebooklm?",
              "score": 1,
              "created_utc": "2026-01-12 23:08:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz65i0i",
          "author": "bsenftner",
          "text": "Add which nobody seems to add to RAG, which is business-wise really critically important: create a library for the tracking of the expense of RAG, of GraphRAG or really any RAG-like solution. Which is rather dynamic, because the expense of use of a RAG system depends upon the ingestion cost, the frequency of re-ingestion due to ingested document changes, and then the questions against them, any post-question re-indexing for long conversation support and all these expenses finally summed. In some environments with frequently changing large documents, RAG becomes questionable from an expense standpoint. This type of tracking and financial accounting is sorely lacking in software today, and that probably needs to change. Considering all the paid-use APIs there are now, such needs are inevitable.",
          "score": 1,
          "created_utc": "2026-01-12 14:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6hcv1",
          "author": "Valeria_Xenakis",
          "text": "Build a RAG system that compares two software versions and explains actually breaking changes on upgrade.\n\nInstead of summarizing changelogs, it retrieves real-world breaking changes from release notes, migration guides, and ‚Äúthis update broke my app‚Äù posts.\n\nIt filters changes based on how the user uses the software and classifies them by impact.\nThe focus is answering ‚Äúdo I care and what should I fix?‚Äù rather than just ‚Äúwhat changed.‚Äù",
          "score": 1,
          "created_utc": "2026-01-12 15:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6po7s",
          "author": "AsparagusKlutzy1817",
          "text": "Do you want an AI idea or an agent idea ? RAG is a tool - like a hammer - what is the nail you want to hit ?",
          "score": 1,
          "created_utc": "2026-01-12 16:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8o3l8",
          "author": "yogonflame",
          "text": "start with zeroentropy stack and their discord server.",
          "score": 1,
          "created_utc": "2026-01-12 21:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8yg4s",
          "author": "Strong_Worker4090",
          "text": "What are your top 3 hobbies? I'd like to give you some that you might actually find interesting.",
          "score": 1,
          "created_utc": "2026-01-12 22:43:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjdg36",
              "author": "rayanskrrr",
              "text": "Arghh watching shows animes playing football and arghh doom scrolling",
              "score": 1,
              "created_utc": "2026-01-14 13:47:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzkt34i",
                  "author": "Strong_Worker4090",
                  "text": "Ok cool cool, those hobbies are perfect for a couple beginner RAG projects that don‚Äôt feel NPC:\n\n**1) Anime recs RAG (actually useful)**  \nBuild a small DB from episode summaries + reviews + Reddit threads. Ask stuff like:\n\n* ‚ÄúI liked AOT for politics + pacing, what‚Äôs similar?‚Äù Make it stand out by saving user prefs and citing sources in every answer.\n\n**2) Football injury + lineup tracker RAG**  \nScrape a few reliable injury/news sources on a schedule. Ask:\n\n* ‚ÄúIs \\_\\_\\_ expected to start this weekend?‚Äù Standout feature: time-based answers (‚Äúlatest update as of‚Ä¶‚Äù) + citations.\n\n**3) Doomscroll-to-summary RAG**  \nYou drop links/posts, it ingests and answers:\n\n* ‚ÄúWhat are the 3 main takes across these threads?‚Äù Standout feature: consensus vs disagreement + citations.",
                  "score": 1,
                  "created_utc": "2026-01-14 17:53:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdy3ot",
          "author": "Whole-Assignment6240",
          "text": "shared 20 example project here - [https://cocoindex.io/examples/](https://cocoindex.io/examples/) \\- lmk if it is helpful! (i'm the maintainer of the framework)",
          "score": 1,
          "created_utc": "2026-01-13 17:43:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcgoo8",
      "title": "need help embedding 250M vectors / chunks at 1024 dims, should I self host embedder (BGE-M3) and self host Qdrant OR use voyage-3.5 or 4?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "author": "zriyansh",
      "created_utc": "2026-01-14 07:06:24",
      "score": 11,
      "num_comments": 18,
      "upvote_ratio": 0.92,
      "text": "hey redditors, I am building a legal research RAG tool for law firms, just research and nothing else.\n\n  \nI have around 1.5TB of legal precedence data, parsed them all using 64 core Azure VM, using PyMuPDF + Layout + Pro. Using custom scripts and getting around 30 - 150 files / second parse speed. \n\nVoyage-3-large surpassed voyage-law-2 and now gemini 001 embedder is ranked #2 (MTEB ranking).  Domain specific models are now overthrown by general embedders. \n\nI have around 250 million vectors to embed, and even using voyage-3.5 (0.06$/mill token), the cost is around $3k dollars. \n\n  \nUsing Qdrant cloud will be another $500.\n\n  \nQuestion I need help with:\n\n1. Should I self host embedder and vectorDB? (for chunking as well retrival later on)  \n2. Bear one time cost of it and be hastle free? \n\n\n\nFeel free to DM me for the parsing and chunking and embedding scripts. Using BM25 + RRF + Hybrid search + Rerank using voyage-rank2.5, CRAG + Web Search. \n\n  \nCurrent latency woth 2048 dims on test dataset of 400k legal text vectors is 5 seconds. \n\nChunking by characters and not token.\n\n|Metric|Value|\n|:-|:-|\n|**Avg parsed file size**|68.5 KB|\n|**Sample text length**|2,521 chars (small doc)|\n|**Total PDFs**|16,428,832|\n|**Chunk size**|4,096 chars (\\~1,024 tokens)|\n|**Chunk overlap**|512 chars (\\~128 tokens)|\n|**Min chunk size**|256 chars|\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzi2930",
          "author": "aiprod",
          "text": "Do you have an eval set that would allow you to test recall etc. on a subset of the corpus? Should help with selecting the right model. I‚Äôd also look into getting those 2048 dims down. Will save you a lot on vector db costs and reduces latency. Five seconds seems very slow. How did you test that? Was it pure embedding retrieval or your full retrieval pipeline?",
          "score": 4,
          "created_utc": "2026-01-14 07:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2p88",
              "author": "zriyansh",
              "text": "I dont have an eval set just yet, working on that. This is qdrant telling me about the latency. Okay it improved from yesterday lol. \n\nhttps://preview.redd.it/7ki00keho9dg1.png?width=2378&format=png&auto=webp&s=83bb39f165ed1da3c98c4b0af0d3d162ea4b5706",
              "score": 1,
              "created_utc": "2026-01-14 07:26:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzi4ay7",
                  "author": "aiprod",
                  "text": "That looks better although some of the queries are still a bit slow. How are you running the qdrant cluster? Is that through their cloud offering?",
                  "score": 1,
                  "created_utc": "2026-01-14 07:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzi2gda",
          "author": "bravelogitex",
          "text": "able to embed across a large number of gpu instances? havent heard of that done before",
          "score": 1,
          "created_utc": "2026-01-14 07:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2r62",
              "author": "zriyansh",
              "text": "CPU\\*",
              "score": 1,
              "created_utc": "2026-01-14 07:26:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi3k45",
          "author": "ai_hedge_fund",
          "text": "This is interesting because you use all different units than I usually think in terms of\n\nWhen you say self host, what GPUs do you have or are you backing into a budget?\n\nThen it becomes a question of turnaround time\n\nIf the time isn‚Äôt an issue then I think you can do better than the $3K\n\nIf you want it done fast then $3K to $4K sounds about right if you rent GPUs / have quota in Azure\n\nTo me the cloud DB question depends on how users will access the data and when. I‚Äôd think you could defer that $500 now if it‚Äôs an issue and just store the vectors yourself.",
          "score": 1,
          "created_utc": "2026-01-14 07:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi8rqo",
              "author": "zriyansh",
              "text": "expecting around 50 users in a month, and 10 queries per user each day.\n\nyeah not using token because character is what I understand well, so it works for me. \n\nI have a budget for $1K for now as we dont have any customers, using my savings for this.\n\nAs far as I understanding, embedding and hosting a vector DB is CPU intensive not GPU (can be wrong here), I have 1k$ credit from Azure as I registered my startup with them (and linked my LinkedIn with them as well).   \n  \nIf we break even, I will want to use cloud services and focus on what we do best.",
              "score": 1,
              "created_utc": "2026-01-14 08:23:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nziamtn",
          "author": "mwon",
          "text": "I have also a side project in the field of legal AI, but with considerable lower size. My biggest index has about 11M vectors, size 1024, generated form a fined tuned BGE-M3.   \nI use Milvus with index in disk in  a dedicated server fom Hetzner, and my latency is bellow 0.5s.  \nI think is a bit odd your latency is 5s for only 400k vectors. You should check if everything is ok, because is too much. I also think chunks of 1024 is too much. You will l likely loose a lot of recall.",
          "score": 1,
          "created_utc": "2026-01-14 08:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzib92e",
              "author": "zriyansh",
              "text": "so its self hosted embedder I suppose, what kind of machine are you using? and anything I need to take care of here?",
              "score": 1,
              "created_utc": "2026-01-14 08:47:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzic6mi",
                  "author": "mwon",
                  "text": "Yes, self embedder. I never use embedding services. Very expensive for what they do and no better than many OS solutions that you can instantiate locally.   \n  \nI'm using one of theirs 64GB ram dedicated servers. They are very cheap, like 40-50 EUR/month. \n\nYou need to be careful with your benchmarks estimation. A sample of 400k vector is very small compared with your final production setup. Recall values will be very diferent with 250M vectors.",
                  "score": 1,
                  "created_utc": "2026-01-14 08:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nziqy8d",
              "author": "explodedgiraffe",
              "text": "I am also working on a side project of that size. I was thinking of using qwen 4b embedding and rerank. How was your experience with BGE-M3? I like the architecture of it (sparse/dense/multi vector) but their benchmarks weren't that impressive compared to dense of the same size. Also curious how you fined tuned it for your usecase.\n\n  \n5s must be caused by network latency from external rerank and web search calls?",
              "score": 1,
              "created_utc": "2026-01-14 11:14:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nziseix",
                  "author": "mwon",
                  "text": "qwen-4b vs BGE-M3 both out of the box, qwen likely wins. After all it is a 4B model against vs 0.5B. But you can give a good boost to BGE-M3 by finetuning it, which will give you a small model that does not need GPU for inference. The sparse part is also nice because it allows you to do hybrid with a sparse search in one go. Note however that from my experiences with BGE-M3, BM25 is still better than its sparse.",
                  "score": 1,
                  "created_utc": "2026-01-14 11:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzicnrv",
          "author": "UseMoreBandwith",
          "text": "how long does it take to process 1.5TB ?",
          "score": 1,
          "created_utc": "2026-01-14 09:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzicuih",
              "author": "zriyansh",
              "text": "around 3 days with 64 core CPU, but there exist faster parsers which can parse 4-5k documents per second with such beast machine but I wasn't able to run that properly, its a C implementation of pymupdf4llm-c",
              "score": 2,
              "created_utc": "2026-01-14 09:03:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qbgeap",
      "title": "OSS Alternative to Glean",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbgeap/oss_alternative_to_glean/",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-13 03:21:44",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, Connect any LLM to your internal knowledge sources (Search Engines, Drive, Calendar, Notion and 15+ other connectors) and chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams)\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Multi Collaborative Chats\n* Multi Collaborative Documents\n* Real Time Features\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qbgeap/oss_alternative_to_glean/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzaftxf",
          "author": "Oshden",
          "text": "Hey OP, how would one get involved in testing out the system and giving you some feedback? This looks perfect for something I‚Äôm trying to implement",
          "score": 1,
          "created_utc": "2026-01-13 03:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzai3cy",
              "author": "Uiqueblhats",
              "text": "Hey, we are actively looking for feedback. The easiest way to test the product is to try our cloud version. If you find any bugs, please create an issue at [https://github.com/MODSetter/SurfSense/issues](https://github.com/MODSetter/SurfSense/issues).",
              "score": 1,
              "created_utc": "2026-01-13 03:42:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzd046k",
                  "author": "Oshden",
                  "text": "This looks awesome!! I‚Äôll have to do a bit of research on how this could work but I‚Äôm excited to try it. Maybe even run it locally! Do you guys have a way to set up custom agent instructions to further refine the desired behavior for the chat agent?",
                  "score": 1,
                  "created_utc": "2026-01-13 14:53:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1q7s6dq",
      "title": "RAG tip: stop ‚Äúfixing hallucinations‚Äù until your agent output is schema-validated",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q7s6dq/rag_tip_stop_fixing_hallucinations_until_your/",
      "author": "coolandy00",
      "created_utc": "2026-01-08 23:45:55",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "When answers from my agent went weird, I checked and saw **output drift**.\n\nExample that broke my pipeline:  \n`Sure! { \"route\": \"PLAN\", }`  \nLooks harmless. Parser dies. Downstream agent improvises. Now you‚Äôre ‚Äúdebugging hallucinations.‚Äù\n\n**Rule:** Treat every agent output like an API response.\n\n**What I enforce now**\n\n* Return ONLY valid JSON (no prose, no markdown)\n* Exact keys + exact types (no helpful extra fields or properties)\n* Explicit status: ok / unknown / error\n* Validate between agents\n* Retry max 2 times using validator errors -> else unknown/escalate\n\nRAG gets blamed for a lot of failures that are really just ‚Äúwe trusted untrusted structure.‚Äù\n\nCurious: do you validate router output too, or only final answers?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q7s6dq/rag_tip_stop_fixing_hallucinations_until_your/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyi0b74",
          "author": "durable-racoon",
          "text": "'structured outputs' is a feature that exists on LLM apis.",
          "score": 3,
          "created_utc": "2026-01-08 23:55:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyi1otf",
          "author": "Esseratecades",
          "text": "Unless you're building a chat bot, always use structured output. In fact sometimes when you are building a chat bot you should still use structured output.",
          "score": 3,
          "created_utc": "2026-01-09 00:02:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyiqzvn",
          "author": "getarbiter",
          "text": "Schema validation catches malformed outputs. It doesn't catch coherent-looking outputs that are semantically wrong.\n\nYou can have perfectly valid JSON that's completely incoherent with the source material. Parser passes. Meaning fails.\n\nI built a layer that scores coherence ‚Äî not structure. Query + content in, coherence score out. Same input, same score, every time. Catches drift before it propagates.\n\nSlots after your schema check as a semantic validation step. 26MB, sub-second.\n\ngetarbiter.dev",
          "score": 0,
          "created_utc": "2026-01-09 02:14:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyj4xk9",
              "author": "Wimiam1",
              "text": "I‚Äôm interested in your claims, but you provide zero data to back it up. If you want people to try your product, you should remove 90% of your website‚Äôs extremely repetitive simple demos of how literally every other embedding model works and add some actual retrieval metrics on common benchmark datasets. You keep saying that Arbiter is somehow different from normal embedding models, but there‚Äôs absolutely nothing on your website to actually prove it. ‚ÄúIt goes negative‚Äù doesn‚Äôt count either. I can do that with any embedding model by simply score*2 - 1. If you want people to take you seriously, you need to actually run some commonly used benchmarks and show the results. \n\nEdit: maybe it‚Äôs closer to a cross encoder, but the point still stands",
              "score": 1,
              "created_utc": "2026-01-09 03:30:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nyjkthh",
                  "author": "getarbiter",
                  "text": "You didn't read the site.\n\n72 dimensions. 0.000000 standard deviation across 50 runs.\n\n76% accuracy on brain semantic categories (p=10‚Åª‚Åµ‚Å∏)\n\n+11% vs PCA on sense disambiguation\n\nCelebrex pathway identified without pharma training\n\nCross-lingual transfer with zero parallel corpora\n\nAncient language recognition without training on those scripts\n\n'Benchmark against common datasets' ‚Äî which ones?\n\nARBITER doesn't do similarity. \n\nIt measures whether a candidate satisfies a constraint field. Show me another deterministic coherence engine and I'll run the comparison.\n\nYou're asking a plane to benchmark against horses.\n\nThe data is on the site. The API is public. Run it yourself or don't.",
                  "score": 1,
                  "created_utc": "2026-01-09 05:05:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qblw1k",
      "title": "Best knowledge graph graph view?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qblw1k/best_knowledge_graph_graph_view/",
      "author": "PutridPut7225",
      "created_utc": "2026-01-13 08:19:05",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.92,
      "text": "What is the most advanced graph view out there currently I do find them all pretty limited especially for very high node count. But I also don't know a lot of knowledge graph software. So maybe you guys know something I don't ",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qblw1k/best_knowledge_graph_graph_view/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzbmslo",
          "author": "Rokpiy",
          "text": "neo4j bloom handles large graphs better than most but starts choking around 50k+ nodes. for really high counts, check out graphistry or yworks yed - they use gpu rendering which makes a difference. \n\ngephi is free and handles 100k+ nodes but the ui is dated. if you're dealing with millions of nodes, you'll need something like tigergraph's graph studio or memgraph lab which are built for scale.\n\ndepends on your use case though. what node count are you working with?",
          "score": 2,
          "created_utc": "2026-01-13 08:58:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbw757",
              "author": "PutridPut7225",
              "text": "10 k is enough for me. But I want the visuality as big as possible. So my problem is less how well it can handle many nodes performance speaking wise, but how fast can I find a specific node. So it's more about space management and so on. Anyways thanks for your suggested tools. Appreciate it",
              "score": 1,
              "created_utc": "2026-01-13 10:28:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbnp6d",
          "author": "Striking-Bluejay6155",
          "text": "Consider looking at [FalkorDB's browser](https://browser.falkordb.com/). You can also look at g.v() if you've got an underlying graph database. How many nodes are we talking?",
          "score": 2,
          "created_utc": "2026-01-13 09:07:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbwci9",
              "author": "PutridPut7225",
              "text": "Thanks will look into it. Normally not more than 10k nodes. However I need to find the node I am looking for the fastest way possible in the graph",
              "score": 2,
              "created_utc": "2026-01-13 10:30:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzclk5d",
                  "author": "Striking-Bluejay6155",
                  "text": "You're welcome. Has the issue so far been writing the cypher query? What I shared comes with built-in filters so you could probably narrow down 10k to a few. Plus you can control the sizes of the nodes according to your needs, so the ones you frequently look at can be larger and different color",
                  "score": 2,
                  "created_utc": "2026-01-13 13:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdybh4",
          "author": "Whole-Assignment6240",
          "text": "all kg comes with a browser. comes down to which kg you pick . neo4j is pretty decent, i used in many projects",
          "score": 1,
          "created_utc": "2026-01-13 17:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg10yt",
          "author": "TrustGraph",
          "text": "I've never found graph viewers useful from a data analysis perspective. I've had some folks from Neo4j tell me it's about 50/50, people that love graph viewers vs. people that never use them. I happen to be a never uses them.\n\nIf you want pretty, there's [Graphistry](https://www.graphistry.com/). I can't argue with it's aesthetics. Most all GraphDB systems have graph viewers. In fact, Neo4j's graph viewer is what really gained them their fame.\n\nTrustGraph uses [3D Force Graph](https://github.com/vasturiano/3d-force-graph) to show 3D graphs that could be stored in Cassandra, Neo4j, Memgraph, or FalkorDB. It has a ton of customization available in it.\n\nIf you'd like the ability to build context graphs and view them in 3D in a single platform with zero coding, TrustGraph is free and open source:\n\n[https://github.com/trustgraph-ai/trustgraph](https://github.com/trustgraph-ai/trustgraph)",
          "score": 1,
          "created_utc": "2026-01-13 23:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkmavb",
          "author": "coderarun",
          "text": "Store it in r/DuckDB and query it via r/LadybugDB\n\n[https://adsharma.github.io/explainable-ai/](https://adsharma.github.io/explainable-ai/)\n\nVisualization: you can probably ask your favorite terminal based coding agent to spit out one. Just need a mcp-server to talk to the data source.\n\nI've tested wikidata (90 million nodes) and am currently testing something 3x its size.",
          "score": 1,
          "created_utc": "2026-01-14 17:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkmrpo",
              "author": "coderarun",
              "text": "Also see: [https://adsharma.github.io/duckdb-wikidata-compression/](https://adsharma.github.io/duckdb-wikidata-compression/)",
              "score": 1,
              "created_utc": "2026-01-14 17:25:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q6ecnf",
      "title": "AI agents for searching and reasoning over internal documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q6ecnf/ai_agents_for_searching_and_reasoning_over/",
      "author": "Effective-Ad2060",
      "created_utc": "2026-01-07 12:41:47",
      "score": 10,
      "num_comments": 3,
      "upvote_ratio": 0.86,
      "text": "Hey everyone!\n\nI‚Äôm excited to share something we‚Äôve been building for the past few months -¬†**PipesHub**, a¬†**fully open-source alternative to Glean,** designed to bring powerful Enterprise Search, Agent Builders to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, OneDrive, Outlook, SharePoint Online, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.\n\nThe entire system is built on a¬†**fully event-streaming architecture powered by Kafka**, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data. PipesHub combines a vector database with a knowledge graph and uses Agentic RAG to deliver highly accurate results. We constrain the LLM to ground truth. Provides Visual citations, reasoning and confidence score. Our implementation says Information not found rather than hallucinating.\n\n**Key features**\n\n* Deep understanding of user, organization and teams with enterprise knowledge graph\n* Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama\n* Use any other provider that supports OpenAI compatible endpoints\n* Vision-Language Models and OCR for visual or scanned docs\n* Login with Google, Microsoft, OAuth, or SSO\n* Rich REST APIs for developers\n* All major file types support including pdfs with images, diagrams and charts\n* Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more\n* Reasoning Agent that plans before executing tasks\n* 40+ Connectors allowing you to connect to your entire business apps\n\nCheck it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)\n\n[](https://www.reddit.com/submit/?post_id=t3_1p66o0e)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q6ecnf/ai_agents_for_searching_and_reasoning_over/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "ny7indg",
          "author": "CrytoManiac720",
          "text": "Why do you open source that?",
          "score": 1,
          "created_utc": "2026-01-07 14:54:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "ny8s5xb",
              "author": "Effective-Ad2060",
              "text": "PipesHub works with sensitive enterprise data. Open-sourcing lets teams **see how permissions, indexing, and answers actually work**, instead of trusting a black box. It also makes it easy for developers to self-host, extend connectors, and adopt it faster.\n\nWe build the business on **enterprise features, hosted deployments, and support**, while keeping the core transparent and developer-friendly.",
              "score": 4,
              "created_utc": "2026-01-07 18:21:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyaqokw",
          "author": "TeeRKee",
          "text": "Very cool",
          "score": 1,
          "created_utc": "2026-01-07 23:35:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q3x24f",
      "title": "90% vector storage reduction without sacrificing retrieval quality",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q3x24f/90_vector_storage_reduction_without_sacrificing/",
      "author": "getarbiter",
      "created_utc": "2026-01-04 18:14:00",
      "score": 10,
      "num_comments": 29,
      "upvote_ratio": 0.86,
      "text": "\nIf you're running RAG at scale, you know the pain: embedding dimensions √ó document count √ó storage costs = budget nightmare.\n\nStandard embeddings are 768D (sentence-transformers) or 1536D (OpenAI). That's 3-6KB per vector. At millions of documents, you're looking at terabytes of storage and thousands per month in Pinecone/Weaviate bills.\n\n**What I tested:**\n\nCompressed embeddings down to 72D ‚Äî about 90% smaller ‚Äî and measured what happens to retrieval.\n\n| Metric | 768D Standard | 72D Compressed |\n|--------|---------------|----------------|\n| Storage | 3KB per vector | 288 bytes per vector |\n| Cosine similarity preservation | baseline | 96.53% preserved |\n| Disambiguation (\"bank\" finance vs river) | broken | works |\n\n**The workflow:**\n\n```\nDocuments ‚Üí Compress to 72D ‚Üí Store in your existing Pinecone/Weaviate index ‚Üí Query as normal\n```\n\nNo new infrastructure. Same vector database. Just smaller vectors.\n\n**The counterintuitive part:**\n\nRetrieval got cleaner. Why? Standard embeddings cluster words by surface similarity ‚Äî \"python\" (code) and \"python\" (snake) sit close together. Compressed semantic vectors actually separate different meanings. Fewer false positives in retrieval.\n\n**Monthly cost impact:**\n\n| Current Bill | After 72D |\n|--------------|-----------|\n| $1,000 | ~$100 |\n| $5,000 | ~$500 |\n| $10,000 | ~$1,000 |\n\nStill running tests. Curious if anyone else has experimented with aggressive dimensionality reduction and what you've seen.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q3x24f/90_vector_storage_reduction_without_sacrificing/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxonxyq",
          "author": "KVT_BK",
          "text": "How did you do the compression reducing dimensionality to 72D",
          "score": 2,
          "created_utc": "2026-01-04 20:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxop92r",
              "author": "getarbiter",
              "text": "It‚Äôs not post-hoc dimensionality reduction (PCA/autoencoders/etc.).\n\nThe representation is constructed to preserve semantic constraints before storage, so 72D isn‚Äôt a projection target ‚Äî it‚Äôs the native geometry the system reasons in.\n\nThat‚Äôs why retrieval quality holds under compression, instead of collapsing the way similarity-optimized embeddings do when you drop dimensions.\n\nHappy to compare outcomes. Not going to open-source the internals in a Reddit thread.",
              "score": 0,
              "created_utc": "2026-01-04 20:10:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxo3xlv",
          "author": "Simusid",
          "text": "Or just use FAISS, cost impact $0",
          "score": 3,
          "created_utc": "2026-01-04 18:35:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxo5xit",
              "author": "getarbiter",
              "text": "FAISS is just an index. It doesn‚Äôt compress anything.\n\nYou can absolutely index 72-D vectors in FAISS for free. That‚Äôs not the question.\n\nThe question is how you get to 72-D without destroying retrieval quality.\n\nPCA at ~10.7√ó compression preserves ~87% cosine similarity but collapses sense.\n\nSemantic compression at the same ratio preserves ~96% and maintains disambiguation.\n\nClassic example: ‚Äúbank‚Äù (financial) vs ‚Äúbank‚Äù (river). PCA clusters them together. The retriever pulls the wrong context. Similarity stays high, intent alignment fails.\n\nThe cost reduction is a side effect. \n\nThe point is that retrieval still works after compression.",
              "score": 2,
              "created_utc": "2026-01-04 18:44:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxobbzq",
                  "author": "Simusid",
                  "text": "My point is that I can host it locally for free, with no compression and no loss of semantic representation.",
                  "score": 3,
                  "created_utc": "2026-01-04 19:07:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxol8v4",
          "author": "Trotskyist",
          "text": ">Standard embeddings cluster words by surface similarity ‚Äî \"python\" (code) and \"python\" (snake) sit close together\n\nHold on, what? Unless you're literally just embedding a single word or using word2vec or some other antiquated technique (either way, why would you do this...) they absolutely don't sit close together. That's the entire point. The context a given word exists in is relevant.",
          "score": 3,
          "created_utc": "2026-01-04 19:51:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxx5gj5",
              "author": "Academic_Track_2765",
              "text": "Not sure what he is doing, but yes they dont sit together, unless word2vec is being used..",
              "score": 1,
              "created_utc": "2026-01-06 00:40:27",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nxomv1h",
              "author": "getarbiter",
              "text": "Standard embeddings require explicit context in the input to reliably disambiguate.\n\nGiven a sparse query like ‚Äúpython,‚Äù retrieval depends heavily on corpus bias and similarity thresholds.\n\nWe don‚Äôt.\n\nQuery: ‚Äúpython‚Äù\nCandidates: ‚ÄúPython programming language‚Äù / ‚ÄúMonty Python‚Äù / ‚ÄúPython snake‚Äù\n\nARBITER ranks by coherence, not similarity.\n\nDisambiguation happens in the geometry, not the prompt.\n\nThat‚Äôs the point ‚Äî we‚Äôre not relying on the user to supply context.\n\nThe structure handles it.",
              "score": -1,
              "created_utc": "2026-01-04 19:59:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxof02a",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-01-04 19:23:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxohs3m",
              "author": "getarbiter",
              "text": "No ‚Äî I wouldn‚Äôt describe this as ‚Äúsemantic refactoring.‚Äù\nWhat‚Äôs happening here is compression without semantic collapse. Most embedding pipelines lose meaning because they treat similarity as a proxy for intent. We don‚Äôt.\n\nThe result isn‚Äôt cleaner geometry because of a clever re-arrangement ‚Äî it‚Äôs cleaner because the representation was never optimized around surface correlation in the first place. Meaning is preserved under compression rather than reconstructed after the fact.\n\nThat distinction matters. If you start from similarity-first embeddings, dimensionality reduction destroys disambiguation. If you start from meaning-bearing structure, compression doesn‚Äôt degrade retrieval ‚Äî it removes noise.\n\nThe interesting signal isn‚Äôt that vectors got smaller. It‚Äôs that retrieval quality didn‚Äôt regress when they did.\n\nThat‚Äôs the point.",
              "score": 1,
              "created_utc": "2026-01-04 19:36:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxom1nb",
          "author": "bigsidhu",
          "text": "One other thing that helps is keeping track of what documents and chunks are being used over time, then deleting ones that don‚Äôt/shouldn‚Äôt get pulled.\n\nHave found that there is always a good portion of documents that take up a ton of space that never get pulled and wouldn‚Äôt add value to our top 90% of queries.",
          "score": 1,
          "created_utc": "2026-01-04 19:55:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxoni1z",
              "author": "getarbiter",
              "text": "That's a solid operational practice ‚Äî pruning unused chunks keeps the index lean and reduces noise at retrieval time.\n\nDifferent axis though. Corpus hygiene helps with what's in the index. What I'm focused on is what happens to retrieval quality when you compress what's already there.\n\nYou can have a perfectly pruned corpus and still lose disambiguation if compression collapses meaning.\nBoth matter. They're just solving different problems.",
              "score": 1,
              "created_utc": "2026-01-04 20:02:03",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nxopyji",
              "author": "KVT_BK",
              "text": "What happens to 10% queries after deletion ?",
              "score": 1,
              "created_utc": "2026-01-04 20:13:20",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nxon67j",
          "author": "ItsFuckingRawwwwwww",
          "text": "Solid testing. It really highlights how much 'bloat' exists in standard embeddings if you can strip 90% of the data and still get better results.\n\nWe‚Äôve been attacking this same storage nightmare but approached it from a totally different angle. Instead of just compressing the vectors, we focused on the underlying architecture of the vectors themselves with zero compression needed, but still with using any vector DB.\n\nWe tested our method on the Project Gutenberg library (50k books) and the results were pretty wild:\n-Storage: 99.5% reduction.¬† \n-Speed: 10x faster retrieval.¬† \n-Accuracy: 2.1x improvement.¬† \n\nYou're definitely right about the 'cleaner retrieval,' it turns out a massive chunk of what standard RAG stores is just noise that confuses the model. Eliminating that noise is a huge win.¬† \n\nCurious if you‚Äôve tested how the 72D compression handles highly dynamic data (constant updates)? That‚Äôs usually where we see static compression start to struggle.",
          "score": 1,
          "created_utc": "2026-01-04 20:00:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxplra9",
          "author": "hrishikamath",
          "text": "Without accuracy on evals, it‚Äôs hard to say",
          "score": 1,
          "created_utc": "2026-01-04 22:41:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxpoa2r",
              "author": "getarbiter",
              "text": "Here's what holds at 72D (10.7√ó compression from 768D):\n\nDisambiguation:\n\"bank\" alone ‚Üí financial (0.701) > river (0.412)\n\"bank of the Mississippi river\" ‚Üí shoreline (0.537) > credit union (0.341)\n\n\"Python memory management\" ‚Üí garbage collection (0.507) > ball python (0.406) > Monty Python (0.008)\n\n\"Crane safety regulations on construction sites\" ‚Üí Tower cranes (0.828) > Sandhill cranes (0.325)\nContext shifts the ranking. Same 72D model.\nSemantic rejection:\n\"The pilot took off\"\n  0.618  airplane pilot\n -0.110  remove clothing  ‚Üê NEGATIVE\nIt doesn't just rank low. It rejects.\nvs PCA at same compression:\nSimilarity preservation: 0.8693 ‚Üí 0.9653 (+11%)\nPCA can't separate \"bank\" (financial) from \"bank\" (river).\n\nARBITER does.\n\nWhat eval would you want to see?",
              "score": 1,
              "created_utc": "2026-01-04 22:53:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxpqfz9",
                  "author": "hrishikamath",
                  "text": "On end to end tasks, like actual retrieval task",
                  "score": 1,
                  "created_utc": "2026-01-04 23:04:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nxx619s",
          "author": "Academic_Track_2765",
          "text": "yes we did this too, 1536 to 768 to 384 with minimal loss. If you are using openai embeddings, you can even use a parameter to adjust the size, but there are ways to do this on your own too. You can start with this paper.  \n[https://arxiv.org/abs/2205.13147](https://arxiv.org/abs/2205.13147), also look into spare autoencoders. But I think this thread seems to be some marketing mumbo jumbo...\n\nhere is another post.   \n[https://www.reddit.com/r/devops/comments/1q4bzef/we\\_built\\_a\\_github\\_action\\_that\\_could\\_have/](https://www.reddit.com/r/devops/comments/1q4bzef/we_built_a_github_action_that_could_have/)\n\nI would recommend a grain of salt.",
          "score": 1,
          "created_utc": "2026-01-06 00:43:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxxksav",
              "author": "getarbiter",
              "text": "Look at the table again.\n\nAt 768D (standard embeddings), disambiguation is broken.\n\nAt 72D, disambiguation works.\n\nThat‚Äôs not ‚Äúminimal loss‚Äù ‚Äî that‚Äôs the lower-dimensional representation outperforming the original on semantic separation.\n\nMethods like Matryoshka or sparse autoencoders are optimizing to preserve cosine similarity under compression. \n\nThat‚Äôs useful, but it doesn‚Äôt address cases where the original space already collapses meanings (e.g. ‚Äúbank‚Äù finance vs river, ‚Äúpython‚Äù code vs animal).\n\nThis isn‚Äôt post-hoc shrinking of 1536D vectors. \n\nIt‚Äôs a different representation that encodes meaning directly, which is why retrieval behavior changes rather than just degrading gracefully.\n\nThe DevOps post you linked uses the same engine for a different task ‚Äî coherence checking instead of retrieval.\n\nSame model, different surface area.\n\nIf you want to sanity-check it yourself, there‚Äôs a public endpoint here: https://api.arbiter.traut.ai/public/compare\n\nHappy to run a concrete eval if there‚Äôs a specific retrieval task you care about.",
              "score": 1,
              "created_utc": "2026-01-06 02:02:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzai7x",
      "title": "Metadata extraction from unstructured documents for RAG use cases",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzai7x/metadata_extraction_from_unstructured_documents/",
      "author": "Serious-Barber-2829",
      "created_utc": "2025-12-30 06:14:14",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.92,
      "text": "I'm an engineer at Aryn (aryn.ai) and I work in document parsing and extraction and help customers build RAG solutions.  We recently launched a new metadata extraction feature that allows you to extract metadata/properties of interest from unstructured documents using JSON schemas.  I know this community is really big on various ways of dealing with unstructured documents (PDFs, docx, etc) for the purpose of getting them ready for RAG and LLMs.  Most of the use cases I see talked about here are around pulling out text and chunking and embedding and ingesting into a vector database with a heavy emphasis on self-hosting.  We believe that metadata extraction is going to provide a differentiation for RAG because the process of imposing structure on the data using schemas opens the door for many existing data analytics tools that work on structured data (think relational databases with catalogs).  Anyone actively looking into or working on this for their RAG projects?  Are you already using something for metadata extraction.  If so, how has your experience been using it?  What's working well and what's lacking?  I'd love to hear your experience!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pzai7x/metadata_extraction_from_unstructured_documents/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwp3a2f",
          "author": "AsparagusKlutzy1817",
          "text": "What metadata do you have in mind? The document structure itself like headings, subheadings?\n\nI have been building a text extraction library over christmas: https://github.com/Horsmann/sharepoint-to-text. This one also picks up metadata it finds in the source document. This is currently limited to author, creation date etc. I don't call them metadata but for .docx for instance I also separate the tables to work on them afterwards if any table-processing is desired (caller needs to implement this - i just pull the tables)",
          "score": 1,
          "created_utc": "2025-12-30 07:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrbndg",
              "author": "Serious-Barber-2829",
              "text": "Yes, things like title, authors would be metadata.  But it can be any pieces of information you are interested in pulling out of a document.  Think invoices (invoice number, address, total amount), contracts, tax forms, etc.",
              "score": 1,
              "created_utc": "2025-12-30 16:59:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpawga",
          "author": "Extreme-Brick6151",
          "text": "Metadata is the unsexy part of RAG that actually moves the needle. Once teams enforce schema-level metadata, retrieval quality, filtering, and access control improve way more than just tuning chunk sizes. Curious how you‚Äôre handling schema drift and messy edge cases across mixed doc types.",
          "score": 1,
          "created_utc": "2025-12-30 09:09:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrb9vq",
              "author": "Serious-Barber-2829",
              "text": "\\> Metadata is the unsexy part of RAG that actually moves the needle. Once teams enforce schema-level metadata, retrieval quality, filtering, and access control improve way more than just tuning chunk sizes.\n\nI couldn't agree more!\n\nWe are not yet tackling use cases where schema drift would be an issue.  We are dealing with documents like contracts, invoices, forms, etc.  But there are some \"standard\" practices in streaming/PubSub where you use schema registries and schema validation to deal with schema evolution.",
              "score": 1,
              "created_utc": "2025-12-30 16:58:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nws9o4a",
          "author": "valuechase",
          "text": "In my experience working with complex PDFs with unstructured data, the limitations of RAG are less on retrieval and much more at the parsing step. I‚Äôm working with Financial documents and even the best vision based parsers make mistakes when parsing tables from a pdf. You can mitigate this to an extent by using traditional RAG for narrative and maybe for table related queries, routing those using an index (and metadata extraction of full document) to an LLM, providing the LLM with the full document. This is maybe expensive path but probably more reliable.",
          "score": 1,
          "created_utc": "2025-12-30 19:38:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtpwby",
              "author": "Serious-Barber-2829",
              "text": "Do you have something working reliably enough in production?",
              "score": 1,
              "created_utc": "2025-12-30 23:57:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuh5s0",
          "author": "absqroot",
          "text": "Sorry, I don't quite get it. What do you mean by metadata? Basic metadata about the page, metadata like bounding boxes, font sizes & weights, or metadata like numerical data and tables?",
          "score": 1,
          "created_utc": "2025-12-31 02:32:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwun119",
              "author": "Serious-Barber-2829",
              "text": "Metadata or \"property\" as in any piece of interest.  It can be any of the things you mentioned, but it can be specific values found on a page (invoice number, address, e.g.)",
              "score": 1,
              "created_utc": "2025-12-31 03:06:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwz7wql",
          "author": "drfritz2",
          "text": "Metadata is needed if working with a lot of data. It would be SQL and linked with the vector or graph.\n\nSo if you want to have filters to search, it would be possible.\n\nOf course the metadata template should be very flexible. The model itself should extract and create/adapt the fields. \n\nThe issue os that the hole system should be \"hot\". It's metamorphic, because of the fast pace of the tech development",
          "score": 1,
          "created_utc": "2025-12-31 21:05:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}