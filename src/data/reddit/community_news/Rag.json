{
  "metadata": {
    "last_updated": "2026-02-11 17:29:57",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 155,
    "file_size_bytes": 211031
  },
  "items": [
    {
      "id": "1r1o9qz",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r1o9qz/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:24",
      "score": 117,
      "num_comments": 28,
      "upvote_ratio": 0.98,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) ‚Äì 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt‚Äôs trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r1o9qz/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4s40uy",
          "author": "Negative-Age-4566",
          "text": "Cool project !   \nAs an improvement I'd suggest doing not only semantic search bu hybrid search using dense embeddings for semantic search + BM25 for keyword search.   \nHeres a tutorial on how to do it using qdrant [https://qdrant.tech/documentation/tutorials-search-engineering/reranking-hybrid-search/](https://qdrant.tech/documentation/tutorials-search-engineering/reranking-hybrid-search/)\n\n",
          "score": 11,
          "created_utc": "2026-02-11 11:11:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sdoe4",
              "author": "Cod3Conjurer",
              "text": "Appreciate it\n\nYeah, hybrid search (dense + BM25) makes a lot of sense, especially for exact entity lookups.\n\nI'll definitely explore that direction combining semantic retrieval with lexical scoring could improve precision quite a bit.",
              "score": 4,
              "created_utc": "2026-02-11 12:26:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4qzc2h",
          "author": "redditorialy_retard",
          "text": "Epstein bot incoming¬†",
          "score": 7,
          "created_utc": "2026-02-11 05:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s6zr0",
              "author": "GreenHell",
              "text": "You mean [MechaEpstein-8000](https://www.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/)? Someone at /r/LocalLLaMA already did that.",
              "score": 5,
              "created_utc": "2026-02-11 11:36:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4qzijv",
              "author": "Cod3Conjurer",
              "text": "what?",
              "score": 0,
              "created_utc": "2026-02-11 05:08:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ro0g1",
                  "author": "nikita2206",
                  "text": "As in chatbot with epstein personality, grounded in facts",
                  "score": 3,
                  "created_utc": "2026-02-11 08:44:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rhhbj",
          "author": "FortiCore",
          "text": "Vibe coded ? Looks like result of a one or two prompts  \nsame for CrawlAI-RAG you posted yesterday.\n\nEven this post is AI generated",
          "score": 8,
          "created_utc": "2026-02-11 07:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ro5zy",
              "author": "Cod3Conjurer",
              "text": "Once you build a solid RAG pipeline, you can plug in any large dataset, it's mostly about preprocessing and tuning retrieval, not rewriting everything.\n\nSo no, it's not just \"two prompts.\" The architecture and pipeline matter.\n\nAnd yes, I use Al to help write code and posts, but I understand the full process and control the system end-to-end",
              "score": 2,
              "created_utc": "2026-02-11 08:45:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rs3bm",
                  "author": "FortiCore",
                  "text": "Rudimentary cleaning, chunking, dedups and embedding is a basic functional RAG  \nWouldnt call it a \"Solid RAG pipeline\"\n\nThe thing is AI would do this in 5 prompts, if not three (but it will do in three actually).\n\n**There isnt anything special the way the post makes it look like.**\n\nAll it does for \"**Constantly tweaking for better retrieval & performance**\" is : cleanup new lines, and spaces, 80 chars overlap embed in batch. Tbvh, it not even at the level of a good RAG POC, its a demo for how to do rudimentary RAG with langchain, would fail to give satisfactory reason for almost any real world usecase.\n\nThe post it self is clearly AI written to make it look like a worthy opensource RAG project\n\nStill helpful for some one who want to see RAG in its most simplest form.",
                  "score": 1,
                  "created_utc": "2026-02-11 09:23:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rpukb",
          "author": "ShadowStormDrift",
          "text": "Recommend adding reviewer+corrector agents. \n\nReviewer double checks answers grounded in context and penalizes omissions. \n\nCorrector does what you think it does.\n\nIncreases latency but good when you need to be extra sure you aren't seeing bullshit.\n\nOtherwise providing a source list alongside responses for manual review",
          "score": 3,
          "created_utc": "2026-02-11 09:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tf1fk",
              "author": "Cod3Conjurer",
              "text": "never thought of it that way definitely gonna try",
              "score": 1,
              "created_utc": "2026-02-11 15:51:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rdzv3",
          "author": "-Cubie-",
          "text": "Very cool! What embedding model are you using?",
          "score": 2,
          "created_utc": "2026-02-11 07:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4redml",
              "author": "Cod3Conjurer",
              "text": "I'm using all-MiniLM-L6-v2 from SentenceTransformers, lightweight, fast, and solid for large-scale semantic retrieval.",
              "score": 5,
              "created_utc": "2026-02-11 07:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rhxx3",
                  "author": "-Cubie-",
                  "text": "Nice, thanks",
                  "score": 2,
                  "created_utc": "2026-02-11 07:46:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4t8di8",
          "author": "Tight-Actuary-3369",
          "text": "It's incredible, man. Keep it up, excellent work.",
          "score": 2,
          "created_utc": "2026-02-11 15:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4teczl",
              "author": "Cod3Conjurer",
              "text": "thanks man",
              "score": 1,
              "created_utc": "2026-02-11 15:48:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rd0zw",
          "author": "Extension_Armadillo3",
          "text": "I recommend BM25 to find passages with e.g Elon or something like that. And in addition, a re-ranker, but I think that could be hard to find without Zensurship.",
          "score": 3,
          "created_utc": "2026-02-11 07:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rducs",
              "author": "-Cubie-",
              "text": "Normal cross-encoder rerankers shouldn't censor, but they might be bad at parsing the e.g. email formats. I agree that BM25/Hybrid search would be cool, perhaps even with a slider denoting how much weight to give to the semantic vs lexical side (sometimes you might just want full lexical).",
              "score": 2,
              "created_utc": "2026-02-11 07:08:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4re823",
              "author": "Cod3Conjurer",
              "text": "I'm not using BM25 right now, it's pure semantic retrieval using embeddings (SentenceTransformers + Chroma similarity search).\n\n\nHybrid search with BM25 + vector reranking would definitely be an interesting improvement though.",
              "score": 1,
              "created_utc": "2026-02-11 07:11:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ryy75",
          "author": "nirmalyamisra",
          "text": "whats your hardware and tech stack",
          "score": 1,
          "created_utc": "2026-02-11 10:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s18z0",
              "author": "Cod3Conjurer",
              "text": "5060 8gb vram\n24 gb ram\nI7 14gen \n\nPython, LangChain, fastapi",
              "score": 3,
              "created_utc": "2026-02-11 10:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4s1ska",
                  "author": "nirmalyamisra",
                  "text": "thanks",
                  "score": 2,
                  "created_utc": "2026-02-11 10:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4r03pl",
          "author": "eslove24",
          "text": "Whats your system prompt?",
          "score": 1,
          "created_utc": "2026-02-11 05:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r7sjf",
              "author": "Cod3Conjurer",
              "text": "System prompt:\n\n\n\"You are a retrieval-based assistant. Answer ONLY using the provided context. If the answer is not present, say: 'I could not find this information in the documents.' Limit the answer to 5-6 lines.\"",
              "score": 3,
              "created_utc": "2026-02-11 06:15:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxct95",
      "title": "My weekend project just got a $1,500 buyout offer.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "author": "Physical_Badger1281",
      "created_utc": "2026-02-06 09:08:40",
      "score": 77,
      "num_comments": 45,
      "upvote_ratio": 0.9,
      "text": "I built a simple RAG (AI) starter kit 2 months ago.\n\nThe goal was just to help devs scrape websites and PDFs for their AI chatbots without hitting anti-bot walls.\n\nProgress:\n- 10+ Sales (Organic)\n- $0 Ad Spend\n- $1,500 Acquisition Offer received yesterday.\n\nI see a lot of people overthinking their startup ideas. This is just a reminder that \"boring\" developer tools still work. I solved a scraping problem, put up a landing page, and the market responded.\n\nI'm likely going to reject the offer and keep building, but it feels good to know the asset has value.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3vl7fc",
          "author": "Available-Appeal-173",
          "text": "Did you open source it?",
          "score": 4,
          "created_utc": "2026-02-06 10:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vri0e",
              "author": "Physical_Badger1281",
              "text": "It‚Äôs built on open-source tech (Next.js, LangChain, Pinecone), but the repo itself is a paid boilerplate.\n\n\nI decided to sell it as a 'Source-Available' product because the real value isn't the stack, but the 100+ hours of glue-code (specifically the Puppeteer scraping config and Auth setup) that saves you from debugging for weeks.\nI kept the price super low ($5 range) so it‚Äôs accessible to almost anyone who wants to skip the setup.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 4,
              "created_utc": "2026-02-06 11:02:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xrwyp",
                  "author": "Playwithme408",
                  "text": "Where did you list it.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:43:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vsick",
          "author": "Joy_Boy_12",
          "text": "I didn't understand the use case.\nI have a chatbot but I scrape the website with docling and insert the data to vector db so it will be available for my chatbot.\n\n\nWhere's the need for your product?",
          "score": 3,
          "created_utc": "2026-02-06 11:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vuh35",
              "author": "Physical_Badger1281",
              "text": "Great question. If you're scraping static documentation or PDFs, tools like Docling or standard parsers work perfectly.\n\nThe specific use case FastRAG solves is Client-Side Rendered (SPA) websites. A lot of modern React/Next.js sites return empty HTML until the JavaScript runs. Standard scrapers often fail there.\n\nFastRAG uses a headless browser instance (Puppeteer) to fully hydrate the DOM before scraping, so you catch the content that only appears after JS execution. It‚Äôs overkill for simple docs, but necessary for a robust 'Chat with Any Website' SaaS.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 5,
              "created_utc": "2026-02-06 11:27:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vy3ns",
              "author": "motorsportlife",
              "text": "Curious if you are you hashing files to ensure you don't scrape the same document twice and duplicate entries in the db",
              "score": 2,
              "created_utc": "2026-02-06 11:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w1kej",
                  "author": "Physical_Badger1281",
                  "text": "Currently, we rely on URL-based upserts to prevent database duplication (same URL = overwrite existing vectors)\n\nHowever, actual Content Hashing is on the roadmap for v1.5. It would definitely save on embedding costs for pages that haven't been updated. \n\nIs there a specific library you prefer for that flow?",
                  "score": 1,
                  "created_utc": "2026-02-06 12:20:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4cmf9j",
          "author": "Mary_Avocados",
          "text": "Who and how did you get the offer? Just random email?",
          "score": 3,
          "created_utc": "2026-02-09 00:49:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dh7tl",
              "author": "Physical_Badger1281",
              "text": "From indiemaker, Fastrag is listed there.",
              "score": 1,
              "created_utc": "2026-02-09 03:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o6cei",
          "author": "Fragrant-Dark5656",
          "text": "Looks great ‚ú® How do you get buyers ? Do you do email marketing or social media ads or something else ?",
          "score": 3,
          "created_utc": "2026-02-10 19:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ourdb",
              "author": "Key-Boat-7519",
              "text": "Main thing is hanging where devs already complain about scraping: Reddit, Hacker News, specific Discords. Share quick demos, answer questions, link only when relevant. I use Gumroad for checkout, SimpleAnalytics for tracking, and Pulse for Reddit alerts to jump into fresh scraping threads fast.",
              "score": 2,
              "created_utc": "2026-02-10 21:37:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o407acm",
          "author": "finnomo",
          "text": "Launched projects cost at least 10 times more than that. Don't sell.",
          "score": 2,
          "created_utc": "2026-02-07 01:15:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40t1bl",
              "author": "Physical_Badger1281",
              "text": "Thanks for the sanity check! üôå\n\nSometimes when you're deep in the code, you forget how much effort it actually took to get from 'localhost' to 'live URL with Gumroad attached.'\n\nI definitely decided to hold. The validation of the offer was nice, but the asset is worth way more to me as a business.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-07 03:31:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hgu67",
          "author": "Dmoneybaby23",
          "text": "No offense but $1500 is honestly nothing, i get paid more than that to do nothing while working on projects, you should hold out and wait for your project to get bigger",
          "score": 2,
          "created_utc": "2026-02-09 19:26:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jwxx0",
              "author": "Physical_Badger1281",
              "text": "Yessss!!!\n\nGive it a try https://www.fastrag.live/",
              "score": 1,
              "created_utc": "2026-02-10 03:21:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iz2pt",
          "author": "SharpRule4025",
          "text": "The scraping part is honestly where most RAG projects hit the wall first. Everyone assumes you already have clean text but getting through Cloudflare and anti-bot stuff on modern sites is half the battle.\n\nSmart to build around that pain point. The people buying probably just want content for their vector store without fighting browser fingerprinting for a week.",
          "score": 2,
          "created_utc": "2026-02-10 00:03:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jx97o",
              "author": "Physical_Badger1281",
              "text": "Thanks!!!\nGive it a try [Fastrag](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-10 03:23:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lmdbh",
          "author": "AwayUnderstanding701",
          "text": "Congrats!!! Keep the good work and don't stop building!!",
          "score": 2,
          "created_utc": "2026-02-10 11:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lmgta",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!!!",
              "score": 1,
              "created_utc": "2026-02-10 11:56:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z0dlp",
          "author": "AnxietyPrudent1425",
          "text": "You can go bigger. $1500 is barely worth the AI credits and food",
          "score": 2,
          "created_utc": "2026-02-06 21:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40sn10",
              "author": "Physical_Badger1281",
              "text": "100%. That‚Äôs exactly why I turned it down.\n\nIf I just sell a couple of licenses a week, I beat that offer in a few months. I‚Äôd rather own a cash-flowing asset than take a quick $1.5k exit. I'm betting on the long game here!\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 03:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w8289",
          "author": "StatusFoundation5472",
          "text": "True story",
          "score": 1,
          "created_utc": "2026-02-06 13:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w8fdz",
              "author": "Physical_Badger1281",
              "text": "Facts. Just gotta keep building! üöÄ",
              "score": 1,
              "created_utc": "2026-02-06 13:05:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wcqtf",
          "author": "StatusFoundation5472",
          "text": "I checked out your landing page. A question please. What's your experience with gumroad?",
          "score": 1,
          "created_utc": "2026-02-06 13:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z0uxk",
              "author": "AnxietyPrudent1425",
              "text": "Lemon Squeezy is the way to go. Especially if you sell licenses.",
              "score": 2,
              "created_utc": "2026-02-06 21:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40tefk",
                  "author": "Physical_Badger1281",
                  "text": "Lemon Squeezy, okay I'll surely give it a try.\nThanks!",
                  "score": 2,
                  "created_utc": "2026-02-07 03:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wdpah",
          "author": "Interesting-Town-433",
          "text": "How are you able to run an llm on your site without getting crushed by gpu cost?",
          "score": 1,
          "created_utc": "2026-02-06 13:35:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3weh0l",
              "author": "Physical_Badger1281",
              "text": "Great question. Since this is utilizing OpenAI's API, there are no fixed GPU costs - it's purely pay-as-you-go.\n\nFor the live demo, I default to gpt-4o-mini, which is incredibly cheap. I also have rate limiting set up on Vercel to ensure no one user drains the API credits.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-06 13:40:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wg082",
          "author": "jennylane29",
          "text": "Great tool, wish you good luck!\nHow did you market your solution/landing page? I find getting it out there is particularly tricky.",
          "score": 1,
          "created_utc": "2026-02-06 13:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wj8l7",
              "author": "Physical_Badger1281",
              "text": "It really is the hardest part! For this project, I stuck to a 100% organic 'Build in Public' approach:\n- Reddit: Posting deep-dives on the tech stack (like why I chose Puppeteer over Cheerio) in subreddits like r/Nextjs, r/rag and r/SaaS.\n- Twitter/X: Sharing revenue milestones and 'behind the scenes' screenshots.\nI haven't spent a dime on ads. I think developers just appreciate seeing the code/process rather than a flashy marketing video",
              "score": 3,
              "created_utc": "2026-02-06 14:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xj3wf",
          "author": "TalosStalioux",
          "text": "Congrats dude",
          "score": 1,
          "created_utc": "2026-02-06 17:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xjoq4",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-06 17:03:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o434hzl",
          "author": "Top_Yogurtcloset_258",
          "text": "I might actually buy this, I've been trying to scrape client loaded data for a while using Firecrawl, and it's annoying. Have you used Firecrawl before?",
          "score": 1,
          "created_utc": "2026-02-07 14:43:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o434rek",
              "author": "Physical_Badger1281",
              "text": "No, but you can try the demo\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 14:45:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o436526",
                  "author": "Top_Yogurtcloset_258",
                  "text": "It gave an error when I put the link in the demo\n\nWebsite link I put: [https://partners.naeem.cg.sa/book/eleven-wishes-salon](https://partners.naeem.cg.sa/book/eleven-wishes-salon)  \n\n\nError scraping URL.\n\nError message:  \nAn error occurred with your deployment\n\nFUNCTION\\_INVOCATION\\_TIMEOUT\n\ndxb1::7fw5m-1770475864803-9e9fcb110063",
                  "score": 1,
                  "created_utc": "2026-02-07 14:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h1vxu",
          "author": "NayanCat009",
          "text": "Please close the deal and get the money. Someone is going to build it in hours. Not demotivating or pulling down, this is the harsh reality of software development in the AI era.",
          "score": 1,
          "created_utc": "2026-02-09 18:16:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdkek",
      "title": "So is RAG dead now that Claude Cowork exists, or did we just fall for another hype cycle?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "author": "ethanchen20250322",
      "created_utc": "2026-02-05 06:12:54",
      "score": 55,
      "num_comments": 28,
      "upvote_ratio": 0.78,
      "text": "Every few months someone declares RAG is dead and I have to update my resume again.\n\nThis time it's because¬†**Claude Cowork**¬†(and similar long-running agents) can \"remember\" stuff across sessions. No more context window panic. No more \"as I mentioned earlier\" when you definitely did not mention it earlier.\n\nSo naturally: \"Why do we even need RAG anymore??\"\n\nI actually dug into this and... It's not that simple (shocking, I know).\n\nBasically:\n\n* **Agent memory**¬†= remembers what IT was doing (task state)\n* **RAG**¬†= retrieves what THE WORLD knows (external facts)\n\nOne is your agent's personal journal. The other is the company wiki it keeps forgetting exists.\n\n**An agent**¬†with perfect memory but no retrieval is like a coworker who remembers every meeting but never reads the docs. We've all worked with that guy.\n\n**A RAG system**¬†with no memory is like that other coworker who reads everything but forgets what you talked about 5 minutes ago. Also that guy.\n\nTurns out the answer is: stack both. Memory for state, retrieval for facts, vector DB(Like Milvus) underneath.\n\nRAG isn't dead. It just got a roommate who leaves dishes in the sink.\n\nüëâ Full breakdown here if you want the deep dive [https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md](https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md)\n\n**TL;DR:**¬†Claude Cowork's memory is for tracking task state. RAG is for grounding the model in external knowledge. They're complementary, not competitive. We can all calm down (for now).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3oeh9p",
          "author": "stingraycharles",
          "text": "Eh, Claude Projects has been able to do that since forever, they just made that same functionality available in Claude Cowork. \n\nIf anything, this is a validation of RAGs, as this functionality is an implementation of it.",
          "score": 15,
          "created_utc": "2026-02-05 07:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3plwdr",
          "author": "TechnicalGeologist99",
          "text": "This is just a misunderstanding by people on what RAG is. \n\nIt is often conflated with: semantic search, on prem/hybrid/in-house solutions to document search. \n\nNote that RAG systems are a high level category of systems. Namely those that have the concerns:\n\n- Automatic retrieval of information \n- Injection of relevant information into a prompt for Autoregressive Causal Inference \n\n RAG does not mean \"embed and retrieve documents to guide an LLM at query time\"\n\nRather RAG is a broad topic of which \"embed and retrieve documents....\" is a member. \n\nKey word search is also RAG if you put the results into an LLM.\n\nRandom search is also RAG if you put the results into an LLM.\n\nHaving the system send an email to an office worker to ask for information at query time is still RAG (it's just not useful) \n\nRAG is not dead. It is a battle tested framework for designing systems. \n\nWhen people say \"RAG is dead\" to me that is a smell that tells me they've never written a line of code in their life and they are likely one of those jumped up sales people earning 250k per year for selling copilot licenses.",
          "score": 16,
          "created_utc": "2026-02-05 13:12:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sem33",
              "author": "ThirDiamondEye",
              "text": "I came to see people correcting the question, and am surprised I only found you.",
              "score": 2,
              "created_utc": "2026-02-05 21:20:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sljf2",
                  "author": "TechnicalGeologist99",
                  "text": "Tis a pet peeve of mine haha",
                  "score": 1,
                  "created_utc": "2026-02-05 21:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oo7f9",
          "author": "eurydice1727",
          "text": "Rag protects security. On premise solutions where companies do not want data exposed to any cloud models especially.",
          "score": 13,
          "created_utc": "2026-02-05 08:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oc06f",
          "author": "scoby_cat",
          "text": "RAG is not ‚Äúthe world‚Äù though",
          "score": 3,
          "created_utc": "2026-02-05 06:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3olrnb",
          "author": "Diligent-Builder7762",
          "text": "two ways of doing same thing, rag and custom pipelines to understand the codebase / using llm driven agents to do it... not much different, one seem to be more future leaning agentic way, the other efficiency.",
          "score": 2,
          "created_utc": "2026-02-05 08:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pb0ku",
          "author": "HealthOk5149",
          "text": "I don't think RAG will be dead anytime soon. Many companies still trying to figure out how to incorporate RAG solutions in their processes. In medical/law/financial sectors they have tons od data in air-gapped environment (due to security reasons/policies ofc especially in EU), so proper on-premise RAG would be a blessing for them imo.",
          "score": 2,
          "created_utc": "2026-02-05 11:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pwd2x",
          "author": "Diligent-Fly3756",
          "text": "https://preview.redd.it/96zneunsoohg1.jpeg?width=1107&format=pjpg&auto=webp&s=5e01383bb79ca9f81fc09569fe84dda2fefe7e9a\n\nHere‚Äôs what Boris Cherny (who invented Claude Code) said.",
          "score": 2,
          "created_utc": "2026-02-05 14:12:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d5kxq",
              "author": "sbmitchell",
              "text": "Read as \"we just need to grep shit now\"",
              "score": 1,
              "created_utc": "2026-02-09 02:34:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3t260c",
          "author": "zulrang",
          "text": "It doesn't matter what you label it -- it's all context retrieval.  ",
          "score": 2,
          "created_utc": "2026-02-05 23:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3urpnw",
          "author": "voycey",
          "text": "I swear each time I see something like this I am convinced that no one actually knows what RAG is",
          "score": 2,
          "created_utc": "2026-02-06 05:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3obawp",
          "author": "One_Milk_7025",
          "text": "Good explanation.. rag is not dead and when there is no bm25 similarity or regex doesn't work then when the rag shines.. yes claude regex find will work after 3-4 try because it will guess what can be the word for that particular query..\nA agent need both a rag and a memory.. single memory or rag won't survive solo",
          "score": 3,
          "created_utc": "2026-02-05 06:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pmoox",
              "author": "TechnicalGeologist99",
              "text": "Bm25 is not the opposite of RAG. It is a method of retrieval. Any method of retrieval still fits the RAG framework.\n\nSemantic search is an example of retrieval...it is not the definition of RAG",
              "score": 3,
              "created_utc": "2026-02-05 13:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pneuh",
          "author": "Rock--Lee",
          "text": "Ofcourse it's not dead. I'm building a custom GraphRAG with neo4j which I use as a knowledge base and memory service for my app and agents. I'd like Claude Coworker try to use its memory for multiple books and documents across domains, with user notes in there too.\n\nRAG on itself isn't something you replace. It's a technique and Claude uses RAG, but that doesn't make other RAG systems obsolete. That's like saying should I uninstall Outlook now that Claude can also fetch my mails.",
          "score": 1,
          "created_utc": "2026-02-05 13:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ptv70",
          "author": "Main-Space-3543",
          "text": "Not sure this is the right way to think about RAG.  \n\nThe alternative to RAG is not Cowork or Code.  In fact - Cursor / Anthropic used RAG to power their coding agents at one point.   \n\nAnthropic switched from RAG to agentic search because it‚Äôs allowed them to scale past the limits of RAG.  \n\nAll 3 of these things are worth paying attention and to leverage in the right context:\n\n- increasing context windows \n- RAG\n- agentic search \n\nAll 3 make it possible to improve the context of the query handed to the LLM. \n\nCoding agents / cowork - very different patterns.  \n\nBy the way - the LLM is the representation of the worlds data - not RAG.",
          "score": 1,
          "created_utc": "2026-02-05 13:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk3m0",
          "author": "Wooden_Leek_7258",
          "text": "Its a component not the system. Anyone try blending graph, vector and SQL systems in a layerd approach? Working for me.",
          "score": 1,
          "created_utc": "2026-02-05 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rlwjt",
          "author": "infinitejennifer",
          "text": "lol.",
          "score": 1,
          "created_utc": "2026-02-05 19:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sdpfo",
          "author": "satechguy",
          "text": "RAG is dead, again?",
          "score": 1,
          "created_utc": "2026-02-05 21:15:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t3eb7",
          "author": "CEBarnes",
          "text": "RAG makes using a database more flexible. Like talking to ‚ÄòMother‚Äô in the Alien series. Using AI+skills+api seems better than a fixed UI with functions mapped to buttons.",
          "score": 1,
          "created_utc": "2026-02-05 23:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tg48g",
          "author": "chungyeung",
          "text": "Rag, rag never changes.",
          "score": 1,
          "created_utc": "2026-02-06 00:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x0h49",
          "author": "Challseus",
          "text": "I must be doing rag wrong because every time it‚Äôs dead, it still works for me ü§∑üèæ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-06 15:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xmgv6",
          "author": "jeffreyhuber",
          "text": "Cowork does retrieval augmented generation and search. ",
          "score": 1,
          "created_utc": "2026-02-06 17:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41mf7m",
          "author": "Fantastic_suit143",
          "text": "Woah it didn't know that damn I will try to implement it in my own project now this is revolutionary yeah rag really is that person doesn't remember anything even after reading the context earlier on .",
          "score": 1,
          "created_utc": "2026-02-07 07:22:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o442mqc",
          "author": "Late_Ad_2350",
          "text": "Incompetency level is overwhelming in all of these AI subs...",
          "score": 1,
          "created_utc": "2026-02-07 17:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o449fmz",
          "author": "Usual-Orange-4180",
          "text": "Is a pattern dead now that here is one product from one company?\n\nNo",
          "score": 1,
          "created_utc": "2026-02-07 18:08:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4a6az8",
          "author": "mohdgame",
          "text": "I dont think yoi know what RAG means. How does it have anything to do with claude cowork?",
          "score": 1,
          "created_utc": "2026-02-08 17:19:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fzsdx",
          "author": "Hakkology",
          "text": "Wtf is Rag when did it die and what is claude cowork what are these tools its so hard to be swe these days",
          "score": 1,
          "created_utc": "2026-02-09 15:13:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r129pa",
      "title": "Knowledge Distillation for RAG (Why Ingestion Pipeline Matters More Than Retrieval Algorithm)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r129pa/knowledge_distillation_for_rag_why_ingestion/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-10 14:31:03",
      "score": 54,
      "num_comments": 19,
      "upvote_ratio": 0.94,
      "text": "Been spending way too much time debugging RAG systems that \"should work\" but don't, and wanted to share something that's been bothering me about how we collectively approach this problem.\n\nWe obsess over retrieval algorithms (hybrid search, reranking, HyDE, query decomposition) while completely ignoring that retrieval operates over fundamentally broken representations of knowledge.\n\nI started using a new approach that is working pretty well so far : Instead of chunking, use LLMs at ingestion time to extract and restructure knowledge into forms optimized for retrieval:\n\nLevel 1: Extract facts as explicit SVO sentences\n\nLevel 2 : Synthesize relationships spanning multiple insights\n\nLevel 3 : Document-level summaries for broad queries\n\nLevel 4 : Patterns learned across the entire corpus\n\nEach level serves different query granularities. Precision queries hit insights. Exploratory queries hit concepts/abstracts.\n\nI assume this works well beacuse LLMs during ingestion can spend¬†*minutes*¬†analyzing a document that gets used thousands of times. The upfront cost amortizes completely. And they're genuinely good at:\n\n* Disambiguating structure\n* Resolving implicit context\n* Normalizing varied phrasings into consistent forms\n* Cross-referencing\n\nTested this on a few projects involving financial document corpus : agent with distillation correctly identified which DOW companies were financial institutions, attributed specific risks with page-level citations, and supported claims with concrete figures. Naive chunking agent failed to even identify the companies reliably.\n\nThis is fully automatable with workflow-based pipelines:\n\n1. Table extraction (preserve structure via CV models)\n2. Text generation 1: insights from tables + text\n3. Text generation 2: concepts from insights\n4. Text generation 3: abstracts from concepts\n5. Text generation 4: table schema analysis for SQL generation\n\nEach component receives previous component's output. Final JSON contains original data + all distillation layers.\n\nAnyway figure this is one of those things where the industry is converging on the wrong abstraction and we should probably talk about it more.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r129pa/knowledge_distillation_for_rag_why_ingestion/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4miuy5",
          "author": "charlesrwest0",
          "text": "I have been using a similar approach. I also find it useful to have the model try to predict who would read it/what it would be used for and what associated questions are likely to be asked. Q/A pairs play very nicely with vector databases.",
          "score": 6,
          "created_utc": "2026-02-10 15:07:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mbw5i",
          "author": "Independent-Cost-971",
          "text": "Anyway, wrote this up in more detail if anyone's interested :¬†[https://kudra.ai/knowledge-distillation-for-ai-agents-and-rag-building-hierarchical-knowledge-from-raw-documents/](https://kudra.ai/knowledge-distillation-for-ai-agents-and-rag-building-hierarchical-knowledge-from-raw-documents/)\n\n( shameless self promotion I know but worth a read )",
          "score": 6,
          "created_utc": "2026-02-10 14:31:40",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4stcxg",
              "author": "True_Context_6852",
              "text": "Good read thank you¬†",
              "score": 1,
              "created_utc": "2026-02-11 14:01:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mhk0p",
          "author": "penguinzb1",
          "text": "this resonates. the problem with chunking is it destroys the semantic boundaries that actually matter for retrieval. structuring at ingestion makes sense but the hard part is validating that your distillation pipeline is actually extracting what you need without hallucinating connections",
          "score": 3,
          "created_utc": "2026-02-10 15:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4myuxh",
              "author": "ButterflyEconomist",
              "text": "For the chunking problem, I overlay them.\n\nIf I want to process data abcd, I chunk it into ab, bc, cd",
              "score": 1,
              "created_utc": "2026-02-10 16:24:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mj0ww",
          "author": "minaminotenmangu",
          "text": "The trouble is cost. all knowledge now has to go through an llm + embedding model. I guess its not too bad for some, it might be intetesting to find cheaper models that could do the rewriting of knowledge.\n\ni feel we need a disconnect between what we embed which is a shorter simpler text to what we retrieve.",
          "score": 3,
          "created_utc": "2026-02-10 15:08:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mk1nn",
              "author": "Popular_Sand2773",
              "text": "You just need to do knowledge distillation. You can get llm behavior at a fraction of the price on narrow tasks although the real savings is if you can move off of generative approaches all together while preserving the behavior. ",
              "score": 2,
              "created_utc": "2026-02-10 15:13:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mkfjb",
                  "author": "minaminotenmangu",
                  "text": "What exactly do you mean by knowledge distillation? Ots never clear to me what this actually entails for a corpus.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nlp2e",
              "author": "isthatashark",
              "text": "100% on your suggestion to use cheaper models. I've been doing a lot of research into this lately and you don't need a frontier model to get good results. \n\nWe use this technique for memory consolidation in Hindsight. Smaller models do a surprisingly good job. I mostly use the ones on Groq because the performance is so fast and the cost is low, but Ollama is also an option if you want something local and free (but slower).",
              "score": 2,
              "created_utc": "2026-02-10 18:09:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4modds",
          "author": "DeepWiseau",
          "text": "How does this work with a growing document library? How would it handle several thousand pages being added a week?",
          "score": 2,
          "created_utc": "2026-02-10 15:34:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n1qwh",
              "author": "Krommander",
              "text": "Probably have to push towards recursive indexing and knowledge anchoring to earlier knowledge graphs already in store for smaller increments. Reset for the whole mapping layer in regular intervals if there needs to be pruning of older info or a redraw of conceptual attractors.",
              "score": 1,
              "created_utc": "2026-02-10 16:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n5psc",
          "author": "Informal_Tangerine51",
          "text": "Distillation at ingestion helps retrieval quality but creates a new debugging problem: when retrieval fails, which distillation layer broke?\n\nYour 4-level approach works until precision query returns wrong info. Now you need to trace: was SVO extraction wrong, relationship synthesis hallucinated, summary incomplete, or pattern recognition overgeneralized? Without evidence of what each distillation step produced, debugging is guesswork. LLMs spent minutes analyzing doc, but you don't have artifacts showing what they extracted at each level.\n\nFinancial doc example: agent identified wrong risk attribution. Is it because Level 1 extracted wrong facts, Level 2 synthesized wrong relationships, or retrieval chose wrong layer? Chunking is simpler to debug - you see exactly what text was retrieved. Multi-layer distillation optimizes for quality but trades debuggability. Production systems need both retrieval quality and incident traceability.",
          "score": 2,
          "created_utc": "2026-02-10 16:55:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4meyku",
          "author": "fabkosta",
          "text": "What‚Äôs a CV model in the context of table extraction?",
          "score": 1,
          "created_utc": "2026-02-10 14:47:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n0uqg",
          "author": "Krommander",
          "text": "Very nice. I have found distillation to condense well into a recursive semantic hypergraph, as a map of the knowledge and interrelations.",
          "score": 1,
          "created_utc": "2026-02-10 16:33:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4n8j4q",
          "author": "Forsaken-Cod-4944",
          "text": "I'm new to this but wouldnt it take significantly more time to produce chunks with this (depends on llm i guess?)\n\n",
          "score": 1,
          "created_utc": "2026-02-10 17:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4njusw",
              "author": "UBIAI",
              "text": "I think the distillation happens at the document level, not the chunk level.",
              "score": 1,
              "created_utc": "2026-02-10 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4njxj3",
          "author": "isthatashark",
          "text": "We had to tackle a similar problem in Hindsight. I just published a blog post about it yesterday on how we do memory consolidation to handle this: https://hindsight.vectorize.io/blog/2026/02/09/resolving-memory-conflicts",
          "score": 1,
          "created_utc": "2026-02-10 18:01:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzxttv",
      "title": "I feel semantic search is overused",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzxttv/i_feel_semantic_search_is_overused/",
      "author": "pskd73",
      "created_utc": "2026-02-09 07:24:05",
      "score": 35,
      "num_comments": 38,
      "upvote_ratio": 0.93,
      "text": "I understand semantic search was the basis for RAG along with graph search. That's how we started making the search possible with AI. \n\nBut as the LLMs got so better, I feel we should take a step back and appreciate the simple text search tools as well. In one of my projects, it was doing good with just semantic search. Recently got an issue where it could not find information about code related question.\n\nThe text was right there, but you know the semantic search stuff, it doesn't pick it up if there is no enough information even though it is just there. That's when I took a step back and just did a regular text base regex search tools.\n\nPoint it, the LLMs are a lot better and they try multiple combinations and eventually they find out as we expect them to. This for sure helps in codebases because there it is more of keywords. I immediately made this improvement live and its giving amazing results.\n\nAnybody did this already? any thoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzxttv/i_feel_semantic_search_is_overused/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4edaae",
          "author": "lupin-the-third",
          "text": "Most people are using a combo of semantic and keyword search and then reranking.",
          "score": 16,
          "created_utc": "2026-02-09 07:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edi66",
              "author": "pskd73",
              "text": "I see. Are they just tools for the LLMs? And is the regex based search on text?",
              "score": 2,
              "created_utc": "2026-02-09 07:47:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4eekw5",
                  "author": "lupin-the-third",
                  "text": "You can make a simple react agent, that will have access to a semantic search and a keyword search tool. Depending on what kind of input comes in, you'll probably need an LLM to reshape the input sentences/keywords for regex to get better output.\n\nFor code, it's not very semantically friendly - comments are just about the only thing that might get some hits - so interacting with it is fine with just grep/find. You can create a few different approaches to searching:   \n \\- Use find to get a directory structure, then ask it to use targeted greps using this directorry structure to find valid files and get surround chunks.   \n \\- Simple regex grep on everything in a shotgun blast approach with multiple combinations of what it could be: For example if you ask \"How does auth work here\" You'll want it to break down into stuff like \"auth\" \"authorization\" \"login\" \"token\", to get a wide breadth of information.",
                  "score": 2,
                  "created_utc": "2026-02-09 07:57:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4efhfe",
          "author": "ComputeLanguage",
          "text": "This is why HyDe became a popular paper :)",
          "score": 7,
          "created_utc": "2026-02-09 08:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ed5ex",
          "author": "lez566",
          "text": "I do a combo of semantic search and keyword search and combine them.",
          "score": 3,
          "created_utc": "2026-02-09 07:43:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edft6",
              "author": "pskd73",
              "text": "Good to know that. How exactly do you combine them? They are passed as tools to the LLMs?",
              "score": 1,
              "created_utc": "2026-02-09 07:46:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4edl73",
                  "author": "lez566",
                  "text": "Yes it alls gets merged into one context and then passed to the LLM",
                  "score": 3,
                  "created_utc": "2026-02-09 07:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fe825",
          "author": "Informal_Tangerine51",
          "text": "Hybrid search (semantic + keyword) is standard now but you're hitting the real production issue: when retrieval fails, debugging is archaeology.\n\nSemantic search missed code reference. Regex found it. Great, but when this happens at scale across different query types, how do you know which retrieval method to trust? Users don't tell you \"use keyword search\" - they ask questions and expect answers.\n\nThe problem compounds: query X needs semantic, query Y needs keyword, query Z needs both. LLM tries combinations and \"eventually finds it\" means wasted retrieval calls and latency. More importantly, when it still fails after trying combinations, you can't debug why without logging what each method returned and why the LLM chose path A over B.\n\nWe saw this exact issue. Added BM25 alongside vector search. Results improved but incidents became harder to debug. \"It found the wrong doc\" doesn't tell you which retrieval method failed or why fusion chose incorrectly.\n\nFor production RAG: are you capturing retrieval results from both methods plus fusion decisions? Or just measuring end result quality?\n\nMulti-method retrieval helps accuracy. Evidence capture makes failures debuggable.",
          "score": 3,
          "created_utc": "2026-02-09 13:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fjop2",
              "author": "pskd73",
              "text": "True that. Yes, https://github.com/crawlchat/crawlchat records almost all details like, the search queries, search types, scores, pages retrieved and other things. They help in fine tuning the process",
              "score": 1,
              "created_utc": "2026-02-09 13:44:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fly79",
              "author": "Wide_Brief3025",
              "text": "Capturing detailed logs for both retrieval methods and their fusion outputs is essential for debugging RAG in production. Setting up structured evidence capture lets you see where breakdowns happen. If alerting on specific keywords and tracking conversations across forums helps, ParseStream has tools that could surface those scenarios in real time to help you intervene faster.",
              "score": 1,
              "created_utc": "2026-02-09 13:57:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mfs9r",
              "author": "code_vlogger2003",
              "text": "Have you tried a mixture of encoder's concept of super linked where the high level statement is that if the user search depends on multiple search fields then the routing slm decides how much weight i need to put on each encoder result.",
              "score": 1,
              "created_utc": "2026-02-10 14:52:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ega37",
          "author": "xeraa-net",
          "text": "Definitely infuriating for users if they know something exists and it's simply not found on a direct match.\n\nEither regex (maybe more so on the code side) or BM25 will give a solid baseline and there's a reason why most of the discussion has moved on to hybrid search.",
          "score": 2,
          "created_utc": "2026-02-09 08:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eh0pt",
              "author": "pskd73",
              "text": "Yeah. Good that I am pretty hybrid now :)",
              "score": 1,
              "created_utc": "2026-02-09 08:21:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4eop6m",
          "author": "chungyeung",
          "text": "Welcome to the big o notation engineering world.",
          "score": 2,
          "created_utc": "2026-02-09 09:37:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eppmz",
              "author": "pskd73",
              "text": "I didn‚Äôt get it haha",
              "score": 1,
              "created_utc": "2026-02-09 09:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4eqsc1",
          "author": "exaknight21",
          "text": "BM25 + Lexical + knowledge graphs is de wei.",
          "score": 2,
          "created_utc": "2026-02-09 09:58:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4er9e4",
              "author": "pskd73",
              "text": "Nice, how do you handle the BM25? What tools to do this? Also, how do you make the knowledge graphs? I have experimented with Knowledge Graphs my way and found it doesn't add much value next to semantic + regex based text search",
              "score": 1,
              "created_utc": "2026-02-09 10:03:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ewbpv",
          "author": "RobertLigthart",
          "text": "yea honestly hybrid is the move but even then people overcomplicate it. for most use cases just doing BM25 + vector search with a reranker on top gets you like 90% of the way there... seen too many people jump straight into knowledge graphs and complex pipelines when their actual problem was just bad chunking",
          "score": 2,
          "created_utc": "2026-02-09 10:51:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ey0j7",
              "author": "pskd73",
              "text": "True that. Chunking is critical. At least I have made sure the headings, table headers are carried over.\n\nIn my experience knowledge graphs add very little improvement and a lot of cost involved.\n\nI have reranker too!",
              "score": 1,
              "created_utc": "2026-02-09 11:07:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ewrs4",
          "author": "AICodeSmith",
          "text": "strongly relate, in our BlackSmith multi-agent research platform, we also found that pure semantic search can miss obvious matches, especially in code or structured financial data. We now let LLMs orchestrate *when* to use semantic, graph, or simple keyword/regex search, which has been far more reliable curious if others are moving toward LLM-driven hybrid retrieval as well.",
          "score": 2,
          "created_utc": "2026-02-09 10:55:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ey7dm",
              "author": "pskd73",
              "text": "True that. Thats what I do it in [crawlchat](https://github.com/crawlchat/crawlchat)",
              "score": 1,
              "created_utc": "2026-02-09 11:08:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4f4ezz",
          "author": "joey2scoops",
          "text": "One size does NOT fit all.",
          "score": 2,
          "created_utc": "2026-02-09 12:01:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f4ofe",
              "author": "pskd73",
              "text": "Yup, for sure",
              "score": 1,
              "created_utc": "2026-02-09 12:03:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ffs2o",
          "author": "Live-Guitar-8661",
          "text": "That's exactly what we are doing, and the results have been great. Vector DBs just feel too heavy at this point. I'm focused more on giving the agents the tools they need to find the information, instead of try and have some heavy compute figure it out.",
          "score": 2,
          "created_utc": "2026-02-09 13:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fjted",
              "author": "pskd73",
              "text": "Yes, thats the way to go. I found this is thr bedt workflow",
              "score": 2,
              "created_utc": "2026-02-09 13:45:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fouba",
          "author": "hrishikamath",
          "text": "I mean even Claude code uses pure regex/keyword search but it takes quite a while, semantic search with re ranking works well for my use case.",
          "score": 2,
          "created_utc": "2026-02-09 14:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fp6c4",
              "author": "pskd73",
              "text": "It works but not the best. You can make it better by providing it more ways of exploring the KB as mentioned above",
              "score": 1,
              "created_utc": "2026-02-09 14:16:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kh01p",
          "author": "ApprehensiveYak7722",
          "text": "I am currently using docling for processing pdf and could see that it was extracting all headings in level 2 (##) only and I overcome that with a solution somehow and the next part is even when I say max-tokens = 1024 still few tables or few chunks have crossed that limit.\n\nFew chunks are having both tables and text in the chunk.\n\nFurther, I don‚Äôt understand how to proceed further. Is there any best method to overcome where using that tool should easily find tables and chunk them and differentiate text, tables.",
          "score": 2,
          "created_utc": "2026-02-10 05:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ki265",
              "author": "pskd73",
              "text": "I have done something very similar on https://github.com/crawlchat/crawlchat\n\nYou may check it out here - https://github.com/crawlchat/crawlchat/blob/main/source-sync/src/scrape/markdown-splitter.ts",
              "score": 1,
              "created_utc": "2026-02-10 05:49:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tvd5g",
          "author": "BarrenLandslide",
          "text": "Just use something like Qdrant. It is going to boost your retrieval a lot.",
          "score": 1,
          "created_utc": "2026-02-11 17:07:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxm3z3",
      "title": "I tested Opus 4.6 for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "author": "midamurat",
      "created_utc": "2026-02-06 16:21:47",
      "score": 34,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "I just finished comparing the new Opus 4.6 in a RAG setup against 11 other models.\n\n  \nThe TL;DR results I saw:\n\n* **Factual QA** king: It hit an 81.2% win rate on factual queries\n* **vs. Opus 4.5:** Massive jump in synthesis capabilities (+387 ELO), it no longer degrades as badly on multi-doc queries\n* **vs. GPT-5.1:** 4.6 is more consistent across the board, but GPT-5.1 still wins on deep, long-form synthesis.\n\nVerdict: I'm making this my default for source-critical RAG where accuracy is more imprtant than verbosity.\n\nHappy to answer questions on the data or methodology!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3xv27r",
          "author": "chearmstrong",
          "text": "Using a top-tier model for answer generation in RAG is often unnecessary.\n\nOnce retrieval quality is high and you have post-retrieval steps (relevance filtering / re-ranking, dedupe, metadata filters, chunk stitching), the generator‚Äôs job is mostly summarise, structure, and stay grounded. A cheaper/faster model is usually sufficient.\n\nCommon best practice we‚Äôve seen work well:\n\n- Spend compute on retrieval quality (chunking, filters, re-ranking), not the final generator.\n- Use a fast default generator, and only escalate to a stronger model when signals suggest it‚Äôs needed (low relevance scores, sparse matches, high ambiguity, multi-doc synthesis).\n- Treat generation as a formatting + synthesis step, not the place to ‚Äúfix‚Äù weak retrieval.\n\nIn other words: if you need a very powerful model to get good answers, that‚Äôs often a retrieval problem, not a generation one.",
          "score": 39,
          "created_utc": "2026-02-06 17:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yswzx",
              "author": "midamurat",
              "text": "that's right, i agree. and in this comparison , models were under fixed retrieval + reranking to keep it fair",
              "score": 0,
              "created_utc": "2026-02-06 20:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xrmzb",
          "author": "One_Milk_7025",
          "text": "What is the factual rag means? If the retrieval is perfect any llm can answer that right? How opus is better ? Multi loop or making hyde method to create hypothetical question? For factual rag llm is not needed if the fact checking and rag pipeline is good enough..",
          "score": 3,
          "created_utc": "2026-02-06 17:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ytrk6",
              "author": "midamurat",
              "text": "for factual rag, scifact dataset was used. in theory, what you say might work but in practice, even with the same docs, models differ (like, some over generalize or hide uncertainty). Opus 4.6 was more conservative meaning it actually sticked closer to the source than others",
              "score": -1,
              "created_utc": "2026-02-06 20:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xwfue",
          "author": "Informal-Resolve-831",
          "text": "So how much better it is really?",
          "score": 2,
          "created_utc": "2026-02-06 18:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ysg1n",
              "author": "midamurat",
              "text": "main gain is multi-doc synthesis which is about +387 ELO vs 4.5, much less degradation when sources overlap. or disagree. \n\n(and elo is score from pairwise model-vs-model comparisons using an LLM judge)",
              "score": 1,
              "created_utc": "2026-02-06 20:40:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43s9dj",
          "author": "my_byte",
          "text": "Imagine using the most expensive, premium frontier model for RAG",
          "score": 1,
          "created_utc": "2026-02-07 16:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xg60a",
          "author": "Legitimate-Leek4235",
          "text": "How about Gemini 3.0 flash ? Want to keep costs low and sacrifice bit on quality. How did you measure this ?",
          "score": 1,
          "created_utc": "2026-02-06 16:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xoyhe",
              "author": "midamurat",
              "text": "gemini 3 flash was also very good when I tested especially in terms of being strong in factual RAG. wrote about that too a while ago: [https://agentset.ai/blog/gemini-3-flash](https://agentset.ai/blog/gemini-3-flash) \n\n  \n",
              "score": 0,
              "created_utc": "2026-02-06 17:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xau2g",
          "author": "midamurat",
          "text": "if interested in detail writeup: [https://agentset.ai/blog/opus-4.6-in-rag](https://agentset.ai/blog/opus-4.6-in-rag)",
          "score": 0,
          "created_utc": "2026-02-06 16:22:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o44wiyl",
              "author": "Single-Constant9518",
              "text": "Nice find! That writeup looks detailed. What specific aspects of the new Opus 4.6 did you find most impressive in your testing?",
              "score": 2,
              "created_utc": "2026-02-07 20:05:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48u6a6",
                  "author": "midamurat",
                  "text": "Thank you!  \nI was impressed by how big of an upgrade there was from Opus 4.5 in multi doc queries. Opus 4.6 is much better when reasoning across many docs. Also, it is noticeably better at *not* over-answering. ",
                  "score": 1,
                  "created_utc": "2026-02-08 12:56:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qyck5p",
      "title": "My RAG pipeline costs 3x what I budgeted...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qyck5p/my_rag_pipeline_costs_3x_what_i_budgeted/",
      "author": "Potential-Jicama-335",
      "created_utc": "2026-02-07 12:28:11",
      "score": 33,
      "num_comments": 42,
      "upvote_ratio": 0.95,
      "text": "Built¬†a RAG system¬†over¬†internal¬†docs. Picked¬†Claude¬†Sonnet because¬†it seemed¬†like the best quality-to-price ratio based¬†on what I read¬†online. Everything¬†worked¬†great in testing.\n\nThen¬†I looked¬†at the bill¬†after¬†a¬†week¬†of¬†production¬†traffic. Way¬†over¬†budget. Turns¬†out the actual¬†cost¬†per¬†query¬†is¬†way¬†higher than what I estimated¬†from¬†the pricing¬†page. Something¬†about¬†how¬†different¬†models tokenize the¬†same¬†context¬†differently, so¬†my¬†8k¬†token¬†retrieval chunks¬†cost¬†more on¬†some¬†models than others.\n\nNow¬†I need¬†to find¬†a model that gives¬†similar¬†quality but actually fits¬†my budget.\n\nAnyone¬†dealt¬†with this?\n\nEdit: Thanks everyone for your suggestions ! I'm so grateful for this community's help.  \nI ended up trying a few solutions with my team, and we finally tried [openmark.ai](https://www.openmark.ai) like someone mentioned for automated testing, we managed to find models that perform better for every step of the agentic flows, and that are much more cost efficient, with fallbacks if necessary. Hopefully, we don't get any surprises anymore. üôè",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qyck5p/my_rag_pipeline_costs_3x_what_i_budgeted/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o43inv4",
          "author": "pl201",
          "text": "Bottom line, You picked the most expensive model to process your doc. Put this way, you paid CEO salary to do a delivery job. There are many models that is 5x to 10x cheaper but are fully capable to process doc for your RAG.",
          "score": 8,
          "created_utc": "2026-02-07 15:56:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44iu75",
              "author": "Potential-Jicama-335",
              "text": "Thanks for your input. Yes I'm learning this the hard way. Now I'm trying to find a quick way to find the most appropriate model(s) for the job, as you say, not the ceo job, but the most cost efficient 'person' for the job.  \nI'm on it, I found a few services that could be better than manual testing, I'm in the process of trying them out.\n\nThanks again, nice way to put this.",
              "score": 2,
              "created_utc": "2026-02-07 18:54:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42jplo",
          "author": "Fantastic_suit143",
          "text": "I built mine for free",
          "score": 5,
          "created_utc": "2026-02-07 12:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42v9by",
              "author": "Joy_Boy_12",
              "text": "Can you explain your pipeline?",
              "score": 1,
              "created_utc": "2026-02-07 13:51:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o42ygls",
                  "author": "Fantastic_suit143",
                  "text": "1. The Input (Vectorization)  \n\nMy Flask backend captures the user's text question.  \nBGE-M3 Hugging face (Local Embedding Model).  \nwhat happens: The model converts the English sentence into a 1024-dimensional vector, which is a list of numbers. It does not comprehend the text; it transforms it into a mathematical point in space.  \nWhy this choice? Running this locally guarantees under 100 milliseconds latency and no cost since it's on hugging face backend (free plan).  \n\n2. The Retrieval\nThe system searches my database for relevant information.  \nFAISS.  \nFAISS calculates the Cosine Similarity between the user's question vector and the 130,000+ chunks of text in mine database. It retrieves the Top 3-5 chunks that are mathematically closest to the query. FAISS is designed for speed. It can scan millions of vectors in milliseconds, even on a CPU which is available on hugging face free plan.  \n\n3. The Prompt Construction  \nThe system creates a large prompt for the AI.  \nThe Logic: It combines three components:  \nRole: \"You are a strict Singapore Law Expert‚Ä¶‚Äù  \nContext: The 3-5 text chunks retrieved by FAISS.  \nQuery: The user's original question.  \n\n4. The Generation (Te \"triplefailover\" brain)  \n The prompt is sent to a Large Language Model (LLM) to generate a human-like answer.  \nA reliable \"Chain of Command\":  \nPrimary: Gemini 2.0 Flash (Fastest, High Context).  \nFailover 1: If Gemini fails due to rate limit or error, it switches to Arcee AI trinity large preview(via OpenRouter).  \nFailover 2: If that fails, it switches to Llama 3.3 70B (via Groq).  \nThis guarantees 99.9% uptime.\n\n5. The Output\nAction: The text response streams back to the frontend.  \nReact + Framer Motion.\n The answer appears in  my chat window and thats it I hope this enough explanation also I didn't even invest a single cent in this\nIf you found this helpful thank you very much ‚ò∫Ô∏èüòÅ",
                  "score": 13,
                  "created_utc": "2026-02-07 14:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o42z85o",
          "author": "ChapterEquivalent188",
          "text": "why not taking your time and go local ;) https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit",
          "score": 6,
          "created_utc": "2026-02-07 14:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44j84j",
              "author": "Potential-Jicama-335",
              "text": "Thats a nice suggestion thank you. My company is set on using cloud API services though.",
              "score": 1,
              "created_utc": "2026-02-07 18:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42kiru",
          "author": "Kathane37",
          "text": "There is token variation between models + language can have an impact + pay a lot of attention to caching.\nAlso sonnet for a 8k context is overkill, you can check fiction bench or the blog post context rot from chroma db to choose a model according to the context you have to deal with.",
          "score": 3,
          "created_utc": "2026-02-07 12:41:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44kbsp",
              "author": "Potential-Jicama-335",
              "text": "Interesting take. Thank you for your input. ",
              "score": 1,
              "created_utc": "2026-02-07 19:02:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44t80w",
          "author": "primoco",
          "text": "I went full local to avoid exactly this problem\nI built a RAG system (RAG Enterprise) and decided early on to keep everything local, both embeddings and inference, no API costs, no surprises.\nMy setup: local embeddings with EmbeddingGemma, local LLM inference running on my own hardware, zero per-query costs once set up.\nTrade-offs I accepted: upfront hardware cost (I run this on an RTX 5070 Ti), quality might not match top-tier API models, slower inference than API calls, need to manage infrastructure yourself.\nBut the benefits: completely predictable costs, no tokenization surprises, full privacy (important for internal docs), scales with hardware not with usage.\nIf your budget is tight and you have the technical capability, going local might be worth considering, the initial investment pays off quickly if you have decent traffic volume.\nThat said, if you need API-level quality, others here have mentioned GPT-4o-mini and Haiku as cheaper alternatives worth testing, just make sure you test with the actual tokenizer before committing.",
          "score": 2,
          "created_utc": "2026-02-07 19:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42k7k7",
          "author": "Educational_Cup9809",
          "text": "gemini 2.5",
          "score": 1,
          "created_utc": "2026-02-07 12:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42om47",
          "author": "Infamous_Ad5702",
          "text": "Yes. I built a tool to fix my problem. Tokens were expensive and gpu was out of reach.\n\nI build an index first and then each new query I auto build a fresh knowledge graph.\n\nI can add docs whenever I like.\n\nIt is offline\nZero hallucinations.\n\nMy defence client needed it this way. \nCheap for me, offline security for them.",
          "score": 1,
          "created_utc": "2026-02-07 13:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42ymnc",
              "author": "_dakota__",
              "text": "Can you please elaborate how it works offline?",
              "score": 1,
              "created_utc": "2026-02-07 14:11:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44kldd",
              "author": "Potential-Jicama-335",
              "text": "Fancy way to solve that. My team wouldn't create our own solution though, we don't have time to do that unfortunately. Interesting take though. Thank you. ",
              "score": 1,
              "created_utc": "2026-02-07 19:03:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42vsip",
          "author": "Joy_Boy_12",
          "text": "How is your pipeline works?",
          "score": 1,
          "created_utc": "2026-02-07 13:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44kwfy",
              "author": "Potential-Jicama-335",
              "text": "Its a multi step compression/semantic retrieval flow with several agentic steps to achieve a good compression rate. Not easy to describe in a few words, I assume like for lots rag pipelines. ",
              "score": 1,
              "created_utc": "2026-02-07 19:05:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o431fy7",
          "author": "hrishikamath",
          "text": "Your biggest culprit is sonnet lol. It‚Äôs probably the most expensive LLM, I use like qwen253B for my agent RAG and it‚Äôs fast and fine. It‚Äôs even a agentic rag. Link: https://github.com/kamathhrishi/stratalens-ai",
          "score": 1,
          "created_utc": "2026-02-07 14:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jdys",
              "author": "Potential-Jicama-335",
              "text": "Yes I'm learning this the hard way thank you. Now I'm trying to find a service to find the most cost efficient models as soon as possible, in the process of trying some out. \n\nThanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-07 18:57:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44wne1",
                  "author": "hrishikamath",
                  "text": "I think its better to not use a service like openrouter, but rather you benchmark your RAG with different LLM's. If use some service with that you will have unexpected issues in prod. ",
                  "score": 1,
                  "created_utc": "2026-02-07 20:06:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45yxpc",
                  "author": "Superb_Plane2497",
                  "text": "[together.ai](http://together.ai) is good although focused on opensource/openweight models, which are good enough. I have a much smaller corpus but three layers of authoritative source (canonical, expert opinion, user organisation override). I got better retrieval results by splitting the parent chunks into child chunks with a small token count, and embedding those. Being smaller, a cheap encoding model can be used, and they work better with specific queries anyway. But embedding is a one time cost, and this approach uses the child chunks to find and retrieve a parent chunk so it doesn't save actual tokens when answering queries, except that perhaps it does better retrieval which is also an optimisation (my retrieval is a BM25/vectorisation hybrid which some hand-coded domain-specific assistance provided to BM25). [together.ai](http://together.ai) has a list of recommended models which do well. I allow the user to swap to kimi-2.5 which lets them paste/upload images to their prompt ... however, each user has a quota so if they do more expensive things, they exhaust their quota more quickly, which is a thinking outside the box solution, perhaps. ",
                  "score": 1,
                  "created_utc": "2026-02-07 23:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o433olw",
          "author": "st0ut717",
          "text": "Local llm is the way to go.",
          "score": 1,
          "created_utc": "2026-02-07 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43b9gs",
          "author": "ArturoNereu",
          "text": "Hi.\n\nIs the cost you mention only for requests to Opus? \n\nI want to understand if that's the case, because depending on your requirements, you can offload the cost by: using a less capable LLM to provide the answer, but use a better embedding model to perform the search.",
          "score": 1,
          "created_utc": "2026-02-07 15:20:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jidy",
              "author": "Potential-Jicama-335",
              "text": "This is exactly the kind of 'two step' solution I'm looking into currently. You're right ahead of me ! ",
              "score": 1,
              "created_utc": "2026-02-07 18:58:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43t8yl",
          "author": "ampancha",
          "text": "Model swapping treats the symptom. The actual failure mode here is missing spend controls: per-user caps, token limits, and attribution so you can see which queries and users drive cost. Without those, any model will eventually surprise you. Also worth auditing your 8k chunk strategy; retrieval filtering and smarter chunking often cut costs 30 to 50 percent without touching quality. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-07 16:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4462cb",
          "author": "tillemetry",
          "text": "What about local hardware costs?",
          "score": 1,
          "created_utc": "2026-02-07 17:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44cf87",
          "author": "Rent_South",
          "text": "The¬†tokenization¬†cost¬†thing¬†is something¬†most¬†people don't realize¬†until¬†the¬†bill hits. The¬†same¬†retrieval context¬†costs¬†different¬†amounts on¬†different providers¬†because¬†they¬†tokenize differently. The¬†\"price¬†per million¬†tokens\" on¬†pricing¬†pages¬†is misleading for¬†exactly¬†this reason.\n\nBefore¬†switching¬†models¬†in¬†a¬†RAG pipeline, run¬†your¬†actual¬†retrieval prompts through¬†[openmark.ai](https://www.openmark.ai)¬†and¬†compare¬†real¬†cost¬†per¬†query¬†alongside¬†accuracy. You¬†might¬†find a¬†model¬†that matches¬†quality¬†at¬†a fraction¬†of the price¬†just¬†because¬†it tokenizes your¬†document¬†chunks¬†more¬†efficiently.",
          "score": 1,
          "created_utc": "2026-02-07 18:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jybi",
              "author": "Potential-Jicama-335",
              "text": "The tokenization angle is interesting. I looked it up quickly and f it can save me some time I'll actually try the tool, because testing everything manually has been a nightmare and really unpractical. I'll speak about it to my supervisor tomorrow. I've got a few options to consider.  \nThanks for your  input.",
              "score": 1,
              "created_utc": "2026-02-07 19:00:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ll3e",
          "author": "TechnicalGeologist99",
          "text": "We still building wrappers? \n\nIf you don't own your AI inference stack then what will you do when market pressure forces prices up.",
          "score": 1,
          "created_utc": "2026-02-07 19:08:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44mg7t",
              "author": "Potential-Jicama-335",
              "text": "Hopefully I'd have fallbacks to other APIs, different provider brands, and competition will drive the costs down, and if not, I'd just switch provider. \n\nMy team likes the flexibility of cloud services, we're also not a huge company that can afford localized solution easily, not to mention the time to spend on it. \n\nSo Ideally we'd have a few fallback models, and switch if 1 is not cost efficient anymore.   \nI'm actually looking into some solutions I found thanks to this thread to find optimal models to do that on a schedule. \n\nTHank you for your input. ",
              "score": 1,
              "created_utc": "2026-02-07 19:12:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44wywx",
                  "author": "TechnicalGeologist99",
                  "text": "Competition does drive costs down. \n\nBut costs are artificially low and inference providers are not profitable. The costs will be passed on to you. The issue isn't lack of competition, it's the cost to manufacture and maintain GPUs.",
                  "score": 1,
                  "created_utc": "2026-02-07 20:07:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45c5qf",
          "author": "[deleted]",
          "text": "I built mine for free! I do my entire RAG pipeline with a local model. It's beautiful. ",
          "score": 1,
          "created_utc": "2026-02-07 21:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46ze63",
          "author": "New_Advance5606",
          "text": "Welcome to the club. Depends the scale.¬† You can RAG a document or a thousand for free.¬† When you push the magnitude of order to a million, it costs money.¬†¬†",
          "score": 1,
          "created_utc": "2026-02-08 03:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48i8mo",
          "author": "KyjenYes",
          "text": "worst ad I have seen",
          "score": 1,
          "created_utc": "2026-02-08 11:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ei6pu",
          "author": "SubstantialTea707",
          "text": "You need to rerank with a cross-encoder before the Hybrid search. This is the key to a successful reranking.",
          "score": 1,
          "created_utc": "2026-02-09 08:32:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvy3hv",
      "title": "Context Blindness: A Fundamental Limitation of Vector-Based RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "author": "Diligent-Fly3756",
      "created_utc": "2026-02-04 19:22:45",
      "score": 32,
      "num_comments": 12,
      "upvote_ratio": 0.95,
      "text": "**Retrieval-Augmented Generation (RAG)** has become the dominant paradigm for grounding large language models (LLMs) in external knowledge. Among RAG approaches, **vector-based retrieval**‚Äîwhich embeds documents and queries into a shared semantic space and retrieves the most semantically similar chunks‚Äîhas emerged as the de facto standard.\n\nThis dominance is understandable: vector RAG is simple, scalable, and fits naturally into existing information-retrieval pipelines. However, as LLM systems evolve from single-turn question answering toward multi-turn, agentic, and reasoning-driven applications, the limitations of vector-based RAG are becoming increasingly apparent.\n\nMany of these limitations are well known. Others are less discussed, yet far more fundamental. This article argues that **context blindness**, the inability of vector-based retrieval to condition on full conversational and reasoning context, is the most critical limitation of vector-based RAG, and one that fundamentally constrains its role in modern LLM systems.\n\n# Commonly Discussed Limitations of Vector-Based RAG\n\n**The Limitations of Semantic Similarity**\n\nVector-based retrieval assumes that semantic similarity between a query and a passage is a reliable proxy for relevance. This assumption breaks down in two fundamental ways.\n\nFirst, similarity-based retrieval often misses what should be retrieved (false negatives). User queries typically express intent rather than the literal surface form of the supporting evidence, and the information that satisfies the intent is often implicit, procedural, or distributed across multiple parts of a document. As a result, truly relevant evidence may share little semantic overlap with the query and therefore fails to be retrieved by similarity search, creating a **context gap** between what the user is trying to retrieve and what similarity search can represent.\n\nSecond, similarity-based retrieval often returns what should not be retrieved (false positives). Even when retrieved passages appear highly similar to the query, similarity does not guarantee relevance, especially in domain-specific documents such as financial reports, legal contracts, and technical manuals, where many sections share near-identical language but differ in critical details such as numerical thresholds, applicability conditions, definitions, or exceptions. Vector embeddings tend to blur these distinctions, creating **context confusion**: passages that appear relevant in isolation are retrieved despite being incorrect given the actual scope, constraints, or exceptions. In professional and enterprise settings, this failure mode is particularly dangerous because it grounds confident answers in plausible but incorrect evidence.\n\n**The Limitations of Embedding Models**\n\nEmbedding models transform passages into vector representations. However, the input length limits of the embedding model force documents to be split into chunks, disrupting their structure and introducing information discontinuities. Definitions become separated from constraints, tables from explanations, and exceptions from governing rules. Although often cited as the main limitation of vector-based RAG, chunking is better viewed as a secondary consequence of deeper architectural constraints.\n\n# The Under-Discussed Core Problem: Context Blindness\n\nA core limitation of vector-based RAG that is rarely discussed is its **context blindness**: the retrieval query cannot carry the full context that led to the question. In modern LLM applications, queries are rarely standalone. They depend on prior dialogue, intermediate conclusions, implicit assumptions, operational context, and evolving user intent. Yet vector-based retrieval operates on a short, decontextualized query that must be compressed into one or more fixed-length vectors.\n\nThis compression is not incidental ‚Äî it is fundamental. A vector embedding has limited representational capacity: it must collapse rich, structured reasoning context into a dense numerical representation that cannot faithfully preserve dependencies, conditionals, negations, or conversational state. As a result, vector-based retrieval is inherently **context-independent**. Documents are matched against a static semantic representation rather than the full reasoning state of the system. This creates a structural disconnect: the LLM reasons over a long, evolving context, while the vector retriever operates on a minimal, compressed, and flattened signal. In other words, **the LLM reasoner is stateful, while the vector retriever is not.** Even with prompt engineering, query expansion, multi-vector retrieval, or reranking, this mismatch persists, because the limitation lies in the representational bottleneck of vectors themselves. The vector retriever remains blind to the very context that determines what ‚Äúrelevant‚Äù means.\n\n# Paradigm Shift: From Context-Independent Semantic Similarity to Context-Dependent Relevance Classification\n\nThe solution to context blindness is not a better embedding model or a larger vector database, but a change in how retrieval itself is formulated. Instead of treating retrieval as a semantic similarity search performed by an external embedding model, retrieval should be framed as a **relevance classification problem** executed by an LLM that has access to the **full reasoning context**.\n\nIn this formulation, the question is no longer ‚ÄúWhich passages are closest to this query in embedding space?‚Äù, but rather ‚ÄúGiven everything the system knows so far‚Äîuser intent, prior dialogue, assumptions, and constraints‚Äîis this piece of content relevant or not?‚Äù Relevance becomes an explicit decision conditioned on context, rather than an implicit signal derived from vector proximity.\n\nBecause modern LLMs are designed to reason over long, structured context, they are naturally well-suited to this role. Unlike embedding models, which must compress inputs into fixed-length vectors and inevitably discard structure and dependencies, LLM-based relevance classification can directly condition on the entire conversation history and intermediate reasoning steps. As a result, retrieval becomes context-aware and adapts dynamically as the user‚Äôs intent evolves.\n\nThis shift transforms retrieval from a standalone preprocessing step into part of the reasoning loop itself. Instead of operating outside the LLM stack as a static similarity lookup, retrieval becomes tightly coupled with decision-making, enabling RAG systems that scale naturally to multi-turn, agentic, and long-context settings.\n\n# Scaling Relevance Classification via Tree Search\n\nA common concern with context-dependent, relevance-classification-based retrieval is token efficiency. **Naively classifying relevance over the entire knowledge base via brute-force evaluation is token-inefficient and does not scale.** However, token inefficiency is not inherent to relevance-classification-based retrieval; it arises from flat, brute-force evaluation rather than **hierarchical classification**.\n\nIn **PageIndex**, retrieval is implemented as a **hierarchical relevance classification** over document structure (sections ‚Üí pages ‚Üí blocks), where relevance is evaluated top-down and entire subtrees are pruned once a high-level unit is deemed irrelevant. This transforms retrieval from exhaustive enumeration into selective exploration, focusing computation only on promising regions. The intuition resembles systems such as **AlphaGo**, which achieved efficiency not by enumerating all possible moves, but by navigating a large decision tree through learned evaluation and selective expansion. Similarly, PageIndex avoids wasting tokens on irrelevant content, enabling context-conditioned retrieval that is both more accurate and more efficient than flat vector-based RAG pipelines that depend on large candidate sets, reranking, and repeated retrieval calls.\n\n# The Future of RAG\n\nThe rise of frameworks such as **PageIndex** signals a broader shift in the AI stack. As language models become increasingly capable of planning, reasoning, and maintaining long-horizon context, the responsibility for finding relevant information is gradually **moving** **from the database layer to the model layer**.\n\nThis transition is already evident in the coding domain. Agentic tools such as **Claude Code** are moving beyond simple vector lookups toward active codebase exploration: navigating file hierarchies, inspecting symbols, following dependencies, and iteratively refining their search based on intermediate findings. Generic document retrieval is likely to follow the same trajectory. As tasks become more multi-step and context-dependent, passive similarity search increasingly gives way to structured exploration driven by reasoning.\n\nVector databases will continue to have important, well-defined use cases, such as recommendation systems and other settings, where semantic similarity **is the objective**. However, their historical role as the default retrieval layer for LLM-based systems is becoming less clear. As retrieval shifts from similarity matching to context-dependent decision-making, agentic systems increasingly demand mechanisms that can reason, adapt, and operate over structure, rather than relying solely on embedding proximity.\n\nIn this emerging paradigm, retrieval is no longer a passive lookup operation. It becomes an integral part of the model‚Äôs reasoning process: executed by the model, guided by intent, and grounded in context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3l1d0c",
          "author": "Diligent-Fly3756",
          "text": "PageIndex's GitHub Repo: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-02-04 19:23:59",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3l468x",
              "author": "CathyCCCAAAI",
              "text": "Thanks for sharing! ",
              "score": 2,
              "created_utc": "2026-02-04 19:37:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3l3sz7",
          "author": "Pure_Squirrel175",
          "text": "Thx for sharing this, very insightful",
          "score": 2,
          "created_utc": "2026-02-04 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lme7z",
          "author": "trollsmurf",
          "text": "\"the dominant paradigm for grounding large language models (LLMs) in external knowledge\"\n\nRAG is semantic search with LLM summary and is a kludge that's used way too much for things it's not at all suited for.\n\nI propose:\n\nFuture content solutions need to generate its own code for querying / modifying whatever the user requests. No RAG/CAG, no embedding, always working on the whole corpus (including formatting) via generated code and an LLM being used for understanding intent and generating cohesive and human-understandable output. The generated code can in turn use embedding or whatever is needed to get the results requested.",
          "score": 2,
          "created_utc": "2026-02-04 21:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m25mm",
          "author": "Informal_Tangerine51",
          "text": "Context-aware retrieval is interesting but doesn't solve the accountability gap. When relevance classification retrieves wrong documents, can you prove which ones were evaluated and why they scored as relevant?\n\nYour hierarchical approach prunes irrelevant subtrees efficiently. But when an agent makes bad decision based on retrieved context, debugging needs more than \"it classified these as relevant\" - needs the actual classification scores, which documents were considered, what caused pruning at each level.\n\nWe hit this with vector RAG: retrieval happens, model decides, incident occurs, and we can't replay what was actually retrieved or how fresh it was. Context-aware retrieval improves accuracy but doesn't automatically capture decision evidence.\n\nFor production agents where compliance asks \"prove what documents informed this,\" does your system capture classification decisions as verifiable artifacts? Or focus on improving retrieval accuracy without evidence trails?",
          "score": 2,
          "created_utc": "2026-02-04 22:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o7vfs",
              "author": "Wooden_Leek_7258",
              "text": "SQL is the anwser... you just log the sql queries and the resulting document provided to the LLM. I built my system with a 'glass box' audit capacity. Vector RAGs are not meant for retrieval trying to make it work is just going to leave people dissapointed. This is what that 'Agentic Mirror' article on Medium this week is groping around.",
              "score": 2,
              "created_utc": "2026-02-05 06:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ncmo8",
              "author": "iamaiimpala",
              "text": "These are such good points. Audit trails and governance are seriously lacking in a lot of solutions, and are non-negotiable for real enterprise level adoption.",
              "score": 1,
              "created_utc": "2026-02-05 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nyles",
          "author": "Wooden_Leek_7258",
          "text": "FFS people.\n\nTL;DR Install an SQL Index. Make it query the SQL instead of probability matching.",
          "score": 2,
          "created_utc": "2026-02-05 04:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3on088",
          "author": "New_Animator_7710",
          "text": "This really nails the real failure mode of vector RAG: **relevance is context-dependent, similarity is not**. Once you move beyond single-turn QA, embeddings become a lossy compression of intent and state. Treating retrieval as LLM-conditioned relevance classification‚Äînot proximity in vector space‚Äîfeels like the inevitable shift for agentic and multi-turn systems. Vector RAG still has its place, but as a heuristic, not the reasoning layer.",
          "score": 2,
          "created_utc": "2026-02-05 08:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5ilw",
          "author": "reddefcode",
          "text": "A hybrid approach often works better; use vector search for initial broad recall, then apply LLM-based reranking or classification on a filtered candidate set.",
          "score": 1,
          "created_utc": "2026-02-05 05:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3paz4h",
          "author": "relcyoj",
          "text": "If we‚Äôre talking about enterprise corpora with many small-to-medium documents (TB scale), PageIndex is not a drop-in replacement for vector-based retrieval.\n\nVector/BM25 retrieval still wins at the **global routing problem**: quickly narrowing millions of documents down to a manageable candidate set with low latency and cost. Using PageIndex alone would require LLM-driven relevance decisions across too many documents, which doesn‚Äôt scale well in tokens or latency.",
          "score": 1,
          "created_utc": "2026-02-05 11:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vg554",
          "author": "nitinmms1",
          "text": "Well, another important aspect is the data on which embedding models are trained. \n\nIf that data does not contain the domain specific jargon, symbolic equivalence or taxonomy the quality of embeddings will not be good.",
          "score": 1,
          "created_utc": "2026-02-06 09:17:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qzj9j4",
      "title": "From RAG-powered LLMs to autonomous agents: the real AI production stack in 2026",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzj9j4/from_ragpowered_llms_to_autonomous_agents_the/",
      "author": "devasheesh_07",
      "created_utc": "2026-02-08 20:06:03",
      "score": 31,
      "num_comments": 7,
      "upvote_ratio": 0.82,
      "text": "Over the last few years, most AI conversations revolved around bigger models, benchmarks, and chatbots.\n\nBut looking ahead to 2026, the real shift isn‚Äôt just smarter LLMs ‚Äî it‚Äôs how AI is actually being used in production. AI is moving from assistive software to something closer to a digital workforce.\n\nHere‚Äôs what‚Äôs standing out to me üëá\n\n1. AI is executing work, not just answering prompts\n\nEnterprises are already running AI-powered, end-to-end workflows for billing, customer support, document processing, reporting, and internal ops.\n\nThis isn‚Äôt experimentation or demos anymore ‚Äî it‚Äôs workflow automation at scale.\n\n\n\n2. RAG is becoming the default architecture\n\nRetrieval-Augmented Generation isn‚Äôt optional if you care about accuracy, grounding, or compliance.\n\nConnecting LLMs to internal documents, databases, and vector stores significantly reduces hallucinations and makes AI usable in regulated environments.\n\n\n\n3. LLM progress is about reasoning, not raw size\n\nModel scale mattered early on, but real value now comes from context management, domain awareness, and multi-step reasoning ‚Äî not just parameter count.\n\n\n\n4. AI agents are the real inflection point\n\nAgentic systems don‚Äôt just generate text ‚Äî they plan, use tools, call APIs, and execute multi-step tasks across systems.\n\nLess ‚Äúchatbot‚Äù, more junior employee with supervision and guardrails.\n\n\n\n5. Enterprise AI has moved into production\n\nWe‚Äôre past pilots and proofs-of-concept. Teams are measuring latency, cost, reliability, and productivity gains ‚Äî and AI is starting to affect real KPIs.\n\n\n\n6. Multimodal and domain-specific models are winning\n\nText-only AI is limiting. The future is multimodal systems that understand documents, images, audio, and structured data ‚Äî often fine-tuned or adapted for specific industries.\n\n7. Governance, trust, and compliance are first-class concerns\n\nAudit trails, explainability, bias mitigation, access controls, and data residency are no longer ‚Äúnice to have‚Äù ‚Äî they‚Äôre required for production AI.\n\n8. Edge AI is quietly becoming important\n\nOn-device and local inference unlock lower latency, better privacy, and reduced cloud costs ‚Äî especially for personal and enterprise use cases.\n\nTo me, the takeaway is simple:\n\nThe AI revolution isn‚Äôt coming ‚Äî it‚Äôs already operational.\n\nThe winners won‚Äôt be the ones chasing hype, but the ones building reliable, grounded, agent-driven systems that actually work.\n\nI wrote a longer breakdown here:\n\n [https://www.loghunts.com/rag-llm-agentic-ai-guide-2026](https://www.loghunts.com/rag-llm-agentic-ai-guide-2026)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzj9j4/from_ragpowered_llms_to_autonomous_agents_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4b8h4j",
          "author": "penguinzb1",
          "text": "The agent production gap is real. The jump from \"this works in my demo\" to \"this runs reliably for 10k users\" is where most teams hit the wall.\n\nWhat you said about measuring latency, cost, and reliability is spot on. The problem is most teams only find out their agents are brittle after users start hitting edge cases in production. By then you're firefighting instead of building.\n\nThe governance piece (point 7) ties directly into this‚Äîyou can't just deploy an agent that \"mostly works\" when it's handling regulated data or making consequential decisions. You need to know how it behaves across thousands of scenarios before it touches real users.\n\nWe've been working on simulating agent workflows at scale before deployment to catch these issues early. Helps surface the difference between \"works on my machine\" and \"works in production under load with real user chaos.\"",
          "score": 5,
          "created_utc": "2026-02-08 20:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cbhrs",
          "author": "ArmOk3290",
          "text": "Latency is also a real constraint once you add retrieval, rerank, and tool calls. People talk about multi step agents, but if each step is 2 to 4 seconds, nobody will use it.",
          "score": 2,
          "created_utc": "2026-02-08 23:48:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dxnzn",
          "author": "Kind_Temperature_701",
          "text": "Absolutely agree(point 6) - we have already developed a Gen AI-powered solution for a major media company, which gave us firsthand experience with the capabilities of Multimodal LLMs.",
          "score": 1,
          "created_utc": "2026-02-09 05:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ew6fr",
          "author": "jannemansonh",
          "text": "the point about rag + workflow automation being the real shift is spot on... moved our doc processing workflows to needle app since you just describe what you want and it builds it (has rag built in). way easier than wiring together vector stores + n8n nodes, especially for non-technical folks on our team",
          "score": 1,
          "created_utc": "2026-02-09 10:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eyt1j",
          "author": "AICodeSmith",
          "text": "This aligns closely with what we‚Äôre seeing in BlackSmith AI as well. Our LLM stack focuses on agent-driven workflows, strong retrieval, and domain-specific reasoning so AI can operate reliably in real production environments. If you wants more details feel free to ask.",
          "score": 1,
          "created_utc": "2026-02-09 11:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mv62i",
          "author": "Little-Appearance-28",
          "text": "This is a solid take. I‚Äôve also noticed the shift from just bigger LLMs to **real production use cases** where RAG + agents actually deliver value. Grounding an LLM with retrieval cuts hallucinations and makes answers reliable, and agents that *plan, use tools, and act* are way closer to what businesses actually deploy today than simple chatbots. The future isn‚Äôt just smarter models ‚Äî it‚Äôs workflows that run reliably, measure latency/cost/accuracy, and integrate RAG deeply into real systems.",
          "score": 1,
          "created_utc": "2026-02-10 16:06:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r05za6",
      "title": "Rerankers in RAG: when you need them + the main approaches (no fluff)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r05za6/rerankers_in_rag_when_you_need_them_the_main/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-09 14:42:35",
      "score": 29,
      "num_comments": 14,
      "upvote_ratio": 0.79,
      "text": "If your RAG feels like it‚Äôs ‚Äúalmost there‚Äù ‚Äî rerankers are usually the missing piece.\n\nA reranker sits between retrieval and the LLM:\n\n1. Retrieve a larger candidate set (e.g., top-50)\n2. Rerank those candidates by relevance\n3. Send only top-5/top-10 to the model\n\nThe point: stop feeding the LLM garbage context.\n\n# When rerankers are actually worth it\n\nYou likely need a reranker if:\n\n* The correct chunk is often in top-50, but not in top-5\n* Your corpus has near-duplicates (policy versions, templates, ‚Äúsame doc but updated‚Äù)\n* Queries are long / multi-intent (‚Äúcompare A vs B, cite the latest policy, exclude legacy‚Äù)\n* Dense retrieval returns ‚Äúrelated‚Äù chunks but not the *answer-bearing* chunk\n* Increasing k makes answers worse (more context ‚Üí more confusion)\n\nIf your data is small and clean and top-5 is already precise, rerankers can be extra latency for little gain.\n\n# The main reranker approaches (practical overview)\n\n# 1) Cross-encoder rerankers (most common ‚Äúquality win‚Äù)\n\nScores each *(query, chunk)* pair by reading them together.\n\n* ‚úÖ Best precision (biggest improvement to answer quality)\n* ‚ùå More compute than embeddings\n* Best pattern: retrieve top-50 ‚Üí cross-encoder ‚Üí keep top-5\n\n# 2) Embedding similarity (bi-encoder) (baseline, not really ‚Äúreranking‚Äù)\n\nThis is what most people mean by ‚Äúvector search‚Äù.\n\n* ‚úÖ Fast, scales well\n* ‚ùå Weaker at fine-grained intent (‚Äúwhich chunk actually answers?‚Äù)\n* Best use: candidate generator before a stronger reranker\n\n# 3) Hybrid (BM25 + dense)\n\nCombine lexical matching with embeddings.\n\n* ‚úÖ Great for IDs, error codes, names, exact terms\n* ‚úÖ More robust across weird queries\n* ‚ùå Requires tuning weights / mixing logic\n\n# 4) LLM-as-reranker (works, but don‚Äôt start here)\n\nAsk an LLM to rank chunks (sometimes with custom rules).\n\n* ‚úÖ Very flexible (‚Äúprefer newest doc‚Äù, ‚Äúmust include citation‚Äù, etc.)\n* ‚ùå Slower + expensive + can be inconsistent unless tightly constrained\n* Use when you need domain rules that models don‚Äôt capture well\n\n# 5) Domain-tuned rerankers\n\nFine-tune a reranker on your own relevance data.\n\n* ‚úÖ Big gains in specialized corpora\n* ‚ùå Needs training data + evaluation discipline\n* Worth it when retrieval quality is core to your product\n\n# A simple setup that usually works\n\n* Retrieve top-50\n* Cross-encoder rerank ‚Üí pick top-5\n* Apply filters **before reranking** when possible (permissions, time, doc type)\n* Track metrics like: ‚Äúanswer present in top-N‚Äù and final answer accuracy\n\nThat‚Äôs it. Reranking isn‚Äôt a silver bullet ‚Äî it‚Äôs just the cleanest way to convert ‚Äúthe answer is somewhere in there‚Äù into ‚Äúthe model actually sees it.‚Äù\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r05za6/rerankers_in_rag_when_you_need_them_the_main/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4hc902",
          "author": "Xacius",
          "text": "Thanks ChstGPT",
          "score": 14,
          "created_utc": "2026-02-09 19:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jhos5",
              "author": "stingraycharles",
              "text": "‚Äúno fluff‚Äù \n\n*proceeds to put in shitloads of prose which could have been summed up in one paragraph*",
              "score": 9,
              "created_utc": "2026-02-10 01:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k0zgd",
                  "author": "MullingMulianto",
                  "text": "do you think openai just sets every model's temperature at 100 even especially after everyone shits on them for slop content\n\nbecause I really am just starting to think so",
                  "score": 1,
                  "created_utc": "2026-02-10 03:47:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4l1fe2",
                  "author": "Donkit_AI",
                  "text": "You're welcome to sum it up in one paragraph. üòÅ",
                  "score": 0,
                  "created_utc": "2026-02-10 08:44:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hn9pq",
          "author": "Space__Whiskey",
          "text": "This sounds great.  \nWhat reranker are we talking about here, and which one is straightforward to fine-tune/train?",
          "score": 3,
          "created_utc": "2026-02-09 19:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l35op",
              "author": "Donkit_AI",
              "text": "There's a nice link from u/Comfortable-Fan-580 a few comments below to a nice short post about \"what rerankers are\". In real life though it depends heavily on the use case and the constrains. We're using LLM as a reranker in some of our cases. It is expensive and requires more tinkering to make it right but works better with messy datasets and complex rules.\n\nAs for fine-tuning, cross-encoders are the easiest. You can pick BAAI/bge-reranker family or e.g. cross-encoder/ms-marco-MiniLM-L-6-v2 if we're talking open-source.",
              "score": 1,
              "created_utc": "2026-02-10 09:01:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i4ksh",
          "author": "drink_with_me_to_day",
          "text": "> Apply filters before reranking when possible (permissions, time, doc type)\n\nPermissions check should happen way before this step",
          "score": 2,
          "created_utc": "2026-02-09 21:24:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l253j",
              "author": "Donkit_AI",
              "text": "Filters are on retrieval. That's usually the previous step. I wouldn't say it's \"way before\".",
              "score": 1,
              "created_utc": "2026-02-10 08:51:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iduz0",
          "author": "Comfortable-Fan-580",
          "text": "Simply explained here - https://pradyumnachippigiri.dev/til/ai/rag-reranking",
          "score": 2,
          "created_utc": "2026-02-09 22:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1xip",
              "author": "Donkit_AI",
              "text": "Nice explanation of what reranker is at all. Thank you!\n\nMaybe I should have started with something like this rather than jumping to Step2 (what reranker to use) right away.",
              "score": 1,
              "created_utc": "2026-02-10 08:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ggx1b",
          "author": "jrochkind",
          "text": "This is very helpful, thank you!",
          "score": 2,
          "created_utc": "2026-02-09 16:36:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j2ri2",
          "author": "RoCkyGlum",
          "text": "Rerank is shit",
          "score": 0,
          "created_utc": "2026-02-10 00:24:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qypa0t",
      "title": "Building a Fully Local RAG Pipeline with Qwen 2.5 and ChromaDB",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qypa0t/building_a_fully_local_rag_pipeline_with_qwen_25/",
      "author": "The_Visionary_Grimmy",
      "created_utc": "2026-02-07 21:00:23",
      "score": 25,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "I recently wrote a short technical walkthrough on building a **fully local Retrieval-Augmented Generation (RAG) pipeline** using **Qwen-2.5** and **ChromaDB**. The focus is on keeping everything self-hosted (no cloud APIs) and explaining the design choices around embeddings, retrieval, and generation.\n\nArticle:  \n[https://medium.com/@mostaphaelansari/building-a-fully-local-rag-pipeline-with-qwen-2-5-and-chromadb-968eb6abd708](https://medium.com/@mostaphaelansari/building-a-fully-local-rag-pipeline-with-qwen-2-5-and-chromadb-968eb6abd708)\n\nI also put the reference implementation here in case it‚Äôs useful to anyone experimenting with local RAG setups:  \n[https://github.com/mostaphaelansari/Optimization-and-Deployment-of-a-Retrieval-Augmented-Generation-RAG-System-](https://github.com/mostaphaelansari/Optimization-and-Deployment-of-a-Retrieval-Augmented-Generation-RAG-System-)\n\nHappy to hear feedback or discuss trade-offs (latency, embedding choice, scaling, etc.).",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qypa0t/building_a_fully_local_rag_pipeline_with_qwen_25/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o474i1c",
          "author": "the-vague-blur",
          "text": "Super cool! I'll try this out! \nOut of curiosity, what were the specs of your computer?",
          "score": 2,
          "created_utc": "2026-02-08 04:01:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48icdd",
              "author": "The_Visionary_Grimmy",
              "text": "**RTX 4070 desktop GPU** with **12 GB of GDDR6X VRAM**",
              "score": 1,
              "created_utc": "2026-02-08 11:18:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o464gho",
          "author": "Academic_Track_2765",
          "text": "Excellent work! yes this is the way to do it if you have GPU / memory available. ",
          "score": 1,
          "created_utc": "2026-02-08 00:14:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qynrqv",
      "title": "\" Hierarchical Agentic RAG (Knowledge Graph + Vector) & JSON RAG \" running fully offline on GTX 1650 (Scale Vs Speed)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qynrqv/hierarchical_agentic_rag_knowledge_graph_vector/",
      "author": "D_E_V_25",
      "created_utc": "2026-02-07 20:01:06",
      "score": 25,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hi everyone, I‚Äôm a 1st-year CSE student. I‚Äôve been obsessing over how to run decent RAG pipelines on my consumer laptop (GTX 1650, 4GB VRAM) without relying on any cloud APIs.\n\n‚ÄãI quickly realized that \"one size fits all\" doesn't work when you have limited VRAM. So I ended up building two completely different RAG architectures for my projects, and I‚Äôd love to get some feedback on them.\n\n‚Äã1. The \" HIERARCHICAL AGENTIC RAG WITH HYBRID SEARCH (VRCTOR SEARCH + KNOWLEDGE GRAPH)\" (WiredBrain)::\n\n‚ÄãThe Goal: Handle massive scale (693k chunks) without crashing my RAM.\n\n‚ÄãThe Problem: Standard HNSW indexes were too RAM-heavy and got slow as the dataset grew.\n\n‚ÄãMy Solution: I built a Hierarchical 3-Address Router. Instead of searching everything, it uses a lightweight classifier to route the query to a specific \"Cluster\" (Domain -> Topic -> Entity) before doing the vector search.\n\n‚ÄãThe Result: It cuts the search space by ~99% instantly. I‚Äôm using pgvector to keep the index on system RAM so my GPU is free for generation.\n\n‚ÄãRepo: https://github.com/pheonix-delta/WiredBrain-Hierarchical-Rag\n\n‚Äã2. The \"Speed Demon\" (Axiom Voice Agent)\n\n‚ÄãThe Goal: <400ms latency for a real-time voice assistant.\n‚ÄãThe Problem: Even the optimized Graph RAG was too slow for a fluid conversation.\n\n‚ÄãMy Solution: I built a pure JSON-based RAG. It bypasses the complex graph lookups and loads a smaller, highly specific context directly into memory for immediate \"reflex\" answers. It‚Äôs strictly for the voice agent where speed > depth.\n\n‚ÄãRepo: https://github.com/pheonix-delta/axiom-voice-agent",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qynrqv/hierarchical_agentic_rag_knowledge_graph_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4646g8",
          "author": "Top_Locksmith_9695",
          "text": "Super interesting! Thanks for sharing! Can you give more details on how you did the domain, topic entity?",
          "score": 1,
          "created_utc": "2026-02-08 00:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46x6lv",
              "author": "D_E_V_25",
              "text": "Thanks a lot for giving time to this üòÅüòÅ\n\nNow as for the topic...\n\nMy architectural is divided into gates in the paper I had posted u will see .. domain tagging and 13 gates named with different topics inside that I first tool 2k+ place holders with different names..\n \nSay if maths then inside that chapters and say each chapter with headings and each headings with a sub headings and problems ..\n\nAlthough maths in rag is always a bad idea but in my case it's working with decent percentage of performance also  and the reasons are because of the architecture...\n\nAs u know with such heavy rag local models can't take it up as they will themselves get out of context ...\n\nSo current working whole state I have installed in the laboratory which i can't publicly disclose to the web .. Sorry for that.. But if I get good response this time I plan to move from 250 gb of data inputs to 1tb one as well....\n\nI have open sources the backend with few files missing because I am in the line of getting pre prints acceptance  and journals as well..\n\nBut even if I don't get a valuable feedback... \n\nComments like yours truly motivates me and I feel truly happy to opensource my whole projects ...\n\nPls give a moment to share that and starred the repo it will boost the visibility...\n\nCurrently I have 500+ clones of my projects but sad to see the visibility algorithm works only when u have stars...\nBut no worries .. I am not hungry for stars or fame...\n\nI know a true community and ppl are loving these.. and that was the whole point",
              "score": 2,
              "created_utc": "2026-02-08 03:12:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mw17c",
          "author": "Little-Appearance-28",
          "text": "Running a hierarchical Agentic RAG fully offline on a GTX 1650? That‚Äôs impressive! Combining knowledge graphs with vectors and JSON for RAG really shows the trade-off between scale and speed. It‚Äôs cool to see fully offline setups actually performing well without massive hardware.",
          "score": 1,
          "created_utc": "2026-02-10 16:11:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n50b0",
              "author": "D_E_V_25",
              "text": "Thank you üòä\n\nAnd more importantly I have added reasoning to rag which is unheard of anywhere and also I have put the paper over my repo u can check that out ... \n\nI am looking forward for contributions \n\nCurrently i upgraded systems to get retrieval under 50ms and Its not false .. but with a whole model on my without tensor cores gpu also I have been able to get a eye blink fast re retrieval and now the system is not Hybrid but Tri-search..\n\nThats why I had mentioned I am currently improving thing...\n\nI don't know I did right to put things early onto GitHub but I just want to contribute to the open source world as I also learn a lot from them...\n\nHappy to see u recognising the work !!",
              "score": 1,
              "created_utc": "2026-02-10 16:52:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz1zad",
      "title": "Do companies actually use internal RAG / doc-chat systems in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qz1zad/do_companies_actually_use_internal_rag_docchat/",
      "author": "NetInternational313",
      "created_utc": "2026-02-08 06:49:42",
      "score": 19,
      "num_comments": 49,
      "upvote_ratio": 0.89,
      "text": "I‚Äôm curious how common internal RAG or doc-chat tools really are in practice.\n\nDoes your org have something like:\n\n* chat over internal docs / wikis / tickets\n* an internal knowledge assistant\n* or any RAG-based system beyond a small pilot?\n\nIf yes, is it widely deployed or limited to a few teams?  \nIf no, did it stall at POC due to security, compliance, or other concerns?\n\nGenuinely interested in real-world adoption",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qz1zad/do_companies_actually_use_internal_rag_docchat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o48yy8v",
          "author": "SpectralCoding",
          "text": "We deployed one against our entire internal quality and R&D documentation. 2.2M pages converted to markdown and stored in an Azure AI Search Index. Deployed to ~7,000 users and gets about 250 questions per day. Only a subset of those 7000 work with the data we indexed daily. GPT-5.2 Medium for answer generation. My comment history has some information we got from users about how they use the tool and how it‚Äôs helping them in their daily work. Very interesting responses.\n\nIt‚Äôs costing us about $1k/mo to maintain (OCRing new docs, Search index cost, LLM calls, etc). That‚Äôs more expensive than it probably needs to be but the value we get is more than $0.18/answer on average.",
          "score": 13,
          "created_utc": "2026-02-08 13:28:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4917x7",
              "author": "NetInternational313",
              "text": "Is this tool strictly for internal use, and how did you handle security and access control for such a large doc set?",
              "score": 2,
              "created_utc": "2026-02-08 13:43:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o496vw0",
                  "author": "SpectralCoding",
                  "text": "Yes internal only. Most of the document set is open to all users. However our tool uses Azure AI Search where you can do effectively ‚Äúrecord level‚Äù security with groups. So when you do the retrieval segment they are only searching what they have access to.",
                  "score": 4,
                  "created_utc": "2026-02-08 14:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o491wyp",
              "author": "Thenuges",
              "text": "Are they not worried about the confidential data being sent out to OpenAI? Curious what their thoughts or policies are around this.",
              "score": 2,
              "created_utc": "2026-02-08 13:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o496n67",
                  "author": "SpectralCoding",
                  "text": "No. We use Azure OpenAI and are secured with Microsoft‚Äôs Data Processing Addendum. More of a concern is the tool is so useful and effective it would make an internal bad actors job way easier. A year ago someone would have had to ask around to find the right documents and piece together the entire process for a specific chemical coating or whatever. Now they just ask and it coalesces info from 20 docs and gives them a top tier answer in about 1min. So good for most use cases but also concerning how much data is now immediately available. But that‚Äôs our organization immaturity with document categorization and restricting data access.",
                  "score": 5,
                  "created_utc": "2026-02-08 14:15:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49xfqq",
              "author": "horserino",
              "text": "(disclaimer: I'm mostly a noob on this kind of setup)\n\nHow does the model interface with Azure? Does the model \"call\" a DB search somehow? Or does the search happen first and the results sent in the model's prompt ?",
              "score": 1,
              "created_utc": "2026-02-08 16:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49ygjf",
                  "author": "SpectralCoding",
                  "text": "This is the base project we use and have extended. Lots of docs and videos on it too.\n\nhttps://github.com/Azure-Samples/azure-search-openai-demo\n\nYou answer is here:\n\nhttps://github.com/Azure-Samples/azure-search-openai-demo/releases/tag/2025-11-18\n\nThere is an API layer that uses an LLM to come up with search terms, it searches the index, then creates a new prompt with the data from the search and the users question. Classic RAG pattern. This project is more interesting because it uses some Azure AI Search features to agentically search the index until it either finds relevant info or gives up. Not just a one shot search.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49ybpb",
              "author": "No-Contribution8248",
              "text": "What's the avg latency per question?",
              "score": 1,
              "created_utc": "2026-02-08 16:40:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49z6fn",
                  "author": "SpectralCoding",
                  "text": "Longer since we built or enabled the agentic search capabilities where it will make multiple searches if it doesn‚Äôt find relevant things the first time. Best case for a simple answer is about 2sec-ish? Longer complex multi-faceted answers might take 10sec or so since we have reasoning turned up. But we value completeness of answer and research over speed. We could (and have) turn everything down and generate sub-second streaming response but for our type of data this leads to better accuracy.",
                  "score": 3,
                  "created_utc": "2026-02-08 16:44:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4an39c",
                  "author": "c4cheeku",
                  "text": "I think you‚Äôre taking about similarity search latency. How is this achieved? Given, you‚Äôd do some reasoning and routing followed by retrieval and then answer generation and I am sure you won‚Äôt be able to do so under 10 seconds!!",
                  "score": 1,
                  "created_utc": "2026-02-08 18:37:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4e3m4u",
              "author": "Vvictor88",
              "text": "Wondering how accurate is this? Did data cleaning a part of the Rag setup? Or the document itself is very good to do the rag directly",
              "score": 1,
              "created_utc": "2026-02-09 06:17:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4flgtl",
                  "author": "SpectralCoding",
                  "text": "Data cleaning? The indexing pipeline is complicated. Most of them are source Microsoft Word documents, convert them to PDF with Microsoft Word (via pywin32/COM calls), then OCRs them with Azure Document Intelligence (Layout mode) and gets the Markdown from that. Then chunks that to \\~1,000 token chunks with some other things in there like a look-back for the most recent header to enhance the accuracy of the vector.\n\nThe docs themselves were not good to do RAG on directly because many were scanned images, many were Word docs which the \"easy\" Markdown conversion tools do a terrible job on. There are still some flaws like literally entire Word docs are 20 page tables... It's yuck, but the answers are still pretty good.",
                  "score": 1,
                  "created_utc": "2026-02-09 13:55:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tc5ar",
              "author": "Fabulous_Radish8162",
              "text": "How are you managing relationships between data. Will sales teams for example get specific intelligence from a rag?",
              "score": 1,
              "created_utc": "2026-02-11 15:37:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o47uwl5",
          "author": "mr_dudo",
          "text": "we use one internally for onboarding - mostly hitting our confluence and notion exports. biggest win was honestly just getting the docs into a format the model could actually use. most wikis export as garbage html. ended up using a crawler to convert everything to markdown first, then chunking by headers. way cleaner retrieval than trying to parse the raw exports",
          "score": 4,
          "created_utc": "2026-02-08 07:37:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47yl5x",
              "author": "NetInternational313",
              "text": "That makes sense.\n\nDid you have to put any guardrails or access controls in place (team-based access, doc-level permissions), or is it mostly open internally since it‚Äôs onboarding-focused?\n\n  \nDid the model internally hosted",
              "score": 1,
              "created_utc": "2026-02-08 08:11:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o496mp5",
                  "author": "mr_dudo",
                  "text": "nah we just don't index anything sensitive. onboarding stuff is all public internally anyway - process docs, how to set up your dev env, that kind of thing. way easier than building actual permissions into it. if you need doc-level access control you're gonna have a bad time lol",
                  "score": 1,
                  "created_utc": "2026-02-08 14:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4dih2b",
              "author": "AnxietyPrudent1425",
              "text": "Onboarding clients. Not employees I assume",
              "score": 1,
              "created_utc": "2026-02-09 03:44:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48p6p2",
          "author": "RobertLigthart",
          "text": "from what ive seen most companies have at least tried a POC but the ones that actually make it to production are usually narrow scope... like internal docs search or customer support kb, not the \"ask anything about our company\" thing that everyone demos\n\nthe biggest killer is usually data quality not the RAG pipeline itself. if your internal docs are a mess going in, no amount of fancy chunking or reranking is gonna save it",
          "score": 3,
          "created_utc": "2026-02-08 12:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ya8v",
              "author": "NetInternational313",
              "text": "For the RAGs that made it to prod, did teams have to add doc-level permissions or audit controls, or were those systems usually trusted because of the narrow scope?",
              "score": 1,
              "created_utc": "2026-02-08 13:24:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49yo1a",
          "author": "Educational_Cup9809",
          "text": "I built the whole Custom Assistants as a service internally for my org. It‚Äôs not have 600+ assistants that teams and people created in last few months. This is just beta mode. Some features i built:\n\nData sources: Sharepoint, file uploads (10 formats), confluence, azure blob, zendesk, service now, salesforce.\n\nFeatures:  draft publish mode, auto sync(delta only) file level tracking and metadata tagging. \n\nsecurity: Automated pinecone project and index creation per assistant with. Encryption of chunks.\n\nRBAC: access control using internal AD security groups and roles per assistant (automated)\n\nMCP server and answer generation: Lot of custom filter prompts and assistant level instructions features which are configurable. I also inject Users AD data  for region and other useful user level information for context.\n\nAnother team had already build a model agnostic api layer with openai spec transformation. So i use that for completions and embedding models.\n\nFeedback and insights dashboard. And a conversation history management for analytics.\n\nNow I am diving into graph RAG and structured (database) and Agentic workflows. \n\nHuge success internally.\n\nSecurity and compliance, and also encryption, was discussed heavily 2 years ago before I begin. This should be first layer you properly design and build everything else on top it at all layers",
          "score": 3,
          "created_utc": "2026-02-08 16:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a5ouz",
              "author": "NetInternational313",
              "text": "Looking back, what assumptions you had early on about security, access control, or usage patterns turned out to be wrong once adoption scaled to hundreds of assistants?",
              "score": 1,
              "created_utc": "2026-02-08 17:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47w2t3",
          "author": "Amazing_Alarm6130",
          "text": "We do. Scaled to serve hundreds of employees. Around 10 millions of curated scientific documents.",
          "score": 2,
          "created_utc": "2026-02-08 07:48:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47yexb",
              "author": "NetInternational313",
              "text": "Did you implement any access-control or governance policies to ensure data isolation across teams (e.g., preventing cross-team document access), especially at that scale?",
              "score": 1,
              "created_utc": "2026-02-08 08:09:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4djotj",
                  "author": "Amazing_Alarm6130",
                  "text": "not for the specific use case I worked. but I know they implemented access-control in another one, using AWS RDS to achieve that ",
                  "score": 1,
                  "created_utc": "2026-02-09 03:52:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o47ygks",
              "author": "NetInternational313",
              "text": "do you use internal hosted LLM",
              "score": 1,
              "created_utc": "2026-02-08 08:10:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dk1jj",
                  "author": "Amazing_Alarm6130",
                  "text": "nope",
                  "score": 1,
                  "created_utc": "2026-02-09 03:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48fqss",
              "author": "hashiromer",
              "text": "Do you use any evals?\n\nIn practice, relevance is trivial to check with citations but evaluating completeness of answers/recall seems next to impossible.",
              "score": 1,
              "created_utc": "2026-02-08 10:53:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4dji9p",
              "author": "AnxietyPrudent1425",
              "text": "Interesting. Curious if you have any methods to curate context. Curious if you‚Äôd be interested in beta testing an app we‚Äôre building designed for the human to scope context window so the AI has weighted and targeted context.",
              "score": 1,
              "created_utc": "2026-02-09 03:51:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4isjjh",
          "author": "the_olivenbaum",
          "text": "We have a large production system for a customer built on our own software (Curiosity Workspace) and operating over 10+ TB of data.\nThe system combines NLP/NER, entity linking, and an in-memory knowledge graph, with RAG + similarity search built on top. It‚Äôs used daily by thousands of users as a real internal knowledge tool and assistant over their legacy and live data, not just a chat interface.\nWhat we found is that pure RAG didn‚Äôt scale well at this size and the added structure (entities + graph) was critical, especially for grounding and navigating relationships across documents. Access control uses a ReBAC model with each document having permissions attached in the graph. Enforcing permissions before retrieval and showing clear source attribution were also key to adoption and the customer is planning to expand this system further.\nIn practice, the systems that work tend to look more like search + structured knowledge + LLM, rather than a simple doc-chat layer.",
          "score": 2,
          "created_utc": "2026-02-09 23:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4km5k0",
              "author": "NetInternational313",
              "text": "At what point did you realize pure RAG wouldn‚Äôt scale and what specific failure signals or incidents forced the move toward entities and a knowledge graph? When something goes wrong in production, what evidence do security or compliance teams expect you to produce, and how does the graph-based approach make that possible? What was the biggest ongoing cost or operational burden introduced by adding entity extraction and a graph layer? For the ReBAC model, what level of granularity turned out to be necessary in practice document, section, or entity-level permissions?",
              "score": 2,
              "created_utc": "2026-02-10 06:23:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4knnsg",
                  "author": "the_olivenbaum",
                  "text": "The data is very technical, full of jargon and identifiers - vector embeddings could only get so far with capturing meaning, so structuring the data was key to the success. The problem is that embeddings don't capture well identifiers - so a query for something just a digit away that meant something completely different would have the same vector. For audit: there's an append only log of all data accessed by the user, directly or via search or chat, and logs of all chat interactions. \nThe biggest challenge on building the graph was data acquisition and mapping: we've over 50 data sources integrated in this project from all sorts of internal databases, and building a cohesive view of the data took some time. But it is also done incrementally, continuously throughout the project development and ongoing production usage.\nIt's all using traditional NLP approaches, we don't use LLMs for building the graph in this project both due to cost limitations (traditional NLP handles 100,000s of files/s and enables very quick reprocessing once new datasets are added).\nThe access model is one of the big challenges: it's at the 3 levels (data type, entity level and field level access enforcement - last one we're just adding to the product). And yes the use-case requires it (many data repositories each with it's own rules), strict export control requirements, etc...",
                  "score": 2,
                  "created_utc": "2026-02-10 06:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4agc15",
          "author": "No_Direction_7168",
          "text": "Yes - Many internal docs across tens of thousands.  Then, individual departments make their own to share with tens to hundreds.  I don‚Äôt know their inner workings but I use NotebookLM as a RAG for my personal notes/documents.  The company has also given us personal RAG spaces, but they don‚Äôt work nearly as good as NotebookLM.",
          "score": 1,
          "created_utc": "2026-02-08 18:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ait8e",
              "author": "NetInternational313",
              "text": "When you say the internal RAG doesn‚Äôt work as well, is that mostly about relevance/recall, latency, or how answers are synthesized?",
              "score": 1,
              "created_utc": "2026-02-08 18:18:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4c4cgn",
                  "author": "No_Direction_7168",
                  "text": "Mostly about the quality of the answers.  The responses lack depth and feel like they are sticking close to the actual words of the ingested documents.  The company provided one was more clinical and while it technically contains a correct response.  e.g. I asked it to explain the history and development of a requirement and it responded with the history of the previous revisions of the document.  NLM not only gave me that information but also went on to describe the backstory of the document.. Not sure if it is a backend LLM difference, ingesting/chunking, or Quantization issue but the work one is always a bit lacking.",
                  "score": 2,
                  "created_utc": "2026-02-08 23:05:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4d8pdk",
          "author": "Ecanem",
          "text": "I work for one of the largest professional services firms in the world.  We have many deployed to 10,000-100,000 users.",
          "score": 1,
          "created_utc": "2026-02-09 02:50:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4esjiw",
              "author": "NetInternational313",
              "text": "What were the key controls or guarantees required to get these approved at that scale  especially around access control and auditability? Were there any early failures or rollout lessons before reaching that level of adoption? How do you handle document-level permissions and prevent cross-team data leakage at that user count?",
              "score": 1,
              "created_utc": "2026-02-09 10:15:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4di86o",
          "author": "AnxietyPrudent1425",
          "text": "I assume companies are not buying anything so much that i feel sorry for anyone currently employed planning finance. Lol. You‚Äôre going to lose everything too. You‚Äôre not special because you‚Äôre currently employed. A smart person would shore up any assets they have and prepare for 1930 all over again.",
          "score": 1,
          "created_utc": "2026-02-09 03:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ff6h6",
          "author": "Informal_Tangerine51",
          "text": "Most orgs have pilots, few have production deployments. The gap is usually debuggability and compliance, not capability.\n\nRAG works great in demos. Production question: when it returns wrong answer, can you prove what docs it retrieved, how fresh they were, and whether retrieval logic has regressed since last week? Legal and compliance need verifiable evidence, not \"the model probably used the right docs.\"\n\nSecurity teams block deployment because there's no runtime policy layer. User asks sensitive question, RAG retrieves confidential docs, returns answer with data user shouldn't see. Without enforcement at retrieval time plus audit trails, it's a compliance nightmare waiting to happen.",
          "score": 1,
          "created_utc": "2026-02-09 13:17:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kjcta",
              "author": "NetInternational313",
              "text": "From your experience, what‚Äôs the minimum set of instrumentation or controls a RAG system needs before security and compliance will even consider approving it for production? When a RAG system returns a wrong or sensitive answer, what evidence do legal or compliance teams actually expect to see during review or incident response? What does a ‚Äúruntime policy layer‚Äù look like in practice for RAG  is it mostly retrieval-time RBAC, query classification, or something else?In orgs where you‚Äôve seen RAG make it to production, what was the turning point that closed the gap between pilot and approval?",
              "score": 1,
              "created_utc": "2026-02-10 05:59:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sumyd",
          "author": "EnoughNinja",
          "text": "Most places I've seen get stuck going from POC to production hit the same wall whereby the easy stuff works fine (confluence pages, handbooks, product specs), but the questions people actually care about live in conversations, not documents.\n\n\"What did we decide about the pricing change?\" That answer is spread across a 15-message email thread, a half-written meeting recap, and a Jira comment. \n\nBasic RAG treats each chunk independently so it loses thread structure, who said what, and how decisions evolved.\n\nThe other killer is permissions. The moment you index internal comms, you need per-user access controls. Most POCs skip this and hit a wall when legal reviews it.\n\nThe orgs that make it to production either scope very narrowly to one doc type, or invest in a proper context layer that handles threading and permissions before anything hits the LLM. \n\nWe've been building infrastructure for the email side of this at iGPT, treating email as conversational graphs instead of flat docs. \n\nHappy to share more if it helps",
          "score": 1,
          "created_utc": "2026-02-11 14:08:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy7dhz",
      "title": "Legaltech for Singapore with RAG (version 2)(open source ‚≠ê)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qy7dhz/legaltech_for_singapore_with_rag_version_2open/",
      "author": "Fantastic_suit143",
      "created_utc": "2026-02-07 07:19:06",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nA few Days back, I talked about my pet project, which is a RAG-based search engine on Singaporean laws and acts (Scraping 20,000 pages/sec) with an Apple-inspired user interface.\n\nThis project is open source meaning anyone can use my backend logic but do read the license provided in the GitHub.(Star the repo if you liked it.)\n\nThe community posed some fantastic and challenging questions on answer consistency, complex multi-law queries, and hallucinations. These questions were just incredible. Rather than addressing these questions or issues with patches and fixing them superficially, I decided to revisit the code and refactor significant architectural changes.This version also includes reference to page number of the pdf while answering i have achieved that using metadata while I also building the vector database.\n\nI look forward to sharing with you Version 2.\n\nThe following are specific feedback that I received, and how I went about engineering the solutions:\n\nThe Problem: \"How do you ensure answer quality doesn't drop when the failover switches models?\"\n\nThe Feedback: My back-end has a \"Triple Failover\" system (three models, triple the backups!). I was concerned that moving from a high-end model to a backup model would change the \"answer structure\" or \"personality,\" giving a \"jarring\" effect to the user. The V2 Fix: Model-Specific System Instructions. I have no ability to alter the underlying intelligence of my backup models, so I had to normalize the output of my back-end models. I implemented a dynamic instruction set. If the back-end should fail over to Model B, I inject a specific \"system prompt\" to encourage Model B to conform to the same structure as Model A.\n\n2. The Problem: \"Single queries miss the bigger picture (e.g., 'Starting a business' involves Tax, Labor, AND Banking laws).\"\n\nThe Feedback: A simple semantic search for ‚Äústarting a business‚Äù could yield the Companies Act but completely overlook the Employment Act or Income Tax Act. The V2 Fix: Multi-Query Retrieval (MQR). I decided the cost of computation for MQR was worth it. What we now do is, when you pose an open-ended question, an LLM catches the question and essentially breaks it down into sub-questions that could be ‚ÄúBusiness Registration,‚Äù ‚ÄúCorporate Taxation,‚Äù ‚ÄúHiring Regulations,‚Äù etc. It's a more computationally intensive process, but the depth of the answer is virtually night and day from V1.\n\n3. The Problem: \"Garbage In, Garbage Out (Hallucinations)\n\nThe Feedback: If the search results contain an irrelevant document, the LLM has two choices: either hallucinate an answer or say \"I don't know.\"\nThe V2 Fix: Re-Ranking with Cross-Encoders: I decided to introduce an additional validation layer. Once the initial vector search yields the primary results, the Cross-Encoder model \"reads\" them to ensure that they're indeed relevant to the query before passing them along to the LLM. If they're irrelevant, the results are discarded immediately, greatly reducing the incidence of hallucinations and \"confidently wrong\" answers.\n\n4. The Problem: Agentic Capabilities\n\nAgentic Behavior: I‚Äôve improved the backend logic so that it is less passive. It is moving towards becoming an agent that can interpret the ‚Äúintent‚Äù behind the search terms, not just match words.\n\nVersioning: This is the hardest nut to crack, but I've begun to organize the data to enable versioning in subsequent updates.\n\nTech Stack Recap\n\nFrontend: Apple-inspired minimalist design.\n\nUsing: BGE-M3 as text embedder\n\nBackend: Triple Failover System - 3 AI Models\n\nNew in V2: FAISS + Cross-Encoder Re-ranking + Multi-Query Retrieval. I'm still just a student and learning every day. The project is open source, and I would love it if you could tear it apart again so that I could create Version 3. Links:\n\nGitHub Repo: [https://github.com/adityaprasad-sudo/Explore-Singapore/\n](https://github.com/adityaprasad-sudo/Explore-Singapore/)\nThanks for the user who asked those questions‚Äîyou literally shaped this update!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qy7dhz/legaltech_for_singapore_with_rag_version_2open/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4am8io",
          "author": "Academic_Track_2765",
          "text": "nice! I like that you actually used cross encoder reranking with multi query retrieval. I don't have much time, but I will take a look, you can also implement an llm based classifier with the cross-encoder reranking stage to further improve document routing. ",
          "score": 2,
          "created_utc": "2026-02-08 18:34:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4906sa",
          "author": "FakruddinBaba",
          "text": "Great work, my friend üëè Really impressive execution.\n\nI‚Äôm also a student currently learning AI agents and RAG systems using the LangChain ecosystem, and I‚Äôm at a stage where exploration feels a bit overwhelming‚Äîespecially deciding what skills truly matter for industry-grade systems and how to translate learning into meaningful projects.\nMy goal is to move beyond toy demos and build real-world, production-oriented agents: systems that are reliable, optimistic in decision-making, fault-tolerant, and resilient to errors. I‚Äôm particularly curious about designing agents that handle failures, retries, evaluation, logging/observability, and long-term maintainability‚Äîeven if the project itself is conceptually simple.\n\n\nI‚Äôd love to learn from your experience:\n\nHow did you structure your learning journey?\nWhich skills or concepts gave you the biggest leap toward production-level thinking?\n\n\nWhere did you learn these‚Äîresources, projects, or real-world exposure?\nAnd could you suggest a clear roadmap or realistic industry-style project ideas that actually run end-to-end?\n\n\nYour work is genuinely inspiring, and any guidance would mean a lot. Thanks for sharing your knowledge.",
          "score": 1,
          "created_utc": "2026-02-08 13:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ans3r",
              "author": "Fantastic_suit143",
              "text": "Wonderful comment mate well I was always interested in coding and building something out of it so that it's helpful for anyone and can learn by looking at my code \n1. No I didn't structure my learning journey actually I have a time slot like 10pm to 12am and in that I use reddit and basically just watch some lectures and build my knowledge and share it too!\n2.Well learning python and a little bit of how llm's work what is rag gave me a good boost in my productivity \n3.well the biggest material is youtube it basically learned everything from that and sometimes I used claude and gemini to help me with error in my code.for the real world exposure part for now I just exploring the world as I am still learning and new to these things\n4.i think like building something usefull like instead of just using ai in everything we should prioritize it more on fixing our lives like whose gonna use ai button on a fridge when I just want some food \nSo my suggestion is making something that makes ai not hallucinate and helps out daily lives.\n\nThanks God bless you for the journey ahead!‚ò∫Ô∏è",
              "score": 1,
              "created_utc": "2026-02-08 18:41:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxa4fq",
      "title": "Built a Website Crawler + RAG (fixed it last night üòÖ)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:25:37",
      "score": 18,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I‚Äôm **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff üíª), I thought:  \nEveryone is feeding PDFs‚Ä¶ **why not try something that‚Äôs not PDF ingestion?**\n\nSo I focused on fixing the **real problem ‚Äî crawling quality**.\n\nüîó GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What‚Äôs better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well üëÄ  \nFeedback, suggestions, and ‚≠ês are welcome!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3uy709",
          "author": "Legitimate-Fun7608",
          "text": "the crawler quality insight is spot on. most people underestimate how much bad extraction ruins everything downstream.\n\n  \ncurious - how are you handling duplicate content across pages? (like shared headers/footers that make it through, or pages with similar structure). playwright helps but usually need some fuzzy matching too.",
          "score": 2,
          "created_utc": "2026-02-06 06:32:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v3m2h",
              "author": "Cod3Conjurer",
              "text": "    # Deduplication check\n\n                    import hashlib\n\n                    content_hash = hashlib.md5(copied_text.encode('utf-8')).hexdigest()\n\n                    if content_hash in content_hashes:\n\n                        print(f\"Skipping duplicate content: {clean_url}\")\n\n                        continue\n\n                    content_hashes.add(content_hash)\n\nRight now, I just strict content hashing (MD5) to catch identical pages and URL normalization. I haven't implemented footer-stripping or MinHash yet because the vector store is surprisingly creating good separation between the 'content' chunks and the 'boilerplate' chunks on its own üòÇ  \n",
              "score": 1,
              "created_utc": "2026-02-06 07:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz41dq",
      "title": "Addressing a fundamental flaw in hybrid search by introducing a Log-Odds Conjunction framework in Bayesian BM25",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qz41dq/addressing_a_fundamental_flaw_in_hybrid_search_by/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-08 08:53:42",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "[https://github.com/instructkr/bb25/pull/1](https://github.com/instructkr/bb25/pull/1)\n\nTo the Information Retrieval Community:  \nA significant update has been merged into the Bayesian BM25 (bb25) repository today!\n\nThis update addresses a fundamental flaw in hybrid search known as Conjunction Shrinkage by introducing a Log-Odds Conjunction framework.\n\nIn traditional probabilistic retrieval, calculating the probability that multiple signals are simultaneously satisfied typically relies on the Naive Product Rule.\n\nFor instance, if a document is relevant based on keyword search with a probability of 0.7 and also relevant based on vector semantic search with a probability of 0.7, the standard approach multiplies these to yield 0.49.\n\nIntuitively, however, if two independent pieces of evidence both suggest a document is relevant, our confidence should increase beyond 0.7.\n\nThe product rule causes the final score to decrease toward zero as more signals are added, violating the intuition that corroborating evidence should amplify confidence.\n\nThe solution implemented in this PR resolves this by shifting the calculation from probability space to log-odds space. The mechanism operates in three stages: first, it computes the geometric mean to find the baseline tendency; second, it performs a Log-Odds Transformation to map the bounded probability space to the unbounded log-odds space; and third, it adds a bonus proportional to the logarithm of the number of signals.\n\nThis works because probability space is bounded by 1.0, preventing simple addition. By transforming to log-odds space, we remove this ceiling. Instead of the score shrinking to 0.49, the logic applies an additive bonus for agreeing signals, resulting in amplification where the final score becomes roughly 0.83.\n\nThis implementation is the proof that this structure is not merely a heuristic. The paper demonstrates that rigorous Bayesian inference over multiple signals produces a computational structure formally isomorphic to a feedforward neural network.\n\nThis work proves that the Sigmoid activation function is a mathematical necessity that emerges when converting Bayesian evidence into probability, rather than an arbitrary design choice. Consequently, this implementation demonstrates that a neural network is the natural structure of correct probabilistic reasoning.\n\nThe introduction of Log-Odds Conjunction has yielded measurable improvements on the SQuAD v2.0 benchmark compared to the standard Hybrid OR approach marking a +1.2% improvement.\n\nThis confirms that properly modeling the agreement between text and vector signals yields better ranking performance than simple score summation or probabilistic multiplication. I would like to extend our gratitude to Jaepil for deriving these proofs and contributing the code to bb25.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qz41dq/addressing_a_fundamental_flaw_in_hybrid_search_by/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o48e0fh",
          "author": "Professional-Fox4161",
          "text": "I have a question. I previously understood that BB25 was order-preserving relative to BM25, but the differing performance on several metrics seems to show that it's not true. Could you help me understand ?",
          "score": 3,
          "created_utc": "2026-02-08 10:37:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dfyib",
              "author": "jaepil",
              "text": "I'm the author of the paper. That is an excellent question and shows you‚Äôve read the theorem carefully.\n\nYou are correct that **Theorem 4.3.1** guarantees monotonicity (order-preservation) relative to BM25, but this holds strictly **'for a fixed prior p'**.\n\nHowever, in practice (as detailed in **Section 4.2**), we often apply a **Composite Prior** that incorporates term frequency and document length signals. Because this prior varies dynamically per document, it introduces a Bayesian re-ranking effect that can slightly alter the order compared to raw BM25.\n\nFurthermore, even if the text-only order were identical, the **non-linear sigmoid transformation** changes the *relative distribution* of scores. In a hybrid setting, this calibrated distribution interacts differently with vector scores compared to unbounded BM25 scores, which naturally leads to different (and often improved) ranking metrics.",
              "score": 2,
              "created_utc": "2026-02-09 03:29:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4dhr3r",
                  "author": "jaepil",
                  "text": "Most importantly, regardless of slight ranking shifts, the¬†**engineering efficiency remains intact**.\n\nAs proven in¬†**Theorem 6.1.2**¬†and¬†**Theorem 6.2.1**, the Bayesian transformation is strictly monotonic. This means we can directly utilize existing¬†**WAND**¬†and¬†**Block-Max WAND (BMW)**¬†dynamic pruning algorithms without any modification to the inverted index structure.\n\nIn practice, this ensures that Bayesian BM25 incurs¬†**O(1) overhead**¬†per document (Theorem 9.1.1) and maintains the same query latency profile as standard BM25, making it immediately deployable in production systems like Vespa or Lucene.",
                  "score": 0,
                  "created_utc": "2026-02-09 03:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48rbxy",
          "author": "RobertLigthart",
          "text": "the core intuition makes total sense... if keyword and semantic search both agree a doc is relevant, multiplying probabilities and getting a lower score than either signal alone is obviously wrong. nice to see someone formalize it instead of just hacking around it with weighted sums",
          "score": 2,
          "created_utc": "2026-02-08 12:35:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyvjt1",
      "title": "Lightrag Graph RAG performing worse than open web UI semantic search with similar setup.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qyvjt1/lightrag_graph_rag_performing_worse_than_open_web/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-02-08 01:27:42",
      "score": 15,
      "num_comments": 14,
      "upvote_ratio": 1.0,
      "text": "Hi folks,\nI‚Äôve set up a Graph RAG pipeline using LightRAG with:\nbge-m3 embeddings\nphi-4 / gpt-oss-20B as LLM\nNeo4j for knowledge graph\nMilvus for semantic search\n\nIn parallel, I‚Äôm also running a simpler setup:\nOpen WebUI\nDocling for document parsing\nbge-m3 + vector search (no graph)\n\nSurprisingly, the plain RAG + Docling setup consistently gives better answers for:\ndocument Q&A\nexplanations\nclause lookup\nsummaries\n\nThe Graph RAG feels more brittle and often misses relevant context.\n\nMy questions:\nIs this a known/expected behavior with Graph RAG?\nDoes Graph RAG usually underperform flat RAG unless the use case is explicitly relational?\nAre people using graphs mainly as rerankers / secondary reasoning layers rather than primary retrieval?\n\nWould love to hear real-world experiences before I invest more time tuning the graph side.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qyvjt1/lightrag_graph_rag_performing_worse_than_open_web/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o46k6l7",
          "author": "sp3d2orbit",
          "text": "This is pretty normal from what I‚Äôve seen.\n\nFor document Q&A, flat RAG tends to outperform Graph RAG. Clause lookup, summaries, and explanations depend on preserving local context, and graphs often lose that unless the schema is very deliberately designed. Missing or imperfect edges hurt recall fast.\n\nIn our work we still start with graph-style retrieval, but it‚Äôs ontology-driven rather than document-driven. The graph narrows the candidate space or enforces constraints, and then we fall back to text retrieval for the actual answer. Treating the graph as structure and control, not as the main text retriever, has worked better for us.",
          "score": 5,
          "created_utc": "2026-02-08 01:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46lsz9",
              "author": "wahnsinnwanscene",
              "text": "Do you auto generate the ontology or hand design?",
              "score": 1,
              "created_utc": "2026-02-08 02:00:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o46o4jr",
                  "author": "sp3d2orbit",
                  "text": "Always programmatically generated from the input documents and it can be grounded in a well known ontology like ICD-10 or SNOMED. Or sometimes generate a grounding ontology from a specialized domain. ",
                  "score": 2,
                  "created_utc": "2026-02-08 02:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4899l0",
          "author": "Visual_Algae_1429",
          "text": "20b is nothing for graph rag, use 120b minimum",
          "score": 2,
          "created_utc": "2026-02-08 09:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46qxxg",
          "author": "penguinzb1",
          "text": "yeah docling's chunking is really good for local context. graphs shine when you need to traverse relationships or enforce constraints, but for straight document retrieval they add more overhead than value unless your schema is dialed in",
          "score": 1,
          "created_utc": "2026-02-08 02:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46z7cp",
          "author": "D_E_V_25",
          "text": "Hi buddy!! \nI was one there with the same issue as well.. \n\nhttps://www.reddit.com/r/Rag/s/1ttwNuIlVy \n\nAbove is the post I made here yesterday about hierchial agentic rag with hybrid search ( Knowledge graph + vector search )  and another json rag as well u can choose based on your usage \n..\n\nGive a visit u will get the whole pictur and it might help u set up your own as well  \n\n..\n\nAs for your question I feel ... U might be using graph rag in a wrong and treating it as a king...\n\nIn my 693k stored chunks which is 10% of what raw I had .. 250gb of raw data .. and I stored and things are working as well with a score of 0.878 on scale of 1..\n\nI have made things open source u give a visit and star the repo as very soon I will teach the folks to how to implement them into your own projects ...\n\nCurrently what I made was for my university and sorry but I can't share the whole thing but no worries.. If community loves this idea and projects..\n\nThis time I feel we will together make a 500gb+ thing and whole another level of thing .. üòéüòé",
          "score": 1,
          "created_utc": "2026-02-08 03:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47bn4v",
          "author": "Curious-Sample6113",
          "text": "Not surprised. Saw similar results along the same lines.",
          "score": 1,
          "created_utc": "2026-02-08 04:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49qrkf",
          "author": "No_Wrongdoer41",
          "text": "Me and a small team have built a platform to automatically build a graph for rag out of your documents. We find it's way better than literag. Would love for you to try it (for free) and compare!",
          "score": 1,
          "created_utc": "2026-02-08 16:03:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d0wf3",
              "author": "No_Presence4293",
              "text": "Can i get more info?",
              "score": 1,
              "created_utc": "2026-02-09 02:10:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4d11u0",
                  "author": "No_Wrongdoer41",
                  "text": "yep dming you",
                  "score": 1,
                  "created_utc": "2026-02-09 02:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4eizvc",
          "author": "Equivalent_Tie4071",
          "text": "There is no one size fits all solution in the RAG world. Your doc type, doc quialty, doc relationship, data curation, model used for data process, all play a role on the quality of the retrival. \n\nDoc relationship is the key for using Graph RAG. ‚ÄúWhat companies compete with our suppliers in Europe?‚Äù, your plain RAG will struggle to find an answer but Graph RAG will easily give you the list.",
          "score": 1,
          "created_utc": "2026-02-09 08:40:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4f9dav",
          "author": "AICodeSmith",
          "text": "This makes sense honestly. We‚Äôve seen flat RAG outperform graph setups for straight document Q and A and summaries too. Graphs seem to help more when the question really needs relationships or multi step reasoning, otherwise they can feel a bit fragile.\n\n",
          "score": 1,
          "created_utc": "2026-02-09 12:38:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r16mrx",
      "title": "A Practical RAG Roadmap to Stop LLM Apps from Failing in Production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r16mrx/a_practical_rag_roadmap_to_stop_llm_apps_from/",
      "author": "devasheesh_07",
      "created_utc": "2026-02-10 17:11:32",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.94,
      "text": "After building and breaking a few LLM-based systems, I‚Äôve realized something uncomfortable:\n\nMost failures don‚Äôt come from ‚Äúbad models.‚Äù  \nThey come from **bad retrieval**.\n\nRAG (Retrieval-Augmented Generation) isn‚Äôt a framework or a prompt trick ‚Äî it‚Äôs a systems problem. Here‚Äôs the **short roadmap** that finally made it click for me:\n\n**1. Start with embeddings**  \nIf you don‚Äôt understand how text becomes vectors and how similarity search works, nothing else matters.\n\n**2. Use a vector database properly**  \nStore embeddings, not raw text. Learn indexing, filtering, and why top-k retrieval isn‚Äôt magic.\n\n**3. Chunking is everything**  \nMost vague or wrong answers come from poor chunking, not the LLM. This step is underrated.\n\n**4. Retrieval > model choice**  \nA smaller model with clean, relevant context beats a larger model with noisy retrieval.\n\n**5. Treat RAG like a system, not a demo**  \nLogging, evaluation, failure cases, and feedback loops matter more than fancy prompts.\n\nThis mindset shift helped me understand why many ‚ÄúLLM apps‚Äù look great in demos but fall apart in real use.\n\nI wrote a **fully detailed article** breaking this down step by step   \n[https://www.loghunts.com/rag-in-ai-ml-practical-learning-roadmap](https://www.loghunts.com/rag-in-ai-ml-practical-learning-roadmap)  \nIf anything here is wrong or oversimplified, I‚Äôm very open to corrections ‚Äî happy to update the article.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r16mrx/a_practical_rag_roadmap_to_stop_llm_apps_from/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4nj0o0",
          "author": "HandSuspicious1579",
          "text": "This is something i would not prefer",
          "score": 1,
          "created_utc": "2026-02-10 17:56:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4olojx",
          "author": "Late_Special_6705",
          "text": "404",
          "score": 1,
          "created_utc": "2026-02-10 20:55:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0u2in",
      "title": "Need help with chunking and embedding strategies for my rag model",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r0u2in/need_help_with_chunking_and_embedding_strategies/",
      "author": "Forsaken-Cod-4944",
      "created_utc": "2026-02-10 07:08:26",
      "score": 13,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm fairly new to this field, and after reading a few posts here, I realized that to get accurate and reliable results, I need to carefully design a chunking and embedding strategy. The problem is, I‚Äôm not really sure how to approach that yet.\n\nHere‚Äôs what I‚Äôve built so far:\n\nI created a RAG-based model for companies that can be integrated into their app or website. The idea is that employees or anyone interested can use it to research and ask questions about the company. The data it handles includes:\n\n* Company policies (long, formal documents)\n* HR documents (rule-based and structured)\n* FAQs (short Q&A format)\n* Financial summaries (number-heavy content)\n* Product documentation (technical text and details about previous projects)\n* CSV structured data (tabular format)\n* Website content (marketing and general information)\n\nSince I‚Äôm a second-year student, I can‚Äôt afford paid services, so I‚Äôm planning to run everything locally. Right now, I‚Äôm considering using **baai/bge-m3** for embeddings and retrieval, and **Qwen3‚Äì1.7B-MLX-8bit** for answer generation.\n\nI also have a question about deployment. I‚Äôm planning to dockerize the entire system for a live demo on my resume. If I run the LLM locally, will it still work for someone accessing it from another device? Or would they also need the model running on their own machine?\n\nI‚Äôd really appreciate any guidance or suggestions. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r0u2in/need_help_with_chunking_and_embedding_strategies/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4l63n3",
          "author": "RobertLigthart",
          "text": "for the chunking honestly dont overthink it at the start. use something like recursive character splitting with \\~500-800 tokens and some overlap (\\~100 tokens). the key thing most people miss is that different doc types need different chunk sizes -> FAQs should stay as complete Q&A pairs, policies can be chunked by section headers, and CSV data you probably want to convert to natural language sentences before embedding\n\n  \nbge-m3 is a solid choice for embeddings especially since its multilingual. for deployment if you dockerize it and host it on a VPS or even a free tier cloud instance the LLM runs server-side so anyone can access it through the API... they dont need anything on their machine. just be aware that Qwen 1.7B is going to be pretty limited for complex questions tho",
          "score": 2,
          "created_utc": "2026-02-10 09:30:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8s85",
              "author": "Forsaken-Cod-4944",
              "text": "Thanks for the clarity but i couldnt understand the llm part, im planning to dockerize and upload my project on [render.com](http://render.com) so my question was will the query generation part work on other devices even if my device is switch off or not running?\n\n",
              "score": 1,
              "created_utc": "2026-02-10 09:56:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz01b",
          "author": "notsarthaxx",
          "text": "What are your gpu specs?",
          "score": 1,
          "created_utc": "2026-02-10 13:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7rb5",
              "author": "Forsaken-Cod-4944",
              "text": "4050 6gb, 55W (Laptop)",
              "score": 1,
              "created_utc": "2026-02-10 14:09:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4n8q9c",
                  "author": "notsarthaxx",
                  "text": "damn does running these models lag?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 17:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mp9xh",
          "author": "Popular_Sand2773",
          "text": "You are describing a highly diverse and often large dataset i.e. an entire corporate knowledge base. If you one size fits all this you'll certainly have something but with such a weak llm 1.7B params its going to be hard to cover up ugly/messy retrieval. \n\nIf you want to tackle all of these hierarchical search and provenance are your friends. Honestly though I would take these one at a time and do what's right for each piece then have the agent use the appropriate tool for each job rather than a monolithic retriever. ",
          "score": 1,
          "created_utc": "2026-02-10 15:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n6uys",
              "author": "Forsaken-Cod-4944",
              "text": "I just need the basics since i'm pretty new to this , when i get the logic and build something that can atleast work on low-medium dataset , I'll try switching to 8-70B parameter llms.\n\nand about provenance , is it a type of reranker? if not then can it be used with it",
              "score": 1,
              "created_utc": "2026-02-10 17:00:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4osozg",
                  "author": "Popular_Sand2773",
                  "text": "That makes sense every time I dip my toes in models that size it ends up being rough. Provenance is keeping track of where things come from. So instead of putting everything into a db you add metadata like faq vs HR vs policy. Then an agent can do something like a filtered search. Oh this question is likely about policy let me check policy specific documents. Lots of ways to execute that's just a basic example of using metadata to limit the search space and improve results. These filters are usually applied pre-re-ranker so go nuts!",
                  "score": 1,
                  "created_utc": "2026-02-10 21:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzcjm4",
      "title": "Built a local-first RAG evaluation framework - just shipped LLM-as-Judge with Prometheus2 - need feedbacks. & advices",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzcjm4/built_a_localfirst_rag_evaluation_framework_just/",
      "author": "Ok-Swim9349",
      "created_utc": "2026-02-08 15:57:26",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "Been working on this for a few months. The problem: evaluating RAG pipelines locally without sending data to OpenAI.\n\nRAGAS requires API keys. Giskard is heavy and crashes mid-scan (lost my progress too many times). So I built my own thing.\n\n**The main goal: keep everything on your machine.**\n\n**No data leaving your network, no external API calls, no compliance headaches. If you're working with sensitive data (healthcare, finance, legal & others) or just care about GDPR, you shouldn't have to choose between proper evaluation and data privacy.**\n\n**What it does:**\n\n\\- Retrieval metrics (precision, recall, MRR, NDCG),\n\n\\- Generation evaluation (faithfulness, relevance, hallucination detection),\n\n\\- Synthetic test set generation from your docs,\n\n\\- Checkpointing (crash? resume where you left off) ,\n\n\\- 100% local with Ollama.\n\n**v1.2 addition ‚Äî LLM-as-Judge:**\n\nSomeone on [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/) pointed out that vanilla 7B models aren't great judges. Fair point. So I integrated Prometheus 2 ‚Äî a 7B model fine-tuned specifically for evaluation tasks.\n\nNot perfect, but way better than zero-shot judging with a general model.\n\nRuns on 16GB RAM with Q5 quantization (\\~5GB model). About 20-30s per evaluation on my M2.\n\n**Honest limitations:**\n\n\\- Still slower than cloud APIs (that's the tradeoff for local)\n\n\\- Prometheus 2 is conservative in scoring (tends toward 3/5 instead of 5/5),\n\n\\- Multi-hop reasoning evaluation is limited (on the roadmap)\n\nGitHub: [https://github.com/2501Pr0ject/RAGnarok-AI](https://github.com/2501Pr0ject/RAGnarok-AI)\n\nPyPI: pip install ragnarok-ai\n\nHappy to answer questions or take feedback. Built this because I needed it ‚Äî hope others find it useful too.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzcjm4/built_a_localfirst_rag_evaluation_framework_just/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4at5r8",
          "author": "reujea0",
          "text": "Speaking purely about the llm model behind this, is it really better than sota model, especially cloud ones? I looked at the paper, and the best was gpt 4 and opus 3, which seemed to beat it (very quick read of the paper), so I am wondering if the current gen of sota isn't just better. Would like to be proven wrong",
          "score": 2,
          "created_utc": "2026-02-08 19:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4auj5d",
              "author": "Ok-Swim9349",
              "text": "You're absolutely right ‚Äî and I won't pretend otherwise.  \nGPT-4 and Claude Opus are better judges.\n\n  \nBut that's not the tradeoff I'm optimizing for. \n\nThe question I'm trying to answer is: what's the best judge model you can run locally, on your own hardware, with zero data leaving your network?\n\n  \nFor that specific constraint, Prometheus 2 is currently the best option I've found:  \n\\- Fine-tuned specifically for evaluation (not a general model doing zero-shot)\n\n\\- 72-85% agreement with human judgments\n\n\\- Runs on 16GB RAM\n\n  \nIs it as good as GPT-4? No‚Äî best local alternative for the moment. Is it good enough for most use cases where local matters? I'd argue yes.",
              "score": 1,
              "created_utc": "2026-02-08 19:12:41",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4av8hi",
              "author": "Ok-Swim9349",
              "text": "But here's the thing: cloud isn't an option for everyone. When you work with sensitive data for exemple.",
              "score": 1,
              "created_utc": "2026-02-08 19:16:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4aw223",
                  "author": "reujea0",
                  "text": "Completely agree, confidentiality and sovereignty are real issues and vram lack as well. Would be interested in validating these with like gpt oss and some others, can it be reproduced the testing? Also I love the idea and implementation",
                  "score": 2,
                  "created_utc": "2026-02-08 19:20:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4a5jsz",
          "author": "z0han4eg",
          "text": "I'm seeing a huge number of \"new\" RAGs lately, especially \"local-first\" ones, but I can't understand why people don‚Äôt use ready-made solutions. For example, Manticore supports both full-text and vector search and a bunch of other things, so you can build a waterfall pipeline without leaving the tool, auto-embedding with external models as well as any local ones. It was developed around HNSW. Open source. And we won't even talk about performance (it's Sphinx Engine so nothing will work faster)‚Ä¶",
          "score": 1,
          "created_utc": "2026-02-08 17:15:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4adlwv",
              "author": "Ok-Swim9349",
              "text": "I understand tottaly your point of view, & I share it.  \n  \nBut I think there might be a misunderstanding though.\n\nRAGnarok-AI isn't a RAG system ‚Äî it's an evaluation framework. It doesn't replace Manticore, Qdrant, or any vector store. It evaluates RAG pipelines that use them.\n\nThe problem it solves: you've built a RAG with Manticore (or whatever stack):\n\n\\- How do you know it works well?   \n\\-How do you measure retrieval quality?   \n\\- How do you detect hallucinations in generated answers?\n\nThat's what it does.  \nSo it's actually complementary to tools like Manticore ‚Äî you build your RAG with the stack you want, then evaluate it with RAGnarok-AI.  \nDoes that make more sense?",
              "score": 2,
              "created_utc": "2026-02-08 17:54:19",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4afk5q",
                  "author": "Ok-Swim9349",
                  "text": "And honestly, you raise a fair point about the \"new RAG every week\" fatigue. The space is getting crowded.\n\nBut I'd push back a bit on the framing. The issue isn't that there are too many tools ‚Äî it's that too many of them lock you into paid tiers or cloud dependencies for basic functionality.\n\nLook at what happened with workflow automation: Zapier charges per task, Make charges per operation... then n8n comes along and says \"here, self-host it, own your data, no per-execution fees.\" Same capability, different philosophy.\n\nThat's kind of what I'm going for here. RAGAS is great but requires OpenAI. Giskard has an open-source version but pushes you toward their cloud. Most \"RAG evaluation\" content out there assumes you'll just send your data to GPT-4 and call it a day.\n\nIf you're a solo dev or a small team working with sensitive data, that's not always an option. Sometimes you just want to run \\`pip install something\\` and have it work on your machine, with your models, without asking permission or paying per API call.\n\nThat's the gap I'm trying to fill. Not \"yet another RAG\".\n\n  \nIl mean Look at what's happening with workflow automation: people pay for n8n for example... when they could just learn LangChain/LangGraph and build exactly what they need. Own their stack, understand how it works, no monthly fees, no vendor lock-in.\n\nMaybe the space doesn't need it, Maybe some people don't care about data sovereignty. But with GDPR enforcement ramping up, healthcare AI under scrutiny, and more companies realizing their competitive data shouldn't flow through third-party APIs ‚Äî I'd bet the \"local-first\" crowd is growing, not shrinking.\n\nBut I needed it, so I built it. If others find it useful, great. If not, at least I seriously learned a ton building it.",
                  "score": 2,
                  "created_utc": "2026-02-08 18:03:25",
                  "is_submitter": true,
                  "replies": []
                },
                {
                  "id": "o4ah97c",
                  "author": "z0han4eg",
                  "text": ">\\- How do you know it works well?  \n\\-How do you measure retrieval quality?  \n\\- How do you detect hallucinations in generated answers?\n\nHow? I suffer! xDD",
                  "score": 1,
                  "created_utc": "2026-02-08 18:11:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4cgmt9",
          "author": "Ok-Swim9349",
          "text": "I've run 9 new issues today :\n\nLLM Adapters:  Mistral, Groq, Together  \nVectorStore Adapters: Pinecone, pgv,  Weaviate, Milvius  \nFramework Adapters: Haystack, SemanticKernel\n\nFeel free to open new ones if you have Ideas.  \nThanks!",
          "score": 1,
          "created_utc": "2026-02-09 00:17:54",
          "is_submitter": true,
          "replies": []
        }
      ]
    }
  ]
}