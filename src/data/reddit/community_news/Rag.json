{
  "metadata": {
    "last_updated": "2026-01-27 08:48:56",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 144,
    "file_size_bytes": 168307
  },
  "items": [
    {
      "id": "1qhzs5i",
      "title": "DeepResearch is finally localized! The 8B on-device writing agent AgentCPM-Report is now open-sourced!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhzs5i/deepresearch_is_finally_localized_the_8b_ondevice/",
      "author": "Relevant_Abroad_6614",
      "created_utc": "2026-01-20 12:27:26",
      "score": 43,
      "num_comments": 7,
      "upvote_ratio": 0.95,
      "text": "In an era where **Deep Research** is surging, we all long for a ‚Äúsuper writing assistant‚Äù capable of automatically producing **tens of thousands of words**.\n\nBut‚Äîwhen you‚Äôre holding **corporate strategic plans, unpublished financial reports, or core research data**, would you really dare to upload them to the cloud ‚òÅÔ∏è?\n\nToday, we bring a **game-changing solution**: **AgentCPM-Report** ‚Äî a **localized, private, yet top-tier** deep research agent.\n\nJointly developed by **Tsinghua University NLP Lab**, **Renmin University of China**, **ModelBest**, and the **OpenBMB open-source community**, it is now **open-sourced** on **GitHub, Hugging Face**, and more.\n\n**What does this mean?**\n\nNo expensive compute. No data uploads.\n\nYou can run an **expert-level research assistant entirely on your local machine** üîß\n\nüîç **Why choose AgentCPM-Report?**\n\n‚úÖ **Extreme efficiency ‚Äî doing more with less**\n\nWith only **8B parameters**, it achieves **40+ rounds of deep retrieval** and **nearly 100 steps of chain-of-thought reasoning**, generating logically rigorous, insight-rich **long-form reports** comparable to top closed-source systems.\n\n‚≠êÔ∏è DeepResearch Bench\n\n|Model|Overall|Comprehensiveness|Insight|Instruction Following|Readability|\n|:-|:-|:-|:-|:-|:-|\n|Doubao-research|44.34|44.84|40.56|47.95|44.69|\n|Claude-research|45.00|45.34|42.79|47.58|44.66|\n|OpenAI-deepresearch|46.45|46.46|43.73|49.39|47.22|\n|Gemini-2.5-Pro-deepresearch|49.71|49.51|49.45|50.12|50.00|\n|WebWeaver (Qwen3-30B-A3B)|46.77|45.15|45.78|49.21|47.34|\n|WebWeaver (Claude-Sonnet-4)|50.58|51.45|50.02|50.81|49.79|\n|Enterprise-DR (Gemini-2.5-Pro)|49.86|49.01|50.28|50.03|49.98|\n|RhinoInsigh (Gemini-2.5-Pro)|50.92|50.51|51.45|51.72|50.00|\n|**AgentCPM-Report**|**50.11**|**50.54**|**52.64**|**48.87**|**44.17**|\n\n‚≠êÔ∏è DeepResearch Gym\n\n|Model|Avg.|Clarity|Depth|Balance|Breadth|Support|Insightfulness|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|Doubao-research|84.46|68.85|93.12|83.96|93.33|84.38|83.12|\n|Claude-research|80.25|86.67|96.88|84.41|96.56|26.77|90.22|\n|OpenAI-deepresearch|91.27|84.90|98.10|89.80|97.40|88.40|89.00|\n|Gemini-2.5-pro-deepresearch|96.02|90.71|99.90|93.37|99.69|95.00|97.45|\n|WebWeaver (Qwen3-30b-a3b)|77.27|71.88|85.51|75.80|84.78|63.77|81.88|\n|WebWeaver (Claude-sonnet-4)|96.77|90.50|99.87|94.30|100.00|98.73|97.22|\n|**AgentCPM-Report**|**98.48**|**95.10**|**100.00**|**98.50**|**100.00**|**97.30**|**100.00**|\n\n‚≠êÔ∏è DeepConsult\n\n|Model|Avg.|Win|Tie|Lose|\n|:-|:-|:-|:-|:-|\n|Doubao-research|5.42|29.95|40.35|29.70|\n|Claude-research|4.60|25.00|38.89|36.11|\n|OpenAI-deepresearch|5.00|0.00|100.00|0.00|\n|Gemini-2.5-Pro-deepresearch|6.70|61.27|31.13|7.60|\n|WebWeaver (Qwen3-30B-A3B)|4.57|28.65|34.90|36.46|\n|WebWeaver (Claude-Sonnet-4)|6.96|66.86|10.47|22.67|\n|Enterprise-DR (Gemini-2.5-Pro)|6.82|71.57|19.12|9.31|\n|RhinoInsigh (Gemini-2.5-Pro)|6.82|68.51|11.02|20.47|\n|**AgentCPM-Report**|**6.60**|**57.60**|**13.73**|**28.68**|\n\n‚úÖ **Physical isolation, true local security**\n\nDesigned for **high-privacy scenarios**, it supports **fully offline deployment**, eliminating cloud data leakage risks.\n\nYou can mount **local knowledge bases**, ensuring sensitive data **never leaves your domain** while still producing professional-grade reports.\n\nüòé **Try it now: put DeepResearch on your hard drive**\n\n**AgentCPM-Report** is now available on **GitHub | Hugging Face | ModelScope | GitCode | Modelers**, and we warmly invite developers to try it out and co-build the ecosystem!\n\n**GitHubÔºöüîó**¬†[https://github.com/OpenBMB/AgentCPM](https://github.com/OpenBMB/AgentCPM)\n\n**HuggingFaceÔºö üîó**¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n\nIf you find our work helpful, please consider giving us a ‚≠ê **Star** & üíñ **Like**\\~",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qhzs5i/deepresearch_is_finally_localized_the_8b_ondevice/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0o2fw3",
          "author": "New_Wear4177",
          "text": "Fantastic Work! Put the DeepResearch in edges.",
          "score": 3,
          "created_utc": "2026-01-20 14:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r1vb7",
          "author": "Unique-Temperature17",
          "text": "This is really cool stuff, thanks for sharing! The privacy-first approach with full offline deployment is exactly what's needed for handling sensitive docs. Will definitely check out the GitHub repo soon. Would love to see this integrated into apps - having an 8B model that punches above its weight like this would be a game-changer for local document workflows.",
          "score": 2,
          "created_utc": "2026-01-20 22:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13rtvh",
          "author": "New_Animator_7710",
          "text": "Impressive results for 8B, no doubt‚Äîbut I think the more interesting question is *how brittle this is outside the benchmark*. DeepResearch-style agents often look amazing on structured evals, but the real test is messy, underspecified prompts and evolving goals over long sessions.",
          "score": 2,
          "created_utc": "2026-01-22 19:41:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17j2yf",
              "author": "Relevant_Abroad_6614",
              "text": "You‚Äôre absolutely right ‚Äî we also place a strong emphasis on user experience.\n\nDeepResearch is relatively easier in this regard, because its evaluation is done via LLM-as-a-Judge on long-form text, so it‚Äôs not as easy to hack.\n\nYou can also give this model a try. From what we‚Äôve seen in the open-source community, some people have used it for web-agent‚Äìlike tasks with pretty solid results ‚Äî which honestly surprised us as well.\n\nThanks again for your interest and support!",
              "score": 1,
              "created_utc": "2026-01-23 09:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19c4e0",
                  "author": "New_Animator_7710",
                  "text": "Sure",
                  "score": 1,
                  "created_utc": "2026-01-23 16:05:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e33n4",
          "author": "BarrenLandslide",
          "text": "Very nice. I am going to test it next week and share my results. Thank you for providing this.",
          "score": 2,
          "created_utc": "2026-01-24 07:45:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shhe7",
          "author": "jerrysyw",
          "text": "Dose anyone test it?",
          "score": 1,
          "created_utc": "2026-01-26 09:34:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjvqd4",
      "title": "Vector dbs aren't memory (learned this the hard way building a coding agent)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjvqd4/vector_dbs_arent_memory_learned_this_the_hard_way/",
      "author": "Sweet121",
      "created_utc": "2026-01-22 14:19:15",
      "score": 41,
      "num_comments": 22,
      "upvote_ratio": 0.81,
      "text": "So i spent the last month losing my mind building a personal coding tutor agent.\n\nWhe problem:\n\nWhe goal was simple: an agent that remembers my skill level, current project, and coding style (like 'i hate list comprehensions, just give me loops'). i did the standard thing: pinecone, chunking, and a RAG pipeline\n\nIt worked for like an hour. by day 3, the agent was a complete mess. it would retrieve code snippets from a project i finished two weeks ago. or worse, i'd tell it 'im switching to rust now', and it would still pull python examples because they were 'semantically similar' to my query. its honestly such a pain.\n\nWhat i tried:\n\ni tried everything to fix this:\n\n* crammed the context window: got expensive fast, and the model got 'lost in the middle'.\n* summarization chains: tried summarizing old convos, but it lost the specific details (like WHY i chose a specific library).\n* metadata filtering: helps, but managing that manually is a nightmare.\n\nthe breakthrough:\n\ni realized i was treating memory like a static library, but human memory is dynamic. we dont remember everything with equal weight. some things need to be forgotten, some need to be merged, and some only matter when they're actually relevant.\n\nthis might sound obvious in hindsight, but i realized i wasn‚Äôt missing a better database ‚Äî i was missing an operating system for memory.\n\nwhat i‚Äôm experimenting with:\n\nSo i built (and open sourced) something called MemOS.\n\nits a memory management layer that sits between your LLM and your storage. instead of just dumping text into a vector store, it treats memory with a lifecycle:\n\n* generated: raw info comes in\n* activated: relevant stuff is pulled into 'Working Memory' (RAM)\n* merged: repeated or evolving preferences get collapsed instead of duplicated.\n* activated: only stuff that actually matters *now* gets pulled into working memory.\n\nit also separates Facts (what happened) from Preferences (what i like). this was the real game changer for me. now, when i ask for a hotel recommendation, it checks my PREFERENCES (cheap, clean) and searches FACTS (hotels in the area).\n\nIm really trying to make this solid for production use, not just a toy demo\n\nthe repo is here: [https://github.com/MemTensor/MemOS](https://github.com/MemTensor/MemOS)\n\ndocs/cloud trial if you dont want to self-host: [https://memos-docs.openmem.net/cn](https://memos-docs.openmem.net/cn)\n\nWould love to hear how you guys are handling long-term user state vs static RAG. is anyone else trying to build an 'OS' layer for this?\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qjvqd4/vector_dbs_arent_memory_learned_this_the_hard_way/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o12b91h",
          "author": "Expensive_Culture_46",
          "text": "Oh look an ad",
          "score": -38,
          "created_utc": "2026-01-22 15:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12am84",
          "author": "Inner_Possibility310",
          "text": "context pollution is exactly what killed my last project. i hated that the bot would bring up 'User likes pizza' when i was asking about a 'Python pizza library'. does this actually filter that out?",
          "score": 3,
          "created_utc": "2026-01-22 15:43:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cehp",
              "author": "No-Living-8429",
              "text": "MemOS doesn‚Äôt treat all memory as global context. It separates facts, preferences, tools, and knowledge into different memory types, and only activates what‚Äôs relevant to the current task.\nSo ‚ÄúUser likes pizza‚Äù won‚Äôt leak into a ‚ÄúPython pizza library‚Äù query unless the retrieval policy explicitly says it should. Memory is filtered, scoped, and gated, not blindly stuffed back into the prompt.\n\nIn other words: MemOS is built to stop context pollution by design, not by prompt hacks.",
              "score": 1,
              "created_utc": "2026-01-22 15:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o13q7rp",
          "author": "New_Animator_7710",
          "text": "This hits hard because it‚Äôs *exactly* the failure mode of‚Äô ‚Äújust throw it in a vector DB‚Äù memory. You nailed it: the problem isn‚Äôt retrieval, it‚Äôs **state management**‚Äîforgetting, merging, and prioritizing like a real system, not a library. MemOS feels way closer to how humans (and good dev tools) actually work than most RAG stacks I‚Äôve seen",
          "score": 3,
          "created_utc": "2026-01-22 19:33:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11zga1",
          "author": "Crafty_Disk_7026",
          "text": "These tools never work for me.  What works is having a completely clean session and only giving the llm the specific info it needs.   There's no point to storing memory of 2 weeks ago when your app was a Python app if it's a rust app now....\n\nI would encourage you to do some benchmarking.  Try a task with your memory layer and without it and you may be surprised that it's worse with memory since it's just overloading the context with unnecessary info.",
          "score": 4,
          "created_utc": "2026-01-22 14:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o128cvy",
              "author": "Academic_Track_2765",
              "text": "This! Exactly this!",
              "score": 1,
              "created_utc": "2026-01-22 15:32:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12bmdq",
              "author": "Sweet121",
              "text": "This! You nailed the exact problem I hit.\n\nWhen I switched from Python to Rust, my previous RAG setup kept pulling Python snippets because they were 'semantically similar' to the coding task. It was worse than useless‚Äîit was confusing the model.\n\nThat's actually why I'm trying to treat memory with a lifecycle (forgetting mechanism) rather than a static dump. A 'clean session' is great, but I hated re-prompting 'I prefer concise code' or 'don't use unwrap()' every single time.\n\nI'm accepting your challenge on the 'with vs without' benchmark. I want to prove (to myself mostly) that selective memory > zero memory > bad memory. Will update the repo when I have those numbers.",
              "score": 1,
              "created_utc": "2026-01-22 15:47:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1eb6gl",
              "author": "LettuceEfficient7170",
              "text": "I feel the same way",
              "score": 1,
              "created_utc": "2026-01-24 08:58:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12b57s",
          "author": "not_-ram",
          "text": "interesting. so if i'm building a travel agent, and the user changes their mind from 'Budget' to 'Luxury' halfway through, does MemOS catch that? or do i get conflicting memories?",
          "score": 2,
          "created_utc": "2026-01-22 15:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cv88",
              "author": "No-Living-8429",
              "text": "yep, preferences are mutable memories in MemOS.\n\nwhen a user switches from Budget ‚Üí Luxury, the old preference is updated/archived, not kept active. Only the latest state is recalled ‚Äî no conflicting context, no stale decisions.",
              "score": 2,
              "created_utc": "2026-01-22 15:53:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ew04",
              "author": "Sweet121",
              "text": "thats the 'Conflict Resolution' part of the lifecycle. when new info conflicts with old info (high confidence), MemOS updates the state instead of just appending a new row",
              "score": 1,
              "created_utc": "2026-01-22 16:02:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bjqq",
          "author": "Aslymcrumptionpenis",
          "text": "yeah, distinguishing between semantic similarity and actual relevance is hard. how are you handling the 'Merging' part? is it an LLM call in the background?",
          "score": 1,
          "created_utc": "2026-01-22 15:47:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12d43k",
              "author": "No-Living-8429",
              "text": "merging is LLM-assisted, but gated.\n\nwe first use embeddings to detect potential duplicates, then call an LLM only to decide merge vs keep (and how to rewrite). If merged, the old memory is archived so it won‚Äôt steer future decisions.",
              "score": 1,
              "created_utc": "2026-01-22 15:54:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12by6n",
          "author": "IncreaseWilling6396",
          "text": "how is this different from MemGPT? or just using LangChain's EntityMemory? feels like we are reinventing the wheel.",
          "score": 1,
          "created_utc": "2026-01-22 15:49:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12disw",
              "author": "No-Living-8429",
              "text": "the difference is scope and lifecycle.\n\nMemGPT / EntityMemory mainly store + retrieve text inside an agent loop. MemOS treats memory as a system layer: multi-type (facts, prefs, tools), merge/archive over time, scheduled recall, and cross-agent reuse ‚Äî so you don‚Äôt just remember, you prevent stale or conflicting memory from steering decisions.",
              "score": 1,
              "created_utc": "2026-01-22 15:56:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14pm08",
          "author": "toothpastespiders",
          "text": ">Would love to hear how you guys are handling long-term user state vs static RAG. is anyone else trying to build an 'OS' layer for this?\n\nSadly, that's essentially my view of the only fully effective way to handle things. I think effective memory needs to be either custom made from the ground up to fit individual needs and styles or existing projects heavily modified to do the same.",
          "score": 1,
          "created_utc": "2026-01-22 22:21:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eaypc",
          "author": "LettuceEfficient7170",
          "text": "That's so cool, I want to try it!",
          "score": 1,
          "created_utc": "2026-01-24 08:56:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eg94d",
              "author": "Sweet121",
              "text": "yeah,  pls",
              "score": 1,
              "created_utc": "2026-01-24 09:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11y0yi",
          "author": "Academic_Track_2765",
          "text": "Ok bro, what are you selling?",
          "score": -36,
          "created_utc": "2026-01-22 14:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o125eou",
              "author": "Sweet121",
              "text": "This is an open-source project, and the community allows limited promotion. What are you yelling about here?",
              "score": 6,
              "created_utc": "2026-01-22 15:18:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o12842b",
                  "author": "Academic_Track_2765",
                  "text": "Just sick of people selling stuff, when there are production grade systems available. RAG is not rocket science. Install FAISS and be on your way if you want something basic. What really bothers me is that people say use this use that, without any benchmarks e.g., How do you know if weaviate is better than pinecone? If you aren‚Äôt in production than why aren‚Äôt you just using chromadb / faiss? There is already Mem0 and Zep and few other frameworks for memory. If you are going to advertise please share how it compares with other similar frameworks. Besides that I have no problem.",
                  "score": -2,
                  "created_utc": "2026-01-22 15:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlftqz",
      "title": "Which Vector DB should I use for production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "author": "Cheriya_Manushyan",
      "created_utc": "2026-01-24 06:42:26",
      "score": 38,
      "num_comments": 83,
      "upvote_ratio": 0.94,
      "text": "I see many enterprises using Pinecone, Weaviate, Milvus, Qdrant etc. Based on your experience, which one is best  for production and why? Help a friend out...üôÇ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1ep896",
          "author": "ampancha",
          "text": "If you're already on Postgres, pgvector is underrated. One less system to secure and operate, and recent benchmarks show it's competitive with the dedicated options at moderate scale.\n\nIf you want a purpose-built vector DB, Qdrant. Best latency performance in most independent tests, and the open-source version is production-ready.\n\nEither works. What usually breaks is the stuff around the DB: missing per-user query limits, no spend caps on embedding calls, no alerting when retrieval patterns drift. Sent you a DM",
          "score": 16,
          "created_utc": "2026-01-24 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o08hm",
              "author": "Appropriate_Ant_4629",
              "text": "Came here to describe a pretty extensive test from my workplace that benchmarked many (qdrant, milvus, chroma, pgvector, databricks's vector search feature, building our own with faiss, building our own with hnswlib, etc) ...    \n... but our findings exactly matched yours.  \n\nWhen we reach a billion vectors we'll re-visit Qdrant - because it is impressive technologically and was easy to shard across compute nodes; but at our modest tens-of-millions, pgvector shined.",
              "score": 3,
              "created_utc": "2026-01-25 18:41:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1et06u",
              "author": "Cheriya_Manushyan",
              "text": "Informative, thanks.",
              "score": 2,
              "created_utc": "2026-01-24 11:41:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1i65yu",
                  "author": "LightShadow",
                  "text": "We're using Qdrant for performance and continuing feature development. We didn't want the vector operations mixed with our normal postgres load in case they slowed each other down.",
                  "score": 2,
                  "created_utc": "2026-01-24 21:51:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nr3j6",
              "author": "licjon",
              "text": "Both also work. You can do a 2-stage search starting with Qdrant and then narrowing even more with pgvector. Works well with cross-document search and large corpora.",
              "score": 2,
              "created_utc": "2026-01-25 18:04:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e2jwz",
          "author": "hrishikamath",
          "text": "Postgres w pgvector lol, I serve like 100k documents and it takes like milli seconds for hybrid search. Edit: repo: https://github.com/kamathhrishi/stratalens-ai and blogpost: https://substack.com/home/post/p-181608263",
          "score": 35,
          "created_utc": "2026-01-24 07:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eemh2",
              "author": "ZiKyooc",
              "text": "That is a lot of seconds",
              "score": 11,
              "created_utc": "2026-01-24 09:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fifrm",
                  "author": "hrishikamath",
                  "text": "Sorry I meant milli seconds loll (editing my comment)",
                  "score": 4,
                  "created_utc": "2026-01-24 14:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1e4j11",
              "author": "Ok-Adhesiveness-4141",
              "text": "Right choice.",
              "score": 3,
              "created_utc": "2026-01-24 07:58:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1e5hci",
              "author": "debauch3ry",
              "text": "~~Are you saying pgvector is bad?~~ (confusion resolved after edit!) I found the tech very reliable with 500k vectors indexed with HNSW giving very fast knn searches. As a general DB it's also fairly high tier.",
              "score": 4,
              "created_utc": "2026-01-24 08:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fijjb",
                  "author": "hrishikamath",
                  "text": "I meant milli seconds I edited my comment",
                  "score": 3,
                  "created_utc": "2026-01-24 14:32:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1foxph",
                  "author": "hrishikamath",
                  "text": "Repo: https://github.com/kamathhrishi/stratalens-ai and blogpost how I did it: https://substack.com/home/post/p-181608263",
                  "score": 2,
                  "created_utc": "2026-01-24 15:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ertbv",
              "author": "psanilp",
              "text": "How do you handle chunking? We use the Azure AI search pipeline and thinking of going local RAG. Do you have a ready product or licensable deployment?",
              "score": 2,
              "created_utc": "2026-01-24 11:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1hgho7",
                  "author": "hrishikamath",
                  "text": "Chunking: [https://substack.com/home/post/p-181608263](https://substack.com/home/post/p-181608263) Yes, a kind of ready product is online. also open source: [https://github.com/kamathhrishi/stratalens-ai](https://github.com/kamathhrishi/stratalens-ai)",
                  "score": 2,
                  "created_utc": "2026-01-24 19:50:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f35x9",
              "author": "Straight-Gazelle-597",
              "text": "solid choice to cover at least 90% of the biz needs;-)",
              "score": 2,
              "created_utc": "2026-01-24 12:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fw61c",
              "author": "virgilash",
              "text": "Yeah, op, give PostgreSQL with pgvector a try üòâ",
              "score": 2,
              "created_utc": "2026-01-24 15:42:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1gigea",
                  "author": "Cheriya_Manushyan",
                  "text": "Yeah, definitely!",
                  "score": 1,
                  "created_utc": "2026-01-24 17:22:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1gil92",
              "author": "Cheriya_Manushyan",
              "text": "Thanks a lot for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 17:23:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e29ow",
          "author": "instantlybanned",
          "text": "I use milvus in production and it works extremely well for me.¬†\n\n\nPostgreSQL with pgvector is not an option for me because of the recall that I need and the speed with which I need the results at the scale that I'm working at. With milvus, I have control over the in memory index that's being used as well as the parameters for the index and the query parameters for the approximate search, allowing me to tune it to have high recall at still very fast speeds.¬†\n\n\nEdit: just for context, I'm working with around 200 million vectors.¬†",
          "score": 7,
          "created_utc": "2026-01-24 07:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6gs5",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing",
              "score": 1,
              "created_utc": "2026-01-24 08:15:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ilmze",
              "author": "MeringueInformal7670",
              "text": "I am using Milvus in production as a consulting gig i did for a startup pretty solid perf so far during internal runs currently running on a single node standalone setup do you mind sharing how does your Milvus infra look like for 200 mil vectors. You can DM me as well thanks.",
              "score": 1,
              "created_utc": "2026-01-24 23:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lrktx",
                  "author": "ChoiceEmpty8485",
                  "text": "Sure! For my Milvus setup with 200 million vectors, I use a distributed architecture with multiple nodes to handle the load. We have optimized the index parameters for both recall and speed, and utilize GPU acceleration for faster queries. Happy to share more specific details if you need!",
                  "score": 1,
                  "created_utc": "2026-01-25 11:57:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e7ex8",
          "author": "pberck",
          "text": "I used lancedb in Rust in my last project. It worked well, but it wasn't a huge amount of data.",
          "score": 5,
          "created_utc": "2026-01-24 08:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1etlhw",
              "author": "Cheriya_Manushyan",
              "text": "Is it open source or like Pinecone?",
              "score": 1,
              "created_utc": "2026-01-24 11:46:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1etvny",
                  "author": "pberck",
                  "text": "It is dual license, open source with apache 2.0 and a cloud version which has a commercial licence. I used the OS version.",
                  "score": 2,
                  "created_utc": "2026-01-24 11:48:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fmb3m",
          "author": "ComputationalPoet",
          "text": "ill add opensearch,  several variations of semantic search (approximate knn, exact cosine similarity), hybrid search and obviously bm25.   Scales well and performs great, though ill admit i dont know some of these other options that well.",
          "score": 4,
          "created_utc": "2026-01-24 14:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dyeel",
          "author": "IdeaAffectionate945",
          "text": "Roll your own, way faster and more flexible. Preferably something that's a \"plugin\" to SQL, allowing you to parametrise your vector retrieval saying stuff such as \"select \\* from rag where x, and distance(...)\"\n\nIt's a 100 times more flexible than whatever Pinecone even \\*can\\* give you in theory.\n\nI'm using SQLite and sqliteai-vector ...",
          "score": 10,
          "created_utc": "2026-01-24 07:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dz14n",
              "author": "Cheriya_Manushyan",
              "text": "I‚Äôve personally been using PostgreSQL with pgvector, but I notice many enterprises prefer databases like Pinecone. I‚Äôm trying to understand the real reasons behind this choice.",
              "score": 9,
              "created_utc": "2026-01-24 07:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1e2dnr",
                  "author": "IdeaAffectionate945",
                  "text": "*\"I notice many enterprises prefer databases like Pinecone\"*\n\nMarketing bs. The best filtering you can do is on meta fields. With integrated into the core DB, you've got a bajillion times the speed, and a bajillion times the flexibility on querying the thing.",
                  "score": 4,
                  "created_utc": "2026-01-24 07:39:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1e188g",
                  "author": "Chucki_e",
                  "text": "I also use pgvector and I don't think you need to use any third-party vector database unless you have some special requirements that force you to. As with so many other architectural/technology choices, it's easier to start simple and then scale up when you actually need to - of course with a plan in mind, but I don't imagine migrating your vector database isn't that big of an issue?",
                  "score": 6,
                  "created_utc": "2026-01-24 07:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q151b",
              "author": "Appropriate_Ant_4629",
              "text": "For roll-you-own, do you prefer faiss, hnswlib, or something else?\n\nIn our benchmarking hnswlib vastly outperformed faiss; though this may have been user error.\n\nBut in the end, for a few million vectors it didn't really matter; and pgvector was more convenient; and for a billion vector test, sharding across nodes became the hard part and qdrant seemed to handle it best.",
              "score": 2,
              "created_utc": "2026-01-26 00:05:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rtauk",
                  "author": "IdeaAffectionate945",
                  "text": "*\"do you prefer faiss, hnswlib\"*\n\nsqliteai-vector isn't using any indexing as far as I know, only HW optimisation and *\"quantisation\"*, which makes a 1,580 dimensional vector become like 250 if you wish, almost without loosing quality ...\n\nCheck it out on GitHub if you wish. Technically, it's years ahead of *\"everything else\"* in the space ...\n\nHowever, for me it's just a lib, and I don't care about its implementations. I know it's capable of serving my sjit faster than anything else I've tried for SQLite, and I know it's doesn't leak memory like everything else I've tried for SQLite. And I know it returns results in sub seconds even through a DB with 100,000 items, and it gives me *\"memory guarantees\"* which I love like crazy.\n\nIt might not be as scalable as some of the other solutions (for PostgreSQL, MySQL, etc), but then again I'm not building Twitter or Instagram either ...",
                  "score": 2,
                  "created_utc": "2026-01-26 06:07:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ec1id",
          "author": "Suspicious-Bite6107",
          "text": "People that say pgvector is slow are usually DEVs that want \"to simplify their life\" and dont look or make any benchmark, also because they don't read they still think you have to use postgres  with pgvector only, and forget about pgvectorscale or diskann indexes....thank you for still providing work, I think that even with AI there is enough stupidity in this world to allow to have work for the next 50 years :) \n\nPS - your app isn't special, you are not going to have to handle 100k concurent transactions unless as usuall you keep not using pooling :p",
          "score": 6,
          "created_utc": "2026-01-24 09:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eu7ei",
              "author": "Cheriya_Manushyan",
              "text": "Well, you have a fair point.",
              "score": 1,
              "created_utc": "2026-01-24 11:51:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1errhw",
          "author": "Useful-Disk3725",
          "text": "I had qdrant for some time, really fast. Then switched to MariaDB 11.8. Vector search is really fast, but insert is slow. I think qdrant had a buffering system, and building indexes from time to time (standing 100% cpu spikes regularly). I mad a similar thing, getting vectors to a buffer table without index and in a separate cronned process moving in batches.\n\nThe key is, never mix vector index search with regular searches, that‚Äôs a known limitation in almost all databases. Though overcome is easy (technologies advertised as hybrid search are only hack solutions)",
          "score": 3,
          "created_utc": "2026-01-24 11:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1et5tn",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3jfr",
          "author": "Academic_Track_2765",
          "text": "Many options, if you guys use azure use azure search, I have deployed solutions with chromadb, neo4j, weaviate. See what costs the least for your dimensions X documents and use that. I think it mostly comes down to what your company / IT dept is ok with and the costs. You can even build your own if you like.",
          "score": 2,
          "created_utc": "2026-01-24 07:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e8uj9",
              "author": "Cheriya_Manushyan",
              "text": "From your experience, which option do you prefer if the goal is good performance at a low cost?",
              "score": 1,
              "created_utc": "2026-01-24 08:37:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3v7x",
          "author": "Effective-Ad2060",
          "text": "We use qdrant (supports hybrid search out of the box)  \nFor reference:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)",
          "score": 2,
          "created_utc": "2026-01-24 07:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1egvk3",
              "author": "KYDLE2089",
              "text": "+1 we do too work really good",
              "score": 2,
              "created_utc": "2026-01-24 09:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1emf8s",
              "author": "Cheriya_Manushyan",
              "text": "Noted.",
              "score": 0,
              "created_utc": "2026-01-24 10:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e4vaf",
          "author": "nborwankar",
          "text": "Depends on corpus size, whether there is need for joining with relational data and whether there is need to scale up massively",
          "score": 2,
          "created_utc": "2026-01-24 08:01:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eseds",
              "author": "Cheriya_Manushyan",
              "text": "That's an interesting case you shared.",
              "score": 0,
              "created_utc": "2026-01-24 11:36:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gwlhy",
                  "author": "nborwankar",
                  "text": "My default suggestion for workgroup or departmental size enterprise applications is just use Postgres w pgvector which gives you a hybrid relational vector db with ability to join across relational and vector data using just SQL syntax.\n\nIt is not as performant as the pure vector databases but it is very familiar and it is the easiest way to get started while you figure out what you want to do. This is very important for onboarding DB developers.\n\nNot to blow my own horn but I have a book called ‚ÄúVector Databases‚Äù from OReilly coming out in a few months - available on Amazon.\n\nIf you want to read parts of it for free - sign up for the OReilly Platform - it‚Äôs free for 7 days.\n\nThe first chapter deals with these issues. You can also DM me if you have specific questions.",
                  "score": 1,
                  "created_utc": "2026-01-24 18:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1eit8q",
          "author": "RolandRu",
          "text": "In my opinion there is no single ‚Äúbest‚Äù vector DB for production.\n\nI‚Äôm building a code-focused RAG. For now FAISS is enough for me, but I also added BM25 search, hybrid search and a dependency graph between code chunks.\n\nAfter some time I realized that new requirements will only make my custom code more complicated. In practice it feels like I‚Äôm rebuilding features that Weaviate already has (BM25 + hybrid + graph/relations).\n\nQdrant can be faster, but for me the difference like 25ms vs 35ms doesn‚Äôt really matter. Native support for everything I need matters more, so the next step will be migrating to Weaviate and testing it in real use.\n\nAt the same time I will keep FAISS as a nice option for people who want to run the project quickly without setting up a container and configuring Weaviate.\n\nSo Weaviate ‚Äî **if someone thinks this is a mistake, please let me know** üôÇ",
          "score": 2,
          "created_utc": "2026-01-24 10:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esoxa",
              "author": "Cheriya_Manushyan",
              "text": "Noted",
              "score": 1,
              "created_utc": "2026-01-24 11:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1erp4p",
          "author": "psanilp",
          "text": "PostGres with some plugin seems to be the preferred route. Having said that, does anyone here have a 'rag in a box' i can install and deploy at a legal firm?",
          "score": 2,
          "created_utc": "2026-01-24 11:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exwig",
              "author": "seomonstar",
              "text": "anything for legal firms needs heavy skills and lots of custom work. Thats why there is so much money in it. from parsing to chunking to retrieval to context and session management.  There are some open source rag things on github, but I wouldnt be deploying them in a law firm ‚Ä¶",
              "score": 2,
              "created_utc": "2026-01-24 12:21:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1etdu0",
              "author": "Cheriya_Manushyan",
              "text": "By 'rag in a box', you mean low code/no code solution?",
              "score": 1,
              "created_utc": "2026-01-24 11:44:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g34yq",
                  "author": "psanilp",
                  "text": "I understand RAG pipeline and we currently use Azure AI Search with inbuilt vectorisation/hybrid/semantic search . a) There are some issues related to chunking longer docs. b) Some firms who are willing to pay, would like a setup where the RAG is done locally so everything stays within the firewall. Hence no external api calls. So my query was if someone has an end to end solution we can buy/license and deploy on a hardware that is physically placed in the client's premises.",
                  "score": 2,
                  "created_utc": "2026-01-24 16:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1f3g4n",
          "author": "a_developer_2025",
          "text": "Doesn‚Äôt pgvector apply metadata filtering only after searching by vectors? If that‚Äôs still true, it is a big issue if you have small datasets per metadata.",
          "score": 2,
          "created_utc": "2026-01-24 13:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fphel",
          "author": "bzImage",
          "text": "Qdrant.",
          "score": 2,
          "created_utc": "2026-01-24 15:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ftsfa",
          "author": "seomonstar",
          "text": "Postgre scaled to 800 million users with openai, still at their core, so anyone having problems with pgvector should bear in mind its likely a skill issue unless they have 800 million users or more . https://openai.com/index/scaling-postgresql/",
          "score": 2,
          "created_utc": "2026-01-24 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzv2h",
          "author": "Leather-Departure-38",
          "text": "Align with your current cloud infra and db",
          "score": 2,
          "created_utc": "2026-01-24 18:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixl6d",
          "author": "purposefulCA",
          "text": "Our org has mssql. We just pushed all our vectors there. Working fine so far",
          "score": 2,
          "created_utc": "2026-01-25 00:10:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l8ufi",
          "author": "my_byte",
          "text": "Lots of considerations. Personally, my go to if working with a python or nodejs codebase is mongodb. Postgres is also a good choice. Big fan of not adding multiple db's & search engines to your stack. If you end up scaling a ton, migrating to a dedicated search engine isn't terribly hard.",
          "score": 2,
          "created_utc": "2026-01-25 09:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1la8xl",
          "author": "HealthOk5149",
          "text": "Postgres + pgvector with HNSW indexing or + pgvectorscale with DiskANN indexing when RAM becomes an issue at scale because HNSW needs a lot of RAM",
          "score": 2,
          "created_utc": "2026-01-25 09:26:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1llcpd",
          "author": "TemporaryMaybe2163",
          "text": "Genuine question: I wonder why nobody commented about Oracle 26ai. \nIs it bad?",
          "score": 2,
          "created_utc": "2026-01-25 11:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mp3in",
          "author": "martinschaer",
          "text": "SurrealDB. It scales without costing a fortune, runs in single node or distributed, and it is multi model in case you want to do hybrid search with BM25 or graph",
          "score": 2,
          "created_utc": "2026-01-25 15:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shv08",
          "author": "jerrysyw",
          "text": "I had used Milvus for embedding store and chunk text with PG wiht  my product",
          "score": 2,
          "created_utc": "2026-01-26 09:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e6g4b",
          "author": "debauch3ry",
          "text": "I bet those enterprises using Pinecone also write their backend software in python or node.js.\n\nI use PaaS cloud postgres DB + pgvector, as I find it a good mix of having control vs delegating the infrastructure to a cloud provider.\n\nQdrant you can run locally via docker which is testament to their confidence in their tech. They're my next-to-try.",
          "score": 3,
          "created_utc": "2026-01-24 08:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eskjy",
              "author": "Cheriya_Manushyan",
              "text": "For most of them, a python based stack is used.",
              "score": 2,
              "created_utc": "2026-01-24 11:37:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dw4nh",
          "author": "PiaRedDragon",
          "text": "Qdrant or neo4j.",
          "score": 2,
          "created_utc": "2026-01-24 06:44:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dwhad",
              "author": "Cheriya_Manushyan",
              "text": "Can you give reasons, focusing on performance and cost compared to other databases?",
              "score": 2,
              "created_utc": "2026-01-24 06:47:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1enimk",
          "author": "crishoj",
          "text": "Are you certain you even need vectors? Have you tried regular keyword search? Have the agent come up with relevant search terms",
          "score": 2,
          "created_utc": "2026-01-24 10:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esw2l",
              "author": "Cheriya_Manushyan",
              "text": "I haven't tried keyword only search, will checkout.",
              "score": 1,
              "created_utc": "2026-01-24 11:40:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1etdxu",
          "author": "Professional_Cup6629",
          "text": "might be a good read: https://agentset.ai/blog/best-vector-db-for-rag",
          "score": 1,
          "created_utc": "2026-01-24 11:44:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ey7jg",
              "author": "Cheriya_Manushyan",
              "text": "Will check.",
              "score": 1,
              "created_utc": "2026-01-24 12:23:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f35tp",
          "author": "Live-Guitar-8661",
          "text": "Postgres",
          "score": 1,
          "created_utc": "2026-01-24 12:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jd3lw",
          "author": "Grocery_Odd",
          "text": "Went through making this decision for a separate project, developed and applied this framework to get an eval-driven solution: [https://github.com/conclude-ai/rag-select](https://github.com/conclude-ai/rag-select)",
          "score": 1,
          "created_utc": "2026-01-25 01:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vt3gt",
          "author": "Green_Crab_9726",
          "text": "FalkorDB",
          "score": 1,
          "created_utc": "2026-01-26 20:08:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjp5vy",
      "title": "Turn documents into an interactive mind map + chat (RAG) üß†üìÑ",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjp5vy/turn_documents_into_an_interactive_mind_map_chat/",
      "author": "sAI_Innovator",
      "created_utc": "2026-01-22 08:29:10",
      "score": 37,
      "num_comments": 6,
      "upvote_ratio": 0.95,
      "text": "Built an app that converts any PDF/DOCX into an interactive mind map (NotebookLM-style).\n\n‚Ä¢ Click a node ‚Üí summary + keywords + ask questions\n\n‚Ä¢ Chat with the whole document (RAG + sources)\n\n‚Ä¢ Document history saved\n\nStack: React + FastAPI, LlamaIndex (parent‚Äìchild), optional Docling parsing.\n\nRepo: https://github.com/SaiDev1617/mindmap\n\nWould love feedback!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qjp5vy/turn_documents_into_an_interactive_mind_map_chat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o11uwuz",
          "author": "CommercialComputer15",
          "text": "How does it organize and recognise relationships between documents? Semantically? Is it a graph?",
          "score": 2,
          "created_utc": "2026-01-22 14:26:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14up4d",
              "author": "sAI_Innovator",
              "text": "Using Hierarchical Llamaindex node parser üëç",
              "score": 1,
              "created_utc": "2026-01-22 22:50:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bfp6",
          "author": "Aslymcrumptionpenis",
          "text": "oh wow thats helpful",
          "score": 1,
          "created_utc": "2026-01-22 15:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14us0k",
              "author": "sAI_Innovator",
              "text": "Thank you! Please check out the repo.",
              "score": 1,
              "created_utc": "2026-01-22 22:51:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14mnnb",
          "author": "Unique-Temperature17",
          "text": "Great stuff, congrats on shipping this! The mind map visualisation approach is a nice twist on the usual RAG chat interface. Will definitely clone and check it out over the weekend. Always cool to see LlamaIndex projects in the wild.",
          "score": 1,
          "created_utc": "2026-01-22 22:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14u1ei",
              "author": "sAI_Innovator",
              "text": "cool. Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 22:47:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qispty",
      "title": "Best production-ready RAG framework",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qispty/best_productionready_rag_framework/",
      "author": "marcusaureliusN",
      "created_utc": "2026-01-21 08:43:54",
      "score": 34,
      "num_comments": 22,
      "upvote_ratio": 0.97,
      "text": "Best open-source RAG framework for production?\n\nWe are building a RAG service for an insurance company. Given a query about medical history, the goal is to retrieve relevant medical literature and maybe give some short summary.\n\nService will run on internal server with no access to Internet. Local LLM will be self-hosted with GPU. Is there any production(not research) focused RAG framework? Must-have feature is retrieval of relevant evidences. It will be great if the framework handles most of the backend stuff.\n\nMy quick research gives me LlamaIndex, Haystack, R2R. Any suggestions/advice would be great!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qispty/best_productionready_rag_framework/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17d2a3",
          "author": "Intelligent_Push7935",
          "text": "If your main requirement is strong evidence retrieval, I would start from the retrieval layer.\n\nZeroEntropy has an end to end retrieval stack (document ingestion/parsing + hybrid search + reranking), and they also offer an on-prem option so it can run inside your own network.\n\nA simple production pattern is:\n\n- do a broad first pass retrieval to collect candidates\n\n- rerank the candidates so the top results are the best evidence\n\n- only summarize from the top reranked chunks and return those chunks as citations\n\nIf you need something you can run fully offline with open weights, zerank-1-small is available on Hugging Face. If you want instruction following in the reranker,  zerank-2 is built for that.\n\nThey also have embeddings (zembed-1), but it is still early preview / private beta based on their own site, so I would treat it as optional for now.",
          "score": 9,
          "created_utc": "2026-01-23 08:17:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0toefv",
          "author": "bravelogitex",
          "text": "I would try [https://ragflow.io/](https://ragflow.io/) since it's a complete solution\n\nIf it is lacking I'd go with haystack, my research showed that to be robust. R2R is unsupported and the repo is a ghost town.",
          "score": 6,
          "created_utc": "2026-01-21 08:48:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17fr7x",
          "author": "Cool_Drive_2090",
          "text": "i would try zeroentropy.dev",
          "score": 3,
          "created_utc": "2026-01-23 08:42:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0udsll",
          "author": "Effective-Ad2060",
          "text": "You should give PipesHub a try.\n\nPipesHub can answer any queries from your existing knowledge base, provides Visual Citations and supports direct integration with File uploads, Google Drive,¬†Gmail, OneDrive, SharePoint Online, Outlook, Dropbox and more. Our implementation (Multimodal Agentic Graph RAG) says Information not found rather than hallucinating.¬†You can self-host, choose any AI model including local inferencing models of your choice.  \nOur AI accuracy is best in class\n\nGitHub Link :  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)",
          "score": 7,
          "created_utc": "2026-01-21 12:27:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vrbjq",
              "author": "Clay_Ferguson",
              "text": "pipeshub looks cool, but many people (including me) don't want to touch anything that's not \\`MIT License\\`. Every other license is trying to limit you in some way.",
              "score": 7,
              "created_utc": "2026-01-21 16:42:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wc3ft",
          "author": "OnyxProyectoUno",
          "text": "The framework choice matters less than getting your document processing pipeline right. Medical literature has complex structures that most parsing approaches butcher. Tables, references, nested sections all get scrambled during ingestion, and you won't discover this until retrieval returns garbage.\n\nLlamaIndex and Haystack handle orchestration well enough, but they won't fix upstream problems. If your parsing mangles a critical study methodology or splits dosage information across chunks, no amount of sophisticated retrieval will recover that context. You need visibility into what your documents actually look like after processing.\n\nR2R has decent observability features, which helps with debugging retrieval issues. But the real problems usually trace back to chunking strategy and how you're handling document structure. Medical papers aren't just text blocks. They have hierarchies, cross-references, and metadata that needs to survive the processing pipeline.\n\nI've been building vectorflow.dev to tackle exactly this visibility problem, letting teams preview their processed documents before committing to a pipeline configuration. For your use case, I'd focus on getting document processing right first, then layer whichever orchestration framework fits your infrastructure constraints.\n\nWhat does your medical literature look like? PDFs with complex formatting, or cleaner structured documents?",
          "score": 3,
          "created_utc": "2026-01-21 18:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0trtgh",
          "author": "No_Kick7086",
          "text": "How much data are you embedding, how is it structured,  that matters, a lot and what format is it. Also this is an art form in my experience, a lot depends on what document formats are being embedded, are they all different formats by different authors. Data prep and parsing is one of the hardest things to get right in a commercial setting in my experience. I built a quite advanced customer service rag saas for small businesses and I get new edge cases all the time.\n\n Having only one customer can simplify things but it all depends on the scale of the work and it sounds like you need sources citing etc. Why not use a good opensource model on its own locally, unless your planning on adding medical texts as rag",
          "score": 2,
          "created_utc": "2026-01-21 09:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vrzl8",
          "author": "Clay_Ferguson",
          "text": "I have the same question myself, but I'd phrase it as best LangChain-based RAG framework that's MIT License. You can then use LangChain Openwork as the GUI if you want.",
          "score": 2,
          "created_utc": "2026-01-21 16:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wfa4s",
          "author": "ampancha",
          "text": "All three frameworks can handle the retrieval mechanics, but for insurance and medical data the harder problem is what sits around them: audit trails for every retrieval, PII redaction before anything hits the LLM context, and strict filtering so the system only surfaces evidence from approved document sets.  \nFramework choice matters less than whether you can prove to compliance that a query about Patient A never leaked context from Patient B. Sending you a DM with more specifics",
          "score": 2,
          "created_utc": "2026-01-21 18:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1b26bq",
              "author": "Ok-Durian8329",
              "text": "You nailed it.",
              "score": 1,
              "created_utc": "2026-01-23 20:50:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x835w",
          "author": "CarefulDeer84",
          "text": "I'd say LlamaIndex or R2R depending on how much control you want. LlamaIndex abstracts a lot which is nice, but R2R gives you more flexibility for custom retrieval logic.\n\nFor medical literature though, retrieval quality matters way more than framework choice. We had Lexis Solutions set up a system with Voyage embeddings and proper chunking strategies that actually understood medical context instead of just semantic similarity. Made a huge difference in precision for our healthcare client. If you're doing production insurance stuff, getting the embedding model and chunk strategy right is probably more important than which framework you pick.",
          "score": 2,
          "created_utc": "2026-01-21 20:37:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0v3e8z",
          "author": "Live-Guitar-8661",
          "text": "If you want to try something early, shoot me a DM.",
          "score": 1,
          "created_utc": "2026-01-21 14:52:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vvu9p",
          "author": "PurpleCollar415",
          "text": "Although not \"production ready\" or a framework, my RAG system using Qdrant + Voyage AI embedding models is supremely optimized for accuracy retrieval. You can ingest any corpora, I just used agent framework documentation.\n\n[https://github.com/MattMagg/agentic-rag-sdk](https://github.com/MattMagg/agentic-rag-sdk)",
          "score": 1,
          "created_utc": "2026-01-21 17:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w86o9",
          "author": "Legitimate-Leek4235",
          "text": "You can use the framework which built this : https://github.com/traversaal-ai/lennyhub-rag",
          "score": 1,
          "created_utc": "2026-01-21 17:57:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wguu4",
          "author": "Academic_Track_2765",
          "text": "Azure search, its built for large scale production systems.",
          "score": 1,
          "created_utc": "2026-01-21 18:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10swmq",
          "author": "Ch3mCat",
          "text": "ColPali (Layra or else...) ?",
          "score": 1,
          "created_utc": "2026-01-22 10:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11yv6j",
          "author": "vinoonovino26",
          "text": "Nexa.ai has a product called hyperlink, might wanna try it",
          "score": 1,
          "created_utc": "2026-01-22 14:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14sgt3",
          "author": "primateprime_",
          "text": "There isn't a best. You have to figure out what performance (answer quality, response time, general user experience) and weigh that against how much time and money you are willing to allocate. \nSo, how smart, how fast, and how much dakka do you want to throw at it?",
          "score": 1,
          "created_utc": "2026-01-22 22:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18z5w4",
          "author": "prodigy_ai",
          "text": "We‚Äôre going with enhanced GraphRAG, especially because we‚Äôre targeting healthcare and legal use cases. In research and academic contexts, GraphRAG consistently outperforms standard RAG, so it‚Äôs the better fit for what we‚Äôre building.",
          "score": 1,
          "created_utc": "2026-01-23 15:05:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzfrf",
          "author": "ajay-c",
          "text": "Interesting",
          "score": 1,
          "created_utc": "2026-01-24 18:36:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ohkja",
          "author": "heybigeyes123",
          "text": "Try our finblade.ai",
          "score": 1,
          "created_utc": "2026-01-25 19:57:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmlxfw",
      "title": "Looking for testers: 100% local RAG system with one-command setup",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "author": "primoco",
      "created_utc": "2026-01-25 15:23:30",
      "score": 28,
      "num_comments": 26,
      "upvote_ratio": 0.94,
      "text": "Hey everyone! üëã\n\nI've been working on an open-source RAG system and I'm looking for people willing to test it and give honest feedback.\n\n\\*\\*What it does:\\*\\*\n\nA document processing and Q&A system that runs 100% locally on your machine. No API keys needed, no data leaving your computer.\n\n\\*\\*Why I built it:\\*\\*\n\nI was frustrated with RAG solutions that either required cloud services, complex Docker configurations, or hours of setup. I wanted something that \"just works\" out of the box for businesses that need complete data privacy.\n\n\\*\\*Tech stack:\\*\\*\n\n\\- FastAPI backend\n\n\\- React + Vite frontend  \n\n\\- Qdrant for vector storage\n\n\\- Ollama for local LLMs (Qwen2.5 or Mistral 7B)\n\n\\- BAAI/bge-m3 embeddings\n\n\\*\\*Key features:\\*\\*\n\n\\- Single command to start everything (\\`./setup.sh standard\\`)\n\n\\- Completely offline after initial setup\n\n\\- Supports 29 languages\n\n\\- Multi-user with JWT auth and role-based access\n\n\\- OCR support (Apache Tika + Tesseract)\n\n\\- Handles PDF, DOCX, PPTX, XLSX, TXT, MD and more\n\n\\- Tested with 10,000+ documents\n\n\\*\\*What I'm looking for:\\*\\*\n\n\\- Feedback on installation experience (Ubuntu 20.04+)\n\n\\- Real-world testing with your own documents\n\n\\- Bug reports and edge cases\n\n\\- Suggestions for improvements\n\n\\*\\*Repo:\\*\\* https://github.com/I3K-IT/RAG-Enterprise\n\nI've stress-tested it with large documents (including the Mueller Report) but I'd love to see how it handles different use cases and languages.\n\nHappy to answer any questions!\n\n\\---\n\n\\*Fully open-source under AGPL-3.0. No paid tiers, no telemetry, no external calls.\\*",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1o03id",
          "author": "ampancha",
          "text": "Nice work on the local-first setup. One thing worth stress-testing before enterprise users hit it: retrieval-augmented systems are vulnerable to prompt injection via document content, and multi-user setups without per-user rate limits or query attribution can get abused fast. Both failure modes are invisible until production. Sent you a DM with more detail.",
          "score": 2,
          "created_utc": "2026-01-25 18:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o66iq",
              "author": "primoco",
              "text": "Really appreciate this feedback ‚Äî you‚Äôre raising exactly the kind of security considerations that matter for production deployments.\nYou‚Äôre right that both prompt injection via document content and multi-user abuse patterns are often invisible until they hit you in production. These are definitely on my radar for hardening the system.\nChecked your DM ‚Äî thanks for the detailed insights. I‚Äôll follow up there.\nFor anyone else reading: this kind of security-focused feedback is gold. If you spot potential vulnerabilities or have suggestions, Issues and DMs are always welcome.",
              "score": 1,
              "created_utc": "2026-01-25 19:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nbwin",
          "author": "Chrys",
          "text": "I could give it a try but I have a Mac mini. What do you think? It will be too slow?",
          "score": 1,
          "created_utc": "2026-01-25 17:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nu1eo",
              "author": "primoco",
              "text": "Hi Chris, it depends on which Mac Mini you have.\nApple Silicon (M1/M2/M4) with 16GB+ RAM: Should work well! Ollama runs natively on Apple Silicon and uses the integrated GPU. Performance won‚Äôt match a dedicated NVIDIA card, but it‚Äôs definitely usable.\nIntel Mac Mini: Will be slower since it runs on CPU only.\nThe catch: The automated setup.sh script is designed for Ubuntu + NVIDIA. On Mac, you‚Äôd need to set things up manually:\n\t‚àô\tInstall Docker Desktop\n\t‚àô\tInstall Ollama for Mac\n\t‚àô\tRun Qdrant via Docker\n\t‚àô\tStart the backend/frontend manually\nIf you share your Mac Mini specs (chip + RAM), I can give you a better estimate and maybe help with Mac-specific instructions. It would actually be great to have Mac compatibility documented!",
              "score": 1,
              "created_utc": "2026-01-25 18:16:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ogig8",
                  "author": "Chrys",
                  "text": "Mac mini M4 10-core CPU 10-core GPU 16GB/256GB. \nWhich local LLM do you suggest?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:52:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ou98e",
          "author": "fredastere",
          "text": "Def check it out ty",
          "score": 1,
          "created_utc": "2026-01-25 20:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwajj",
          "author": "Character_Pie_5368",
          "text": "Is there a mcp server interface? I have some pentesting frameworks that I‚Äôd like to try and interface with thisz",
          "score": 1,
          "created_utc": "2026-01-26 02:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rs2va",
              "author": "primoco",
              "text": "Not yet ‚Äî MCP server interface isn‚Äôt implemented at the moment.\nCurrently the system exposes a REST API (FastAPI backend on port 8000) that you could target for pentesting. The main endpoints are:\n\t‚àô\t/api/v1/query ‚Äî RAG queries\n\t‚àô\t/api/v1/documents ‚Äî document upload/management\n\t‚àô\t/api/v1/auth ‚Äî JWT authentication\nThat said, MCP support is an interesting idea for the roadmap ‚Äî would make integration with Claude and other tools much smoother.\nIf you run your pentesting frameworks against it and find vulnerabilities, I‚Äôd genuinely appreciate the feedback! There‚Äôs a SECURITY.md for responsible disclosure.\nWhat frameworks are you planning to use? Curious what you‚Äôre testing for.",
              "score": 1,
              "created_utc": "2026-01-26 05:58:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ratzb",
          "author": "floating___around",
          "text": "I am interested. Would a 3090 run that program?",
          "score": 1,
          "created_utc": "2026-01-26 04:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ru7re",
              "author": "primoco",
              "text": "Absolutely! A 3090 with 24GB VRAM is perfect ‚Äî even more than my test setup (RTX 5070 Ti, 16GB).\n\nThe setup script has three profiles:\n Profile   |GPU VRAM|LLM Model  |RAM  |\n |----------|--------|-----------|-----|\n |`minimal` |8-12GB  |Mistral 7B |16GB |\n |`standard`|12-16GB |Qwen2.5 14B|32GB |\n |`advanced`|16-24GB |Qwen2.5 32B|128GB|\n \nWith your 3090 (24GB), you could run:\n \n ```\n ./setup.sh advanced\n ```\n \nThis installs `qwen2.5:32b-instruct-q4_K_M` ‚Äî the most capable model available.\n \nNote: The advanced profile expects 128GB system RAM. If you have less, you can still run `./setup.sh advanced` but consider switching to the 14B model in `docker-compose.yml` if you experience issues.\n \nAlternatively, `./setup.sh standard` with the 14B model is a safe choice that will run great on your hardware.\nLet me know how it goes!\n\n-----\n\nCos√¨ diamo info complete e accurate. Vuoi modificare qualcosa?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2026-01-26 06:14:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rj89p",
          "author": "ggone20",
          "text": "\nSearch performance metrics? ‚ÄòTested on 10,000+ documents‚Äô with just a Qdrant db and an old small models? Idk‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-26 04:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rv6ux",
              "author": "primoco",
              "text": "Qdrant handles vector search efficiently ‚Äî at 10K docs the bottleneck is typically LLM inference, not retrieval. Query times stay in the 2-4 second range in my testing.\nI don‚Äôt have formal benchmark charts published yet, but it‚Äôs something I‚Äôd like to add to the documentation. If you run stress tests on your setup, I‚Äôd be happy to include real-world metrics from the community!",
              "score": 1,
              "created_utc": "2026-01-26 06:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rwkiv",
                  "author": "ggone20",
                  "text": "I really mean retrieval performance for non-trivial queries.",
                  "score": 1,
                  "created_utc": "2026-01-26 06:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rrwi1",
          "author": "AwayLuck7875",
          "text": "Rag ollama vulkan??",
          "score": 1,
          "created_utc": "2026-01-26 05:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rvm6p",
              "author": "primoco",
              "text": "Are you asking about Vulkan support for AMD/Intel GPUs?\nRAG Enterprise is currently tested with NVIDIA GPUs (CUDA). Ollama does have experimental Vulkan support for AMD cards, but I haven‚Äôt tested that combination yet.\nIn theory it should work ‚Äî the backend just calls Ollama‚Äôs API, it doesn‚Äôt care what‚Äôs running underneath. But I can‚Äôt guarantee performance or stability on Vulkan.\nIf you have an AMD GPU and want to try it:\n\t1.\tInstall Ollama with Vulkan support manually\n\t2.\tSkip the automated setup and configure services individually\n\t3.\tPoint the backend to your Ollama instance\nLet me know your GPU ‚Äî happy to help figure out if it‚Äôs worth trying!",
              "score": 1,
              "created_utc": "2026-01-26 06:25:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rxwbk",
          "author": "AwayLuck7875",
          "text": "Maybe byt work",
          "score": 1,
          "created_utc": "2026-01-26 06:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s4h4j",
              "author": "primoco",
              "text": "Sorry, didn't quite catch that ‚Äî could you clarify? Happy to help if you have questions!",
              "score": 1,
              "created_utc": "2026-01-26 07:37:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rzws6",
          "author": "arxdit",
          "text": "Definitely something that many of us are working on.\n\nI have built my own, and it supports both ollama and openai api key.\n\non ollama I rely mostly on qwen3-coder:30b which is nothing short of AMAZING.\n\nI made my own curated indexing & retrieval algos focusing on handling ever expanding knowledge - including an endless conversation manager that does not compact - it's here \\[[https://github.com/andreirx/FRAKTAG](https://github.com/andreirx/FRAKTAG)\\] if you want to check it out\n\nOh and having a CLI makes it usable by claude code",
          "score": 1,
          "created_utc": "2026-01-26 06:59:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s5csz",
              "author": "primoco",
              "text": "Nice! Just checked out FRAKTAG ‚Äî interesting approach with the non-compacting conversation manager. Different philosophy from what I'm doing but I can see the value for ever-expanding knowledge bases.\n\nThe documentation is really thorough ‚Äî that's not easy to maintain, kudos for that.\n\nThe CLI + Claude Code integration is a smart move ‚Äî that's actually something I should consider adding to RAG Enterprise.\n\nqwen3-coder:30b is impressive, though it needs serious VRAM. I've kept the default models smaller (7B-14B) to lower the entry barrier, but power users can definitely swap in larger models.\n\nCool to see others building in this space ‚Äî plenty of room for different approaches! ü§ù",
              "score": 1,
              "created_utc": "2026-01-26 07:45:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1s7eiw",
                  "author": "arxdit",
                  "text": "I wanted to have the capability of targeting enterprise level private deployments - like a law firm",
                  "score": 1,
                  "created_utc": "2026-01-26 08:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qj13kb",
      "title": "I built my own hierarchical document chunker, sharing it in case it helps anyone else.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qj13kb/i_built_my_own_hierarchical_document_chunker/",
      "author": "Important_Proof5480",
      "created_utc": "2026-01-21 15:36:02",
      "score": 27,
      "num_comments": 10,
      "upvote_ratio": 0.97,
      "text": "A while back I was working on a RAG pipeline that needed to extract structured clauses from dense legal and financial documents. I tried tools like Docling, which worked okay to parse the data, but were too slow for my use case, and tended to flatten the hierarchy. Everything ended up on the same level, which killed context for citations and retrieval.\n\nI needed something which could track deep nesting like this:\n\n* \\# Article II THE MERGER\n* \\## 2.7 Effect on Capital Stock ¬†\n* \\### (b) Statutory Rights of Appraisal ¬†\n* \\#### (i) Notwithstanding anything to the contrary‚Ä¶\n\nAfter a bunch of tweaking, I ended up writing my own parsing + chunking logic that:\n\n* Traverses the document hierarchy tree and attaches the complete heading path to every chunk (so you can feed the full path to the LLM for precise citations)\n* Links chunks by chunk\\_id and parent\\_chunk\\_id ‚Äî at inference time you can easily pull parent chunks or siblings for extra context\n* Only splits on structural boundaries, so each chunk is semantically clean and there are basically 0 mid-sentence cuts\n\nIt worked really well for my project, so I wrapped it in a small frontend and published it as DocSlicer.\n\nTry it here: [https://www.docslicer.ai/](https://www.docslicer.ai/)\n\nJust drop in a PDF or URL, no sign-up needed. Export to json or parquet.\n\nIt's still early and I'm actively improving it, but it already works nicely for long financial or legal docs. Would love to hear real feedback.\n\nHappy to chat in the comments or DMs!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qj13kb/i_built_my_own_hierarchical_document_chunker/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0xtfcz",
          "author": "Clay_Ferguson",
          "text": "Thanks for sharing the project, although I'm probably going to try Docling first to see what kind of performance I get. You might need to be sure your setup is using VRAM correctly and also turn off the OCR flag, which are two things I just now learned from this conversation with Gemini:\n\n[https://gemini.google.com/share/ff2b793b8617](https://gemini.google.com/share/ff2b793b8617)",
          "score": 4,
          "created_utc": "2026-01-21 22:15:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vl0c6",
          "author": "Viqqo",
          "text": "Just tried it for Annex 11: Computerised Systems (edit: 2025), and the results are‚Ä¶ not that good. The document regions are not accurate, the headings are a mess and so are the resulting document hierarchy.",
          "score": 3,
          "created_utc": "2026-01-21 16:13:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vq24h",
              "author": "Important_Proof5480",
              "text": "Thanks for trying it out and for the feedback, that‚Äôs really helpful.\n\nThe doc you mention is actually a great example of a case that‚Äôs still tricky. The line numbers interfere with heading detection, so the structure can get pretty messy right now.\n\nI'm already working on handling line-numbered documents better (filtering them from the actual content), and it‚Äôs part of the next iteration of the parser.\n\nIf you‚Äôre up for it, feel free to try a few other documents as well and let me know if you see similar issues or anything else that looks off. Real-world examples like this are super valuable for improving it.",
              "score": 1,
              "created_utc": "2026-01-21 16:36:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0x6e07",
              "author": "cat47b",
              "text": "Know of any libraries that are better?",
              "score": 1,
              "created_utc": "2026-01-21 20:30:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0w9hdc",
          "author": "Weird-Investment9522",
          "text": "Very cool project, congrats on building it all yourself! I tested this on some SEC filings and the results look solid, hierarchy is definitely captured better than other pdf to md tools I've tried. I'll try it on a larger doc set and send you a dm with feedback.\n\nHow are you identifying the layouts and hierarchy? Is it mostly heuristic-based, or are you using any ML/LLMs?",
          "score": 2,
          "created_utc": "2026-01-21 18:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wdyvc",
              "author": "Important_Proof5480",
              "text": "Appreciate you trying it out! It‚Äôs fully heuristic-based right now, with no LLMs in the parsing step. That keeps it deterministic, fast, and easier to debug.\n\nThe logic is mostly driven by bounding box geometry and typography signals like font size, weight, alignment, vertical spacing, ...\n\nCurious to hear your thoughts after you‚Äôve tried it on a few more documents.",
              "score": 1,
              "created_utc": "2026-01-21 18:22:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xe0jl",
          "author": "Unique-Temperature17",
          "text": "This looks really smart - the hierarchical path tracking is exactly what's missing from most chunkers. I've run into the same flattening issue with legal docs where you lose all the section context. Will definitely give it a spin this weekend with some contracts I've been working with. Nice work shipping this!",
          "score": 2,
          "created_utc": "2026-01-21 21:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11x85t",
              "author": "Important_Proof5480",
              "text": "Great, curious to know how it went and if you have any feedback!",
              "score": 1,
              "created_utc": "2026-01-22 14:38:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1gvmpu",
          "author": "JustSentYourMomHome",
          "text": "I'd love to see an option for engineering codes like ASME. Cool project.",
          "score": 2,
          "created_utc": "2026-01-24 18:19:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ooq5s",
              "author": "Important_Proof5480",
              "text": "Thanks for trying it out! I sent you a DM to learn more about your use case. This is still a very early version and mostly built to gather feedback, so it‚Äôs great to hear about examples like engineering codes.",
              "score": 2,
              "created_utc": "2026-01-25 20:28:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qhvzy7",
      "title": "Compiled a list of ùêöùê∞ùêûùê¨ùê®ùê¶ùêû ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhvzy7/compiled_a_list_of_ùêöùê∞ùêûùê¨ùê®ùê¶ùêû_ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨/",
      "author": "midamurat",
      "created_utc": "2026-01-20 08:52:51",
      "score": 25,
      "num_comments": 7,
      "upvote_ratio": 0.96,
      "text": "Been working on reranking for a while and kept finding info all over the place - different docs, papers, blog posts. Put together what I found in case it helps someone else.\n\n**What it includes:** \n\n* Code to get started quickly (both API and self-hosted)\n* Which models to use for different situations\n* About 20 papers - older foundational ones and recent stuff from 2024-2025\n* How to plug into LangChain, LlamaIndex, etc.\n* Benchmarks and how to measure performance\n* Live leaderboard for comparing models\n\nSome of the recent papers cover interesting approaches like test-time compute for reranking, KV-cache optimizations for throughput, and RL-based dynamic document selection.\n\nStill adding to it as I find more useful stuff. If you've come across resources I missed, feel free to contribute or drop suggestions.\n\n  \nGitHub: [https://github.com/agentset-ai/awesome-rerankers](https://github.com/agentset-ai/awesome-rerankers)\n\nHappy to answer questions about specific models or implementations if anyone's working on similar stuff!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qhvzy7/compiled_a_list_of_ùêöùê∞ùêûùê¨ùê®ùê¶ùêû_ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0n2y7y",
          "author": "Professional_Cup6629",
          "text": "thanks for the list!",
          "score": 1,
          "created_utc": "2026-01-20 09:47:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n32br",
              "author": "midamurat",
              "text": "glad if it helps :)",
              "score": 1,
              "created_utc": "2026-01-20 09:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0neqyl",
          "author": "No_Kick7086",
          "text": "Nice work",
          "score": 1,
          "created_utc": "2026-01-20 11:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nt2t8",
              "author": "midamurat",
              "text": "üôå üôå",
              "score": 1,
              "created_utc": "2026-01-20 13:13:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qibjv",
          "author": "McNiiby",
          "text": "How does this compare to MTEB/RTEB? Why are some of the more popular STOA models missing from the Agentset leaderboard like Llama, gemini, and Qwen3?",
          "score": 1,
          "created_utc": "2026-01-20 20:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ua3rd",
              "author": "midamurat",
              "text": "hugging face leaderboards do not contain rerankers and you're right many other models are missing from Agentset leaderboard. We plan to add models as we test so the list isn't complete yet",
              "score": 1,
              "created_utc": "2026-01-21 12:00:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17581v",
          "author": "Commercial_Range_957",
          "text": "Thanks for the list and if possible could you please let me know if minilm-l6 on cpu(azure aks) for 10k requests per day will workout ??",
          "score": 1,
          "created_utc": "2026-01-23 07:07:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhxtt2",
      "title": "Chunking without document hierarchy breaks RAG quality",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhxtt2/chunking_without_document_hierarchy_breaks_rag/",
      "author": "Upset-Pop1136",
      "created_utc": "2026-01-20 10:41:45",
      "score": 24,
      "num_comments": 17,
      "upvote_ratio": 0.95,
      "text": "I tested a few AI agent builders (Dify, Langflow, n8n, LyZR). Most of them chunk documents by size, but they ignore document hierarchy (doc name, section titles, headings).  \n\n\nSo each chunk loses context and doesn‚Äôt ‚Äúknow‚Äù what topic it belongs to.  \n\n\nSimple fix: **Contextual Prefixing**\n\nBefore embedding, prepend hierarchy like this:\n\n`Document: Admin Guide`\n\n`Section: Security > SSL Configuration`\n\n`[chunk content]`  \n\n\nThis adds a few tokens but improves retrieval a lot.\n\nSurprised this isn‚Äôt common. Does anyone know a builder that already supports hierarchy-aware chunking?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qhxtt2/chunking_without_document_hierarchy_breaks_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0ng5eg",
          "author": "janus2527",
          "text": "Docling",
          "score": 4,
          "created_utc": "2026-01-20 11:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nit6k",
          "author": "Live-Guitar-8661",
          "text": "Totally agree. Also just breaking for chunks wherever.\n\nWe do hierarchy, smart chunks, expanded context, etc. I would love some beta users to test\n\nhttps://orchata.ai",
          "score": 3,
          "created_utc": "2026-01-20 12:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0njsv5",
              "author": "Upset-Pop1136",
              "text": "Thanks I would love to check it.",
              "score": 2,
              "created_utc": "2026-01-20 12:11:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0nnqsc",
                  "author": "Live-Guitar-8661",
                  "text": "Free to sign up, just go to https://app.orchata.ai/signup, let me know what you think!",
                  "score": 1,
                  "created_utc": "2026-01-20 12:39:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0no99r",
          "author": "SiebenZwerg",
          "text": "why before embedding?  \nI thought of this as well but I would have saved the document and section as metadata and provided it as additional context during retrieval so that i don't have 1000 chunks with similar lines at the start.",
          "score": 3,
          "created_utc": "2026-01-20 12:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ntdfy",
              "author": "DotPhysical1282",
              "text": "Agree, what are the benefits of running through it before embedding?",
              "score": 2,
              "created_utc": "2026-01-20 13:15:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0q29ld",
              "author": "Clay_Ferguson",
              "text": "But those similar lines of text at the start ARE part of the context of that chunk. What would be interesting is a hybrid approach where the 'category/metadata' for each 'chunk' is still linked (by relational DB field) to the chunk, but where the 'category/metadata' has IT'S OWN vector generated. So this means would be like having two semantic searches. First you identify things matching the high level category, and then once you narrow down you do semantic search on just the chunks (that don't have the metadata, or the duplicate lines at the start)\n\nI haven't yet done RAG myself, so I may be missing something.",
              "score": 1,
              "created_utc": "2026-01-20 19:44:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0naasd",
          "author": "One_Milk_7025",
          "text": "Yes this is not common but this does improve the retrieval quality a lot.. I use this but not sure any library support this or not.\nYou should keep this recursive depth limit so that the breadcumbs doesn't overflow.. good to see more people using this ‚úåÔ∏è",
          "score": 2,
          "created_utc": "2026-01-20 10:54:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nad4n",
          "author": "Final_Special_7457",
          "text": "I saw someone on YouTube talk about this",
          "score": 2,
          "created_utc": "2026-01-20 10:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ogzaa",
              "author": "rshah4",
              "text": "Maybe me, over at Contextual AI we do this and I have shared/shown this technique.",
              "score": 2,
              "created_utc": "2026-01-20 15:20:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ndpq5",
          "author": "Code-Axion",
          "text": "Https://hierarchychunker.codeaxion.com \n\nSee it in action\nhttps://youtu.be/czO39PaAERI?si=1t_J4NZYUcFU1m1E\n\nCheck this out",
          "score": 2,
          "created_utc": "2026-01-20 11:24:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0no4ae",
          "author": "seomonstar",
          "text": "any open source solutions . it sounds a good idea but a lot of promos in this thread.",
          "score": 2,
          "created_utc": "2026-01-20 12:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oqm4u",
          "author": "Ecstatic_Heron_7944",
          "text": "Yep, this is the same idea behind [https://www.anthropic.com/engineering/contextual-retrieval](https://www.anthropic.com/engineering/contextual-retrieval) (Sep 2024).  \nFrom the article:\n\n    original_chunk = \"The company's revenue grew by 3% over the previous quarter.\"\n    \n    contextualized_chunk = \"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\"",
          "score": 2,
          "created_utc": "2026-01-20 16:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tln01",
          "author": "coderarun",
          "text": "Related: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-01-21 08:22:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vi5e5",
          "author": "Important_Proof5480",
          "text": "Yeah, totally agree with the idea. Hierarchy-aware chunking makes a huge difference in retrieval quality.\n\nIn my case I ended up with a slightly lighter version of that. I only prepend the direct header (the immediate parent), not the full path. Once paths get deep and headings are generic, you shift the embedding away from the chunk‚Äôs true meaning. \n\nI would usually do:\n\nHeader: SSL Configuration ¬†\n\n\\[chunk content\\]\n\nAnd then store the full hierarchy separately as structured metadata. At query time I can still surface or inject the full path if the LLM needs grounding or references, without polluting the embedding itself.\n\nBut every use case is different of course.\n\nI just published my chunker at [https://www.docslicer.ai/](https://www.docslicer.ai/) feel free to give it a try and let me know what you think.",
          "score": 2,
          "created_utc": "2026-01-21 16:01:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zpmu9",
          "author": "Academic_Track_2765",
          "text": "This is not true, there are many ways to do this. You can embed metadata, you can do semantic chunking, you can do topic modeling and then chunks, you can do KG. There is no one way to do it.",
          "score": 2,
          "created_utc": "2026-01-22 04:37:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjx9yy",
      "title": "üöÄ We designed a white-box RAG framework with a built-in AI developer assistant ‚Äî feel free to give it a try!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjx9yy/we_designed_a_whitebox_rag_framework_with_a/",
      "author": "Relevant_Abroad_6614",
      "created_utc": "2026-01-22 15:20:04",
      "score": 22,
      "num_comments": 2,
      "upvote_ratio": 0.9,
      "text": "üöÄ **Introducing UltraRAG 3.0: Reject \"Black Box\" Development. Make Every Line of Inference Logic Visible!**\n\nüèÅ **UltraRAG 3.0** solves the \"Last Mile\" problem in RAG development, developed by THUNLP, NEUIR, OpenBMB & AI9Stars.\n\nüêô **GitHub:** [https://github.com/OpenBMB/UltraRAG](https://github.com/OpenBMB/UltraRAG)\n\nüìö **Tutorial:** [https://ultrarag.openbmb.cn/pages/en/getting\\_started/introduction](https://ultrarag.openbmb.cn/pages/en/getting_started/introduction)\n\n**Key Highlights:**\n\n‚ö° **WYSIWYG Pipeline Builder**\n\nFrom logic to prototype in seconds. Our dual-mode builder (Canvas + Code) syncs in real-time. Click \"Build\" and your static logic instantly becomes an interactive UI. No more boilerplate code!\n\nüîç **Pixel-Level \"White-Box\" Visualization**\n\nStop guessing. The \"Show Thinking\" panel visualizes the entire inference trajectory‚Äîloops, branches, and tool calls. Debug bad cases instantly by comparing retrieval chunks vs. model hallucinations.\n\nü§ñ **Built-in AI Developer Assistant**\n\nStuck on config? The embedded AI Assistant knows the framework inside out. Just use natural language to generate Pipeline configurations, optimize Prompts, or explain parameters.\n\nüî¨ **DeepResearch Engine**\n\nPowered by **AgentCPM-Report**, it supports \"Writing-as-Reasoning.\" The system dynamically plans, retrieves, and deepens content to generate professional, cited reports automatically.\n\n**Check out the demo video on the GitHub pageÔºÅ**",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qjx9yy/we_designed_a_whitebox_rag_framework_with_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17j8td",
          "author": "Relevant_Abroad_6614",
          "text": "https://preview.redd.it/knov7unxf2fg1.png?width=3686&format=png&auto=webp&s=d173ebf383e8371cd9f08f851e7473c9b3f93981",
          "score": 1,
          "created_utc": "2026-01-23 09:14:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o17j9b8",
          "author": "Relevant_Abroad_6614",
          "text": "https://preview.redd.it/1wvuoz51g2fg1.png?width=3734&format=png&auto=webp&s=304ef394c007ea63f4b2792b772b3b5924716bbf",
          "score": 1,
          "created_utc": "2026-01-23 09:14:40",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkupw7",
      "title": "We did RAG on the r/Rag Reddit channel - Free To Use",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkupw7/we_did_rag_on_the_rrag_reddit_channel_free_to_use/",
      "author": "jannemansonh",
      "created_utc": "2026-01-23 16:02:28",
      "score": 19,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hey, creator of Needle here. This channel is one of the reasons we started Needle as a RAG API in the first place! So many real insights from people actually building things.\n\nSo we made a free, public search tool that lets you explore everything discussed here in 2025.\n\nUseful if you're:\n\n* A dev looking for RAG advice\n* A business person exploring how RAG can help\n* A founder researching trends and common problems\n\nWould love your feedback and curious: what questions would you find most interesting to explore?\n\nCompletely free, no signup required: [https://needle.app/featured-collections/reddit-rag-2025](https://needle.app/featured-collections/reddit-rag-2025)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qkupw7/we_did_rag_on_the_rrag_reddit_channel_free_to_use/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o19sp9g",
          "author": "Popular_Sand2773",
          "text": "Without any attribution it's hard to tell what's just world knowledge vs. actually driven by your retrieval so the value add is really unclear.",
          "score": 3,
          "created_utc": "2026-01-23 17:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bpwwt",
              "author": "jannemansonh",
              "text": "You can see the references that are given back in the chat.",
              "score": 1,
              "created_utc": "2026-01-23 22:43:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1der3y",
                  "author": "Popular_Sand2773",
                  "text": "Sorry if it was there it wasn‚Äôt obvious.",
                  "score": 1,
                  "created_utc": "2026-01-24 04:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dp6ac",
          "author": "Key-Contact-6524",
          "text": "Lovely bro , Thanks a ton",
          "score": 1,
          "created_utc": "2026-01-24 05:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecmhn",
              "author": "jannemansonh",
              "text": "Sure! Happy you find it helpful!",
              "score": 1,
              "created_utc": "2026-01-24 09:12:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cako",
          "author": "lucido_dio",
          "text": "RAG-ception :D one step closer to singularity",
          "score": 1,
          "created_utc": "2026-01-23 16:06:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19diah",
              "author": "jannemansonh",
              "text": "Haha, geeky jokes always make me laugh",
              "score": 1,
              "created_utc": "2026-01-23 16:11:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnplpb",
      "title": "Built a tool for visualizing text chunking strategies",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "author": "Crazy-Plan8697",
      "created_utc": "2026-01-26 19:09:49",
      "score": 17,
      "num_comments": 0,
      "upvote_ratio": 0.96,
      "text": "Hey :)\n\nSome time ago I wanted to learn the fundamentals of RAG, and I started with chunking. Greg Kamradt‚Äôs tool ([https://chunkviz.up.railway.app/](https://chunkviz.up.railway.app/)) helped me understand the basics, and it was a great starting point.\n\nWhile digging deeper, especially when reading papers like [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997), I noticed that **semantic** and **agentic** chunking are showing up more often and are getting adopted in new research. But I couldn‚Äôt find any visualizers that supported those methods, so I tried to build one myself.\n\nI put together **Chunking-Vis**, a small web tool for anyone who wants to explore and learn how different chunking strategies behave. I think it can be especially helpful if you‚Äôre new to RAG and want to see how chunks are formed before they‚Äôre sent to an embedding model or retrieval pipeline.\n\n# What Chunking-Vis supports\n\n* Character-level chunking\n* Word-level chunking\n* Token-level chunking (GPT-4o tokenizer)\n* Recursive chunking\n* Semantic chunking\n* Agentic chunking (Phi-3 powered, available locally on CPU)\n\nThere‚Äôs also a **Snapshot** feature that lets you save and compare different chunking configurations side-by-side, which can make experimentation easier.\n\n# Live Demo\n\n[https://chunkingvis-production.up.railway.app](https://chunkingvis-production.up.railway.app)  \n(Agentic chunking is disabled in the demo due to compute limits.)\n\n# Github Repository\n\n[https://github.com/MichalZnalezniak/Chunking-Vis](https://github.com/MichalZnalezniak/Chunking-Vis)\n\nHope some of you find it useful ‚Äî and if you have ideas, feedback, or suggestions, please let me know. Thank you!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qjf80l",
      "title": "Recommendations for cheaper alternatives to ElasticSearch",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjf80l/recommendations_for_cheaper_alternatives_to/",
      "author": "shanukag",
      "created_utc": "2026-01-22 00:18:42",
      "score": 13,
      "num_comments": 24,
      "upvote_ratio": 0.89,
      "text": "Hi everyone,\n\nI‚Äôm building an AI-assisted search feature for an early-stage legal-tech platform and I‚Äôm looking for recommendations for cheaper alternatives to Elasticsearch that still work well for hybrid search use cases.\n\n# The challenge\n\nWe‚Äôre not doing traditional full-text search only. The system needs to support:\n\n* Keyword search\n* Vector similarity search (embeddings)\n* Filtering on metadata (jurisdiction, document type, status, etc.)\n* Reasonable relevance out of the box (I‚Äôd rather not hand-roll ranking logic)\n\nThe content itself is mostly static (guides and reference documents), and traffic is currently low since this is still early days  - but the search quality matters because it feeds into an LLM for AI-assisted answers.\n\n# What we‚Äôve implemented so far\n\n* Elasticsearch as the search layer\n* Hybrid search (keyword + vector)\n* Semantic-style retrieval for RAG workflows\n* Minimal custom scoring or tuning - mostly relying on built-in capabilities\n\nFrom a technical perspective, Elasticsearch works well. From a **cost perspective**, it feels hard to justify right now.\n\n# The problem\n\nEven at low usage, the baseline pricing and add-ons start to add up quickly. I‚Äôm trying to keep infrastructure spend sensible until there‚Äôs clearer traction, without completely downgrading search quality.\n\n# What I‚Äôm hoping to find\n\n* A more startup-friendly alternative to Elasticsearch\n* Supports keyword + vector search (or a realistic hybrid approach)\n* Can handle filters and structured metadata cleanly\n* Prefer managed or low-ops solutions\n* Not looking to fully custom-build a search engine unless there‚Äôs a strong reason\n\nIf you‚Äôve built something similar (hybrid search feeding LLMs) and had to balance cost vs relevance, I‚Äôd really appreciate any recommendations.\n\nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qjf80l/recommendations_for_cheaper_alternatives_to/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0yhjex",
          "author": "mwon",
          "text": "I assume you are using cloud and not community edition. Leave the cloud and rent a VM somewhere and use opensearch or milvus.",
          "score": 11,
          "created_utc": "2026-01-22 00:21:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12ick1",
              "author": "vangapr",
              "text": "You can self host elasticsearch also, no licensing issues.",
              "score": 3,
              "created_utc": "2026-01-22 16:18:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yqkwo",
          "author": "raul3820",
          "text": "https://turbopuffer.com/",
          "score": 2,
          "created_utc": "2026-01-22 01:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z3j5x",
          "author": "madbuda",
          "text": "Open search?? Foss fork if ES",
          "score": 2,
          "created_utc": "2026-01-22 02:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zcj47",
          "author": "anthony_doan",
          "text": "> We‚Äôre not doing traditional full-text search only.\n\nThen use postgres.\n\nElasticsearch and database that base on lucene are using a Trie data structure that is optimize for doc text search. \n\nIf you're not doing that then you shouldn't be using ES. The overhead of using is an undertaking. You either pay for managed services (cloud) or manage it yourself.\n\nI did RAG on Postgres with PGvector. It's fine and plus I also needed a RMDB which reduce the tech stack and complication.\n\nThe PGvector extension will vectorize for you and store vector and vector your query to compare the stored vectors.\n\nYou can either have Postgresql compare the vector or you can pull out the vector and use your programming language to compare it.",
          "score": 2,
          "created_utc": "2026-01-22 03:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zzg5d",
              "author": "[deleted]",
              "text": "[deleted]",
              "score": 0,
              "created_utc": "2026-01-22 05:47:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0zzsw2",
                  "author": "anthony_doan",
                  "text": "> Geez, pg and elasticsearch have completely different use cases.\n\nYeah... I believe I alluded to that with the datastructure that Elasticsearch uses.\n\n> Pg only works if you have less than 100 million rows.\n\nThat doesn't mean anything if I don't know anything about your data. (edit: What type of data, how big it is, how it's structure)\n\n> We do that in less than a month\n\nCool story.\n\n---\n\nedit/update:\n\nI'm here trying to help and address OP's use case. I also gave my reasonings to back it up.\n\nHere come along some rando bragging about his accomplishment without any details nor does it address or attempt to address the user's situation. There's no alternative solution to mine. Just random vague stuff. There's no real counterpoint at all. \n\nIt screams me-me-me-me and the lack of technical and leadership skill.",
                  "score": 3,
                  "created_utc": "2026-01-22 05:50:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0zrr80",
          "author": "Curious-Sample6113",
          "text": "Manticore",
          "score": 2,
          "created_utc": "2026-01-22 04:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10rv34",
          "author": "Present-Reaction4344",
          "text": "Apache Solr",
          "score": 2,
          "created_utc": "2026-01-22 10:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o134zh8",
              "author": "crewone",
              "text": "Solr in every way is not up to date with opensearch or elastic. It doesn't make it bad per se,  but it is lagging in terms of features compared to the well maintained opensearch and elasticsearch.",
              "score": 1,
              "created_utc": "2026-01-22 17:59:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0z5l55",
          "author": "Infamous_Ad5702",
          "text": "I do semantic search, deterministic.\nTokens are too much so I built this\n\n\n* No gpu \n* No hallucination \n* Works offline.\n\n\nI got sick of embedding and chunking. My defence client needed to be offline. So I built Leonata. Happy to go through my process with you or anyone keen..",
          "score": 2,
          "created_utc": "2026-01-22 02:36:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10p6o2",
              "author": "cisspstupid",
              "text": "I would like to learn from you.",
              "score": 1,
              "created_utc": "2026-01-22 09:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16iez7",
                  "author": "Infamous_Ad5702",
                  "text": "Great. I‚Äôll send an invite",
                  "score": 2,
                  "created_utc": "2026-01-23 04:20:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0yi1cg",
          "author": "SerDetestable",
          "text": "pgvector?",
          "score": 1,
          "created_utc": "2026-01-22 00:24:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ykxp7",
              "author": "Expensive_Culture_46",
              "text": "I audibly laughed because I had a baby data scientist try to tell me you can‚Äôt do vector in PG and this is the third time since then someone has mentioned it.",
              "score": 3,
              "created_utc": "2026-01-22 00:39:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yp6lh",
          "author": "jba1224a",
          "text": "Does s3 vectors not work for your use case?\n\nIt is a little slower, but vastly cheaper. \n\nCaveat is you would need to write your own code for lookups, but it would be lightweight and easily done in python.",
          "score": 1,
          "created_utc": "2026-01-22 01:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yyxqf",
          "author": "hrishikamath",
          "text": "I used Postgres for finance rag over sec filings and earnings calls. Like 100k docs. Check it out: https://github.com/kamathhrishi/stratalens-ai",
          "score": 1,
          "created_utc": "2026-01-22 01:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zro93",
          "author": "bzImage",
          "text": "Qdrant",
          "score": 1,
          "created_utc": "2026-01-22 04:51:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1005yh",
          "author": "IdeaAffectionate945",
          "text": "How many records are we talking about? Thousands or millions?",
          "score": 1,
          "created_utc": "2026-01-22 05:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o103ggq",
          "author": "Party-Cartographer11",
          "text": "A bit on terminology...\n\n\nElasticsearch is usually used to indicate the free opensource Elasticsearch. I know there have been changes in the licensing, but pretty sure there a free versions.\n\n\nElastic is the paid version.\n\n\nSo maybe look at the OSS version.¬† Also, as others have stated, I have used Postgres with PG vector.¬† It works well and has some math built in (e.g. cosine for similarity).",
          "score": 1,
          "created_utc": "2026-01-22 06:19:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11x89t",
          "author": "digital_legacy",
          "text": "We built our own Java Vector engine on top of an older version of Elasticsearch\n\n[https://github.com/entermedia-community/entermedia-server/tree/main/src/org/entermediadb/ai/knn](https://github.com/entermedia-community/entermedia-server/tree/main/src/org/entermediadb/ai/knn)",
          "score": 1,
          "created_utc": "2026-01-22 14:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o126i2j",
          "author": "Past-Grapefruit488",
          "text": "What is teh issue with cost..  Hordware ? Hosting ? License ?",
          "score": 1,
          "created_utc": "2026-01-22 15:23:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13s7vi",
          "author": "New_Animator_7710",
          "text": "One alternative perspective: if your corpus is mostly static and low-traffic, you might be over-indexing on ‚Äúsearch infra‚Äù too early. For legal RAG, **retrieval accuracy often comes more from parsing quality, chunking strategy, and reranking** than from the search engine itself.\n\nIn early stages, a simpler stack (Postgres + pgvector or even file-level indexes) plus a strong cross-encoder reranker can outperform Elastic at a fraction of the cost‚Äîand gives you flexibility before committing to heavyweight infra. Elastic shines at scale, but it‚Äôs not always the best *learning-phase* tool.",
          "score": 1,
          "created_utc": "2026-01-22 19:43:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1lq8lt",
          "author": "Green_Crab_9726",
          "text": "Meilisearch my friend",
          "score": 1,
          "created_utc": "2026-01-25 11:46:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yl3mc",
          "author": "Expensive_Culture_46",
          "text": "What‚Äôs your expected user load? Maybe azure blob with AI search enabled. Then set for pay as you go.",
          "score": 0,
          "created_utc": "2026-01-22 00:40:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj38u8",
      "title": "Limits of File System Search (and Why you need RAG)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qj38u8/limits_of_file_system_search_and_why_you_need_rag/",
      "author": "rshah4",
      "created_utc": "2026-01-21 16:53:15",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Nice analysis comparing file system search (think Claude Code) or RAG (chunks). Filesystem works great with a small number of files, but as you get to adding more documents it doesn't scale as well.\n\n* 100 docs: FS takes 11.8 seconds compared to 9.9 for RAG \n* 1000 docs: FS takes 33 seconds compared to 8.4 for RAG\n\nIt's good reminder and data point for why we use RAG. Check our the post for lots more details: [https://www.llamaindex.ai/blog/did-filesystem-tools-kill-vector-search](https://www.llamaindex.ai/blog/did-filesystem-tools-kill-vector-search)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qj38u8/limits_of_file_system_search_and_why_you_need_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0w3rrm",
          "author": "No-Marionberry-772",
          "text": "you're cherry picking.\n\n\nTheir answer isnt use RAG, its, as it usually is, \"it depends\"\n\n\ntheir tests showed file system to be more accurate, so if you care more about accuracy than speed then FS wins.\n\n\nRAG also requires more setup, where FS just exists and is already supported by many tools, RAG requires the installation, integration and configuration of not 1 but many pieces of software, for performance gains that appear fairly marginal at a glance, but at massive scales that could matter.\n\n\nThey dont state the kind of hardware they tested on.¬† An HDD and an SSD are going to have completely different performance profiles, and they seem to assume that you cant optimize file system access performance, which may or may not be a valid depending on where the performance gaps come from.\n\n\nIts still good research however, they just needed to add a lot more detail, and people should read the whole article rather than cherry pick.",
          "score": 2,
          "created_utc": "2026-01-21 17:38:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w93ws",
              "author": "rshah4",
              "text": "Fair enough, this is just one data point, so not saying we should only do RAG.  (The results did show RAG was better than file system, but its close and not a hill I would die on).   \nI do think there is a strong generalization though as your number of files increase (and size of files increase), you are going to need to move to some sort of search/retrieval approach like RAG.    \nI just liked this because it actually gave me a data point.  Now we just need more data points üòÄ",
              "score": 2,
              "created_utc": "2026-01-21 18:01:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xmr6i",
          "author": "seomonstar",
          "text": "Interesting. Fs is also severely hampered by file size. not to mention early context bloat from many files and potential issues with agents.",
          "score": 1,
          "created_utc": "2026-01-21 21:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ddzmp",
          "author": "Infamous_Ad5702",
          "text": "I haven‚Äôt pushed my tool to the edge yet on file size‚Ä¶I‚Äôll keep going",
          "score": 1,
          "created_utc": "2026-01-24 04:28:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qngww9",
      "title": "GraphRAG D√©j√† Vu: Freezing Edges = Graph DB Repeat? (Prod Trade-offs)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "author": "dqj1998",
      "created_utc": "2026-01-26 14:05:21",
      "score": 11,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "**Update (Jan 27, 2026)**:  \nThanks for the great discussion here in r/RAG! Some highlights from related threads:\n- Determinism & reproducibility as key to relational DB win (echoing why Graph DBs struggled).\n- Real prod experiences: keep graphs deterministic/auditable (e.g., calls/imports/FKs), avoid LLM-guessed edges clutter.\n- Links shared: [DDG preprint](https://zenodo.org/records/18373053) and [RoslynIndexer repo](https://github.com/RusieckiRoland/RoslynIndexer) for deterministic code RAG.\n\n---\n\nr/RAG ‚Äî GraphRAG hype (explicit graphs over vector RAG) feels like 70s graph DBs (IMS/CODASYL): explicit relations won benchmarks but lost to relational cuz upfront assumptions brittle.\n\n**Hype vs Reality**\nLLM infers entities/relations ‚Üí persist edges ‚Üí query traversal. Cool for global search, but edges = ingestion-time guesses ‚Üí bias for new intents.\n\n**Core Brittleness**\nFrom my [r/programming post](https://www.reddit.com/r/programming/comments/1pz6pj3/graphrag_is_just_graph_databases_all_over_again/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): Nodes=facts, edges=guesses. Scoped query-time inference (BM25+vectors+rerank) often better for ambiguous RAG (no freeze).\n\n**Pushback & Predictability**\nComments nailed it: auditable edges > opaque LLMs (prod win). Dynamic rebuilds? Viable, but maintenance cost high vs simple hybrid RAG.\n\nShines: stable domains (regs/code deps). Fails: intent-shifting queries.\n\nMedium breakdown:[Medium friend link](https://medium.com/sisai/graphrags-deja-vu-why-are-we-repeating-the-same-mistakes-f6852f54bde0?sk=2692c642e7dfb19e9d552162462384c4)\n\n\nProd experiences? GraphRAG beat baseline RAG on your corpus (e.g., multi-hop QA, latency)? Hybrid + dynamic graphs? Or stick to rerank?\n\nShare benchmarks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1uoi8i",
          "author": "RolandRu",
          "text": "Really happy I found this Reddit btw ‚Äî the topics here are genuinely interesting and they kind of force you to think things through.\n\nAnd yeah, you‚Äôre right: it really depends.\n\nGraphs *can* be brittle when the edges are basically guessed during ingestion (LLM-inferred relations). You‚Äôre sort of freezing assumptions that may not match what people will ask later.\n\nBut it‚Äôs very use-case dependent. For code, I honestly think a dependency graph is pretty much **non-optional**. Calls/imports/inheritance aren‚Äôt opinions ‚Äî they‚Äôre real structure. Without graph expansion you often end up with random snippets, and vanilla RAG struggles badly with questions like ‚Äúwhere does this start?‚Äù or ‚Äúwhat does this change affect?‚Äù, because you‚Äôre missing the whole call chain.",
          "score": 3,
          "created_utc": "2026-01-26 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xa5pj",
              "author": "dqj1998",
              "text": "I totally agree! Seeing your comment about code dependencies resonated with me, I also believe that code is essentially a set of pre-defined dependency chains, and debugging is about constantly patching the gap between these pre-defined chains and reality.\n\nWhile my thinking is still quite rudimentary, that's precisely why I wanted to discuss it further here.\n\nThank you for sharing your real-world experience! In this era where AI is coding faster and faster, this kind of discussion is really interesting‚Äîdo you think AI might represent a spiral of dependency reduction?",
              "score": 1,
              "created_utc": "2026-01-27 00:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xg881",
                  "author": "RolandRu",
                  "text": "I see it like this: dependencies aren‚Äôt going away, because they‚Äôre basically a consequence of architecture (boundaries, responsibilities, contracts). But AI lowers the cost of doing things ‚Äúthe right way‚Äù ‚Äî it‚Äôs easier to add an adapter, an interface, a test, validation, or split a big chunk into smaller parts, without feeling like you‚Äôre wasting time on repetitive stuff. So you‚Äôre less tempted to cram everything into one file/method ‚Äújust because it‚Äôs faster.‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-27 00:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1upwsy",
          "author": "hhussain-",
          "text": "Determinism is what really makes this paradigm *shake*.  \nYour Reddit post and Medium article are excellent ‚Äî they pinpoint both the possibilities *and* the limitations very clearly.\n\nWhat ultimately made relational databases win wasn‚Äôt just performance, but **determinism and reproducibility enabled by a new computing model**.\n\nI had a similar discussion on [r/Rag post](https://www.reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) about deterministic graphs vs probabilistic vectors/embeddings. That thread, along with related discussions, helped isolate the issue to what seems like a missing graph category.\n\nInteresting to see this resurfacing now. I‚Äôve just published a timestamped preprint defining a *Deterministic Domain Graph (DDG)* category:  \n[https://zenodo.org/records/18373053](https://zenodo.org/records/18373053)\n\nI‚Äôm currently working on a framework to construct DDGs in practice, and early experiments suggest this is feasible even for large real-world codebases.",
          "score": 3,
          "created_utc": "2026-01-26 17:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vrjfq",
              "author": "RolandRu",
              "text": "Thanks for sharing the article ‚Äî honestly pretty interesting read.\n\nThis is actually close to where I ended up while building a **code RAG** system. I‚Äôm trying to keep the *edges* deterministic + auditable (calls/imports/inheritance, ReadsFrom/WritesTo, FKs etc.), and I‚Äôm really trying to avoid freezing ‚ÄúLLM-guessed‚Äù relations during ingestion.\n\nI kind of treat vectors as *ranking / fuzzy recall*, but the graph as the closed-world structure that should rebuild the same way every time. For example I force stable outputs (sorted nodes/edges) and I also add missing TABLE nodes so the SQL graph is actually closed (nodes + edges), not half implicit.\n\nOne thing I‚Äôd highlight though: **heuristics ‚â† inference**. I‚Äôm fine with fixed, testable heuristics (like inline SQL detection) ‚Äî even if it‚Äôs not perfect, it‚Äôs still deterministic and you can regression-test it. What I‚Äôm trying to avoid is context-dependent enrichment that changes depending on the model/prompt or whatever the ‚Äúbest guess‚Äù is this week.\n\nIf you‚Äôre curious, this repo is just the **indexing part** (Roslyn/.NET side). The actual RAG pipeline / retrieval is in a separate project:  \n[https://github.com/RusieckiRoland/RoslynIndexer](https://github.com/RusieckiRoland/RoslynIndexer)\n\nAlso curious how you want to handle schema evolution / versioning for DDGs on big real repos ‚Äî do you version the domain spec per build, kind of like a compiler?",
              "score": 2,
              "created_utc": "2026-01-26 20:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xcvpz",
                  "author": "dqj1998",
                  "text": "Wow, thank you both‚Äîthis thread is gold! \n\nu/hhussain, your DDG preprint looks fascinating‚Äîdeterminism + reproducibility as the key to why relational won makes total sense, and early experiments on large codebases sound promising. Will dive into it right away.\n\nu/RolandRu, super appreciate you sharing your setup and the RoslynIndexer repo! Exactly aligns with what I've been thinking: keep the graph as closed-world, deterministic structure (hard facts like calls/imports/inheritance/ReadsFrom/WritesTo/FKs), and treat vectors as fuzzy recall/ranking. Love the emphasis on heuristics (fixed, testable, regression-friendly) vs. context-dependent LLM guesses‚Äîthat's the brittleness killer.Your approach to forcing stable outputs (sorted nodes/edges) and adding missing nodes for closed structure is smart‚Äîavoids the \"half implicit\" mess. Curious on a couple things:\n\n\\* How do you handle schema evolution/versioning in big repos? Per-build domain spec like a compiler, or something else?\n\n\\* Have you seen measurable wins on those chain-tracing queries (e.g., impact analysis) vs. vanilla RAG?\n\n\\* Any thoughts on hybrid with dynamic rebuilds for evolving code, or is pure deterministic the way?\n\nThis ties perfectly into what I'm exploring next: code itself as frozen presuppositions/dependencies, and debug as closing the gap to reality. If you're open, I'd love to reference/link your repo/preprint in upcoming posts (with credit, of course).\n\nThanks again‚Äîthreads like this are why I love posting here!",
                  "score": 2,
                  "created_utc": "2026-01-27 00:30:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkeiss",
      "title": "I built a RAG as a second brain",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkeiss/i_built_a_rag_as_a_second_brain/",
      "author": "mapt0nik",
      "created_utc": "2026-01-23 02:30:16",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.87,
      "text": "I work in software development. On my typical day I go from meeting to meeting and review lots of API contracts and design diagrams. The other day I simply couldn't recall something in a group discussion (I guess my brain was overloaded).\n\nThen it hit me: Why don't I build an app to house all stuff I have gone through so I could ask it for anything whenever? Works like the Pensieve in Harry Potter Dumbledore stores and retrieves memories. üßô‚Äç‚ôÇÔ∏è ü™Ñ  It is like my second brain.\n\nSo I vibe-coded [Second Brain](https://second-brain-484821.web.app/). A private RAG to serve as a digital twin for your personal and/or professional knowledge base.\n\nIf you are interested, here is the [blog](https://maptonik.hashnode.dev/second-brain-enters-public-preview) on my story behind it. Check it out. All comments are welcome.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qkeiss/i_built_a_rag_as_a_second_brain/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1bepnf",
          "author": "Smart_MoneyTor",
          "text": "I am not particularly against one trying to productize their vibe coded app, but there is a surge in these private RAG 2nd brain kind of apps. \n\nPersonally, I‚Äôd be more interested in the approach, the knowledge representation, the retrieval strategy, the performance metrics, the architectural design .. the novelty really. If it‚Äôs just stitching off-the-shelf tooling, I could just make use of any of the more privacy-friendly alternatives. That being said, I get that not everyone is fond of open sourcing their stuff, but good luck to you OP!",
          "score": 6,
          "created_utc": "2026-01-23 21:48:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmoxj2",
      "title": "Looking for RAG Engineer / AI Partner ‚Äî Real Estate + SMB Automation (Paid Contract, Long-Term Potential)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "author": "TheGloomWalker",
      "created_utc": "2026-01-25 17:12:28",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Hey everyone, I‚Äôm building a small AI services company focused on deploying custom RAG-based systems and internal AI tools for small and mid-sized businesses (starting with real estate automation and an industrial services company).\n\nI currently have infrastructure running (servers, cloud resources, deployment environment) and guaranteed business interest, but I‚Äôm looking to bring on a technical partner or contractor who can help design and implement production-grade RAG pipelines.\n\nWhat I‚Äôm building initially:\n\nReal estate automation use cases:\n\n\t‚Ä¢\tScraping + ingesting foreclosure / distressed property listings\n\n\t‚Ä¢\tStructured document ingestion (county data, CSVs, PDFs)\n\n\t‚Ä¢\tSearch + semantic querying over listings and owner data\n\n\t‚Ä¢\tEmail / outreach workflow integration (CRM-style pipelines)\n\nEnterprise pilot project (industrial company):\n\n\t‚Ä¢\tInternal document RAG (finance, operations, SOPs, contracts)\n\n\t‚Ä¢\tSecure knowledge base assistant for staff\n\n\t‚Ä¢\tRole-based access control\n\n\t‚Ä¢\tData isolation + security-first architecture\n\nThis company i‚Äôm working initial contract with is real (50m valuation), has active operations, and is willing to deploy AI internally across departments. Their IT team is security-focused, so experience with data isolation, permissioning, private vector DBs, and secure API practices is important.\n\nWhat I‚Äôm looking for:\n\nSomeone with experience in:\n\n\t‚Ä¢\tRAG pipelines (LangChain, LlamaIndex, custom pipelines, etc.)\n\n\t‚Ä¢\tVector DBs (Pinecone, Weaviate, Qdrant, FAISS, Chroma)\n\n\t‚Ä¢\tEmbeddings + chunking strategies\n\n\t‚Ä¢\tAPI integration\n\n\t‚Ä¢\tAuth / security best practices\n\n\t‚Ä¢\tCloud deployment (Docker, VPS, AWS/GCP/Hetzner/etc.)\n\n\t‚Ä¢\tBonus: web scraping + ETL pipelines\n\nCompensation:\n\n\t‚Ä¢\tPaid contract work (budget available)\n\n\t‚Ä¢\tOpportunity for ongoing partnership if things go well\n\n\t‚Ä¢\tOpen to milestone-based payments\n\nWhat I‚Äôd like to see from you:\n\n\t‚Ä¢\tBrief background / experience\n\n\t‚Ä¢\tAny demos, repos, or projects you‚Äôve built\n\n\t‚Ä¢\tWhat stack you prefer working with\n\n\t‚Ä¢\tAvailability\n\nIf you‚Äôre interested, DM me or reply here and we can talk details.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1nnecp",
          "author": "DeadPukka",
          "text": "Have a look at [Graphlit](https://www.graphlit.com). We do this as a platform and can help with custom services. \n\nNo need to build this yourself in 2026.",
          "score": 2,
          "created_utc": "2026-01-25 17:49:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqnhe",
          "author": "ampancha",
          "text": "The enterprise pilot is where this gets interesting. Role-based access control in RAG isn't a UI toggle; it has to happen at retrieval time, or users can still surface documents they shouldn't see through indirect queries. Their IT team will ask how you verify that isolation actually holds under adversarial prompts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-01-25 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1odwl7",
          "author": "pk13055",
          "text": "Sounds interesting [pk13055.com](https://pk13055.com)",
          "score": 1,
          "created_utc": "2026-01-25 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwvna",
          "author": "radicalpeaceandlove",
          "text": "I am an AI/ML engineer leaving Optum, would be open to chatting",
          "score": 1,
          "created_utc": "2026-01-26 02:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rnp1d",
          "author": "Fear_ltself",
          "text": "I‚Äôd love to give it a shot, I‚Äôve been working exactly with these programs for over 2 years and think we could get something up and running with what you‚Äôre looking for very quickly.",
          "score": 1,
          "created_utc": "2026-01-26 05:26:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqkah",
          "author": "BallinwithPaint",
          "text": "Hey, saw your post. This is right up my alley. I specialize in building the exact kind of autonomous RAG and automation pipelines you're describing. My experience includes projects involving live web scraping for data ingestion and building secure, production-grade agentic systems.\n\n\n\nSending you a DM with my portfolio and more details now.",
          "score": 0,
          "created_utc": "2026-01-25 18:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkjetz",
      "title": "What RAG topics would you actually read a deep-dive on?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkjetz/what_rag_topics_would_you_actually_read_a/",
      "author": "Hungry-Amount-2730",
      "created_utc": "2026-01-23 06:26:47",
      "score": 10,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Hey folks, been diving into RAG implementations lately and wondering what specific areas you'd actually want to see covered in depth? I'm thinking about writing something more technical/methodical but want to make sure it's stuff people actually care about.\n\nAre there any particular challenges or sub-topics around retrieval that you find interesting? Like chunking strategies, hybrid search approaches, reranking methods, that kind of thing? Or maybe semantic code search since that seems to be picking up lately?\n\nJust curious what gaps you see in the current content out there. What would make you actually sit down and read through a longer piece?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qkjetz/what_rag_topics_would_you_actually_read_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17si8c",
          "author": "TannerTot69",
          "text": "Dive into chunking strategies and hybrid search approaches especially in how they handle different types of data. Also reranking methods are something I feel doesn‚Äôt get covered enough, so a detailed breakdown on how to improve retrieval quality with those would be valuable.",
          "score": 7,
          "created_utc": "2026-01-23 10:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17dxec",
          "author": "NoBlueberry6793",
          "text": "check out this discord server \"context engineers\" it's all about reranking, new rag approaches etc..lots of smart & helpful folks\n\nhttps://discord.gg/HxBBDN3Q",
          "score": 5,
          "created_utc": "2026-01-23 08:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17mdzv",
              "author": "Hungry-Amount-2730",
              "text": "thank you very much for valuable info :)",
              "score": 2,
              "created_utc": "2026-01-23 09:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17r95x",
          "author": "monikaTechCuriosity",
          "text": "What about Knowledge Graph?",
          "score": 2,
          "created_utc": "2026-01-23 10:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19oxwd",
              "author": "prodigy_ai",
              "text": "Agree! graph could be a valuable solution",
              "score": 1,
              "created_utc": "2026-01-23 17:03:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1b58jv",
          "author": "patbhakta",
          "text": "RLM + Graph",
          "score": 2,
          "created_utc": "2026-01-23 21:04:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mwdka",
              "author": "Jords13xx",
              "text": "RLM with graph structures sounds intriguing! Would love to see a deep dive on how those two can enhance retrieval accuracy and efficiency. What specific use cases are you thinking about?",
              "score": 1,
              "created_utc": "2026-01-25 15:52:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1b3z6o",
          "author": "drfritz2",
          "text": "Multimodal, hybrid (SQL), parsing, metadata and open source",
          "score": 1,
          "created_utc": "2026-01-23 20:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p9kdy",
              "author": "maigpy",
              "text": "hybrid (SQL) what does that mean",
              "score": 1,
              "created_utc": "2026-01-25 21:59:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xac8x",
                  "author": "drfritz2",
                  "text": "It means that its possible to search SQL and Embeddings \n\nLets say you have tons of documents with the same \"subject\", you can have SQL metadata and then \"filter\" by other criteria do find what you are looking for.",
                  "score": 1,
                  "created_utc": "2026-01-27 00:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gwf28",
          "author": "ajay-c",
          "text": "I need to know more about rag",
          "score": 1,
          "created_utc": "2026-01-24 18:23:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjeq7l",
      "title": "Designing a layout-agnostic PDF table parser for financial statements (Graph RAG use case) ‚Äî how would you approach this?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjeq7l/designing_a_layoutagnostic_pdf_table_parser_for/",
      "author": "Nivedh2004",
      "created_utc": "2026-01-21 23:58:07",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": " I‚Äôm building a document ingestion component for a Graph RAG pipeline. My responsibility is only the document ‚Üí structured facts layer (not embeddings, not retrieval).\n\nThe documents are financial statement PDFs (balance sheets, consolidated statements, etc.), and I‚Äôm running into a fundamental problem that I want architectural opinions on.\n\nThe problem:\n\nThe PDFs contain tables with multi-level column headers, for example:\n\nParent headers like ‚ÄúTHE GROUP‚Äù and ‚ÄúTHE HOLDING COMPANY‚Äù\n\nChild headers like 2024 / 2023 under each parent\n\n Visually, the parent header spans two child columns, but the PDF has no explicit colspan metadata\n\nIssues I‚Äôm seeing consistently:\n\nParent headers get attached to only one year (2023 or 2024)\n\nLeft-aligned row labels are sometimes treated as paragraph text and omitted from the table\n\nPage titles / section headers sometimes get parsed as tables (which I don‚Äôt mind semantically)\n\n Different PDFs of the same domain use different layouts, spacing, alignment, and font styles\n\nI‚Äôve realized that expecting a single tool to output a ‚Äúcorrect table‚Äù is unrealistic, especially when layouts vary.\n\nMy question:\n\nIf you were in my position and had to build something robust to unseen layouts, what would you do?\n\nThanks in advance, interested in how others have approached this.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qjeq7l/designing_a_layoutagnostic_pdf_table_parser_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o18kaof",
          "author": "ScrapeAlchemist",
          "text": "Hi,\n\nI've dealt with this exact problem ‚Äî multi-level headers in financial PDFs where the visual layout has no structural metadata.\n\n**What worked for me:**\n\n1. **Don't try to parse the table directly** ‚Äî convert each page to an image and send it to an LLM with vision (GPT-4o or Claude). Define your exact output schema in the prompt. The model \"sees\" the colspan visually even though the PDF has no explicit metadata for it.\n\n2. **Two-pass approach for accuracy** ‚Äî first pass extracts raw cell values + positions. Second pass uses the visual layout to infer header hierarchy. You prompt the LLM with something like \"which parent header does each year column belong to?\" and it gets it right because it can see the spatial relationship.\n\n3. **Handle layout variation with few-shot examples** ‚Äî include 2-3 different table layouts in your prompt as examples of correct output. The model generalizes to unseen layouts better than any rule-based parser.\n\n4. **For the row labels being dropped** ‚Äî this happens because PDF extractors treat left-aligned text as paragraph content. Vision-based approach sidesteps this entirely since it reads the table as a human would.\n\n**On making it robust to unseen layouts:**\n\nThe key insight is to separate extraction from normalization. Extract what you can see (cells, positions, text) and then normalize into your target schema in a second step. This way when a new layout appears, you only need to adjust the normalization logic, not rebuild the parser.\n\nFor your Graph RAG pipeline specifically ‚Äî output each row as a structured fact with full context: `{entity: \"Total Assets\", value: 1234, year: 2024, group: \"THE GROUP\", source_doc: \"balance_sheet_2024.pdf\", page: 3}`. This makes the graph construction downstream much cleaner.\n\nHope this helps!",
          "score": 2,
          "created_utc": "2026-01-23 13:50:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o190zbi",
              "author": "GP_103",
              "text": "This",
              "score": 1,
              "created_utc": "2026-01-23 15:14:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o19p2sn",
              "author": "Nivedh2004",
              "text": "Doesn't it take too long to process 200-300page pdf using vision model? How to tackle that",
              "score": 1,
              "created_utc": "2026-01-23 17:03:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19pydp",
                  "author": "ScrapeAlchemist",
                  "text": "Not at all, do it in batches, parallel API calls.",
                  "score": 1,
                  "created_utc": "2026-01-23 17:07:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1b8srd",
          "author": "drfritz2",
          "text": "Already tried mineru , docling and paddleOCR?",
          "score": 1,
          "created_utc": "2026-01-23 21:21:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1b96ht",
          "author": "patbhakta",
          "text": "This is a very hard problem... because you have different sources... \nLet's take a simple example of SEC documents... \nSeems simple but documents range from 100-300 pages. \nSure modern LLMs can take all that in but not well at all even though SEC docs are pretty structured because only a handful of companies submit them. \n\nMost of it is verbage that any LLM can handle \nCharts... Ehh good luck getting that data right, same with tables.\n\nLet's say you solved that problem with parsers\n\nNow it's in your database and you might even get a decent graph\\vectors\n\nNow scale that for multiple SEC filings and now you have a dumpster of heavily weighted verbage. Your retrieval is suffering of old data plus multiple filings that's irrelevant to the user here and now. \n\nThis is all assuming your pipeline is set to scale large documents, tables, charts etc... that's hard enough, now you have to face end user or even worse AI having to re-use that data along with real-time data.",
          "score": 1,
          "created_utc": "2026-01-23 21:23:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r4kld",
          "author": "teroknor92",
          "text": "for such complex pdfs you can first try using LLMs and parse the page, extract required data. You can also use APIs from ParseExtract, Llamaparse for this.",
          "score": 1,
          "created_utc": "2026-01-26 03:25:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk5ctc",
      "title": "Building RAG systems: the hard parts aren‚Äôt the models",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qk5ctc/building_rag_systems_the_hard_parts_arent_the/",
      "author": "nuvintaillc",
      "created_utc": "2026-01-22 20:12:04",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Working on RAG systems recently.\n\nThe biggest challenges haven‚Äôt been the models ‚Äî they‚Äôve been:\n\n* structuring data for retrieval\n* managing context windows\n* handling latency and evaluation\n\nFeels like a good example of where AI is moving: away from demos and toward real systems engineering.\n\nCurious how others are handling RAG evaluation in production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qk5ctc/building_rag_systems_the_hard_parts_arent_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o19txbg",
          "author": "Popular_Sand2773",
          "text": "I think the big one that get's overlooked is what I call the CEO test. \n\nWe had spent a year building this application etc etc. Finally launch it and CEOs Son tries to use it and hits an error. Fast forward to everyone being called on in on the weekend to solve this one guy's problem. After that the whole project lost momentum. It worked fine for 99% of people but that didn't matter because the wrong executive had the wrong experience.\n\nAll that to say reliability and consistency. You've got to track and manage drift.",
          "score": 3,
          "created_utc": "2026-01-23 17:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ah0gf",
          "author": "ggone20",
          "text": "Go deep. I was literally just talking about this in another thread. Orchstrators orchestrating orchestrators before any work gets done. Adds slight latency (Cerebras is nice if you can use it), but managing context windows is the name of the game. \n\nOf course that‚Äôs after data ingestion pipes. I recommend doing it multiple ways. For PDFs, for example, we break it into 3 page chunks, have an LLM do semantic chunking of the extracted text, we turn those same 3 page chunks into images and have a vLM create semantic chunks, a few other tricks‚Ä¶ then we use a small model to coalesce them (same with PowerPoints, word docs, architectural/engineering drawings), and then vectorize those semantic chunks after it‚Äôs been been viewed a few different ways along with a set of questions that might be asked of those chunks based on the industry/business/workflows you have‚Ä¶ then graph them‚Ä¶ we also use KBLaM and a few other custom models with dynamic knowledge base loading based on the query (the KBs are built dynamically also)\n\nThis is gold. Complicated gold, but gold nonetheless. Write it down ü§£üôÉ. Otherwise it‚Äôs magic. Works better than anything else we‚Äôve tested. I‚Äôm leaving a lot out but there‚Äôs a lot there also.\n\nDiffMem and MemVid are interesting. RAG at scale is one of the few things in life where more complicated is better. Sigh‚Ä¶ lol so hard.",
          "score": 1,
          "created_utc": "2026-01-23 19:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ug90f",
              "author": "nuvintaillc",
              "text": "\\+1 to the idea that RAG complexity lives outside the model.\n\nIn practice, what‚Äôs worked for us is treating RAG as a *systems problem*, not a retrieval problem:\n\n‚Ä¢ **Multi-path ingestion** (semantic chunks + structural units like sections, tables, references)  \n‚Ä¢ **Query-time KB assembly** instead of static context windows  \n‚Ä¢ **Evaluation tied to user workflows**, not just answer correctness (the ‚ÄúCEO test‚Äù resonates hard)\n\nFor large artifacts (standards, policies, email threads), we‚Äôve found that **reference-aware chunking + lightweight graph traversal** beats naive vector search every time.\n\nThe interesting shift for me: RAG quality improves more from **better data topology and runtime orchestration** than from swapping models. Models are becoming commodities; *context engineering* isn‚Äôt.\n\nCurious if others are measuring success with task completion / decision confidence rather than traditional IR metrics?",
              "score": 2,
              "created_utc": "2026-01-26 16:39:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v9l2z",
                  "author": "ggone20",
                  "text": "You got it!",
                  "score": 1,
                  "created_utc": "2026-01-26 18:44:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1g9p9h",
              "author": "JustSentYourMomHome",
              "text": "I'm trying to build a RAG that ingests ASME code books to help me look things up. This sounds like what I need to be doing.",
              "score": 1,
              "created_utc": "2026-01-24 16:43:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1h33ne",
                  "author": "ggone20",
                  "text": "Oh yea for advanced reasoning over large artifacts like standards and regulatory principles, definitely need multiple paths. I did some extensive work in hydrogen and had to create a field ‚Äòhydrogen expert‚Äô chatbot for construction and engineering teams on the ground to reference things like NFPA2 and others. \n\nKnowledge graphs, at minimum, are needed for multi-hop reasoning. Most standards have tons of cross referencing also. Find a few items, hop around to references, you can get good answers if you handle ingestion correctly.",
                  "score": 2,
                  "created_utc": "2026-01-24 18:51:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1t0ztf",
          "author": "EnoughNinja",
          "text": "The data structuring piece is an issue, especially when you're dealing with email threads where the actual context is scattered.\n\nWe ended up building an API that just handles the email context assembly part (thread reconstruction, role detection, that whole mess). Turns out \\~200ms retrieval with proper citations is possible when you're not fighting with generic RAG pipelines.",
          "score": 1,
          "created_utc": "2026-01-26 12:18:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}