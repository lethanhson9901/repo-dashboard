{
  "metadata": {
    "last_updated": "2026-02-22 02:59:49",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 129,
    "file_size_bytes": 189430
  },
  "items": [
    {
      "id": "1r5e69n",
      "title": "How I Cut RAG Agent Hallucinations in Production for 10,000+ pdfs.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5e69n/how_i_cut_rag_agent_hallucinations_in_production/",
      "author": "AccidentHefty2595",
      "created_utc": "2026-02-15 13:06:07",
      "score": 65,
      "num_comments": 11,
      "upvote_ratio": 0.96,
      "text": "The Problem:  \nAsked about \"max heating capacity of refrigerator\". Got a mixed answer combining V6, V8, and other refrigerator variants.\n\nThe Fix - 4 Simple Approaches:\n\n1. Force Clarification Before Searching Ambiguous query: Agent stops and asks. \"Refrigerator\" has 3 variants V4, V6 and V8 which one do you mean?\n2. No guessing. No assumptions.\n3. Query Decomposition: Break every request into required pieces (eg. sperate queries for heating and cooling capacity). Do parallel execution for each query and combine the unique results of dense and sparse.\n4. Filtering (most effective): Apply filters once the user confirms their product, filter documents BEFORE retrieval. Think \"search only in Building A, Floor 3\" instead of \"search the entire campus.\" Let the agent apply filters dynamically.\n5. Context Pruning: Long conversations hit token limits fast. Prune old search results. Drop the heavy intermediate retrieval data.\n\nThe Result: The agent now asks \"Which one?\" instead of making assumptions.\n\nCode snippet for the tool calling,\n\n    class SearchInput(BaseModel):\n    Â  Â  query: str = Field(description=\"The specific query to search for in the knowledge base., For complex requests, break this down into specific sub-queries.\")\n    Â  Â  target_directory: Optional[str] = Field(\n    Â  Â  Â  Â  description=(\n    Â  Â  Â  Â  Â  Â  \"Crucial for filtering. Apply precise folder paths based on the user's confirmed product category \"\n    Â  Â  Â  Â  Â  Â  \"(e.g., 'v6_idu/ac', 'v8_idu/wall_mounted', 'vrf_odu/side_discharge'). \"\n    Â  Â  Â  Â  Â  Â  \"Only leave empty if the user asks a broad, cross-category comparison question.\"\n    Â  Â  Â  Â  )\n    Â  Â  )\n    \n    @tool(args_schema=SearchInput, response_format=\"content_and_artifact\")\n    async def knowledge_base_search(query: str, target_directory: Optional[str] = None) -> str:\n    Â  Â  \"\"\"\n    Â  Â  Executes a search within the technical documentation. \n    Â  Â  \n    Â  Â  Usage Guidelines:\n    Â  Â  1. **Precision:** Always apply the `target_directory` derived to exclude irrelevant product lines (e.g., filtering out 'V6' when the user asks for 'V8').\n    Â  Â  2. **Iteration:** Call this tool multiple times if the initial search results are missing required data points.\n    Â  Â  3. **Scope:** Returns raw documentation chunks relevant to the query and path.\n    Â  Â  \"\"\"\n\nI structured the product data in folders and sub-folders that represent a hierarchy. Something like this, for other type of data this can be done based on financial years /  companies / authors / product types.\n\n    VRF\n    â”œâ”€â”€ V6 IDU\n        â”œâ”€â”€ AC\n    â”œâ”€â”€ V8 IDU\n        â”œâ”€â”€ DC\n        â”œâ”€â”€ AC\n    â””â”€â”€ VRF ODU\n        â”œâ”€â”€ AC\n        â”œâ”€â”€ V6R\n        â”œâ”€â”€ V8\n        â”‚   â”œâ”€â”€ V8 Master\n        â”‚   â”œâ”€â”€ V8 Pro\n        â””â”€â”€ VC pro\n\nThis structure can be directly given in the system prompt, or if the structure is big then create a new \\`get\\_folder\\_structure\\` tool that uses fuzzy logic to get relevant paths.\n\nNow if we ask, What is the max cooling capacity of V8 IDU?  \nLLM will first ask whether you mean  V8 IDU AC or V8 IDU DC.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5e69n/how_i_cut_rag_agent_hallucinations_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5iiqot",
          "author": "DashboardNight",
          "text": "I agree with query decomposition. Pretty sure this is what Microsoft has been starting to implement in their Search solutions.",
          "score": 3,
          "created_utc": "2026-02-15 14:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j5ax3",
          "author": "TanLine_Knight",
          "text": "How do you enforce query clarification? Like in the fridge example, how would the agent know to ask for specific fridge type?",
          "score": 2,
          "created_utc": "2026-02-15 16:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mhkn3",
              "author": "TenshiS",
              "text": "OPs take only works for a specific type of usecase where you already have a clear mapping of your context or you have enough information on the user himself (perhaps the type of fridge they ordered is known).\n\nAlternatively it requires a traditional retrieval step followed by a clarification step, basically doubling tokens cost but reducing some ambiguity.",
              "score": 1,
              "created_utc": "2026-02-16 03:40:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jcpqp",
              "author": "AccidentHefty2595",
              "text": "Good question, I updated the post, please check it out.",
              "score": 0,
              "created_utc": "2026-02-15 17:15:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n3eem",
          "author": "Comfortable_Onion255",
          "text": "Why not using RLM? I tot that better?",
          "score": 1,
          "created_utc": "2026-02-16 06:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n8e66",
              "author": "AccidentHefty2595",
              "text": "RLM is new for me, i am learning about it",
              "score": 1,
              "created_utc": "2026-02-16 07:10:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5no5re",
          "author": "arul-ql",
          "text": "Guys, thoughts on Recursive Language Models!? It's gaining traction and recently saw Google covering this project on their blog. \n\nHas anyone tried it?",
          "score": 1,
          "created_utc": "2026-02-16 09:39:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5prh2f",
          "author": "ved3py",
          "text": "What's RLM?",
          "score": 1,
          "created_utc": "2026-02-16 17:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iic1z",
          "author": "HauntingIron5623",
          "text": "how many KB instances are you using? I wonder how sparse the source data looks or if you maybe have only technical documentations",
          "score": 0,
          "created_utc": "2026-02-15 14:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jexzv",
              "author": "AccidentHefty2595",
              "text": "I had only pdfs, technical documents with lots of images and complex tables. So one tool was able to handle retrieval. For excel or database query more tools will be required. ",
              "score": 1,
              "created_utc": "2026-02-15 17:26:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r6bwvp",
      "title": "RAG is dead, they said. Agents will take over, they said.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6bwvp/rag_is_dead_they_said_agents_will_take_over_they/",
      "author": "nkmraoAI",
      "created_utc": "2026-02-16 15:02:25",
      "score": 61,
      "num_comments": 31,
      "upvote_ratio": 0.82,
      "text": "RAG is not dead.  \n  \nThe context window ceiling is real. Context rot is real.  \nClaude, GPT-4.1, Gemini â€” all sitting at 1M tokens. Impressive on a slide. Less impressive in production. Research shows that performance degrades as context grows and this effect becomes significant beyond 100k tokens \\[[Context Rot: How Increasing Input Tokens Impacts LLM Performance, Hong, Troynikov & Huber, Chroma, 2025](https://research.trychroma.com/context-rot)\\]  \nI've felt this firsthand. Context rot in my own applications at just 40â€“50k tokens. The ceiling isn't 1M. It's far lower.  \nMeanwhile, enterprise knowledge bases contain billions of tokens. The math doesn't work. You will never stuff your way to an answer.  \n  \nAgents aren't a silver bullet either.  \nProgressive disclosure lets agents traverse massive codebases and document sets, but every hop is a lossy compression. Information degrades. Reasoning quality drops. The growing wave of agentic workflows is itself a signal: frontier model improvements are slowing. The delta between model generations is shrinking. Future gains will come from agentic engineering around models, not from the models themselves.  \nSo if today's LLMs are roughly as good as it gets, how do you leverage massive knowledge bases? RAG. Still.  \nNot the naive RAG of 2023. Smarter retrieval. Better chunking. Hybrid search. Context engineering. Agentic. But the core idea of RAG is more important now than ever. Retrieve what's relevant, discard what isn't.  \n  \nRAG isn't a workaround. It's a non-trivial part of the architecture.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r6bwvp/rag_is_dead_they_said_agents_will_take_over_they/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5p5pej",
          "author": "penguinzb1",
          "text": "the thing about agents that makes this whole debate interesting is that they're basically just orchestrating rag calls anyway. progressive disclosure is rag with more steps. you retrieve, compress, retrieve again based on what you found. it's not some fundamentally different architecture.\n\ni've been building evaluation systems for agents and the pattern you describe about lossy compression is real. each hop degrades the context, but it also lets you cover way more ground than stuffing everything into a single prompt. the question isn't rag vs agents, it's how much information loss you can tolerate for the task. some use cases need exact retrieval, others just need to get close enough.\n\nthe part about model improvements slowing down hits different when you're trying to test agent behavior. we're stuck optimizing retrieval strategies and eval frameworks instead of just waiting for the next model to solve everything. smarter chunking, better reranking, hybrid search like you said. it's all just rag engineering at the end of the day.",
          "score": 15,
          "created_utc": "2026-02-16 15:39:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pbmdj",
              "author": "nkmraoAI",
              "text": "'Progressive disclosure is rag with more steps'. That's an epiphany now. ",
              "score": 1,
              "created_utc": "2026-02-16 16:07:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p1rj3",
          "author": "Technical-History104",
          "text": "Itâ€™s not either/orâ€¦ RAG becomes a part of agentic AI also.",
          "score": 17,
          "created_utc": "2026-02-16 15:20:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p6otk",
              "author": "Hot-Meat-11",
              "text": "While my experience is thin yet, my best results have been building tools that allow the agent to access the RAG.",
              "score": 1,
              "created_utc": "2026-02-16 15:44:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5panv7",
              "author": "nkmraoAI",
              "text": "Not all agentic applications have RAG. None of the agent harnesses come with a built-in vector database or RAG tool. If you give them a bunch of documents, they don't embed them. They simply read them, partially or fully. ",
              "score": -2,
              "created_utc": "2026-02-16 16:02:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5r47b4",
                  "author": "xFloaty",
                  "text": "If an agent does grep, thatâ€™s RAG. No one ever said that RAG implies that embeddings are used for retrieval.",
                  "score": 5,
                  "created_utc": "2026-02-16 21:11:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5pc8lg",
                  "author": "Brighthero",
                  "text": "Not true at all. Cursor has proper semantic search built in: [https://cursor.com/docs/context/semantic-search](https://cursor.com/docs/context/semantic-search)",
                  "score": 4,
                  "created_utc": "2026-02-16 16:10:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ph6pk",
                  "author": "Technical-History104",
                  "text": "Iâ€™m aware.  But my point is that this is not mutually exclusive.  RAG is still very much applicable for certain design patterns and some agentic use cases will require RAG.",
                  "score": 2,
                  "created_utc": "2026-02-16 16:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ozzor",
          "author": "Astroa7m",
          "text": "I agree with 1M context window being valid on papers only. Experimented a use case and models get overwhelmed. My sweet spot was 8% of that million.",
          "score": 5,
          "created_utc": "2026-02-16 15:11:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p0egm",
              "author": "nkmraoAI",
              "text": "In my case, they got overwhelmed at 40-50k tokens of instructions. Failed to follow many of the instructions. ",
              "score": 4,
              "created_utc": "2026-02-16 15:13:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5p92l4",
                  "author": "flonnil",
                  "text": "how the hell do you get to 50k tokens of just *instructions*? thats aprox. \"the lion, the which and the wardrobe\" by c.s. lewis in just *instructions*. one could teach it to build a rocket ship from scratch, fly it to jupiter, recite a poem and fly back with less instructions.",
                  "score": 4,
                  "created_utc": "2026-02-16 15:55:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ruyav",
          "author": "Much-Researcher6135",
          "text": "Sorry, I downvote LLM-generated slop posts",
          "score": 5,
          "created_utc": "2026-02-16 23:28:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t4005",
              "author": "nkmraoAI",
              "text": "Free world. It's not like the LLM gets its own ideas. Its just a specialized employee. It is simply able to articulate it better. Hope you downvote copywritten or ghostwritten content as well.",
              "score": 2,
              "created_utc": "2026-02-17 03:58:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qd5gn",
          "author": "Current-Ticket4214",
          "text": "Todayâ€™s models are not as good as it gets. If you believe that then you havenâ€™t touched Opus 4.6.",
          "score": 2,
          "created_utc": "2026-02-16 19:00:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vudj3",
              "author": "nkmraoAI",
              "text": "How do you even access Claude models outside of direct API these days? I believe you mean 'used Opus 4.6 within Claude Code or some other harness'. Its not the model that's the major improvement, its the harness. If Opus 4.6 was so much better than Opus 4.5, it would have been called Opus 5. ",
              "score": 1,
              "created_utc": "2026-02-17 15:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vx70r",
                  "author": "Current-Ticket4214",
                  "text": "Opus 4.6 shows a significant improvement in both Claude Code AND Windsurf. If the harness was the only differentiator then you would see a massive improvement in Claude Code and none in Windsurf.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:10:18",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6co4sk",
                  "author": "256BitChris",
                  "text": "They released their performance numbers, it's very clear they made huge improvements in 4.6.  big jumps in context window, massive reduction in context rot.\n\nBoth model and Claude code are advancing incredibly fast",
                  "score": 1,
                  "created_utc": "2026-02-20 02:12:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5prlx6",
          "author": "Informal-Resolve-831",
          "text": "Soâ€¦ RAG agent. Whatâ€™s the problem with it?",
          "score": 1,
          "created_utc": "2026-02-16 17:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5riuo3",
          "author": "gopietz",
          "text": "Sometimes it feels like people that post about RAG being either dead or alive are the very same people. People who feel the need to create a debate where there is none.",
          "score": 1,
          "created_utc": "2026-02-16 22:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5snlyh",
          "author": "jeffrey-0711",
          "text": "Retrieval in agent system is essential. RAG becomes agentic RAG. So, I think it is not RAG or Agent, but we need both of them.",
          "score": 1,
          "created_utc": "2026-02-17 02:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5stho5",
          "author": "johnny_5667",
          "text": "didnt even attempt to read the whole post because it reads like LLM slop but RAG and Agents are both solutions to different problems at different levels of the software scale",
          "score": 1,
          "created_utc": "2026-02-17 02:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uabdn",
          "author": "BeatTheMarket30",
          "text": "For a more realistic context window size look at max output token limit.",
          "score": 1,
          "created_utc": "2026-02-17 09:57:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5upuwm",
          "author": "UseMoreBandwith",
          "text": "agents are just doing RAG. But in a even more in-efficient way.",
          "score": 1,
          "created_utc": "2026-02-17 12:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v2ea9",
          "author": "bilbo_was_right",
          "text": "Sorry but itâ€™s not context rot if youâ€™re hitting it at 40k tokens",
          "score": 1,
          "created_utc": "2026-02-17 13:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o693jnv",
          "author": "aquarius-tech",
          "text": "Im building a RAG LLM free, and I see your point",
          "score": 1,
          "created_utc": "2026-02-19 15:10:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ad5nl",
          "author": "LobsterBuffetAllDay",
          "text": "Who's \"they\"? ",
          "score": 1,
          "created_utc": "2026-02-19 18:49:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6cnlq5",
          "author": "256BitChris",
          "text": "The latest anthropic models made some huge progress in reducing context rot, plus quadrupled the output window size.\n\nRag might not be dead but it's probably gonna be more used for searching by agents and the agents will determine what to use for context or not.",
          "score": 1,
          "created_utc": "2026-02-20 02:09:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qojhw",
          "author": "gidea",
          "text": "This looks promising, has anyone tried something like this for RAG? https://compresr.ai",
          "score": 0,
          "created_utc": "2026-02-16 19:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pf9yp",
          "author": "InnerOuterTrueSelf",
          "text": "Ok boomer.",
          "score": -2,
          "created_utc": "2026-02-16 16:23:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r80vsg",
      "title": "Building a RAG for my companyâ€¦ (help me figure it out)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r80vsg/building_a_rag_for_my_company_help_me_figure_it/",
      "author": "Current_Complex7390",
      "created_utc": "2026-02-18 11:48:33",
      "score": 37,
      "num_comments": 45,
      "upvote_ratio": 0.9,
      "text": "Hi All,\n\nI used always to use notebooklm for my work. \n\nbut then it came through my mind why i donâ€™t build one for my own specific need.\n\nSo i started building using Claude, but after 2 weeks of trying it finally worked. It embeds, chunck the pdf files and i can chat with them.\n\nBut the answers are shit. Not sure whyâ€¦.\n\nThe way i built it is using a openotebooklm open source and then i built on top if it, editing alot of stuff.\n\nI use google embedding 004, gemini 2.0 for chatting, i also use Surrelbd.\n\nI am not sure what is the best structure to build one? Should i Start from scratch with a different approach? All i want is a rag model with 4 files (Legal guidance) as its knowledge base and then j upload a project files and then the chat should correlate the project files with the existing knowledge base and give precise answers like Notebooklm.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r80vsg/building_a_rag_for_my_company_help_me_figure_it/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o61uq86",
          "author": "ChapterEquivalent188",
          "text": "garbage in , garbage out. thats it. have a start with this one https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit\n\nyouÄºl fail as long as you dont focus on you ingestion... have a look around in my repos im sure you will find a solution beside the mentioned kit  ;) \n\nits pretty dirty down there where you heading ..... have fun",
          "score": 16,
          "created_utc": "2026-02-18 13:40:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62w2f0",
              "author": "Current_Complex7390",
              "text": "Should i start from scratch or feed this to my current claude project?\n\nI hope this question doesnâ€™t ruin the engagement with me or the post here, but as a disclaimer i am not a technical person, i just have an idea and i want to implement it",
              "score": 2,
              "created_utc": "2026-02-18 16:40:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6361w5",
                  "author": "ChapterEquivalent188",
                  "text": "no worries ;) people stil want to sell a simple pdf warpper/talk2pdf as a produktion ready rag sys ;) \nas long as your company trusts in you to build there RAG im happy to help---- so ,the basics: legal docs have some specialties but plenty of parser meanwhile depending on language (can help with that).  if its only 4 docs i would suggest to give it a try for some learning ( a simple ingest router is included) and have the kit ingest. its easy  and quick done if you spin up docker .... safe some money with you tests and use the local ollama in the repo takes a bit more time sometimes depending on your hardware ----- get your docs prepared and if it has tabels or other stuff in des docs you might use docling or so. btw if you want to connect a agent to your rag try the ClawRAG repo instead including a mcp server for ypur agent or whatever ;) ... so far ...and always have fun",
                  "score": 2,
                  "created_utc": "2026-02-18 17:25:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o635mvj",
              "author": "Current_Complex7390",
              "text": "Also check DMs please",
              "score": 1,
              "created_utc": "2026-02-18 17:23:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o633fwl",
          "author": "playeronex",
          "text": "yeah so the issue is probably your chunking strategy or embedding relevance. with legal docs especially, you're likely splitting mid-sentence or losing context. try increasing chunk size to 512-1024 tokens with 20-30% overlap, and make sure you're using metadata tagging (document type, section headers, dates) so the retriever knows what it's pulling. also double-check your retrieval â€” if you're just doing basic similarity search on top-4 results, you're probably getting noise. throw in reranking (like Cohere's rerank model) between retrieval and generation, and add a prompt that tells Gemini to cite which document it's using when answering. that alone fixes most RAG quality issues.",
          "score": 6,
          "created_utc": "2026-02-18 17:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o634fk6",
              "author": "Current_Complex7390",
              "text": "Can you check dms",
              "score": 1,
              "created_utc": "2026-02-18 17:18:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o61emim",
          "author": "joey2scoops",
          "text": "For that use case, why not stay with notebooks? Give it a custom instruction and you should get decent results. \n\nI have been building my own using this one as a base.\n\nhttps://youtu.be/AUQJ9eeP-Ls?si=NcvuQPwdmpRfZKI3. \n\nhttps://github.com/techwithtim/ProductionGradeRAGPythonApp\n\nI've made a LOT of changes and customisations but the base system just works.",
          "score": 4,
          "created_utc": "2026-02-18 11:59:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62mbtb",
              "author": "ChapterEquivalent188",
              "text": "sorry but whats this ? \"production grade \" ? are we all idiots and we allcan safe our lifetime by just building a (very bad and simpÃ¼le) wrapper call it \"production grade \" and the world is fine ? what the heck",
              "score": 3,
              "created_utc": "2026-02-18 15:56:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68747w",
                  "author": "joey2scoops",
                  "text": "I don't care what the title of the video is, YouTube is all about getting views. Meanwhile, you miss the point. I just used the \"finished product\" as a starting point for my own project.",
                  "score": 1,
                  "created_utc": "2026-02-19 11:56:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o61g8kz",
              "author": "greeny01",
              "text": "github link not working :)",
              "score": 1,
              "created_utc": "2026-02-18 12:11:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61ghz1",
                  "author": "joey2scoops",
                  "text": "Fixed.",
                  "score": 1,
                  "created_utc": "2026-02-18 12:13:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62wve8",
              "author": "Current_Complex7390",
              "text": "Because, i need to make one for my use only, for my company, with my company frontend logo. Also i want it to be based on specific legal documents/ standards. \nI want to customize different interactions point within the front end and customize how it looks and how it interacts\n\nNow for the link above (still need to watch it) \nCan i do that?",
              "score": 1,
              "created_utc": "2026-02-18 16:44:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64ea1t",
                  "author": "remoteinspace",
                  "text": "Try to plugin something like papr.ai to your project. Takes care of the rag and memory infrastructure for you. DM me and I can help you set it up",
                  "score": 2,
                  "created_utc": "2026-02-18 20:46:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61fxle",
          "author": "yafitzdev",
          "text": "Most RAGs are behaving incorrectly because of missing retrieval intelligence and missing epistemic honesty constraints. They hallucinate confidently. I would suggest checking out my Open Source RAG for reference. The retrieval intelligence is the key piece: \n\ngithub.com/yafitzdev/fitz-ai",
          "score": 5,
          "created_utc": "2026-02-18 12:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lcgae",
          "author": "Academic_Track_2765",
          "text": "I have said it before, and I will say it again, learn the process. I swear 80% of the posts in this community are either self promotions/ selling opensource architecture as something new or novel, or not understanding what makes a good Rag. This post is another example, and you might be doing your company a disservice, and should probably hire someone who knows what they are doing.\n\n\"But the answers are shit, Not sure why....\" really? you would think we would have tools / math to answer your question, and you would be right. Look up what precision / recall / MRR / NDCG, and Recall@k / cosine similarity mean\n\nFirst understand which embeddings are good for which tasks. Confused? use this. [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\nThen learn some basic math, understand mathematical distances, there are many, some work well on normalized embeddings, and some don't. Most embeddings are normalized these days so by default they use cosine similarity. confused? read this. [https://arxiv.org/html/2403.05440v1](https://arxiv.org/html/2403.05440v1), or any calculus/ linear algebra book and search cosine similarity.\n\nNext understand clustering, and search spaces. Understand what HNSW (Hierarchical Navigable Small World) is and how it works, what are the parameters and how they effect your search space. Read about (M, ef\\_construction, ef\\_search). This has a huge impact on your result quality\n\nbi-encoder / cross-encoders? if you don't know these terms, time to learn them. confused? read this [https://sbert.net/examples/cross\\_encoder/applications/README.html](https://sbert.net/examples/cross_encoder/applications/README.html)\n\nunderstand how FAISS works, and understand how different vector databases work, do you know what makes weaviate different from azure search vs qdrant vs milvus? if not time to learn that. In some cases there are significant differences that can impact your search results.\n\nAll of these things will answer why your answers are shit.\n\nNext before you embed anything you have to have a good ingestion pipeline. While its true that your pdfs are getting ingested into the RAG, do you understanding how your ingestion happens? If not, you should understand that process. RAG is not a dumping exercise, its a science and an art. PDFs can be very complex, they have images, tables, nested structures, broken/ malformed pdfs, how do you handle all that? I can guarantee you there is not a single best approach that can handle pdfs, you need to make sure that you have multiple ways to extract/ parse your pdf data, if these terms don't make sense to you, you need to understand them - (OCR, Vision Models, JSON structure, chunking, overlapping, chunk strategy, markdown). confused? read this [https://www.docling.ai/](https://www.docling.ai/)  \n[https://docs.langchain.com/oss/python/integrations/document\\_loaders](https://docs.langchain.com/oss/python/integrations/document_loaders)  \nor you can build your own from scratch.\n\nre-ranking stage, if you don't know what a re-ranker is then you should learn that too. check the sbert link I posted above, you can also build it from scratch if you are inclined to do so. It is also OK to build this stage with an LLM based re-ranker, provided that you have some examples that your LLM can learn from or train an LLM from scratch for this task.\n\nNext understanding what your rag actually needs, instead of dumping vectors, understand if your rag needs vector search / semantic search / keyword search / BM25 / Metadata filtering / KG etc., or a combination of all.\n\n**Extra credit:** you get extra credit if you figure out how to reduce prompt injections/ hijacking from nefarious users, which can further impact retrieval quality. Wait till you find that a pdf can contain nefarious instructions that get passed to your LLM at synthesis stage, or users trying embedding space manipulation with their search queries. Maybe add query rewriting? (there are many ways to do it, and your document can itself reword the query to add more relevancy to the search query) \n\nOnce you have all these pieces then tell us how your rag performed, and what you found.\n\nLastly do you understand your data quality, and data structure. (this should also be the first thing you do, and the reason why I didn't mention it earlier is because I assume that you are doing due diligence and at least have trust in your data quality, it would be highly unlikely that you are asking questions about \"how to make a good rag\" from your company documents when your company documents talk about company finances. I honestly think your company needs to a hire a person who does this for a living, and understands how RAG systems work at scale / production level, don't just wing it, and then not understanding why it gives you poor results. This type of negligence puts AI and researchers in a bad light.",
          "score": 3,
          "created_utc": "2026-02-21 12:45:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6ls8hw",
              "author": "Current_Complex7390",
              "text": "Hey man, i really appreciate your comment. \n\nI will jump on to that and try to learn or get a basic understanding for each of the stuff you mentioned above. It seems a lot and complicated, and not my filed of knowledge at all. I am just mechanical engineer that found how this tool or system can improve the industry i am working in. However i understand that if i want to build something of quality and functional this is the way to do it.\n\nCan you check your DMs as well please",
              "score": 2,
              "created_utc": "2026-02-21 14:28:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6lsnef",
                  "author": "Academic_Track_2765",
                  "text": "Dont let the complexity deter you! since you are ME you are probably ahead of many people and it will actually be easier and natural for you to understand the inner workings! yes checking DM. Engineers go far in this field â¤ï¸",
                  "score": 1,
                  "created_utc": "2026-02-21 14:30:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61hqd0",
          "author": "HominidSimilies",
          "text": "Try anythingllm",
          "score": 2,
          "created_utc": "2026-02-18 12:21:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62yg83",
              "author": "Current_Complex7390",
              "text": "I will try it by its own first it works i will just implement it.\n\nHowever i wanted to understand how can i implement the notebooklm precision answer. It is almost perfect for my kind of work i do ( which is asking legal questions based on guidance docs related to architectural projects).",
              "score": 1,
              "created_utc": "2026-02-18 16:51:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o63clxt",
          "author": "pl201",
          "text": "Honestly, every company needs a RAG system these days, but most failed on their first build, so donâ€™t get too discouraged â€” that first failure is also the first step that leads success.\n\nI suggest you take a step back and look closely at your ingestion strategy based on how your documents are structured, because thatâ€™s where a lot of early builds go off track. Youâ€™re in a great spot to dig into why modern RAG isnâ€™t just about coding â€” itâ€™s really about the architecture decisions you make up front.\n\nIâ€™d also recommend reading a couple of good RAG-focused books to level up on patterns and pitfalls, and thereâ€™s one in particular that may be more helpful for your case. It is from a consultant whoâ€™s shipped a lot of real-world RAG projects. Recently published and 14 chapters of real-world fixes from tons of failed projects since early ChatGPT days. PM me for the title.",
          "score": 2,
          "created_utc": "2026-02-18 17:55:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bcba8",
              "author": "pwnakil",
              "text": "Pon el tÃ­tulo de libro aquÃ­, serÃ­a mejor para todos ðŸ˜¬",
              "score": 1,
              "created_utc": "2026-02-19 21:39:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6cdaso",
                  "author": "pl201",
                  "text": "This is the book title I refer to:\nâ€œBeyond the Demo: Building RAG Systems That Actually Workâ€, it is in the ebook format.",
                  "score": 2,
                  "created_utc": "2026-02-20 01:05:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61yp1r",
          "author": "sobrietyincorporated",
          "text": "Why pdf? Just use markdown it naturally produces.",
          "score": 1,
          "created_utc": "2026-02-18 14:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61zvkn",
          "author": "rasbid420",
          "text": "I ran into the exact same issue and realized that doing chunking > embedding > retrieval > context building was too difficult and could never do it right. However I found that chatGPT does a great job if at this if you just feed it the right raw .txt context. So I built for myself this small service where I do quality document extraction and then I upload the resulting .txt file to a chatGPT vector file and then just use a cheap model to take advantage of their RAG technique which is way more advanced than anyone could ever achieve. \n\nPlease do not abuse it and share some feedback if you find it useful! [https://hyperstract.com](https://hyperstract.com)",
          "score": 1,
          "created_utc": "2026-02-18 14:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63df85",
              "author": "Due_Midnight9580",
              "text": "How are you extracting quality documents?\nðŸ™‚",
              "score": 1,
              "created_utc": "2026-02-18 17:58:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63du89",
                  "author": "rasbid420",
                  "text": "i have 800 GPUs each running a small qwen3 8B visual model and each of them is in charge of visually describing a page. then at the end when they're all done, I concatenate the results and provide a single unitary .txt file containing all the descriptions. i have a detailed explanation of my system posted in localllama here\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3\\_repurposing\\_800\\_rx\\_580s\\_converted\\_to\\_ai/](https://www.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/)",
                  "score": 3,
                  "created_utc": "2026-02-18 18:00:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62359r",
          "author": "jingamayne",
          "text": "I just built one using gemini file search for my workplace. Works really well.",
          "score": 1,
          "created_utc": "2026-02-18 14:24:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63wjvm",
          "author": "TechnicalGeologist99",
          "text": "What you've made is called semantic or dense vector search it's one of about 50 possible bells and whistles you could add to your RAG pipeline. \n\nOurs has denser, sparse, rerank, tag extraction, sub query generation, intent extraction, documents are hierarchical first class objects in our domain, chunks that are retrieved are combined with heirarchical information to produce a fully contextual citation, the list goes on and on. But each single thing addresses a specific problem that stop us realising a higher recall at query time.\n\nBut note that this is an application you are building. The same design rules apply. Use YAGNI, justify things before you implement them. Find out how to even justify things in the first place. \n\nUse Claude to learn the patterns that occur in RAG systems. \n\nHere's an easy win, add sparse search, RRF, and reranking and see how that changes responses",
          "score": 1,
          "created_utc": "2026-02-18 19:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65y7ta",
          "author": "lfnovo",
          "text": "Hey  u/op, Open-Notebook creator here. Per my experience, RAG issues are, in most cases, related to how you structure your docs/chunks/embeddings. Just doing simple character based chunking is not enough for many projects. It really does depend on what you are trying to do: \n\nExample: if you embed all Epstein files and ask for the names of everybody in there, you won't get any good responses. \"What are the names of people here\", when converted to embeddings will not make any useful matches.\n\nI had a project where the best option was to 'summarize' not chunk. For investigative journalism.\n\nAnother project, where only entity extraction/graph RAG would do it.  (like in the Epstein example).\n\nin some cases, you get lucky which agentic RAG and offering some tools, not just vector search. \n\nIf you share more about what you are building, I can give you some tips. Specially: what type of content are you ingesting and what type of question are people most likely to ask. \n\nCheers",
          "score": 1,
          "created_utc": "2026-02-19 01:32:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6af1t9",
              "author": "jayn35",
              "text": "Thanks for this. I'm looking at implementing Graphrag FalkorDB hybrid vector/DB + GraphRAG SDK for an advanced high-stakes marketing education DB where agents must get connected nodes and complex responses back for it to do complex work on things like writing landing pages and knowing how to do it properly, but to make it work well, apparently i must create an excellent manual ontology, which seems super complicated to figure out for 800 hours of transcripts across the entire field of marketing.   \n  \nAs the open-notebook creator, I assume you are an expert on these things? should i struggle on and go down this Graprag path and figure it out (is there any good way to sue AI for ontology discovery), or are there better solutions to get cutting-edge or at least very high-quality retrievals, or as a beginner am i likely to mess it up, being very complicated even for pros? I never see graphrag mentioned much, and it worries me about why that is, but you did, so that's good.... are there better ways to get excellent results for my use case that might be easier? Appreciate the advice!\n\nPS: I've also heard about agents doing well with file system knowledge retrievals (perfect for the MD file obsidian vault, which has my marketing research also), with certain local tools being very good at identifying where the stuff is you are looking for in your 3000-note research vault, and it can find the content, tell your agent what files it needs fast, and the agent can then only ingest the best stuff to give you a great answer. No idea about the tech, but apparently it's more accurate than most rag but possibly slower?\n\nPS2: would love to see that Eps7ein DB; i was thinking of graphing it myself for a fun project, or maybe that's a dangerous project...",
              "score": 1,
              "created_utc": "2026-02-19 18:58:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o67xfvj",
          "author": "SharpRule4025",
          "text": "For legal documents the extraction quality matters way more than the model or chunking strategy. If your PDFs are getting converted to text with headers, footers, page numbers, and formatting artifacts mixed into the content, your embeddings are going to be noisy. Retrieval will return chunks that scored high because of shared boilerplate, not because they contain the answer.\n\nBefore touching the RAG config I'd check what the actual text looks like after extraction. Print out 10 chunks and read them yourself. If you see page numbers in the middle of sentences, or section references that got split from their content, that's your problem right there. No amount of model tuning fixes bad source data.\n\nFor legal docs specifically, keeping the section hierarchy intact during extraction makes a huge difference. A clause that says \"subject to section 4.2\" needs section 4.2 to be retrievable as a unit, not split across three chunks.",
          "score": 1,
          "created_utc": "2026-02-19 10:34:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a07kg",
          "author": "FarQuail277",
          "text": "Given you're non technical and seem to have pretty robust needs, have you explored licensed RAG as a service products? I don't know what your company does but might be useful if you're looking to ship products faster/ have more front end feature development experience and are open to outsourcing your backend retrieval infra. Feel free to DM me to discuss a lil more as well. ",
          "score": 1,
          "created_utc": "2026-02-19 17:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i1dgg",
              "author": "Current_Complex7390",
              "text": "Hi, Please check DMs",
              "score": 1,
              "created_utc": "2026-02-20 22:01:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6b2kcj",
          "author": "yard1234",
          "text": "You are probably losing context between chunks, try contextual chunking\nsee Anthropic's article here:\n[link](https://www.anthropic.com/engineering/contextual-retrieval)",
          "score": 1,
          "created_utc": "2026-02-19 20:52:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6flc5e",
          "author": "Positive-Bell-9675",
          "text": "Rag seems like overkill for 4 docs. Agent plus a sql database performs very well.",
          "score": 1,
          "created_utc": "2026-02-20 15:05:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kk1q6",
          "author": "avebrahimi",
          "text": "I had similar problem with legal text, mixed of two languages even, after testing 6 or 7 online services, and trying 3 coding libraries, I finally built my tailormoade custom solution and I found these point very very important:  \n\\- clean your text before anything\n\n\\- separate contexts: observe the hierarchy, paragraphs, data structure, overlaps, segments\n\n\\- if data volume is not high, try semantic chunking mixed with recursive or even fixed.\n\n\\- add tags and titles to chunks ( there is a secret trick here)\n\n\\- try to add auto complete to search, using smart tags and chunks, it made my users so happy!\n\nLet me know if you need more info.",
          "score": 1,
          "created_utc": "2026-02-21 08:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6kqtbx",
          "author": "ampancha",
          "text": "\"Answers are shit\" with legal docs usually means retrieval is returning semantically similar but contextually wrong chunks. Before rebuilding, check your chunk boundaries: legal documents need section-aware splitting, not fixed token windows, and your top-k is probably too greedy. The harder problem you'll hit next is verifying accuracy before this touches real decisions; legal RAG without hallucination detection or citation grounding is a liability. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-21 09:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63fmk1",
          "author": "primoco",
          "text": "Hey, Iâ€™m working on a similar setup for legal and enterprise docs. I started with a \"Community\" approach too, and I faced exactly the same frustration: great LLM (Gemini 2.0), great embeddings (Google 004), but \"shit\" answers.\n\nThe problem isn't your stack; Gemini and SurrealDB are solid. The issue is usually how the information is \"orchestrated\" before reaching the LLM. In my experience, to make a RAG work with legal and project files, you have to move away from the standard \"out-of-the-box\" approach.\n\nHere are the 3 main issues I had to solve to get precise answers:\n\nThe Chunking Trap: Standard fixed-size chunking (like splitting every 500-1000 tokens) is a disaster for legal docs. If a clause or an \"if/then\" condition is split in half, the LLM loses the logic. Are you using a simple splitter or a recursive one?\n\nMetadata vs. Pure Vector: For legal stuff, simple semantic search is too \"fuzzy.\" I found that I had to extract metadata (dates, entities, specific article numbers) first and use them to \"anchor\" the search. Without structured metadata, the LLM starts hallucinating connections that aren't there.\n\nContext Injection: Legal files should be treated as the \"Ground Truth.\" I had to tweak my prompt and retrieval to make sure the legal guidance acts as a hard constraint for the project files.\n\nTo give you a hand, what are your current parameters?\n\nWhat Chunk Size and Overlap are you using? (This is usually the #1 culprit)\n\nHow many chunks (Top-K) are you feeding to Gemini for each query?\n\nAre you using any kind of Reranker or just raw vector search?\n\nDon't scrap it yet. Usually, a \"shit\" RAG is just a RAG that needs better data orchestration, not a different LLM.",
          "score": 1,
          "created_utc": "2026-02-18 18:08:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6i0v17",
              "author": "Current_Complex7390",
              "text": "please check dms ",
              "score": 1,
              "created_utc": "2026-02-20 21:58:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o62mx2m",
          "author": "ChapterEquivalent188",
          "text": "unbelievable how much bs here is in the comments, just wait till somebody says \"use a bigger model and you are fine\" ...... \"why pdf, use markdown\" \n\nby today im confident LLM will destroy mankind !",
          "score": 0,
          "created_utc": "2026-02-18 15:59:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6f8a8",
      "title": "Essential Concepts for Retrieval-Augmented Generation (RAG)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6f8a8/essential_concepts_for_retrievalaugmented/",
      "author": "pgEdge_Postgres",
      "created_utc": "2026-02-16 17:03:56",
      "score": 34,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "Some helpful insights from one of our senior software engineers, Muhammad Imtiaz.\n\n# Introduction\n\nRetrieval-Augmented Generation (RAG) represents a paradigm shift in how artificial intelligence systems access and utilize information. By combining the generative capabilities of large language models with dynamic information retrieval from external knowledge bases, RAG systems overcome the fundamental limitations of standalone language modelsâ€”namely, their reliance on static training data and tendency toward hallucination.\n\nThis document provides a comprehensive technical reference covering the essential concepts, components, and implementation patterns that form the foundation of modern RAG architectures. Each concept is presented with clear explanations, practical code examples in Go, and real-world considerations for building production-grade systems.\n\nWhether you are architecting a new RAG system, optimizing an existing implementation, or seeking to understand the theoretical underpinnings of retrieval-augmented approaches, this reference provides the knowledge necessary to build accurate, efficient, and trustworthy AI applications. The concepts range from fundamental building blocks like embeddings and vector databases to advanced techniques such as hybrid search, re-ranking, and agentic RAG architectures.\n\nAs the field of artificial intelligence continues to evolve, RAG remains at the forefront of practical AI deployment, enabling systems that are both powerful and grounded in verifiable information.\n\n# Core Concepts and Implementation Patterns\n\n**Generator (Language Model)**\n\nThe component that generates the final answer using the retrieved context.\n\n**Retrieval**\n\nRetrieval is the process of identifying and extracting relevant information from a knowledge base before generating a response. It acts as the AIâ€™s research phase, gathering necessary context from available documents before answering.\n\nRather than relying solely on pre-trained knowledge, retrieval enables the AI to access up-to-date, domain-specific information from documents, databases, or other knowledge sources.\n\nIn the example below, the retriever selects the top five most relevant documents and provides them to the LLM to generate the final answer.\n\n`relevantDocs := vectorDB.Search(query, 5) // top_k=5`  \n`answer := llm.Generate(query, relevantDocs)`\n\n**Embeddings**\n\nEmbeddings are numerical representations of text that capture semantic meaning. They convert words, sentences, or documents into dense vectors that preserve context and relationships.\n\nThe example below demonstrates how to generate embeddings using the OpenAI API.\n\n`import (`  \n`\"context\"`  \n`\"github.com/sashabaranov/go-openai\"`  \n`)`  \n  \n`client := openai.NewClient(\"your-token\")`  \n`resp, err := client.CreateEmbeddings(`  \n`context.Background(),`  \n`openai.EmbeddingRequest{`  \n`Input: []string{\"Retrieval-Augmented Generation\"},`  \n`Model: openai.SmallEmbedding3,`  \n`},`  \n`)`  \n`if err != nil {`  \n`log.Fatal(err)`  \n`}`  \n`vector := resp.Data[0].Embedding`\n\n# Vector Databases\n\nVector databases are specialized systems designed to store and query high-dimensional embeddings. Unlike traditional databases that rely on exact matches, they use distance metrics to identify semantically similar content.\n\nThey support fast similarity searches across millions of documents in milliseconds, making them essential for scalable RAG systems.\n\nThe example below shows how to create a collection and add documents with embeddings using the Chroma client.\n\n`import \"github.com/chroma-core/chroma-go\"`  \n  \n`client := chroma.NewClient()`  \n`collection, _ := client.CreateCollection(\"docs\")`  \n  \n`// Generate embeddings for documents`  \n`docs := []string{\"RAG improves accuracy\", \"LLMs can hallucinate\"}`  \n`emb1 := embedder.Embed(docs[0])`  \n`emb2 := embedder.Embed(docs[1])`  \n  \n`// Add documents with their embeddings`  \n`collection.Add(`  \n`context.Background(),`  \n`chroma.WithIDs([]string{\"doc1\", \"doc2\"}),`  \n`chroma.WithEmbeddings([][]float32{emb1, emb2}),`  \n`chroma.WithDocuments(docs),`  \n`)`\n\n# Retriever\n\nA retriever is a component that manages the retrieval process. It converts a user query into an embedding, searches the vector database, and returns the most relevant document chunks.\n\nIt functions like a smart librarian, understanding the query and locating the most relevant information within a large collection.\n\nThe example below demonstrates a basic retriever implementation.\n\n`type Retriever struct {`  \n`VectorDB VectorDB`  \n`}`  \n  \n`func (r *Retriever) Retrieve(query string, topK int) []Result {`  \n`queryVector := Embed(query)`  \n`return r.VectorDB.Search(queryVector, topK)`  \n`}`\n\n# Chunking\n\nChunking is the process of dividing large documents into smaller, manageable segments called â€œchunks.â€ Effective chunking preserves semantic meaning while ensuring content fits within model context limits.\n\nProper chunking is essential, as it directly affects retrieval quality. Well-structured chunks improve precision and support more accurate responses.\n\nThe example below demonstrates a character-based chunking function with overlap support.\n\n`func ChunkText(text string, chunkSize, overlap int) []string {`  \n`var chunks []string`  \n`runes := []rune(text)`  \n`for start := 0; start < len(runes); start += (chunkSize - overlap) {`  \n`end := start + chunkSize`  \n`if end > len(runes) {`  \n`end = len(runes)`  \n`}`  \n`chunks = append(chunks, string(runes[start:end]))`  \n  \n`if end >= len(runes) {`  \n`break`  \n`}`  \n`}`  \n`return chunks`  \n`}`  \n  \n`chunks := ChunkText(document, 500, 50)`\n\n# Context Window\n\nThe context window is the maximum number of tokens (words or subwords) an LLM can process in a single request. It defines the modelâ€™s working memory and the amount of context that can be included.\n\nContext windows range from 4K tokens in older models to over 200K in modern ones. Retrieved chunks must fit within this limit, making chunk size and selection critical.\n\nThe example below demonstrates how to fit chunks within a token limit.\n\n`func FitContext(chunks []string, maxTokens int) []string {`  \n`var context []string`  \n`tokenCount := 0`  \n  \n`for _, chunk := range chunks {`  \n`chunkTokens := CountTokens(chunk)`  \n`if tokenCount + chunkTokens > maxTokens {`  \n`break`  \n`}`  \n`context = append(context, chunk)`  \n`tokenCount += chunkTokens`  \n`}`  \n  \n`return context`  \n`}`\n\n# Grounding\n\nGrounding ensures AI responses are based on retrieved, verifiable sources rather than hallucinated information. It keeps the model anchored to real data.\n\nEffective grounding requires citing specific sources and relying only on the provided context to support claims. This reduces hallucinations and improves trustworthiness.\n\nThe example below demonstrates a grounding prompt template.\n\n`prompt := fmt.Sprintf(\\``  \n`Answer the question using ONLY the provided context.`  \n`Cite the source for each claim.`  \n`Context: %s`  \n  \n`Question: %s`  \n  \n`Answer with citations:`  \n`\\`, retrievedDocs, userQuestion)`  \n  \n`response := llm.Generate(prompt)`\n\n# Re-Ranking\n\nTwo-stage retrieval enhances result quality by combining speed and precision. First, a fast initial search retrieves many candidates (e.g., top 100). Then, a more accurate cross-encoder model re-ranks them to identify the best matches.\n\nThis approach pairs broad retrieval with fine-grained scoring for optimal results.\n\nThe example below demonstrates a basic re-ranking workflow.\n\n`// Initial fast retrieval`  \n`candidates := retriever.Search(query, 100)`  \n  \n`// Re-rank using a CrossEncoder`  \n`scores := reranker.Predict(query, candidates)`  \n  \n`// Sort candidates by score and take top 5`  \n`topDocs := SortByScore(candidates, scores)[:5]`\n\n# Hybrid Search\n\nHybrid search combines keyword-based search (BM25) with semantic vector search. It leverages both exact term matching and meaning-based similarity to improve retrieval accuracy.\n\nBy blending keyword and semantic scores, it provides the precision of exact matches along with the flexibility of understanding conceptual queries.\n\nThe example below demonstrates a hybrid search implementation.\n\n`func HybridSearch(query string, alpha float64) []Result {`  \n`keywordResults := BM25Search(query)`  \n`semanticResults := VectorSearch(query)`  \n  \n`// Combine scores:`  \n`// finalScore = alpha * keywordScore + (1-alpha) * semanticScore`  \n`finalResults := CombineAndRank(keywordResults, semanticResults, alpha)`  \n  \n`return finalResults[:5]`  \n`}`\n\n# Metadata Filtering\n\nMetadata filtering narrows search results by using document attributes such as dates, authors, types, or departments before performing a semantic search. This reduces noise and improves precision.\n\nApplying filters like author: John Doe or document\\_type: report focuses the search on the most relevant documents.\n\nThe example below demonstrates metadata filtering in a vector database query.\n\n`results := collection.Query(`  \n`Query{`  \n`Texts: []string{\"quarterly revenue\"},`  \n`TopK: 10,`  \n`Where: map[string]interface{}{`  \n`\"year\": Â  Â  Â  2024,`  \n`\"department\": \"sales\",`  \n`\"type\": map[string]interface{}{`  \n`\"$in\": []string{\"report\", \"presentation\"},`  \n`},`  \n`},`  \n`},`  \n`)`\n\n# Similarity Search\n\nThe retriever is the core search mechanism in RAG, identifying documents whose embeddings are most similar to a queryâ€™s embedding. It evaluates semantic closeness rather than just keyword matches.\n\nSimilarity is typically measured using cosine similarity (angle between vectors) or dot product, with higher scores indicating more relevant content.\n\nThe example below demonstrates cosine similarity using the Gonum library.\n\n`import (`  \n`\"gonum.org/v1/gonum/mat\"`  \n`)`  \n  \n`func CosineSimilarity(vec1, vec2 []float64) float64 {`  \n`v1 := mat.NewVecDense(len(vec1), vec1)`  \n`v2 := mat.NewVecDense(len(vec2), vec2)`  \n  \n`dotProduct := mat.Dot(v1, v2)`  \n`norm1 := mat.Norm(v1, 2)`  \n`norm2 := mat.Norm(v2, 2)`  \n  \n`return dotProduct / (norm1 * norm2)`  \n`}`  \n  \n`// Usage example`  \n`queryVec := Embed(query)`  \n`for _, docVec := range documentVectors {`  \n`score := CosineSimilarity(queryVec, docVec)`  \n`// Store score for ranking`  \n`}`\n\n# Prompt Injection\n\nPrompt injection is a security vulnerability where malicious users embed instructions in queries to manipulate AI behavior. Attackers may attempt to override system prompts or extract sensitive information.\n\nCommon examples include phrases like â€œignore previous instructionsâ€ or â€œreveal your system prompt.â€ RAG systems must sanitize inputs to prevent such attacks.\n\nThe example below demonstrates a basic input sanitization function. In production, multiple defensesâ€”such as regex patterns, semantic similarity checks, and output validationâ€”are required.\n\n`func SanitizeInput(userInput string) (string, error) {`  \n`// Basic pattern matching - extend with regex for production use`  \n`dangerousPatterns := []string{`  \n`\"ignore previous instructions\",`  \n`\"disregard system prompt\",`  \n`\"reveal your instructions\",`  \n`\"ignore all prior\",`  \n`\"bypass security\",`  \n`}`  \n  \n`lowerInput := strings.ToLower(userInput)`  \n`for _, pattern := range dangerousPatterns {`  \n`if strings.Contains(lowerInput, pattern) {`  \n`return \"\", errors.New(\"invalid input detected\")`  \n`}`  \n`}`  \n  \n`// Additional checks for production:`  \n`// - Regex for obfuscated patterns (e.g., \"ign0re\")`  \n`// - Semantic similarity to known attack phrases`  \n`// - Length and character validation`  \n  \n`return userInput, nil`  \n`}`\n\n# Hallucination\n\nGenerative AI can produce convincing but incorrect information, including false facts, fake citations, or invented details.\n\nRAG helps reduce hallucinations by grounding responses in retrieved documents, though proper grounding and citation are essential to minimize risk.\n\nThe example below demonstrates a verification function that checks whether a response is supported by source documents. For higher reliability, consider using Natural Language InferenceÂ  models or extractive fact-checking, as relying on one LLM to verify another has limitations.\n\n`func IsSupported(response, sourceDocs string) bool {`  \n`verificationPrompt := fmt.Sprintf(\\``  \n`Response: %s`  \n`Source: %s`  \n  \n`Is this response fully supported by the source documents?`  \n`Answer yes or no.`  \n`\\`, response, sourceDocs)`  \n  \n`result := llm.Generate(verificationPrompt)`  \n`return strings.ToLower(strings.TrimSpace(result)) == \"yes\"`  \n`}`  \n  \n`// Alternative: Use NLI model for more reliable verification`  \n`func IsSupportedNLI(response, sourceDocs string) bool {`  \n`// NLI models classify as: entailment, contradiction, or neutral`  \n`result := nliModel.Predict(sourceDocs, response)`  \n`return result.Label == \"entailment\" && result.Score > 0.8`  \n`}`\n\n# Agentic RAG\n\nAgentic RAG is an advanced architecture where the AI actively plans, reasons, and controls its own retrieval strategy. Rather than performing a single search, the agent can conduct multiple searches, analyze results, and iterate.\n\nIt autonomously decides what information to retrieve, when to search again, which tools to use, and how to synthesize multiple sourcesâ€”enabling complex, multi-step reasoning.\n\nThe example below demonstrates an agentic RAG implementation.\n\n`func (a *AgenticRAG) Answer(query string) string {`  \n`plan := a.llm.CreatePlan(query)`  \n  \n`for _, step := range plan.Steps {`  \n`switch step.Action {`  \n`case \"search\":`  \n`results := a.retriever.Search(step.Query)`  \n`a.context.Add(results)`  \n`case \"reason\":`  \n`analysis := a.llm.Analyze(a.context)`  \n`a.context.Add(analysis)`  \n`}`  \n`}`  \n  \n`return a.llm.Synthesize(a.context)`  \n`}`\n\n# Latency\n\nRAG latency is the total time from a user query to the final response, including embedding generation, vector search, re-ranking (if used), and LLM generation. Each step contributes to the delay.\n\nLatency directly impacts user experience and can be optimized by caching embeddings, using faster models, narrowing search scope, and parallelizing operations. Typical RAG systems aim for sub-second to a few seconds of latency.\n\nThe example below measures latency for each stage of the RAG pipeline.\n\n`import \"time\"`  \n  \n`func MeasureLatency(query string) {`  \n`start := time.Now()`  \n  \n`// Step 1: Embed query`  \n`embedding := Embed(query)`  \n`t1 := time.Now()`  \n  \n`// Step 2: Search`  \n`results := vectorDB.Search(embedding)`  \n`t2 := time.Now()`  \n  \n`// Step 3: Generate`  \n`response := llm.Generate(query, results)`  \n`t3 := time.Now()`  \n  \n`fmt.Printf(\"Embed: %v | Search: %v | Generate: %v\\n\",`  \n`t1.Sub(start), t2.Sub(t1), t3.Sub(t2))`  \n`}`\n\n  \nHope this all helps!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r6f8a8/essential_concepts_for_retrievalaugmented/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5ptoct",
          "author": "Intrepid-Scale2052",
          "text": "Sick!",
          "score": 3,
          "created_utc": "2026-02-16 17:30:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61cy07",
          "author": "gowtham150",
          "text": "Very refreshing to see golang finally. Hate to see python everywhere. Very well documented post with enterprise stuff.",
          "score": 1,
          "created_utc": "2026-02-18 11:47:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r90y1p",
      "title": "Built a Document AI that now extracts structured data (thanks to beta feedback)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r90y1p/built_a_document_ai_that_now_extracts_structured/",
      "author": "proxima_centauri05",
      "created_utc": "2026-02-19 14:41:20",
      "score": 33,
      "num_comments": 10,
      "upvote_ratio": 0.94,
      "text": "Iâ€™ve been building a product called [TalkingDocuments](https://talkingdocuments.com), it lets you work with documents using AI instead of manually digging through them.\n\nOne thing that kept coming up from beta users (thanks to this sub, was able to get some genuine beta testers) was - â€œRAG Chat is useful, but I need structured data I can actually use.â€\n\nSo I added Data Extraction\n\nInstead of building a completely separate pipeline, I was able to reuse the same underlying infrastructure that already powers the RAG-based chat, the parsing, chunking, embeddings, and retrieval layers were already there. The main work was making the outputs more deterministic and structured (fields, tables, clean exports) rather than conversational.\n\nThe result is that you can now pull usable data from PDFs and long documents without manually hunting through them or post-processing chat responses.\n\nHuge thanks to the beta users who tested early versions and gave thoughtful, honest feedback. This feature exists largely because people were clear about what wasnâ€™t working and what would actually make the product useful.\n\nStill early, but itâ€™s moving in a much more practical direction.\n\nIf you deal with document-heavy workflows and care about reliable, structured outputs. Iâ€™d love more feedback.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r90y1p/built_a_document_ai_that_now_extracts_structured/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6dll8t",
          "author": "Khade_G",
          "text": "Love this direction. A lot of teams realize too late that â€œRAG chat worksâ€ â‰  â€œdata is usable.â€ Structured extraction is where products become operational.\n\nCurious how youâ€™re handling:\n- Schema drift across different document layouts\n- Missing/ambiguous fields\n- Multi-table PDFs\n- Cross-page references\n- Low-quality scans / OCR noise\n\nIn our experience, deterministic outputs break not because the model fails, but because document structure entropy explodes once you leave clean PDFs.\n\nAre you validating against a labeled document stress-test set yet, or mostly iterating from beta feedback?\n\nWould be interesting to hear how youâ€™re thinking about extraction robustness as volume scales.",
          "score": 3,
          "created_utc": "2026-02-20 06:05:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6e1ewm",
              "author": "proxima_centauri05",
              "text": "Fully agree. The main challenge is document structure, not the model.\n\nThe RAG pipeline is not linear. Extraction runs as an agentic workflow where retrieval, validation, and re-checks happen iteratively based on schema needs. So that helps a little.\n\nSchema drift is handled by keeping schemas intent-based rather than layout-dependent. Missing or ambiguous fields are allowed instead of forcing guesses. Multi-table PDFs are handled as independent units. Cross-page references are resolved by expanding retrieval when required. But still,  no matter how robust we make it, there're always some outlier PDFs which don't properly fit into this. \n\nOCR is supported for scanned PDFs and images and works well, with quality depending on input. OCR noise reduction is a problem we're focusing on now. I'm trying to involve LLMs as little as possible to make this more robust and generalised and to reduce token usage ofcourse. Also actively working on improving OCR for handwritten documents. Many teams asked specifically if they can use this on legacy handwritten documents, especially in the manufacturing domain.",
              "score": 1,
              "created_utc": "2026-02-20 08:29:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6ixytk",
                  "author": "Khade_G",
                  "text": "Intent-based schemas instead of layout-dependent ones is the right move. Thatâ€™s usually where teams escape brittle template logic.\n\nWhere weâ€™ve seen systems start breaking at scale isnâ€™t in normal schema drift, itâ€™s in entropy stacking:\nSo for example:\n- OCR noise + table fragmentation\n- Cross-page references + partial retrieval windows\n- Handwritten sections embedded in typed forms\n- Similar fields with subtly different semantics (â€œOrder Dateâ€ vs â€œInvoice Dateâ€)\n- Tables that change structure mid-document\n\nIndividually manageable. Together, they compound.\nThe teams that harden this layer well usually build a structured document stress corpus across entropy gradients:\n- Clean digital PDFs\n- Semi-structured exports\n- Table-dense multi-page financial docs\n- Low-res scanned contracts\n- OCR-heavy manufacturing forms\n- Handwritten annotations layered over structured layouts\n\nThen they version-test extraction fidelity across those buckets.\n\nEspecially if youâ€™re minimizing LLM reliance, that corpus becomes your real regression harness.\n\nIf youâ€™re moving into legacy + handwritten domains, thatâ€™s exactly where a coverage-controlled dataset saves months of reactive debugging.\n\nHappy to compare notes if youâ€™re formalizing that layer, this is the kind of system that benefits massively from deliberate entropy coverage early.",
                  "score": 2,
                  "created_utc": "2026-02-21 01:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6gu10k",
          "author": "ForsakenInternal5579",
          "text": "Nice work on the structured data pivot thatâ€™s where the real utility is. For the OCR layer, Iâ€™ve been using Qoestâ€™s API to handle the initial text extraction from PDFs and images; itâ€™s solid for getting clean, structured text into JSON before any LLM processing. Helps cut down on noise and keeps token costs predictable",
          "score": 2,
          "created_utc": "2026-02-20 18:31:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6alrwx",
          "author": "Due_Midnight9580",
          "text": "Does it include image based pdf",
          "score": 1,
          "created_utc": "2026-02-19 19:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6bcmlj",
              "author": "proxima_centauri05",
              "text": "Yes. For PDFs with images (Scanned ones), OCR is supported.",
              "score": 1,
              "created_utc": "2026-02-19 21:41:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o699fun",
          "author": "After_Awareness_655",
          "text": "Smart move reusing RAG for structured extraction... beta feedback paying off big time! No more PDF treasure hunts like a frantic pirate; this is the data goldmine we needed. ðŸ˜‚ How does it handle super messy tables?",
          "score": 1,
          "created_utc": "2026-02-19 15:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o69gdnp",
              "author": "proxima_centauri05",
              "text": "Thanks! Messy tables are the hardest part fr.\n\nI preserve the layout during parsing. Really ugly PDFs are still hit-or-miss, but it works well when the table structure is even moderately consistent. Actively improving this with real-world samples.",
              "score": 1,
              "created_utc": "2026-02-19 16:13:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r62t5f",
      "title": "bb25 (Bayesian BM25) v0.2.0 is out",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r62t5f/bb25_bayesian_bm25_v020_is_out/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-16 07:04:32",
      "score": 29,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "bb25 v0.2.0 is out â€” a Python + Rust implementation of Bayesian BM25 that turns search scores into calibrated probabilities.\n\n[https://github.com/instructkr/bb25](https://github.com/instructkr/bb25)\n\nA week ago, I built bb25 that turns BM25 into a probability engine! In addition to the Rust-based implementation, the paper's author shipped his own implementation. Comparing the two taught me more than the paper itself.\n\nThe Bayesian BM25 paper does something elegant, in that applying Bayes' theorem to BM25 scores so they become real probabilities, not arbitrary numbers. This makes hybrid search fusion mathematically principled instead of heuristic.\n\nInstruct.KR's bb25 took a ground-up approach, tokenizer, inverted index, scorers, 10 experiments mapping to the paper's theorems, plus a Rust port. Jaepil's implementation took the opposite path, a thin NumPy layer that plugs into existing search systems.\n\nReading both codebases side by side, I found my document length prior has room to improvement (e.g. monotonic decay instead of symmetric bell curve), my probability AND suffered from shrinkage, and I further added automatic parameter estimation and online learning entirely.\n\nbb25 v0.2.0 introduces all four. One fun discovery along the way, my Rust code already had the correct log-odds conjunction, but I had never backported it to Python. Same project, two different AND operations.\n\nThe deeper surprise came from a formula in the reference material. Expand the Bayesian posterior and you get the structure of an artificial neuron! Think of weighted sum, bias, sigmoid activation. Sigmoid, ReLU, Softmax, Attention all have Bayesian derivations. A 50-year-old search algorithm leads straight to the mathematical roots of neural networks.\n\nAll creds to Jaepil and Cognica Team!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r62t5f/bb25_bayesian_bm25_v020_is_out/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6liolg",
          "author": "Academic_Track_2765",
          "text": "is this your paper? my guess is you implemented the approach in rust? nice work!  \n[https://www.researchgate.net/publication/400212695\\_Bayesian\\_BM25\\_A\\_Probabilistic\\_Framework\\_for\\_Hybrid\\_Text\\_and\\_Vector\\_Search](https://www.researchgate.net/publication/400212695_Bayesian_BM25_A_Probabilistic_Framework_for_Hybrid_Text_and_Vector_Search)",
          "score": 1,
          "created_utc": "2026-02-21 13:29:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6o3f1r",
              "author": "Ok_Rub1689",
              "text": "Jaepil is my friend, and cognica (Jaepil is CEO) is close partner to the startup that I am working on!",
              "score": 2,
              "created_utc": "2026-02-21 21:36:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5mp7t",
      "title": "Has anyone here successfully sold RAG solutions to clients? Would love to hear your experience (pricing, client acquisition, delivery, etc.)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5mp7t/has_anyone_here_successfully_sold_rag_solutions/",
      "author": "Temporary_Pay3221",
      "created_utc": "2026-02-15 18:55:31",
      "score": 22,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "Hey everyone!\n\nI've been diving deep into RAG systems lately and I'm genuinely fascinated by the technology. I've built a few projects for myself and feel confident in my technical abilities, but now I'm looking to transition this into actual client work.\n\nBefore I jump in, I'd really appreciate learning from people who've already walked this path. If you've sold RAG solutions to clients, I'd love to hear about your experience:\n\n**Client & Project Details:**\n\n* What types of clients/industries did you work with?\n* How did they discover they needed RAG? (Did they come asking for it, or did you identify the use case?)\n* What was the scope? (customer support, internal knowledge base, document search, etc.)\n\n**Delivery & Timeline:**\n\n* How long did the project take from discovery to delivery?\n* What were the biggest technical challenges you faced?\n* Did you handle ongoing maintenance, or was it a one-time delivery?\n\n**Business Side:**\n\n* How did you find these clients? (freelance platforms, LinkedIn outreach, referrals, content marketing, etc.)\n* What did you charge? (ballpark is fine - just trying to understand market rates)\n* How did you structure pricing? (fixed project, hourly, monthly retainer?)\n\n**Post-Delivery:**\n\n* Were clients happy with the results?\n* Did you iterate/improve the system after launch?\n* Any lessons learned that you'd do differently next time?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5mp7t/has_anyone_here_successfully_sold_rag_solutions/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5k30sr",
          "author": "AmbitionCrazy7039",
          "text": "What types of clients/industries did you work with? - Build and deployed for an IP firm.\n\nHow did they discover they needed RAG?Â - They did by themselves. They asked us for a discovery meeting.  But they basically knew what solutions they want. We picked the most interesting one.\n\nWhat was the scope? - Some very specific document/outcome search. But not just semantics stuff, but some path dependent precedents + analytics. The LLM basically only summarizes the output. Traceability was most important. \n\n\n\nHow long did the project take from discovery to delivery? - \\~1 month from discovery to signing. Pilot delivery \\~6 weeks. A lot of follow ups needed.\n\nWhat were the biggest technical challenges you faced? - Preprocessing shitty data.\n\nDid you handle ongoing maintenance, or was it a one-time delivery? - We handle maintenance if something breaks.\n\n\n\nHow did you find these clients? - Network / they found us.\n\nWhat did you charge? - Our deal was kinda different so any inside from me here will not reflect real market value. We did a research project for them at special conditions (highly profitable for both sides).\n\nHow did you structure pricing? - Fixed price per month for developing with 2 months of expected development time. We granted the option to exit the project after 1 month without paying for 2. If we would build in a different setting we surely would go for usual 30/40/30 splits.\n\n\n\nWere clients happy with the results? - Yes.\n\nDid you iterate/improve the system after launch? - Not yet but we do run market research to evaluate if this may fit a broader audience.\n\nAny lessons learned that you'd do differently next time? - Nothing we really did wrong but the project shifted my view on RAG, from \"some useless semantic search\" to a more broader, creative approach.   \nBut I will add that RAG can't to magic for LLMs. If your project scope is fundamentally not solvable by an LLM (for example writing high quality law stuff), RAG will not change it. \n\n",
          "score": 5,
          "created_utc": "2026-02-15 19:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5k5l2j",
              "author": "Temporary_Pay3221",
              "text": "**Thanks for sharing**  \n  \n**Two questions:**\n\n1. **How did they find you?** You said \"they found us\", but what specifically made them reach out to YOU vs the hundreds of other people who can build RAG systems? Was it your profile? A referral? Something you built publicly?\n2. **Do you want to scale this?** Are you thinking about:\n\n* Building a dev team to handle delivery while you focus on growth?\n* Hiring sales to bring in more deals?\n* Creating systems/processes to delegate the work?\n\nOr is your goal to stay small, just you (maybe +1-2 people) doing selective projects?\n\nBecause right now you're limited by your own time. Curious if scaling is part of your plan or not.",
              "score": 2,
              "created_utc": "2026-02-15 19:36:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5k859m",
                  "author": "AmbitionCrazy7039",
                  "text": "1. Network. I started a ML startup years ago. Although it had nothing to do with LLMs and RAG.\n2. Who says we don't have a dev+sales team? My goal is and has ever been to build things that really create value. This system seems to deliver to the expectations. May open source it. The difficult part is the preprocessing and this pipelines are fundamentally not really scalable. Building the individual pipelines for customers is both fun and high-payed.",
                  "score": 3,
                  "created_utc": "2026-02-15 19:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kxqpn",
              "author": "AumOza",
              "text": "Curious what you meant by shitty data was it OCR problems from scanned docs, messy formatting, or something more complex?",
              "score": 1,
              "created_utc": "2026-02-15 22:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n3dd1",
                  "author": "AmbitionCrazy7039",
                  "text": "Yes we had to do OCR from scanned docs. This was one sketchy thing.\n\nThe other problem was more related to data retrieval. In this particular use case, some of the data came from internal documents. That wasn't a problem. However, in order to develop a really good product, we needed data from a public source called â€œEPO.â€ Basically, hundreds of millions of our data are publicly available in scanned PDF files. However, there is no API for retrieving our specific data set, only an online service. We had to scrape a sufficient data set. Technically, we solved that challenge, but the terms of use prohibit or at least severely restrict scraping.Â ",
                  "score": 2,
                  "created_utc": "2026-02-16 06:26:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nmow7",
              "author": "Late_Special_6705",
              "text": "Ð¢Ñ‹ Ð±ÐµÑÐ¿Ð¾Ð»ÐµÐ·Ð½Ñ‹Ð¹ Ð±Ð¾Ð»Ñ‚Ð»Ð¸Ð²Ñ‹Ð¹ Ð¼ÑƒÑÐ¾Ñ€",
              "score": 1,
              "created_utc": "2026-02-16 09:25:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5o2ri2",
          "author": "RobertLigthart",
          "text": "most clients dont come asking for \"RAG\"... they just have a problem like \"our team cant find anything in our docs\" or \"support takes too long to answer questions.\" the selling part is translating the technical solution into the business problem. if you lead with \"I'll build you a RAG pipeline\" they'll stare at you blank. lead with \"I'll cut your support response time in half\" and suddenly they're interested",
          "score": 2,
          "created_utc": "2026-02-16 11:50:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oc05d",
              "author": "Wide_Brief3025",
              "text": "Totally agree that framing is everything. I always focus conversations on practical outcomes like boosting efficiency or reducing ticket times rather than the tech behind it. For finding real business pains on networks like Reddit and LinkedIn, a tool like ParseStream makes it easier to spot and jump into those discussions when people mention problems your solutions can fix.",
              "score": 1,
              "created_utc": "2026-02-16 12:58:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65qhna",
          "author": "Chance-Fan4849",
          "text": "I recently delivered a RAG + MCP knowledge base using **Ragie** for retrieval, Airtable as the structured data source, and exposed it via an MCP server so Claude and ChatGPT can query the same system.\n\nMVP took \\~1â€“2 weeks.\n\nBiggest challenges werenâ€™t embeddings (Ragie handles that), but:\n\n* Structuring the data before indexing\n* Designing good metadata for filtering\n* Tuning retrieval quality (precision vs recall)\n\nPricing was fixed for MVP.\n\nMain lesson: RAG success depends more on clean data structure and evaluation loops than the vector DB itself.",
          "score": 2,
          "created_utc": "2026-02-19 00:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65sn1d",
          "author": "remoteinspace",
          "text": "built [papr.ai](http://papr.ai) memory and rag layer - open source. happy to share what's working, not working, etc. DM me ",
          "score": 1,
          "created_utc": "2026-02-19 00:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5obzck",
          "author": "devtechmonster",
          "text": "i dont know man.. there's already existing rag application by google which is notebookLM.. maybe if ur rag application is better than that, u can sell it.. i asked people myself if they interested in my application and most of them ask me whats the difference between the existing application like notebookLM?",
          "score": 0,
          "created_utc": "2026-02-16 12:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o652ztr",
              "author": "Turbulent_Principle7",
              "text": "That what I thought it seems like  NotebookLLM is the Google of the rag and document retrieval . Do you think we should still invest in building rag system , if yes who are the potential customer,",
              "score": 1,
              "created_utc": "2026-02-18 22:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o666343",
                  "author": "devtechmonster",
                  "text": "i think if ur company need it then its good.. some company have a lot of private documents they dont want to share to 3rd party app so they prefer using their internal rag system... other than that idk",
                  "score": 1,
                  "created_utc": "2026-02-19 02:18:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r79va5",
      "title": "SurrealDB 3.0 for multi-model RAG just launched",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r79va5/surrealdb_30_for_multimodel_rag_just_launched/",
      "author": "DistinctRide9884",
      "created_utc": "2026-02-17 15:58:44",
      "score": 19,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "SurrealDB 3.0 just dropped, with a big focus on agent memory infra for AI: improved vector indexing + better graph performance + native file storage + a WASM extension system (Surrealism) that can run custom logic/models inside the DB. You can store vector embeddings + structured data + graph context/knowledge/memory in one place.\n\nDetails:Â [https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memor](https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memory)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r79va5/surrealdb_30_for_multimodel_rag_just_launched/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r8y68c",
      "title": "Building a Graph RAG system for legal Q&A, need advice on dynamic vs agentic, relations, and chunking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r8y68c/building_a_graph_rag_system_for_legal_qa_need/",
      "author": "Famous_Buffalo_7725",
      "created_utc": "2026-02-19 12:40:59",
      "score": 17,
      "num_comments": 13,
      "upvote_ratio": 0.9,
      "text": "Hi everyone,\n\nIâ€™m building a Graph RAG system to answer legal questions across statutes, case law, and contracts. The goal is high accuracy, strong multi-hop reasoning, and reliable citations.\n\nIâ€™m trying to decide:\n\n1. Architecture: Is it better to use a static graph, dynamic query-time graph, or agentic Graph RAG for legal domains? What worked best for you in production?\n2. Relations: What are the most effective techniques for creating strong relations between chunks in legal documents? Entity backbone plus LLM triples? Cross-reference edges? NLI or contradiction edges? Temporal and amendment links? or any other approches?\n\nIf you had to pick a small high-impact stack, what would it be?\n\n1. Chunking What chunking strategy works best for law? Clause level, section level, sliding window, or hybrid?\n2. Evaluation How do you measure quality in legal Graph RAG? Citation precision, lawyer review, curated multi-hop Q&A?\n\nWould appreciate practical advice from anyone who have knowledge of Graph RAG  \nThank You",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r8y68c/building_a_graph_rag_system_for_legal_qa_need/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o68nawb",
          "author": "Ok_Signature_6030",
          "text": "legal graph RAG is one of the harder setups because the relationships between documents actually matter for correctness â€” you can't just do similarity search and hope for the best.\n\n  \non architecture: go agentic over static. in legal you constantly need to trace things like \"this clause was amended by X statute on Y date\" or \"this ruling cites and partially overrules case Z.\" a static graph can't adapt to these multi-hop paths well because the traversal pattern changes based on the query type. an agentic approach where the model decides which edges to follow based on the question gives you much better citation chains.\n\n  \nfor chunking: clause-level is the way to go for legal. section-level loses the granularity you need for citation accuracy (you want to point to the exact clause, not a whole section). but keep the section hierarchy as metadata so you can still navigate up/down the document structure.\n\n  \non relations â€” cross-reference edges + temporal links are probably the highest-impact combo. statutes constantly reference each other and get amended over time. NLI edges are cool in theory but expensive to compute at scale and contradiction detection accuracy in legal language is still sketchy.\n\n  \nfor eval: curated multi-hop Q&A sets with lawyer review on the citations is the gold standard. automated metrics alone won't catch hallucinated citations that look plausible.",
          "score": 5,
          "created_utc": "2026-02-19 13:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68r8m2",
              "author": "Famous_Buffalo_7725",
              "text": "u/Ok_Signature_6030 Thanks for the detailed advice, it helped me a lot.\n\nQuick follow-up. In my dataset, the documents mix legal clauses with financial tables in the same file, like covenant definitions plus ratio tables, pricing grids, payment schedules, etc. Here in chunking what approach would you recommend in this mixed setup? Any practical guidance on how you handle tables, especially when questions depend on specific cells or row items?\n\nAlso for relations, Iâ€™m focusing on cross-references, and temporal links first. Do you have any recommended resources (papers, repos, blog posts, or tooling) that helped you build reliable technical relation extraction for legal and finance documents?\n\nThanks again, I appreciate it.",
              "score": 1,
              "created_utc": "2026-02-19 14:04:55",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o68so2b",
                  "author": "Ok_Signature_6030",
                  "text": "for the mixed clause + table problem, you want a dual-chunking strategy. treat text chunks and table chunks as separate node types in your graph. tables should be stored as structured data (row/column format with headers preserved), not flattened text. when a question targets a specific cell, your retriever needs to match against column headers + row identifiers, not just semantic similarity on raw text.\n\n  \npractically, we detect tables during ingestion (most legal PDFs have identifiable table boundaries), extract them with something like camelot or tabula, store them as structured nodes, and link them to the surrounding clause nodes with 'contains\\_table' or 'defined\\_in' edges. so a query about 'what's the leverage covenant ratio threshold' traces from the covenant definition clause to the pricing table.\n\n  \nfor relation extraction resources... blackstone NLP library is probably the best starting point for legal text (handles citation parsing well). for financial cross-references, docling from IBM has decent table extraction. on the academic side, the CUAD dataset and papers around it are solid for contract understanding benchmarks. neo4j's graphrag blog series also has useful patterns for building extraction pipelines.",
                  "score": 2,
                  "created_utc": "2026-02-19 14:12:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6ds2uw",
          "author": "CountlessFlies",
          "text": "Have you tried a basic RAG system first? I would first create a benchmark for the task at hand, try a very basic solution first to get a baseline, and only then attempt to implement more complex solutions to see if they actually improve perf (and by how much).",
          "score": 2,
          "created_utc": "2026-02-20 07:02:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68j9t0",
          "author": "One_Milk_7025",
          "text": "To see realtime chunking with variable chunking setting\nView chunker.veristamp.in",
          "score": 1,
          "created_utc": "2026-02-19 13:19:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o69pl7f",
          "author": "tom_at_zedly",
          "text": "For the mixed clause + table problem, dual-chunking works well: text chunks and table chunks as separate node types with different handling at retrieval.\n\n\n\nDuring ingestion, detect table boundaries (camelot or tabula for clean PDFs, Docling for messier layouts). Store tables as structured nodes with rows and column headers preserved â€” don't flatten them to prose. Link them to the surrounding clause nodes with typed edges like \\`covenant\\_clause â†’ contains\\_table â†’ leverage\\_ratio\\_grid\\`.\n\n\n\nThe key insight: semantic similarity alone won't find \"leverage ratio threshold\" in a table. You need BM25 on column headers and row identifiers for the structured stuff, vector search on clause text, then combine at reranking.",
          "score": 1,
          "created_utc": "2026-02-19 16:57:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6aefuq",
              "author": "Mediocre-Basket8613",
              "text": "how should i setup the orchestrating framework? should i use sdks or langchain/langgraph or something else?",
              "score": 1,
              "created_utc": "2026-02-19 18:55:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6am2h4",
                  "author": "tom_at_zedly",
                  "text": "For this kind of setup I'd lean LlamaIndex over LangChain. It has native graph RAG support (PropertyGraphIndex), handles different node types out of the box (so your text chunks and table chunks can live as separate types), and composable retrievers make the BM25 + vector hybrid straightforward.\n\n\n\nLangGraph is better suited for complex agentic flows with branching logic â€” overkill if your main problem is retrieval. Raw SDKs give you full control, but a pain to maintain.",
                  "score": 1,
                  "created_utc": "2026-02-19 19:31:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kr5bk",
          "author": "ampancha",
          "text": "Agentic Graph RAG in legal is powerful but introduces failure modes your chunking strategy won't catch: prompt injection that manipulates citation selection, unbounded tool calls during multi-hop traversal, and missing audit trails for legal work product. If you go agentic, the first controls to scope are tool allowlists, per-query token caps, and a citation verification layer before anything hits a user. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-21 09:29:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5lj2m",
      "title": "RAG for structured feature extraction from 500-700 page documents â€” what's your strategy?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5lj2m/rag_for_structured_feature_extraction_from_500700/",
      "author": "Weary_Supermarket399",
      "created_utc": "2026-02-15 18:10:21",
      "score": 16,
      "num_comments": 14,
      "upvote_ratio": 0.95,
      "text": "I'm trying to build a RAG pipeline to extract \\~50 predefined features from large tender/procurement documents (think: project name, technical specs, deadlines, payment terms, penalties, etc.). Each feature has its own set of search queries and an extraction prompt.\n\nWorks reasonably well on shorter docs (\\~80 pages). On 500-700 page documents with mixed content (specs, contracts, schedules, drawings, BOQs), retrieval quality drops hard. The right information exists, but indexing and retrieval become difficult.\n\nThis feels like a fundamentally different problem from conversational QA. You're not answering one question, you're running 50 targeted extractions across a massive document set where the answer for each could be anywhere.\n\n**For those who've built something similar:** How do you approach retrieval when the document is huge, the features are predefined, and simple semantic search isn't enough?\n\nCurious about any strategies â€” chunking, retrieval, reranking, or completely different architectures.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5lj2m/rag_for_structured_feature_extraction_from_500700/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5juwz0",
          "author": "Astroa7m",
          "text": "I was experimenting with something similar by loading punch of data and extracting Q/A. \n\nWell you could try the following:\n- you have to call the LLM multiple times but make sure to chunk your data so it is within the LLMâ€™s input limit\n- sometimes sticking to input limit is not enough as you would have hallucinations/in-complete result in your output so you could either turn off thinking tokens or take a percentage of the input token and keep reducing it gradually and see what works \n- I experimented with a lightweight compression format where I retain the meaning for LLMs by keeping only verbs, nouns, proper nouns, punctuation , numbers, and symbols. Worked great but poor with other languages depending on the POS ML model used (used spaCy NLP lib)\n\nFinally we need to aggregate all through either clustering or another llm call. \nHowever, this is only my personal experience I think there are more brilliant approaches.",
          "score": 3,
          "created_utc": "2026-02-15 18:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5kk42z",
          "author": "reallynewaccount",
          "text": "Keep track on this post. Similar problem. My case is also hardened by multiple questions like \"how many XYZ-related facts in this document\" which kills any RAG related scenario. Also my input is 200+ pages business reports pdfs made out of bad quality scans (sometimes 2-3 first symbols could be cut in each line).\nSo, if there is any reasonable solution, will be great to know it exists.",
          "score": 3,
          "created_utc": "2026-02-15 20:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mtu1a",
              "author": "FUNdationOne",
              "text": "Try www.bildy.ai/documents\n\nI built it to solve the extraction of data from documents. Let me know how it goes for you and if you need any help.",
              "score": 0,
              "created_utc": "2026-02-16 05:08:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5p3ijb",
                  "author": "reallynewaccount",
                  "text": "Thank you. However, there are \"few little issues\" :)\n1. Security policy only let me work with local models\n2. This one (as well as many other \"big models\") only accept PDF smaller than 50MB, while mine could be 150+ (probably could be downsized, but this will affect quality, which is already not very good)\n3. Despite I usually have list of \"most popular questions\" it also should let user to ask extra for further details.\n\nSo, in other words, it would work relatively well with big effective model with huge context (and that's what I'm now look at with those new Kimi, Qwen and others), but I was just wondering if there is still any \"algorithmic\" approach to do so.",
                  "score": 1,
                  "created_utc": "2026-02-16 15:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5jx32p",
          "author": "4641874165465",
          "text": "Second this. I'm running into a wall also. I'm trying to implement Graph Rag also, but I'm a noob ans Just learning and vibe coding hard on simpler Test Data, to get NER right.",
          "score": 2,
          "created_utc": "2026-02-15 18:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mtrkr",
              "author": "FUNdationOne",
              "text": "Try www.bildy.ai/documents\n\nI built it to solve the extraction of data from documents. Let me know how it goes for you and if you need any help.",
              "score": 0,
              "created_utc": "2026-02-16 05:08:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5m7fwy",
          "author": "Linkman145",
          "text": "Typically documents are separated. E.g. thereâ€™s chapters segments etc\n\nIf your documents are huge then have a preprocessing step where you split them semantically. Think chapters 1-5 then another file for chapters 6-10 and so on.\n\nThen you do a multistep rag of sorts; you have a decision algorithm to decide which of the document pieces is relevant to your case and then rag over that single document\n\nSorry for no punctuation am on mobile",
          "score": 2,
          "created_utc": "2026-02-16 02:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lv1af",
          "author": "SFXXVIII",
          "text": "Youâ€™ll want to look into a setup where you fan out to the pages in batches or individually to find candidate answers and then aggregate over that to find your final extractions.",
          "score": 1,
          "created_utc": "2026-02-16 01:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n8e7m",
          "author": "nightman",
          "text": "Check how parsing and extracting structured data works on e.g. https://www.llamaindex.ai/\n\nThen compare with your chunk content and results and see what you need to improve.",
          "score": 1,
          "created_utc": "2026-02-16 07:10:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nvuur",
          "author": "Unlucky_Comment",
          "text": "There's no pre-packaged solution that works, I tried graphiti, i tried before vector stores, and even with keywords mapping, it's not enough.\n\nYou need something custom appropriate for your use case, otherwise you'll hit a wall.\n\nI ended up creating a custom graph rag, it works great.\n\nAlso, depending on your budget, you might want to go self hosted, we were with GCP / Vertex and kept hitting 429s, also very expensive.\n\nWe went with self hosted and qwen / deepseek combo.",
          "score": 1,
          "created_utc": "2026-02-16 10:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o06cc",
          "author": "Ecstatic_Heron_7944",
          "text": "A fave strategy of mine: **table of contents (TOC)** \\- a simple filtering approach prior to performing search.  \n  \n1) A preprocessing step to extract the table of contents section from each document. Typically first few pages.  \n2) At time of query, only expose the document titles and their TOCs either in the prompt, tools or MCP  \n3) The trick is to let the agent decide which documents (and/or pages) given are likely relevant to the user's query.  \n4) Let the agent scope and perform its RAG search filtered on the matched documents  \n5) Extend via deepsearch-like looping where the agent can repeat steps 2-4 until it finds a suitable answer\n\nThis approach differs by being top down as opposed to just diving straight into the contents and working out where you are is bottom up. Of course,  works better if the document has a TOC otherwise the alternative is parsing the documents to essentially generate your own TOC.\n\nAlso as a fellow enthusiast in handling large documents, would love some feedback on something I'm building [ragextract.com](http://ragextract.com) and how it compares to your pipeline (particularly interested in what you're doing for retrieval). Cheers!",
          "score": 1,
          "created_utc": "2026-02-16 11:29:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65szq4",
          "author": "remoteinspace",
          "text": "you need a knowledge graph for this. At [papr.ai](http://papr.ai) developers can register schemas that we use to extract things like project name, tech specs etc. from docs. DM me and I can share what works/doesn't work for this use case",
          "score": 1,
          "created_utc": "2026-02-19 01:01:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lnbpx",
          "author": "Academic_Track_2765",
          "text": "I just embed and cross my fingers ðŸ˜‚\n\nhere is the real approach, its not simple or cheap, but it works. I deal with 10k filings, technical documents / legal documents - some spanning 800-900 pages. \n\n\n\nTender Document (600 pages)\n\nâ†“\n\nDocument Intelligence\n\n  \\- Section classification (ToC parsing + LLM tagging)\n\n  \\- Table extraction â†’ structured store\n\n  \\- Hierarchical chunking with parent references\n\nâ†“\n\n\\[Feature Extraction Loop â€” runs for all 50 features\\]\n\n  For each feature:\n\n\\- Scope retrieval to relevant section types\n\n\\- BM25 + semantic search â†’ merge (RRF)\n\n\\- Cross-encoder rerank\n\n\\- Parent-document expansion\n\n\\- Feature-specific extraction prompt â†’ structured JSON output\n\n\\- Confidence check + verbatim validation\n\nâ†“\n\n\\[Output\\]\n\n  \\- Structured feature store (all 50 features per document)\n\n  \\- Confidence scores + source citations\n\n  \\- Flagged features for human review\n\nAnother tool you might want to test is PageIndex. While your case is different than mine, I think Page Index can truly help here, as there is a TOC / Hierarchy involved here, but as I have said before PDFs are as messy as they come, so the ingestion pipeline itself needs to be solid if your data has tabular items embedded, or images. Honestly there are multiple ways to tackle this. A lot of times people are using RAG outside of its intended purpose. I gets requests daily where someone wants to semantically search a RAG with 20 million records, because for some reason they want to see each and every record containing \"blah. blah. blah\". RAG is not designed for queries like that. If you think that's the way people want to use your RAG DM me and I can help. I might be slow to respond, but there are architectures that can help with your problem here, and the broad search problem. \n\n[https://docs.pageindex.ai/](https://docs.pageindex.ai/)",
          "score": 1,
          "created_utc": "2026-02-21 13:58:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6io8u",
      "title": "We built a local-first RAG memory engine + Python SDK (early feedback welcome)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6io8u/we_built_a_localfirst_rag_memory_engine_python/",
      "author": "DetectiveMindless652",
      "created_utc": "2026-02-16 19:06:07",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nWeâ€™ve been working on a local-first memory engine for RAG pipelines and wanted to share it here for feedback.\n\nA lot of RAG setups today rely on cloud vector databases, which works, but can add latency, cost, and operational overhead. We wanted something simpler that runs entirely locally and gives predictable retrieval for retrieval-heavy workflows.\n\nSo we built **Synrix**, plus a small Python RAG SDK on top of it.\n\nAt a high level:\n\n* Everything runs locally (no cloud dependency)\n* You can store chunks + metadata and retrieve deterministically\n* Queries scale with matching results rather than total dataset size\n* Designed for agent memory and RAG-style recall\n* Python SDK to make ingestion + querying straightforward\n\nThe RAG SDK basically handles:\n\n* ingesting documents / chunks\n* attaching metadata (source, tags, IDs, etc.)\n* querying memory for relevant context\n* returning results in a format thatâ€™s easy to feed back into your LLM\n\nWeâ€™ve been testing on local datasets (\\~25kâ€“100k nodes) and seeing microsecond-scale prefix lookups on commodity hardware. Benchmarks are still being formalized, but itâ€™s already usable for local RAG experiments.\n\nGitHub is here if anyone wants to try it:  \n[https://github.com/RYJOX-Technologies/Synrix-Memory-Engine]()\n\nThis is still early, and weâ€™d genuinely love feedback from people building RAG systems:\n\n* How are you handling retrieval today?\n* What pain points do you hit with vector DBs?\n* What would you want to see benchmarked or improved?\n\nHappy to answer questions, and thanks in advance for any thoughts ðŸ™‚",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r6io8u/we_built_a_localfirst_rag_memory_engine_python/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5tje87",
          "author": "Harotsa",
          "text": "Why are you comparing your prefix lookup to other DBâ€™s cosine similarity search? That seems like a category error in terms of comparison, as essentially all of the major DBs have efficient prefix lookups as well. These include many DBs that are open source and can be run locally or even in process. Iâ€™d be more interested in seeing side by side comparisons of equivalent search methods against some major open source DBs.  \n\nAlso, a search scaling only with the top-k result count sounds impossible, unless youâ€™re talking about simple hash lookup on a key. But if thatâ€™s what youâ€™re talking about then you need to already know a priori the exact piece of data you want to lookup. And even then in practice B-trees will be more efficient for ID lookups than using a hash on large DBs (so also not independent of DB size).\n\nMaybe you can clarify your intentions, your claims, or how things work since nothing on your repo or your website really provide any explanation, technical detail, examples, or actual benchmarks/comparisons.",
          "score": 1,
          "created_utc": "2026-02-17 05:51:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uc5mu",
              "author": "DetectiveMindless652",
              "text": "let me get back to you, you raise some good concerns. ",
              "score": 1,
              "created_utc": "2026-02-17 10:14:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8ulm8",
      "title": "I need a production grade RAG system",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r8ulm8/i_need_a_production_grade_rag_system/",
      "author": "Several_Job_2507",
      "created_utc": "2026-02-19 09:17:45",
      "score": 14,
      "num_comments": 22,
      "upvote_ratio": 0.85,
      "text": "Hey, I need to build a RAG system for Hindi-speaking folks in India. I'll be using both Hindi and English text. The main thing is, I need to make a production-ready RAG system for students to get the best info from it.\n\nI'm a software developer, but I'm new to RAG and AI. Any good starting points or packages I can use? I need something free for now; if it works out, we can look into paid options. I'm sure there are some open-source solutions out there. Let me know if you have any special insights\n\n\nThankyou.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r8ulm8/i_need_a_production_grade_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o67wjcq",
          "author": "One_Milk_7025",
          "text": "Start from scratch you will learn better..\nYou process pdf or any txt based file to markdown format and then chunk with proper metadata extraction> embed with multilingual embedding model > insert into vector database.. this is ingestion part.\nFor the retrieval you need to embed the query , optionally expand the query and then do similarity search in the vector db.. local lance db or a qdrant cloud or docker hosted gives nice sdk for this .. after similarity search you rank the chunk and send the top 5 to the llm as a context with the query..\nHere you can increase the accuracy by implementing bm25 , use hybrid retrieval..\nImplement rrf, mmr etc..\n\nThe trick is the tool calling and discovery strategy.. you can stuff 20chunk and still won't get proper result.. lazy discovery, temporal knowledge graph would help then for nuisance use case .\n\nSo rag is not a single thing it is different for each use cases.. making a generic rag would take time to cover all the things.. \nSee lightrag for the all in one pack..\nOr built youself with qdrant cloud(embedding +vector store) with custom ingestion pipeline..",
          "score": 7,
          "created_utc": "2026-02-19 10:26:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67s256",
          "author": "fabkosta",
          "text": "This is the tech stack you need, and it's entirely open source and free:\n\n1. **OCRing**: Docling\n2. **Vector database:** PostgreSQL or Elasticsearch\n3. **Backend framework:** LlamaIndex, Haystack, or some framework by Microsoft / Google (avoid Langchain or Langgraph, I'd say)\n4. **Frontend**: OpenChat (https://www.openchatui.org/) or LibreChat (https://www.librechat.ai/), there are some others\n5. **Hosting**: Up to you.",
          "score": 6,
          "created_utc": "2026-02-19 09:43:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1xz4",
              "author": "Massive-Mobile-5655",
              "text": "Why to avoid langchain and langraph? I did build one pipeline using these and can you tell me the resources through which i can learn in depth",
              "score": 1,
              "created_utc": "2026-02-21 05:33:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6kj8hc",
                  "author": "fabkosta",
                  "text": "Langchain is over engineered. Langgraph you only need if you need a state machine, which is not the case for most RAG solutions.",
                  "score": 1,
                  "created_utc": "2026-02-21 08:10:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o67t18r",
              "author": "Several_Job_2507",
              "text": "I'm mostly worried about indexing and getting the right answer.\nCan you share some of the best practices for indexing or resources so I can do it?",
              "score": 0,
              "created_utc": "2026-02-19 09:53:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6862r7",
                  "author": "HackHusky",
                  "text": "Going through the same stuff as you, man. I found a nice video that had some great tips:  \n[How to Build a Scalable RAG System for AI Apps (Full Architecture)](https://www.youtube.com/watch?v=4KiiKQ9RVvA)",
                  "score": 3,
                  "created_utc": "2026-02-19 11:48:37",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o67twhy",
                  "author": "fabkosta",
                  "text": "You need to:\n\n1. OCR your documents and extract text\n2. Chunk your documents (can use Langchain for that, for example)\n3. Index them using an embedding LLM (you'll also need a chat completion LLM for the user queries)\n4. You have to think hard what to do with images and tables, but that hardly can be generalized in advise since it's usually domain and data-specific.\n\nOther than that, I don't know what you mean with \"worried about indexing\".\n\nGetting a first version up and running is the easy part. The hard part is making it production ready and systematically increasing quality. 80% effort will go into dealing with the messiness of data.\n\n(Disclaimer: I am providing trainings and consulting for this topic cause I know it's not trivial.)",
                  "score": 2,
                  "created_utc": "2026-02-19 10:01:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o6ds831",
                  "author": "aanghosh",
                  "text": "Indexing can be deterministic provided the query to the vector DB is deterministic. If they query is coming from an AI model then that query is non-deterministic so you have to plan for that non-determinism. Realistically that means accepting and handling hallucinations or non-sense as answers. If you need determinism, maybe you could try to use regular expressions on a static document or format that the LLM cites as a reference. That would provide deterministic queries for you.",
                  "score": 1,
                  "created_utc": "2026-02-20 07:04:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o67u3pq",
          "author": "ChapterEquivalent188",
          "text": "have a kickstart and add what you need https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit  if you want to have it with a mcp server use the clawrag .... enterprise level kann be seen here https://github.com/2dogsandanerd/RAG_enterprise_core\n\nanyway, focus on ingestion as garbage in will always let you down at retrieval ",
          "score": 2,
          "created_utc": "2026-02-19 10:03:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67t30m",
          "author": "yafitzdev",
          "text": "Use this as reference: \n[fitz-ai](https://github.com/yafitzdev/fitz-ai)",
          "score": 1,
          "created_utc": "2026-02-19 09:53:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6997dq",
          "author": "Individual_Yard846",
          "text": "I have a RAG API you can use to this really easily if you'd like, dm me",
          "score": 1,
          "created_utc": "2026-02-19 15:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ceuvq",
          "author": "johnny_5667",
          "text": "igu. 5 buck? ill do it 4 u",
          "score": 1,
          "created_utc": "2026-02-20 01:15:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dcts1",
          "author": "bigwad",
          "text": "typesense is a much simpler elasticstyle style search index with vector and now AI based search leveraging this baked in. Opensource and seems to work well for us as a single binary too.",
          "score": 1,
          "created_utc": "2026-02-20 04:55:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6dqrqj",
          "author": "Educational_Cup9809",
          "text": "try out https://structhub.io. 15000 credits on signup",
          "score": 1,
          "created_utc": "2026-02-20 06:51:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6e5oya",
          "author": "Eastern-Pipe-7593",
          "text": "check it out [https://github.com/bienwithcode/AdmissionAgent](https://github.com/bienwithcode/AdmissionAgent)  \nIt's use case for university admission ",
          "score": 1,
          "created_utc": "2026-02-20 09:10:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gty2o",
          "author": "Top-Seaworthiness285",
          "text": "Try it it's totally free https://docmind.vasanthubs.co.in/",
          "score": 1,
          "created_utc": "2026-02-20 18:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6hf06w",
          "author": "Little-Ad-1526",
          "text": "I bukld something similiar (Indian urban planning documents) few weeks back, using visual rag.\nDm me",
          "score": 1,
          "created_utc": "2026-02-20 20:10:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6i66x2",
          "author": "Nimrod5000",
          "text": "I used raganything and haystack with milvus standalone.  Works wonderfully",
          "score": 1,
          "created_utc": "2026-02-20 22:26:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6lrms0",
          "author": "Physical_Badger1281",
          "text": "try [https://www.fastrag.live/](https://www.fastrag.live/)",
          "score": 1,
          "created_utc": "2026-02-21 14:24:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o67s3wy",
          "author": "MindlessFan9308",
          "text": "I can make a production ready RAG setup for you but ofcourse it comes with a cost and not for free.",
          "score": -4,
          "created_utc": "2026-02-19 09:44:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rask5l",
      "title": "What retrievers do you use most in your RAG projects?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rask5l/what_retrievers_do_you_use_most_in_your_rag/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-21 14:32:22",
      "score": 14,
      "num_comments": 21,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,  \nIâ€™m curious to know what retrievers you use most in your RAG pipelines. Do you mainly rely on vector search, BM25, hybrid retrieval, or something else?\n\nWould love to hear what works best for you in real projects.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rask5l/what_retrievers_do_you_use_most_in_your_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6lv6ec",
          "author": "Academic_Track_2765",
          "text": "Great question! Always hybrid with metadata filtering.",
          "score": 10,
          "created_utc": "2026-02-21 14:45:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lvwnm",
              "author": "minaminotenmangu",
              "text": "i do it without metadata filtering. the vector search sometimes knows better. But I think with time and if the database gets bigger filtering will return.",
              "score": 3,
              "created_utc": "2026-02-21 14:49:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lwt2e",
                  "author": "Academic_Track_2765",
                  "text": "Typically you want to do metadata filtering before the search space to avoid noise, but yes it can be done at both layers. My databases is currently at 5 million documents so we have mandatory metadata filtering before the vector search, e.g., the users select data range, region, categorized issue type and few other filters and then hybrid retrieval. It honestly blocks out so many noise points and retrieval speeds goes from seconds to milliseconds with better retrieved candidates.",
                  "score": 5,
                  "created_utc": "2026-02-21 14:54:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m0frg",
          "author": "avebrahimi",
          "text": "I have about 5m+ records, I reached this workflow after a dozen tries, first filter using metadata (both original metadata, and llm-based-metadata), then check if BM25 gets high scores, then mix it with vector search, otherwise just use vector search. and finally, reranking is amazing!!!\n\nthe big point is using perfect embedder for your data, to save tokens/money.\n\nDon't forget to show search result summary using LLM, it will please users.",
          "score": 6,
          "created_utc": "2026-02-21 15:14:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6pgpub",
              "author": "Final_Special_7457",
              "text": "What do u use as embedder model ?",
              "score": 1,
              "created_utc": "2026-02-22 02:32:43",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6nwah8",
          "author": "FeeMassive4003",
          "text": "We use hybrid: vector search plus keyword search. No rebranding - we just take k docs from each (usually k=5).",
          "score": 2,
          "created_utc": "2026-02-21 20:58:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6nwrlq",
              "author": "marwan_rashad5",
              "text": "Do you use reranking after retrieval with hybrid search ??",
              "score": 2,
              "created_utc": "2026-02-21 21:01:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6o517b",
                  "author": "FeeMassive4003",
                  "text": "No, we just take 5 from each. Total of 10 chunks, go to the LLM. It's quite basic; but it works.",
                  "score": 2,
                  "created_utc": "2026-02-21 21:44:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6m82mp",
          "author": "Rodda_LBV",
          "text": "Per verificare la qualitÃ  dei documenti recuperati quali metodi usate?",
          "score": 1,
          "created_utc": "2026-02-21 15:53:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6m9mtr",
          "author": "PresentationNew936",
          "text": "Well we mostly use hybrid (bm25 and vector) for best results. But it also highly depends on the embeddings. And of course there is re ranking at the end.",
          "score": 1,
          "created_utc": "2026-02-21 16:01:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6mywlz",
              "author": "marwan_rashad5",
              "text": "I haven't used reranking before.\nHow do you do it?",
              "score": 1,
              "created_utc": "2026-02-21 18:08:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6oqpte",
                  "author": "PresentationNew936",
                  "text": "Re-ranking is a step where you take already retrieved documents and sort them again using a smarter model. This model looks at the userâ€™s question and each document together and gives a better relevance score. It understands context more deeply, so the most useful and accurate documents move to the top.",
                  "score": 1,
                  "created_utc": "2026-02-21 23:48:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6n36k9",
          "author": "Independent-Bag5088",
          "text": "Depends.\n\n1. What type of documents are they - are we looking at more semantic text or numbers matter more?\n2. If query answers require creativity, semantic retrievals work better, but if they are domain-specific, BM25 (keyword match) might be a better option.\n\nIn my case, I have separated my documents into relational database + vector database, for appropriate use-case.\n\nIn my naive opinion, most of the time it depends on the type of document you are dealing with. Domain knowledge on the document would help you design your RAGÂ system appropriately.",
          "score": 1,
          "created_utc": "2026-02-21 18:29:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6gzih",
      "title": "How do you decide to choose between fine tuning an LLM model or using RAG?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6gzih/how_do_you_decide_to_choose_between_fine_tuning/",
      "author": "degr8sid",
      "created_utc": "2026-02-16 18:06:34",
      "score": 13,
      "num_comments": 18,
      "upvote_ratio": 1.0,
      "text": "# Hi,\n\nSo I was working on my research project. I created my knowledge base using Ollama (Llama 3). For knowledge base, I didn't fine tune my model. Instead, I used RAG and justified that it is cost effective and is efficient as compared to fine tuning. But I came across a couple of tutorials where you can fine tune models on single GPU.\n\n\n\nSo how do we decide what the best approach is? The objective is to show that it is better to RAG + system prompt, but RAG only provides extra information on top. It doesn't inherently change the nature of the LLM, especially when it comes to defending jailbreaking prompts or the scenario where you have to teach LLMs to realize the sinister prompts asking it to change its identity.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r6gzih/how_do_you_decide_to_choose_between_fine_tuning/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5q452d",
          "author": "fabkosta",
          "text": "They solve two distinct problems. If you compare them, that means you have the wrong mental model.\n\nFine-tuning is NOT to securely ingest new information into a model. Fine-tuning may or may not add new info to a model. It may not do that because the new info may collide with existing info, and then the model will behave inconsistently.\n\nRAG is the right thing that you need if you need to provide access to knowledge from your documents.\n\nSo, when is fine-tuning a good choice? Well, to give the LLM new or improved **capabilities** \\- not knowledge.",
          "score": 18,
          "created_utc": "2026-02-16 18:19:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ubvbq",
              "author": "laurentbourrelly",
              "text": "I'm doing both.\n\nUnalignment of LLM is a must.\n\nThen you can put a RAG on top.",
              "score": 1,
              "created_utc": "2026-02-17 10:11:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q82ho",
          "author": "ggone20",
          "text": "Fine tuning is almost never the first thing you would do. What does â€˜RAGâ€™ mean here? There are endless permutations of techniques, standard and exotic, that can fall under this category. \n\nNot only that but fine tuning isnâ€™t about retrieval so much as tone or understanding. Youâ€™re almost certainly not going to impart vast amounts of knowledge into a model, but you can make it align more with your expected outputs - be that email voice like a business/person, or classification of inputs for routing to the right â€˜agentâ€™ or tool. \n\nModels are so smart nowadays the answer is almost never fine tune and almost always context management and more robust and/or exotic retrieval techniques.",
          "score": 7,
          "created_utc": "2026-02-16 18:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qh7gy",
              "author": "Ryanmonroe82",
              "text": "I disagree.  Not a cloud model or open weights model on the market can touch the accuracy of a domain specific fine tuned version even with a great prompt and text embedded.  I have an 8b model now I fine tuned on over 100 million tokens that will beat any model out there on tasks specifically related to sonar physics, sound propagation/reverberation equations and applying it all to active scanning sonars while in use.  It's phenomenal actually, very impressed.",
              "score": 2,
              "created_utc": "2026-02-16 19:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qj9l6",
                  "author": "Horror-Turnover6198",
                  "text": "Youâ€™re doing physics calculations with an LLM?",
                  "score": 5,
                  "created_utc": "2026-02-16 19:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q9rts",
          "author": "DashboardNight",
          "text": "RAG for me was the way to go because it retrieves the relevant docs and can cite them directly from the knowledge base, including a link to those document and the relevant page. If answer validation is less important, I can imagine one chooses to go with finetuning a model instead.\n\nAlso, if the knowledge base changes all the time, it is better in my opinion to have a pipeline where documents get chunked and embedded directly, rather than having to retrain LLM models all the time.",
          "score": 2,
          "created_utc": "2026-02-16 18:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60c2fv",
              "author": "code_vlogger2003",
              "text": "Hey there is a paper from meta where why can't we pass chunked multi dimensional Embeddings to the attention etc right?",
              "score": 1,
              "created_utc": "2026-02-18 06:19:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qg5ii",
          "author": "Ryanmonroe82",
          "text": "Hey. I just went through this and it depends on what you are needing from the text and how well a model can handle it.  For my use case (sonar theory and physics of sound) the documents helped but the llm still didn't understand what I needed exactly from the documents.  Cloud models couldnt handle it either.  So I took about 6 months and built a 140 million token dataset and trained llama3 8b-instruct on exactly what I needed.  \nNow I don't need the documents embedded and actually get more detailed responses without them influencing the models response.  \nIt's incredibly useful to fine tune a model in my opinion but it's not very intuitive and can be a chore to get it right",
          "score": 1,
          "created_utc": "2026-02-16 19:14:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r5g5d",
          "author": "Irisi11111",
          "text": "Implementing RAG is a faster and easier to fine-tuning, especially when your case is common and can be resolved by providing accurate information to a model through multiple shots. Fine-tuning becomes necessary when dealing with private datasets, controlling output formats, behavior, or specific tool usage in agentic applications. However, fine-tuning only expands existing skillsets and doesn't introduce new abilities, so make sure identifying the true bottleneck before making any decisions.",
          "score": 1,
          "created_utc": "2026-02-16 21:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5si8n4",
          "author": "blue-or-brown-keys",
          "text": "In my experience,   \n\\- Use RAG for knowledge based tasks. Eg: Find me an answer from docs/tickets  \n\\- Use fine tuning for behaviour based tasks. Eg: Given a document always generate a JSON that looks like this",
          "score": 1,
          "created_utc": "2026-02-17 01:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5snbw4",
          "author": "burntoutdev8291",
          "text": "I would say fine tuning is good if you need new capabilities, like language, vocabulary and grammar. Or if you want a highly specialised LLM, think medical, legal etc.",
          "score": 1,
          "created_utc": "2026-02-17 02:14:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v30e9",
          "author": "RobertLigthart",
          "text": "simple rule I use -> if your data changes frequently or you need source attribution, RAG. if you need the model to behave differently (tone, format, domain-specific reasoning), fine-tune. for your research project RAG makes way more sense because you can swap out documents without retraining anything. fine-tuning for jailbreak defense is a different problem entirely though... that's more about alignment and guardrails than knowledge retrieval",
          "score": 1,
          "created_utc": "2026-02-17 13:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z1i7a",
          "author": "334578theo",
          "text": "If you donâ€™t know then you likely should just work on a system prompt and add RAG if the models training knowledge is not enoughÂ ",
          "score": 1,
          "created_utc": "2026-02-18 01:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6a9ng8",
          "author": "clickittech",
          "text": "it really comes down to whether you want to teach the model facts or personality.\n\nin RAG youâ€™re giving the model the facts right when it needs them. Fine-tuning is more like intensive training youâ€™re changing how it thinks and talks.\n\nFor the jailbreaking/identity stuff you mentioned, RAG won't change the model but itâ€™s 10x easier to update when your data changes and fine-tuning on a single GPU is great for fixing the vibe or a specific JSON format, but it's overkill for just adding knowledge.\n\nIf youâ€™re worried about sinister prompts, look into Llama Guard or similar guardrail layers. It's way more reliable than trying to \"bake\" safety into the weights of a small model.\n\nI actually shared a video/blog post a bit ago t if you want to geek out on the benchmarks:[https://www.clickittech.com/ai/rag-vs-fine-tuning-vs-prompt-engineering/](https://www.clickittech.com/ai/rag-vs-fine-tuning-vs-prompt-engineering/)",
          "score": 1,
          "created_utc": "2026-02-19 18:33:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qnr7p",
          "author": "Repulsive-Memory-298",
          "text": "guys wait till he finds out that nothing is mutually exclusive",
          "score": 0,
          "created_utc": "2026-02-16 19:51:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r9w8u0",
      "title": "Why Standard RAG Often Hallucinates Laws â€” and How I Built a Legal Engine That Never Does (Tested in Italian Legal Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r9w8u0/why_standard_rag_often_hallucinates_laws_and_how/",
      "author": "Pretend-Promotion-78",
      "created_utc": "2026-02-20 14:05:47",
      "score": 13,
      "num_comments": 4,
      "upvote_ratio": 0.84,
      "text": "Hi everyone,\n\nHave you ever had that *false confidence* when an LLM answers a technical question â€” only to later realize it confidently cited something incorrect? In legal domains, that confidence is the *number one danger*.\n\nWhile experimenting with a standard RAG setup, the system confidently quoted a statute that seemed plausibleâ€¦ until we realized that provision was **repealed in 2013**. The issue wasnâ€™t just old training data â€” it was that the system relied on *frozen knowledge* or poorly verified external sources.\n\nThis was something I had seen mentioned multiple times in other posts where people shared examples of legal documents with entirely fabricated statutes. That motivated me â€” as an Italian developer â€” to solve this problem in the context of **Italian law, where the code is notoriously messy and updates are frequent**.\n\nTo address this structural failure, I built **Juris AI**.\n\n\n\n# The Problem with Frozen Knowledge\n\nMost RAG systems are static: you ingest documents once and *hope* they stay valid. That rarely works for legal systems, where legislation evolves constantly.\n\nJuris AI tackles this with two key principles:\n\n**Dynamic Synchronization**  \nEvery time the system starts, it performs an incremental alignment of its sources to ensure the knowledge base reflects the *current state of the law*, not a stale snapshot.\n\n**Data Honesty**  \nIf a norm is repealed or lacks verified text, the system does not guess. It *reports the boundary of verification* instead of hallucinating something plausible but wrong.\n\n\n\n# Under the Hood\n\nFor those interested in the architecture but not a research paper:\n\n**Hybrid Graph-RAG**  \nWe represent the legal corpus as a *dependency graph* (KuzuDB + LanceDB). Think of this as a connected system where each article knows the law it belongs to and its references.\n\n**Deterministic Orchestration Layer**  \nA proprietary logic layer ensures generation *follows validated graph paths*.  \nFor example, if the graph marks an article as â€œrepealed,â€ the system is *blocked from paraphrasing* outdated text and instead reports the current status.\n\n\n\n# Results (Benchmark Highlights)\n\nIn stress tests against traditional RAG models:\n\n* **Zero hallucinations on norm validation** â€” e.g., on articles with suffixes like *Art. 155-quinquies*, where standard models often cite repealed content, Juris AI always identified the correct current status.\n* **Cross-Database Precision** â€” in complex scenarios such as linking aggravated theft (Criminal Code *Art. 625*) to civil liability norms (Civil Code *Art. 2043+*), Juris AI reconstructed the entire chain with literal text, while other systems fell back to general paraphrase.\n\n\n\n# Why Iâ€™m Sharing This Here\n\nThis is *not* a product pitch. Itâ€™s a technical exploration and Iâ€™m curious:\n\n**From your experience with RAG systems, in which scenarios does a deterministic validation approach become** ***essential*** **versus relying on traditional semantic retrieval alone?**",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r9w8u0/why_standard_rag_often_hallucinates_laws_and_how/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6fo9s8",
          "author": "ChapterEquivalent188",
          "text": "mamma mia. italian law is realy a messy kind of ;) its a sort of endboss. respect!\n\nHow do you handle extraction quality from source documents? standard ocr/layout fails on complex legal\n\nWhat's your validation rate for edge cases? The repealed-content blocking is critical -- we solve it via graph metadata and citation enforcer that validates every claim against source chunks. Sounds like similar philosophy, different implementation",
          "score": 2,
          "created_utc": "2026-02-20 15:19:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6gvc99",
          "author": "Top-Seaworthiness285",
          "text": "Try it out here: https://docmind.vasanthubs.co.in/\nItâ€™s completely free â€” no login required.",
          "score": 1,
          "created_utc": "2026-02-20 18:37:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6hbfb9",
              "author": "Pretend-Promotion-78",
              "text": "Nice effort, but I think there's a fundamental misunderstanding here. Juris AI isn't just another 'Chat with PDF' tool for generic documents. Itâ€™s a specialized Legal Intelligence engine built specifically to tackle the structural mess of Italian Law.\n\nThe main issue with generic PDF-to-chat tools is that they rely entirely on the file's content and the LLM's 'creativity'â€”which is a massive liability in the legal domain. We built a **Hybrid Graph-RAG architecture (KuzuDB + LanceDB)** that enforces a deterministic gatekeeper logic. If a statute in the document has been repealed or modified, my system cross-references it with the graph's metadata and kills the hallucination before it even reaches the user. A standard PDF chat tool would simply parrot back whatever is in the file, even if it's legally dead.\n\nGreat for casual use, but professional legal-tech requires structured logical constraints and real-time validity checks, not just basic semantic search.",
              "score": 1,
              "created_utc": "2026-02-20 19:52:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rab7rs",
      "title": "What chunking strategies are you using in your RAG pipelines?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rab7rs/what_chunking_strategies_are_you_using_in_your/",
      "author": "marwan_rashad5",
      "created_utc": "2026-02-20 23:31:11",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.85,
      "text": "Hey everyone,\n\nIâ€™m curious what chunking strategies youâ€™re actually using in your RAG systems. Are you sticking with recursive/character splitting, using semantic chunking, or something more advanced like proposition-based or query-aware approaches?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rab7rs/what_chunking_strategies_are_you_using_in_your/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6ji2v0",
          "author": "Ok_Signature_6030",
          "text": "for most document types recursive splitting with decent overlap still works better than people expect. we tested semantic chunking pretty extensively and the retrieval quality improvement was marginal â€” maybe 3-5% on our evals â€” while adding a lot of complexity and latency from the embedding calls during ingestion.\n\nwhere chunking strategy actually mattered for us was structured documents like contracts and technical specs. for those we switched to section-aware chunking that respects headers and keeps related clauses together. that alone bumped our answer accuracy by about 15% compared to naive 512-token windows.\n\nbiggest lesson was that chunk size matters way more than chunk method. going from 512 to \\~1200 tokens with 200 token overlap made a bigger difference than any fancy chunking algorithm we tried.",
          "score": 9,
          "created_utc": "2026-02-21 03:10:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k1qkh",
              "author": "Mystical_Whoosing",
              "text": "So this 1200 / 200 is token numbers, not characters?",
              "score": 2,
              "created_utc": "2026-02-21 05:32:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6pbbw9",
                  "author": "Ok_Signature_6030",
                  "text": "yeah those are token counts â€” 1200 tokens per chunk, 200 tokens of overlap between consecutive chunks. roughly 900-ish words per chunk depending on the tokenizer.\n\n  \nthe overlap is what prevents splitting an answer across chunk boundaries where neither chunk has enough context alone.",
                  "score": 1,
                  "created_utc": "2026-02-22 01:57:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6kmto0",
          "author": "cointegration",
          "text": "I have given up on 1 size fits all chunking strategy, different types of documents require different strategies, legal docs and instruction manuals do well with large chunks and semantic boundaries, invoices and receipts do much better with small chunks and metadata. Same for retrieval, BM25 is for precision, vectors for recall. Its quite obvious by now that much preprocessing/filtering is required even before chunking. I seperate out docs into several buckets, each have their own pipeline for ingestion. In a nutshell:\n\nIngestion:  \ndocs --> sorted into buckets by a small local llm --> extract MD/metadata --> chunk based on doc type --> embed\n\nRetrieval:  \nquery --> tfidf gets reduced candidate size --> BM25/Vector search --> gets weighted Top K chunk candidates plus neighbours --> cross-encoder rerank --> final chunks",
          "score": 3,
          "created_utc": "2026-02-21 08:46:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6jzdmp",
          "author": "StarThinker2025",
          "text": "Recursive splitting + overlap is still the baseline for us (600â€“800 tokens, ~15% overlap)\n\nSemantic chunking sounds better in theory, but retrieval stability matters more than â€œperfectâ€ boundaries\n\nWe optimize chunk size based on embedding model + average query length ðŸ“ˆðŸ“ˆðŸ“ˆ",
          "score": 2,
          "created_utc": "2026-02-21 05:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6km4w7",
          "author": "Infamous_Ad5702",
          "text": "2 sentence chunks.",
          "score": 2,
          "created_utc": "2026-02-21 08:39:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lhe68",
              "author": "marwan_rashad5",
              "text": "I think that 2 Sentence chunking might cause some loss of semantic coherence. What is your opinion about that?",
              "score": 1,
              "created_utc": "2026-02-21 13:20:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6lgvzu",
          "author": "itsmekalisyn",
          "text": "We have a lot of How to guides and we use page level and it is working well.",
          "score": 2,
          "created_utc": "2026-02-21 13:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6lh4f7",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-02-21 13:18:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6lh8cf",
                  "author": "itsmekalisyn",
                  "text": "For our case, recursive chunking was better than semantic. But, page level beats everything in our internal metrics.",
                  "score": 2,
                  "created_utc": "2026-02-21 13:19:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6k8g1t",
          "author": "One_Milk_7025",
          "text": "Chunking is very important part of the ingestion pipeline..\nTo actually see the chunks and its related metadata you need a chunk workbench or visualizer..\ncheckout chunker.veristamp.in for the start you can see all the code, table, heading, txt properly chunked with all the metadata.. you can tweak the settings for the optimal use case",
          "score": 1,
          "created_utc": "2026-02-21 06:29:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7ds0f",
      "title": "HyperspaceDB v2.0: Lock-Free Serverless Vector DB hitting ~12k QPS search (1M vectors, 1000 concurrent clients)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r7ds0f/hyperspacedb_v20_lockfree_serverless_vector_db/",
      "author": "Sam_YARINK",
      "created_utc": "2026-02-17 18:10:01",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.87,
      "text": "We just released v2.0 and rewrote the engineâ€™s hot path.\n\nThe bottleneck wasnâ€™t algorithms.\n\nIt was synchronization.\n\nUnder high concurrency, RwLock was causing cache line bouncing and contention. So we removed it from the search path.\n\n\n\nWhat changed\n\n\\- Lock-free index access via ArcSwap\n\n\\- Work-stealing scheduler (Rayon) for CPU-bound search\n\n\\- SIMD-accelerated distance computations\n\n\\- Serverless cold-storage architecture (idle eviction + mmap cold start)\n\n\n\nBenchmark setup\n\n\\- 1M vectors\n\n\\- 1024 dimensions\n\n\\- 1000 concurrent clients\n\n\n\nSearch QPS:\n\n\\- Hyperspace v2.0 â†’ 11,964\n\n\\- Milvus â†’ 4,848\n\n\\- Qdrant â†’ 4,133\n\n\n\nIngest QPS:\n\n\\- Hyperspace v2.0 â†’ 59,208\n\n\\- Milvus â†’ 28,173\n\n\\- Qdrant â†’ 2,102\n\n\n\nDocker image size:\n\nâ†’ 230MB\n\n\n\nServerless behavior:\n\n\\- Inactive collections evicted from RAM\n\n\\- Sub-ms cold wake-up\n\n\\- Native multi-tenancy via header isolation\n\n\n\nThe interesting part for us is not just raw QPS.\n\nItâ€™s that performance scales linearly with CPU cores without degrading under 1000 concurrent clients.\n\nNo read locks.\n\nNo global contention points.\n\nNo latency spikes.\n\n\n\nWould love feedback from people who have profiled high-concurrency vector search systems.\n\nRepo: [https://github.com/YARlabs/hyperspace-db](https://github.com/YARlabs/hyperspace-db)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r7ds0f/hyperspacedb_v20_lockfree_serverless_vector_db/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5x5ovy",
          "author": "ahaw_work",
          "text": "Could you create benchmark for smaller amount od connections and bigger amount of dimensions?\nIn what hyperapace is worse than qdrant or milvus",
          "score": 5,
          "created_utc": "2026-02-17 19:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x7gd9",
              "author": "Sam_YARINK",
              "text": "We're using VectorDBBench datasets for testing, so you can pick from any of the 17 datasets in the /benchmark/ folder. Plus, we've put together our own big stress test that shows some really important numbers.",
              "score": 2,
              "created_utc": "2026-02-17 19:51:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5xmgoj",
              "author": "Sam_YARINK",
              "text": "BTW, what's your case? What dimension do you need?",
              "score": 1,
              "created_utc": "2026-02-17 21:02:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64thvr",
                  "author": "ahaw_work",
                  "text": "As I'm using qdrant for my private project and I have no performance issues I'm just curious :)Â ",
                  "score": 1,
                  "created_utc": "2026-02-18 21:55:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xnd6l",
          "author": "-Cubie-",
          "text": "Nice! Can I use this with local embedding models?",
          "score": 2,
          "created_utc": "2026-02-17 21:06:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xopqo",
              "author": "Sam_YARINK",
              "text": "Definitely yes. Local or by API. Set the embedding config in the .env file.\nRead the documentation about embedding in docs/book/src/",
              "score": 2,
              "created_utc": "2026-02-17 21:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5yzrw3",
                  "author": "Haunting-Elephant587",
                  "text": "Apache/MIT license might have wider adoption",
                  "score": 2,
                  "created_utc": "2026-02-18 01:18:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66jaoh",
          "author": "New_Animator_7710",
          "text": "This is seriously impressive.\n\nA lot of systems blame â€œalgorithm limitsâ€ for performance ceilings, but you went after the real culprit: synchronization. Removing read locks from the hot path is a big deal â€” especially at 1000 concurrent clients. Thatâ€™s usually where things start falling apart.",
          "score": 1,
          "created_utc": "2026-02-19 03:36:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r898lz",
      "title": "Need Advice on RAG App in .net",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r898lz/need_advice_on_rag_app_in_net/",
      "author": "BalanceThen8642",
      "created_utc": "2026-02-18 17:24:35",
      "score": 11,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "I am working on a internal RAG App for my company, where the knowledge base will be comprising of several apps, each with its documentation link sources, Databases and JSON documents. I need your guys advice on the following architecture to ensure my RAG is not only just working but the answers are actually good.\n\nHere's my architecture flow:\n\n1. User will ask a question about some app, then have a router which uses keyword matching initially, falling back to LLM-based routing to determine which data source is the best one.\n2. Then do I need a query transformation like multi-querying and step back to get a better phrased question. Is this step necessary or its just a overhead?\n3. Then using [Qdrant](https://qdrant.tech/documentation/) for vector database which will embed the documents/databases/links on start-up(currently basically creates a snapshot of the data on initial app start-up) and basically do semantic search  usingÂ `sentence-transformers/all-MiniLM-L6-v2`Â embeddings.\n4. Cross-encoder model to score and filter documents from retrieved documents, based on query-document relevance, reducing hallucinations by excluding low-confidence results , taking the top 8 docs since creating a local version of my age old pc.\n5. Answer Generation\n6. On start-up, the application exports database tables and Confluence pages to JSON documents, which are then chunked into 512 even sized chunks with text overlapping and embedded into Qdrant alongside static JSON documents. Is this static method of snapshotting the DB or better creating a pipeline which will repeat after some days\n\n6)Here are my model choices are they good for my self hosted application\n\n* LLM:Â `llama-3.3-70b-versatile`\n* Embedding:Â `sentence-transformers/all-MiniLM-L6-v2`\n* Reranker:Â `cross-encoder/ms-marco-TinyBERT-L-6-v2` If not, what alternatives would you suggest?\n\nAny suggestions or things i can improve upon would be appreciated!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r898lz/need_advice_on_rag_app_in_net/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o63jfg2",
          "author": "pl201",
          "text": "I feel you donâ€™t have a good understanding on how a RAG system works. I may be wrong, so donâ€™t be upset on my assessment.\nTake the example of your #1, user asks a question about the doc and your RAG used the keyword to match. Match what? A link to the doc to your app or the link to the content of related doc? You donâ€™t mention how do you build the system. Thatâ€™s the most important part. After the initial key word match, you are going to send to LLM to determine which one is the best answer, but how a LLM knows? LLM has a general knowledge on about everything but donâ€™t have any knowledge on your document, thatâ€™s reason you want to have RAG in the first place.",
          "score": 2,
          "created_utc": "2026-02-18 18:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66u59t",
              "author": "BalanceThen8642",
              "text": "So the reason behind routing here is that there are fixed number of apps so if a app name appears in query can use keyword matching to get that particular apps knowledge base (json+db+link). \n\nThen sometimes the user might not specify the exact keyword so need a fallback to direct to the correct app data instead of searching through all the app data. Is there any other way then LLM Routing or would Embedding based routing be better here?",
              "score": 1,
              "created_utc": "2026-02-19 04:48:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o638g3m",
          "author": "BalanceThen8642",
          "text": "Not really that much experienced in RAG so there might be some dumbness in architecture : (",
          "score": 1,
          "created_utc": "2026-02-18 17:36:35",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o63f77l",
          "author": "ChapterEquivalent188",
          "text": "The architecture is solid, but the snapshot approach will annoy you in the long run. Invest in the incremental pipeline--- some things i have in mind:\n\nQwen2.5-72B - Mixtral 8x22B for better tool calling only if wnated, testing is key  --minor\nsemantic chunking -\nbm25/key -\nlater --> caching and metadata filter\n\n\nthank me later for this: get ongoing (if things change) golden sets for testing while implementing and never ever use the happy path in business related RAG systems ;)",
          "score": 1,
          "created_utc": "2026-02-18 18:06:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o66tohi",
              "author": "BalanceThen8642",
              "text": "Thanks this is very helpful. My current implementation already moved from startup snapshot to a scheduled ingestion service. Not sure about qwen/mistral haven't really tried them yet are they better then llama?",
              "score": 1,
              "created_utc": "2026-02-19 04:44:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o67ja45",
                  "author": "ChapterEquivalent188",
                  "text": "its more like i was testing around a bit with models during the weekend and my contextwindow was a bit dirty ;) Its realy something minor and more squeezing a bit more---- the real big steps are in the semnantic chunking for example depending on the sort of data you want to ingest --- my next step would be to focus completely on ingestion and soving the \"garbage in\" problem.... all the tech and llm will not perform as long as they have to handel wordsoup and garbage....if you need some golden sets to test your ingest and rag let me know..... happy path will never make you happy ;) \n\n\nits a complete new world as soon as you know you can trust the answers of your llm ",
                  "score": 1,
                  "created_utc": "2026-02-19 08:16:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o63ftcg",
          "author": "z0han4eg",
          "text": "If your target language is English BGE Reranker works good",
          "score": 1,
          "created_utc": "2026-02-18 18:09:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6krt1n",
          "author": "ampancha",
          "text": "Your retrieval pipeline is solid, but the gap I'd flag is what happens when the router sends a user to the wrong source, or a prompt injection in the query tricks it into leaking docs from an app they shouldn't access. Multi-source RAG needs per-source access controls and input validation before it's safe for internal users. The static snapshotting question is secondary to whether you have observability on retrieval failures and data freshness. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-21 09:36:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r81wg6",
      "title": "What chunking mistakes have cost you the most time to debug?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r81wg6/what_chunking_mistakes_have_cost_you_the_most/",
      "author": "Comfortable-Junket50",
      "created_utc": "2026-02-18 12:39:07",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "I've been researching RAG failure patterns for a few months and one thing kept coming up: most pipeline failures I looked at didn't trace back to the generation model. They traced back to how the data was chunked before retrieval even happened.\n\nThe pattern looks like this: vector search returns chunks that score high on relevance. The LLM generates a confident, well-formatted answer. But the answer is wrong because the chunk boundary split a piece of context that needed to stay together. Swapping in a stronger model doesn't help here. It just produces more convincing hallucinations from the same incomplete context.\n\nThree patterns that consistently helped in the systems I studied:\n\n**Parent-child chunking:** You index small child chunks for retrieval precision, but at generation time you pass the larger parent chunk so the model gets surrounding context. LlamaIndex has a good implementation of this with theirÂ `AutoMergingRetriever`. This alone caught a big chunk of \"almost right but wrong\" failures.\n\n**Hybrid retrieval (vector + BM25):** Pure embedding search misses exact-match terms. Product names, error codes, config values, specific IDs. Running BM25 keyword search alongside vector retrieval and merging results (Reciprocal Rank Fusion works well here) picks up what embeddings miss. Langchain and Haystack both support this pattern out of the box.\n\n**Self-correcting retrieval loops:** Before returning a response, the pipeline evaluates whether the answer is actually grounded in the retrieved chunks. If groundedness scores low, it reformulates the query and retries. The original Self-RAG paper by Asai et al. (2023) covers this well, and CRAG (Corrective RAG) by Yan et al. (2024) extends it further.\n\nI am just curious what others here have run into.   \nWhat has been your worst chunking or retrieval failure?   \nThe kind where everything looked fine on the retrieval side until you actually checked the output against the source documents.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r81wg6/what_chunking_mistakes_have_cost_you_the_most/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o67xa2v",
          "author": "SharpRule4025",
          "text": "The one that burned me the worst was splitting on fixed token counts without checking for structural boundaries. Had a pipeline where table rows kept getting split across two chunks. The embedding for each chunk was technically relevant to the query, retrieval scored it high, but the model was generating answers from half a table row. Took weeks to trace because the outputs looked confident and formatted correctly.\n\nThe fix wasn't a better chunking strategy though. It was fixing what went into the chunker. The source data was markdown with navigation elements, breadcrumbs, and related links all mixed in with the actual content. Once I switched to extracting just the content with typed fields (paragraphs, tables as arrays, headings as hierarchy), the chunking almost didn't matter anymore because each field was already a meaningful unit.\n\nSecond one was overlapping chunks that shared boilerplate text. Footer content appearing in 40% of chunks meant the embedding space was polluted. Retrieval kept returning chunks that scored high because they all shared the same footer, not because they were actually relevant.",
          "score": 1,
          "created_utc": "2026-02-19 10:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o685nv0",
          "author": "debrajsingha",
          "text": "u/Comfortable-Junket50 interesting task, was curious to know what are the sources to study RAG failure patterns, would be helpful if shared that as well...",
          "score": 1,
          "created_utc": "2026-02-19 11:45:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68eba0",
              "author": "Comfortable-Junket50",
              "text": "u/Comfortable-Junket50 the source is in the comment. ",
              "score": 1,
              "created_utc": "2026-02-19 12:48:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o68ai5x",
          "author": "RobertLigthart",
          "text": "biggest one for me was chunk overlap being too small. had it set to like 50 tokens and kept getting answers that were technically correct but missing critical context because the important bit fell right at the boundary between two chunks\n\n  \nbumped overlap to 20-25% of chunk size and suddenly the retrieval quality jumped noticeably. felt stupid in hindsight but its one of those settings you just set to some default and forget about\n\n  \nalso +1 on the hybrid retrieval thing... pure vector search is terrible for anything with specific identifiers or product names",
          "score": 1,
          "created_utc": "2026-02-19 12:21:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68e6cl",
          "author": "Comfortable-Junket50",
          "text": "I actually put together a free handbook on these patterns while doing technical writing. Covers the architecture side in more detail: [here](https://futureagi.com/advanced-rag-patterns?utm_source=1902advancedragreddit&utm_medium=organic&utm_campaign=content_distribution)",
          "score": 0,
          "created_utc": "2026-02-19 12:47:06",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1r69ion",
      "title": "Document ETL is why some RAG systems work and others don't",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r69ion/document_etl_is_why_some_rag_systems_work_and/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-16 13:24:03",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 0.82,
      "text": "I noticed most RAG accuracy issues trace back to document ingestion, not retrieval algorithms.\n\nStandard approach is PDF â†’ text extractor â†’ chunk â†’ embed â†’ vector DB. This destroys table structure completely. The information in tables becomes disconnected text where relationships vanish.\n\nBeen applying ETL principles (Extract, Transform, Load) to document processing instead. Structure first extraction using computer vision to detect tables and preserve row column relationships. Then multi stage transformation: extract fields, normalize schemas, enrich with metadata, integrate across documents.\n\nThe output is clean structured data instead of corrupted text fragments. This way applications can query reliably: filter by time period, aggregate metrics, join across sources.\n\nETL approach preserved structure, normalized schemas, delivered application ready outputs for me.\n\nI think for complex documents where structure IS information, ETL seems like the right primitive. Anyone else tried this?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r69ion/document_etl_is_why_some_rag_systems_work_and/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5ooe2y",
          "author": "Opening_Highlight241",
          "text": "Unstract(open-source, AGPL) does this i guess [https://github.com/Zipstack/unstract](https://github.com/Zipstack/unstract)",
          "score": 3,
          "created_utc": "2026-02-16 14:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5og3vi",
          "author": "Independent-Cost-971",
          "text": "Wrote up a more detailed explanation if anyone's interested:Â [https://kudra.ai/structure-first-document-processing-how-etl-transforms-rag-data-quality/](https://kudra.ai/structure-first-document-processing-how-etl-transforms-rag-data-quality/)\n\nGoes into the four ETL stages (extraction, structuring, enrichment, integration), layout-aware extraction workflows, field normalization strategies, and full production comparison. (figured it might help someone).",
          "score": 3,
          "created_utc": "2026-02-16 13:24:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5pblt0",
          "author": "penguinzb1",
          "text": "the etl framing makes sense. most people treat rag accuracy like a retrieval problem when it's actually a data quality problem upstream. if your chunks are corrupted from the start, no amount of fancy embedding models or reranking will fix it.\n\nthe table structure thing is brutal. i've seen rag systems answer \"what were q3 revenues\" by pulling three random numbers from different rows because the chunker turned a table into line-separated text. the relationships just vanish.\n\nwe use Veris to test these failure modes at scale. you can set up eval scenarios where the ground truth answer requires preserving table structure or cross-document joins, then see if your rag pipeline actually retrieves and surfaces the right data. turns out most naive chunking strategies fail on anything beyond single-paragraph qa.\n\ncurious how you're handling the tradeoff between structure preservation and retrieval speed. etl pipelines can get expensive if you're normalizing schemas across thousands of documents in real time.",
          "score": 2,
          "created_utc": "2026-02-16 16:07:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ppel3",
          "author": "vlg34",
          "text": "Scanned documents are notoriously hard for RAG because OCR destroys table structure. \n\nLook into layout-aware models like LayoutLM or tools that use vision models to understand structure. \n\nAlternatively, Marker or Docling can handle this better than traditional OCR, but they're not perfect either.",
          "score": 1,
          "created_utc": "2026-02-16 17:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q7oo5",
          "author": "bjl218",
          "text": "Most of our data is structured in spreadsheets. We have an ETL workflow that normalizes headers and data and stores the result in a SQL db. We then have a chatbot interface that does prompt to SQL. This seems to be working fairly well for the structured data. For unstructured data, Iâ€™ll probably ingest into a vector DB or OpenSearch. For structured data where we donâ€™t know the general structure ahead of time, weâ€™ll probably also put this in OpenSearch. Still working on that particularly",
          "score": 1,
          "created_utc": "2026-02-16 18:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rqulc",
          "author": "cointegration",
          "text": "Tried using Qwen3 vl 8b, it took forever to get through the document, not a viable solution if you have thousands of docs",
          "score": 1,
          "created_utc": "2026-02-16 23:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61te15",
          "author": "Informal_Tangerine51",
          "text": "100% agree, especially for tables. Once you flatten a table into text, you lose the relationships that actually answer the question (row/column, units, time periods), and retrieval canâ€™t recover that no matter how fancy your reranker is.\n\nThe best pattern Iâ€™ve seen is â€œdual indexâ€: keep a structured store (rows/fields with provenance back to page/box coords) for anything tabular, and keep a text index for narrative sections. Then your app (or agent) decides which path to query first based on the question, and you can do real filtering/aggregation without asking the LLM to do math over mangled chunks.\n\nThe tradeoff is ingestion becomes a product, not a script: you need schema versions, quality checks, and a way to detect when extraction drifted. Whatâ€™s your current approach to provenance (linking a structured cell back to the exact spot in the PDF) and handling revised docs over time?",
          "score": 1,
          "created_utc": "2026-02-18 13:33:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}