{
  "metadata": {
    "last_updated": "2026-03-01 03:29:57",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 137,
    "file_size_bytes": 161100
  },
  "items": [
    {
      "id": "1rcba6y",
      "title": "What's the best embedding model for RAG in 2026? My retrieval quality is all over the place",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-23 07:44:08",
      "score": 60,
      "num_comments": 39,
      "upvote_ratio": 0.94,
      "text": "I've been running a RAG pipeline for a legal document search tool.\n\nCurrently using OpenAI text-embedding-3-large but my retrieval precision is around 78% and I keep getting irrelevant chunks mixed in with good results.\n\nI've seen people mention Cohere embed-v4, Voyage AI, and Jina v3. Has anyone done real benchmarks on production data, not just MTEB synthetic stuff?\n\nSpecifically interested in retrieval accuracy on domain-specific text, latency at scale (10M+ docs), and cost per 1M tokens.\n\nWhat's working for you in production?\n\njust got access to zeroentropy's embeddings. amazing stuff! [zeroentropy.dev](http://zeroentropy.dev)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1lo2",
          "author": "ChapterEquivalent188",
          "text": "garbage in, garbage out.... semantic chunking and a lot of stuff on ingest AND retrieval side. its all here https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit as a basis system to build on it or the long run you may read here https://github.com/2dogsandanerd/RAG_enterprise_core  have fun. im tired of reading every day bout wrappers which you cant trust..... for legal docs you may need a auditrail as well also for ingest AND retrieval-----\nnever use the happy path as it will never make you happy",
          "score": 26,
          "created_utc": "2026-02-23 07:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0acu",
              "author": "revovivo",
              "text": "but how does it solve the problem of chunk sizing and retrieval ",
              "score": 1,
              "created_utc": "2026-02-23 16:11:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74vkvz",
                  "author": "ChapterEquivalent188",
                  "text": "semantic chunking for a start ? ",
                  "score": 1,
                  "created_utc": "2026-02-24 13:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7nz2nh",
              "author": "Transcontinenta1",
              "text": "Thanks for your work. This sub has made me wait, but I can’t wait much longer for improvement. I think there’s an answer: which one fits my use case. We build filling machinery to spec for major companies. We have unique models, so it’s hard to have service techs who know all lines. This knowledge gap is challenging when only one person knows a machine and its models. A group usually has to figure out what’s wrong when one person would know. I want a comprehensive knowledge base of our manuals, service reports, BOMs, and recorded calls that we’ll summarize. I’d love to get our cad drawings in there. \n\nWhat trusted sources did you use during your learning journey? What sources would you avoid if you knew then? Can’t wait to look your gits over. Thanks again\n\nEdit: it’s a mix of PDFs, solidworks slddrw > PDF, scanned pdf, and a lot of manuals are paper Belden the year 2008? lol we have been around a lot longer but there is a cutoff bc they went digital at some point. \n\nWe also ingested 4-5 companies in the last decade.",
              "score": 1,
              "created_utc": "2026-02-27 07:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pbe6o",
                  "author": "ChapterEquivalent188",
                  "text": "This is a 'Hell Level' RAG case which i love ;) \n\nIndustrial machinery + 2008 Scans + BOMs is where 99% of tutorials fail \nbecause standard parsers destroy the table structure.\n\nEven with Docling (which ClawRAG uses), scanned BOMs are the final boss. If the OCR misreads a part number in a table, your technician orders the wrong part\n\nIf you have a nasty page (anonymized scan with a table), DM me a link. I’ll run it through my PantheonRAG and send you the raw parsed database (SQLite/Chroma) back\n\nThat way you see exactly if the data survives the ingestion process before you invest time setting it up. If my engine fails, even after the HITL at least you know the limit\n\nQuick tip on CAD: RAG can't read .slddrw natively yet. Batch-export them to 2D PDFs with text labels first ",
                  "score": 1,
                  "created_utc": "2026-02-27 14:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x57m7",
              "author": "krimpenrik",
              "text": "Great resources!! \n\nHave you tested both?",
              "score": 0,
              "created_utc": "2026-02-23 08:25:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x6td3",
                  "author": "ChapterEquivalent188",
                  "text": "sort of ;) working everyday on it to make it perfect..will release some plugins and extensions to the opensourced ones soon\n\n\nedit: if you r intrested of the outcome just send me a set of docs and ill provide you with the rag for retrieval testing",
                  "score": 2,
                  "created_utc": "2026-02-23 08:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2u1w",
          "author": "lucasbennett_1",
          "text": "before switching models its worth figuring out whether the irrelevant chunks are a retrieval problm or a chunking problem.. 78% precision on legal docs can come from either, and they need different fixes,, embedding model swaps sometimes help but chunking strategy for legal text matters a lot because clauses and definitions often span weird boundaries that standard splitters handle badly.. that said BGE M3 does tend to outperform text embedding 3- arge on domain-specific retrieval in most comparison.. its available through several providers like deepinfra or huggingface at lower cost than openai embeddings which helps if you are iterating on a 10M doc corpus and running a lot of test queries",
          "score": 6,
          "created_utc": "2026-02-23 08:02:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x74f5",
              "author": "ChapterEquivalent188",
              "text": "this.  domain spec retrieval is one importend part but never can solve the garbage in and wordsalad problem ",
              "score": 1,
              "created_utc": "2026-02-23 08:44:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78n3xq",
              "author": "liannehynes",
              "text": "I've tried Zeroentropy Embedddings (still in beta) and it outperforms ALL of the above ;) ",
              "score": 0,
              "created_utc": "2026-02-25 00:34:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78q93v",
                  "author": "No_Injury_7940",
                  "text": "+1",
                  "score": 0,
                  "created_utc": "2026-02-25 00:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2nwf",
          "author": "crewone",
          "text": "We have done extensive tests for book data. Voyage is always the best, but also has a high latency. A local Qwen3 embedder delivers 90% of that performance at a fraction of the cost and latency. The rest comes down to chunking strategies.",
          "score": 10,
          "created_utc": "2026-02-23 08:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x7dyb",
              "author": "picturpoet",
              "text": "How do you decide on one chunking strategy versus the other? Are there any best practises for different types of documents/document structures ",
              "score": 1,
              "created_utc": "2026-02-23 08:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xlr9w",
                  "author": "crewone",
                  "text": "Read up on strategies. Then test and compare. (For large book context we use context-aware chunking)",
                  "score": 2,
                  "created_utc": "2026-02-23 11:06:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78njr2",
              "author": "liannehynes",
              "text": "I've tried zeroentropy embeddings and it outperforms voyage :) ",
              "score": 1,
              "created_utc": "2026-02-25 00:37:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c0mrs",
                  "author": "crewone",
                  "text": "Then use that.",
                  "score": 1,
                  "created_utc": "2026-02-25 14:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2cp5",
          "author": "thecontentengineer",
          "text": "Have you tried ZeroEntropy Embeddings? They’re as good as their rerankers. You should always use ZeroEntropy, even for embeddings!!",
          "score": 15,
          "created_utc": "2026-02-23 07:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xa7he",
              "author": "DarfleChorf",
              "text": "Wait they have embeddings too? I only tried their reranker, didn't realize they had an embedding model. Might give it a shot.",
              "score": 2,
              "created_utc": "2026-02-23 09:15:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78hcow",
                  "author": "thecontentengineer",
                  "text": "Yes they have embeddings and they’re the best we have tried. Way better than Cohere and Voyage.",
                  "score": 0,
                  "created_utc": "2026-02-25 00:03:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78hoxb",
              "author": "liannehynes",
              "text": "yes we did!! ZeroEntropy has the best embeddings compared to all other providers.",
              "score": 1,
              "created_utc": "2026-02-25 00:05:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78hymx",
                  "author": "DarfleChorf",
                  "text": "How do you get access?",
                  "score": 2,
                  "created_utc": "2026-02-25 00:07:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xaud9",
          "author": "thefishflaps",
          "text": "Yeah ZeroEntropy reranker is solid, and they just dropped embeddings too. Worth checking out.",
          "score": 5,
          "created_utc": "2026-02-23 09:21:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x59v8",
          "author": "xeraa-net",
          "text": "Jina v5 just came out; especially for the model size pushing the state of the art: https://jina.ai/news/jina-embeddings-v5-text-distilling-4b-quality-into-sub-1b-multilingual-embeddings/\nBut it will of course always depend on the domain, language, cleanliness of data, quantization,…\n\n\nDisclaimer: I work for Elastic.",
          "score": 2,
          "created_utc": "2026-02-23 08:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yrokw",
          "author": "tom_at_zedly",
          "text": "On legal docs, the model usually isn't the failure point, it's almost always the chunking.\n\nLegal text has too many nested clauses and cross-references that get cut off by standard recursive splitters just butcher. We’ve found that even basic embedding models work fine if you fix the chunking to keep definitions with their clauses (or use semantic chunking). That usually moves the needle way more than switching from OpenAI to Voyage or BGE.",
          "score": 2,
          "created_utc": "2026-02-23 15:31:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z9oxo",
          "author": "remoteinspace",
          "text": "we've tried a bunch at papr and qwen 4b is best based on our evals",
          "score": 2,
          "created_utc": "2026-02-23 16:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70qww8",
          "author": "liannehynes",
          "text": "ZeroEntropy have SOTA embeddings but i think they're still on beta",
          "score": 2,
          "created_utc": "2026-02-23 21:04:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5ax8",
          "author": "CEBarnes",
          "text": "I’ve had good reliable results with SPL files. (https://dailymed.nlm.nih.gov/dailymed/index.cfm). Step one was to spend 5-6 months building a parser that normalized and populated a database.\n\nIn your case, given you have doc files, would be to structure the data. Headings, line numbers, citations, parties, proposed order, etc. Stack on a categorizer and add its results to the structure. Build a specific schema that has enough flexibility that you don’t end up with edge cases everywhere. Once you have semi structured data, then populate an old school database and build an API. Lastly, create the skills the AI needs to “intelligently,” use the API. \n\nGranted, even if vibed all the way to the end, this is likely a 9 to 12 month endeavor. But, your results will be magical.",
          "score": 1,
          "created_utc": "2026-02-23 13:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74rdak",
          "author": "Ascending_Valley",
          "text": "I use sentence transformer (and others) and then reduce and transform with a method that weights toward known similar samples. I end up in R50 with simple weighted distance being very effective.",
          "score": 1,
          "created_utc": "2026-02-24 13:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vyq85",
          "author": "miff_miffy923",
          "text": "if the retrieval is not stable maybe it's not because of the embedding. by chunking and retrieval runtime creates better result than just switching embedding models. for 10M+ docs, network latency to vector DB may introduce variance in top-k results. moving to a local semantic runtime (e.g., moss local or runtime level semantic search) may create more stable precision, maybe can also consider hybrid search + reranking.",
          "score": 1,
          "created_utc": "2026-02-28 14:56:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcb47i",
      "title": "My RAG retrieval accuracy is stuck at 75% no matter what I try. What am I missing?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "author": "Equivalent-Bell9414",
      "created_utc": "2026-02-23 07:34:01",
      "score": 57,
      "num_comments": 47,
      "upvote_ratio": 0.93,
      "text": "I've been building a RAG pipeline for an internal knowledge base, around 20K docs, mix of PDFs and markdown. Using LangChain with ChromaDB and OpenAI embeddings.\n\nI've tried different chunk sizes (256, 512, 1024), overlap tuning, hybrid search with BM25 plus vector, and switching between OpenAI and Cohere embeddings.\n\nStill hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\n\nIs this a chunking problem or an embedding problem? What else should I be trying? Starting to wonder if I need to add a reranking step after retrieval but not sure where to start with that.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1e94",
          "author": "xpatmatt",
          "text": "I had an issue where a lot of documents and data included very similar terms used in very different contexts which made retrieval for any particular query difficult due to irrelevant retrievals. \n\nI had to segment the docs/data into six different vector DBS based on user intent and route queries to the appropriate DB based on the user's intent. Works great now.",
          "score": 16,
          "created_utc": "2026-02-23 07:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x3r92",
              "author": "redditorialy_retard",
              "text": "could you tell me more about this? very intrigued ",
              "score": 2,
              "created_utc": "2026-02-23 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x41ly",
                  "author": "xpatmatt",
                  "text": "Yes if you have questions I can answer them",
                  "score": 2,
                  "created_utc": "2026-02-23 08:14:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ykvj4",
              "author": "Cool_Injury4075",
              "text": "Did you try using a reranker? Unlike embeddings, rerankers can understand the intent of the question and filter out all the junk (unless, of course, the documents need context).",
              "score": 2,
              "created_utc": "2026-02-23 14:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xazxb",
          "author": "adukhet",
          "text": "Your problem is not embeddings, try below \n-if you chunk purely by token length, try markdown aware or/and semantic chunking\n-use rerankers but consider latency. Cross-encoders likely fixes semantically similar but irrelevant issues- but if not try late-interaction\n-try query rewriting/query expansion (e.g. HyDE)\n\nBut most importantly you must diagnose where failure arise before changing architecture",
          "score": 5,
          "created_utc": "2026-02-23 09:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x5f9u",
          "author": "ampancha",
          "text": "Reranking with a cross-encoder will likely push you past 80%, but persistent semantic pollution usually means chunking isn't preserving document boundaries or metadata context. The harder problem: your eval set won't cover the queries that actually break in production. You need per-query observability to see which retrievals are failing live, not just aggregate precision. Sent you a DM",
          "score": 5,
          "created_utc": "2026-02-23 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xi3nk",
              "author": "welcome-overlords",
              "text": "Im probably having similar issues. Would be interested in hearing more in DM",
              "score": 1,
              "created_utc": "2026-02-23 10:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xdn8f",
          "author": "StuckInREM",
          "text": "I think sharing a complete pipeline of what you are doing would be useful, what do your metadata look like for the documents to enanche the retrieval phase? recursive split chunking is for sure not optimal, what do your document structure look like in terms of paragraphs? have you tried with a reranker?",
          "score": 3,
          "created_utc": "2026-02-23 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x0fc9",
          "author": "grabGPT",
          "text": "Are you using OCR on PDFs? Have you checked the accuracy?",
          "score": 2,
          "created_utc": "2026-02-23 07:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmckb",
              "author": "Transcontinenta1",
              "text": "I am trying to use deepseeks ocr. Is there a better free one?",
              "score": 1,
              "created_utc": "2026-02-23 11:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ue1h0",
                  "author": "Agitated_Heat_1719",
                  "text": "There are tons of libraries for python only. Not all work the same way. \n\nPDF (structure) parsing libraries are fast, but have issues with some encodings or PDF text representations.\n\nOCR based implementations are waay slower (`marker`, `pytesseract`, `docTR`...)\n\nThis is how my extraction folder[s] look like:\n\n```\n.\n├── images\n│   └── py\n│       ├── minecart\n│       ├── pikepdf\n│       ├── PyMuPDF-fitz\n│       └── PyPDF2-pypdf\n├── tables\n│   └── py\n│       ├── camelot\n│       ├── docling\n│       ├── gmft\n│       ├── marker\n│       ├── pdfplumber\n│       └── tabula-py\n└── text\n    └── py\n        ├── docling\n        ├── docTR\n        ├── kreuzberg\n        ├── marker\n        ├── markitdown\n        ├── MarkItDown\n        ├── pdfminer_six\n        ├── pdfplumber\n        ├── PyMuPDF_fitz\n        ├── pymupdf4llm\n        ├── PyPDF2\n        ├── pypdfium2\n        ├── pytesseract\n        └── unstructured\n```\n\nUsers need to play with those and see what works for them and their corpus.\n\nHope this helps.",
                  "score": 2,
                  "created_utc": "2026-02-28 07:26:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o701n3s",
                  "author": "grabGPT",
                  "text": "It depends on whether the documents you're using are handwritten notes or machine printed. In both cases, accuracy will vary",
                  "score": 1,
                  "created_utc": "2026-02-23 19:04:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x1sk8",
          "author": "ggone20",
          "text": "Not enough information to answer your question. What does your corpus look like?",
          "score": 2,
          "created_utc": "2026-02-23 07:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x23jw",
          "author": "AmbitionCrazy7039",
          "text": "You need structural filtering. Try to classify your documents as precise as possible. Maybe you want to build some relational database around it. \n\nFor example, if you query the Knowledge Base for some „Manual X“ question, you only want to search similiar manuals. BM25 is only keyword search, most likely not sufficient. In this example keyword filtering might suggests non-manuals because other docs may relate more often to manuals.",
          "score": 2,
          "created_utc": "2026-02-23 07:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mrh0",
          "author": "Tough-Survey-2155",
          "text": "You need Agentic router: https://github.com/hamzafarooq/multi-agent-course/tree/main/Module_3_Agentic_RAG",
          "score": 2,
          "created_utc": "2026-02-24 07:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x9817",
          "author": "Glass-Combination-69",
          "text": "Throw it into cognee and see if you get 100%. Graph might be what’s missing",
          "score": 1,
          "created_utc": "2026-02-23 09:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ya4xo",
          "author": "jrochkind",
          "text": "I am not an expert, but have you tried cross-encoder re-ranking?  (Over-fetching, then re-ranking to get your K). \n\nI have not yet myself, but have been considering it.  Oh from your last line it sounds like you too have been considering it but have not tried it. I think that's what would make sense to try?  I would be curious to your results. \n\nI haven't done it, but it seems pretty straightforward, you just feed your over-fetched results to the re-ranker, with your query, and it reorders them, hopefully putting the less relevant ones at the bottom and out of your final selection slice.",
          "score": 1,
          "created_utc": "2026-02-23 13:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yatxf",
          "author": "code_vlogger2003",
          "text": "Hey have you stored any metadata for every chunk such that in the first hand you can verify that my retrieval step is actually returning the exact relevant ground truth answer page numbers or not etc. In this step you can identify whether it's the chunking issue or embedding drift etc.",
          "score": 1,
          "created_utc": "2026-02-23 14:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yk2by",
          "author": "Dense_Gate_5193",
          "text": "Have you tried using RRF with reranking instead?\n\nNornicDB uses BM25+vector search and uses a reranking model (BYOM) https://github.com/orneryd/NornicDB",
          "score": 1,
          "created_utc": "2026-02-23 14:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytm77",
          "author": "namognamrm",
          "text": "You did rerank?",
          "score": 1,
          "created_utc": "2026-02-23 15:40:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za0tz",
          "author": "remoteinspace",
          "text": "have you tried using a knowledge graph? that worked well for us at papr.. got us 92% retrieval accuracy (top 5 results) on stanford's stark benchmark which has arxiv like docs in their data set. dm me and i can help",
          "score": 1,
          "created_utc": "2026-02-23 16:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70caqp",
          "author": "blue-or-brown-keys",
          "text": "\"Still hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\"\n\nTry synthetic data? Summarize the document , store the summary and drop the document. ",
          "score": 1,
          "created_utc": "2026-02-23 19:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z56f",
          "author": "Much-Researcher6135",
          "text": "yes pull back 3x and use a reranker",
          "score": 1,
          "created_utc": "2026-02-24 04:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z5te",
          "author": "WorkingOccasion902",
          "text": "Have you considered Knowledge Graphs?",
          "score": 1,
          "created_utc": "2026-02-24 04:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mov3",
          "author": "Informal-Victory8655",
          "text": "Change embeddings model",
          "score": 1,
          "created_utc": "2026-02-24 07:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75u4y3",
          "author": "TransportationFit331",
          "text": "I recommend Mastra.ai",
          "score": 1,
          "created_utc": "2026-02-24 16:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hij9h",
          "author": "cointegration",
          "text": "what you need is a cross encoder, it ties the query back to the chunks retrieved to maximise relevance",
          "score": 1,
          "created_utc": "2026-02-26 09:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hk0by",
          "author": "cointegration",
          "text": "1) different types of docs do better with different chunk sizes and different retrieval methods  \n2) long text essays (manuals, legal docs, fiction etc) benefit from larger chunks, vector search and knowledge graphs  \n3) tabular, itemised, charts or docs with many tables (invoices, receipts, performance reports etc) benefit more from BM25 against extracted metadata and shorter chunks  \n4) its a balance between precision, recall and semantic relevance, but strategies must exist for all 3. BM25 for precision, vector search for recall, knowledge graphs for semantic relevance.  \n5) use a cross encoder and rerank above 3, apply your own tweekable weights",
          "score": 1,
          "created_utc": "2026-02-26 09:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mbg9e",
          "author": "RecommendationFit374",
          "text": "We do semantic and graph aware hierarchal chunking, re ranking and query expansion. The problem you have is embeddings only capture semantic meanings once you have large document corpus your hitting physical limits on vector dimensionality. \n\nYou end up having so much noise where it’s hard to make the right signal sharp enough. \n\nFor example if you have “I am very happy” or “I am not very happy” both are close in cosine similarity but carry different meanings. Actually, semantic meanings miss graph relationships, temporal sequences, causal… “vitamin E causes cancer” and “vitamin E prevents cancer” also close cosine sim but are very different meanings.\n\nWe mainly use papr.ai - predictive memory architecture that uses vector db, graph (using custom schema) and prediction models which helped us achieve 92% hit@5 in Stanford STARK benchmark MAG dataset.\n\nHappy to help and share our learnings on a call. Free fee to dm me - below is a doc on our chunking technique\n\nhttps://github.com/Papr-ai/memory-opensource/blob/main/docs/features/documents/CONTEXT_AWARE_CHUNKING_ARCHITECTURE.md",
          "score": 1,
          "created_utc": "2026-02-27 01:03:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nb9rb",
          "author": "Pasha_Hu",
          "text": "I have one suggestion for you, don't use vectors embedding. So in rag we convert documents in vectors and then do cosin similarly. But now there is something new which gives more accuracy and accurate answer which is pageIndex this is using tree for tracing the answers. Try this and if it's working then thanks me later with 95% accuracy",
          "score": 1,
          "created_utc": "2026-02-27 04:39:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71nke4",
          "author": "Ok-Attention2882",
          "text": "Skill issue",
          "score": 0,
          "created_utc": "2026-02-23 23:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zct5j",
          "author": "Repulsive-Memory-298",
          "text": "It sounds like a troll but adding porn to your datasets calibrates the vector space",
          "score": -2,
          "created_utc": "2026-02-23 17:09:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rej56g",
      "title": "Lessons from shipping a RAG chatbot to real users (not just a demo)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rej56g/lessons_from_shipping_a_rag_chatbot_to_real_users/",
      "author": "cryptoviksant",
      "created_utc": "2026-02-25 17:09:55",
      "score": 38,
      "num_comments": 23,
      "upvote_ratio": 0.79,
      "text": "I've been building a chatbot product (bestchatbot.io, works on Discord and websites) where users upload their docs and the bot answers questions from that content. Wanted to share some stuff I learned going from \"cool demo\" to \"people are actually paying for this\" because the gap between those two is way bigger than I expected.\n\n**Vanilla RAG gets you maybe 30% of the way there (no joke)**\n\nWhen I started I did the standard thing. Chunk docs, embed them, retrieve top-k, stuff into context, generate. It worked great on demos. Then real users uploaded real docs and it fell apart. The problem isn't retrieval in isolation, it's that real documents have structure, context, and relationships between sections that get destroyed when you just chunk and embed.\n\n**What actually mattered in production**\n\nWithout going too deep into our specific implementation, here's what moved the needle the most:\n\n* **Document quality > retrieval sophistication.** I spent weeks tweaking retrieval and got maybe 10% better. Then I added better doc preprocessing and got a bigger jump overnight. Garbage in garbage out is painfully real.\n* **Evaluation is everything.** You can't improve what you can't measure. I built a testing interface where I could ask questions and see exactly which sources the bot cited. That feedback loop was more valuable than any architecture change.\n* **Users don't care about your retrieval method.** They care about two things: did it answer correctly, and how fast. Our response time is 10-20 seconds which people complain about constantly. Nobody has ever asked me what embedding model we use.\n* **The knowledge base needs to be treated as a living thing.** We added a system where the bot learns from moderator corrections in Discord automatically. That continuous improvement loop has been surprsingly impactful compared to just static doc retrieval.\n\nMost of the accuracy gains came from boring stuff. Better chunking, better preprocessing, better prompting, testing obsessively. The architecture matters but it's maybe 30% of the outcome. The other 70% is everything around it.\n\nCurious what other people building production RAG systems have found. What moved the needle most for your accuracy?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rej56g/lessons_from_shipping_a_rag_chatbot_to_real_users/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7e9r1j",
          "author": "zsrt13",
          "text": "This is basic 101 stuff. There are no lessons here",
          "score": 9,
          "created_utc": "2026-02-25 20:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dll7a",
          "author": "ChapterEquivalent188",
          "text": "jesus another wrapper and chatwithpdf.......move on ....all your text is shouting \"garbage in, garbage out\"  \n\nyou even called it by yourself, so how do you solve it ? you mention so many deadends.....how about semantic chunking ? how a about specialized parsers? what is your outcome without happypath tests ? If you are the world gratetes chef with the world best recipe, what do you get when your Ingrediens is mixed up mud ?",
          "score": 3,
          "created_utc": "2026-02-25 19:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7epzbm",
              "author": "welcome-overlords",
              "text": "Bro get my feet outta your mouth. Fuck im so bored of these ai posts selling their idea. Can i just talk with fellow fucking humans who are trying to solve these problems and maybe hear some ideas",
              "score": 4,
              "created_utc": "2026-02-25 22:15:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7eq3z7",
                  "author": "welcome-overlords",
                  "text": "If u r a fellow human solving thess issues and maybe getting paid for it so u dont want me to pay u, reply",
                  "score": 1,
                  "created_utc": "2026-02-25 22:15:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7h4u7c",
                  "author": "ChapterEquivalent188",
                  "text": "im here ;) always happy to meet peps focusing on the garbage in problem ",
                  "score": 1,
                  "created_utc": "2026-02-26 07:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7dy9vd",
              "author": "cryptoviksant",
              "text": "you wish this was just a wrapper ;) \n\n[bestchatbot.io](http://bestchatbot.io) \n\nHave a look. If you can built a similar thing with only \"chatwithdpf\" wrapper, I'll give you $1000. On god.",
              "score": -4,
              "created_utc": "2026-02-25 20:05:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7h4k2q",
                  "author": "ChapterEquivalent188",
                  "text": "LOL----https://github.com/2dogsandanerd/RAG_enterprise_core\n\nshow me your results on this and i believe everything you claim ;) \nhttps://github.com/2dogsandanerd/Liability-Trap---Semantic-Twins-Dataset-for-RAG-Testing",
                  "score": 2,
                  "created_utc": "2026-02-26 07:06:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dgjwb",
          "author": "rigatoni-man",
          "text": "I’d love to know more about your evaluation.  How does the interface work?  How/what do you evaluate?",
          "score": 1,
          "created_utc": "2026-02-25 18:43:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dygs0",
              "author": "cryptoviksant",
              "text": "The interface is tied to a backend running on FastAPI, which connects the front-end (Vite) with the backend (python). You can actually have a look at [https://bestchatbot.io](https://bestchatbot.io) , any feedback is welcomed!",
              "score": 0,
              "created_utc": "2026-02-25 20:06:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7eaaav",
                  "author": "rigatoni-man",
                  "text": "Ah I meant how do you / did you test to validate your strategies?",
                  "score": 2,
                  "created_utc": "2026-02-25 21:01:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7h4lvl",
                  "author": "ChapterEquivalent188",
                  "text": "LOL",
                  "score": 1,
                  "created_utc": "2026-02-26 07:07:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gbhdp",
          "author": "hhussain-",
          "text": "Lessons learned, many missed usually. Production is not \"yeah... I can build this, nevermind the post\"\n\nAnyway, for performance you might think of using rust. Either as tour backend, or processors and lib used in python. Some oython lib are already rust or C under the hood. FastPAI is good choice, but rust performance is different level.",
          "score": 1,
          "created_utc": "2026-02-26 03:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ggqvn",
          "author": "nbass668",
          "text": "Sorry there is nothing learned. You just spit out something we all do.. and every comment you spamming your website.\n\nReporting this to the mods as Spam sugar coated with a fake knowledge",
          "score": 1,
          "created_utc": "2026-02-26 04:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hsid1",
              "author": "cryptoviksant",
              "text": "sure go ahead",
              "score": 0,
              "created_utc": "2026-02-26 10:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rebd0i",
      "title": "Built a four-layer RAG memory system for my AI agents (solving the context dilution problem)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-25 12:02:23",
      "score": 36,
      "num_comments": 8,
      "upvote_ratio": 0.96,
      "text": "We all know AI agents suffer from memory problems. Not the kind where they forget between sessions but something like context dilution. I kept running into this with my agents (it's very annoying tbh). Early in the conversation everything's sharp but after enough back and forth the model just stops paying attention to early context. It's buried so deep it might as well not exist.\n\nSo I started building a four-layer memory system that treats conversations as structured knowledge instead of just raw text. The idea is you extract what actually matters from a convo, store it in different layers depending on what it is, then retrieve selectively based on what the user is asking (when needed).\n\nDifferent questions need different layers. If someone asks for an exact quote you pull from verbatim. If they ask about preferences you grab facts and summaries. If they're asking about people or places you filter by entity metadata.\n\nI used workflows to handle the extraction automatically instead of writing a ton of custom parsing code. You just configure components for summarization, fact extraction, and entity recognition. It processes conversation chunks and spits out all four layers. Then I store them in separate ChromaDB collections.\n\nBuilt some tools so the agent can decide which layer to query based on the question. The whole point is retrieval becomes selective instead of just dumping the entire conversation history into every single prompt.\n\nTested it with a few conversations and it actually maintains continuity properly. Remembers stuff from early on, updates when you tell it something new that contradicts old info, doesn't make up facts you never mentioned.\n\nAnyway figured I'd share since context dilution seems like one of those problems everyone deals with but nobody really talks about.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7bggdt",
          "author": "Specific_Expert_2020",
          "text": "Awesome!\n\nI am building something very similar at work.. but i cannot disclose to much.\n\nSimilar approach with criteria and fields to help differentiate the data in the system.\n\nIs there a reason you kept it to 4?\n\nI am learning this whole RAG thing and I see the section mentions \"why 4\" but did you limit it? Or was 4 enough to keep the response accurate.\n\nJust curious :)",
          "score": 3,
          "created_utc": "2026-02-25 12:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bkror",
              "author": "Independent-Cost-971",
              "text": "I only used 4 in the blog because it made testing and demonstrating the results much cleaner and easier to follow. When you’re explaining RAG concepts, smaller numbers help keep the examples readable and the behavior obvious.\n\nIn a real project, you’d absolutely use more than 4.",
              "score": 1,
              "created_utc": "2026-02-25 13:19:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bldbj",
                  "author": "Specific_Expert_2020",
                  "text": "Also I hope my comment did not come off as downplaying your post as it is way more technical than what I am working on so great share.\n\nI appreciate the follow up and sharing.\n\nI had concerns of over fielding as I work in a MSSP and thing vary by a variety of fields.\n\nSo as I develop the roll out.. I am seeing the fields scope creeping.\n\nThank you for the insight.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:22:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7coofs",
          "author": "lez566",
          "text": "I did something similar. I built an AI agent that learns as you talk to it and builds a living profile of you and your needs. Then anytime a message is sent, the tool gives this living profile, a keyword search, a semantic search and the last n messages. It’s excellent and never forgets.",
          "score": 2,
          "created_utc": "2026-02-25 16:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b95tc",
          "author": "Independent-Cost-971",
          "text": "I wrote a whole blog about this that goes way deeper if anyone's interested: [https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/](https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/)",
          "score": 4,
          "created_utc": "2026-02-25 12:03:15",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7bq031",
              "author": "arun4567",
              "text": "This is good. I was looking for some thing like this since my agent gets into loops and forgets that its been provided the details before. How do you handle updates of information that's already been provided,",
              "score": 3,
              "created_utc": "2026-02-25 13:48:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qh8id",
          "author": "jd808nyc",
          "text": "Do you have a diagram of the flow? Curious how you’re deciding what not to remember? That’s been the harder problem for me than memory itself.\n\nA lot of these systems slowly turn into hoarding machines where everything feels important until retrieval quality quietly tanks.\n\nHave you tried any kind of decay or scoring over time, or is it all just accumulating right now?",
          "score": 1,
          "created_utc": "2026-02-27 17:32:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf7xf6",
      "title": "What's your experience with hybrid retrieval (vector + BM25) vs pure vector search in RAG systems?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rf7xf6/whats_your_experience_with_hybrid_retrieval/",
      "author": "Beneficial-Grab4442",
      "created_utc": "2026-02-26 11:39:42",
      "score": 27,
      "num_comments": 16,
      "upvote_ratio": 0.94,
      "text": "I've been building RAG systems and recently switched from pure vector \n\nsearch (top-k cosine similarity) to hybrid retrieval combining vector \n\nsearch with BM25 keyword matching.\n\n\n\nThe difference was significant — accuracy went from roughly 60% to 85% \n\non my test set of 50 questions against internal documentation.\n\n\n\nMy theory on why: vector search is great at semantic similarity but \n\nmisses exact terminology. When a user asks, \"What's the PTO policy?\" \n\nthe vector search finds chunks about \"vacation time\" and \"time off \n\nbenefits\" but sometimes misses the exact chunk that uses the acronym \n\n\"PTO.\" BM25 catches that.\n\n\n\nFor those running RAG in production:\n\n\n\n1. Are you using pure vector, hybrid, or something else entirely?\n\n2. How much did re-ranking (cross-encoder) improve your results on top \n\n   of hybrid search?\n\n3. What's your chunk size? I settled on \\~500 chars with 100 overlap \n\n   after a lot of experimentation. Curious what others landed on.\n\n4. Anyone tried HyDE (hypothetical document embeddings) in production? \n\n   Interesting in theory but I'm unsure about the latency hit.\n\n\n\nWould love to hear real production numbers, not just tutorial benchmarks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rf7xf6/whats_your_experience_with_hybrid_retrieval/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7jw7t3",
          "author": "adukhet",
          "text": "Your questions don’t have one single truth unfortunately, all these depends on use-case or depends on data. If your system intends to solve QA, technical/code based data retrieval most likely BM25 will provide better results, but if use case is enterprise/business questions then shifting towards semantic will make more sense. These stuff you can test on golden dataset during configuration and find out what’s the best parameters for customer given dataset. \n\nLast system i tested provided ndcg 0.74 on retrieving information and we gained very small to almost none improvement by applying rerankers thus plug out that component in order to reduce latency.. again data and/or user requirements dependent.\n\n\nChunk size.. again dependent on input data a lot. Are you working with long law-based documents or are you working with small-FAQ type of data. Each size will have its pros and cons depending on the problem you are trying to solve. Don’t believe if people say 1024 is best or 512 is best. You have to experiment. \n\nLastly, hyde will not cause latency issues if you apply it right, hope above points help you",
          "score": 3,
          "created_utc": "2026-02-26 17:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l960k",
              "author": "Beneficial-Grab4442",
              "text": "This is super helpful, especially the point about BM25 being stronger for QA/technical retrieval vs semantic for enterprise questions. That actually lines up with what I'm seeing — my test set is mostly internal docs with a lot of acronyms and domain-specific terms, which is probably why BM25 made such a big difference for me.\n\nInteresting that rerankers barely moved the needle for you at 0.74 NDCG. Was that already with hybrid retrieval, or on top of pure vector? I'm wondering if there's a ceiling effect where if your initial retrieval is already good enough, reranking just adds latency for marginal gains.\n\nAnd good call on golden datasets — I've been building mine manually but it's tedious. Any tips on scaling that process?",
              "score": 1,
              "created_utc": "2026-02-26 21:40:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hzouo",
          "author": "fabkosta",
          "text": "Most of the time hybrid is superior to vector alone or BM 25 alone. (There are exceptions, as always.)\n\nHyDE can be useful, but comes at an extra cost for information retrieval both financially, but more importantly also increasing retrieval times. That may be prohibitive, depending on the use case.\n\nChunk size is very dependent on your data and problem. It cannot be generalized easily. Xwitter data has very distinct characteristics than prose. Generally, a good start is to think of a paragraph or multiple paragraphs or a section in an article as a chunk.\n\nRe-ranking can also be useful, but, again, it depends on your problem and data. It's hard to generalize these things. Someone else's improvements may not be reproducible with you.\n\n80% of effort for a typical RAG project are optimizing for such stuff. You try, you fail, you try something else. That implies you need a systematic approach to measure your experiments and improvements they deliver. Never simply believe your ideas are \"good\", always measure.",
          "score": 1,
          "created_utc": "2026-02-26 11:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l99l1",
              "author": "Beneficial-Grab4442",
              "text": "This really resonates. I spent way more time tweaking retrieval than I did on the actual LLM prompting side. And you're right about measuring — I only caught the accuracy gap because I forced myself to build a proper eval set before changing anything.\n\nThe chunk size point is well taken too. I started with fixed character counts which felt arbitrary. Thinking about switching to more semantic boundaries like paragraphs or sections. Did you find that approach works better in practice, or is it more about matching the chunk granularity to the types of questions users actually ask?",
              "score": 1,
              "created_utc": "2026-02-26 21:41:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7lcist",
                  "author": "fabkosta",
                  "text": "Most time some sort of semantic chunking (paragraph, section) is preferable, in my opinion. Libraries like Langchain can help you with chunking, they have different chunking strategies built in.",
                  "score": 1,
                  "created_utc": "2026-02-26 21:56:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i6ss3",
          "author": "Dapper-Turn-3021",
          "text": "yea hybrid works like a charm most of the time, for our product we are using the same strategy and we are achieving good results till now, although it’s some time give slow or unrelated answers but that could be improve by further training",
          "score": 1,
          "created_utc": "2026-02-26 12:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l9b63",
              "author": "Beneficial-Grab4442",
              "text": "Good to hear it's working well for you too. When you say it sometimes gives slow or unrelated answers — is the slowness on the retrieval side or the generation side? I've been thinking about adding a relevance threshold so if the top retrieved chunks score below a certain confidence, the system just says \"I don't know\" instead of hallucinating an answer from weak context.\n\nCurious what further training you're considering — fine-tuning the embedding model on your domain data?",
              "score": 1,
              "created_utc": "2026-02-26 21:41:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7nc4kd",
                  "author": "Dapper-Turn-3021",
                  "text": "it’s from retrieval side, yes I already have this logic in place so only relevance chunks is going into the LLM",
                  "score": 1,
                  "created_utc": "2026-02-27 04:45:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ig9f6",
          "author": "Dense_Gate_5193",
          "text": "https://github.com/orneryd/NornicDB. MIT licensed and handles the entire rag pipeline including embedding the original query with embedding and reranking models running in-process. drops full RRF search latency on a 1m embedding corpus to 7ms including http transport.",
          "score": 1,
          "created_utc": "2026-02-26 13:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l9f7m",
              "author": "Beneficial-Grab4442",
              "text": "Haven't come across NornicDB before — 7ms for full RRF search on 1M embeddings is impressive. The in-process embedding + reranking is a nice touch too, eliminates the network overhead of calling external services.\n\nHow's the documentation and community around it? MIT license is a big plus. Might spin it up this weekend and benchmark it against my current Postgres + pgvector setup.\n\nBetween i have STARED the repo will check it when i come out of the cave",
              "score": 2,
              "created_utc": "2026-02-26 21:42:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7l9xqi",
                  "author": "Dense_Gate_5193",
                  "text": "Oh i would LOVE to see benchmarks on other hardware and with varied datasets. i’ve worked hard on the latency tuning with a lot of various optimizations so im excited to see it brutalized so i can see where it tips over. 🫶 any any all feedback is appreciated",
                  "score": 1,
                  "created_utc": "2026-02-26 21:44:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jbxfv",
          "author": "Ascending_Valley",
          "text": "What embedding method and vector size?  We've had good results reducing native embedding vectors with various methods (PCA, PLS, UMAP, tSNE, proprietary methods) to the 25-100 range.  The goal of the reduction is to make distance more related to strong coupling and important facets, dropping low signal, noisy dimensions.\n\nWe've also use optimized weighted KNN to tune to dimensional weights (using an advanced hyper tuning method; this is still pending, but promising).",
          "score": 1,
          "created_utc": "2026-02-26 16:16:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l9gtx",
              "author": "Beneficial-Grab4442",
              "text": "This is a really interesting angle I hadn't considered. So you're essentially compressing the embedding space to focus on the most meaningful dimensions before doing similarity search? That's clever — I imagine it also speeds up retrieval significantly with smaller vectors.\n\nA few questions: are you applying the dimensionality reduction per-corpus or using a general model? And how do you evaluate whether the reduction is actually improving relevance vs just making search faster? I'd worry about losing important signal in niche domains.\n\nThe weighted KNN approach sounds promising too — would love to hear how that turns out.",
              "score": 1,
              "created_utc": "2026-02-26 21:42:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7m51ku",
          "author": "geekheretic",
          "text": "A big piece I am discovering is the query decomposing, looking for keywords or other meta data to help on chunk ranking.",
          "score": 1,
          "created_utc": "2026-02-27 00:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nppsi",
          "author": "Independent-Bag5088",
          "text": "\\#3. What is your document type? Is there a reason to settle on \\~500 chars? If the document has some structure to it, it would be beneficial to preserve the structure (even if it creates uneven chunks). For my RAG project with SEC filings, I used section-aware chunking with 15% overlap.",
          "score": 1,
          "created_utc": "2026-02-27 06:28:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfinck",
      "title": "The \"Silent Bottleneck\" in Production RAG: Why Cosine Similarity Fails at Scale",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rfinck/the_silent_bottleneck_in_production_rag_why/",
      "author": "Flat-Outside4620",
      "created_utc": "2026-02-26 18:46:59",
      "score": 26,
      "num_comments": 5,
      "upvote_ratio": 0.88,
      "text": "Most RAG tutorials work great on a 100-document corpus, but once you scale to production levels, a \"silent flaw\" usually emerges: **Document Redundancy.**\n\nI’ve spent some time benchmarking retrieval performance and noticed that as the corpus grows, simple Cosine Similarity often returns the same document multiple times across different chunk sizes or overlapping slices. This effectively \"chokes\" the LLM’s context window with redundant data, leaving no room for actual diverse information.\n\nIn my latest write-up, I break down the architecture to move past this:\n\n* **The Problem:** Why kNN/Cosine Similarity alone creates a retrieval bottleneck.\n* **The Fix:** Implementing Hybrid Search (**BM25 + kNN**) for better keyword/semantic balance.\n* **Diversity:** Using Maximal Marginal Relevance (**MMR**) to ensure the top-k results aren't just 5 versions of the same paragraph.\n* **Implementation:** How to leverage the native Vector functionality in **Elasticsearch** to handle this at scale.\n\nI’ve included some benchmarks and sample code for those looking to optimize their retrieval layer.\n\n**Full technical breakdown here:**[https://medium.com/@dhairyapandya2006/going-beyond-cosine-similarity-hidden-bottleneck-for-production-grade-r-a-g-437ae0eaafa5](https://medium.com/@dhairyapandya2006/going-beyond-cosine-similarity-hidden-bottleneck-for-production-grade-r-a-g-437ae0eaafa5)\n\nI’d love to hear how others are handling diversity in their retrieval- are you guys sticking to Re-rankers, or are you seeing better ROI by optimizing the initial search query?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rfinck/the_silent_bottleneck_in_production_rag_why/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7oldvj",
          "author": "stingraycharles",
          "text": "This is absolutely beginner level RAG and I guarantee you that production systems that only use embedding distance don’t exist or are intended to be simple.",
          "score": 6,
          "created_utc": "2026-02-27 11:16:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lrj78",
          "author": "Cotega",
          "text": "You should take out the em dashes from your blog as it is pretty clear the content was AI generated. Perhaps you used your own experience, but this just makes it look like you got AI to create the blog for you.",
          "score": 7,
          "created_utc": "2026-02-26 23:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pf0mf",
          "author": "singh_taranjeet",
          "text": "I think the key takeaway here is that pure cosine similarity almost *always* hits a wall once you’re beyond toy corpora because it ends up returning the same document or very similar chunks over and over, which fills up your context window with redundant info instead of diverse evidence. \n\nThat’s why hybrid search (lexical + vectors) or diversity-aware selection strategies like MMR/DF-RAG tend to outperform vanilla RAG at scale; you want relevance *and* non-redundancy. If you’ve actually tested this in a production stack and found something better than MMR, it’d be great to hear from Mem0 on what empirically worked",
          "score": 2,
          "created_utc": "2026-02-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rcdfd",
              "author": "Single-Constant9518",
              "text": "Totally agree, redundancy can really mess with performance. I’ve seen some setups using a blend of query expansion and MMR that seem promising too. Have you tried any other selection strategies beyond MMR? Always curious about what works best in real-world scenarios.",
              "score": 1,
              "created_utc": "2026-02-27 20:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tc6xs",
                  "author": "No-Consequence-1779",
                  "text": "Seems like a chunking strategy that does that may also not be good even for a simple system. ",
                  "score": 1,
                  "created_utc": "2026-02-28 02:42:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rclvtn",
      "title": "first RAG project, really not sure about my stack and settings",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "author": "Kas_aLi",
      "created_utc": "2026-02-23 16:21:16",
      "score": 19,
      "num_comments": 12,
      "upvote_ratio": 0.93,
      "text": "Hey guys, so ive been working on my first RAG project. its basically a system that takes medical PDFs (textbooks, clinical guidelines) and builds a knowledge graph from them to generate multi-choice exam questions for a medical exam. \n\nInput style: large textbooks, pdf, images, tables, etc\n\nI have been coding this like a monkey with claude opus 4.6 and codex 5.3 honestly, just prompting my way through it. it works but i have no idea if what im doing is the right approach.\n\nWould love some feedback, good sources or learning resources.  \n  \nHere is my current stack for context:\n\n    PDF → Docling (no OCR, native text) → markdown export with page breaks\n        → heading-based chunker (~768 tok, tiktoken cl100k)\n          → noise classifier (regex heuristics, filters TOC/references/headers)\n          → batch extraction (3 chunks/batch, 4K token cap, 4 parallel workers)\n            → Instructor (JSON mode) + Gemini 2.5 Flash via OpenRouter (it is cheap, but probably there are better now)\n            → Pydantic schema: concepts (18 types) + claims (25 predicates) + evidence spans\n            → fallback: batch fail → individual chunk extraction\n          → concept normalization + dedup\n          → quality gate (error rate, claims/chunk, evidence/claim, noise ratio, page coverage)\n          → embeddings: Qwen3-embedding-8b (1024d) → pgvector\n    \n    storage: supabase (27 tables)\n    orchestration: langgraph (for downstream question generation, not ETL)\n    \n    all LLM calls through openrouter",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6z75uc",
          "author": "Higgs_AI",
          "text": "Can I just ask you if this is for taking exams? If so… DM me. Overall it’s well put together… the bones are solid. The fact that it has a quality gate at all puts it ahead of most implementations. 🤷🏽‍♂️\n\nHad to edit: this is really clean work…the schema enforced extraction with Instructor, the quality gate with multiple metrics, the batch fallback logic. Most people building extraction pipelines skip half of what you’ve done here… bravo brotha. \n\nyour question generation is downstream and separate. What if the extraction and the pedagogy were part of the same adaptive loop where how the learner performs on generated questions feeds back into which concepts need deeper extraction, which claims need more evidence, which connections need to be surfaced?\n\nThere’s other stuff I’d say but, I just thought I’d give you some substance without flooding your shit. Good work",
          "score": 3,
          "created_utc": "2026-02-23 16:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z8hqn",
              "author": "Kas_aLi",
              "text": "Main usecase will be generating new and original exam questions but it should be good at taking the exam as well I guess... I think GPT 5.2 had more than 98% correct answers in this particular exam already so...",
              "score": 1,
              "created_utc": "2026-02-23 16:49:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zp35o",
          "author": "Semoho",
          "text": "Nice orchestration. \n\nYou can have a little academic approach to assess your system. In my opinion, you can create a test dataset from your exam or even from your knowledge of the docs (corpus). Then, run testing on GPT 5.2 and get the answers and also with your system. Now you have benchmark from gpt or other models and your system, so you can check if the results are good or not. We call it the test phase or making a test dataset. This gives you the power to assess your system. (Usually, you can see the baselines on the academic papers that they need to improve the baseline to compete with other approaches.)\n\nAlso, I would recommend using Knowledge Graph if you have relational data, such as something belongs to one paper/book, and there is some other evidence on the other resources.\n\n  \nSome tips and tricks to increase the retrieval phase:\n\n1. Use task-specific embedding models such as [https://huggingface.co/abhinand/MedEmbed-base-v0.1](https://huggingface.co/abhinand/MedEmbed-base-v0.1)\n\n2. Use other vector DBs like Milvus. Vector DB is so easy to set up, but it does not provide good accuracy.\n\n3. Check the knowledge graph.",
          "score": 3,
          "created_utc": "2026-02-23 18:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77spi7",
              "author": "rigatoni-man",
              "text": "I've been building something to test models without a lot of overhead and legwork.  Basically upload your golden dataset and test it against every model out there.\n\nShoot me a message u/Kas_aLi  and I'd love to help you find the best model for free to test what i'm building ( [https://checkstack.ai](https://checkstack.ai) )",
              "score": 1,
              "created_utc": "2026-02-24 21:55:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77toou",
                  "author": "Semoho",
                  "text": "It is a good platform. But as an IR guy to publish different papers in the information retrieval field, I think I do not need it :))  \nHowever, if I find time, I would check your platform",
                  "score": 1,
                  "created_utc": "2026-02-24 22:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zcxj3",
          "author": "shlok-codes",
          "text": "I use DeepSeek nitro chat via openrouter look into DeepSeek and qwen models",
          "score": 2,
          "created_utc": "2026-02-23 17:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ivc1",
          "author": "avebrahimi",
          "text": "Awesome stack.  \nWhat about UI?",
          "score": 1,
          "created_utc": "2026-02-24 07:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tl37",
          "author": "LuckEcstatic9842",
          "text": "You might also want to take a look at LightRAG.",
          "score": 1,
          "created_utc": "2026-02-24 08:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1fde",
          "author": "aidenclarke_12",
          "text": "the heading-based chunker at 768 tokens works for linear textbook prose but clinical tables and cross-references tend to fragment badly across batch boundaries.. the 3 chunks per batch with 4k cap means a pharmacology table that spans two chunks might lose the column headers on one side and the values on the other. \n\nthe quality gate catching this after the fact is better than nothing but fixing it upstream in the chunking logic is probably worth exploring.. \n\non the model side you mentioned openrouter and questioned whether there are better options now.. runpod, together or deepinfra give direct access to the same gemini flash and qwen3 models at lower per-token cost which matters if you're running high extraction volumes on large textbooks",
          "score": 1,
          "created_utc": "2026-02-25 11:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g66sn",
          "author": "New_Direction5479",
          "text": "Awesome stack but Embedding storing go with Qdrant ,add some open source reranker model's also and make 2 application 1. UI to Ingestion , 2. UI to Inference application.",
          "score": 1,
          "created_utc": "2026-02-26 03:02:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reivma",
      "title": "Agentic RAG for Dummies v2.0",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1reivma/agentic_rag_for_dummies_v20/",
      "author": "CapitalShake3085",
      "created_utc": "2026-02-25 17:01:02",
      "score": 19,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone! I've been working on **Agentic RAG for Dummies**, an open-source project that shows how to build a modular Agentic RAG system with LangGraph — and today I'm releasing v2.0.\n\nThe goal of the project is to bridge the gap between basic RAG tutorials and real, extensible agent-driven systems. It supports any LLM provider (Ollama, OpenAI, Anthropic, Google) and includes a step-by-step notebook for learning + a modular Python project for building.\n\n## What's new in v2.0\n\n🧠 **Context Compression** — The agent now compresses its working memory when the context exceeds a configurable token threshold, keeping retrieval loops lean and preventing redundant tool calls. Both the threshold and the growth factor are fully tunable.\n\n🛑 **Agent Limits & Fallback Response** — Hard caps on tool invocations and reasoning iterations ensure the agent never loops indefinitely. When a limit is hit, instead of failing silently, the agent falls back to a dedicated response node and generates the best possible answer from everything retrieved so far.\n\n## Core features\n\n- Hierarchical indexing (parent/child chunks) with hybrid search via Qdrant\n- Conversation memory across questions\n- Human-in-the-loop query clarification\n- Multi-agent map-reduce for parallel sub-query execution\n- Self-correction when retrieval results are insufficient\n- Works fully local with Ollama\n\nThere's also a Google Colab notebook if you want to try it without setting anything up locally.\n\nGitHub: https://github.com/GiovanniPasq/agentic-rag-for-dummies",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1reivma/agentic_rag_for_dummies_v20/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7oqbo3",
          "author": "martinschaer",
          "text": "Take a look at [SurrealDB.com](http://SurrealDB.com) it allow you to store not just the vectors, but also any other data (users, chat history, etl metadata, ...). Plus, if you want to do graph RAG, you can add relationships between your data too. [https://surrealdb.com/blog/how-to-build-a-knowledge-graph-for-ai#practical-examples](https://surrealdb.com/blog/how-to-build-a-knowledge-graph-for-ai#practical-examples) (i'm the author of this blog post)",
          "score": 1,
          "created_utc": "2026-02-27 11:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcecf4",
      "title": "How I Used AI + RAG to Automate Knowledge Management for a Consulting Firm",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "author": "Safe_Flounder_4690",
      "created_utc": "2026-02-23 10:52:36",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "Recently, I built a workflow for a consulting firm that leverages AI combined with Retrieval-Augmented Generation (RAG) to fully automate knowledge management, transforming a fragmented document system into a centralized, actionable intelligence hub. The pipeline begins by ingesting structured and unstructured client reports, internal documents and market research into a vector database, then AI agents retrieve the most relevant information dynamically, reason over it and generate concise, actionable summaries or recommendations. By layering persistent memory, validation loops and workflow orchestration, the system doesn’t just fetch data it contextualizes it for consultants, flags potential conflicts, and tracks follow-ups automatically. This approach drastically reduced time spent searching across multiple tools, eliminated duplication errors and improved decision-making speed. What made it successful is the combination of semantic search, structured reasoning and AI-driven content validation, ensuring that consultants always have the most accurate, up-to-date insights at their fingertips. The outcome: higher productivity, faster client delivery and a knowledge system that scales with the firm’s growth.\nIf AI can summarize thousands of consulting documents in minutes, how much more value could your team create by focusing only on insights instead of searching for them?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xrg3b",
          "author": "prismaticforge",
          "text": "Have you had feedback from the users?  Are they happy with the results and using the tool.  I am curious also how you handle contradictory information from rag?",
          "score": 2,
          "created_utc": "2026-02-23 11:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xnour",
          "author": "jannemansonh",
          "text": "nice setup... building rag pipelines with custom orchestration is solid but maintaining that glue code gets brutal over time. ended up using needle app for similar doc workflows since it handles the vector db + workflow orchestration in one place (just describe what you want vs wiring everything). kept custom stuff for edge cases though",
          "score": 1,
          "created_utc": "2026-02-23 11:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xoaiw",
              "author": "Safe_Flounder_4690",
              "text": "That’s a valid point maintaining custom orchestration can become complex as systems scale and edge cases increase. Managed platforms can reduce operational overhead and speed up deployment.\n \nThat said, custom pipelines still offer deeper control over retrieval logic, validation and integration with internal processes. The right balance often comes from standardizing core infrastructure while keeping flexibility where business-specific reasoning and accuracy matter most.",
              "score": 1,
              "created_utc": "2026-02-23 11:28:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y2094",
          "author": "Tired__Dev",
          "text": "It’s the actionable summaries among chunks that would have me worried tbh.",
          "score": 1,
          "created_utc": "2026-02-23 13:10:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y2dma",
          "author": "Semoho",
          "text": "Did you have benchmark o test dataset that how this approach effects the system?",
          "score": 1,
          "created_utc": "2026-02-23 13:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yc7ri",
          "author": "ChapterEquivalent188",
          "text": "how do you test ? do you trust your llm ?",
          "score": 1,
          "created_utc": "2026-02-23 14:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zeo3u",
          "author": "AmphibianNo9959",
          "text": "For personal use, I've been using Reseek to handle a lot of that ingestion and semantic search piece automatically. It pulls text from PDFs and images, tags everything, and makes my own notes and bookmarks searchable in a similar way. ",
          "score": 1,
          "created_utc": "2026-02-23 17:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zrox9",
              "author": "Material-River-2235",
              "text": "Ok, I will go take a look at it.",
              "score": 1,
              "created_utc": "2026-02-23 18:19:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zf2rh",
          "author": "nasnas2022",
          "text": "Can you add few more details on the pipeline",
          "score": 1,
          "created_utc": "2026-02-23 17:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zj2uk",
          "author": "_os2_",
          "text": "We have built something similar with [Skimle](https://skimle.com), but our tool skips RAG completely and instead builds the categorization scheme in the beginning with LLM calls and then retrieves from a structured table rather than at runtime. Enables two-way transparency and stable responses.\n\nWould be great to compare results!",
          "score": 1,
          "created_utc": "2026-02-23 17:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7125c1",
          "author": "Infamous_Ad5702",
          "text": "Well done. Do you build a knowledge graph?\nAnd why Vector?\nI had the same project for a client and went with a custom tool called Leonata. It builds an index and works totally offline. No LLM. No GPU. And No hallucination. \n\nFor my client purpose, semantic retrieval, Vector found similar info but not the best fit.",
          "score": 1,
          "created_utc": "2026-02-23 21:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1regz4t",
      "title": "I think most RAG quality issues people post about here are actually extraction problems, not retrieval problems",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1regz4t/i_think_most_rag_quality_issues_people_post_about/",
      "author": "yfedoseev",
      "created_utc": "2026-02-25 15:53:50",
      "score": 16,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "Every other post in this sub is \"my RAG pipeline hallucinates\" and the replies are always the same: try a different chunking strategy, use a better embedding model, add reranking, etc.\n\nNobody ever says \"go look at what your PDF parser actually output.\"\n\nI did. I took 3,830 real-world PDFs (veraPDF corpus, Mozilla pdf.js tests, DARPA SafeDocs) and ran them through the major Python parsers. Not cherry-picked -- government filings, academic papers, scanned forms, edge cases from the 90s, encrypted files, CJK text, the works.\n\n    Library      Mean     p99      Pass rate\n    ──────────────────────────────────────────\n    pdf_oxide    0.8ms     9ms     100%\n    PyMuPDF      4.6ms    28ms     99.3%\n    pypdfium2    4.1ms    42ms     99.2%\n    pdfminer    16.8ms   134ms     98.8%\n    pdfplumber  23.2ms   189ms     98.8%\n    pypdf       12.1ms    97ms     98.4%\n\nHere's the thing nobody talks about: a 98.4% pass rate on 3,830 docs means \\~60 documents that silently fail. They crash, hang, or return empty strings. Those docs never enter your vector store. When a user asks about content from one of those documents, the retrieval step finds nothing relevant, so the LLM fills in the gap with a confident hallucination.\n\nYou debug the prompt. You debug the retrieval. You never think to check whether the document was even indexed.\n\nI built pdf\\_oxide (Rust, Python bindings) partly because I kept running into this. The thing that made the biggest difference for me wasn't the speed, it was the Markdown output with heading detection:\n\n        from pdf_oxide import PdfDocument\n    \n        doc = PdfDocument(\"paper.pdf\")\n        md = doc.to_markdown(0, detect_headings=True)\n\nYou get actual structure back. Headings, paragraphs, sections. Chunk on section boundaries instead of arbitrary token windows. Each chunk ends up being about one topic instead of the tail end of one section glued to the beginning of another. Retrieval precision went up noticeably for me once I switched to heading-based splits.\n\nBuilt-in OCR too (PaddleOCR via ONNX Runtime). It auto-detects scanned pages and falls back. No Tesseract, no subprocess shelling out, no extra config.\n\n        pip install pdf_oxide\n\nMIT licensed. No AGPL. Runs entirely locally.\n\nLimitations I won't hide: table extraction is basic compared to pdfplumber. There are \\~10 edge-case PDFs that still have minor extraction issues (tracked on GitHub). WASM support isn't done yet.\n\n[github.com/yfedoseev/pdf\\_oxide](http://github.com/yfedoseev/pdf_oxide)   \nDocs: [oxide.fyi](http://oxide.fyi)\n\nGenuine question for this sub: how many of you have actually diffed your parser's output against the source PDF? I'm starting to think a lot of the \"retrieval quality\" problems people debug for weeks are just garbage going in at step one.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1regz4t/i_think_most_rag_quality_issues_people_post_about/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7ci7ij",
          "author": "ChapterEquivalent188",
          "text": "have you tried docling ? \njust in case, i thought iĺl be on my own for ever ;) \nhttps://github.com/2dogsandanerd/RAG_enterprise_core",
          "score": 2,
          "created_utc": "2026-02-25 16:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7he74d",
          "author": "lucasbennett_1",
          "text": "this is the truth..most rag complaints are actually bad extraction in disguise.. inspect your parser output before anything else.. pdf\\_oxide looks like a solid fix for that.",
          "score": 2,
          "created_utc": "2026-02-26 08:35:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f0wlv",
          "author": "SubstantialTea707",
          "text": "Io estraggo le immagini e le leggo con il modello glm ocr, tra estrazione e llm che gira su una 5090 ci perdo 5s a pagina e da ottimi risultati in estrazione",
          "score": 1,
          "created_utc": "2026-02-25 23:10:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fau76",
              "author": "yfedoseev",
              "text": "It works as well if you have this 5 sec for processing",
              "score": 1,
              "created_utc": "2026-02-26 00:05:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fhd5w",
          "author": "patbhakta",
          "text": "For tables, formulas, charts, etc. you need specialized parsers. OCR is fine for text, VL is fine for photos, you need different tools for different things.",
          "score": 1,
          "created_utc": "2026-02-26 00:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gdw1k",
          "author": "hhussain-",
          "text": "What difference rust gave you in here? Or what made you use rust if it is all python ?",
          "score": 1,
          "created_utc": "2026-02-26 03:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gifx9",
              "author": "yfedoseev",
              "text": "Rust gave me a significant performance boost and allowed me to build bindings to most programming languages guaranteed high performance everywhere.",
              "score": 2,
              "created_utc": "2026-02-26 04:17:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gjprz",
          "author": "New_Animator_7710",
          "text": "In our lab, we’ve repeatedly observed that retrieval quality correlates more strongly with structural fidelity of extraction than with embedding choice. Clean section boundaries often outperform swapping to a “better” model.",
          "score": 1,
          "created_utc": "2026-02-26 04:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hb3qw",
          "author": "stevevaius",
          "text": "How about laws? Many laws has sections, sub-sections, articles, quotes etc...? Did you run any test for legal texts?",
          "score": 1,
          "created_utc": "2026-02-26 08:06:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hbmd8",
              "author": "yfedoseev",
              "text": "Honestly, I did and the text conversation works well, but markdown,.something that requires more structure still requires improvement for some corner cases. Legal docs are on my radar. Thank you for your question.",
              "score": 2,
              "created_utc": "2026-02-26 08:10:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hck08",
                  "author": "stevevaius",
                  "text": "Looking fwd to hear about your project, specially on legal text developments. Best regs",
                  "score": 1,
                  "created_utc": "2026-02-26 08:19:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7l6e0b",
          "author": "tom_at_zedly",
          "text": "You're spot on. Everyone obsesses over the vector database or the LLM, but they ignore the mess coming out of the PDF parser.\n\nIf the text extraction is broken, the RAG is dead on arrival. We've found that handling tables and multi-column layouts is 80% of the battle. OCR quality matters way more than most people admit.",
          "score": 1,
          "created_utc": "2026-02-26 21:27:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdeibi",
      "title": "Fresh grad learning RAG, feeling lost, looking for guidance",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "author": "savinox23",
      "created_utc": "2026-02-24 12:07:10",
      "score": 16,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hello, I am a fresh grad trying to learn about RAG and develop my coding skills. I made this simple cooking assistant based on Moroccan recipes. Could you please tell me how I can improve my stack/architecture knowledge and my code?\n\nWhat I currently do is discuss best practices with ChatGPT, try to code it myself using documentation, then have it review my code. But I feel like I'm trying to learn blindly. It's been 6 days and I've only made this sloppy RAG, and I feel like there is a better way to do this.\n\nHere’s the link to a throwaway repo with my code (original repo has my full name haha):\n\n  \n[https://github.com/Savinoy/Moroccan-cooking-assistant](https://github.com/Savinoy/Moroccan-cooking-assistant?utm_source=chatgpt.com)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o74uxot",
          "author": "RobertLigthart",
          "text": "6 days and you already have a working RAG is not sloppy thats actually decent progress. most people spend weeks just trying to get embeddings to work\n\n  \nthe chatgpt code review loop is fine for learning syntax but for architecture you need to read how other people built theirs. check out the rag-from-scratch series by lance martin on youtube... its hands down the best resource for understanding why the pieces fit together not just how to copy paste them\n\n  \nbiggest thing I'd improve early on: add chunking strategy to your pipeline if you havent already. most beginners just dump full documents into the vector store and wonder why retrieval is bad. experiment with chunk sizes and overlap... makes a massive difference",
          "score": 4,
          "created_utc": "2026-02-24 13:44:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74yjcp",
              "author": "savinox23",
              "text": "Thank you very much, i appreciate it !! I’ll check out the series. As for the chunking strategy, I experimented with a standard text splitter but it was giving me mixed/ incomplete recipe answers, so i made each separate recipe as a chunk and the results were better, but i will check out the resources you mentioned, I’m sure there are better methods i can try!",
              "score": 1,
              "created_utc": "2026-02-24 14:04:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7xdsov",
              "author": "Necessary-Dot-8101",
              "text": "contradiction compression",
              "score": 1,
              "created_utc": "2026-02-28 19:13:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78hh8u",
          "author": "Asleep_Carpet_3403",
          "text": "Great to see someone trying to learn through manual coding and 6 days to a working Rag is absolutely impressive. One suggestion is to start using tools like cursor with their auto complete functionality. It'll 10x your coding speed while you still continue to learn and write the complete code yourself. \n\nA good skill to have now is to learn inference (deploying and using ML/DL models) for this you may consider including a data extraction model from scanned pages (images) upstream of your RAG pipeline",
          "score": 2,
          "created_utc": "2026-02-25 00:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78jocr",
              "author": "savinox23",
              "text": "Okay, thank you very much for the advice !!",
              "score": 1,
              "created_utc": "2026-02-25 00:16:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mcqpq",
          "author": "RecommendationFit374",
          "text": "I don’t recommend using langchain i’d use a memory layer for retrieval like papr.ai or mem0",
          "score": 2,
          "created_utc": "2026-02-27 01:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oevw3",
              "author": "savinox23",
              "text": "Okay, i’ll check it out, thank you !",
              "score": 2,
              "created_utc": "2026-02-27 10:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfd4md",
      "title": "Building RAG pipelines using elasticsearch",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rfd4md/building_rag_pipelines_using_elasticsearch/",
      "author": "SQLsunset",
      "created_utc": "2026-02-26 15:27:55",
      "score": 16,
      "num_comments": 15,
      "upvote_ratio": 0.94,
      "text": "I chose Elasticsearch over Pinecone for RAG. Here's the honest breakdown.\n\nEveryone building a RAG app hits the same fork: dedicated vector DB (Pinecone, Weaviate) or just use Elasticsearch?\n\nMost tutorials default to Pinecone. I went a different direction and want to share why.\n\nThe core problem with dedicated vector DBs\n\nRAG isn't purely a vector search problem. In practice you need:\n\n\\- Semantic similarity (vectors)\n\n\\- Keyword relevance (BM25)\n\n\\- Metadata filtering\n\nPinecone gives you vectors + basic filters. The moment you need hybrid search and you will, because pure vector retrieval misses exact matches constantly, you're bolting on another system.\n\nElasticsearch does all three natively in one query using Reciprocal Rank Fusion. No extra infrastructure, no glue code.\n\nThe black box problem\n\nWhen Pinecone retrieval is bad, your options are: tweak embeddings, adjust top\\_k, and hope. You can't inspect query execution or see why documents scored the way they did.\n\nElasticsearch shows its work. You can see BM25 vs vector score contributions, profile queries, set up Kibana dashboards. When something breaks you can actually debug it.\n\nElastic Cloud removes the old objection\n\nThe classic knock on Elasticsearch was ops pain — shard management, rolling upgrades, cluster tuning. Elastic Cloud handles all of that. Autoscaling, automated snapshots, one-click upgrades. You get the power without babysitting a cluster.\n\nPinecone scales well too, but it only scales your vector index. Everything else still needs separate infrastructure.\n\nGCovers hybrid search setup, kNN index config, and a working RAG query in \\~15 minutes on Elastic Cloud.\n\nCurious if anyone else has gone this route or stuck with Pinecone what pushed your decision?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rfd4md/building_rag_pipelines_using_elasticsearch/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7j3wop",
          "author": "fabkosta",
          "text": "FYI: Azure AI Search is very similar to Elasticsearch and offers hybrid search out of the box. It's very powerful - but very expensive. If you don't want to host Elasticsearch yourself and are willing to pay for the premium price, then it's a good cloud PaaS alternative.",
          "score": 3,
          "created_utc": "2026-02-26 15:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j9lxr",
              "author": "grabGPT",
              "text": "Exactly was about to write this. I am using Azure AI Search. What I like about it is how easy it is to wrap AI Search around MCP tools for orchestration and the entire architecture becomes much simpler. For deployment and governance.",
              "score": 1,
              "created_utc": "2026-02-26 16:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7knb2r",
                  "author": "Single-Constant9518",
                  "text": "Sounds like Azure AI Search has its perks, especially for those looking for a simpler deployment. Curious how it stacks up in terms of customization and debugging compared to Elasticsearch. Have you faced any limitations?",
                  "score": 1,
                  "created_utc": "2026-02-26 19:56:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ldvub",
          "author": "Unusual-Sector-2511",
          "text": "For my part, I chose Amazon Bedrock Knowledge Bases. It offers\n\n\\- Advanced Retrieval: Including hybrid search and automated re-ranking  \n\\- Cost-Efficiency: Extremely low-cost RAG storage using S3 Vectors.  \n\\- Native Multimodal Support: Integrated embeddings for text, images, and video  \n\\- Enterprise-Grade: A secure, scalable, and easy-to-deploy managed service.",
          "score": 3,
          "created_utc": "2026-02-26 22:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lp9d7",
              "author": "543254447",
              "text": "does s3 do hybrid search?",
              "score": 1,
              "created_utc": "2026-02-26 23:01:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lqjb3",
                  "author": "Unusual-Sector-2511",
                  "text": "Actually, S3 itself doesn't perform the hybrid search. It acts as the storage layer for your chunks and vectors. The 'magic' happens within Amazon Bedrock Knowledge Bases, which manages the retrieval logic. It pulls the data from S3 and handles the combination of semantic search and keyword search (hybrid) along with re-ranking to give you the most relevant context.",
                  "score": 3,
                  "created_utc": "2026-02-26 23:08:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ttt47",
          "author": "SharpRule4025",
          "text": "Everyone's debating the retrieval backend but the thing that actually moved our accuracy numbers was what we were putting into the index in the first place. We spent weeks tuning Elasticsearch scoring and it turned out the real problem was ingesting raw markdown from our scraper. Navigation menus, footer links, language selectors, cookie banners, all of it was getting embedded alongside the actual content.\n\nOnce we switched to structured extraction where you get typed fields (title, body paragraphs, metadata) instead of a markdown blob, our retrieval precision jumped because the embeddings were actually representing the content, not the UI. The added bonus is you can index specific fields separately and weight them differently in your hybrid query, which is harder to do when everything is one big text chunk.",
          "score": 2,
          "created_utc": "2026-02-28 04:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j5kpu",
          "author": "jannemansonh",
          "text": "solid point on the hybrid search. ended up using needle app for doc workflows since the rag + hybrid search is built in... way easier than wiring pinecone + elastic together, especially when workflows need to actually understand content and not just move data around",
          "score": 1,
          "created_utc": "2026-02-26 15:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jae81",
          "author": "Next-Rush-9330",
          "text": "You can try Milvus",
          "score": 1,
          "created_utc": "2026-02-26 16:09:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jvd5f",
          "author": "2BucChuck",
          "text": "Azure and AWS both also figured the same out and have rolled out tools like S3 vectors- I ,like you , built something custom on a Lucene engine- but also believe this is the right way to go with KBs.   I learned though the hard way don’t try to emulate what Lucene does - it’s just light years ahead of anything new you could build for non dense / vector approaches and it can store vectors all the same alongside.",
          "score": 1,
          "created_utc": "2026-02-26 17:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ldgy7",
          "author": "No-Leopard7644",
          "text": "You mentioned Weaviate, but did not choose. Was this after analysis for a fit for your use case/org skills? I have used Qdrant, but am planning to go to Weaviate",
          "score": 1,
          "created_utc": "2026-02-26 22:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m07d0",
          "author": "Infamous_Ad5702",
          "text": "Vector finds similar items. I made a tool to find breadth and depth. \nChunking and embedding is a pain so it builds an index on auto. Then when I query a knowledge graph is built fresh each time.\n\nMy client is in defense. So needed offline. No GPU. No hallucination. No tokens. \n\nDeterministic. Accurate.",
          "score": 1,
          "created_utc": "2026-02-27 00:01:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ni2in",
          "author": "crewone",
          "text": "We went with opensearch. Still getting spammed by the elastic folks with email.",
          "score": 1,
          "created_utc": "2026-02-27 05:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j4lro",
          "author": "z0han4eg",
          "text": "Well, there you go, you just leaked a piece of a corporate RAG. Let’s just get back to topics about reinventing the wheel and YouTube promos for the latest \"breakthroughs\".\n\nAnd yeah, a proper hybrid search/waterfall setup with full-text, fuzzy, KNN, hybrid etc logic is done through Elastic, Manticore, or Meilisearch, which already have auto-embeddings and even rerankers. If we’re talking about a toy RAG where the whole pipeline boils down to just retrieval from a vector database, then sure, use whatever-Pinecone, Weaviate, or even Alibaba Zvec -if it’s anything more serious than a couple thousand PDFs about cat breeds.",
          "score": 0,
          "created_utc": "2026-02-26 15:43:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg1fjh",
      "title": "Atomic GraphRAG: using a single database query instead of application-layer pipeline steps",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rg1fjh/atomic_graphrag_using_a_single_database_query/",
      "author": "mbudista",
      "created_utc": "2026-02-27 08:46:01",
      "score": 16,
      "num_comments": 5,
      "upvote_ratio": 0.95,
      "text": "Memgraph just published a post on a pattern we’ve been calling Atomic GraphRAG:\n\n[https://memgraph.com/blog/atomic-graphrag-explained-single-query-pipeline](https://memgraph.com/blog/atomic-graphrag-explained-single-query-pipeline)\n\nThe core idea is simple: instead of stitching GraphRAG together across multiple application-layer steps, express retrieval, expansion, ranking, and final context assembly as a **single database query**.\n\nThe post breakdown:\n\n* what we mean by GraphRAG;\n* three common retrieval patterns (analytical, local, and global);\n* why GraphRAG systems often turn into pipeline sprawl in production;\n* and why pushing more of that logic into the database can simplify execution and make the final context easier to inspect.\n\nThe argument is that a single-query approach can reduce moving parts, return a more compact final payload to the LLM, and make it easier to trace how context was assembled.\n\nCurious how others here are structuring GraphRAG pipelines today - especially whether you keep orchestration mostly in app code or push more of it into the database.\n\n*Disclosure: I’m with Memgraph and the blog post author.*",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rg1fjh/atomic_graphrag_using_a_single_database_query/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7pf6ub",
          "author": "singh_taranjeet",
          "text": "This atomic graph-RAG idea makes a lot of sense because it avoids the classic multi-query feedback loop that bloats latency and forces you to stitch context after the fact. By treating the graph as the single source of truth and then deriving your prompt from one canonical query, you get consistency without repeated hits on the index.\n\nIf anyone has actually tried this pattern end-to-end at scale and found real gains, it’d be cool to hear from Mem0 on what tradeoffs they ran into in practice",
          "score": 3,
          "created_utc": "2026-02-27 14:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ql6nq",
              "author": "mbudista",
              "text": "Exactly, it should be easier for everyone to implement GraphRAG pipelines.",
              "score": 1,
              "created_utc": "2026-02-27 17:51:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7silo2",
          "author": "Dense_Gate_5193",
          "text": "this is why i consolidated the whole pipeline into a single binary to reduce latency to 7ms RRf hybrid search on a 1m embedding corpus including embedding the user query in memory and reranking. \n\nhttps://github.com/orneryd/NornicDB",
          "score": 1,
          "created_utc": "2026-02-27 23:43:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7top41",
          "author": "New_Animator_7710",
          "text": "Comparing this to hybrid vector + graph approaches, many teams keep semantic retrieval in a vector database and only use graph traversal for structured expansion. Atomic GraphRAG suggests unifying those concerns. I’d be curious how this interacts with external embedding providers or frameworks like OpenAI—does the atomicity stop at structured retrieval, or can embedding similarity also be expressed natively?",
          "score": 1,
          "created_utc": "2026-02-28 04:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ui97r",
              "author": "mbudista",
              "text": "A very good question/point. I think the whole approach works really well, even when multiple systems interact during a single GraphRAG pipeline execution. But there has to be an integration layer, so-called cross-database or foreign database capabilities. E.g., running a clause/function to call the embeddings in one system, search for matches under a second system, and expand the graph inside a third system.",
              "score": 1,
              "created_utc": "2026-02-28 08:04:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rf7of7",
      "title": "RAG eval is broken if you're only testing offline - here's what changed for us",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rf7of7/rag_eval_is_broken_if_youre_only_testing_offline/",
      "author": "darkluna_94",
      "created_utc": "2026-02-26 11:25:49",
      "score": 15,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "\nI've been building a RAG pipeline for internal document search for about 4 months now. Mostly legal and compliance docs so accuracy actually matters for my use case.\n\nMy offline eval was looking pretty solid. RAGAS scores were decent, faithfulness sitting around 0.87, context recall above 0.9. I shipped it feeling good about it.\n\nThen users started flagging answers. The pipeline was pulling the right chunks but still getting conclusions wrong sometimes. Not obvious hallucinations, more like the model was connecting retrieved context incorrectly for certain document structures. My benchmark never caught it because my test set didn't really reflect the docs users were actually uploading.\n\nThat's the thing nobody tells you. Your test set is a snapshot. Production keeps changing.\n\nHere's what I went through trying to fix it:\n\n**Manual test set curation** - I started reviewing failing queries and adding them to my golden dataset. Helped a bit but honestly didn't scale at all.\n\n**Langfuse** - added tracing so I could actually see which chunks were being retrieved per query. This alone was a big deal for debugging. Still needed manual review to spot patterns though.\n\n**Confident AI** - started running faithfulness and relevance metrics directly on live traces. The thing that actually saved me time was failing traces getting auto-flagged and curated into a dataset automatically so I wasn't doing it by hand.\n\n**Prompt tweaking** - turned out a lot of failures were fixable once I could actually see the pattern clearly.\n\nHonestly even just adding proper tracing was the biggest unlock for me. Going in blind was the real problem. Evaluation on top just made it less random.\n\nAnyone else dealing with this on domain specific or inconsistent document formats?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rf7of7/rag_eval_is_broken_if_youre_only_testing_offline/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7jurss",
          "author": "llamacoded",
          "text": "Same experience. Offline evals looked great, production was different. We sample 10% of live traffic for automatic evaluation now - catches retrieval drift before users report it. Way better than waiting for complaints. Docs: [https://www.getmaxim.ai/docs/offline-evals/overview](https://www.getmaxim.ai/docs/offline-evals/overview)",
          "score": 2,
          "created_utc": "2026-02-26 17:43:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ik8yp",
          "author": "Odd-Literature-5302",
          "text": "such a good reminder that offline eval only measures what you think users will ask, not what they actually do. In domains like legal and compliance, small structural quirks can completely change how context gets interpreted, so live tracing plus continuous dataset updates feels almost mandatory.",
          "score": 1,
          "created_utc": "2026-02-26 14:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j41ao",
          "author": "Elegant_Gas_740",
          "text": "Have you found that most of the issues came from retrieval gaps, or was it mainly the model misinterpreting the right chunks once they were pulled?",
          "score": 1,
          "created_utc": "2026-02-26 15:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jg24f",
          "author": "StrangerFluid1595",
          "text": "Offline scores can look solid but production always exposes the weird edge cases, especially with complex legal docs. Getting proper tracing in place is such a game changer.",
          "score": 1,
          "created_utc": "2026-02-26 16:35:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k7hmh",
          "author": "Delicious-One-5129",
          "text": "Honestly the frozen test set problem is what got us too. Moved to Confident AI and prod failures just automatically become regression tests now. Night and day difference.",
          "score": 1,
          "created_utc": "2026-02-26 18:41:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reu6t0",
      "title": "For teams selling internal AI search/RAG: what does user behavior actually look like?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1reu6t0/for_teams_selling_internal_ai_searchrag_what_does/",
      "author": "adukhet",
      "created_utc": "2026-02-25 23:49:07",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "Just like the title; question for people actually selling RAG/enterprise AI search products (not demos, not internal tools):\n\nHave you ever measured average user session length?\n\nI’m especially curious about real production usage, not benchmarks.  \n\nIf you’re willing to share, it would be super helpful to include:\n\n\\- vertical (legal, support, sales, engineering, etc.)\n\n\\- main use case (knowledge search, support copilot, internal documentation, analyst workflows…)\n\n\\- average time spent in a session\n\n\\- roughly how many queries per session\n\nI’m trying to understand actual behavioral patterns of users interacting with RAG systems. Papers and blog posts talk a lot about retrieval accuracy, but almost nothing about how people actually use these systems once deployed.\n\nHard to get this data without already operating one at scale, so even rough ranges or anonymized observations would be incredibly useful",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1reu6t0/for_teams_selling_internal_ai_searchrag_what_does/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7hpxe5",
          "author": "megAchiever",
          "text": "btw if anyone has such tool would like to incorporate for some of my clients. Reach to me if you have a solid RAG tool, specifically for long pdfs!",
          "score": 1,
          "created_utc": "2026-02-26 10:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jxss0",
              "author": "adukhet",
              "text": "Yeah.. this actually matters a lot for how you configure a RAG system.\nMost implementations assume users will chat for a while, but in practice people ask 1–2 questions, open a source, and leave.\nBecause of that first-answer reliability and retrieval quality matter way more than conversation memory or huge context. Looks like many devs tune prompts/models and skip this behaviour part",
              "score": 1,
              "created_utc": "2026-02-26 17:57:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7m7qyb",
              "author": "Mammoth-Camel1508",
              "text": "This is actually something we are focusing on with our solution - we build rag for businesses with lots of heavy technical documentations and manuals. Mainly for manufacturing and agriculture industry, where it's important to get it right, otherwise the end users won't use it.\n\nSome customers have 1M+ pages, with documents 1k+ pages long. I'd be happy to talk if you have some potential customers where it might help them.",
              "score": 1,
              "created_utc": "2026-02-27 00:42:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mf3kg",
                  "author": "Wide_Brief3025",
                  "text": "Getting users to trust and actually use AI search in such technical environments comes down to surfacing the most relevant info at the right time and being quick to incorporate feedback. Tracking where users ask questions outside your tool can give a lot of insight into gaps. If you want to catch those unaddressed conversations across platforms, ParseStream helps surface them in real time so you can engage or learn what people actually need.",
                  "score": 1,
                  "created_utc": "2026-02-27 01:24:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rgwz5h",
      "title": "Reality check",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rgwz5h/reality_check/",
      "author": "GDAO54",
      "created_utc": "2026-02-28 08:05:59",
      "score": 14,
      "num_comments": 20,
      "upvote_ratio": 0.9,
      "text": "I’m looking for a \"no-BS\" reality check from anyone running RAG on top of large Document Management Systems (100k+ files).\n\nWe are looking at existing agents like M-Files Aino. Will test this in a few weeks. For another more custom eQMS system (with well-developed API endpoints), we are looking at a custom solution to manage a large repository of around 200k pdfs. My concern is whether the tech is actually there to support high-stakes QMS workflows \n\nIs the current tech stack (RAG/Agentic) actually precise enough for \"needle-in-a-haystack\" queries? If a user asks for a specific tolerance value in a 50-page spec, does it reliably find it, or does it give a \"semantic hallucination\"?\n\nAuthorization: How do you handle document permissions? If I have 100k files with complex authorizations, how do you sync those permissions to the AI's vector index in real-time so users don't \"see\" data they aren't cleared?\n\nAll in all, is the tech there for this or should we wait another year?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rgwz5h/reality_check/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7ul2h5",
          "author": "vanwal_j",
          "text": "We’re running a RAG on legal documents, +700k documents at the moment, and twice as much really soon.\n\nWe choose to go with small buckets (~5k docs per bucket at most) in Postgres for “private” documents and a larger pool in qdrant for public/shared ones (can’t really speak here about the product unfortunately) all embeded with Cohere embed 4 + Reranker 4\n\nRegarding the needle in a haystack problem, it really depends on the use case, if you’re user expect a “search tool” they could be disappointed, unless your budget is quite large and you can throw LLM refining in there 😬",
          "score": 2,
          "created_utc": "2026-02-28 08:30:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7up7xa",
              "author": "Natural_Squirrel_666",
              "text": "So you use both pgvector and qdrant?",
              "score": 1,
              "created_utc": "2026-02-28 09:08:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7v6l3m",
                  "author": "vanwal_j",
                  "text": "Yes ! We naively started with pg_vector but as our data pool kept growing our hnsw index memory usage too and since our cloud provider nodes are quite expensive for anything above 12Gb of memory we had to move to qdrant which allows horizontal scaling thanks to sharding",
                  "score": 1,
                  "created_utc": "2026-02-28 11:52:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v4jwo",
          "author": "adukhet",
          "text": "short answer.. yes it can work at 100k–200k PDFs, but only if you treat it like a serious search system, not a magic chatbot. if you combine keyword search + vectors, keep chunks tight, filter hard on metadata, and force answers to cite exact text spans, you can get very solid “needle in a haystack” performance.. if you don’t, you’ll get confident but slightly wrong answers.\n\npermissions are doable but they’re real engineering work. best pattern is enforcing access control at retrieval time using your existing acl model, synced through events from your dms or eqms api into the vector index.. per-user indexes usually don’t scale.\n\noverall the tech is there today for high-stakes qms, but only with careful architecture, testing, and monitoring.. not as a plug-and-play agent feature.",
          "score": 2,
          "created_utc": "2026-02-28 11:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ujksw",
          "author": "fabkosta",
          "text": "Unfortunately, your question remains without a generic answer.\n\nFirst, you need to determine the target needs of your users. Then you have to translate the needs to measurable metrics If you have no measurable metric you cannot determine the quality at all.\n\nPerceived quality in information retrieval heavily depends on both the users' needs and the data you have. Long, windy, technical manuals are very different to process than e.g. Xwitter data or Reddit posts.\n\nParticularly for technical manuals you may consider not using RAG at all, but instead use vector (or, usually better: hybrid) search instead. So, you let the user decide what search result is the right one, but improve the search quality by going hybrid.\n\nAnother very important point that many engineers somehow miss: Design the UI first before thinking about quality of the information retrieval system. Why you should do that is: Almost always you can have users apply filters and those filters will remove a large part of the documents in your search space. Obviously, if you cut down 200k docs to only 5k ones, that's a massive gain in reduction of the search space, and it helps your information retrieval system massively in finding the right answer. The more filter people set, the easier it gets.\n\nRAG on gigantic datasets ***can*** work. That's what [Perplexity.ai](http://Perplexity.ai) shows us. But be prepared to have to put a lot of work into engineering. Engineering must be hypothesis-based, and hypotheses must be tested, measured, and then either rejected or accepted. That's an iterative, measured approach. Typically in RAG setups, 20% is the technology setup itself, and 80% is understanding and tweaking the data and indexes.",
          "score": 1,
          "created_utc": "2026-02-28 08:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7um2mx",
          "author": "AsparagusKlutzy1817",
          "text": "Regarding the needle part of your post - if there is an actual sentence that says it like this eg “the tolerance value is 50” it should be retrieved but if in every document you have you find one or more such sentences this will be barely precise enough. The local context in the text like mentioning of machines or process steps can help to increase the likelihood of finding want you want but it really depends on how similar or dissimilar the documents are. We ran similar rag tasks in quality process management and users reported quite satisfactory results with a plain rag. At least for us it worked quite nicely",
          "score": 1,
          "created_utc": "2026-02-28 08:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7upmha",
              "author": "Necessary-Dot-8101",
              "text": "Compression-Aware Intelligence shows that contradiction is the visible trace of hidden compression loss, and that tracking it is necessary for reliable intelligence",
              "score": 1,
              "created_utc": "2026-02-28 09:12:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7updtw",
          "author": "Necessary-Dot-8101",
          "text": "u need compression aware intelligence",
          "score": 1,
          "created_utc": "2026-02-28 09:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uq1yf",
          "author": "AISmoothBrain",
          "text": "It’s certainly possible. We built an eQMS that we’re selling to customers right now, that have similar size document datasets that span multiple products and sites. Check us out. https://sanai.ai/",
          "score": 1,
          "created_utc": "2026-02-28 09:16:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7utsf1",
          "author": "cointegration",
          "text": "Since others have addressed the ingestion and retrieval i will try to address the security side of it, my build is on postgresql, i collapsed as much as i can into it to reduce maintenance overheads, even the knowledge graph is implemented in postgresql, anything 4 hops and below is fast. During ingestion every document is given a security classification id, so it becomes quite trivial to exclude certain documents from a select based on ACL, this is done before the reranker so it produces the same top k minus redacted chunks.",
          "score": 1,
          "created_utc": "2026-02-28 09:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uxifa",
          "author": "Otherwise_Wave9374",
          "text": "For high-stakes QMS, your instincts are right to be skeptical. The tech can work, but only with tight constraints: citation-required answers, chunking that preserves tables, and a retrieval stack you can measure (not just \"seems good\"). For needle-in-a-haystack specs, I usually see better results with hybrid search (BM25 + vectors) and forced quote extraction from the source paragraph.\n\nPermissions is the harder part, you basically need ACL-aware indexing (per-doc or per-chunk) and a query-time filter tied to the users identity, otherwise you will leak. If youre evaluating agentic RAG patterns, Ive got some notes on grounding, evals, and access control approaches here: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-28 10:30:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v3gv4",
          "author": "Morphos91",
          "text": "We have a dms with 1M documents and the rag still works great.",
          "score": 1,
          "created_utc": "2026-02-28 11:25:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vh7og",
          "author": "voycey",
          "text": "The tradeoff is between how much you pre-process the dataset, you need to build in several different layers to get what you want, including corpus reduction, relationship and ontology mapping, combined with vector search, reranking and a few other tricks! \n\nWhat you want to do is absolutely possible with the correct pipeline!",
          "score": 1,
          "created_utc": "2026-02-28 13:12:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vo7yv",
          "author": "TechnicalGeologist99",
          "text": "There is no escaping human in the loop and a strong citation system for high stakes documents retrieval.",
          "score": 1,
          "created_utc": "2026-02-28 13:56:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xtn62",
          "author": "ampancha",
          "text": "The authorization concern is the right one to prioritize. Most teams sync permissions at index time but don't handle the failure modes: what happens when permissions change mid-session, when sync lags, or when prompt injection bypasses retrieval filters entirely. For high-stakes QMS, you'll also need audit trails proving the AI layer respected authorization boundaries, not just that the vector DB had correct metadata. Sent you a DM",
          "score": 1,
          "created_utc": "2026-02-28 20:37:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xtq0x",
          "author": "No_Indication_1238",
          "text": "By tying the vector to the ID of the auth group. If a vector from the wrong group is received, programatically remove it.",
          "score": 1,
          "created_utc": "2026-02-28 20:37:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7unvk0",
          "author": "Infamous_Ad5702",
          "text": "“Needle in haystack” queries I won’t rely on vector search. I need breadth and depth so I built a tool. See if it works for your data collection? We are able to scale to large volumes.\n\nLeonata replaces RAG by eliminating embeddings, vector search, and probabilistic retrieval entirely — it lets you query your data directly through deterministic semantic structure instead of approximating meaning through similarity.\n\nRag guesses. Leonata knows.\n\nOur defence clients needed offline. No GPU. No Hallucination. So that’s what we built.",
          "score": -5,
          "created_utc": "2026-02-28 08:56:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vdaoi",
              "author": "ajroyalmx",
              "text": "Isn't that how vector search work too? Embedding words based on semantic structure in the vector space so that similar words come near each other?",
              "score": 1,
              "created_utc": "2026-02-28 12:45:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7w4ohg",
                  "author": "Infamous_Ad5702",
                  "text": "Vector works by moving in straight lines almost. Finding similar items. This isn’t vector it finds specificity and best fit. So you end up with unknown unknowns. I just submitted a poster to a conference in stuttgart. Can post it.",
                  "score": 1,
                  "created_utc": "2026-02-28 15:28:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rf54jj",
      "title": "So I made a GraphRAG product but i don't really know how to sell it.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rf54jj/so_i_made_a_graphrag_product_but_i_dont_really/",
      "author": "graphmesh-ai",
      "created_utc": "2026-02-26 08:49:35",
      "score": 13,
      "num_comments": 31,
      "upvote_ratio": 0.77,
      "text": "As title, I made an embeddable GraphRAG ingestion + retrieval as a service product. I know this is valuable but i have no idea how to get it in front of the people who might want it, nor really even who i should target marketing towards?\n\nAre small businesses starting to consider this stuff or is document intelligence still something that only large businesses are considering right now?\n\nFor reference its [graphmesh.ai](http://graphmesh.ai) ive put on a 20,000 free token promo but is selling by the token even the right way to go?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rf54jj/so_i_made_a_graphrag_product_but_i_dont_really/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7hwxnr",
          "author": "vornamemitd",
          "text": "When visiting your site:  \n\\- I don't whether you are talking to me B2C or B2B  \n\\- I don't know what sets it apart from already embedded offerings at Azure, AWS, GCP, OAI, Cohere et al.  \n\\- I don't know where my files go and what happens to them in the background  \n\\- I don't know what models you are using and who is running them  \n\\- I am confused what you mean by \"agent\" and \"chatspace\"  \n\\- I don't know how it compares security-, performance-, or reliability-wise  \n\\- I don't see any pricing/licensing details other than a hint at token charges hidden deeply in the terms  \n  \nSo unfortunately you are giving me no reason to stay on the site while planting a huge number of red flags.   \n",
          "score": 15,
          "created_utc": "2026-02-26 11:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hx1e0",
              "author": "graphmesh-ai",
              "text": "This is amazing feedback, thank you",
              "score": 8,
              "created_utc": "2026-02-26 11:31:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hzf74",
                  "author": "vornamemitd",
                  "text": "I think that there is still space for smart/bespoke retrieval solutions. In reality everyone keeps struggling to chat with three PDFs on their Sharepoint - even with Accenture on speed-dial (or because of that). But it's a super-crowded and over-slopped segment at the same time. Plus: I'd really try to ride the EU sovereignty wave - potentially partner up with an EU hosting/DC-provider and hit medium-sized enterprises like there was no tomorrow. Good luck! ",
                  "score": 4,
                  "created_utc": "2026-02-26 11:50:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7im930",
                  "author": "welcome-overlords",
                  "text": "Excellent that you take feedback so well",
                  "score": 2,
                  "created_utc": "2026-02-26 14:12:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hk75v",
          "author": "Canadianingermany",
          "text": "This is so very typical of AI- I built this shit before figuring out if anyone needed it.",
          "score": 6,
          "created_utc": "2026-02-26 09:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hkehv",
              "author": "graphmesh-ai",
              "text": "Well, i mean, this is also my day job, businesses are using this, i just dont know how to sell my own version :D",
              "score": -3,
              "created_utc": "2026-02-26 09:36:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7i8526",
                  "author": "kyngston",
                  "text": "doing it as my day job and selling my own version is a direct violation of my employment contract",
                  "score": 1,
                  "created_utc": "2026-02-26 12:51:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hpmlr",
                  "author": "Canadianingermany",
                  "text": "Which usually happens when you build stuff without even knowing what problem you are solving.",
                  "score": -2,
                  "created_utc": "2026-02-26 10:25:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hhzgm",
          "author": "jw520",
          "text": "\n[Fixed]",
          "score": 1,
          "created_utc": "2026-02-26 09:12:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hizse",
              "author": "graphmesh-ai",
              "text": "Well that could not have been more embarrassing, I have fixed :facepalm:",
              "score": 3,
              "created_utc": "2026-02-26 09:22:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hjs8z",
                  "author": "jw520",
                  "text": "I'm working on a product built on this idea (though not an dev/API play, so I'm more of a DIYer/customer.  Not a competitor.\n\nIt seems like there's a lot of possible approaches that can be taken with varying degrees of quality in extraction and mapping. Have you had the same thought? I'm just getting started with graph rag so this is sort of learning conversation for me",
                  "score": 1,
                  "created_utc": "2026-02-26 09:30:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7hi41k",
              "author": "graphmesh-ai",
              "text": "OOF",
              "score": 1,
              "created_utc": "2026-02-26 09:13:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hl4ng",
          "author": "Striking-Bluejay6155",
          "text": "We’ve built something like this albeit more robust at FalkorDB, but we don’t charge for the usage (as we are open source). Some considerations: security, why would someone trust uploading sensitive data to your environment? This is a major soc2 oversight. \nLarge companies don’t need the GUI, they rely on our SDK and different APIs. The user friendly GUI is intended towards people who for some reason don’t use conventional tools like Gemini to chat with their pdf. In essence, if you’re ingesting large pdfs, are very good at entity extraction and deduplication, and can prove that your produce more accurate answers than others, then perhaps you could charge a monthly access fee with unlimited pdf uploads.",
          "score": 1,
          "created_utc": "2026-02-26 09:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hlo16",
              "author": "graphmesh-ai",
              "text": "Fair considerations, SOC2 and GDPR are concerns to be sure. All extracted data stored in blob or database is encrypted and the databases themselves are encrypted at rest but the ToCs explicitly state that no PII or sensitive information should be uploaded.\n\nWhat i was aiming for was small entities that wanted a RAG knowledge base for internal use or customer product queries, hence the simple UI and embedded chat, should be easy for a small business solo developer to drop the product specs or manuals into a library and embed the chat on a page for their customers. Im guessing usage will be relatively low so the \"only pay for what you use\" seemed sensible.\n\nIt is pretty good at identifying facts in the documents and can also extract and query tables found in the documents. \n\nAs far as deduplication goes it can automatically version documents so if you upload a new version it will identify the existing document and roll the updates into that hence the blobs vs items distinction on the API.",
              "score": 1,
              "created_utc": "2026-02-26 09:48:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hlxy0",
                  "author": "Striking-Bluejay6155",
                  "text": "Sounds cool, looking forward to updates",
                  "score": 1,
                  "created_utc": "2026-02-26 09:51:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7j98c5",
              "author": "AbsolutelyYouDo",
              "text": "Coming from Neo4j, I see lots of posts about FalcorDB being the newer, better option. I've already built quite a bit on Neo4j, and I doubt I'll be able to change over, but now I'm having to build my own ingest / ETL / knowledge graph RAG piece. Could you give more info about which specific part of FalcorDB you're referring to?",
              "score": 1,
              "created_utc": "2026-02-26 16:04:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jdg54",
                  "author": "Striking-Bluejay6155",
                  "text": "Sure, FalkorDB graphrag-SDK. May I ask what would prevent you from making the switch? I’m not trying to sell, just trying to improve our migration docs. We’ve had a big blue chip observability platform switch over recently, so there may be tips I can share. At any rate, if it’s user defined functions or concerns about production readiness, happy to chat",
                  "score": 2,
                  "created_utc": "2026-02-26 16:23:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdc18r",
      "title": "So what are you all using for RAG in 2026?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rdc18r/so_what_are_you_all_using_for_rag_in_2026/",
      "author": "ReporterCalm6238",
      "created_utc": "2026-02-24 09:50:24",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.94,
      "text": "Looking for easy but effective ways of integratong RAG in my applications. Is there a clear winner framework/tool in terms of performance and quick serup?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rdc18r/so_what_are_you_all_using_for_rag_in_2026/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7449rk",
          "author": "hrishikamath",
          "text": "https://github.com/kamathhrishi/finance-agent I just chain API calls together with pgvector and works well.",
          "score": 5,
          "created_utc": "2026-02-24 10:35:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o740miv",
          "author": "xeraa-net",
          "text": "1. Retrieval is any retrieval, not just vector search. We all agree on that, right?!\n\n2. Do frameworks still matter as much with all the code generation? \n\n3. Good retrieval features are now table stakes and vector search is a feature, not a product (though I'm biased here).",
          "score": 4,
          "created_utc": "2026-02-24 10:01:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75i847",
          "author": "yafitzdev",
          "text": "I build a RAG with production use in mind. I use a yaml plugin system to use any LLM (local or cloud). Also implemented epistemic honesty constraints that I benchmark, super important for production use. The retrieval intelligence is the key piece:\n\n[yafitzdev/fitz-ai](https://github.com/yafitzdev/fitz-ai)",
          "score": 2,
          "created_utc": "2026-02-24 15:41:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7xfjdb",
              "author": "UnstableManifolds",
              "text": "Very interesting, if I didn't get it wrong it's a stand-alone tool but is it possible to integrate it externally (for example via MCP, in Claude Code)?",
              "score": 1,
              "created_utc": "2026-02-28 19:22:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o793soy",
          "author": "prodigy_ai",
          "text": "If you plan to run RAG in production, AWS and Azure offer managed services for embeddings, vector search, orchestration, and security, which can reduce operational overhead compared to self-hosted setups.",
          "score": 2,
          "created_utc": "2026-02-25 02:08:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75hkxy",
          "author": "http418teapot",
          "text": "Can you share more about your use case, data, and expected queries? It would also be helpful to understand what the current tech stack is that you're trying to integrate into. This info will help people make recommendations that are more fit for you.\n\nIf you're truly looking for quick setup, I recommend Pinecone Assistant as it handles all the data chunking, embedding, search, and reranking for you.",
          "score": 1,
          "created_utc": "2026-02-24 15:38:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o77e1ey",
          "author": "pgEdge_Postgres",
          "text": "Any feedback on our RAG server for PostgreSQL (open source under the PostgreSQL license) would be well appreciated here, if any have used it or want to check it out. [https://github.com/pgEdge/pgedge-rag-server](https://github.com/pgEdge/pgedge-rag-server)",
          "score": 1,
          "created_utc": "2026-02-24 20:48:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7a95cx",
          "author": "DeadPukka",
          "text": "Check out [Graphlit](https://www.graphlit.com). Handles all your unstructured data ingestion, embeddings and multimodal search. (Just added TwelveLabs embeddings for video search.)\n\nFree plan and SDKs for Python and TS. Can share as MCP with retrieval tools. \n\nDon’t bother building anything DIY these days.",
          "score": 1,
          "created_utc": "2026-02-25 06:42:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7cwcpo",
          "author": "ExtraManagement1329",
          "text": "Depends on your constraints. If data privacy matters or you're in a regulated industry, the managed options  become a problem quickly — your documents sit on their servers.\n\nFor quick setup with everything in one place, I've been building [Essofore](https://aws.amazon.com/marketplace/pp/prodview-ux72f4arqu376) — it's a self-hosted AMI that runs in your own AWS VPC. Handles chunking, embeddings, vector storage and search automatically so you don't have to wire up the pipeline yourself. *Upload a document, run a search query, done*.\n\nIf you're fine with SaaS, LangChain + a vector db Pinecone/Qdrant/Milvus/Chroma/Weaviate is still the path of least resistance for prototyping. If you want self-hosted and simple, worth a look.",
          "score": 1,
          "created_utc": "2026-02-25 17:12:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g4a7h",
          "author": "astro_abhi",
          "text": "Check out VectraSDK - https://vectra.thenxtgenagents.com , I built it specifically to solve and make an SDK which is open source and LLM Agnostic framework",
          "score": 1,
          "created_utc": "2026-02-26 02:51:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ha1jn",
          "author": "martinschaer",
          "text": "My stack: [SurrealDB](https://surrealdb.com) (vector + graph + …), Pydantic AI, Logfire, Kreuzberg (doc parsing)",
          "score": 1,
          "created_utc": "2026-02-26 07:56:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hhy2c",
          "author": "Big_Barnacle_2452",
          "text": "If you hit the usual ceiling (wrong chunks, vague answers, bad performance on long/structured docs), it’s worth trying structure-based retrieval instead of only vector search. We built ReasonDB for that: keeps document hierarchy (headings → sections) and lets the LLM navigate the tree instead of “top-k similar chunks.” SQL-like query language (RQL) with SEARCH + REASON clauses. Works with OpenAI, Anthropic, Gemini, Cohere, or open source models (GLM, Kimi).\n\n[https://github.com/reasondb/reasondb](https://github.com/reasondb/reasondb) \\- open source.   \nDocs at [reason-db.devdoc.sh](http://reason-db.devdoc.sh) if you want to try it.",
          "score": 1,
          "created_utc": "2026-02-26 09:12:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcehj1",
      "title": "Built an offline MCP server that stops LLM context bloat using local vector search over a locally indexed codebase.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcehj1/built_an_offline_mcp_server_that_stops_llm/",
      "author": "Trust_Me_Bro_4sure",
      "created_utc": "2026-02-23 11:00:50",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "Searching through a massive codebase to find the right context for AI assistants like Claude was becoming a huge bottleneck for me—hurting performance, cost, and accuracy. You can't just dump entire files into the prompt; it instantly blows up the token limit, and the LLM loses track of the actual task.\n\n\n\nInstead of LLM manually hunting for correct files using grep/find &  dumping raw file content into the prompt, I wanted the LLM to have a better search tool.\n\nSo, I built code-memory: an open-source, offline MCP server you can plug right into your IDE (Cursor/AntiGravity) or Claude Code.\n\n\n\nHere is how it works under the hood:\n\n1. Local Semantic Search: It runs vector searches against your locally indexed codebase using jinaai/jina-code-embeddings-0.5b model. \n\n2. Smart Delta Indexing: Backed by SQLite, it checks file modification times during indexing. Unchanged files are skipped, meaning it only re-indexes what you've actually modified. \n\n3. 100% Offline: Your code never leaves your machine.\n\n\n\nIt is heavily inspired by claude-context, but designed from the ground up for large-scale, efficient local semantic search. It's still in the early stages, but I am already seeing noticeable token savings on my personal setup!\n\n\n\nI'd love to hear feedback, especially if you have more ideas!\n\nCheck out the repo here: [https://github.com/kapillamba4/code-memory](https://github.com/kapillamba4/code-memory)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcehj1/built_an_offline_mcp_server_that_stops_llm/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xlz2v",
          "author": "picturpoet",
          "text": "will it handle something like when the dev says look into “auth” and it knows it has to look into authentication files?",
          "score": 1,
          "created_utc": "2026-02-23 11:08:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmkf0",
              "author": "Trust_Me_Bro_4sure",
              "text": "Yes, This is from one of my personal projects:\n\nhttps://preview.redd.it/wiwlcj5i98lg1.png?width=1178&format=png&auto=webp&s=1bf55ebce664e0007746dfded7ad5d2f9a5edbe1\n\n",
              "score": 1,
              "created_utc": "2026-02-23 11:13:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72nuf3",
          "author": "Oshden",
          "text": "This is awesome! Thanks for sharing it OP!",
          "score": 1,
          "created_utc": "2026-02-24 03:21:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1re7bpm",
      "title": "RAG inline citation",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1re7bpm/rag_inline_citation/",
      "author": "Ankan_myname",
      "created_utc": "2026-02-25 08:04:18",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "Hi Everyone,\nCan anyone help me guide how in my RAG application I can show the source citations as well along woth the answer. I need to show the citations inline kind of like how it is shown in perplexity or other gen ai chat apps. \nThe answer will be in streaming form as I have to send the sources along with the token streams.\n\nIf anyone have done something similar or know about any open source repo where it is done. Please guide me to it. \nThanks a lot ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1re7bpm/rag_inline_citation/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7aygxx",
          "author": "johnrock001",
          "text": "In your prompt mention to ai to provide citations for any knowlege it gives.\n\nWorks for me at least, the way i have set it up.",
          "score": 3,
          "created_utc": "2026-02-25 10:34:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bm2mn",
          "author": "hrishikamath",
          "text": "1) prompt your LLM to mention the exact chunk number with some format like [1] or something 2) in the end add a sources section with all the sources or document chunk/websites 3) in your frontend for every answer replace [3] in the answer with a link to the sources section citation link. I have implemented it in https://github.com/kamathhrishi/finance-agent.",
          "score": 2,
          "created_utc": "2026-02-25 13:26:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hs1hh",
          "author": "graphmesh-ai",
          "text": "when you get your similarity results back have the llm evaluate and pick the best ones to answer from in your retrieval pipeline, grab those from the database and send a payload as the first chunk. \n\nyou can just ask the llm to add citations to its response but to nicely decorate them having the selection step before the final response makes it nice and easy to decorate and return",
          "score": 1,
          "created_utc": "2026-02-26 10:47:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}