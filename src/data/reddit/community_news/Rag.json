{
  "metadata": {
    "last_updated": "2026-02-16 09:18:16",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 137,
    "file_size_bytes": 204036
  },
  "items": [
    {
      "id": "1r1o9qz",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r1o9qz/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:24",
      "score": 282,
      "num_comments": 45,
      "upvote_ratio": 0.98,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) – 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nIt’s trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r1o9qz/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4s40uy",
          "author": "Negative-Age-4566",
          "text": "Cool project !   \nAs an improvement I'd suggest doing not only semantic search bu hybrid search using dense embeddings for semantic search + BM25 for keyword search.   \nHeres a tutorial on how to do it using qdrant [https://qdrant.tech/documentation/tutorials-search-engineering/reranking-hybrid-search/](https://qdrant.tech/documentation/tutorials-search-engineering/reranking-hybrid-search/)\n\n",
          "score": 21,
          "created_utc": "2026-02-11 11:11:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sdoe4",
              "author": "Cod3Conjurer",
              "text": "Appreciate it\n\nYeah, hybrid search (dense + BM25) makes a lot of sense, especially for exact entity lookups.\n\nI'll definitely explore that direction combining semantic retrieval with lexical scoring could improve precision quite a bit.",
              "score": 6,
              "created_utc": "2026-02-11 12:26:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rpukb",
          "author": "ShadowStormDrift",
          "text": "Recommend adding reviewer+corrector agents. \n\nReviewer double checks answers grounded in context and penalizes omissions. \n\nCorrector does what you think it does.\n\nIncreases latency but good when you need to be extra sure you aren't seeing bullshit.\n\nOtherwise providing a source list alongside responses for manual review",
          "score": 9,
          "created_utc": "2026-02-11 09:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tf1fk",
              "author": "Cod3Conjurer",
              "text": "never thought of it that way definitely gonna try",
              "score": 1,
              "created_utc": "2026-02-11 15:51:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50yg30",
                  "author": "MrDetectiveSir",
                  "text": "How can you think of this? This is literally bare minimum",
                  "score": 1,
                  "created_utc": "2026-02-12 18:38:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzc2h",
          "author": "redditorialy_retard",
          "text": "Epstein bot incoming ",
          "score": 10,
          "created_utc": "2026-02-11 05:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s6zr0",
              "author": "GreenHell",
              "text": "You mean [MechaEpstein-8000](https://www.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/)? Someone at /r/LocalLLaMA already did that.",
              "score": 7,
              "created_utc": "2026-02-11 11:36:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4qzijv",
              "author": "Cod3Conjurer",
              "text": "what?",
              "score": 0,
              "created_utc": "2026-02-11 05:08:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ro0g1",
                  "author": "nikita2206",
                  "text": "As in chatbot with epstein personality, grounded in facts",
                  "score": 4,
                  "created_utc": "2026-02-11 08:44:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4t8di8",
          "author": "Tight-Actuary-3369",
          "text": "It's incredible, man. Keep it up, excellent work.",
          "score": 3,
          "created_utc": "2026-02-11 15:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4teczl",
              "author": "Cod3Conjurer",
              "text": "thanks man",
              "score": 1,
              "created_utc": "2026-02-11 15:48:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o55h4tl",
                  "author": "DeliciousWalk9535",
                  "text": "Appreciate it! Always looking to improve. Any specific features or optimizations you think would be cool to add?",
                  "score": 1,
                  "created_utc": "2026-02-13 12:17:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4vzz3x",
          "author": "Legitimate_Sherbet_7",
          "text": "I have built rag pipelines commercial and open source that can also do this . But from what I understand the most important information in these files is graphical in nature. The challenge is to sift through and draw conclusions on millions of pages of video graphical and text media. RAG alone is  not going to help do that on its own .",
          "score": 3,
          "created_utc": "2026-02-11 23:16:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xk95e",
              "author": "Cod3Conjurer",
              "text": "RAG is a retrieval layer, not a full investigative system. Multimodal indexing + structured extraction would be the next level.",
              "score": 1,
              "created_utc": "2026-02-12 05:05:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4z50ei",
                  "author": "Legitimate_Sherbet_7",
                  "text": "Yes very nice work. I did not know the dataset was available in txt format. It would be interesting to experiment with different persona layer prompts to look for certain types of patterns. Good luck with your continued development.",
                  "score": 2,
                  "created_utc": "2026-02-12 13:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yyzlc",
          "author": "agilek",
          "text": "Deploy it somewhere, male it accessible for normal people and you’ll get massive traffic! JMail lovers will go nuts.",
          "score": 3,
          "created_utc": "2026-02-12 12:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zosnt",
              "author": "Cod3Conjurer",
              "text": "Yeah, that’s the plan.  \nIf it gets enough traction and consistent usage, I’ll deploy it online and make it publicly accessible.",
              "score": 1,
              "created_utc": "2026-02-12 15:04:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53fv25",
          "author": "PayMelodic3377",
          "text": "I think it’s awesome you’re open-sourcing this. Having spent 13 years at Google Legal Discovery and 16 years as a computer forensic examiner, I can tell you from experience: raw email data is messy, and AI safety guardrails make it dangerously polite.\n\nMost AI models are trained to be agreeable, not skeptical. They won't naturally assume a group sounds criminal or is hiding something—they tend to treat document content as a \"fact\" by default. But in high-stakes truth-seeking, the text is often the smoke screen.\nIn my opinion, to make this truly groundbreaking, the RAG pipeline needs to incorporate a Dynamic Truth Table (a live Chronology), a people index that tracks people as objects so the system can do a better job of knowing J.E=Jeffery Epstein and also jevacation@gmail.com=Jeffery Epstein etc. \n\nThere are reports from back in the 90s that Trump would call into the news rooms under a fake name (always the same fake name) to spread rumors about himself and keep his name in the news cycle - something a living rolodex would help to normalize  \n\n\nWe’ve seen this play out in real-time with the 2026 investigations in the UK and Belarus. For years, people like Prince Andrew claimed they had no contact with Epstein after 2009, but the latest document trickles are proving that wasn't true. Storing facts like Prince Andrew claims he has not contacted Epstein since 2009 would hopefully be able to identify where the document trove conflicts with Prince Andrews claims.  \n\nIf you can build a pipeline that is \"living and breathing,\" you could feed it those new jurisdictions' findings, estate records, and community-sourced photos as they surface. By grounding the AI in a constantly updating chronology rather than just a static vector database, you give the agent a way to \"cross-examine\" the documents.\nWhen the machine has a verified timeline to push back against the raw text, it moves from being a simple search tool to a forensic powerhouse.\n\nKeep it up—this has the potential to be a world-class truth-seeking engine.",
          "score": 3,
          "created_utc": "2026-02-13 02:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4rhhbj",
          "author": "FortiCore",
          "text": "Vibe coded ? Looks like result of a one or two prompts  \nsame for CrawlAI-RAG you posted yesterday.\n\nEven this post is AI generated",
          "score": 9,
          "created_utc": "2026-02-11 07:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ro5zy",
              "author": "Cod3Conjurer",
              "text": "Once you build a solid RAG pipeline, you can plug in any large dataset, it's mostly about preprocessing and tuning retrieval, not rewriting everything.\n\nSo no, it's not just \"two prompts.\" The architecture and pipeline matter.\n\nAnd yes, I use Al to help write code and posts, but I understand the full process and control the system end-to-end",
              "score": 4,
              "created_utc": "2026-02-11 08:45:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uyf50",
                  "author": "UseMoreBandwith",
                  "text": "except, it is not 'solid'.   \nChunking is not correct. (no variable size, no chunking by paragraph, wordlength or sentence...)  \nThe cleaning is silly.  \nNo reranking.  \netc...etc..",
                  "score": 5,
                  "created_utc": "2026-02-11 20:10:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4rs3bm",
                  "author": "FortiCore",
                  "text": "Rudimentary cleaning, chunking, dedups and embedding is a basic functional RAG\nWouldnt call it a \"Solid RAG pipeline\"\n\nThe thing is AI would do this in 5 prompts, if not three (but it will do in three actually).\n\n**There isnt anything special the way the post makes it look like.**\n\nAll it does for \"**Constantly tweaking for better retrieval & performance**\" is : cleanup new lines, and spaces, 80 chars overlap embed in batch. \n\n\nTbvh, it not even at the level of a good RAG POC, its a demo for how to do rudimentary RAG with langchain, would fail to give satisfactory result for almost any real world usecase.\n\nThe post it self is clearly AI written to make it look like a worthy opensource RAG project\n\nStill helpful for some one who want to see RAG in its most simplest form.",
                  "score": 6,
                  "created_utc": "2026-02-11 09:23:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rdzv3",
          "author": "-Cubie-",
          "text": "Very cool! What embedding model are you using?",
          "score": 2,
          "created_utc": "2026-02-11 07:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4redml",
              "author": "Cod3Conjurer",
              "text": "I'm using all-MiniLM-L6-v2 from SentenceTransformers, lightweight, fast, and solid for large-scale semantic retrieval.",
              "score": 7,
              "created_utc": "2026-02-11 07:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rhxx3",
                  "author": "-Cubie-",
                  "text": "Nice, thanks",
                  "score": 2,
                  "created_utc": "2026-02-11 07:46:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xrt67",
          "author": "Otherwise-Platypus38",
          "text": "The source code does not have the proper layout of a FastAPI project. To have all the routes in main is the worst way to structure a FastAPI project. Vibe-coded I assume. Main reason is the format of the comments.",
          "score": 2,
          "created_utc": "2026-02-12 06:07:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zirqi",
              "author": "Cod3Conjurer",
              "text": "The current structure is more prototype-oriented than production-grade.  \nsome parts were AI-assisted, but the architecture decisions and debugging were mine",
              "score": 0,
              "created_utc": "2026-02-12 14:33:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rd0zw",
          "author": "Extension_Armadillo3",
          "text": "I recommend BM25 to find passages with e.g Elon or something like that. And in addition, a re-ranker, but I think that could be hard to find without Zensurship.",
          "score": 3,
          "created_utc": "2026-02-11 07:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rducs",
              "author": "-Cubie-",
              "text": "Normal cross-encoder rerankers shouldn't censor, but they might be bad at parsing the e.g. email formats. I agree that BM25/Hybrid search would be cool, perhaps even with a slider denoting how much weight to give to the semantic vs lexical side (sometimes you might just want full lexical).",
              "score": 2,
              "created_utc": "2026-02-11 07:08:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4re823",
              "author": "Cod3Conjurer",
              "text": "I'm not using BM25 right now, it's pure semantic retrieval using embeddings (SentenceTransformers + Chroma similarity search).\n\n\nHybrid search with BM25 + vector reranking would definitely be an interesting improvement though.",
              "score": 1,
              "created_utc": "2026-02-11 07:11:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ryy75",
          "author": "nirmalyamisra",
          "text": "whats your hardware and tech stack",
          "score": 1,
          "created_utc": "2026-02-11 10:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s18z0",
              "author": "Cod3Conjurer",
              "text": "5060 8gb vram\n24 gb ram\nI7 14gen \n\nPython, LangChain, fastapi",
              "score": 3,
              "created_utc": "2026-02-11 10:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4s1ska",
                  "author": "nirmalyamisra",
                  "text": "thanks",
                  "score": 2,
                  "created_utc": "2026-02-11 10:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uk9dt",
          "author": "degr8sid",
          "text": "Quick question! I'm a newbie with RAG and was wondering why didn't you finetune the model instead of RAG?",
          "score": 1,
          "created_utc": "2026-02-11 19:03:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56fmdj",
          "author": "licjon",
          "text": "This would be a challenging project. I considered it myself. The size, as you already mentioned, is a challenge, which would be fine on its own, but it is a large amount of unstructured data. Much of the data is messy. For example, I looked at a random flight log and found a little known Eastern European model's name. The first name was misspelled and the complete name looked like 2 last names on the log. I would be surprised if an LLM caught that. A lot of data is redacted as well. There is also a lot of ambiguity and lack of context. A lot of emails are continuing from conversations. The famous email with a photo attachment that says \"Age 10\". Without knowing the photo, it could mean anything. People ascribe ill intent because of Epstein's reputation. An LLM may or may not for good or ill. ",
          "score": 1,
          "created_utc": "2026-02-13 15:29:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59d9fe",
          "author": "PipeGenAI",
          "text": "Any knowledge graph in architecture?",
          "score": 1,
          "created_utc": "2026-02-14 00:22:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a213d",
          "author": "404llm",
          "text": "Improve the embedding with https://jigsawstack.com/embedding",
          "score": 1,
          "created_utc": "2026-02-14 02:58:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r03pl",
          "author": "eslove24",
          "text": "Whats your system prompt?",
          "score": 1,
          "created_utc": "2026-02-11 05:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r7sjf",
              "author": "Cod3Conjurer",
              "text": "System prompt:\n\n\n\"You are a retrieval-based assistant. Answer ONLY using the provided context. If the answer is not present, say: 'I could not find this information in the documents.' Limit the answer to 5-6 lines.\"",
              "score": 3,
              "created_utc": "2026-02-11 06:15:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r47duk",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Was Wrong.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r47duk/we_benchmarked_7_chunking_strategies_most_best/",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-14 01:30:40",
      "score": 103,
      "num_comments": 20,
      "upvote_ratio": 0.93,
      "text": "If you've built a RAG system, you've had the chunking conversation. Somebody on your team (or [a Medium post](https://medium.com/%40adnanmasood/chunking-strategies-for-retrieval-augmented-generation-rag-a-comprehensive-guide-5522c4ea2a90)) told you to \"just use 512 tokens with 50-token overlap\" or \"semantic chunking is strictly better.\"\n\nWe (hello from the R&D team at Vecta!) decided to test these claims. We created a small corpus of real academic papers spanning AI, astrophysics, mathematics, economics, social science, physics, chemistry, and computer vision. Then, we ran every document through seven different chunking strategies and measured retrieval quality and downstream answer accuracy.\n\nCritically, we designed the evaluation to be **fair**: each strategy retrieves a different number of chunks, calibrated so that every strategy gets approximately **2,000 tokens of context** in the generation prompt. This eliminates the confound where strategies with larger chunks get more context per retrieval, and ensures we're measuring chunking quality, not context window size.\n\nThe \"boring\" strategies won. The hyped strategies failed. And the relationship between chunk granularity and answer quality is more nuanced than most advice suggests.\n\n# Setup\n\n# Corpus\n\nWe assembled a diverse corpus of 50 academic papers (905,746 total tokens) deliberately spanning similar disciplines, writing styles, and document structures: Papers ranged from 3 to 112 pages and included technical dense mathematical proofs pertaining to fundamental ML research. All PDFs were converted to clean markdown using [MarkItDown](https://github.com/microsoft/markitdown), with OCR artifacts and single-character fragments stripped before chunking.\n\n# Chunking Strategies Tested\n\n1. **Fixed-size, 512 tokens**, 50-token overlap\n2. **Fixed-size, 1024 tokens**, 100-token overlap\n3. **Recursive character splitting**, LangChain-style `RecursiveCharacterTextSplitter` at 512 tokens\n4. **Semantic chunking**, embedding-based boundary detection (cosine similarity threshold 0.7)\n5. **Document-structure-aware**, splitting on markdown headings/sections, max 1024 tokens\n6. **Page-per-chunk**, one chunk per PDF page, using MarkItDown's form-feed (`\\f`) page boundaries\n7. **Proposition chunking**, LLM-decomposed atomic propositions following [Dense X Retrieval](https://arxiv.org/abs/2312.06648) with the paper's exact extraction prompt\n\nAll chunks were embedded with `text-embedding-3-small` and stored in local ChromaDB. Answer generation used `gemini-2.5-flash-lite` via OpenRouter. We generated 30 ground-truth Q&A pairs using Vecta's synthetic benchmark pipeline.\n\n# Equal Context Budget: Adaptive Retrieval k\n\nMost chunking benchmarks use a fixed top-k (e.g., k=10) for all strategies. This is fundamentally unfair: if fixed-1024 retrieves 10 chunks, the generator sees \\~10,000 tokens of context; if proposition chunking retrieves 10 chunks at 17 tokens each, the generator gets \\~170 tokens. The larger-chunk strategy wins by default because it gets more context, not because its chunking is better.\n\nWe fix this by computing an **adaptive k** for each strategy. This targets \\~2,000 tokens of retrieved context for every strategy. The computed values:\n\n|Strategy|Avg Tokens/Chunk|Adaptive k|Expected Context|\n|:-|:-|:-|:-|\n|Page-per-Chunk|961|2|\\~1,921|\n|Doc-Structure|937|2|\\~1,873|\n|Fixed 1024|658|3|\\~1,974|\n|Fixed 512|401|5|\\~2,007|\n|Recursive 512|397|5|\\~1,984|\n|Semantic|43|46|\\~1,983|\n|Proposition|17|115|\\~2,008|\n\nNow every strategy gets \\~2,000 tokens to work with. Differences in accuracy reflect genuine chunking quality, not context budget.\n\n# How We Score Retrieval: Precision, Recall, and F1\n\nWe evaluate retrieval at two granularities: **page-level** (did we retrieve the right pages?) and **document-level** (did we retrieve the right documents?). At each level, the core metrics are precision, recall, and F1.\n\nLet R be the set of retrieved items (pages or documents) and G be the set of ground-truth relevant items.\n\nPrecision measures: of everything we retrieved, what fraction was actually relevant? A retriever that returns 5 pages, 4 of which contain the answer, has a precision of 0.8. High precision means low noise in the context window.\n\nRecall measures: of everything that *was* relevant, what fraction did we find? If 3 pages contain the answer and we retrieved 2 of them, recall is 0.67. High recall means we're not missing important information.\n\nF1 is the harmonic mean of precision and recall. It penalizes strategies that trade one for the other and rewards balanced retrieval.\n\n**Why two granularities matter.** Page-level metrics tell you whether you're pulling the right *passages*. Document-level metrics tell you whether you're pulling from the right *sources*. A strategy can score high page-level recall (finding many relevant pages) while scoring low document-level precision (those pages are scattered across too many irrelevant documents). As we'll see, the tension between these two levels is one of the main findings.\n\n# Results\n\n# The Big Picture\n\n[Figure 1: Complete metrics heatmap. Green is good, red is bad.](https://www.runvecta.com/blog/chunking/metrics_heatmap.png)\n\n|Strategy|k|Doc F1|Page F1|Accuracy|Groundedness|\n|:-|:-|:-|:-|:-|:-|\n|**Recursive 512**|5|0.86|**0.92**|**0.69**|0.81|\n|**Fixed 512**|5|0.85|0.88|0.67|**0.85**|\n|**Fixed 1024**|3|**0.88**|0.72|0.61|0.86|\n|**Doc-Structure**|2|0.88|0.69|0.52|0.84|\n|**Page-per-Chunk**|2|0.88|0.69|0.57|0.81|\n|**Semantic**|46|0.42|0.91|0.54|0.81|\n|**Proposition**|115|0.27|**0.97**|0.51|**0.87**|\n\n**Recursive splitting wins on accuracy (69%) and page-level retrieval (0.92 F1).** The 512-token strategies lead on generation quality, while larger-chunk strategies lead on document-level retrieval but fall behind on accuracy.\n\n# Finding 1: Recursive and Fixed Splitting Often Outperforms Fancier Strategies\n\n[Figure 2: Accuracy and groundedness by strategy. Recursive and fixed 512 lead on accuracy.](https://www.runvecta.com/blog/chunking/generation_quality.png)\n\nLangChain's `RecursiveCharacterTextSplitter` at 512 tokens achieved the highest accuracy (**69%**) across all seven strategies. Fixed 512 was close behind at 67%. Both strategies use 5 retrieved chunks for \\~2,000 tokens of context.\n\nWhy does recursive splitting edge out plain fixed-size? It tries to break at natural boundaries, paragraph breaks, then sentence breaks, then word breaks. On academic text, this preserves logical units: a complete paragraph about a method, a full equation derivation, a complete results discussion. The generator gets chunks that make semantic sense, not arbitrary windows that may cut mid-sentence.\n\nRecursive 512 also achieved the best page-level F1 (**0.92**), meaning it reliably finds the right pages *and* produces accurate answers from them.\n\n# Finding 2: The Granularity-Retrieval Tradeoff Is Real\n\n[Figure 3: Radar chart, recursive 512 (orange) has the fullest coverage. Large-chunk strategies skew toward doc retrieval but lose on accuracy.](https://www.runvecta.com/blog/chunking/radar_comparison.png)\n\nWith a 2,000-token budget, a clear tradeoff emerges:\n\n* **Smaller chunks (k=5)** achieve higher accuracy (67-69%) because 5 retrieval slots let you sample from 5 different locations in the corpus, each precisely targeted\n* **Larger chunks (k=2-3)** achieve higher document F1 (0.88) because each retrieved chunk spans more of the relevant document, but the generator gets fewer, potentially less focused passages\n\nFixed 1024 scored the best document F1 (**0.88**) but only 61% accuracy. With just k=3, you get 3 large passages, great for document coverage, but if even one of those passages isn't well-targeted, you've wasted a third of your context budget.\n\n# Finding 3: Semantic Chunking Collapses at Scale\n\n[Figure 4: Chunk size distribution. Semantic and proposition chunking produce extremely small fragments.](https://www.runvecta.com/blog/chunking/chunk_size_distribution.png)\n\nSemantic chunking produced **17,481 chunks averaging 43 tokens** across 50 papers. With k=46, the retriever samples from 46 different tiny chunks. The result: only **54% accuracy** and **0.42 document F1**.\n\nHigh page F1 (0.91) reveals what's happening: the retriever *finds the right pages* by sampling many tiny chunks from across the corpus. But document-level retrieval collapses because those 46 chunks come from dozens of different documents, diluting precision. And accuracy suffers because 46 disconnected sentences don't form a coherent narrative for the generator.\n\n**The fundamental problem:** semantic chunking optimizes for retrieval-boundary purity at the expense of context coherence. Each chunk is a \"clean\" semantic unit, but a single sentence chunk may lack the surrounding context needed for generation.\n\n# Finding 4: The Page-Level Retrieval Story\n\n[Figure 5: Page-level precision-recall tradeoff. Recursive 512 achieves the best balance.](https://www.runvecta.com/blog/chunking/precision_recall.png)\n\n[Figure 6: Page-level and document-level F1. The two metrics tell different stories.](https://www.runvecta.com/blog/chunking/retrieval_f1.png)\n\nPage-level and document-level retrieval tell opposite stories under constrained context:\n\n* **Fine-grained strategies** (proposition k=115, semantic k=46) achieve high page F1 (0.91-0.97) by sampling many pages, but low doc F1 (0.27-0.42) because those pages come from too many documents\n* **Coarse strategies** (page-chunk k=2, doc-structure k=2) achieve high doc F1 (0.88) by retrieving fewer, more relevant documents, but lower page F1 (0.69) because 2 chunks can only cover 2 pages\n\n**Recursive 512 at k=5 hits the best balance**: 0.92 page F1 and 0.86 doc F1. Five chunks is enough to sample multiple relevant pages while still concentrating on a few documents.\n\n[Figure 7: Document-level precision, recall, and F1 detail. Large-chunk strategies lead on precision; fine-grained strategies lead on recall.](https://www.runvecta.com/blog/chunking/document_level.png)\n\n# What This Means for Your RAG System\n\n# The Short Version\n\n1. **Use recursive character splitting at 512 tokens.** It scored the highest accuracy (69%), best page F1 (0.92), and strong doc F1 (0.86). It's the best all-around strategy on academic text.\n2. **Fixed-size 512 is a strong runner-up** with 67% accuracy and the highest groundedness among the top performers (85%).\n3. **If document-level retrieval matters most**, use fixed-1024 or page-per-chunk (0.88 doc F1), but accept lower accuracy (57-61%).\n4. **Don't use semantic chunking on academic text.** It fragments too aggressively (43 avg tokens) and collapses on document retrieval (0.42 F1).\n5. **Don't use proposition chunking for general RAG.** 51% accuracy isn't production-ready. It's only viable if you value groundedness over correctness.\n6. **When benchmarking, equalize the context budget.** Fixed top-k comparisons are misleading. Use adaptive k = round(target\\_tokens / avg\\_chunk\\_tokens).\n\n# Why Academic Papers Specifically?\n\nWe deliberately chose to saturate the academic paper region of the embedding space with 50 papers spanning 10+ disciplines. When your knowledge base contains papers that all discuss \"evaluation,\" \"metrics,\" \"models,\" and \"performance,\" the retriever has to make fine-grained distinctions. That's when chunking quality matters most.\n\nIn a mixed corpus of recipes and legal contracts, even bad chunking might work because the embedding distances between domains are large. Academic papers are the *hard case* for chunking, and if a strategy works here, it'll work on easier data too.\n\n# How We Measured This (And How You Can Too)\n\nMy team built [Vecta](https://www.runvecta.com/) specifically to meet the need for precise RAG evaluation software. It generates synthetic benchmark Q&A pairs across multiple semantic granularities, then measures precision, recall, F1, accuracy, and groundedness against your actual retrieval pipeline.\n\nThe benchmarks in this post were generated and evaluated using Vecta's SDK (`pip install vecta`)\n\n# Limitations, Experiment Design, and Further Work\n\nThis experiment was deliberately small-scale: 50 papers, 30 synthetic Q&A pairs, one embedding model, one retriever, one generator. That's by design. We wanted something reproducible that a single engineer could rerun in an afternoon, not a months-long research project. The conclusions should be read with that scope in mind.\n\n**Synthetic benchmarks are not human benchmarks.** Our ground-truth Q&A pairs were generated by Vecta's own pipeline, which means there's an inherent alignment between how questions are formed and how they're evaluated. Human-authored questions would be a stronger test. That said, Vecta's benchmark generation does produce complex multi-hop queries that require synthesizing information across multiple chunks and document locations, so these aren't trivially easy questions that favor any one strategy by default.\n\n**One pipeline, one result.** Everything here runs on `text-embedding-3-small`, ChromaDB, and `gemini-2.5-flash-lite`. Swap any of those components and the rankings could shift. We fully acknowledge this. Running the same experiment across multiple embedding models, vector databases, and generators would be valuable follow-up work, and it's on our roadmap.\n\n**The equal context budget is a deliberate constraint, not a flaw.** Some readers may object that semantic and proposition chunking are \"meant\" to be paired with rerankers, fusion, or hierarchical aggregation. But if a chunking strategy only works when combined with additional infrastructure, that's important to know. Equal context budgets ensure we're comparing chunking quality at roughly equal generation cost. A strategy that requires a reranker to be competitive is a more expensive strategy, and that should factor into the decision.\n\n**Semantic chunking was not intentionally handicapped.** Our semantic chunking produced fragments averaging 43 tokens, which is smaller than most production deployments would target. This was likely due to a poorly tuned cosine similarity threshold (0.7) rather than any deliberate sabotage. But that's actually the point: semantic chunking requires careful threshold tuning, merging heuristics, and often parent-child retrieval to work well. When those aren't perfectly dialed in, it degrades badly. Recursive splitting, by contrast, produced strong results with default parameters. The brittleness of semantic chunking under imperfect tuning is itself a finding.\n\n**What we'd like to do next:**\n\n* Rerun the experiment with human-authored Q&A pairs alongside the synthetic benchmark\n* Test across multiple embedding models (`text-embedding-3-large`, open-source alternatives) and generators (GPT-4o, Claude, Llama)\n* Add reranking and hierarchical retrieval stages, then measure whether the rankings change when every strategy gets access to the same post-retrieval pipeline\n* Expand the corpus beyond academic papers to contracts, documentation, support tickets, and other common RAG domains\n* Test semantic chunking with properly tuned thresholds, chunk merging, and sliding windows to establish its ceiling\n\nIf you run any of these experiments yourself, we'd genuinely like to see the results.\n\nHave a chunking strategy that worked surprisingly well (or badly) for you? We'd love to hear about it. Reach out via DM!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r47duk/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5a7tkl",
          "author": "Wimiam1",
          "text": "What made you decide that a fixed cosine threshold of 0.7 was the way to go?",
          "score": 11,
          "created_utc": "2026-02-14 03:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bczg9",
              "author": "TeamCaspy",
              "text": "Probably benchmarked all thresholds?",
              "score": 1,
              "created_utc": "2026-02-14 09:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cjxz0",
                  "author": "Wimiam1",
                  "text": "I really doubt it. There’s no reason semantic chunking should perform so much worse. 47 tokens per chunk on average is a huge red flag to me. These papers are all strictly about a specific topic. That means semantic homogeneity. That means you need a higher threshold to get nicely sized chunks. Furthermore, a good threshold for one document/author might not be good for another. That’s why there are so many adaptive threshold techniques.",
                  "score": 3,
                  "created_utc": "2026-02-14 15:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bh260",
              "author": "Distinct-Target7503",
              "text": "yeah exactly... also those embedders work really bad with fixed similarity thresholds, they are simply not trained for that.",
              "score": 1,
              "created_utc": "2026-02-14 10:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cnhk0",
                  "author": "Jords13xx",
                  "text": "For sure, fixed thresholds can really limit the flexibility of the model. It's like trying to fit a square peg in a round hole when the data might not even have a consistent shape!",
                  "score": 1,
                  "created_utc": "2026-02-14 15:21:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5agbj7",
          "author": "One_Milk_7025",
          "text": "These are the most basic ones.. there are way more variant and strategy out there..\n\nFor starter checkout the chunking playground \nChunker.veristamp.in you can tweak and play all these strategy in browser",
          "score": 9,
          "created_utc": "2026-02-14 04:38:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a8di9",
          "author": "pl201",
          "text": "1. You should publish your code to make your findings more credible. Chunking is only one small step in the whole setup and process. Each step will impact the final retrieval. Docs should be cleaned and OCR results should be reviewed since error rate normally is pretty high.\n2. You should release the doc you have used so other people can validate your claims. There is no worse or better chunks strategy without references to doc type, quality, and relationship. A good chunk method to your doc RAG may be very bad to my.",
          "score": 14,
          "created_utc": "2026-02-14 03:41:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b18pk",
          "author": "nithril",
          "text": "The chunk size of the doc structure approach must be limited, it is a non sense to have such a big chunk. It must be similar to what the recursive approach is doing. Combining the two approaches is actually better, or a doc structure at the paragraph level to have a max chunk size of 512",
          "score": 3,
          "created_utc": "2026-02-14 07:37:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a5vye",
          "author": "zmanning",
          "text": "this is an AI slop advertisement ",
          "score": 11,
          "created_utc": "2026-02-14 03:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bg05s",
              "author": "skadoodlee",
              "text": "This entire subreddit is legitimately the worst on the whole of Reddit ",
              "score": 4,
              "created_utc": "2026-02-14 10:01:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5moggz",
                  "author": "SkyFeistyLlama8",
                  "text": "Full of practitioners reinventing the wheel over and over again.\n\nThe chunking strategy has to fit your corpus and subject matter. If you're dealing with lots of short documents, then stuff the entire thing into your context. If you have to deal with documents that are hundreds of pages long, then $deity help you because it's still not a solved problem.",
                  "score": 1,
                  "created_utc": "2026-02-16 04:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5b20uw",
          "author": "Educational_Cup9809",
          "text": "With new llm and embedding models i dont even care about chunking that much. Always go for page level  and files without pages go for 7000 tokens. spending time on Graph and metadata techniques gives me far better results. Future is all about agentic anyway",
          "score": 2,
          "created_utc": "2026-02-14 07:44:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bvwoe",
              "author": "Free-Ferret7135",
              "text": "Very true! They did semantic chunking with avrg of 43 tokens LOL",
              "score": 5,
              "created_utc": "2026-02-14 12:27:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5c6i2l",
              "author": "nithril",
              "text": "7000 tokens, the best way to produce a vector that is just a semantic soup. Agentic needs precision in the retrieval.",
              "score": 2,
              "created_utc": "2026-02-14 13:42:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5hvmhc",
              "author": "Marengol",
              "text": "Can you elaborate, please? (on graph and metadata techniques)",
              "score": 1,
              "created_utc": "2026-02-15 12:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5brxen",
          "author": "48K",
          "text": "Hopefully simple question: for every query all these approaches produce multiple scores. How do you combine them into a single score?",
          "score": 1,
          "created_utc": "2026-02-14 11:54:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cutwr",
          "author": "my_byte",
          "text": "Test 500 tokens, no overlap with voyage context embeddings and see how they compare to your benchmark winner",
          "score": 1,
          "created_utc": "2026-02-14 15:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eg35g",
          "author": "king_vis",
          "text": "LLM chunking is all you need",
          "score": 1,
          "created_utc": "2026-02-14 20:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f5kr3",
          "author": "guesdo",
          "text": "In my personal experience, doc structure is the best if you delegate RAG to the LLM itself. Instead of you handling retrieval, leave it to the LLM via MCP with a couple of tools including semantic and reverse index search. Reason being, the LLM can explore faster and further and iterate on results while getting exact sections on searches optimizing context size.",
          "score": 1,
          "created_utc": "2026-02-14 23:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bnxel",
          "author": "jannemansonh",
          "text": "the chunking optimization rabbit hole is real... this is why we moved doc workflows to needle app since it handles the rag layer (chunking, embeddings, retrieval). way easier than tuning chunk sizes and testing strategies every time requirements change",
          "score": 0,
          "created_utc": "2026-02-14 11:18:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r129pa",
      "title": "Knowledge Distillation for RAG (Why Ingestion Pipeline Matters More Than Retrieval Algorithm)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r129pa/knowledge_distillation_for_rag_why_ingestion/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-10 14:31:03",
      "score": 60,
      "num_comments": 30,
      "upvote_ratio": 0.95,
      "text": "Been spending way too much time debugging RAG systems that \"should work\" but don't, and wanted to share something that's been bothering me about how we collectively approach this problem.\n\nWe obsess over retrieval algorithms (hybrid search, reranking, HyDE, query decomposition) while completely ignoring that retrieval operates over fundamentally broken representations of knowledge.\n\nI started using a new approach that is working pretty well so far : Instead of chunking, use LLMs at ingestion time to extract and restructure knowledge into forms optimized for retrieval:\n\nLevel 1: Extract facts as explicit SVO sentences\n\nLevel 2 : Synthesize relationships spanning multiple insights\n\nLevel 3 : Document-level summaries for broad queries\n\nLevel 4 : Patterns learned across the entire corpus\n\nEach level serves different query granularities. Precision queries hit insights. Exploratory queries hit concepts/abstracts.\n\nI assume this works well beacuse LLMs during ingestion can spend *minutes* analyzing a document that gets used thousands of times. The upfront cost amortizes completely. And they're genuinely good at:\n\n* Disambiguating structure\n* Resolving implicit context\n* Normalizing varied phrasings into consistent forms\n* Cross-referencing\n\nTested this on a few projects involving financial document corpus : agent with distillation correctly identified which DOW companies were financial institutions, attributed specific risks with page-level citations, and supported claims with concrete figures. Naive chunking agent failed to even identify the companies reliably.\n\nThis is fully automatable with workflow-based pipelines:\n\n1. Table extraction (preserve structure via CV models)\n2. Text generation 1: insights from tables + text\n3. Text generation 2: concepts from insights\n4. Text generation 3: abstracts from concepts\n5. Text generation 4: table schema analysis for SQL generation\n\nEach component receives previous component's output. Final JSON contains original data + all distillation layers.\n\nAnyway figure this is one of those things where the industry is converging on the wrong abstraction and we should probably talk about it more.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r129pa/knowledge_distillation_for_rag_why_ingestion/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4miuy5",
          "author": "charlesrwest0",
          "text": "I have been using a similar approach. I also find it useful to have the model try to predict who would read it/what it would be used for and what associated questions are likely to be asked. Q/A pairs play very nicely with vector databases.",
          "score": 6,
          "created_utc": "2026-02-10 15:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ydr06",
              "author": "Independent-Cost-971",
              "text": "Totally agree. Q/A generation is basically intent distillation at ingestion time. You’re not just storing content,  you’re storing how it’s likely to be queried.",
              "score": 2,
              "created_utc": "2026-02-12 09:36:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mhk0p",
          "author": "penguinzb1",
          "text": "this resonates. the problem with chunking is it destroys the semantic boundaries that actually matter for retrieval. structuring at ingestion makes sense but the hard part is validating that your distillation pipeline is actually extracting what you need without hallucinating connections",
          "score": 5,
          "created_utc": "2026-02-10 15:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ydvof",
              "author": "Independent-Cost-971",
              "text": "Exactly. Chunking optimizes for token limits, not semantic coherence. And yeah validation is the hard part. I’ve been thinking about grounding each distillation layer back to source spans + enforcing traceability so nothing gets synthesized without citations.",
              "score": 2,
              "created_utc": "2026-02-12 09:37:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4myuxh",
              "author": "ButterflyEconomist",
              "text": "For the chunking problem, I overlay them.\n\nIf I want to process data abcd, I chunk it into ab, bc, cd",
              "score": 1,
              "created_utc": "2026-02-10 16:24:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mbw5i",
          "author": "Independent-Cost-971",
          "text": "Anyway, wrote this up in more detail if anyone's interested : [https://kudra.ai/knowledge-distillation-for-ai-agents-and-rag-building-hierarchical-knowledge-from-raw-documents/](https://kudra.ai/knowledge-distillation-for-ai-agents-and-rag-building-hierarchical-knowledge-from-raw-documents/)\n\n( shameless self promotion I know but worth a read )",
          "score": 6,
          "created_utc": "2026-02-10 14:31:40",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4stcxg",
              "author": "True_Context_6852",
              "text": "Good read thank you ",
              "score": 2,
              "created_utc": "2026-02-11 14:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ydsij",
                  "author": "Independent-Cost-971",
                  "text": "Appreciate it! glad it resonated.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:36:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mj0ww",
          "author": "minaminotenmangu",
          "text": "The trouble is cost. all knowledge now has to go through an llm + embedding model. I guess its not too bad for some, it might be intetesting to find cheaper models that could do the rewriting of knowledge.\n\ni feel we need a disconnect between what we embed which is a shorter simpler text to what we retrieve.",
          "score": 3,
          "created_utc": "2026-02-10 15:08:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mk1nn",
              "author": "Popular_Sand2773",
              "text": "You just need to do knowledge distillation. You can get llm behavior at a fraction of the price on narrow tasks although the real savings is if you can move off of generative approaches all together while preserving the behavior. ",
              "score": 2,
              "created_utc": "2026-02-10 15:13:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mkfjb",
                  "author": "minaminotenmangu",
                  "text": "What exactly do you mean by knowledge distillation? Ots never clear to me what this actually entails for a corpus.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nlp2e",
              "author": "isthatashark",
              "text": "100% on your suggestion to use cheaper models. I've been doing a lot of research into this lately and you don't need a frontier model to get good results. \n\nWe use this technique for memory consolidation in Hindsight. Smaller models do a surprisingly good job. I mostly use the ones on Groq because the performance is so fast and the cost is low, but Ollama is also an option if you want something local and free (but slower).",
              "score": 2,
              "created_utc": "2026-02-10 18:09:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4modds",
          "author": "DeepWiseau",
          "text": "How does this work with a growing document library? How would it handle several thousand pages being added a week?",
          "score": 2,
          "created_utc": "2026-02-10 15:34:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n1qwh",
              "author": "Krommander",
              "text": "Probably have to push towards recursive indexing and knowledge anchoring to earlier knowledge graphs already in store for smaller increments. Reset for the whole mapping layer in regular intervals if there needs to be pruning of older info or a redraw of conceptual attractors.",
              "score": 1,
              "created_utc": "2026-02-10 16:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n5psc",
          "author": "Informal_Tangerine51",
          "text": "Distillation at ingestion helps retrieval quality but creates a new debugging problem: when retrieval fails, which distillation layer broke?\n\nYour 4-level approach works until precision query returns wrong info. Now you need to trace: was SVO extraction wrong, relationship synthesis hallucinated, summary incomplete, or pattern recognition overgeneralized? Without evidence of what each distillation step produced, debugging is guesswork. LLMs spent minutes analyzing doc, but you don't have artifacts showing what they extracted at each level.\n\nFinancial doc example: agent identified wrong risk attribution. Is it because Level 1 extracted wrong facts, Level 2 synthesized wrong relationships, or retrieval chose wrong layer? Chunking is simpler to debug - you see exactly what text was retrieved. Multi-layer distillation optimizes for quality but trades debuggability. Production systems need both retrieval quality and incident traceability.",
          "score": 2,
          "created_utc": "2026-02-10 16:55:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye2dj",
              "author": "Independent-Cost-971",
              "text": "Totally fair. That’s why I treat each distillation layer as a first-class artifact, persisted, versioned, and fully traceable back to source spans. If retrieval fails, you can inspect exactly what Level 1 extracted, what Level 2 synthesized, etc.\n\nI’d argue chunking feels easier to debug because it’s shallow, not because it’s better, you see the text, but you don’t see the reasoning gap. Distillation just makes the abstraction explicit, so you have to engineer observability into it.",
              "score": 1,
              "created_utc": "2026-02-12 09:39:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4meyku",
          "author": "fabkosta",
          "text": "What’s a CV model in the context of table extraction?",
          "score": 1,
          "created_utc": "2026-02-10 14:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye40a",
              "author": "Independent-Cost-971",
              "text": "CV = Computer Vision. In table extraction it usually refers to vision models (like layout detection or table structure recognition models) that identify table boundaries, rows, columns, and cell relationships from PDFs or scanned docs before OCR/LLM processing.",
              "score": 1,
              "created_utc": "2026-02-12 09:39:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n0uqg",
          "author": "Krommander",
          "text": "Very nice. I have found distillation to condense well into a recursive semantic hypergraph, as a map of the knowledge and interrelations.",
          "score": 1,
          "created_utc": "2026-02-10 16:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye6up",
              "author": "Independent-Cost-971",
              "text": "That’s a great way to frame it. recursive semantic hypergraph is exactly the right abstraction. Once you move beyond flat chunks, knowledge naturally becomes nodes + higher-order relations. Distillation is basically constructing that graph explicitly instead of hoping embeddings infer it implicitly.",
              "score": 2,
              "created_utc": "2026-02-12 09:40:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yyy13",
                  "author": "Krommander",
                  "text": "Yes exactly, by building your recursive semantic hypergraphs based on validated synthesis of a subject, you are able to drop hallucinations and shorten latency.\n\n\nMemory modules with this architecture are compact but deliver a high quality restoration of facts. \n\n\nThere are some interesting publications on HyperRAG I stumbled upon this summer, that demonstrate the quality and speed of recalls when using the hypergraphs to index the complete sources, instead of flat pair embeddings. ",
                  "score": 1,
                  "created_utc": "2026-02-12 12:37:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zt7b2",
                  "author": "Krommander",
                  "text": "Feng, Y., Hu, H., Hou, X., et al. (2025). Hyper-RAG: Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation. arXiv preprint arXiv:2504.08758.\n\n[https://doi.org/10.48550/arXiv.2504.08758](https://doi.org/10.48550/arXiv.2504.08758)\n\nGarcía, F. G., Shi, Q., & Feng, Z. (2025). Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification. arXiv preprint arXiv:2509.05741.\n\n[https://arxiv.org/pdf/2509.05741](https://arxiv.org/pdf/2509.05741)\n\nHan, H., Wang, Y., Shomer, H., Guo, K., Ding, J., Lei, Y., Halappanavar, M., Rossi, R. A., Mukherjee, S., Tang, X., He, Q., Hua, Z., Long, B., Zhao, T., Shah, N., Javari, A., Xia, Y., & Tang, J. (2025). Retrieval-Augmented Generation with Graphs (GraphRAG). arXiv preprint arXiv:2501.00309.\n\n[https://arxiv.org/pdf/2501.00309](https://arxiv.org/pdf/2501.00309)\n\nHuang, C., Huang, H., Yu, T., Xie, K., Wu, J., Zhang, S., Mcauley, J., Jannach, D., & Yao, L. (2025). A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms. arXiv preprint arXiv:2504.16420.\n\n[https://arxiv.org/pdf/2504.16420](https://arxiv.org/pdf/2504.16420)\n\nLuo, H., Chen, G., Zheng, Y., et al. (2025). HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation. arXiv preprint arXiv:2503.21322.\n\n[https://doi.org/10.48550/arXiv.2503.21322](https://doi.org/10.48550/arXiv.2503.21322) \n\nLuo, H., E, H., Chen, G., Lin, Q., Guo, Y., Xu, F., Kuang, Z., Song, M., Wu, X., Zhu, Y., & Tuan, L. A. (2025). Graph-R1: Towards Agentic Graphrag Frame-Work Via End-To-End Reinforcement Learning. arXiv preprint arXiv:2507.21892.\n\n[https://arxiv.org/pdf/2507.21892](https://arxiv.org/pdf/2507.21892)\n\nSharma, K., Kumar, P., & Li, Y. (2024). OG-RAG: Ontology-grounded retrieval-augmented generation for large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025).\n\n2025.emnlp-main.1674.pdf\n\nWang, C., Deng, W., Guan, W., Lu, Q., & Jiang, N. (2025). Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering. arXiv preprint arXiv:2508.11247.\n\n[https://arxiv.org/pdf/2508.11247](https://arxiv.org/pdf/2508.11247)\n\nZhu, Z., Huang, T., Wang, K., Ye, J., Chen, X., & Luo, S. (2025). Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey. arXiv preprint arXiv:2504.10499.\n\n[https://arxiv.org/pdf/2504.10499](https://arxiv.org/pdf/2504.10499)",
                  "score": 1,
                  "created_utc": "2026-02-12 15:26:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n8j4q",
          "author": "Forsaken-Cod-4944",
          "text": "I'm new to this but wouldnt it take significantly more time to produce chunks with this (depends on llm i guess?)\n\n",
          "score": 1,
          "created_utc": "2026-02-10 17:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4njusw",
              "author": "UBIAI",
              "text": "I think the distillation happens at the document level, not the chunk level.",
              "score": 2,
              "created_utc": "2026-02-10 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4njxj3",
          "author": "isthatashark",
          "text": "We had to tackle a similar problem in Hindsight. I just published a blog post about it yesterday on how we do memory consolidation to handle this: https://hindsight.vectorize.io/blog/2026/02/09/resolving-memory-conflicts",
          "score": 1,
          "created_utc": "2026-02-10 18:01:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xw71l",
          "author": "SharpRule4025",
          "text": "This matches what I've been seeing. Spent weeks tuning rerankers and hybrid search, then realized the chunks themselves were garbage because the source extraction was throwing navigation, sidebars, and footer text into the same chunks as actual content.\n\nSwitched to extracting content into structured fields (headings, paragraphs, lists, metadata) before chunking and the retrieval quality jumped immediately. No reranker changes needed. The hierarchy gives you natural chunk boundaries instead of arbitrary token windows.\n\nThe part about knowledge distillation at ingestion time is interesting. Basically front-loading the intelligence into the pipeline instead of hoping retrieval figures it out. Feels obvious in hindsight but most RAG tutorials skip this entirely and jump straight to vector search tuning.",
          "score": 1,
          "created_utc": "2026-02-12 06:46:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye8ui",
              "author": "Independent-Cost-971",
              "text": "Exactly. Most RAG issues I’ve seen weren’t retrieval problems, they were representation problems. If the chunks mix navigation noise with signal, no reranker will save you.\n\nOnce you respect document structure, retrieval suddenly looks “smart” without changing the algorithm. Distillation just pushes that idea further; fix the knowledge representation first, then worry about search.",
              "score": 1,
              "created_utc": "2026-02-12 09:41:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3mmxy",
      "title": "Semantic chunking + metadata filtering actually fixes RAG hallucinations",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r3mmxy/semantic_chunking_metadata_filtering_actually/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-13 11:21:35",
      "score": 56,
      "num_comments": 23,
      "upvote_ratio": 0.91,
      "text": "I noticed that most people don't realize their chunking and retrieval strategy might be causing their RAG hallucinations.\n\nFixed-size chunking (split every 512 tokens regardless of content) fragments semantic units. Single explanation gets split across two chunks. Tables lose their structure. Headers separate from data. The chunks going into your vector DB are semantically incoherent.\n\nI've been testing semantic boundary detection instead where I use a model to find where topics actually change. Generate embeddings for each sentence, calculate similarity between consecutive ones, split when it sees sharp drops. The results are variable chunks but each represents a complete clear idea.\n\nThis alone gets 2-3 percentage points better recall but the bigger win for me was adding metadata. I pass each chunk through an LLM to extract time periods, doc types, entities, whatever structured info matters and store that alongside the embedding.\n\nThis metadata filters narrow the search space first, then vector similarity runs on that subset. Searching 47 relevant chunks instead of 20,000 random ones.\n\nFor complex documents with inherent structure this seems obviously better than fixed chunking. Anyway thought I should share. :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r3mmxy/semantic_chunking_metadata_filtering_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o55jh9b",
          "author": "Ok_Signature_6030",
          "text": "the metadata filtering part is honestly where i've seen the biggest wins too. went through a similar journey — started with fixed 512 token chunks, moved to semantic splitting, and finally added metadata.\n\n  \none thing worth mentioning though: running every chunk through an LLM for metadata extraction gets expensive fast. we had around 40k chunks and the extraction step alone was costing more than the actual inference. what worked better for us was a hybrid approach — extract what you can from document structure (headers, file paths, timestamps in the text) with regex/rules first, then only use the LLM for ambiguous stuff like topic classification.\n\n  \nalso for the similarity drop detection, we found that just cosine similarity between consecutive sentences was too noisy. adding a sliding window average and looking for drops relative to the local mean rather than an absolute threshold made it way more stable. otherwise you get weird splits in the middle of lists or code blocks where embedding similarity naturally dips.\n\n  \nthe 47 vs 20,000 chunks comparison is real though. metadata pre-filtering basically turns your vector search from \"find needles in a haystack\" into \"find needles in a small pile of needles.\"",
          "score": 8,
          "created_utc": "2026-02-13 12:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57lfqe",
              "author": "welcome-overlords",
              "text": "Thanks. The post and your comment gives me hope that it was the right choice to spend all these weeks not making meaningful progress (i.e. tickets done), that i have arrived at similar thoughts than you.\n\nQuestion:\nHow do you concretely use the metadata filtering? What kind of instructions your RAG agent gets to understand how to use them correctly?",
              "score": 1,
              "created_utc": "2026-02-13 18:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5abi2p",
                  "author": "Ok_Signature_6030",
                  "text": "concretely we do two things: first at ingest time, each chunk gets tagged with structured metadata (doc\\_type, date\\_range, entities, section\\_header). we use a small LLM call per chunk for this - costs pennies and runs async.\n\nthen at query time, the RAG agent has a system prompt that says \"before searching, extract any filters from the user query: dates, document types, named entities. pass these as metadata filters to the retrieval function.\" the agent calls the retrieval tool with both the semantic query AND the metadata filters.\n\nso if someone asks \"what was the Q3 revenue guidance?\" the agent extracts date\\_range=Q3 and doc\\_type=earnings, passes those as filters, and vector search only runs on chunks matching those constraints. way fewer hallucinations because the search space is already relevant.",
                  "score": 4,
                  "created_utc": "2026-02-14 04:03:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57lnud",
              "author": "Particular-Gur-1339",
              "text": "How do you filter based on metadata?",
              "score": 1,
              "created_utc": "2026-02-13 18:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ac9bc",
                  "author": "Ok_Signature_6030",
                  "text": "depends on the vector db but the general pattern is: at ingest time, run each chunk through an LLM to extract structured fields (doc\\_type, date\\_range, entity names, whatever matters for your domain). store those as filterable metadata alongside the embedding.\n\n  \nthen at query time, parse the user's question for those same fields before you even hit the vector search. something like \"what was revenue in Q3 2024\" becomes a metadata filter {date\\_range: 'Q3 2024', doc\\_type: 'financial'} + the semantic query.\n\n  \nif you're using pinecone or weaviate they have native metadata filtering. with pgvector you'd just add WHERE clauses. qdrant has payload filters. the key is the filtering happens BEFORE the ANN search so you're only doing similarity over the pre-filtered subset.",
                  "score": 2,
                  "created_utc": "2026-02-14 04:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55a9op",
          "author": "Independent-Cost-971",
          "text": "Wrote up a more detailed explanation if anyone's interested: [https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/](https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/)\n\nGoes into the different semantic chunking approaches (embedding similarity detection, LLM-driven structural analysis, proposition extraction) and the full metadata enrichment pipeline. Probably more detail than necessary but figured it might help someone else debugging the same issues.",
          "score": 6,
          "created_utc": "2026-02-13 11:24:34",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o55ddn0",
          "author": "One_Milk_7025",
          "text": "Checkout the chunk visualizer for detailed metadata extraction \nChunker.veristamp.in\nChunking is the heart of rag without proper chunking it's just waste of money",
          "score": 2,
          "created_utc": "2026-02-13 11:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55o7y4",
          "author": "_nku",
          "text": "We're running a RAG on a very structured content body where the authors writing in markdown natively and the style is very structured into chapters / subchapters etc logically.   We've leveraged that day one in our RAG, the chunking was paragraph / section based from the beginning although we had to write a custom chunker based on markdown ASTs.   It's keeping the top level page title and context of the chunk and it also keeps the subtitle / intro paragraph of the whole page.     \nIt worked very well although we have to live with larger than typical chunk sizes.  But it's a luxury to be able to work on such as structured content base vs. word documents with all headings being just bold print.  Overall a niche case but if your app has such a high structure content foundation it's a waste to not leverage it. ",
          "score": 2,
          "created_utc": "2026-02-13 13:04:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55mb83",
          "author": "2BucChuck",
          "text": "Tried entity based meta data ? Just curious what smaller models people might be using for cheap enrichment successfully",
          "score": 1,
          "created_utc": "2026-02-13 12:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56dw68",
          "author": "Marzou2",
          "text": "?",
          "score": 1,
          "created_utc": "2026-02-13 15:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56r8cp",
          "author": "tzt1324",
          "text": "Totally agree. But this setup is a lot more expensive. Now imagine running all epstein files through your pipeline. 100 GB or more. Dozens of thousands of documents",
          "score": 1,
          "created_utc": "2026-02-13 16:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56wpyl",
          "author": "Alternative_Nose_874",
          "text": "We saw same thing in our RAG projects: fixed-size chunking break meaning and you get weird retrieval. When we moved to semantic splits plus metadata filters (time/entity/doc type), hallucinations dropped a lot because search is on smaller, more relevant set. Curious what you use to extract metadata, do you do it offline in pipeline or at ingest time?",
          "score": 1,
          "created_utc": "2026-02-13 16:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5723is",
          "author": "Delicious-One-5129",
          "text": "This is a great point. A lot of “RAG hallucinations” are really retrieval failures caused by bad chunk boundaries.\n\nSemantic chunking + metadata pre filtering makes the retriever do less guessing and more narrowing. Searching 40 relevant chunks instead of 20k noisy ones is a huge difference in signal quality.",
          "score": 1,
          "created_utc": "2026-02-13 17:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57lu1u",
              "author": "Particular-Gur-1339",
              "text": "How do you filter based on metadata?",
              "score": 1,
              "created_utc": "2026-02-13 18:51:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57lowu",
          "author": "Particular-Gur-1339",
          "text": "How do you filter based on metadata?",
          "score": 1,
          "created_utc": "2026-02-13 18:50:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a8lpd",
          "author": "eurydice1727",
          "text": "Yupp.",
          "score": 1,
          "created_utc": "2026-02-14 03:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5az0sa",
              "author": "Aggressive_Bed2609",
              "text": "For sure! It's wild how much fixed chunking can mess with the coherence of the data. Your approach with semantic boundary detection sounds promising—definitely a game changer for retrieval accuracy.",
              "score": 1,
              "created_utc": "2026-02-14 07:16:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5apnug",
          "author": "Final_Special_7457",
          "text": "First  \" What did u use for your method of chunking ? A code or a model that handle the chunking boundaries ?\n2nd : it isn't too expensive to pass each chunk through a llm ?",
          "score": 1,
          "created_utc": "2026-02-14 05:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e1col",
          "author": "Jazzcornersmut",
          "text": "Are you looking for a CTO role? \nI need what you do!!",
          "score": 1,
          "created_utc": "2026-02-14 19:33:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5e69n",
      "title": "How I Cut RAG Agent Hallucinations in Production for 10,000+ pdfs.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5e69n/how_i_cut_rag_agent_hallucinations_in_production/",
      "author": "AccidentHefty2595",
      "created_utc": "2026-02-15 13:06:07",
      "score": 54,
      "num_comments": 9,
      "upvote_ratio": 0.95,
      "text": "The Problem:  \nAsked about \"max heating capacity of refrigerator\". Got a mixed answer combining V6, V8, and other refrigerator variants.\n\nThe Fix - 4 Simple Approaches:\n\n1. Force Clarification Before Searching Ambiguous query: Agent stops and asks. \"Refrigerator\" has 3 variants V4, V6 and V8 which one do you mean?\n2. No guessing. No assumptions.\n3. Query Decomposition: Break every request into required pieces (eg. sperate queries for heating and cooling capacity). Do parallel execution for each query and combine the unique results of dense and sparse.\n4. Filtering (most effective): Apply filters once the user confirms their product, filter documents BEFORE retrieval. Think \"search only in Building A, Floor 3\" instead of \"search the entire campus.\" Let the agent apply filters dynamically.\n5. Context Pruning: Long conversations hit token limits fast. Prune old search results. Drop the heavy intermediate retrieval data.\n\nThe Result: The agent now asks \"Which one?\" instead of making assumptions.\n\nCode snippet for the tool calling,\n\n    class SearchInput(BaseModel):\n        query: str = Field(description=\"The specific query to search for in the knowledge base., For complex requests, break this down into specific sub-queries.\")\n        target_directory: Optional[str] = Field(\n            description=(\n                \"Crucial for filtering. Apply precise folder paths based on the user's confirmed product category \"\n                \"(e.g., 'v6_idu/ac', 'v8_idu/wall_mounted', 'vrf_odu/side_discharge'). \"\n                \"Only leave empty if the user asks a broad, cross-category comparison question.\"\n            )\n        )\n    \n    @tool(args_schema=SearchInput, response_format=\"content_and_artifact\")\n    async def knowledge_base_search(query: str, target_directory: Optional[str] = None) -> str:\n        \"\"\"\n        Executes a search within the technical documentation. \n        \n        Usage Guidelines:\n        1. **Precision:** Always apply the `target_directory` derived to exclude irrelevant product lines (e.g., filtering out 'V6' when the user asks for 'V8').\n        2. **Iteration:** Call this tool multiple times if the initial search results are missing required data points.\n        3. **Scope:** Returns raw documentation chunks relevant to the query and path.\n        \"\"\"\n\nI structured the product data in folders and sub-folders that represent a hierarchy. Something like this, for other type of data this can be done based on financial years /  companies / authors / product types.\n\n    VRF\n    ├── V6 IDU\n        ├── AC\n    ├── V8 IDU\n        ├── DC\n        ├── AC\n    └── VRF ODU\n        ├── AC\n        ├── V6R\n        ├── V8\n        │   ├── V8 Master\n        │   ├── V8 Pro\n        └── VC pro\n\nThis structure can be directly given in the system prompt, or if the structure is big then create a new \\`get\\_folder\\_structure\\` tool that uses fuzzy logic to get relevant paths.\n\nNow if we ask, What is the max cooling capacity of V8 IDU?  \nLLM will first ask whether you mean  V8 IDU AC or V8 IDU DC.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5e69n/how_i_cut_rag_agent_hallucinations_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5iiqot",
          "author": "DashboardNight",
          "text": "I agree with query decomposition. Pretty sure this is what Microsoft has been starting to implement in their Search solutions.",
          "score": 2,
          "created_utc": "2026-02-15 14:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j5ax3",
          "author": "TanLine_Knight",
          "text": "How do you enforce query clarification? Like in the fridge example, how would the agent know to ask for specific fridge type?",
          "score": 2,
          "created_utc": "2026-02-15 16:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mhkn3",
              "author": "TenshiS",
              "text": "OPs take only works for a specific type of usecase where you already have a clear mapping of your context or you have enough information on the user himself (perhaps the type of fridge they ordered is known).\n\nAlternatively it requires a traditional retrieval step followed by a clarification step, basically doubling tokens cost but reducing some ambiguity.",
              "score": 1,
              "created_utc": "2026-02-16 03:40:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jcpqp",
              "author": "AccidentHefty2595",
              "text": "Good question, I updated the post, please check it out.",
              "score": 0,
              "created_utc": "2026-02-15 17:15:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n3eem",
          "author": "Comfortable_Onion255",
          "text": "Why not using RLM? I tot that better?",
          "score": 1,
          "created_utc": "2026-02-16 06:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n8e66",
              "author": "AccidentHefty2595",
              "text": "RLM is new for me, i am learning about it",
              "score": 1,
              "created_utc": "2026-02-16 07:10:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5iic1z",
          "author": "HauntingIron5623",
          "text": "how many KB instances are you using? I wonder how sparse the source data looks or if you maybe have only technical documentations",
          "score": 0,
          "created_utc": "2026-02-15 14:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jexzv",
              "author": "AccidentHefty2595",
              "text": "I had only pdfs, technical documents with lots of images and complex tables. So one tool was able to handle retrieval. For excel or database query more tools will be required. ",
              "score": 1,
              "created_utc": "2026-02-15 17:26:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r05za6",
      "title": "Rerankers in RAG: when you need them + the main approaches (no fluff)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r05za6/rerankers_in_rag_when_you_need_them_the_main/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-09 14:42:35",
      "score": 32,
      "num_comments": 15,
      "upvote_ratio": 0.81,
      "text": "If your RAG feels like it’s “almost there” — rerankers are usually the missing piece.\n\nA reranker sits between retrieval and the LLM:\n\n1. Retrieve a larger candidate set (e.g., top-50)\n2. Rerank those candidates by relevance\n3. Send only top-5/top-10 to the model\n\nThe point: stop feeding the LLM garbage context.\n\n# When rerankers are actually worth it\n\nYou likely need a reranker if:\n\n* The correct chunk is often in top-50, but not in top-5\n* Your corpus has near-duplicates (policy versions, templates, “same doc but updated”)\n* Queries are long / multi-intent (“compare A vs B, cite the latest policy, exclude legacy”)\n* Dense retrieval returns “related” chunks but not the *answer-bearing* chunk\n* Increasing k makes answers worse (more context → more confusion)\n\nIf your data is small and clean and top-5 is already precise, rerankers can be extra latency for little gain.\n\n# The main reranker approaches (practical overview)\n\n# 1) Cross-encoder rerankers (most common “quality win”)\n\nScores each *(query, chunk)* pair by reading them together.\n\n* ✅ Best precision (biggest improvement to answer quality)\n* ❌ More compute than embeddings\n* Best pattern: retrieve top-50 → cross-encoder → keep top-5\n\n# 2) Embedding similarity (bi-encoder) (baseline, not really “reranking”)\n\nThis is what most people mean by “vector search”.\n\n* ✅ Fast, scales well\n* ❌ Weaker at fine-grained intent (“which chunk actually answers?”)\n* Best use: candidate generator before a stronger reranker\n\n# 3) Hybrid (BM25 + dense)\n\nCombine lexical matching with embeddings.\n\n* ✅ Great for IDs, error codes, names, exact terms\n* ✅ More robust across weird queries\n* ❌ Requires tuning weights / mixing logic\n\n# 4) LLM-as-reranker (works, but don’t start here)\n\nAsk an LLM to rank chunks (sometimes with custom rules).\n\n* ✅ Very flexible (“prefer newest doc”, “must include citation”, etc.)\n* ❌ Slower + expensive + can be inconsistent unless tightly constrained\n* Use when you need domain rules that models don’t capture well\n\n# 5) Domain-tuned rerankers\n\nFine-tune a reranker on your own relevance data.\n\n* ✅ Big gains in specialized corpora\n* ❌ Needs training data + evaluation discipline\n* Worth it when retrieval quality is core to your product\n\n# A simple setup that usually works\n\n* Retrieve top-50\n* Cross-encoder rerank → pick top-5\n* Apply filters **before reranking** when possible (permissions, time, doc type)\n* Track metrics like: “answer present in top-N” and final answer accuracy\n\nThat’s it. Reranking isn’t a silver bullet — it’s just the cleanest way to convert “the answer is somewhere in there” into “the model actually sees it.”\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r05za6/rerankers_in_rag_when_you_need_them_the_main/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4hc902",
          "author": "Xacius",
          "text": "Thanks ChstGPT",
          "score": 16,
          "created_utc": "2026-02-09 19:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jhos5",
              "author": "stingraycharles",
              "text": "“no fluff” \n\n*proceeds to put in shitloads of prose which could have been summed up in one paragraph*",
              "score": 10,
              "created_utc": "2026-02-10 01:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k0zgd",
                  "author": "MullingMulianto",
                  "text": "do you think openai just sets every model's temperature at 100 even especially after everyone shits on them for slop content\n\nbecause I really am just starting to think so",
                  "score": 1,
                  "created_utc": "2026-02-10 03:47:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4l1fe2",
                  "author": "Donkit_AI",
                  "text": "You're welcome to sum it up in one paragraph. 😁",
                  "score": 0,
                  "created_utc": "2026-02-10 08:44:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hn9pq",
          "author": "Space__Whiskey",
          "text": "This sounds great.  \nWhat reranker are we talking about here, and which one is straightforward to fine-tune/train?",
          "score": 3,
          "created_utc": "2026-02-09 19:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l35op",
              "author": "Donkit_AI",
              "text": "There's a nice link from u/Comfortable-Fan-580 a few comments below to a nice short post about \"what rerankers are\". In real life though it depends heavily on the use case and the constrains. We're using LLM as a reranker in some of our cases. It is expensive and requires more tinkering to make it right but works better with messy datasets and complex rules.\n\nAs for fine-tuning, cross-encoders are the easiest. You can pick BAAI/bge-reranker family or e.g. cross-encoder/ms-marco-MiniLM-L-6-v2 if we're talking open-source.",
              "score": 1,
              "created_utc": "2026-02-10 09:01:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i4ksh",
          "author": "drink_with_me_to_day",
          "text": "> Apply filters before reranking when possible (permissions, time, doc type)\n\nPermissions check should happen way before this step",
          "score": 2,
          "created_utc": "2026-02-09 21:24:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l253j",
              "author": "Donkit_AI",
              "text": "Filters are on retrieval. That's usually the previous step. I wouldn't say it's \"way before\".",
              "score": 1,
              "created_utc": "2026-02-10 08:51:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iduz0",
          "author": "Comfortable-Fan-580",
          "text": "Simply explained here - https://pradyumnachippigiri.dev/til/ai/rag-reranking",
          "score": 2,
          "created_utc": "2026-02-09 22:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1xip",
              "author": "Donkit_AI",
              "text": "Nice explanation of what reranker is at all. Thank you!\n\nMaybe I should have started with something like this rather than jumping to Step2 (what reranker to use) right away.",
              "score": 1,
              "created_utc": "2026-02-10 08:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ggx1b",
          "author": "jrochkind",
          "text": "This is very helpful, thank you!",
          "score": 2,
          "created_utc": "2026-02-09 16:36:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2st4j",
      "title": "Vectorless RAG (Why Document Trees Beat Embeddings for Structured Documents)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2st4j/vectorless_rag_why_document_trees_beat_embeddings/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-12 13:01:07",
      "score": 31,
      "num_comments": 20,
      "upvote_ratio": 0.86,
      "text": "I've been messing around with vectorless RAG lately and honestly it's kind of ridiculous how much we're leaving on the table by not using it properly.\n\nThe basic idea makes sense on paper. Just build document trees instead of chunking everything into embedded fragments, let LLMs navigate structure instead of guessing at similarity. But the way people actually implement this is usually pretty half baked. They'll extract some headers, maybe preserve a table or two, call it \"structured\" and wonder why it's not dramatically better than their old vector setup.\n\nThink about how humans actually navigate documents. We don't just ctrl-f for similar sounding phrases. We navigate structure. We know the details we want live in a specific section. We know footnotes reference specific line items. We follow the table of contents, understand hierarchical relationships, cross reference between sections.\n\nIf you want to build a vectorless system you need to keep all that in mind and go deeper than just preserving headers. Layout analysis to detect visual hierarchy (font size, indentation, positioning), table extraction that preserves row-column relationships and knows which section contains which table, hierarchical metadata that maps the entire document structure, and semantic labeling so the LLM understands what each section actually contains.\"\n\nTested this on a financial document RAG pipeline and the performance difference isn't marginal. Vector approach wastes tokens processing noise and produces low confidence answers that need manual follow up. Structure approach retrieves exactly what's needed and answers with actual citations you can verify.\n\nI think this matters more as documents get complex. The industry converged on vector embeddings because it seemed like the only scalable approach. But production systems are showing us it's not actually working. We keep optimizing embedding models and rerankers instead of questioning whether semantic similarity is even the right primitive for document retrieval.\n\nAnyway feels like one of those things where we all just accepted the vector search without questioning if it actually maps to how structured documents work.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r2st4j/vectorless_rag_why_document_trees_beat_embeddings/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4zo9de",
          "author": "Distinct-Target7503",
          "text": "Just a question... how do you build the tree? do you feed each page of a document to a VLM and rely on its ocrd text? isn't that quite expensive in term of tokens?\n\nI ask because, even without tabs/images, usually headers extraction from PDFs is not really reliable",
          "score": 1,
          "created_utc": "2026-02-12 15:01:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53kiyt",
              "author": "Clipbeam",
              "text": "This. It would either be super expensive or slow and error prone if you try to use local models.",
              "score": 2,
              "created_utc": "2026-02-13 02:57:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50pwzw",
          "author": "bac2qh",
          "text": "I am going to test out pageindex for my openclaw soon because the idea makes sense to me and embeddings feel underwhelming for my use case. I have a feeling that it’s going to consume a lot of tokens though",
          "score": 1,
          "created_utc": "2026-02-12 17:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51691m",
              "author": "Single-Constant9518",
              "text": "Pageindex sounds like an interesting approach! Token consumption can be a concern, but if it helps structure your data more effectively, it might be worth the trade-off. Just keep an eye on how it handles complex documents; that’s where the real benefits could show.",
              "score": 1,
              "created_utc": "2026-02-12 19:15:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o517ina",
                  "author": "bac2qh",
                  "text": "Yeah hopefully it can turn to a POC at work too.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:21:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51z34r",
          "author": "Intrepid-Scale2052",
          "text": "Do you mean the RAG first searches by document metadata, and then searches inside the document. Instead of the contents? \n\nI'm interested, I'm trying to build a searchable archive. What if the header does not say enough? What if you want to search, \"can you find me historical accounts of xxx?\"",
          "score": 1,
          "created_utc": "2026-02-12 21:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o538kcc",
          "author": "licjon",
          "text": "I think it depends on file formats, domain, and purpose. I think a layered approach is the way to go. I prefer to filter with FTS, then do a semantic search on the filtered candidates. ",
          "score": 1,
          "created_utc": "2026-02-13 01:43:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55frgp",
          "author": "DetectiveWeary9674",
          "text": "Isn't this what GraphRAG is for, allowing the LLM to navigate a structured web of information and relationships ?",
          "score": 1,
          "created_utc": "2026-02-13 12:07:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nl3js",
          "author": "BarrenLandslide",
          "text": "Check Out Taxoadapt. Might be something for you.",
          "score": 1,
          "created_utc": "2026-02-16 09:10:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z2stx",
          "author": "Independent-Cost-971",
          "text": "\n\nWrote this up in more detail if anyone's interested : [https://kudra.ai/vectorless-rag-why-document-tree-navigation-outperforms-semantic-search/](https://kudra.ai/vectorless-rag-why-document-tree-navigation-outperforms-semantic-search/)\n\n( shameless plug I know but worth a read )",
          "score": 0,
          "created_utc": "2026-02-12 13:03:09",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4z86co",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -1,
          "created_utc": "2026-02-12 13:35:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zf26g",
              "author": "exaknight21",
              "text": "I use knowledge graphs + hybrid search, i don’t have this issue? My use case requires semantic relationship trees, technical requirements in this document -> section page 10 section 2.10 Manufacturers item a. Benjamin Moore (specified) paint. Or in a spec table page 67 item 13 requires 200 SF of abatemet in room 201 library.\n\nThis is scalable per project and is working. I am baffled why it won’t work for you.",
              "score": 6,
              "created_utc": "2026-02-12 14:13:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4znp49",
                  "author": "Distinct-Target7503",
                  "text": ">knowledge graphs\n\nhow do you build the knowledge graph?\nit is usually really expensive in terms of tokens, or am I doing something wrong?",
                  "score": 1,
                  "created_utc": "2026-02-12 14:58:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2hlzd",
      "title": "RAG for AI memory: why is everyone indexing databases instead of markdown files?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2hlzd/rag_for_ai_memory_why_is_everyone_indexing/",
      "author": "ProfessionalLaugh354",
      "created_utc": "2026-02-12 02:38:26",
      "score": 30,
      "num_comments": 12,
      "upvote_ratio": 0.89,
      "text": "I've been building memory systems for agents and noticed something weird. Most memory solutions follow this pattern:\n\n**Standard RAG approach (Mem0, Zep, etc.):**\n\n* Store memories in database (PostgreSQL, MongoDB, whatever)\n* Query through APIs\n* To inspect: write code to query DB\n* To edit: call update endpoints\n* To migrate: export → transform → reimport\n\n**Alternative approach (inspired by OpenClaw):**\n\n* Store memories in markdown files\n* Embed and index in vector store (same as above)\n* Query through APIs (same as above)\n* To inspect: `cat memory/MEMORY.md`\n* To edit: vim/VSCode the file, auto-reindexes\n* To migrate: `cp -r memory/ new-system/`\n\nThe retrieval layer is identical - both use vector search + reranking. The only difference is the source of truth.\n\n**Why markdown seems better for memory:**\n\n**Debuggability** \\- When retrieval returns wrong context, you can grep through source files instead of writing DB queries. `rg \"Redis config\" memory/` beats SQL any day.\n\n**Version control** \\- `git log memory/MEMORY.md` shows you exactly when bad info entered the system. Database audit logs? Painful.\n\n**Chunk inspection** \\- See the actual document structure. Databases flatten everything into rows. Markdown preserves semantic boundaries (headings, paragraphs).\n\n**Hybrid search** \\- BM25 keyword search works naturally on markdown. On JSON in databases? Need full-text indexes and special config.\n\n**Cold start** \\- New developer? `git clone`, read the markdown, understand what the AI knows. Database? Need credentials, connection, schema knowledge.\n\n**The RAG perspective:**\n\nFrom a pure retrieval standpoint, markdown has advantages:\n\n* Semantic chunking is easier (split by headings/paragraphs)\n* Context preservation (you can read surrounding text naturally)\n* Deduplication is straightforward (content hash)\n* A/B testing embeddings is trivial (reindex from source)\n\n  \n==========================\n\n**So,,,, What I built:**\n\nGot so convinced by this I built \\`memsearch\\` , a memory package ready for agent memory usage.\n\nhttps://github.com/zilliztech/memsearch\n\nbasically proper RAG over markdown files with:\n\n* Hybrid search (vector + BM25, weighted fusion)\n* File watching + auto-indexing\n* Chunk deduplication (saves 20-30% on embedding costs)\n* Framework agnostic\n\n**My question to the community:**\n\nIs there a technical reason database-first is better that I'm missing? Or is it just convention?\n\nThe only argument I hear is \"scale\" but most agent memory is < 100MB even after months. That's nothing for modern RAG systems.\n\nWould love to hear from people who've built production RAG systems. What breaks when you use files instead of databases?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r2hlzd/rag_for_ai_memory_why_is_everyone_indexing/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4x2y1f",
          "author": "Ok_Signature_6030",
          "text": "you're right that files work great for the debugging and versioning use cases. we actually use markdown-based memory for a few internal tools and git log on the memory files has saved us more than once.\n\nthe place it breaks down for us is concurrent access. when you have multiple agents writing to the same memory store at the same time, file locking becomes a real headache. databases handle that natively. the other thing is structured queries... like if you need \"show me all memories tagged with customer X from the last 30 days\" that's trivial in sql but painful to grep through markdown.\n\nfor single-agent setups under 100mb though, yeah honestly files are probably underrated as a source of truth.",
          "score": 12,
          "created_utc": "2026-02-12 03:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xydfk",
              "author": "ProfessionalLaugh354",
              "text": "That’s exactly why we use a vector database to asynchronously sync content from Markdown—only reindexing changed chunks. We then use ANN or hybrid search for queries, which solves both concurrency and structured search neatly.",
              "score": 1,
              "created_utc": "2026-02-12 07:06:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o53o9ni",
                  "author": "Ok_Signature_6030",
                  "text": "yeah that hybrid approach makes sense... markdown as source of truth, vector db as the query layer. the changed-chunk reindexing is key, full reindex every time would kill you on cost. how are you detecting which chunks actually changed? content hashing or something fancier?",
                  "score": 1,
                  "created_utc": "2026-02-13 03:21:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xx4et",
          "author": "-Cubie-",
          "text": "Very cool! Is there also CrossEncoder reranking possible? I find it helps retrieval nicely, e.g.ms-marco-MiniLM-L6-v2",
          "score": 2,
          "created_utc": "2026-02-12 06:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ys4q5",
          "author": "RobertLigthart",
          "text": "100% agree on this. I use markdown files for all my project context and agent memory and its just so much simpler to debug and maintain\n\n  \nthe concurrency argument is valid for multi-agent setups but honestly most people arent running multiple agents writing to the same memory simultaneously. for single agent workflows (which is like 90% of use cases) files work perfectly\n\n  \nbiggest win for me is the git versioning. being able to git diff your AI's memory and see exactly what changed is incredibly useful for debugging weird behavior. try doing that with a postgres table",
          "score": 2,
          "created_utc": "2026-02-12 11:47:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwtis",
          "author": "owlpellet",
          "text": "Initial thoughts:\n\n1. I'm curious how often you're updating embedding. Embedding is expensive at scale.\n2. putting user data into git is a non-starter for production systems in my org. If agent memory has PII in it, you're putting that into a database, with access controls.\n\nMy guess is that once you start covering edge cases, you are now a database developer. Postgres is pretty good. \n\nAgree that optimizing for inspectability is a good idea. Dev environment vs test/stage/prod environment maybe.",
          "score": 2,
          "created_utc": "2026-02-12 15:43:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56lped",
              "author": "Organic-Pitch-2873",
              "text": "when the file change, we use a watcher to detect the file change, and reembed them. \nPII is a good advise",
              "score": 1,
              "created_utc": "2026-02-13 15:58:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52b6eo",
          "author": "jcheroske",
          "text": "What do you think of this: [https://github.com/basicmachines-co/basic-memory](https://github.com/basicmachines-co/basic-memory)",
          "score": 2,
          "created_utc": "2026-02-12 22:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y521y",
          "author": "TeeRKee",
          "text": "Did you consider the hardware req to locally embed and manage the whole vector db pipeline?",
          "score": 1,
          "created_utc": "2026-02-12 08:09:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56mk87",
              "author": "Organic-Pitch-2873",
              "text": "local embedding model +milvus lite",
              "score": 1,
              "created_utc": "2026-02-13 16:02:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o557twr",
          "author": "phrobot",
          "text": "In a SaaS system, how do users edit the markdown files? That’s why everyone uses DB and API.\nBut for personal, local, developer use, yes it is much simpler to use files.",
          "score": 1,
          "created_utc": "2026-02-13 11:03:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3oiyz",
      "title": "Chunking for RAG: the boring part that decides your accuracy (practical guide)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r3oiyz/chunking_for_rag_the_boring_part_that_decides/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-13 13:00:36",
      "score": 28,
      "num_comments": 3,
      "upvote_ratio": 0.94,
      "text": "Looks like it's a chunking day here. :) Let me add my 5 cents.\n\nMost “RAG accuracy” problems show up later as people tweak rerankers, prompts, models, etc.\n\nBut a huge % of failures start earlier: chunking.\n\nIf the right info can’t be retrieved cleanly, the model can’t “think” its way out. It’ll either hallucinate, or answer confidently from partial context.\n\nLet's start with the definition: A chunk is the smallest unit of meaning (!) that can answer a real question without needing its neighbors.\n\nToo big → you retrieve the answer plus extra junk → model gets distracted (precision drops). Too small → you retrieve fragments → missing context (recall drops). Wrong boundaries → meaning gets shredded (definitions, steps, tables…).\n\n# 3 common symptoms your chunking is broken\n\n1. Chunks too big: top-k retrieval contains the answer but also unrelated sections → the LLM free-associates.\n2. Chunks too small: the answer exists but is split across boundaries → retrieval misses it.\n3. Bad split points: tables, lists, procedures, “Definitions” sections → you split exactly where coherence matters.\n\n# There are actually 3 chunking modes that cover most real-world docs (NOT ALL of them, still)\n\n# Mode 1) Structure-first (best default)\n\nUse for: technical manuals, policies, specs, handbooks, wikis (anything with headings).\n\nHow to do it:\n\n* Chunk by heading hierarchy (H2/H3 sections)\n* Keep paragraphs intact\n* Keep lists/tables/code blocks intact\n* Store section\\_path metadata (e.g., Security > Access Control > MFA)\n\nWhy it works: your doc already has a map. Don’t throw it away.\n\n# Mode 2) Semantic windows (for messy conversational text)\n\nUse for: transcripts, email threads, Slack dumps, scraped webpages (weak structure, topic drift).\n\nHow to do it:\n\n* Build topic-coherent “windows” (don’t hard-split blindly)\n* Use adaptive overlap only when meaning crosses boundaries\n   * Q → A turns\n   * follow-ups (“what about…”, “as mentioned earlier…”)\n   * references to earlier context\n\nWhy it works: conversation doesn’t respect token boundaries.\n\n# Mode 3) Atomic facts + parent fallback (support/FAQ style)\n\nUse for: FAQs, troubleshooting, runbooks, support KBs (answers are small + repetitive).\n\nHow to do it:\n\n* Index atomic chunks (1–3 paragraphs or one step-group)\n* Store pointer to parent section\n* Retrieval policy:\n   * fetch atom first\n   * if answer looks incomplete / low confidence → fetch parent\n\nWhy it works: high precision by default, but you can pull context when needed.\n\n# Most useful tweaks\n\n# Overlap: use it like salt, not soup\n\nDon't do “20% overlap” everywhere.\n\nOverlap is for dependency, not tradition:\n\n* 0 overlap: self-contained sections\n* small overlap: narrative text\n* bigger overlap: procedures + conversational threads + “as mentioned above” content\n\n# Tables are special (many mess this up)\n\nDo not split tables mid-row or mid-header.\n\n* Store the whole table as one chunk + create a table summary chunk\n* Or chunk by row, but repeat headers + key columns in every row chunk\n\n# Metadata: the cheap accuracy boost people sometimes forget\n\nStore at least:\n\n* `doc_id`\n* `section_path`\n* `chunk_type` (policy / procedure / faq / table / code)\n* `version` / `effective_date` (if docs change)\n* `audience` (legal / support / eng)\n\nThis enables filtering before vector search and reduces “wrong-but-related” retrieval.\n\n# How to test chunking fast (with no fancy eval framework)\n\nTake 30 real user questions (not synthetic).\n\nFor each:\n\n* retrieve top-5\n* score: Does any chunk contain the answer verbatim or with minimal inference?\n\nInterpretation:\n\n* Often “no” → boundaries wrong / chunks too small / missing metadata filters\n* Answer exists but not ranked → ranking/reranker/metadata issue\n\nBonus gut-check: Take 10 questions and open the top retrieved chunk. If you keep thinking “the answer is almost here but needs the previous paragraph”… your chunk boundaries are wrong.\n\n# Practical starting defaults (if you just want numbers)\n\nThese aren’t laws, just decent baselines:\n\n* Manuals/policies/specs: structure-first, \\~300–800 tokens\n* Procedures: chunk by step groups, keep prerequisites + warnings with steps\n* FAQs/support: atomic \\~150–400 tokens + parent fallback\n* Transcripts: semantic windows \\~200–500 tokens + adaptive overlap\n\n# What we actually do for large-scale production use cases\n\nWe test extensively and automate the whole process\n\n* Chunking is automated per document type and ALWAYS considers document structure (no mid-word/sentence/table breaks)\n* For each document type there's more than one chunking approach\n* Evals are automated (created automatically and tested automatically on every pipeline change)\n* Extensive testing is the core. For each project different chunking strategies are tested and compared versus each other (here automated evals add velocity) As a result of these automations we receive good accuracy with little \"manual RAG drag\" and in a matter of days.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r3oiyz/chunking_for_rag_the_boring_part_that_decides/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5aq9vn",
          "author": "Otherwise-Platypus38",
          "text": "Chunking is the main driving factor in most RAG scenarios. A good chunking strategy with well formed metadata can really increase your accuracy. Having an in-house pipeline makes things easier, as you know your data best, as well as you know your domain best, so you can anticipate end user interactions better.\n\nI have implemented a chunking strategy based (completely in-house), where you convert unstructured PDF data to markdown, and use OCR on figures. Then, creating chunks for sections and sub-sections, and creating separate chunks for figures and tables with cross-references in the main chunk. This allows to preserve to a large extent the hierarchy. \n\nSpending some time in the document preparation and processing stage has been quite fruitful. Although, nowadays most providers have the option to use in-built retrieval tools, it’s quite satisfying to see your own implementation working in a real world product.",
          "score": 3,
          "created_utc": "2026-02-14 05:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55vw61",
          "author": "Ok_Revenue9041",
          "text": "Great summary on why chunking is so critical for RAG results. Testing with real user questions is overlooked but makes a huge difference in catching subtle retrieval issues early. If you want to make sure your brand’s content shows up more often and accurately in AI powered searches, MentionDesk can help optimize how your chunks get picked up across these platforms.",
          "score": 1,
          "created_utc": "2026-02-13 13:48:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r36f72",
      "title": "Increasing your chunk size solves lots of problems - the default 1024 bit chunk size is too small",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r36f72/increasing_your_chunk_size_solves_lots_of/",
      "author": "Free-Ferret7135",
      "created_utc": "2026-02-12 21:42:49",
      "score": 22,
      "num_comments": 30,
      "upvote_ratio": 0.87,
      "text": "Chunking documents into small 1024 bit looks very outdated to me. But even for something enterprise like Google Vertex AI Search this is still the case.\n\nLLMs are so much better at processing large context windows than they were yesterday, last month, last year.\n\nWith Gemini, Llama, Opus etc. being able to easily read and understand 300-400 pages at max, you can generously feed it 30-50 pages and still get a good result without \"lost in the middle\" IMHO.\n\n  \nSimply increasing chunk size, ideally at positions that make sementically sense (e.g. full chapters from a particular topic) and then feeding the top k chunks into one of the above LLM ... bing, you have a 95-100% accurate RAG.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r36f72/increasing_your_chunk_size_solves_lots_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o52267o",
          "author": "flonnil",
          "text": "you are wrong, but given you are only here to inform us that you are right, i think i'll let you find out yourself.",
          "score": 30,
          "created_utc": "2026-02-12 21:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o522ek8",
              "author": "mrnoirblack",
              "text": "I'm interested in why is he wrong as I haven't had experience with huge context models",
              "score": 2,
              "created_utc": "2026-02-12 21:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o524mac",
                  "author": "flonnil",
                  "text": "\\- input-token costs off the charts, particularly with said big context models  \n\\- amount of consulted sources decreases  \n\\- granular information gets lost in the haystack  \n\\- llm context gets polluted to absolute hell  \n\\- [what this guy pointed out niceley](https://www.reddit.com/r/Rag/comments/1r36f72/comment/o55et6w/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): search accuracy gets diluted   \n\\- multi-step or multi-retrieval agents will despite what OP says absolutely hit context ceiling  \n\\- while providers claim context windows of N, performance usually massively drops at arround max 0.4 N. Big context window doesnt mean its best to fill it to the brim, in the contrary.  \nGenerally: for contextual-understandig-type things, rather big chunks are fine, for exact granular data retrieval smaller are better.",
                  "score": 14,
                  "created_utc": "2026-02-12 21:59:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52gg3n",
          "author": "Harotsa",
          "text": "The bottleneck for chunk size was never the context window of the reader, but rather the context window of the embedder as well as the “effective” context size of the embedder.\n\nEven SOTA embedding models today don’t have much more than a few thousand tokens of context max, which is a hard limit on your chunk size assuming you are going to use vector search.\n\nAdditionally there is a very significant regression to the mean issue with embeddings, where if you put too many sentences together you start losing the signal of that piece of text.\n\nI think the larger effective context size of the readers (decoder LLMs) means more context can be retrieved, but it shouldn’t have too much impact on the size of the chunks you are embedding.",
          "score": 8,
          "created_utc": "2026-02-12 23:00:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52nppf",
          "author": "RobertLigthart",
          "text": "the point about embedder context being the actual bottleneck is the real answer here. you can feed 50 pages to gemini all day but if your embedding model only meaningfully represents the first 512 tokens of each chunk then your retrieval quality tanks regardless of chunk size\n\n  \nin practice what worked for me is chunking larger (2-4k tokens) but with overlap AND storing a summary embedding alongside the full chunk. retrieve on the summary, pass the full chunk to the LLM. best of both worlds\n\n  \nthe cost argument matters too though. if youre doing thousands of queries a day those fat chunks in the context window add up fast",
          "score": 6,
          "created_utc": "2026-02-12 23:41:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55et6w",
          "author": "Donkit_AI",
          "text": "I agree to comments from u/flonnil, but want to add to it.\n\nIncreasing chunk size feels like a win until you realize you’re just paying for 'Context Dilution.'\n\nMathematically, when you bloat chunks, your cosine similarity starts measuring the 'average' of a 6,000-token soup rather than the specific needle you’re looking for. You end up with higher latency, higher token costs, and a model that gets 'Lost in the Middle'. The latter won't happen in case the answer fits in 1 or 2 chunks, but with pushing more chunks into the answer and agentic workflows where context is bloated by all the instructions and tools, it will take a significant toll.\n\nBesides, by nature it's an endless optimization process. It must be based on evals to see, if it's a gain or a loss in your specific case. We automate experimentation and just find the mathematical 'sweet spot' for each specific use case.",
          "score": 3,
          "created_utc": "2026-02-13 12:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58rngs",
              "author": "Free-Ferret7135",
              "text": "\"a model that gets 'Lost in the Middle\"\n\nGpt 5.2 does not do this anymore even at large contexts, the LLMs got so much better to the point that \"Lost in the middle\" is neglible. \n\nI can send you dozens of examples where it finds the needle in the hay even with bloated prompt. But give me one prompt where gpt 5.2 exhibits \"Lost in the middle\" and I will take back what I said. ;)\n\n\"higher latency, higher token costs\"\n\nTrue, but these are ressource limitations rather than technical limitations.",
              "score": 1,
              "created_utc": "2026-02-13 22:18:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53gu0m",
          "author": "InitialJelly7380",
          "text": "good to know...thanks",
          "score": 2,
          "created_utc": "2026-02-13 02:34:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53t180",
          "author": "Ok-Attention2882",
          "text": "Increasing chunk size is a COPE.",
          "score": 2,
          "created_utc": "2026-02-13 03:52:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o523ctr",
          "author": "butwhol",
          "text": "What about Context bloat /context rot issues for ai agents using your rag?",
          "score": 1,
          "created_utc": "2026-02-12 21:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52six3",
              "author": "Free-Ferret7135",
              "text": "Valid concern, will need some sort of compaction to tackle this.",
              "score": 1,
              "created_utc": "2026-02-13 00:08:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52jf67",
          "author": "88chilly",
          "text": "1k chunks feel like a relic at this point. With the context windows we have now it makes way more sense to chunk by meaningful sections instead of arbitrarily small slices. Bigger semantically clean chunks plus solid retrieval gets you way closer to reliable RAG. Designing around old limits just holds things back.",
          "score": 1,
          "created_utc": "2026-02-12 23:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o531oti",
              "author": "Kerbourgnec",
              "text": "Why would you use LLM context size to argue about embedding chunk size?\n\nEmbeddings and their chunk size have one goal only: when searching for X, I get the most relevant results. Optimize your embedder and chunk size for that only.\n\nIf you want to give more to your LLM, nothing prevents you to give ten pages around the chunk you retrieve. There is no reason except for laziness to only return the exact chunk related to the embedding it found.",
              "score": 1,
              "created_utc": "2026-02-13 01:01:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52lvd7",
          "author": "Ryanmonroe82",
          "text": "Have you tried using only top_p and setting top_k = 0.0, min_p = 1? \nI too have found larger chunks work much better.  Using models that use a hybrid mamba architecture (nemotron 9b/12b V2) handle the larger chunks very well.",
          "score": 1,
          "created_utc": "2026-02-12 23:30:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52soi1",
              "author": "Free-Ferret7135",
              "text": "Yes, but never used nemotron 9b/12b V2.  How do you compare its performance against other llms?",
              "score": 1,
              "created_utc": "2026-02-13 00:09:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5308h3",
                  "author": "Ryanmonroe82",
                  "text": "Both punch above their weight considerably. Using BF16/F16 is key though.  Check into the Mamba hybrids and see the advantages over a the traditional transformer and MoE architecture. \nAnother one I really like is RNJ-1-Instruct, on hugging face it’s available in F32 and it’s incredibly good.",
                  "score": 1,
                  "created_utc": "2026-02-13 00:53:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52nt8j",
          "author": "Weary_Long3409",
          "text": "Depends on documents nature. I use 6k chunk size for my >8k legal analysis docs. Doc size range from 1k to 8k tokens, mostly <6k tokens.\n\nThe golden rule is, the more data you have, the larger chunk size should be. Small chunks in a large dataset prone to break cosine similarities.",
          "score": 1,
          "created_utc": "2026-02-12 23:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54jqa8",
              "author": "Late_Special_6705",
              "text": "I think you're confusing something. Where did you find the 6k? Model name",
              "score": 1,
              "created_utc": "2026-02-13 07:19:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o531h6n",
          "author": "Kerbourgnec",
          "text": "Why would you use LLM context size to argue about embedding chunk size?\n\nEmbeddings and their chunk size have one goal only: when searching for X, I get the most relevant results. Optimize your embedder and chunk size for that only.\n\nIf you want to give more to your LLM, nothing prevents you to give ten pages around the chunk you retrieve. There is no reason except for laziness to only return the exact chunk related to the embedding it found.",
          "score": 1,
          "created_utc": "2026-02-13 01:00:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o535vmo",
              "author": "Free-Ferret7135",
              "text": "\"only return the exact chunk\" only works for atomic retrievals where you are asking something very specific.   \nBut many datasets have more context around it, relationships between chunks etc.  \nNow you might say \"well, then you should build your knowledge graph around that instead of RAG\" but I find it much easier to ingest data into a RAG then building complicated graphs and I see larger chunk sizes as key in making them suitable for contextual understanding.",
              "score": 1,
              "created_utc": "2026-02-13 01:27:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53c45b",
          "author": "yangastas_paradise",
          "text": "I don't understand how chunk size is related to context window ? Chunk size is more about capturing the right semantic meaning so that your RAG can fetch the relevant info at runtime. Too small then the semantic meaning is too narrow, and too large the meaning is diluted. \n\nI actually use a multi chunk strategy and that works well .",
          "score": 1,
          "created_utc": "2026-02-13 02:05:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53m8wb",
              "author": "Space__Whiskey",
              "text": "I recently discovered this, and it improved things significantly.\n\nThe analogy is, to put out more fishing lines to increase the chance of catching your fish.",
              "score": 1,
              "created_utc": "2026-02-13 03:08:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57146u",
          "author": "Otherwise_Corgi_5940",
          "text": "Hi guys I am currently done a job as a junior AI/ML engineer now I am doing a project on the rag I have finished the document extraction with the mistral ocr now I want to move on chucking part can you guys help with some advice how to do it with Enterprise level?",
          "score": 1,
          "created_utc": "2026-02-13 17:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58bchh",
          "author": "blue-or-brown-keys",
          "text": "RAG will bring multiple chunks and context windows can be large. You problem is not with chunk size, but that when you break into chunks you are losing context, try padding with document context/summary",
          "score": 1,
          "created_utc": "2026-02-13 20:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59eblt",
          "author": "Legitimate_Sherbet_7",
          "text": "There are a lot of different opinions here on chunk size and accuracy. Based on my own development there are mixed results from just changing chunk sizes and overlap. There is the quality of the question being asked. The \",role,\" or personality prompt and how it's tailored to the application and the model being used makes a huge difference. It's not a one size fits all its maybe for some applications getting to a configuration where one size fits most. But like everyone else I'm still learning.",
          "score": 1,
          "created_utc": "2026-02-14 00:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a8f7p",
          "author": "eurydice1727",
          "text": "Parent - Child Chunking solves a lot of this. I use parent with payload only, a top ranked child chunk after dedupe and reranking then calls for the parent chunk which provides further context. Parents are up to 3500 tokens and children I max at 900",
          "score": 1,
          "created_utc": "2026-02-14 03:41:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r16mrx",
      "title": "A Practical RAG Roadmap to Stop LLM Apps from Failing in Production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r16mrx/a_practical_rag_roadmap_to_stop_llm_apps_from/",
      "author": "devasheesh_07",
      "created_utc": "2026-02-10 17:11:32",
      "score": 20,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "After building and breaking a few LLM-based systems, I’ve realized something uncomfortable:\n\nMost failures don’t come from “bad models.”  \nThey come from **bad retrieval**.\n\nRAG (Retrieval-Augmented Generation) isn’t a framework or a prompt trick — it’s a systems problem. Here’s the **short roadmap** that finally made it click for me:\n\n**1. Start with embeddings**  \nIf you don’t understand how text becomes vectors and how similarity search works, nothing else matters.\n\n**2. Use a vector database properly**  \nStore embeddings, not raw text. Learn indexing, filtering, and why top-k retrieval isn’t magic.\n\n**3. Chunking is everything**  \nMost vague or wrong answers come from poor chunking, not the LLM. This step is underrated.\n\n**4. Retrieval > model choice**  \nA smaller model with clean, relevant context beats a larger model with noisy retrieval.\n\n**5. Treat RAG like a system, not a demo**  \nLogging, evaluation, failure cases, and feedback loops matter more than fancy prompts.\n\nThis mindset shift helped me understand why many “LLM apps” look great in demos but fall apart in real use.\n\nI wrote a **fully detailed article** breaking this down step by step   \n[https://www.loghunts.com/rag-in-ai-ml-practical-learning-roadmap](https://www.loghunts.com/rag-in-ai-ml-practical-learning-roadmap)  \nIf anything here is wrong or oversimplified, I’m very open to corrections — happy to update the article.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r16mrx/a_practical_rag_roadmap_to_stop_llm_apps_from/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4nj0o0",
          "author": "HandSuspicious1579",
          "text": "This is something i would not prefer",
          "score": 1,
          "created_utc": "2026-02-10 17:56:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4olojx",
          "author": "Late_Special_6705",
          "text": "404",
          "score": 1,
          "created_utc": "2026-02-10 20:55:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r235rh",
      "title": "What do you use for scraping data from URLs?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r235rh/what_do_you_use_for_scraping_data_from_urls/",
      "author": "Physical_Badger1281",
      "created_utc": "2026-02-11 17:11:42",
      "score": 19,
      "num_comments": 15,
      "upvote_ratio": 0.93,
      "text": "Hey all,\n\nQuick question — what’s your go-to setup for scraping data from websites?\n\nI’ve used Python (requests + BeautifulSoup) and Puppeteer, but I’m seeing more people recommend Playwright, Scrapy, etc.\n\nWhat are you using in 2026 and why?\nDo you bother with proxies / rotation, or keep it simple?\n\nI've developed [Fastrag](https://www.fastrag.live) you can check the demo.\n\nCurious what’s working best for you.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r235rh/what_do_you_use_for_scraping_data_from_urls/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4txdnd",
          "author": "RoyalTitan333",
          "text": "I prefer Firecrawl(self hosted). \n\nHere’s what I like about it. You point it at a site and it handles crawling, rendering, structured extraction, and even markdown output without forcing you to stitch together five different tools. For projects where the goal is usable data fast, that matters more than having ultimate low level control.\n\nI still reach for Playwright when a site is heavily interactive or guarded, and Scrapy is hard to beat for very large, rule driven crawls. But for a huge middle ground, especially content heavy sites.",
          "score": 9,
          "created_utc": "2026-02-11 17:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uc54l",
              "author": "laurentbourrelly",
              "text": "I am old school, but gotta admit Firecrawl is awesome.",
              "score": 1,
              "created_utc": "2026-02-11 18:25:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4txcv5",
          "author": "Cod3Conjurer",
          "text": "A few days before this, I built a similar project using BeautifulSoup4 + Playwright + RAG for dynamic website crawling and retrieval.\n\nRepo: https://github.com/AnkitNayak-eth/CrawlAI-RAG",
          "score": 4,
          "created_utc": "2026-02-11 17:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uptd5",
          "author": "johnrock001",
          "text": "playwright or selenium",
          "score": 2,
          "created_utc": "2026-02-11 19:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e1vi7",
          "author": "AdZestyclose9517",
          "text": "Cool project :)\n\nregarding your questions...depends on what you're feeding the RAG pipeline. if it's mostly text content from known sites, requests +beautifulsoup still works fine.   \nOr sometimes even plain playwright.   \nthe problem starts when sites have cloudflare or heavy JS rendering. for that i use bright data's web unlocker, it handles anti-bot automatically and returns the full rendered page. for SERP results i use their SERP API since it gives you structured JSON directly, skips the whole parsing step which keeps your chunking pipeline cleaner.",
          "score": 2,
          "created_utc": "2026-02-14 19:35:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uxxot",
          "author": "bigahuna",
          "text": "Scrapy https://www.scrapy.org/ with faker.",
          "score": 1,
          "created_utc": "2026-02-11 20:08:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v3hex",
          "author": "One_Milk_7025",
          "text": "Dude for bulk and stable scalable scraping use @crawl4ai they are the best.. it's open source and blazing fast for various task but the bulk extraction is their niche",
          "score": 1,
          "created_utc": "2026-02-11 20:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xw31a",
          "author": "SharpRule4025",
          "text": "Depends on what you're feeding the data into. If it's going into a RAG pipeline, the scraping part is only half the problem. The real pain is getting clean, structured content out of whatever HTML you pulled.\n\nI was using Playwright plus a bunch of custom extraction logic for a while. Worked fine until I had to deal with sites behind Cloudflare or DataDome, then it turned into a proxy rotation mess on top of everything else.\n\nLately I've been sending URLs through a scraping API that handles the rendering and anti-bot stuff, then returns structured JSON with headings, paragraphs, links separated out. Saves me from writing extraction code per site and the chunks map way better to embeddings than raw markdown does.\n\nFor simple static pages though, requests plus BeautifulSoup is still hard to beat. No reason to overcomplicate it if the site cooperates.",
          "score": 1,
          "created_utc": "2026-02-12 06:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xwfgg",
          "author": "Physical_Badger1281",
          "text": "I worked on a project called [Fastrag](https://www.fastrag.live), where I used Puppeteer for data scraping, and it’s working well.",
          "score": 1,
          "created_utc": "2026-02-12 06:48:50",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4ygss5",
          "author": "CapMonster1",
          "text": "For me it really depends on the target. If it’s a simple/static site, I still go with Python & requests & BeautifulSoup, super fast and low overhead. For anything JS-heavy, Playwright has been my go-to lately, feels more stable than Puppeteer and handles modern frontends better.\n\nScrapy is great when you need structure and scale, but for small side projects it can feel like overkill. Proxies depend on how aggressive the site ismsometimes you don’t need them, but once you hit rate limits or bot protection, rotation becomes mandatory.",
          "score": 1,
          "created_utc": "2026-02-12 10:05:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3kk3b",
      "title": "Love-hate relationship with Docling, or am I missing something?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r3kk3b/lovehate_relationship_with_docling_or_am_i/",
      "author": "SkyStrong7441",
      "created_utc": "2026-02-13 09:13:23",
      "score": 18,
      "num_comments": 13,
      "upvote_ratio": 0.95,
      "text": "Docling is a great parser for pdfs (I've only tried pdfs)! With their DocumentConverter I convert my pdf to a DoclingDoc, from there I easily export it as a dict of the following format:  \n  \n{   \n schema\\_name:  \n version:   \n name:   \n origin:  \n furniture:  \n body:  // This is the order of blocks they appear in the pdf  \n groups: // This is where list items are grouped together  \n texts:  // list items and pure text blocks are found here  \n pictures:  // Pictures if I have to guess.   \n tables: // This is where all tables are found   \n key\\_value\\_items:   \n form\\_items:  \n pages:  \n}\n\nI can use texts to get any text block from the pdf.    \nI can use groups and texts together to recreate any list from the pdf.   \nWithin tables I have all the cells to recreate any table.   \nAnd with body I can piece it all together.   \n  \nThis is given that nothing is lost in the docling conversion, and for most pdfs I try this on there always is. There's always some block either missing or not part of the correct group, for example: \n\n* list items interpreted as being text block, thus not being part of a list group\n* header of a table not being interpreted as the header of that table, but as a header of the section the table lies within. So basically the table is missing a piece of information. \n\nWith my projects demand for accuracy Docling is not enough, but it's so so close! \n\nPlease tell me if there's some way to configure Docling, possibly making it convert tables differently? Or maybe there is some functionality of Docling I'am not utilizing?  \nOr maybe this is the exact problem with pdfs having different layout, and for 100% accuracy I need another approach than Docling?\n\nThank you for taking your time!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r3kk3b/lovehate_relationship_with_docling_or_am_i/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o54wlmw",
          "author": "fabkosta",
          "text": "You are not fighting Docling, you are fighting PDF. If you would try the same with Azure Document Intelligence, you'd end up with a similar experience.\n\nWhat can help (but does not always) is post-processing. You use all sorts of tricks to re-establish relationships that were missed by the OCRing tool. For example, you could use an LLM and ask it to put things together in a coherent way.",
          "score": 3,
          "created_utc": "2026-02-13 09:19:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o551rs3",
              "author": "Intelligent-Form6624",
              "text": "Azure Content Understanding is more recent than Document Intelligence. It is LLM-enhanced. I find it’s slightly better than Document Intelligence but still not perfect. I think they ought to improve on it.\n\nYou’re right, post-processing can help _a lot_. I’m in the middle of using Open AI Codex to build a pipeline to extract financial tables from PDFs.\n\nThe pipeline feeds PDF to Azure Content Understanding (prebuilt-layout), receives .md+json result, and performs significant post-processing using Gemini-2.5-Flash via VertexAI API. After a lot of work, I am beginning to see perfect results on my test documents. We’ll see how it performs on the rest of the corpus.\n\nAside from these ‘production-ready’ solution (Azure + VertexAI), I suggest seriously looking at specialised OCR VLMs and their corresponding pipeline software. Specifically; GLM-OCR, MinerU and PaddleOCR-VL.\n\nDepending on the sensitivity of your data, you may need to arrange an API endpoint for these models yourself. For example; RunPod, Azure Container App, Google Cloud Run etc",
              "score": 2,
              "created_utc": "2026-02-13 10:08:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o554l6i",
                  "author": "fabkosta",
                  "text": "These are really good pointers!\n\nAre your documents having a common structure, or is each document very distinct from the last one?\n\nMy experience is that if you have common docs, there are quite a few tricks you can apply. But if each document is very different from the last one, then that's going to be really hard.",
                  "score": 1,
                  "created_utc": "2026-02-13 10:35:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5d1es7",
                  "author": "Alternative-Poet973",
                  "text": "You could use an OCR API for that financial table pipeline. I use Qoest's OCR API for similar document processing, and it handles the structured data extraction from PDFs pretty accurately so you can skip a lot of that post processing work",
                  "score": 1,
                  "created_utc": "2026-02-14 16:32:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o554ktp",
          "author": "fustercluck6000",
          "text": "Right there with you, I think a major issue is the lack of documentation, like there’s this constant feeling that it’s more than capable of achieving accuracy I need if only I could figure out how tune things just right. \n\nI highly recommend spaCy-layout—it basically adds spaCy magic on top of Docling and I’ve found it makes a noticeable difference in terms of indexing quality",
          "score": 2,
          "created_utc": "2026-02-13 10:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56a89c",
          "author": "vlg34",
          "text": "You can also try an LLM document parser such as Airparser for example",
          "score": 2,
          "created_utc": "2026-02-13 15:03:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56nnrk",
          "author": "One_Milk_7025",
          "text": "Docling is not the end game.. use it just to parse but there is a whole lot of post processing before ingestion begins. But yes depends upon on the need",
          "score": 1,
          "created_utc": "2026-02-13 16:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iy0nt",
          "author": "sus4nne",
          "text": "How does your post-processing pipeline work? Which tools, which prompts?",
          "score": 1,
          "created_utc": "2026-02-15 16:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5k0tza",
          "author": "SkyStrong7441",
          "text": "Great feedback and insights. I'm still having an issue I'd like to show you with this example of a table (in swedish):  \nDocling parses the complete table correctly except the row header i marked in red. Docling believes that to be a section header just like \"6.2 Fribelopp\". \n\nThis is a major issue since it causes the first 3 lines to loose their context since their header is gone, plus the whole table is now under that section header marked in red. \n\nIs the solution to parse tables specifically with VLM's?\n\nhttps://preview.redd.it/o2ccxzm0jpjg1.png?width=635&format=png&auto=webp&s=82c232690b0f4cc1885c98d167ad8e721755f466\n\n",
          "score": 1,
          "created_utc": "2026-02-15 19:13:19",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o55b4pw",
          "author": "Independent-Cost-971",
          "text": "Try [Kudra.ai](http://Kudra.ai) You won't regret it I promise. ",
          "score": -1,
          "created_utc": "2026-02-13 11:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o592u9p",
              "author": "Awkward-Customer",
              "text": "Using a third party for document processing often isn't an option for privacy/confidentiality reasons.",
              "score": 1,
              "created_utc": "2026-02-13 23:20:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5mp7t",
      "title": "Has anyone here successfully sold RAG solutions to clients? Would love to hear your experience (pricing, client acquisition, delivery, etc.)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5mp7t/has_anyone_here_successfully_sold_rag_solutions/",
      "author": "Temporary_Pay3221",
      "created_utc": "2026-02-15 18:55:31",
      "score": 17,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "Hey everyone!\n\nI've been diving deep into RAG systems lately and I'm genuinely fascinated by the technology. I've built a few projects for myself and feel confident in my technical abilities, but now I'm looking to transition this into actual client work.\n\nBefore I jump in, I'd really appreciate learning from people who've already walked this path. If you've sold RAG solutions to clients, I'd love to hear about your experience:\n\n**Client & Project Details:**\n\n* What types of clients/industries did you work with?\n* How did they discover they needed RAG? (Did they come asking for it, or did you identify the use case?)\n* What was the scope? (customer support, internal knowledge base, document search, etc.)\n\n**Delivery & Timeline:**\n\n* How long did the project take from discovery to delivery?\n* What were the biggest technical challenges you faced?\n* Did you handle ongoing maintenance, or was it a one-time delivery?\n\n**Business Side:**\n\n* How did you find these clients? (freelance platforms, LinkedIn outreach, referrals, content marketing, etc.)\n* What did you charge? (ballpark is fine - just trying to understand market rates)\n* How did you structure pricing? (fixed project, hourly, monthly retainer?)\n\n**Post-Delivery:**\n\n* Were clients happy with the results?\n* Did you iterate/improve the system after launch?\n* Any lessons learned that you'd do differently next time?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5mp7t/has_anyone_here_successfully_sold_rag_solutions/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5k30sr",
          "author": "AmbitionCrazy7039",
          "text": "What types of clients/industries did you work with? - Build and deployed for an IP firm.\n\nHow did they discover they needed RAG? - They did by themselves. They asked us for a discovery meeting.  But they basically knew what solutions they want. We picked the most interesting one.\n\nWhat was the scope? - Some very specific document/outcome search. But not just semantics stuff, but some path dependent precedents + analytics. The LLM basically only summarizes the output. Traceability was most important. \n\n\n\nHow long did the project take from discovery to delivery? - \\~1 month from discovery to signing. Pilot delivery \\~6 weeks. A lot of follow ups needed.\n\nWhat were the biggest technical challenges you faced? - Preprocessing shitty data.\n\nDid you handle ongoing maintenance, or was it a one-time delivery? - We handle maintenance if something breaks.\n\n\n\nHow did you find these clients? - Network / they found us.\n\nWhat did you charge? - Our deal was kinda different so any inside from me here will not reflect real market value. We did a research project for them at special conditions (highly profitable for both sides).\n\nHow did you structure pricing? - Fixed price per month for developing with 2 months of expected development time. We granted the option to exit the project after 1 month without paying for 2. If we would build in a different setting we surely would go for usual 30/40/30 splits.\n\n\n\nWere clients happy with the results? - Yes.\n\nDid you iterate/improve the system after launch? - Not yet but we do run market research to evaluate if this may fit a broader audience.\n\nAny lessons learned that you'd do differently next time? - Nothing we really did wrong but the project shifted my view on RAG, from \"some useless semantic search\" to a more broader, creative approach.   \nBut I will add that RAG can't to magic for LLMs. If your project scope is fundamentally not solvable by an LLM (for example writing high quality law stuff), RAG will not change it. \n\n",
          "score": 5,
          "created_utc": "2026-02-15 19:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5k5l2j",
              "author": "Temporary_Pay3221",
              "text": "**Thanks for sharing**  \n  \n**Two questions:**\n\n1. **How did they find you?** You said \"they found us\", but what specifically made them reach out to YOU vs the hundreds of other people who can build RAG systems? Was it your profile? A referral? Something you built publicly?\n2. **Do you want to scale this?** Are you thinking about:\n\n* Building a dev team to handle delivery while you focus on growth?\n* Hiring sales to bring in more deals?\n* Creating systems/processes to delegate the work?\n\nOr is your goal to stay small, just you (maybe +1-2 people) doing selective projects?\n\nBecause right now you're limited by your own time. Curious if scaling is part of your plan or not.",
              "score": 2,
              "created_utc": "2026-02-15 19:36:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5k859m",
                  "author": "AmbitionCrazy7039",
                  "text": "1. Network. I started a ML startup years ago. Although it had nothing to do with LLMs and RAG.\n2. Who says we don't have a dev+sales team? My goal is and has ever been to build things that really create value. This system seems to deliver to the expectations. May open source it. The difficult part is the preprocessing and this pipelines are fundamentally not really scalable. Building the individual pipelines for customers is both fun and high-payed.",
                  "score": 3,
                  "created_utc": "2026-02-15 19:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kxqpn",
              "author": "AumOza",
              "text": "Curious what you meant by shitty data was it OCR problems from scanned docs, messy formatting, or something more complex?",
              "score": 1,
              "created_utc": "2026-02-15 22:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n3dd1",
                  "author": "AmbitionCrazy7039",
                  "text": "Yes we had to do OCR from scanned docs. This was one sketchy thing.\n\nThe other problem was more related to data retrieval. In this particular use case, some of the data came from internal documents. That wasn't a problem. However, in order to develop a really good product, we needed data from a public source called “EPO.” Basically, hundreds of millions of our data are publicly available in scanned PDF files. However, there is no API for retrieving our specific data set, only an online service. We had to scrape a sufficient data set. Technically, we solved that challenge, but the terms of use prohibit or at least severely restrict scraping. ",
                  "score": 1,
                  "created_utc": "2026-02-16 06:26:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r2mpjb",
      "title": "Kreuzberg v4.3.0 and benchmarks",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2mpjb/kreuzberg_v430_and_benchmarks/",
      "author": "Goldziher",
      "created_utc": "2026-02-12 07:02:10",
      "score": 16,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\nI have two announcements related to [Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg): \n\n1. We released our new [comparative benchmarks](https://kreuzberg.dev/benchmarks). These have a slick UI and we have been working hard on them for a while now (more on this below), and we'd love to hear your impressions and get some feedback from the community!\n2. We released v4.3.0, which brings in a bunch of improvements including PaddleOCR as an optional backend, document structure extraction, and native Word97 format support. More details below.\n\n## What is Kreuzberg?\n\n[Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg) is an open-source (MIT license) polyglot document intelligence framework written in Rust, with bindings for Python, TypeScript/JavaScript (Node/Bun/WASM), PHP, Ruby, Java, C#, Golang and Elixir. It's also available as a docker image and standalone CLI tool you can install via homebrew.\n\nIf the above is unintelligible to you (understandably so), here is the TL;DR: Kreuzberg allows users to extract text from 75+ formats (and growing), perform OCR, create embeddings and quite a few other things as well. This is necessary for many AI applications, data pipelines, machine learning, and basically any use case where you need to process documents and images as sources for textual outputs.\n\n## Comparative Benchmarks\n\nOur new comparative benchmarks UI is live here: https://kreuzberg.dev/benchmarks\n\nThe comparative benchmarks compare Kreuzberg with several of the top open source alternatives - Apache Tika, Docling, Markitdown, Unstructured.io, PDFPlumber, Mineru, MuPDF4LLM. In a nutshell - Kreuzberg is 9x faster on average, uses substantially less memory, has much better cold start, and a smaller installation footprint. It also requires less system dependencies to function (only __optional__ system dependency for it is onnxruntime, for embeddings/PaddleOCR).\n\nThe benchmarks measure throughput, duration, p99/95/50, memory, installation size and cold start with more than 50 different file formats. They are run in GitHub CI on ubuntu latest machines and the results are published into GitHub releases (here is an [example](https://github.com/kreuzberg-dev/kreuzberg/releases/tag/benchmark-run-21923145045)). The [source code](https://github.com/kreuzberg-dev/kreuzberg/tree/main/tools/benchmark-harness) for the benchmarks and the full data is available in GitHub, and you are invited to check it out.\n\n## V4.3.0 Changes\n\nThe v4.3.0 full release notes can be found here: https://github.com/kreuzberg-dev/kreuzberg/releases/tag/v4.3.0\n\nKey highlights:\n\n1. PaddleOCR optional backend - in Rust. Yes, you read this right, Kreuzberg now supports PaddleOCR in Rust and by extension - across all languages and bindings except WASM. This is a big one, especially for Chinese speakers and other east Asian languages, at which these models excel.\n\n2. Document structure extraction - while we already had page hierarchy extraction, we had requests to give document structure extraction similar to Docling, which has very good extraction. We now have a different but up to par implementation that extracts document structure from a huge variety of text documents - yes, including PDFs.\n\n3. Native Word97 format extraction - wait, what? Yes, we now support the legacy `.doc` and `.ppt` formats directly in Rust. This means we no longer need LibreOffice as an optional system dependency, which saves a lot of space. Who cares you may ask? Well, usually enterprises and governmental orgs to be honest, but we still live in a world where legacy is a thing.\n\n## How to get involved with Kreuzberg\n\n- Kreuzberg is an open-source project, and as such contributions are welcome. You can check us out on GitHub, open issues or discussions, and of course submit fixes and pull requests. Here is the GitHub: https://github.com/kreuzberg-dev/kreuzberg\n- We have a [Discord Server](https://discord.gg/rzGzur3kj4) and you are all invited to join (and lurk)!\n\nThat's it for now. As always, if you like it -- star it on GitHub, it helps us get visibility!\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r2mpjb/kreuzberg_v430_and_benchmarks/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4y41ed",
          "author": "bonsaisushi",
          "text": "That's impressive, I'm gonna definitely try it out! Is there a reason Marker was not included in the benchmarks?",
          "score": 2,
          "created_utc": "2026-02-12 08:00:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4y6dfj",
              "author": "Goldziher",
              "text": "Hi, simple reason - Marker is GPU only. We will do this in the future.",
              "score": 2,
              "created_utc": "2026-02-12 08:22:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4y88j0",
                  "author": "bonsaisushi",
                  "text": "Thanks!",
                  "score": 1,
                  "created_utc": "2026-02-12 08:41:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yd9ya",
          "author": "Fun-Purple-7737",
          "text": "Sorry for my ignorance, but does Kreuzberg understand diagrams and figures? And I do not mean OCR, but really understanding what connects to what etc. I other words, is there either internal or external VLM component for deeper visual understanding other than OCR? If not, Docling is still the way I am afraid..",
          "score": 2,
          "created_utc": "2026-02-12 09:31:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4yqnj9",
              "author": "Goldziher",
              "text": "There is no VLM at all. If you need a vision model, then you'll have to create a pipeline.",
              "score": 1,
              "created_utc": "2026-02-12 11:35:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4z39zj",
          "author": "vanwal_j",
          "text": "Cool! Will give it a try. \nEven tho my current pain point is scaling and handling burst without breaking the bank 😬",
          "score": 2,
          "created_utc": "2026-02-12 13:06:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o543wgz",
          "author": "Ok_Mirror7112",
          "text": "Good job, will test on this tomorrow ",
          "score": 1,
          "created_utc": "2026-02-13 05:08:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0u2in",
      "title": "Need help with chunking and embedding strategies for my rag model",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r0u2in/need_help_with_chunking_and_embedding_strategies/",
      "author": "Forsaken-Cod-4944",
      "created_utc": "2026-02-10 07:08:26",
      "score": 13,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "I’m fairly new to this field, and after reading a few posts here, I realized that to get accurate and reliable results, I need to carefully design a chunking and embedding strategy. The problem is, I’m not really sure how to approach that yet.\n\nHere’s what I’ve built so far:\n\nI created a RAG-based model for companies that can be integrated into their app or website. The idea is that employees or anyone interested can use it to research and ask questions about the company. The data it handles includes:\n\n* Company policies (long, formal documents)\n* HR documents (rule-based and structured)\n* FAQs (short Q&A format)\n* Financial summaries (number-heavy content)\n* Product documentation (technical text and details about previous projects)\n* CSV structured data (tabular format)\n* Website content (marketing and general information)\n\nSince I’m a second-year student, I can’t afford paid services, so I’m planning to run everything locally. Right now, I’m considering using **baai/bge-m3** for embeddings and retrieval, and **Qwen3–1.7B-MLX-8bit** for answer generation.\n\nI also have a question about deployment. I’m planning to dockerize the entire system for a live demo on my resume. If I run the LLM locally, will it still work for someone accessing it from another device? Or would they also need the model running on their own machine?\n\nI’d really appreciate any guidance or suggestions. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r0u2in/need_help_with_chunking_and_embedding_strategies/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4l63n3",
          "author": "RobertLigthart",
          "text": "for the chunking honestly dont overthink it at the start. use something like recursive character splitting with \\~500-800 tokens and some overlap (\\~100 tokens). the key thing most people miss is that different doc types need different chunk sizes -> FAQs should stay as complete Q&A pairs, policies can be chunked by section headers, and CSV data you probably want to convert to natural language sentences before embedding\n\n  \nbge-m3 is a solid choice for embeddings especially since its multilingual. for deployment if you dockerize it and host it on a VPS or even a free tier cloud instance the LLM runs server-side so anyone can access it through the API... they dont need anything on their machine. just be aware that Qwen 1.7B is going to be pretty limited for complex questions tho",
          "score": 2,
          "created_utc": "2026-02-10 09:30:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8s85",
              "author": "Forsaken-Cod-4944",
              "text": "Thanks for the clarity but i couldnt understand the llm part, im planning to dockerize and upload my project on [render.com](http://render.com) so my question was will the query generation part work on other devices even if my device is switch off or not running?\n\n",
              "score": 1,
              "created_utc": "2026-02-10 09:56:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz01b",
          "author": "notsarthaxx",
          "text": "What are your gpu specs?",
          "score": 1,
          "created_utc": "2026-02-10 13:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7rb5",
              "author": "Forsaken-Cod-4944",
              "text": "4050 6gb, 55W (Laptop)",
              "score": 1,
              "created_utc": "2026-02-10 14:09:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4n8q9c",
                  "author": "notsarthaxx",
                  "text": "damn does running these models lag?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 17:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mp9xh",
          "author": "Popular_Sand2773",
          "text": "You are describing a highly diverse and often large dataset i.e. an entire corporate knowledge base. If you one size fits all this you'll certainly have something but with such a weak llm 1.7B params its going to be hard to cover up ugly/messy retrieval. \n\nIf you want to tackle all of these hierarchical search and provenance are your friends. Honestly though I would take these one at a time and do what's right for each piece then have the agent use the appropriate tool for each job rather than a monolithic retriever. ",
          "score": 1,
          "created_utc": "2026-02-10 15:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n6uys",
              "author": "Forsaken-Cod-4944",
              "text": "I just need the basics since i'm pretty new to this , when i get the logic and build something that can atleast work on low-medium dataset , I'll try switching to 8-70B parameter llms.\n\nand about provenance , is it a type of reranker? if not then can it be used with it",
              "score": 1,
              "created_utc": "2026-02-10 17:00:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4osozg",
                  "author": "Popular_Sand2773",
                  "text": "That makes sense every time I dip my toes in models that size it ends up being rough. Provenance is keeping track of where things come from. So instead of putting everything into a db you add metadata like faq vs HR vs policy. Then an agent can do something like a filtered search. Oh this question is likely about policy let me check policy specific documents. Lots of ways to execute that's just a basic example of using metadata to limit the search space and improve results. These filters are usually applied pre-re-ranker so go nuts!",
                  "score": 1,
                  "created_utc": "2026-02-10 21:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xw7j8",
          "author": "SharpRule4025",
          "text": "For corporate knowledge bases with mixed document types, the chunking strategy matters a lot more than the embedding model. A few things that helped me when I was working on something similar.\n\nPDFs and Word docs, convert to markdown first so you have consistent formatting. For web content, strip everything except the actual body content before chunking. Navigation, headers, footers, all of that pollutes your embeddings.\n\nFor chunk size, 500 to 800 tokens with 100 token overlap is a safe starting point. But the real win comes from respecting document structure. Split on headings and section boundaries instead of arbitrary character counts. A chunk that starts mid-paragraph and ends mid-sentence will retrieve poorly regardless of your embedding model.\n\nAlso, storing metadata alongside chunks (source document, section heading, document type) lets you filter at query time which makes a bigger difference than most retrieval tricks.",
          "score": 1,
          "created_utc": "2026-02-12 06:46:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r14lw9",
      "title": "Latest embedding Voyage 4 in RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r14lw9/latest_embedding_voyage_4_in_rag/",
      "author": "midamurat",
      "created_utc": "2026-02-10 15:58:41",
      "score": 12,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "It's been some time since Voyage 4 embedding was released. But couldn't find much on how it actually is in real-world RAG. So I tested it myself.\n\nIt turns out, it is pretty good! \n\n**Setup**:\n\n* 14 models, 6 datasets (finance, web, SciFact, entity retrieval)\n* Metrics: NDCG@10 + pairwise LLM-judge (Elo) on Top-5 chunks\n\n  \nSome findings:\n\n* Voyage-4 is the **strongest general-purpose** retriever among other 14\n\n• 61.7% win rate, Elo 1606, 0.859 NDCG@10\n\n* Biggest improvement is Top-5 density (≈2.3× stronger vs Top-10 eval)\n* Very **strong** on entity-rich / business docs\n* **Weak** on SciFact (high tie rate, behaves more like a semantic matcher than a reasoner)\n* Voyage-4 vs OpenAI text-embedding-3-large was a **stalemate**\n\n  \nI concluded that if your RAG setup is optimized for small context windows (Top-3 / Top-5), Voyage-4 is a solid upgrade. \n\nHappy to share more details if useful. \n\n(posted more detailed breakdown here: [https://agentset.ai/blog/voyage-4](https://agentset.ai/blog/voyage-4) )\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r14lw9/latest_embedding_voyage_4_in_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4n24ry",
          "author": "-Cubie-",
          "text": "Did you try their open weight voyage-4-nano? That one interests me the most.",
          "score": 2,
          "created_utc": "2026-02-10 16:39:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n3u5n",
              "author": "midamurat",
              "text": "not yet, but i'll def try!",
              "score": 2,
              "created_utc": "2026-02-10 16:46:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4qhb6a",
                  "author": "beallg",
                  "text": "Yes, please add nano! Being their only open-source offering it would be excellent to see how it compares to their larger models and against openAI",
                  "score": 1,
                  "created_utc": "2026-02-11 03:04:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n53rh",
          "author": "Informal_Tangerine51",
          "text": "Good benchmark but misses the production question: when retrieval quality degrades over time, how do you debug it?\n\nVoyage-4 wins on Top-5 density today. Next month your queries start returning worse results. Without logging what Voyage-4 returned vs what users actually needed, you can't tell if it's: query distribution shift, index staleness, model behavior change, or edge cases your eval datasets missed.\n\nNDCG and Elo show snapshot quality. Production needs: retrieval decision traces (what was retrieved, what scores, why Top-1 vs Top-2), query patterns that fail, ability to replay historical retrievals to catch regressions. Otherwise \"quality degraded\" becomes guesswork with re-indexing or model switching.\n\nAre you capturing retrieval results in production to validate eval findings hold, or assuming benchmark performance stays consistent?",
          "score": 1,
          "created_utc": "2026-02-10 16:52:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n89wk",
              "author": "midamurat",
              "text": "Good point. I have per-query breakdowns and LLM-judge evaluation of top-5 results, but I don't track temporal drift or test against your specific query distribution. I think it would be valuable to add monthly re-evaluation. ",
              "score": 1,
              "created_utc": "2026-02-10 17:07:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4nsw9l",
          "author": "my_byte",
          "text": "Can you rerun with nano model for the queries? The shared embedding space to be able to use a small and cheap model for queries seems very compelling.\nAlso - you benchmarked the regular, not the large model? And with 1k dimensions rather than 2k? I wonder how well the better model at twice the dimensions would've done.",
          "score": 1,
          "created_utc": "2026-02-10 18:41:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4o9uwx",
              "author": "midamurat",
              "text": "yes, that'd be interesting! i'll run with nano and large models",
              "score": 1,
              "created_utc": "2026-02-10 20:00:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ofwvf",
                  "author": "my_byte",
                  "text": "Awesome. Keep us updated! I assume your dataset is private?",
                  "score": 1,
                  "created_utc": "2026-02-10 20:28:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4nucha",
          "author": "Federal_Act_3325",
          "text": "I notice that the voyage-4 model is significantly cheaper than OpenAI's text-embedding-3-large in terms of per-token and storage (dimension size).\n\nI wonder if voyage-4-large would come out a clear winner as it is more comparable price-wise?",
          "score": 1,
          "created_utc": "2026-02-10 18:48:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4o9qqy",
              "author": "midamurat",
              "text": "I'll run voyage large too, let's see how it performs\n\n",
              "score": 1,
              "created_utc": "2026-02-10 19:59:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rbd9e",
          "author": "AICodeSmith",
          "text": "if Voyage-4 really outperforms most retrievers in tight Top-5 contexts and matches OpenAI’s embeddings overall, has anyone seen how it handles messy, real-world document retrieval versus more structured data?\n\n",
          "score": 1,
          "created_utc": "2026-02-11 06:46:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ry05w",
              "author": "midamurat",
              "text": "speaking of real-world document, one of my datasets is business reports from many corporates. and it actually was strongest in this dataset",
              "score": 1,
              "created_utc": "2026-02-11 10:17:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2lpi7",
      "title": "Stop manually implementing SOTA papers. Benchmark RAG with one command",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2lpi7/stop_manually_implementing_sota_papers_benchmark/",
      "author": "Reasonable-Front471",
      "created_utc": "2026-02-12 06:03:14",
      "score": 12,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I’m the creator of AutoRAG (4.6K stars). I love squeezing out RAG performance, but I got tired of paying the \"Research Tax\"—the endless cycle of re-formatting data and hard-coding paper implementations just to test a new idea.\n\nSo I built AutoRAG-Research.\n\nWhat I built:\n\n* One-Command Benchmarking: Run SOTA pipelines against your data instantly.\n* Unified Datasets: Pre-formatted datasets + pre-computed embeddings. No more format hell.\n* Paper-to-Code: SOTA implementations. ready out of the box.\n* Python, MIT Licensed, Open Source.\n\nWhy I built this: Every paper claims SOTA, but you don't know until you run it on *your* workload. This tool lets you stop being a Data Cleaner and start being a Researcher again.\n\nRepo:[https://github.com/NomaDamas/AutoRAG-Research](https://github.com/NomaDamas/AutoRAG-Research)\n\n\n\nFeel free to roast the code or suggest a SOTA pipeline you think we should implement next!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r2lpi7/stop_manually_implementing_sota_papers_benchmark/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4xx0ap",
          "author": "ChapterEquivalent188",
          "text": "Huge respect for automating the Research Tax!  I've been following AutoRAG, and it’s exactly the tooling maturity this space needs\n\nI took a slightly different path: Instead of tuning parameters for SOTA, I spent 2 years building a 'paranoid' architecture focused on reliability (Multi-Lane Consensus, Docling, Graph-Verification). Basically, throwing multiple agents at the problem to force validity\n\nI’d genuinely love a roast from someone who understands the SOTA benchmarks. If you have a moment, I dropped the architecture manifest here: https://github.com/2dogsandanerd/RAG_enterprise_core : Do you see a future where AutoRAG can benchmark these complex, non-linear agentic flows (Consensus Engines), or will benchmarks always favor linear pipelines?\n\nthx",
          "score": 1,
          "created_utc": "2026-02-12 06:54:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52xev1",
              "author": "Reasonable-Front471",
              "text": "Thanks for the comment! I found it really interesting that you focused on reliability — that angle definitely stood out to me. Appreciate you sharing this, I’ll dig into it more.\n\nTo be honest, while AutoRAG *could* handle non-linear flows, I felt like it would require a pretty major overhaul to do it properly. So I ended up creating AutoRAG-Research with a new architecture specifically to explore non-linear, agentic flows more deeply.\n\nGoing forward, AutoRAG-Research will include a lot more non-linear flows (there are already a few in there). Excited to keep expanding it",
              "score": 2,
              "created_utc": "2026-02-13 00:36:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o549lt4",
                  "author": "ChapterEquivalent188",
                  "text": ">",
                  "score": 1,
                  "created_utc": "2026-02-13 05:53:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzzvqm",
      "title": "Email threads are probably the hardest RAG problem",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzzvqm/email_threads_are_probably_the_hardest_rag_problem/",
      "author": "EnoughNinja",
      "created_utc": "2026-02-09 09:34:19",
      "score": 12,
      "num_comments": 15,
      "upvote_ratio": 0.83,
      "text": "Most RAG patterns assume stable documents. PDFs, wikis, knowledge bases are all text with stable boundaries where chunking is at least defensible, even if it's imperfect, whereas email can completely fall apart once you actually try to build on it.\n\nBecause an email thread is a moving target where every reply contains some version of the conversation history, but never the same version. Mail clients work differently, one might nest everything, others paste raw text, or strip the headers, and so you end up having to deal with overlapping partial copies of a conversation, and if you chunk that naively you get duplication at the embedding level where the same paragraph shows up three or four times, slightly reformatted, and your retrieval keeps surfacing what is essentially identical content framed different ways while your relevance scores quietly lose meaning.\n\nYou could try deduping, only to hit the real trap of quoted text, which is almost never character-identical across clients. Fuzzy matching helps until it starts collapsing genuinely distinct messages that happen to share phrasing.\n\nThreading doesn't work either because thread IDs break on forwards, subject line changes, and missing headers, so messages that are obviously part of the same conversation from a human perspective end up scattered across your index like unrelated documents.\n\nWhere this really breaks down is when you ask actual work questions.\n\n\"What did we agree with this client?\" almost never lives in one message or one thread. It's spread across a sales thread, a legal side-thread, and an internal discussion the client never saw, each with different participants and visibility.\n\nSemantic search can retrieve pieces of each but it has no concept of who knew what or when. Metadata like To, CC, BCC, timestamps, and thread boundaries ends up mattering more than the text content itself because those fields determine the informational topology of the conversation.\n\nStrip that away and the model reasons over a flattened reality where everyone saw everything simultaneously, which is why questions about knowledge gaps or outdated commitments produce confident nonsense.\n\nAnd even after retrieval, you can't just concatenate chunks and hope. Ordering across threads matters, attribution matters, and a two-line reply from the CEO carries far more weight than a long, tangential FYI thread, but nothing in the embedding space captures that distinction.\n\nWe built an API to handle this full pipeline (igpt.ai) because treating email like document RAG breaks down exactly in the cases that matter: cross-thread, multi-participant queries that real work actually depends on.\n\nEmail is encoded in conversation, with participant dynamics, visibility constraints, and temporal dependencies baked in, and standard RAG architectures that ignore that structure will produce answers that sound right while being wrong about the actual situation.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzzvqm/email_threads_are_probably_the_hardest_rag_problem/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4fj3qy",
          "author": "StackOwOFlow",
          "text": "You talked about everything that doesn't work. What does?",
          "score": 2,
          "created_utc": "2026-02-09 13:41:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fy7sc",
              "author": "Technical-Will-2862",
              "text": "its a secret ad for their API (which imma vibe code and open source in 10 min out of spite)",
              "score": 9,
              "created_utc": "2026-02-09 15:05:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l1twm",
                  "author": "EnoughNinja",
                  "text": "First of all, there's nothing secret, the link is literally right there. I couldn't be more transparent\n\nSecond, if you can vibe code cross-thread resolution with participant visibility tracking in 10 min I want to see that repo.",
                  "score": 1,
                  "created_utc": "2026-02-10 08:48:47",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4kzz5h",
              "author": "patbhakta",
              "text": "it's an ad for thier product...but in principle it's really not hard to do what they did unless they created isolated graphs for each user but that can get loud very fast.  \n  \nmost non production RAG systems are using the same recipe for everything, it might work for a kid with a microwave at home but in an actual restaurant kitchen you need different tools for different reciepes.\n\n  \n1) emails are conversational, so treat them like a conversation instead of a knowledgebase or document...so incorporate that into your parsing strategy\n\n  \n2) emails are time stamped, there's a newer version similar to git versioning, so perhaps go with a database like duckdb\n\n  \n3) emails have tons of replies, long threads of regurgitated stuff, email signatures and disclaimers as if they are important, auto responders, etc. this repetition will skew your vectors and rankings emensly. you need to use a combination of pre-parsing and dedups.",
              "score": 1,
              "created_utc": "2026-02-10 08:30:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4lfk6u",
                  "author": "EnoughNinja",
                  "text": "You’re right that parsing, dedup, and threading are table stakes and yes we do all of that\n\nBut that stack only gives you clean messages, but not the informational topology of the system, i.e. who knew what, when commitments were superseded, which messages carried authority or which conversations were visible to which participants\n\nThat’s why questions like “what did we agree on?” fail even after “good” retrieval because the answer doesn’t live in a thread, it lives across threads with different visibility, timing, and authority and none of that survives flattening into embeddings\n\nThe “isolated graphs get loud” problem is exactly the point. If you don’t build that isolation and orchestration layer, the system produces answers that sound right while being wrong about the actual state of work.",
                  "score": 1,
                  "created_utc": "2026-02-10 10:59:15",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fscb3",
          "author": "wurzelbrunft",
          "text": "One attempt to solve this is to feed the mail-thread into an LLM and prompt it to clean it up. Then the cleaned up version goes into the vector database.",
          "score": 2,
          "created_utc": "2026-02-09 14:34:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fxy7g",
              "author": "Technical-Will-2862",
              "text": "they could create a script that cleans it up automatically and then add a temporal matrix to use timing + semantic continuity to bring in a relevant flow of information rather than static pieces. ",
              "score": 1,
              "created_utc": "2026-02-09 15:04:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4l1yj1",
                  "author": "EnoughNinja",
                  "text": "The temporal matrix part undersells how nasty the edge cases get. Thread IDs breaking on forwards, quoted text that's almost-but-not-quite identical across clients, visibility differences between To/CC/BCC, the script ends up being a pretty thick system.",
                  "score": 2,
                  "created_utc": "2026-02-10 08:50:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4gumyy",
          "author": "AuditMind",
          "text": "I think the difficulty here is that email threads carry state and authority, not just content.\n\nRAG assumes document-like inputs with stable meaning, but emails evolve over time and across participants. Once that structure is flattened into text chunks, the model has no reliable way to reason about precedence, visibility, or validity, so it may produce answers that sound right without reflecting the true outcome.\n\nIn that sense, the problem is primarily structural rather than purely a retrieval issue.",
          "score": 2,
          "created_utc": "2026-02-09 17:41:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4fvz6z",
          "author": "StackOwOFlow",
          "text": "Email RAG final boss: [https://jmail.world/about/doj-2026](https://jmail.world/about/doj-2026)",
          "score": 1,
          "created_utc": "2026-02-09 14:53:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5lj2m",
      "title": "RAG for structured feature extraction from 500-700 page documents — what's your strategy?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5lj2m/rag_for_structured_feature_extraction_from_500700/",
      "author": "Weary_Supermarket399",
      "created_utc": "2026-02-15 18:10:21",
      "score": 12,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I'm trying to build a RAG pipeline to extract \\~50 predefined features from large tender/procurement documents (think: project name, technical specs, deadlines, payment terms, penalties, etc.). Each feature has its own set of search queries and an extraction prompt.\n\nWorks reasonably well on shorter docs (\\~80 pages). On 500-700 page documents with mixed content (specs, contracts, schedules, drawings, BOQs), retrieval quality drops hard. The right information exists, but indexing and retrieval become difficult.\n\nThis feels like a fundamentally different problem from conversational QA. You're not answering one question, you're running 50 targeted extractions across a massive document set where the answer for each could be anywhere.\n\n**For those who've built something similar:** How do you approach retrieval when the document is huge, the features are predefined, and simple semantic search isn't enough?\n\nCurious about any strategies — chunking, retrieval, reranking, or completely different architectures.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5lj2m/rag_for_structured_feature_extraction_from_500700/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5juwz0",
          "author": "Astroa7m",
          "text": "I was experimenting with something similar by loading punch of data and extracting Q/A. \n\nWell you could try the following:\n- you have to call the LLM multiple times but make sure to chunk your data so it is within the LLM’s input limit\n- sometimes sticking to input limit is not enough as you would have hallucinations/in-complete result in your output so you could either turn off thinking tokens or take a percentage of the input token and keep reducing it gradually and see what works \n- I experimented with a lightweight compression format where I retain the meaning for LLMs by keeping only verbs, nouns, proper nouns, punctuation , numbers, and symbols. Worked great but poor with other languages depending on the POS ML model used (used spaCy NLP lib)\n\nFinally we need to aggregate all through either clustering or another llm call. \nHowever, this is only my personal experience I think there are more brilliant approaches.",
          "score": 3,
          "created_utc": "2026-02-15 18:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jx32p",
          "author": "4641874165465",
          "text": "Second this. I'm running into a wall also. I'm trying to implement Graph Rag also, but I'm a noob ans Just learning and vibe coding hard on simpler Test Data, to get NER right.",
          "score": 2,
          "created_utc": "2026-02-15 18:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mtrkr",
              "author": "FUNdationOne",
              "text": "Try www.bildy.ai/documents\n\nI built it to solve the extraction of data from documents. Let me know how it goes for you and if you need any help.",
              "score": 0,
              "created_utc": "2026-02-16 05:08:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kk42z",
          "author": "reallynewaccount",
          "text": "Keep track on this post. Similar problem. My case is also hardened by multiple questions like \"how many XYZ-related facts in this document\" which kills any Related scenario. Also my input is 200+ pages business reports pdfs made out of bad quality scans (sometimes 2-3 first symbols could be cut in each line).\nSo, if there is any reasonable solution, will be great to know it exists.",
          "score": 2,
          "created_utc": "2026-02-15 20:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mtu1a",
              "author": "FUNdationOne",
              "text": "Try www.bildy.ai/documents\n\nI built it to solve the extraction of data from documents. Let me know how it goes for you and if you need any help.",
              "score": 0,
              "created_utc": "2026-02-16 05:08:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5m7fwy",
          "author": "Linkman145",
          "text": "Typically documents are separated. E.g. there’s chapters segments etc\n\nIf your documents are huge then have a preprocessing step where you split them semantically. Think chapters 1-5 then another file for chapters 6-10 and so on.\n\nThen you do a multistep rag of sorts; you have a decision algorithm to decide which of the document pieces is relevant to your case and then rag over that single document\n\nSorry for no punctuation am on mobile",
          "score": 2,
          "created_utc": "2026-02-16 02:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lv1af",
          "author": "SFXXVIII",
          "text": "You’ll want to look into a setup where you fan out to the pages in batches or individually to find candidate answers and then aggregate over that to find your final extractions.",
          "score": 1,
          "created_utc": "2026-02-16 01:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n8e7m",
          "author": "nightman",
          "text": "Check how parsing and extracting structured data works on e.g. https://www.llamaindex.ai/\n\nThen compare with your chunk content and results and see what you need to improve.",
          "score": 1,
          "created_utc": "2026-02-16 07:10:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}