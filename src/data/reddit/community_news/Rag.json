{
  "metadata": {
    "last_updated": "2026-02-19 09:09:59",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 133,
    "file_size_bytes": 198460
  },
  "items": [
    {
      "id": "1r47duk",
      "title": "We Benchmarked 7 Chunking Strategies. Most 'Best Practice' Advice Was Wrong.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r47duk/we_benchmarked_7_chunking_strategies_most_best/",
      "author": "Confident-Honeydew66",
      "created_utc": "2026-02-14 01:30:40",
      "score": 113,
      "num_comments": 21,
      "upvote_ratio": 0.94,
      "text": "If you've built a RAG system, you've had the chunking conversation. Somebody on your team (or [a Medium post](https://medium.com/%40adnanmasood/chunking-strategies-for-retrieval-augmented-generation-rag-a-comprehensive-guide-5522c4ea2a90)) told you to \"just use 512 tokens with 50-token overlap\" or \"semantic chunking is strictly better.\"\n\nWe (hello from the R&D team at Vecta!) decided to test these claims. We created a small corpus of real academic papers spanning AI, astrophysics, mathematics, economics, social science, physics, chemistry, and computer vision. Then, we ran every document through seven different chunking strategies and measured retrieval quality and downstream answer accuracy.\n\nCritically, we designed the evaluation to be **fair**: each strategy retrieves a different number of chunks, calibrated so that every strategy gets approximately **2,000 tokens of context** in the generation prompt. This eliminates the confound where strategies with larger chunks get more context per retrieval, and ensures we're measuring chunking quality, not context window size.\n\nThe \"boring\" strategies won. The hyped strategies failed. And the relationship between chunk granularity and answer quality is more nuanced than most advice suggests.\n\n# Setup\n\n# Corpus\n\nWe assembled a diverse corpus of 50 academic papers (905,746 total tokens) deliberately spanning similar disciplines, writing styles, and document structures: Papers ranged from 3 to 112 pages and included technical dense mathematical proofs pertaining to fundamental ML research. All PDFs were converted to clean markdown using [MarkItDown](https://github.com/microsoft/markitdown), with OCR artifacts and single-character fragments stripped before chunking.\n\n# Chunking Strategies Tested\n\n1. **Fixed-size, 512 tokens**, 50-token overlap\n2. **Fixed-size, 1024 tokens**, 100-token overlap\n3. **Recursive character splitting**, LangChain-style `RecursiveCharacterTextSplitter` at 512 tokens\n4. **Semantic chunking**, embedding-based boundary detection (cosine similarity threshold 0.7)\n5. **Document-structure-aware**, splitting on markdown headings/sections, max 1024 tokens\n6. **Page-per-chunk**, one chunk per PDF page, using MarkItDown's form-feed (`\\f`) page boundaries\n7. **Proposition chunking**, LLM-decomposed atomic propositions following [Dense X Retrieval](https://arxiv.org/abs/2312.06648) with the paper's exact extraction prompt\n\nAll chunks were embedded with `text-embedding-3-small` and stored in local ChromaDB. Answer generation used `gemini-2.5-flash-lite` via OpenRouter. We generated 30 ground-truth Q&A pairs using Vecta's synthetic benchmark pipeline.\n\n# Equal Context Budget: Adaptive Retrieval k\n\nMost chunking benchmarks use a fixed top-k (e.g., k=10) for all strategies. This is fundamentally unfair: if fixed-1024 retrieves 10 chunks, the generator sees \\~10,000 tokens of context; if proposition chunking retrieves 10 chunks at 17 tokens each, the generator gets \\~170 tokens. The larger-chunk strategy wins by default because it gets more context, not because its chunking is better.\n\nWe fix this by computing an **adaptive k** for each strategy. This targets \\~2,000 tokens of retrieved context for every strategy. The computed values:\n\n|Strategy|Avg Tokens/Chunk|Adaptive k|Expected Context|\n|:-|:-|:-|:-|\n|Page-per-Chunk|961|2|\\~1,921|\n|Doc-Structure|937|2|\\~1,873|\n|Fixed 1024|658|3|\\~1,974|\n|Fixed 512|401|5|\\~2,007|\n|Recursive 512|397|5|\\~1,984|\n|Semantic|43|46|\\~1,983|\n|Proposition|17|115|\\~2,008|\n\nNow every strategy gets \\~2,000 tokens to work with. Differences in accuracy reflect genuine chunking quality, not context budget.\n\n# How We Score Retrieval: Precision, Recall, and F1\n\nWe evaluate retrieval at two granularities: **page-level** (did we retrieve the right pages?) and **document-level** (did we retrieve the right documents?). At each level, the core metrics are precision, recall, and F1.\n\nLet R be the set of retrieved items (pages or documents) and G be the set of ground-truth relevant items.\n\nPrecision measures: of everything we retrieved, what fraction was actually relevant? A retriever that returns 5 pages, 4 of which contain the answer, has a precision of 0.8. High precision means low noise in the context window.\n\nRecall measures: of everything that *was* relevant, what fraction did we find? If 3 pages contain the answer and we retrieved 2 of them, recall is 0.67. High recall means we're not missing important information.\n\nF1 is the harmonic mean of precision and recall. It penalizes strategies that trade one for the other and rewards balanced retrieval.\n\n**Why two granularities matter.** Page-level metrics tell you whether you're pulling the right *passages*. Document-level metrics tell you whether you're pulling from the right *sources*. A strategy can score high page-level recall (finding many relevant pages) while scoring low document-level precision (those pages are scattered across too many irrelevant documents). As we'll see, the tension between these two levels is one of the main findings.\n\n# Results\n\n# The Big Picture\n\n[Figure 1: Complete metrics heatmap. Green is good, red is bad.](https://www.runvecta.com/blog/chunking/metrics_heatmap.png)\n\n|Strategy|k|Doc F1|Page F1|Accuracy|Groundedness|\n|:-|:-|:-|:-|:-|:-|\n|**Recursive 512**|5|0.86|**0.92**|**0.69**|0.81|\n|**Fixed 512**|5|0.85|0.88|0.67|**0.85**|\n|**Fixed 1024**|3|**0.88**|0.72|0.61|0.86|\n|**Doc-Structure**|2|0.88|0.69|0.52|0.84|\n|**Page-per-Chunk**|2|0.88|0.69|0.57|0.81|\n|**Semantic**|46|0.42|0.91|0.54|0.81|\n|**Proposition**|115|0.27|**0.97**|0.51|**0.87**|\n\n**Recursive splitting wins on accuracy (69%) and page-level retrieval (0.92 F1).** The 512-token strategies lead on generation quality, while larger-chunk strategies lead on document-level retrieval but fall behind on accuracy.\n\n# Finding 1: Recursive and Fixed Splitting Often Outperforms Fancier Strategies\n\n[Figure 2: Accuracy and groundedness by strategy. Recursive and fixed 512 lead on accuracy.](https://www.runvecta.com/blog/chunking/generation_quality.png)\n\nLangChain's `RecursiveCharacterTextSplitter` at 512 tokens achieved the highest accuracy (**69%**) across all seven strategies. Fixed 512 was close behind at 67%. Both strategies use 5 retrieved chunks for \\~2,000 tokens of context.\n\nWhy does recursive splitting edge out plain fixed-size? It tries to break at natural boundaries, paragraph breaks, then sentence breaks, then word breaks. On academic text, this preserves logical units: a complete paragraph about a method, a full equation derivation, a complete results discussion. The generator gets chunks that make semantic sense, not arbitrary windows that may cut mid-sentence.\n\nRecursive 512 also achieved the best page-level F1 (**0.92**), meaning it reliably finds the right pages *and* produces accurate answers from them.\n\n# Finding 2: The Granularity-Retrieval Tradeoff Is Real\n\n[Figure 3: Radar chart, recursive 512 (orange) has the fullest coverage. Large-chunk strategies skew toward doc retrieval but lose on accuracy.](https://www.runvecta.com/blog/chunking/radar_comparison.png)\n\nWith a 2,000-token budget, a clear tradeoff emerges:\n\n* **Smaller chunks (k=5)** achieve higher accuracy (67-69%) because 5 retrieval slots let you sample from 5 different locations in the corpus, each precisely targeted\n* **Larger chunks (k=2-3)** achieve higher document F1 (0.88) because each retrieved chunk spans more of the relevant document, but the generator gets fewer, potentially less focused passages\n\nFixed 1024 scored the best document F1 (**0.88**) but only 61% accuracy. With just k=3, you get 3 large passages, great for document coverage, but if even one of those passages isn't well-targeted, you've wasted a third of your context budget.\n\n# Finding 3: Semantic Chunking Collapses at Scale\n\n[Figure 4: Chunk size distribution. Semantic and proposition chunking produce extremely small fragments.](https://www.runvecta.com/blog/chunking/chunk_size_distribution.png)\n\nSemantic chunking produced **17,481 chunks averaging 43 tokens** across 50 papers. With k=46, the retriever samples from 46 different tiny chunks. The result: only **54% accuracy** and **0.42 document F1**.\n\nHigh page F1 (0.91) reveals what's happening: the retriever *finds the right pages* by sampling many tiny chunks from across the corpus. But document-level retrieval collapses because those 46 chunks come from dozens of different documents, diluting precision. And accuracy suffers because 46 disconnected sentences don't form a coherent narrative for the generator.\n\n**The fundamental problem:** semantic chunking optimizes for retrieval-boundary purity at the expense of context coherence. Each chunk is a \"clean\" semantic unit, but a single sentence chunk may lack the surrounding context needed for generation.\n\n# Finding 4: The Page-Level Retrieval Story\n\n[Figure 5: Page-level precision-recall tradeoff. Recursive 512 achieves the best balance.](https://www.runvecta.com/blog/chunking/precision_recall.png)\n\n[Figure 6: Page-level and document-level F1. The two metrics tell different stories.](https://www.runvecta.com/blog/chunking/retrieval_f1.png)\n\nPage-level and document-level retrieval tell opposite stories under constrained context:\n\n* **Fine-grained strategies** (proposition k=115, semantic k=46) achieve high page F1 (0.91-0.97) by sampling many pages, but low doc F1 (0.27-0.42) because those pages come from too many documents\n* **Coarse strategies** (page-chunk k=2, doc-structure k=2) achieve high doc F1 (0.88) by retrieving fewer, more relevant documents, but lower page F1 (0.69) because 2 chunks can only cover 2 pages\n\n**Recursive 512 at k=5 hits the best balance**: 0.92 page F1 and 0.86 doc F1. Five chunks is enough to sample multiple relevant pages while still concentrating on a few documents.\n\n[Figure 7: Document-level precision, recall, and F1 detail. Large-chunk strategies lead on precision; fine-grained strategies lead on recall.](https://www.runvecta.com/blog/chunking/document_level.png)\n\n# What This Means for Your RAG System\n\n# The Short Version\n\n1. **Use recursive character splitting at 512 tokens.** It scored the highest accuracy (69%), best page F1 (0.92), and strong doc F1 (0.86). It's the best all-around strategy on academic text.\n2. **Fixed-size 512 is a strong runner-up** with 67% accuracy and the highest groundedness among the top performers (85%).\n3. **If document-level retrieval matters most**, use fixed-1024 or page-per-chunk (0.88 doc F1), but accept lower accuracy (57-61%).\n4. **Don't use semantic chunking on academic text.** It fragments too aggressively (43 avg tokens) and collapses on document retrieval (0.42 F1).\n5. **Don't use proposition chunking for general RAG.** 51% accuracy isn't production-ready. It's only viable if you value groundedness over correctness.\n6. **When benchmarking, equalize the context budget.** Fixed top-k comparisons are misleading. Use adaptive k = round(target\\_tokens / avg\\_chunk\\_tokens).\n\n# Why Academic Papers Specifically?\n\nWe deliberately chose to saturate the academic paper region of the embedding space with 50 papers spanning 10+ disciplines. When your knowledge base contains papers that all discuss \"evaluation,\" \"metrics,\" \"models,\" and \"performance,\" the retriever has to make fine-grained distinctions. That's when chunking quality matters most.\n\nIn a mixed corpus of recipes and legal contracts, even bad chunking might work because the embedding distances between domains are large. Academic papers are the *hard case* for chunking, and if a strategy works here, it'll work on easier data too.\n\n# How We Measured This (And How You Can Too)\n\nMy team built [Vecta](https://www.runvecta.com/) specifically to meet the need for precise RAG evaluation software. It generates synthetic benchmark Q&A pairs across multiple semantic granularities, then measures precision, recall, F1, accuracy, and groundedness against your actual retrieval pipeline.\n\nThe benchmarks in this post were generated and evaluated using Vecta's SDK (`pip install vecta`)\n\n# Limitations, Experiment Design, and Further Work\n\nThis experiment was deliberately small-scale: 50 papers, 30 synthetic Q&A pairs, one embedding model, one retriever, one generator. That's by design. We wanted something reproducible that a single engineer could rerun in an afternoon, not a months-long research project. The conclusions should be read with that scope in mind.\n\n**Synthetic benchmarks are not human benchmarks.** Our ground-truth Q&A pairs were generated by Vecta's own pipeline, which means there's an inherent alignment between how questions are formed and how they're evaluated. Human-authored questions would be a stronger test. That said, Vecta's benchmark generation does produce complex multi-hop queries that require synthesizing information across multiple chunks and document locations, so these aren't trivially easy questions that favor any one strategy by default.\n\n**One pipeline, one result.** Everything here runs on `text-embedding-3-small`, ChromaDB, and `gemini-2.5-flash-lite`. Swap any of those components and the rankings could shift. We fully acknowledge this. Running the same experiment across multiple embedding models, vector databases, and generators would be valuable follow-up work, and it's on our roadmap.\n\n**The equal context budget is a deliberate constraint, not a flaw.** Some readers may object that semantic and proposition chunking are \"meant\" to be paired with rerankers, fusion, or hierarchical aggregation. But if a chunking strategy only works when combined with additional infrastructure, that's important to know. Equal context budgets ensure we're comparing chunking quality at roughly equal generation cost. A strategy that requires a reranker to be competitive is a more expensive strategy, and that should factor into the decision.\n\n**Semantic chunking was not intentionally handicapped.** Our semantic chunking produced fragments averaging 43 tokens, which is smaller than most production deployments would target. This was likely due to a poorly tuned cosine similarity threshold (0.7) rather than any deliberate sabotage. But that's actually the point: semantic chunking requires careful threshold tuning, merging heuristics, and often parent-child retrieval to work well. When those aren't perfectly dialed in, it degrades badly. Recursive splitting, by contrast, produced strong results with default parameters. The brittleness of semantic chunking under imperfect tuning is itself a finding.\n\n**What we'd like to do next:**\n\n* Rerun the experiment with human-authored Q&A pairs alongside the synthetic benchmark\n* Test across multiple embedding models (`text-embedding-3-large`, open-source alternatives) and generators (GPT-4o, Claude, Llama)\n* Add reranking and hierarchical retrieval stages, then measure whether the rankings change when every strategy gets access to the same post-retrieval pipeline\n* Expand the corpus beyond academic papers to contracts, documentation, support tickets, and other common RAG domains\n* Test semantic chunking with properly tuned thresholds, chunk merging, and sliding windows to establish its ceiling\n\nIf you run any of these experiments yourself, we'd genuinely like to see the results.\n\nHave a chunking strategy that worked surprisingly well (or badly) for you? We'd love to hear about it. Reach out via DM!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r47duk/we_benchmarked_7_chunking_strategies_most_best/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5a7tkl",
          "author": "Wimiam1",
          "text": "What made you decide that a fixed cosine threshold of 0.7 was the way to go?",
          "score": 9,
          "created_utc": "2026-02-14 03:37:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bczg9",
              "author": "TeamCaspy",
              "text": "Probably benchmarked all thresholds?",
              "score": 1,
              "created_utc": "2026-02-14 09:31:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cjxz0",
                  "author": "Wimiam1",
                  "text": "I really doubt it. There’s no reason semantic chunking should perform so much worse. 47 tokens per chunk on average is a huge red flag to me. These papers are all strictly about a specific topic. That means semantic homogeneity. That means you need a higher threshold to get nicely sized chunks. Furthermore, a good threshold for one document/author might not be good for another. That’s why there are so many adaptive threshold techniques.",
                  "score": 5,
                  "created_utc": "2026-02-14 15:02:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5bh260",
              "author": "Distinct-Target7503",
              "text": "yeah exactly... also those embedders work really bad with fixed similarity thresholds, they are simply not trained for that.",
              "score": 1,
              "created_utc": "2026-02-14 10:11:55",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5cnhk0",
                  "author": "Jords13xx",
                  "text": "For sure, fixed thresholds can really limit the flexibility of the model. It's like trying to fit a square peg in a round hole when the data might not even have a consistent shape!",
                  "score": 1,
                  "created_utc": "2026-02-14 15:21:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5agbj7",
          "author": "One_Milk_7025",
          "text": "These are the most basic ones.. there are way more variant and strategy out there..\n\nFor starter checkout the chunking playground \nChunker.veristamp.in you can tweak and play all these strategy in browser",
          "score": 10,
          "created_utc": "2026-02-14 04:38:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a8di9",
          "author": "pl201",
          "text": "1. You should publish your code to make your findings more credible. Chunking is only one small step in the whole setup and process. Each step will impact the final retrieval. Docs should be cleaned and OCR results should be reviewed since error rate normally is pretty high.\n2. You should release the doc you have used so other people can validate your claims. There is no worse or better chunks strategy without references to doc type, quality, and relationship. A good chunk method to your doc RAG may be very bad to my.",
          "score": 15,
          "created_utc": "2026-02-14 03:41:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b18pk",
          "author": "nithril",
          "text": "The chunk size of the doc structure approach must be limited, it is a non sense to have such a big chunk. It must be similar to what the recursive approach is doing. Combining the two approaches is actually better, or a doc structure at the paragraph level to have a max chunk size of 512",
          "score": 3,
          "created_utc": "2026-02-14 07:37:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a5vye",
          "author": "zmanning",
          "text": "this is an AI slop advertisement ",
          "score": 11,
          "created_utc": "2026-02-14 03:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bg05s",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 5,
              "created_utc": "2026-02-14 10:01:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5moggz",
                  "author": "SkyFeistyLlama8",
                  "text": "Full of practitioners reinventing the wheel over and over again.\n\nThe chunking strategy has to fit your corpus and subject matter. If you're dealing with lots of short documents, then stuff the entire thing into your context. If you have to deal with documents that are hundreds of pages long, then $deity help you because it's still not a solved problem.",
                  "score": 1,
                  "created_utc": "2026-02-16 04:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5b20uw",
          "author": "Educational_Cup9809",
          "text": "With new llm and embedding models i dont even care about chunking that much. Always go for page level  and files without pages go for 7000 tokens. spending time on Graph and metadata techniques gives me far better results. Future is all about agentic anyway",
          "score": 2,
          "created_utc": "2026-02-14 07:44:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5bvwoe",
              "author": "Free-Ferret7135",
              "text": "Very true! They did semantic chunking with avrg of 43 tokens LOL",
              "score": 6,
              "created_utc": "2026-02-14 12:27:55",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5c6i2l",
              "author": "nithril",
              "text": "7000 tokens, the best way to produce a vector that is just a semantic soup. Agentic needs precision in the retrieval.",
              "score": 2,
              "created_utc": "2026-02-14 13:42:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5hvmhc",
              "author": "Marengol",
              "text": "Can you elaborate, please? (on graph and metadata techniques)",
              "score": 1,
              "created_utc": "2026-02-15 12:15:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5brxen",
          "author": "48K",
          "text": "Hopefully simple question: for every query all these approaches produce multiple scores. How do you combine them into a single score?",
          "score": 1,
          "created_utc": "2026-02-14 11:54:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5cutwr",
          "author": "my_byte",
          "text": "Test 500 tokens, no overlap with voyage context embeddings and see how they compare to your benchmark winner",
          "score": 1,
          "created_utc": "2026-02-14 15:59:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5eg35g",
          "author": "king_vis",
          "text": "LLM chunking is all you need",
          "score": 1,
          "created_utc": "2026-02-14 20:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5f5kr3",
          "author": "guesdo",
          "text": "In my personal experience, doc structure is the best if you delegate RAG to the LLM itself. Instead of you handling retrieval, leave it to the LLM via MCP with a couple of tools including semantic and reverse index search. Reason being, the LLM can explore faster and further and iterate on results while getting exact sections on searches optimizing context size.",
          "score": 1,
          "created_utc": "2026-02-14 23:15:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60urrj",
          "author": "whatsoinc",
          "text": "Do you have a managed RAG product? I would like to use it.\n\n",
          "score": 1,
          "created_utc": "2026-02-18 09:08:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5bnxel",
          "author": "jannemansonh",
          "text": "the chunking optimization rabbit hole is real... this is why we moved doc workflows to needle app since it handles the rag layer (chunking, embeddings, retrieval). way easier than tuning chunk sizes and testing strategies every time requirements change",
          "score": 0,
          "created_utc": "2026-02-14 11:18:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5e69n",
      "title": "How I Cut RAG Agent Hallucinations in Production for 10,000+ pdfs.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5e69n/how_i_cut_rag_agent_hallucinations_in_production/",
      "author": "AccidentHefty2595",
      "created_utc": "2026-02-15 13:06:07",
      "score": 64,
      "num_comments": 11,
      "upvote_ratio": 0.95,
      "text": "The Problem:  \nAsked about \"max heating capacity of refrigerator\". Got a mixed answer combining V6, V8, and other refrigerator variants.\n\nThe Fix - 4 Simple Approaches:\n\n1. Force Clarification Before Searching Ambiguous query: Agent stops and asks. \"Refrigerator\" has 3 variants V4, V6 and V8 which one do you mean?\n2. No guessing. No assumptions.\n3. Query Decomposition: Break every request into required pieces (eg. sperate queries for heating and cooling capacity). Do parallel execution for each query and combine the unique results of dense and sparse.\n4. Filtering (most effective): Apply filters once the user confirms their product, filter documents BEFORE retrieval. Think \"search only in Building A, Floor 3\" instead of \"search the entire campus.\" Let the agent apply filters dynamically.\n5. Context Pruning: Long conversations hit token limits fast. Prune old search results. Drop the heavy intermediate retrieval data.\n\nThe Result: The agent now asks \"Which one?\" instead of making assumptions.\n\nCode snippet for the tool calling,\n\n    class SearchInput(BaseModel):\n        query: str = Field(description=\"The specific query to search for in the knowledge base., For complex requests, break this down into specific sub-queries.\")\n        target_directory: Optional[str] = Field(\n            description=(\n                \"Crucial for filtering. Apply precise folder paths based on the user's confirmed product category \"\n                \"(e.g., 'v6_idu/ac', 'v8_idu/wall_mounted', 'vrf_odu/side_discharge'). \"\n                \"Only leave empty if the user asks a broad, cross-category comparison question.\"\n            )\n        )\n    \n    @tool(args_schema=SearchInput, response_format=\"content_and_artifact\")\n    async def knowledge_base_search(query: str, target_directory: Optional[str] = None) -> str:\n        \"\"\"\n        Executes a search within the technical documentation. \n        \n        Usage Guidelines:\n        1. **Precision:** Always apply the `target_directory` derived to exclude irrelevant product lines (e.g., filtering out 'V6' when the user asks for 'V8').\n        2. **Iteration:** Call this tool multiple times if the initial search results are missing required data points.\n        3. **Scope:** Returns raw documentation chunks relevant to the query and path.\n        \"\"\"\n\nI structured the product data in folders and sub-folders that represent a hierarchy. Something like this, for other type of data this can be done based on financial years /  companies / authors / product types.\n\n    VRF\n    ├── V6 IDU\n        ├── AC\n    ├── V8 IDU\n        ├── DC\n        ├── AC\n    └── VRF ODU\n        ├── AC\n        ├── V6R\n        ├── V8\n        │   ├── V8 Master\n        │   ├── V8 Pro\n        └── VC pro\n\nThis structure can be directly given in the system prompt, or if the structure is big then create a new \\`get\\_folder\\_structure\\` tool that uses fuzzy logic to get relevant paths.\n\nNow if we ask, What is the max cooling capacity of V8 IDU?  \nLLM will first ask whether you mean  V8 IDU AC or V8 IDU DC.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5e69n/how_i_cut_rag_agent_hallucinations_in_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5iiqot",
          "author": "DashboardNight",
          "text": "I agree with query decomposition. Pretty sure this is what Microsoft has been starting to implement in their Search solutions.",
          "score": 3,
          "created_utc": "2026-02-15 14:46:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5j5ax3",
          "author": "TanLine_Knight",
          "text": "How do you enforce query clarification? Like in the fridge example, how would the agent know to ask for specific fridge type?",
          "score": 2,
          "created_utc": "2026-02-15 16:39:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mhkn3",
              "author": "TenshiS",
              "text": "OPs take only works for a specific type of usecase where you already have a clear mapping of your context or you have enough information on the user himself (perhaps the type of fridge they ordered is known).\n\nAlternatively it requires a traditional retrieval step followed by a clarification step, basically doubling tokens cost but reducing some ambiguity.",
              "score": 1,
              "created_utc": "2026-02-16 03:40:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5jcpqp",
              "author": "AccidentHefty2595",
              "text": "Good question, I updated the post, please check it out.",
              "score": 0,
              "created_utc": "2026-02-15 17:15:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5n3eem",
          "author": "Comfortable_Onion255",
          "text": "Why not using RLM? I tot that better?",
          "score": 1,
          "created_utc": "2026-02-16 06:26:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5n8e66",
              "author": "AccidentHefty2595",
              "text": "RLM is new for me, i am learning about it",
              "score": 1,
              "created_utc": "2026-02-16 07:10:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5no5re",
          "author": "arul-ql",
          "text": "Guys, thoughts on Recursive Language Models!? It's gaining traction and recently saw Google covering this project on their blog. \n\nHas anyone tried it?",
          "score": 1,
          "created_utc": "2026-02-16 09:39:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5prh2f",
          "author": "ved3py",
          "text": "What's RLM?",
          "score": 1,
          "created_utc": "2026-02-16 17:20:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iic1z",
          "author": "HauntingIron5623",
          "text": "how many KB instances are you using? I wonder how sparse the source data looks or if you maybe have only technical documentations",
          "score": 0,
          "created_utc": "2026-02-15 14:44:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jexzv",
              "author": "AccidentHefty2595",
              "text": "I had only pdfs, technical documents with lots of images and complex tables. So one tool was able to handle retrieval. For excel or database query more tools will be required. ",
              "score": 1,
              "created_utc": "2026-02-15 17:26:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r3mmxy",
      "title": "Semantic chunking + metadata filtering actually fixes RAG hallucinations",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r3mmxy/semantic_chunking_metadata_filtering_actually/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-13 11:21:35",
      "score": 58,
      "num_comments": 24,
      "upvote_ratio": 0.91,
      "text": "I noticed that most people don't realize their chunking and retrieval strategy might be causing their RAG hallucinations.\n\nFixed-size chunking (split every 512 tokens regardless of content) fragments semantic units. Single explanation gets split across two chunks. Tables lose their structure. Headers separate from data. The chunks going into your vector DB are semantically incoherent.\n\nI've been testing semantic boundary detection instead where I use a model to find where topics actually change. Generate embeddings for each sentence, calculate similarity between consecutive ones, split when it sees sharp drops. The results are variable chunks but each represents a complete clear idea.\n\nThis alone gets 2-3 percentage points better recall but the bigger win for me was adding metadata. I pass each chunk through an LLM to extract time periods, doc types, entities, whatever structured info matters and store that alongside the embedding.\n\nThis metadata filters narrow the search space first, then vector similarity runs on that subset. Searching 47 relevant chunks instead of 20,000 random ones.\n\nFor complex documents with inherent structure this seems obviously better than fixed chunking. Anyway thought I should share. :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r3mmxy/semantic_chunking_metadata_filtering_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o55jh9b",
          "author": "Ok_Signature_6030",
          "text": "the metadata filtering part is honestly where i've seen the biggest wins too. went through a similar journey — started with fixed 512 token chunks, moved to semantic splitting, and finally added metadata.\n\n  \none thing worth mentioning though: running every chunk through an LLM for metadata extraction gets expensive fast. we had around 40k chunks and the extraction step alone was costing more than the actual inference. what worked better for us was a hybrid approach — extract what you can from document structure (headers, file paths, timestamps in the text) with regex/rules first, then only use the LLM for ambiguous stuff like topic classification.\n\n  \nalso for the similarity drop detection, we found that just cosine similarity between consecutive sentences was too noisy. adding a sliding window average and looking for drops relative to the local mean rather than an absolute threshold made it way more stable. otherwise you get weird splits in the middle of lists or code blocks where embedding similarity naturally dips.\n\n  \nthe 47 vs 20,000 chunks comparison is real though. metadata pre-filtering basically turns your vector search from \"find needles in a haystack\" into \"find needles in a small pile of needles.\"",
          "score": 9,
          "created_utc": "2026-02-13 12:33:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57lfqe",
              "author": "welcome-overlords",
              "text": "Thanks. The post and your comment gives me hope that it was the right choice to spend all these weeks not making meaningful progress (i.e. tickets done), that i have arrived at similar thoughts than you.\n\nQuestion:\nHow do you concretely use the metadata filtering? What kind of instructions your RAG agent gets to understand how to use them correctly?",
              "score": 1,
              "created_utc": "2026-02-13 18:49:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5abi2p",
                  "author": "Ok_Signature_6030",
                  "text": "concretely we do two things: first at ingest time, each chunk gets tagged with structured metadata (doc\\_type, date\\_range, entities, section\\_header). we use a small LLM call per chunk for this - costs pennies and runs async.\n\nthen at query time, the RAG agent has a system prompt that says \"before searching, extract any filters from the user query: dates, document types, named entities. pass these as metadata filters to the retrieval function.\" the agent calls the retrieval tool with both the semantic query AND the metadata filters.\n\nso if someone asks \"what was the Q3 revenue guidance?\" the agent extracts date\\_range=Q3 and doc\\_type=earnings, passes those as filters, and vector search only runs on chunks matching those constraints. way fewer hallucinations because the search space is already relevant.",
                  "score": 4,
                  "created_utc": "2026-02-14 04:03:15",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o57lnud",
              "author": "Particular-Gur-1339",
              "text": "How do you filter based on metadata?",
              "score": 1,
              "created_utc": "2026-02-13 18:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5ac9bc",
                  "author": "Ok_Signature_6030",
                  "text": "depends on the vector db but the general pattern is: at ingest time, run each chunk through an LLM to extract structured fields (doc\\_type, date\\_range, entity names, whatever matters for your domain). store those as filterable metadata alongside the embedding.\n\n  \nthen at query time, parse the user's question for those same fields before you even hit the vector search. something like \"what was revenue in Q3 2024\" becomes a metadata filter {date\\_range: 'Q3 2024', doc\\_type: 'financial'} + the semantic query.\n\n  \nif you're using pinecone or weaviate they have native metadata filtering. with pgvector you'd just add WHERE clauses. qdrant has payload filters. the key is the filtering happens BEFORE the ANN search so you're only doing similarity over the pre-filtered subset.",
                  "score": 3,
                  "created_utc": "2026-02-14 04:08:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55a9op",
          "author": "Independent-Cost-971",
          "text": "Wrote up a more detailed explanation if anyone's interested: [https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/](https://kudra.ai/metadata-enriched-retrieval-the-next-evolution-of-rag/)\n\nGoes into the different semantic chunking approaches (embedding similarity detection, LLM-driven structural analysis, proposition extraction) and the full metadata enrichment pipeline. Probably more detail than necessary but figured it might help someone else debugging the same issues.",
          "score": 9,
          "created_utc": "2026-02-13 11:24:34",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o55ddn0",
          "author": "One_Milk_7025",
          "text": "Checkout the chunk visualizer for detailed metadata extraction \nChunker.veristamp.in\nChunking is the heart of rag without proper chunking it's just waste of money",
          "score": 2,
          "created_utc": "2026-02-13 11:49:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55o7y4",
          "author": "_nku",
          "text": "We're running a RAG on a very structured content body where the authors writing in markdown natively and the style is very structured into chapters / subchapters etc logically.   We've leveraged that day one in our RAG, the chunking was paragraph / section based from the beginning although we had to write a custom chunker based on markdown ASTs.   It's keeping the top level page title and context of the chunk and it also keeps the subtitle / intro paragraph of the whole page.     \nIt worked very well although we have to live with larger than typical chunk sizes.  But it's a luxury to be able to work on such as structured content base vs. word documents with all headings being just bold print.  Overall a niche case but if your app has such a high structure content foundation it's a waste to not leverage it. ",
          "score": 2,
          "created_utc": "2026-02-13 13:04:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55mb83",
          "author": "2BucChuck",
          "text": "Tried entity based meta data ? Just curious what smaller models people might be using for cheap enrichment successfully",
          "score": 1,
          "created_utc": "2026-02-13 12:52:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56dw68",
          "author": "Marzou2",
          "text": "?",
          "score": 1,
          "created_utc": "2026-02-13 15:20:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56r8cp",
          "author": "tzt1324",
          "text": "Totally agree. But this setup is a lot more expensive. Now imagine running all epstein files through your pipeline. 100 GB or more. Dozens of thousands of documents",
          "score": 1,
          "created_utc": "2026-02-13 16:24:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56wpyl",
          "author": "Alternative_Nose_874",
          "text": "We saw same thing in our RAG projects: fixed-size chunking break meaning and you get weird retrieval. When we moved to semantic splits plus metadata filters (time/entity/doc type), hallucinations dropped a lot because search is on smaller, more relevant set. Curious what you use to extract metadata, do you do it offline in pipeline or at ingest time?",
          "score": 1,
          "created_utc": "2026-02-13 16:50:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5723is",
          "author": "Delicious-One-5129",
          "text": "This is a great point. A lot of “RAG hallucinations” are really retrieval failures caused by bad chunk boundaries.\n\nSemantic chunking + metadata pre filtering makes the retriever do less guessing and more narrowing. Searching 40 relevant chunks instead of 20k noisy ones is a huge difference in signal quality.",
          "score": 1,
          "created_utc": "2026-02-13 17:16:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o57lu1u",
              "author": "Particular-Gur-1339",
              "text": "How do you filter based on metadata?",
              "score": 1,
              "created_utc": "2026-02-13 18:51:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57lowu",
          "author": "Particular-Gur-1339",
          "text": "How do you filter based on metadata?",
          "score": 1,
          "created_utc": "2026-02-13 18:50:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a8lpd",
          "author": "eurydice1727",
          "text": "Yupp.",
          "score": 1,
          "created_utc": "2026-02-14 03:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5az0sa",
              "author": "Aggressive_Bed2609",
              "text": "For sure! It's wild how much fixed chunking can mess with the coherence of the data. Your approach with semantic boundary detection sounds promising—definitely a game changer for retrieval accuracy.",
              "score": 1,
              "created_utc": "2026-02-14 07:16:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5apnug",
          "author": "Final_Special_7457",
          "text": "First  \" What did u use for your method of chunking ? A code or a model that handle the chunking boundaries ?\n2nd : it isn't too expensive to pass each chunk through a llm ?",
          "score": 1,
          "created_utc": "2026-02-14 05:52:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5e1col",
          "author": "Jazzcornersmut",
          "text": "Are you looking for a CTO role? \nI need what you do!!",
          "score": 1,
          "created_utc": "2026-02-14 19:33:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6bwvp",
      "title": "RAG is dead, they said. Agents will take over, they said.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6bwvp/rag_is_dead_they_said_agents_will_take_over_they/",
      "author": "nkmraoAI",
      "created_utc": "2026-02-16 15:02:25",
      "score": 56,
      "num_comments": 27,
      "upvote_ratio": 0.81,
      "text": "RAG is not dead.  \n  \nThe context window ceiling is real. Context rot is real.  \nClaude, GPT-4.1, Gemini — all sitting at 1M tokens. Impressive on a slide. Less impressive in production. Research shows that performance degrades as context grows and this effect becomes significant beyond 100k tokens \\[[Context Rot: How Increasing Input Tokens Impacts LLM Performance, Hong, Troynikov & Huber, Chroma, 2025](https://research.trychroma.com/context-rot)\\]  \nI've felt this firsthand. Context rot in my own applications at just 40–50k tokens. The ceiling isn't 1M. It's far lower.  \nMeanwhile, enterprise knowledge bases contain billions of tokens. The math doesn't work. You will never stuff your way to an answer.  \n  \nAgents aren't a silver bullet either.  \nProgressive disclosure lets agents traverse massive codebases and document sets, but every hop is a lossy compression. Information degrades. Reasoning quality drops. The growing wave of agentic workflows is itself a signal: frontier model improvements are slowing. The delta between model generations is shrinking. Future gains will come from agentic engineering around models, not from the models themselves.  \nSo if today's LLMs are roughly as good as it gets, how do you leverage massive knowledge bases? RAG. Still.  \nNot the naive RAG of 2023. Smarter retrieval. Better chunking. Hybrid search. Context engineering. Agentic. But the core idea of RAG is more important now than ever. Retrieve what's relevant, discard what isn't.  \n  \nRAG isn't a workaround. It's a non-trivial part of the architecture.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r6bwvp/rag_is_dead_they_said_agents_will_take_over_they/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5p5pej",
          "author": "penguinzb1",
          "text": "the thing about agents that makes this whole debate interesting is that they're basically just orchestrating rag calls anyway. progressive disclosure is rag with more steps. you retrieve, compress, retrieve again based on what you found. it's not some fundamentally different architecture.\n\ni've been building evaluation systems for agents and the pattern you describe about lossy compression is real. each hop degrades the context, but it also lets you cover way more ground than stuffing everything into a single prompt. the question isn't rag vs agents, it's how much information loss you can tolerate for the task. some use cases need exact retrieval, others just need to get close enough.\n\nthe part about model improvements slowing down hits different when you're trying to test agent behavior. we're stuck optimizing retrieval strategies and eval frameworks instead of just waiting for the next model to solve everything. smarter chunking, better reranking, hybrid search like you said. it's all just rag engineering at the end of the day.",
          "score": 12,
          "created_utc": "2026-02-16 15:39:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5pbmdj",
              "author": "nkmraoAI",
              "text": "'Progressive disclosure is rag with more steps'. That's an epiphany now. ",
              "score": 1,
              "created_utc": "2026-02-16 16:07:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p1rj3",
          "author": "Technical-History104",
          "text": "It’s not either/or… RAG becomes a part of agentic AI also.",
          "score": 15,
          "created_utc": "2026-02-16 15:20:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p6otk",
              "author": "Hot-Meat-11",
              "text": "While my experience is thin yet, my best results have been building tools that allow the agent to access the RAG.",
              "score": 1,
              "created_utc": "2026-02-16 15:44:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o5panv7",
              "author": "nkmraoAI",
              "text": "Not all agentic applications have RAG. None of the agent harnesses come with a built-in vector database or RAG tool. If you give them a bunch of documents, they don't embed them. They simply read them, partially or fully. ",
              "score": -3,
              "created_utc": "2026-02-16 16:02:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5r47b4",
                  "author": "xFloaty",
                  "text": "If an agent does grep, that’s RAG. No one ever said that RAG implies that embeddings are used for retrieval.",
                  "score": 6,
                  "created_utc": "2026-02-16 21:11:49",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5pc8lg",
                  "author": "Brighthero",
                  "text": "Not true at all. Cursor has proper semantic search built in: [https://cursor.com/docs/context/semantic-search](https://cursor.com/docs/context/semantic-search)",
                  "score": 3,
                  "created_utc": "2026-02-16 16:10:05",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5ph6pk",
                  "author": "Technical-History104",
                  "text": "I’m aware.  But my point is that this is not mutually exclusive.  RAG is still very much applicable for certain design patterns and some agentic use cases will require RAG.",
                  "score": 2,
                  "created_utc": "2026-02-16 16:32:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ozzor",
          "author": "Astroa7m",
          "text": "I agree with 1M context window being valid on papers only. Experimented a use case and models get overwhelmed. My sweet spot was 8% of that million.",
          "score": 5,
          "created_utc": "2026-02-16 15:11:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p0egm",
              "author": "nkmraoAI",
              "text": "In my case, they got overwhelmed at 40-50k tokens of instructions. Failed to follow many of the instructions. ",
              "score": 4,
              "created_utc": "2026-02-16 15:13:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5p92l4",
                  "author": "flonnil",
                  "text": "how the hell do you get to 50k tokens of just *instructions*? thats aprox. \"the lion, the which and the wardrobe\" by c.s. lewis in just *instructions*. one could teach it to build a rocket ship from scratch, fly it to jupiter, recite a poem and fly back with less instructions.",
                  "score": 4,
                  "created_utc": "2026-02-16 15:55:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5ruyav",
          "author": "Much-Researcher6135",
          "text": "Sorry, I downvote LLM-generated slop posts",
          "score": 4,
          "created_utc": "2026-02-16 23:28:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5t4005",
              "author": "nkmraoAI",
              "text": "Free world. It's not like the LLM gets its own ideas. Its just a specialized employee. It is simply able to articulate it better. Hope you downvote copywritten or ghostwritten content as well.",
              "score": 2,
              "created_utc": "2026-02-17 03:58:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qd5gn",
          "author": "Current-Ticket4214",
          "text": "Today’s models are not as good as it gets. If you believe that then you haven’t touched Opus 4.6.",
          "score": 2,
          "created_utc": "2026-02-16 19:00:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5vudj3",
              "author": "nkmraoAI",
              "text": "How do you even access Claude models outside of direct API these days? I believe you mean 'used Opus 4.6 within Claude Code or some other harness'. Its not the model that's the major improvement, its the harness. If Opus 4.6 was so much better than Opus 4.5, it would have been called Opus 5. ",
              "score": 1,
              "created_utc": "2026-02-17 15:56:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5vx70r",
                  "author": "Current-Ticket4214",
                  "text": "Opus 4.6 shows a significant improvement in both Claude Code AND Windsurf. If the harness was the only differentiator then you would see a massive improvement in Claude Code and none in Windsurf.",
                  "score": 1,
                  "created_utc": "2026-02-17 16:10:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5prlx6",
          "author": "Informal-Resolve-831",
          "text": "So… RAG agent. What’s the problem with it?",
          "score": 1,
          "created_utc": "2026-02-16 17:20:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5riuo3",
          "author": "gopietz",
          "text": "Sometimes it feels like people that post about RAG being either dead or alive are the very same people. People who feel the need to create a debate where there is none.",
          "score": 1,
          "created_utc": "2026-02-16 22:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5snlyh",
          "author": "jeffrey-0711",
          "text": "Retrieval in agent system is essential. RAG becomes agentic RAG. So, I think it is not RAG or Agent, but we need both of them.",
          "score": 1,
          "created_utc": "2026-02-17 02:16:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5stho5",
          "author": "johnny_5667",
          "text": "didnt even attempt to read the whole post because it reads like LLM slop but RAG and Agents are both solutions to different problems at different levels of the software scale",
          "score": 1,
          "created_utc": "2026-02-17 02:51:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5uabdn",
          "author": "BeatTheMarket30",
          "text": "For a more realistic context window size look at max output token limit.",
          "score": 1,
          "created_utc": "2026-02-17 09:57:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5upuwm",
          "author": "UseMoreBandwith",
          "text": "agents are just doing RAG. But in a even more in-efficient way.",
          "score": 1,
          "created_utc": "2026-02-17 12:10:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v2ea9",
          "author": "bilbo_was_right",
          "text": "Sorry but it’s not context rot if you’re hitting it at 40k tokens",
          "score": 1,
          "created_utc": "2026-02-17 13:30:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qojhw",
          "author": "gidea",
          "text": "This looks promising, has anyone tried something like this for RAG? https://compresr.ai",
          "score": 0,
          "created_utc": "2026-02-16 19:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pf9yp",
          "author": "InnerOuterTrueSelf",
          "text": "Ok boomer.",
          "score": -3,
          "created_utc": "2026-02-16 16:23:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r80vsg",
      "title": "Building a RAG for my company… (help me figure it out)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r80vsg/building_a_rag_for_my_company_help_me_figure_it/",
      "author": "Current_Complex7390",
      "created_utc": "2026-02-18 11:48:33",
      "score": 36,
      "num_comments": 26,
      "upvote_ratio": 0.93,
      "text": "Hi All,\n\nI used always to use notebooklm for my work. \n\nbut then it came through my mind why i don’t build one for my own specific need.\n\nSo i started building using Claude, but after 2 weeks of trying it finally worked. It embeds, chunck the pdf files and i can chat with them.\n\nBut the answers are shit. Not sure why….\n\nThe way i built it is using a openotebooklm open source and then i built on top if it, editing alot of stuff.\n\nI use google embedding 004, gemini 2.0 for chatting, i also use Surrelbd.\n\nI am not sure what is the best structure to build one? Should i Start from scratch with a different approach? All i want is a rag model with 4 files (Legal guidance) as its knowledge base and then j upload a project files and then the chat should correlate the project files with the existing knowledge base and give precise answers like Notebooklm.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r80vsg/building_a_rag_for_my_company_help_me_figure_it/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o61uq86",
          "author": "ChapterEquivalent188",
          "text": "garbage in , garbage out. thats it. have a start with this one https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit\n\nyouĺl fail as long as you dont focus on you ingestion... have a look around in my repos im sure you will find a solution beside the mentioned kit  ;) \n\nits pretty dirty down there where you heading ..... have fun",
          "score": 14,
          "created_utc": "2026-02-18 13:40:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62w2f0",
              "author": "Current_Complex7390",
              "text": "Should i start from scratch or feed this to my current claude project?\n\nI hope this question doesn’t ruin the engagement with me or the post here, but as a disclaimer i am not a technical person, i just have an idea and i want to implement it",
              "score": 2,
              "created_utc": "2026-02-18 16:40:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6361w5",
                  "author": "ChapterEquivalent188",
                  "text": "no worries ;) people stil want to sell a simple pdf warpper/talk2pdf as a produktion ready rag sys ;) \nas long as your company trusts in you to build there RAG im happy to help---- so ,the basics: legal docs have some specialties but plenty of parser meanwhile depending on language (can help with that).  if its only 4 docs i would suggest to give it a try for some learning ( a simple ingest router is included) and have the kit ingest. its easy  and quick done if you spin up docker .... safe some money with you tests and use the local ollama in the repo takes a bit more time sometimes depending on your hardware ----- get your docs prepared and if it has tabels or other stuff in des docs you might use docling or so. btw if you want to connect a agent to your rag try the ClawRAG repo instead including a mcp server for ypur agent or whatever ;) ... so far ...and always have fun",
                  "score": 2,
                  "created_utc": "2026-02-18 17:25:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o635mvj",
              "author": "Current_Complex7390",
              "text": "Also check DMs please",
              "score": 1,
              "created_utc": "2026-02-18 17:23:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o633fwl",
          "author": "playeronex",
          "text": "yeah so the issue is probably your chunking strategy or embedding relevance. with legal docs especially, you're likely splitting mid-sentence or losing context. try increasing chunk size to 512-1024 tokens with 20-30% overlap, and make sure you're using metadata tagging (document type, section headers, dates) so the retriever knows what it's pulling. also double-check your retrieval — if you're just doing basic similarity search on top-4 results, you're probably getting noise. throw in reranking (like Cohere's rerank model) between retrieval and generation, and add a prompt that tells Gemini to cite which document it's using when answering. that alone fixes most RAG quality issues.",
          "score": 4,
          "created_utc": "2026-02-18 17:13:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o634fk6",
              "author": "Current_Complex7390",
              "text": "Can you check dms",
              "score": 1,
              "created_utc": "2026-02-18 17:18:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o61emim",
          "author": "joey2scoops",
          "text": "For that use case, why not stay with notebooks? Give it a custom instruction and you should get decent results. \n\nI have been building my own using this one as a base.\n\nhttps://youtu.be/AUQJ9eeP-Ls?si=NcvuQPwdmpRfZKI3. \n\nhttps://github.com/techwithtim/ProductionGradeRAGPythonApp\n\nI've made a LOT of changes and customisations but the base system just works.",
          "score": 4,
          "created_utc": "2026-02-18 11:59:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62mbtb",
              "author": "ChapterEquivalent188",
              "text": "sorry but whats this ? \"production grade \" ? are we all idiots and we allcan safe our lifetime by just building a (very bad and simpüle) wrapper call it \"production grade \" and the world is fine ? what the heck",
              "score": 3,
              "created_utc": "2026-02-18 15:56:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o61g8kz",
              "author": "greeny01",
              "text": "github link not working :)",
              "score": 1,
              "created_utc": "2026-02-18 12:11:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o61ghz1",
                  "author": "joey2scoops",
                  "text": "Fixed.",
                  "score": 1,
                  "created_utc": "2026-02-18 12:13:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o62wve8",
              "author": "Current_Complex7390",
              "text": "Because, i need to make one for my use only, for my company, with my company frontend logo. Also i want it to be based on specific legal documents/ standards. \nI want to customize different interactions point within the front end and customize how it looks and how it interacts\n\nNow for the link above (still need to watch it) \nCan i do that?",
              "score": 1,
              "created_utc": "2026-02-18 16:44:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64ea1t",
                  "author": "remoteinspace",
                  "text": "Try to plugin something like papr.ai to your project. Takes care of the rag and memory infrastructure for you. DM me and I can help you set it up",
                  "score": 1,
                  "created_utc": "2026-02-18 20:46:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o61fxle",
          "author": "yafitzdev",
          "text": "Most RAGs are behaving incorrectly because of missing retrieval intelligence and missing epistemic honesty constraints. They hallucinate confidently. I would suggest checking out my Open Source RAG for reference. The retrieval intelligence is the key piece: \n\ngithub.com/yafitzdev/fitz-ai",
          "score": 3,
          "created_utc": "2026-02-18 12:09:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61hqd0",
          "author": "HominidSimilies",
          "text": "Try anythingllm",
          "score": 2,
          "created_utc": "2026-02-18 12:21:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o62yg83",
              "author": "Current_Complex7390",
              "text": "I will try it by its own first it works i will just implement it.\n\nHowever i wanted to understand how can i implement the notebooklm precision answer. It is almost perfect for my kind of work i do ( which is asking legal questions based on guidance docs related to architectural projects).",
              "score": 1,
              "created_utc": "2026-02-18 16:51:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o63clxt",
          "author": "pl201",
          "text": "Honestly, every company needs a RAG system these days, but most failed on their first build, so don’t get too discouraged — that first failure is also the first step that leads success.\n\nI suggest you take a step back and look closely at your ingestion strategy based on how your documents are structured, because that’s where a lot of early builds go off track. You’re in a great spot to dig into why modern RAG isn’t just about coding — it’s really about the architecture decisions you make up front.\n\nI’d also recommend reading a couple of good RAG-focused books to level up on patterns and pitfalls, and there’s one in particular that may be more helpful for your case. It is from a consultant who’s shipped a lot of real-world RAG projects. Recently published and 14 chapters of real-world fixes from tons of failed projects since early ChatGPT days. PM me for the title.",
          "score": 2,
          "created_utc": "2026-02-18 17:55:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61yp1r",
          "author": "sobrietyincorporated",
          "text": "Why pdf? Just use markdown it naturally produces.",
          "score": 1,
          "created_utc": "2026-02-18 14:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61zvkn",
          "author": "rasbid420",
          "text": "I ran into the exact same issue and realized that doing chunking > embedding > retrieval > context building was too difficult and could never do it right. However I found that chatGPT does a great job if at this if you just feed it the right raw .txt context. So I built for myself this small service where I do quality document extraction and then I upload the resulting .txt file to a chatGPT vector file and then just use a cheap model to take advantage of their RAG technique which is way more advanced than anyone could ever achieve. \n\nPlease do not abuse it and share some feedback if you find it useful! [https://hyperstract.com](https://hyperstract.com)",
          "score": 1,
          "created_utc": "2026-02-18 14:07:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63df85",
              "author": "Due_Midnight9580",
              "text": "How are you extracting quality documents?\n🙂",
              "score": 1,
              "created_utc": "2026-02-18 17:58:30",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o63du89",
                  "author": "rasbid420",
                  "text": "i have 800 GPUs each running a small qwen3 8B visual model and each of them is in charge of visually describing a page. then at the end when they're all done, I concatenate the results and provide a single unitary .txt file containing all the descriptions. i have a detailed explanation of my system posted in localllama here\n\n[https://www.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3\\_repurposing\\_800\\_rx\\_580s\\_converted\\_to\\_ai/](https://www.reddit.com/r/LocalLLaMA/comments/1r87ou8/update3_repurposing_800_rx_580s_converted_to_ai/)",
                  "score": 3,
                  "created_utc": "2026-02-18 18:00:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o62359r",
          "author": "jingamayne",
          "text": "I just built one using gemini file search for my workplace. Works really well.",
          "score": 1,
          "created_utc": "2026-02-18 14:24:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63wjvm",
          "author": "TechnicalGeologist99",
          "text": "What you've made is called semantic or dense vector search it's one of about 50 possible bells and whistles you could add to your RAG pipeline. \n\nOurs has denser, sparse, rerank, tag extraction, sub query generation, intent extraction, documents are hierarchical first class objects in our domain, chunks that are retrieved are combined with heirarchical information to produce a fully contextual citation, the list goes on and on. But each single thing addresses a specific problem that stop us realising a higher recall at query time.\n\nBut note that this is an application you are building. The same design rules apply. Use YAGNI, justify things before you implement them. Find out how to even justify things in the first place. \n\nUse Claude to learn the patterns that occur in RAG systems. \n\nHere's an easy win, add sparse search, RRF, and reranking and see how that changes responses",
          "score": 1,
          "created_utc": "2026-02-18 19:23:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65y7ta",
          "author": "lfnovo",
          "text": "Hey  u/op, Open-Notebook creator here. Per my experience, RAG issues are, in most cases, related to how you structure your docs/chunks/embeddings. Just doing simple character based chunking is not enough for many projects. It really does depend on what you are trying to do: \n\nExample: if you embed all Epstein files and ask for the names of everybody in there, you won't get any good responses. \"What are the names of people here\", when converted to embeddings will not make any useful matches.\n\nI had a project where the best option was to 'summarize' not chunk. For investigative journalism.\n\nAnother project, where only entity extraction/graph RAG would do it.  (like in the Epstein example).\n\nin some cases, you get lucky which agentic RAG and offering some tools, not just vector search. \n\nIf you share more about what you are building, I can give you some tips. Specially: what type of content are you ingesting and what type of question are people most likely to ask. \n\nCheers",
          "score": 1,
          "created_utc": "2026-02-19 01:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o62mx2m",
          "author": "ChapterEquivalent188",
          "text": "unbelievable how much bs here is in the comments, just wait till somebody says \"use a bigger model and you are fine\" ...... \"why pdf, use markdown\" \n\nby today im confident LLM will destroy mankind !",
          "score": 0,
          "created_utc": "2026-02-18 15:59:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o63fmk1",
          "author": "primoco",
          "text": "Hey, I’m working on a similar setup for legal and enterprise docs. I started with a \"Community\" approach too, and I faced exactly the same frustration: great LLM (Gemini 2.0), great embeddings (Google 004), but \"shit\" answers.\n\nThe problem isn't your stack; Gemini and SurrealDB are solid. The issue is usually how the information is \"orchestrated\" before reaching the LLM. In my experience, to make a RAG work with legal and project files, you have to move away from the standard \"out-of-the-box\" approach.\n\nHere are the 3 main issues I had to solve to get precise answers:\n\nThe Chunking Trap: Standard fixed-size chunking (like splitting every 500-1000 tokens) is a disaster for legal docs. If a clause or an \"if/then\" condition is split in half, the LLM loses the logic. Are you using a simple splitter or a recursive one?\n\nMetadata vs. Pure Vector: For legal stuff, simple semantic search is too \"fuzzy.\" I found that I had to extract metadata (dates, entities, specific article numbers) first and use them to \"anchor\" the search. Without structured metadata, the LLM starts hallucinating connections that aren't there.\n\nContext Injection: Legal files should be treated as the \"Ground Truth.\" I had to tweak my prompt and retrieval to make sure the legal guidance acts as a hard constraint for the project files.\n\nTo give you a hand, what are your current parameters?\n\nWhat Chunk Size and Overlap are you using? (This is usually the #1 culprit)\n\nHow many chunks (Top-K) are you feeding to Gemini for each query?\n\nAre you using any kind of Reranker or just raw vector search?\n\nDon't scrap it yet. Usually, a \"shit\" RAG is just a RAG that needs better data orchestration, not a different LLM.",
          "score": 0,
          "created_utc": "2026-02-18 18:08:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2st4j",
      "title": "Vectorless RAG (Why Document Trees Beat Embeddings for Structured Documents)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2st4j/vectorless_rag_why_document_trees_beat_embeddings/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-12 13:01:07",
      "score": 34,
      "num_comments": 20,
      "upvote_ratio": 0.89,
      "text": "I've been messing around with vectorless RAG lately and honestly it's kind of ridiculous how much we're leaving on the table by not using it properly.\n\nThe basic idea makes sense on paper. Just build document trees instead of chunking everything into embedded fragments, let LLMs navigate structure instead of guessing at similarity. But the way people actually implement this is usually pretty half baked. They'll extract some headers, maybe preserve a table or two, call it \"structured\" and wonder why it's not dramatically better than their old vector setup.\n\nThink about how humans actually navigate documents. We don't just ctrl-f for similar sounding phrases. We navigate structure. We know the details we want live in a specific section. We know footnotes reference specific line items. We follow the table of contents, understand hierarchical relationships, cross reference between sections.\n\nIf you want to build a vectorless system you need to keep all that in mind and go deeper than just preserving headers. Layout analysis to detect visual hierarchy (font size, indentation, positioning), table extraction that preserves row-column relationships and knows which section contains which table, hierarchical metadata that maps the entire document structure, and semantic labeling so the LLM understands what each section actually contains.\"\n\nTested this on a financial document RAG pipeline and the performance difference isn't marginal. Vector approach wastes tokens processing noise and produces low confidence answers that need manual follow up. Structure approach retrieves exactly what's needed and answers with actual citations you can verify.\n\nI think this matters more as documents get complex. The industry converged on vector embeddings because it seemed like the only scalable approach. But production systems are showing us it's not actually working. We keep optimizing embedding models and rerankers instead of questioning whether semantic similarity is even the right primitive for document retrieval.\n\nAnyway feels like one of those things where we all just accepted the vector search without questioning if it actually maps to how structured documents work.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r2st4j/vectorless_rag_why_document_trees_beat_embeddings/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4zo9de",
          "author": "Distinct-Target7503",
          "text": "Just a question... how do you build the tree? do you feed each page of a document to a VLM and rely on its ocrd text? isn't that quite expensive in term of tokens?\n\nI ask because, even without tabs/images, usually headers extraction from PDFs is not really reliable",
          "score": 1,
          "created_utc": "2026-02-12 15:01:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53kiyt",
              "author": "Clipbeam",
              "text": "This. It would either be super expensive or slow and error prone if you try to use local models.",
              "score": 2,
              "created_utc": "2026-02-13 02:57:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50pwzw",
          "author": "bac2qh",
          "text": "I am going to test out pageindex for my openclaw soon because the idea makes sense to me and embeddings feel underwhelming for my use case. I have a feeling that it’s going to consume a lot of tokens though",
          "score": 1,
          "created_utc": "2026-02-12 17:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51691m",
              "author": "Single-Constant9518",
              "text": "Pageindex sounds like an interesting approach! Token consumption can be a concern, but if it helps structure your data more effectively, it might be worth the trade-off. Just keep an eye on how it handles complex documents; that’s where the real benefits could show.",
              "score": 1,
              "created_utc": "2026-02-12 19:15:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o517ina",
                  "author": "bac2qh",
                  "text": "Yeah hopefully it can turn to a POC at work too.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:21:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51z34r",
          "author": "Intrepid-Scale2052",
          "text": "Do you mean the RAG first searches by document metadata, and then searches inside the document. Instead of the contents? \n\nI'm interested, I'm trying to build a searchable archive. What if the header does not say enough? What if you want to search, \"can you find me historical accounts of xxx?\"",
          "score": 1,
          "created_utc": "2026-02-12 21:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o538kcc",
          "author": "licjon",
          "text": "I think it depends on file formats, domain, and purpose. I think a layered approach is the way to go. I prefer to filter with FTS, then do a semantic search on the filtered candidates. ",
          "score": 1,
          "created_utc": "2026-02-13 01:43:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55frgp",
          "author": "DetectiveWeary9674",
          "text": "Isn't this what GraphRAG is for, allowing the LLM to navigate a structured web of information and relationships ?",
          "score": 1,
          "created_utc": "2026-02-13 12:07:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nl3js",
          "author": "BarrenLandslide",
          "text": "Check Out Taxoadapt. Might be something for you.",
          "score": 1,
          "created_utc": "2026-02-16 09:10:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z2stx",
          "author": "Independent-Cost-971",
          "text": "\n\nWrote this up in more detail if anyone's interested : [https://kudra.ai/vectorless-rag-why-document-tree-navigation-outperforms-semantic-search/](https://kudra.ai/vectorless-rag-why-document-tree-navigation-outperforms-semantic-search/)\n\n( shameless plug I know but worth a read )",
          "score": 0,
          "created_utc": "2026-02-12 13:03:09",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4z86co",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -2,
          "created_utc": "2026-02-12 13:35:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zf26g",
              "author": "exaknight21",
              "text": "I use knowledge graphs + hybrid search, i don’t have this issue? My use case requires semantic relationship trees, technical requirements in this document -> section page 10 section 2.10 Manufacturers item a. Benjamin Moore (specified) paint. Or in a spec table page 67 item 13 requires 200 SF of abatemet in room 201 library.\n\nThis is scalable per project and is working. I am baffled why it won’t work for you.",
              "score": 6,
              "created_utc": "2026-02-12 14:13:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4znp49",
                  "author": "Distinct-Target7503",
                  "text": ">knowledge graphs\n\nhow do you build the knowledge graph?\nit is usually really expensive in terms of tokens, or am I doing something wrong?",
                  "score": 1,
                  "created_utc": "2026-02-12 14:58:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r6f8a8",
      "title": "Essential Concepts for Retrieval-Augmented Generation (RAG)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6f8a8/essential_concepts_for_retrievalaugmented/",
      "author": "pgEdge_Postgres",
      "created_utc": "2026-02-16 17:03:56",
      "score": 34,
      "num_comments": 2,
      "upvote_ratio": 0.88,
      "text": "Some helpful insights from one of our senior software engineers, Muhammad Imtiaz.\n\n# Introduction\n\nRetrieval-Augmented Generation (RAG) represents a paradigm shift in how artificial intelligence systems access and utilize information. By combining the generative capabilities of large language models with dynamic information retrieval from external knowledge bases, RAG systems overcome the fundamental limitations of standalone language models—namely, their reliance on static training data and tendency toward hallucination.\n\nThis document provides a comprehensive technical reference covering the essential concepts, components, and implementation patterns that form the foundation of modern RAG architectures. Each concept is presented with clear explanations, practical code examples in Go, and real-world considerations for building production-grade systems.\n\nWhether you are architecting a new RAG system, optimizing an existing implementation, or seeking to understand the theoretical underpinnings of retrieval-augmented approaches, this reference provides the knowledge necessary to build accurate, efficient, and trustworthy AI applications. The concepts range from fundamental building blocks like embeddings and vector databases to advanced techniques such as hybrid search, re-ranking, and agentic RAG architectures.\n\nAs the field of artificial intelligence continues to evolve, RAG remains at the forefront of practical AI deployment, enabling systems that are both powerful and grounded in verifiable information.\n\n# Core Concepts and Implementation Patterns\n\n**Generator (Language Model)**\n\nThe component that generates the final answer using the retrieved context.\n\n**Retrieval**\n\nRetrieval is the process of identifying and extracting relevant information from a knowledge base before generating a response. It acts as the AI’s research phase, gathering necessary context from available documents before answering.\n\nRather than relying solely on pre-trained knowledge, retrieval enables the AI to access up-to-date, domain-specific information from documents, databases, or other knowledge sources.\n\nIn the example below, the retriever selects the top five most relevant documents and provides them to the LLM to generate the final answer.\n\n`relevantDocs := vectorDB.Search(query, 5) // top_k=5`  \n`answer := llm.Generate(query, relevantDocs)`\n\n**Embeddings**\n\nEmbeddings are numerical representations of text that capture semantic meaning. They convert words, sentences, or documents into dense vectors that preserve context and relationships.\n\nThe example below demonstrates how to generate embeddings using the OpenAI API.\n\n`import (`  \n`\"context\"`  \n`\"github.com/sashabaranov/go-openai\"`  \n`)`  \n  \n`client := openai.NewClient(\"your-token\")`  \n`resp, err := client.CreateEmbeddings(`  \n`context.Background(),`  \n`openai.EmbeddingRequest{`  \n`Input: []string{\"Retrieval-Augmented Generation\"},`  \n`Model: openai.SmallEmbedding3,`  \n`},`  \n`)`  \n`if err != nil {`  \n`log.Fatal(err)`  \n`}`  \n`vector := resp.Data[0].Embedding`\n\n# Vector Databases\n\nVector databases are specialized systems designed to store and query high-dimensional embeddings. Unlike traditional databases that rely on exact matches, they use distance metrics to identify semantically similar content.\n\nThey support fast similarity searches across millions of documents in milliseconds, making them essential for scalable RAG systems.\n\nThe example below shows how to create a collection and add documents with embeddings using the Chroma client.\n\n`import \"github.com/chroma-core/chroma-go\"`  \n  \n`client := chroma.NewClient()`  \n`collection, _ := client.CreateCollection(\"docs\")`  \n  \n`// Generate embeddings for documents`  \n`docs := []string{\"RAG improves accuracy\", \"LLMs can hallucinate\"}`  \n`emb1 := embedder.Embed(docs[0])`  \n`emb2 := embedder.Embed(docs[1])`  \n  \n`// Add documents with their embeddings`  \n`collection.Add(`  \n`context.Background(),`  \n`chroma.WithIDs([]string{\"doc1\", \"doc2\"}),`  \n`chroma.WithEmbeddings([][]float32{emb1, emb2}),`  \n`chroma.WithDocuments(docs),`  \n`)`\n\n# Retriever\n\nA retriever is a component that manages the retrieval process. It converts a user query into an embedding, searches the vector database, and returns the most relevant document chunks.\n\nIt functions like a smart librarian, understanding the query and locating the most relevant information within a large collection.\n\nThe example below demonstrates a basic retriever implementation.\n\n`type Retriever struct {`  \n`VectorDB VectorDB`  \n`}`  \n  \n`func (r *Retriever) Retrieve(query string, topK int) []Result {`  \n`queryVector := Embed(query)`  \n`return r.VectorDB.Search(queryVector, topK)`  \n`}`\n\n# Chunking\n\nChunking is the process of dividing large documents into smaller, manageable segments called “chunks.” Effective chunking preserves semantic meaning while ensuring content fits within model context limits.\n\nProper chunking is essential, as it directly affects retrieval quality. Well-structured chunks improve precision and support more accurate responses.\n\nThe example below demonstrates a character-based chunking function with overlap support.\n\n`func ChunkText(text string, chunkSize, overlap int) []string {`  \n`var chunks []string`  \n`runes := []rune(text)`  \n`for start := 0; start < len(runes); start += (chunkSize - overlap) {`  \n`end := start + chunkSize`  \n`if end > len(runes) {`  \n`end = len(runes)`  \n`}`  \n`chunks = append(chunks, string(runes[start:end]))`  \n  \n`if end >= len(runes) {`  \n`break`  \n`}`  \n`}`  \n`return chunks`  \n`}`  \n  \n`chunks := ChunkText(document, 500, 50)`\n\n# Context Window\n\nThe context window is the maximum number of tokens (words or subwords) an LLM can process in a single request. It defines the model’s working memory and the amount of context that can be included.\n\nContext windows range from 4K tokens in older models to over 200K in modern ones. Retrieved chunks must fit within this limit, making chunk size and selection critical.\n\nThe example below demonstrates how to fit chunks within a token limit.\n\n`func FitContext(chunks []string, maxTokens int) []string {`  \n`var context []string`  \n`tokenCount := 0`  \n  \n`for _, chunk := range chunks {`  \n`chunkTokens := CountTokens(chunk)`  \n`if tokenCount + chunkTokens > maxTokens {`  \n`break`  \n`}`  \n`context = append(context, chunk)`  \n`tokenCount += chunkTokens`  \n`}`  \n  \n`return context`  \n`}`\n\n# Grounding\n\nGrounding ensures AI responses are based on retrieved, verifiable sources rather than hallucinated information. It keeps the model anchored to real data.\n\nEffective grounding requires citing specific sources and relying only on the provided context to support claims. This reduces hallucinations and improves trustworthiness.\n\nThe example below demonstrates a grounding prompt template.\n\n`prompt := fmt.Sprintf(\\``  \n`Answer the question using ONLY the provided context.`  \n`Cite the source for each claim.`  \n`Context: %s`  \n  \n`Question: %s`  \n  \n`Answer with citations:`  \n`\\`, retrievedDocs, userQuestion)`  \n  \n`response := llm.Generate(prompt)`\n\n# Re-Ranking\n\nTwo-stage retrieval enhances result quality by combining speed and precision. First, a fast initial search retrieves many candidates (e.g., top 100). Then, a more accurate cross-encoder model re-ranks them to identify the best matches.\n\nThis approach pairs broad retrieval with fine-grained scoring for optimal results.\n\nThe example below demonstrates a basic re-ranking workflow.\n\n`// Initial fast retrieval`  \n`candidates := retriever.Search(query, 100)`  \n  \n`// Re-rank using a CrossEncoder`  \n`scores := reranker.Predict(query, candidates)`  \n  \n`// Sort candidates by score and take top 5`  \n`topDocs := SortByScore(candidates, scores)[:5]`\n\n# Hybrid Search\n\nHybrid search combines keyword-based search (BM25) with semantic vector search. It leverages both exact term matching and meaning-based similarity to improve retrieval accuracy.\n\nBy blending keyword and semantic scores, it provides the precision of exact matches along with the flexibility of understanding conceptual queries.\n\nThe example below demonstrates a hybrid search implementation.\n\n`func HybridSearch(query string, alpha float64) []Result {`  \n`keywordResults := BM25Search(query)`  \n`semanticResults := VectorSearch(query)`  \n  \n`// Combine scores:`  \n`// finalScore = alpha * keywordScore + (1-alpha) * semanticScore`  \n`finalResults := CombineAndRank(keywordResults, semanticResults, alpha)`  \n  \n`return finalResults[:5]`  \n`}`\n\n# Metadata Filtering\n\nMetadata filtering narrows search results by using document attributes such as dates, authors, types, or departments before performing a semantic search. This reduces noise and improves precision.\n\nApplying filters like author: John Doe or document\\_type: report focuses the search on the most relevant documents.\n\nThe example below demonstrates metadata filtering in a vector database query.\n\n`results := collection.Query(`  \n`Query{`  \n`Texts: []string{\"quarterly revenue\"},`  \n`TopK: 10,`  \n`Where: map[string]interface{}{`  \n`\"year\":       2024,`  \n`\"department\": \"sales\",`  \n`\"type\": map[string]interface{}{`  \n`\"$in\": []string{\"report\", \"presentation\"},`  \n`},`  \n`},`  \n`},`  \n`)`\n\n# Similarity Search\n\nThe retriever is the core search mechanism in RAG, identifying documents whose embeddings are most similar to a query’s embedding. It evaluates semantic closeness rather than just keyword matches.\n\nSimilarity is typically measured using cosine similarity (angle between vectors) or dot product, with higher scores indicating more relevant content.\n\nThe example below demonstrates cosine similarity using the Gonum library.\n\n`import (`  \n`\"gonum.org/v1/gonum/mat\"`  \n`)`  \n  \n`func CosineSimilarity(vec1, vec2 []float64) float64 {`  \n`v1 := mat.NewVecDense(len(vec1), vec1)`  \n`v2 := mat.NewVecDense(len(vec2), vec2)`  \n  \n`dotProduct := mat.Dot(v1, v2)`  \n`norm1 := mat.Norm(v1, 2)`  \n`norm2 := mat.Norm(v2, 2)`  \n  \n`return dotProduct / (norm1 * norm2)`  \n`}`  \n  \n`// Usage example`  \n`queryVec := Embed(query)`  \n`for _, docVec := range documentVectors {`  \n`score := CosineSimilarity(queryVec, docVec)`  \n`// Store score for ranking`  \n`}`\n\n# Prompt Injection\n\nPrompt injection is a security vulnerability where malicious users embed instructions in queries to manipulate AI behavior. Attackers may attempt to override system prompts or extract sensitive information.\n\nCommon examples include phrases like “ignore previous instructions” or “reveal your system prompt.” RAG systems must sanitize inputs to prevent such attacks.\n\nThe example below demonstrates a basic input sanitization function. In production, multiple defenses—such as regex patterns, semantic similarity checks, and output validation—are required.\n\n`func SanitizeInput(userInput string) (string, error) {`  \n`// Basic pattern matching - extend with regex for production use`  \n`dangerousPatterns := []string{`  \n`\"ignore previous instructions\",`  \n`\"disregard system prompt\",`  \n`\"reveal your instructions\",`  \n`\"ignore all prior\",`  \n`\"bypass security\",`  \n`}`  \n  \n`lowerInput := strings.ToLower(userInput)`  \n`for _, pattern := range dangerousPatterns {`  \n`if strings.Contains(lowerInput, pattern) {`  \n`return \"\", errors.New(\"invalid input detected\")`  \n`}`  \n`}`  \n  \n`// Additional checks for production:`  \n`// - Regex for obfuscated patterns (e.g., \"ign0re\")`  \n`// - Semantic similarity to known attack phrases`  \n`// - Length and character validation`  \n  \n`return userInput, nil`  \n`}`\n\n# Hallucination\n\nGenerative AI can produce convincing but incorrect information, including false facts, fake citations, or invented details.\n\nRAG helps reduce hallucinations by grounding responses in retrieved documents, though proper grounding and citation are essential to minimize risk.\n\nThe example below demonstrates a verification function that checks whether a response is supported by source documents. For higher reliability, consider using Natural Language Inference  models or extractive fact-checking, as relying on one LLM to verify another has limitations.\n\n`func IsSupported(response, sourceDocs string) bool {`  \n`verificationPrompt := fmt.Sprintf(\\``  \n`Response: %s`  \n`Source: %s`  \n  \n`Is this response fully supported by the source documents?`  \n`Answer yes or no.`  \n`\\`, response, sourceDocs)`  \n  \n`result := llm.Generate(verificationPrompt)`  \n`return strings.ToLower(strings.TrimSpace(result)) == \"yes\"`  \n`}`  \n  \n`// Alternative: Use NLI model for more reliable verification`  \n`func IsSupportedNLI(response, sourceDocs string) bool {`  \n`// NLI models classify as: entailment, contradiction, or neutral`  \n`result := nliModel.Predict(sourceDocs, response)`  \n`return result.Label == \"entailment\" && result.Score > 0.8`  \n`}`\n\n# Agentic RAG\n\nAgentic RAG is an advanced architecture where the AI actively plans, reasons, and controls its own retrieval strategy. Rather than performing a single search, the agent can conduct multiple searches, analyze results, and iterate.\n\nIt autonomously decides what information to retrieve, when to search again, which tools to use, and how to synthesize multiple sources—enabling complex, multi-step reasoning.\n\nThe example below demonstrates an agentic RAG implementation.\n\n`func (a *AgenticRAG) Answer(query string) string {`  \n`plan := a.llm.CreatePlan(query)`  \n  \n`for _, step := range plan.Steps {`  \n`switch step.Action {`  \n`case \"search\":`  \n`results := a.retriever.Search(step.Query)`  \n`a.context.Add(results)`  \n`case \"reason\":`  \n`analysis := a.llm.Analyze(a.context)`  \n`a.context.Add(analysis)`  \n`}`  \n`}`  \n  \n`return a.llm.Synthesize(a.context)`  \n`}`\n\n# Latency\n\nRAG latency is the total time from a user query to the final response, including embedding generation, vector search, re-ranking (if used), and LLM generation. Each step contributes to the delay.\n\nLatency directly impacts user experience and can be optimized by caching embeddings, using faster models, narrowing search scope, and parallelizing operations. Typical RAG systems aim for sub-second to a few seconds of latency.\n\nThe example below measures latency for each stage of the RAG pipeline.\n\n`import \"time\"`  \n  \n`func MeasureLatency(query string) {`  \n`start := time.Now()`  \n  \n`// Step 1: Embed query`  \n`embedding := Embed(query)`  \n`t1 := time.Now()`  \n  \n`// Step 2: Search`  \n`results := vectorDB.Search(embedding)`  \n`t2 := time.Now()`  \n  \n`// Step 3: Generate`  \n`response := llm.Generate(query, results)`  \n`t3 := time.Now()`  \n  \n`fmt.Printf(\"Embed: %v | Search: %v | Generate: %v\\n\",`  \n`t1.Sub(start), t2.Sub(t1), t3.Sub(t2))`  \n`}`\n\n  \nHope this all helps!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r6f8a8/essential_concepts_for_retrievalaugmented/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5ptoct",
          "author": "Intrepid-Scale2052",
          "text": "Sick!",
          "score": 3,
          "created_utc": "2026-02-16 17:30:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61cy07",
          "author": "gowtham150",
          "text": "Very refreshing to see golang finally. Hate to see python everywhere. Very well documented post with enterprise stuff.",
          "score": 1,
          "created_utc": "2026-02-18 11:47:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3oiyz",
      "title": "Chunking for RAG: the boring part that decides your accuracy (practical guide)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r3oiyz/chunking_for_rag_the_boring_part_that_decides/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-13 13:00:36",
      "score": 29,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "Looks like it's a chunking day here. :) Let me add my 5 cents.\n\nMost “RAG accuracy” problems show up later as people tweak rerankers, prompts, models, etc.\n\nBut a huge % of failures start earlier: chunking.\n\nIf the right info can’t be retrieved cleanly, the model can’t “think” its way out. It’ll either hallucinate, or answer confidently from partial context.\n\nLet's start with the definition: A chunk is the smallest unit of meaning (!) that can answer a real question without needing its neighbors.\n\nToo big → you retrieve the answer plus extra junk → model gets distracted (precision drops). Too small → you retrieve fragments → missing context (recall drops). Wrong boundaries → meaning gets shredded (definitions, steps, tables…).\n\n# 3 common symptoms your chunking is broken\n\n1. Chunks too big: top-k retrieval contains the answer but also unrelated sections → the LLM free-associates.\n2. Chunks too small: the answer exists but is split across boundaries → retrieval misses it.\n3. Bad split points: tables, lists, procedures, “Definitions” sections → you split exactly where coherence matters.\n\n# There are actually 3 chunking modes that cover most real-world docs (NOT ALL of them, still)\n\n# Mode 1) Structure-first (best default)\n\nUse for: technical manuals, policies, specs, handbooks, wikis (anything with headings).\n\nHow to do it:\n\n* Chunk by heading hierarchy (H2/H3 sections)\n* Keep paragraphs intact\n* Keep lists/tables/code blocks intact\n* Store section\\_path metadata (e.g., Security > Access Control > MFA)\n\nWhy it works: your doc already has a map. Don’t throw it away.\n\n# Mode 2) Semantic windows (for messy conversational text)\n\nUse for: transcripts, email threads, Slack dumps, scraped webpages (weak structure, topic drift).\n\nHow to do it:\n\n* Build topic-coherent “windows” (don’t hard-split blindly)\n* Use adaptive overlap only when meaning crosses boundaries\n   * Q → A turns\n   * follow-ups (“what about…”, “as mentioned earlier…”)\n   * references to earlier context\n\nWhy it works: conversation doesn’t respect token boundaries.\n\n# Mode 3) Atomic facts + parent fallback (support/FAQ style)\n\nUse for: FAQs, troubleshooting, runbooks, support KBs (answers are small + repetitive).\n\nHow to do it:\n\n* Index atomic chunks (1–3 paragraphs or one step-group)\n* Store pointer to parent section\n* Retrieval policy:\n   * fetch atom first\n   * if answer looks incomplete / low confidence → fetch parent\n\nWhy it works: high precision by default, but you can pull context when needed.\n\n# Most useful tweaks\n\n# Overlap: use it like salt, not soup\n\nDon't do “20% overlap” everywhere.\n\nOverlap is for dependency, not tradition:\n\n* 0 overlap: self-contained sections\n* small overlap: narrative text\n* bigger overlap: procedures + conversational threads + “as mentioned above” content\n\n# Tables are special (many mess this up)\n\nDo not split tables mid-row or mid-header.\n\n* Store the whole table as one chunk + create a table summary chunk\n* Or chunk by row, but repeat headers + key columns in every row chunk\n\n# Metadata: the cheap accuracy boost people sometimes forget\n\nStore at least:\n\n* `doc_id`\n* `section_path`\n* `chunk_type` (policy / procedure / faq / table / code)\n* `version` / `effective_date` (if docs change)\n* `audience` (legal / support / eng)\n\nThis enables filtering before vector search and reduces “wrong-but-related” retrieval.\n\n# How to test chunking fast (with no fancy eval framework)\n\nTake 30 real user questions (not synthetic).\n\nFor each:\n\n* retrieve top-5\n* score: Does any chunk contain the answer verbatim or with minimal inference?\n\nInterpretation:\n\n* Often “no” → boundaries wrong / chunks too small / missing metadata filters\n* Answer exists but not ranked → ranking/reranker/metadata issue\n\nBonus gut-check: Take 10 questions and open the top retrieved chunk. If you keep thinking “the answer is almost here but needs the previous paragraph”… your chunk boundaries are wrong.\n\n# Practical starting defaults (if you just want numbers)\n\nThese aren’t laws, just decent baselines:\n\n* Manuals/policies/specs: structure-first, \\~300–800 tokens\n* Procedures: chunk by step groups, keep prerequisites + warnings with steps\n* FAQs/support: atomic \\~150–400 tokens + parent fallback\n* Transcripts: semantic windows \\~200–500 tokens + adaptive overlap\n\n# What we actually do for large-scale production use cases\n\nWe test extensively and automate the whole process\n\n* Chunking is automated per document type and ALWAYS considers document structure (no mid-word/sentence/table breaks)\n* For each document type there's more than one chunking approach\n* Evals are automated (created automatically and tested automatically on every pipeline change)\n* Extensive testing is the core. For each project different chunking strategies are tested and compared versus each other (here automated evals add velocity) As a result of these automations we receive good accuracy with little \"manual RAG drag\" and in a matter of days.",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r3oiyz/chunking_for_rag_the_boring_part_that_decides/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5aq9vn",
          "author": "Otherwise-Platypus38",
          "text": "Chunking is the main driving factor in most RAG scenarios. A good chunking strategy with well formed metadata can really increase your accuracy. Having an in-house pipeline makes things easier, as you know your data best, as well as you know your domain best, so you can anticipate end user interactions better.\n\nI have implemented a chunking strategy based (completely in-house), where you convert unstructured PDF data to markdown, and use OCR on figures. Then, creating chunks for sections and sub-sections, and creating separate chunks for figures and tables with cross-references in the main chunk. This allows to preserve to a large extent the hierarchy. \n\nSpending some time in the document preparation and processing stage has been quite fruitful. Although, nowadays most providers have the option to use in-built retrieval tools, it’s quite satisfying to see your own implementation working in a real world product.",
          "score": 3,
          "created_utc": "2026-02-14 05:58:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55vw61",
          "author": "Ok_Revenue9041",
          "text": "Great summary on why chunking is so critical for RAG results. Testing with real user questions is overlooked but makes a huge difference in catching subtle retrieval issues early. If you want to make sure your brand’s content shows up more often and accurately in AI powered searches, MentionDesk can help optimize how your chunks get picked up across these platforms.",
          "score": 1,
          "created_utc": "2026-02-13 13:48:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r62t5f",
      "title": "bb25 (Bayesian BM25) v0.2.0 is out",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r62t5f/bb25_bayesian_bm25_v020_is_out/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-16 07:04:32",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "bb25 v0.2.0 is out — a Python + Rust implementation of Bayesian BM25 that turns search scores into calibrated probabilities.\n\n[https://github.com/instructkr/bb25](https://github.com/instructkr/bb25)\n\nA week ago, I built bb25 that turns BM25 into a probability engine! In addition to the Rust-based implementation, the paper's author shipped his own implementation. Comparing the two taught me more than the paper itself.\n\nThe Bayesian BM25 paper does something elegant, in that applying Bayes' theorem to BM25 scores so they become real probabilities, not arbitrary numbers. This makes hybrid search fusion mathematically principled instead of heuristic.\n\nInstruct.KR's bb25 took a ground-up approach, tokenizer, inverted index, scorers, 10 experiments mapping to the paper's theorems, plus a Rust port. Jaepil's implementation took the opposite path, a thin NumPy layer that plugs into existing search systems.\n\nReading both codebases side by side, I found my document length prior has room to improvement (e.g. monotonic decay instead of symmetric bell curve), my probability AND suffered from shrinkage, and I further added automatic parameter estimation and online learning entirely.\n\nbb25 v0.2.0 introduces all four. One fun discovery along the way, my Rust code already had the correct log-odds conjunction, but I had never backported it to Python. Same project, two different AND operations.\n\nThe deeper surprise came from a formula in the reference material. Expand the Bayesian posterior and you get the structure of an artificial neuron! Think of weighted sum, bias, sigmoid activation. Sigmoid, ReLU, Softmax, Attention all have Bayesian derivations. A 50-year-old search algorithm leads straight to the mathematical roots of neural networks.\n\nAll creds to Jaepil and Cognica Team!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r62t5f/bb25_bayesian_bm25_v020_is_out/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r36f72",
      "title": "Increasing your chunk size solves lots of problems - the default 1024 bit chunk size is too small",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r36f72/increasing_your_chunk_size_solves_lots_of/",
      "author": "Free-Ferret7135",
      "created_utc": "2026-02-12 21:42:49",
      "score": 24,
      "num_comments": 30,
      "upvote_ratio": 0.9,
      "text": "Chunking documents into small 1024 bit looks very outdated to me. But even for something enterprise like Google Vertex AI Search this is still the case.\n\nLLMs are so much better at processing large context windows than they were yesterday, last month, last year.\n\nWith Gemini, Llama, Opus etc. being able to easily read and understand 300-400 pages at max, you can generously feed it 30-50 pages and still get a good result without \"lost in the middle\" IMHO.\n\n  \nSimply increasing chunk size, ideally at positions that make sementically sense (e.g. full chapters from a particular topic) and then feeding the top k chunks into one of the above LLM ... bing, you have a 95-100% accurate RAG.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r36f72/increasing_your_chunk_size_solves_lots_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o52267o",
          "author": "flonnil",
          "text": "you are wrong, but given you are only here to inform us that you are right, i think i'll let you find out yourself.",
          "score": 32,
          "created_utc": "2026-02-12 21:47:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o522ek8",
              "author": "mrnoirblack",
              "text": "I'm interested in why is he wrong as I haven't had experience with huge context models",
              "score": 2,
              "created_utc": "2026-02-12 21:49:05",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o524mac",
                  "author": "flonnil",
                  "text": "\\- input-token costs off the charts, particularly with said big context models  \n\\- amount of consulted sources decreases  \n\\- granular information gets lost in the haystack  \n\\- llm context gets polluted to absolute hell  \n\\- [what this guy pointed out niceley](https://www.reddit.com/r/Rag/comments/1r36f72/comment/o55et6w/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): search accuracy gets diluted   \n\\- multi-step or multi-retrieval agents will despite what OP says absolutely hit context ceiling  \n\\- while providers claim context windows of N, performance usually massively drops at arround max 0.4 N. Big context window doesnt mean its best to fill it to the brim, in the contrary.  \nGenerally: for contextual-understandig-type things, rather big chunks are fine, for exact granular data retrieval smaller are better.",
                  "score": 14,
                  "created_utc": "2026-02-12 21:59:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52gg3n",
          "author": "Harotsa",
          "text": "The bottleneck for chunk size was never the context window of the reader, but rather the context window of the embedder as well as the “effective” context size of the embedder.\n\nEven SOTA embedding models today don’t have much more than a few thousand tokens of context max, which is a hard limit on your chunk size assuming you are going to use vector search.\n\nAdditionally there is a very significant regression to the mean issue with embeddings, where if you put too many sentences together you start losing the signal of that piece of text.\n\nI think the larger effective context size of the readers (decoder LLMs) means more context can be retrieved, but it shouldn’t have too much impact on the size of the chunks you are embedding.",
          "score": 8,
          "created_utc": "2026-02-12 23:00:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52nppf",
          "author": "RobertLigthart",
          "text": "the point about embedder context being the actual bottleneck is the real answer here. you can feed 50 pages to gemini all day but if your embedding model only meaningfully represents the first 512 tokens of each chunk then your retrieval quality tanks regardless of chunk size\n\n  \nin practice what worked for me is chunking larger (2-4k tokens) but with overlap AND storing a summary embedding alongside the full chunk. retrieve on the summary, pass the full chunk to the LLM. best of both worlds\n\n  \nthe cost argument matters too though. if youre doing thousands of queries a day those fat chunks in the context window add up fast",
          "score": 6,
          "created_utc": "2026-02-12 23:41:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55et6w",
          "author": "Donkit_AI",
          "text": "I agree to comments from u/flonnil, but want to add to it.\n\nIncreasing chunk size feels like a win until you realize you’re just paying for 'Context Dilution.'\n\nMathematically, when you bloat chunks, your cosine similarity starts measuring the 'average' of a 6,000-token soup rather than the specific needle you’re looking for. You end up with higher latency, higher token costs, and a model that gets 'Lost in the Middle'. The latter won't happen in case the answer fits in 1 or 2 chunks, but with pushing more chunks into the answer and agentic workflows where context is bloated by all the instructions and tools, it will take a significant toll.\n\nBesides, by nature it's an endless optimization process. It must be based on evals to see, if it's a gain or a loss in your specific case. We automate experimentation and just find the mathematical 'sweet spot' for each specific use case.",
          "score": 5,
          "created_utc": "2026-02-13 12:00:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o58rngs",
              "author": "Free-Ferret7135",
              "text": "\"a model that gets 'Lost in the Middle\"\n\nGpt 5.2 does not do this anymore even at large contexts, the LLMs got so much better to the point that \"Lost in the middle\" is neglible. \n\nI can send you dozens of examples where it finds the needle in the hay even with bloated prompt. But give me one prompt where gpt 5.2 exhibits \"Lost in the middle\" and I will take back what I said. ;)\n\n\"higher latency, higher token costs\"\n\nTrue, but these are ressource limitations rather than technical limitations.",
              "score": 1,
              "created_utc": "2026-02-13 22:18:21",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53gu0m",
          "author": "InitialJelly7380",
          "text": "good to know...thanks",
          "score": 2,
          "created_utc": "2026-02-13 02:34:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53t180",
          "author": "Ok-Attention2882",
          "text": "Increasing chunk size is a COPE.",
          "score": 2,
          "created_utc": "2026-02-13 03:52:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o523ctr",
          "author": "butwhol",
          "text": "What about Context bloat /context rot issues for ai agents using your rag?",
          "score": 1,
          "created_utc": "2026-02-12 21:53:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52six3",
              "author": "Free-Ferret7135",
              "text": "Valid concern, will need some sort of compaction to tackle this.",
              "score": 1,
              "created_utc": "2026-02-13 00:08:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o52jf67",
          "author": "88chilly",
          "text": "1k chunks feel like a relic at this point. With the context windows we have now it makes way more sense to chunk by meaningful sections instead of arbitrarily small slices. Bigger semantically clean chunks plus solid retrieval gets you way closer to reliable RAG. Designing around old limits just holds things back.",
          "score": 1,
          "created_utc": "2026-02-12 23:16:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o531oti",
              "author": "Kerbourgnec",
              "text": "Why would you use LLM context size to argue about embedding chunk size?\n\nEmbeddings and their chunk size have one goal only: when searching for X, I get the most relevant results. Optimize your embedder and chunk size for that only.\n\nIf you want to give more to your LLM, nothing prevents you to give ten pages around the chunk you retrieve. There is no reason except for laziness to only return the exact chunk related to the embedding it found.",
              "score": 1,
              "created_utc": "2026-02-13 01:01:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o52lvd7",
          "author": "Ryanmonroe82",
          "text": "Have you tried using only top_p and setting top_k = 0.0, min_p = 1? \nI too have found larger chunks work much better.  Using models that use a hybrid mamba architecture (nemotron 9b/12b V2) handle the larger chunks very well.",
          "score": 1,
          "created_utc": "2026-02-12 23:30:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o52soi1",
              "author": "Free-Ferret7135",
              "text": "Yes, but never used nemotron 9b/12b V2.  How do you compare its performance against other llms?",
              "score": 1,
              "created_utc": "2026-02-13 00:09:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5308h3",
                  "author": "Ryanmonroe82",
                  "text": "Both punch above their weight considerably. Using BF16/F16 is key though.  Check into the Mamba hybrids and see the advantages over a the traditional transformer and MoE architecture. \nAnother one I really like is RNJ-1-Instruct, on hugging face it’s available in F32 and it’s incredibly good.",
                  "score": 1,
                  "created_utc": "2026-02-13 00:53:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o52nt8j",
          "author": "Weary_Long3409",
          "text": "Depends on documents nature. I use 6k chunk size for my >8k legal analysis docs. Doc size range from 1k to 8k tokens, mostly <6k tokens.\n\nThe golden rule is, the more data you have, the larger chunk size should be. Small chunks in a large dataset prone to break cosine similarities.",
          "score": 1,
          "created_utc": "2026-02-12 23:41:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54jqa8",
              "author": "Late_Special_6705",
              "text": "I think you're confusing something. Where did you find the 6k? Model name",
              "score": 1,
              "created_utc": "2026-02-13 07:19:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o531h6n",
          "author": "Kerbourgnec",
          "text": "Why would you use LLM context size to argue about embedding chunk size?\n\nEmbeddings and their chunk size have one goal only: when searching for X, I get the most relevant results. Optimize your embedder and chunk size for that only.\n\nIf you want to give more to your LLM, nothing prevents you to give ten pages around the chunk you retrieve. There is no reason except for laziness to only return the exact chunk related to the embedding it found.",
          "score": 1,
          "created_utc": "2026-02-13 01:00:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o535vmo",
              "author": "Free-Ferret7135",
              "text": "\"only return the exact chunk\" only works for atomic retrievals where you are asking something very specific.   \nBut many datasets have more context around it, relationships between chunks etc.  \nNow you might say \"well, then you should build your knowledge graph around that instead of RAG\" but I find it much easier to ingest data into a RAG then building complicated graphs and I see larger chunk sizes as key in making them suitable for contextual understanding.",
              "score": 1,
              "created_utc": "2026-02-13 01:27:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o53c45b",
          "author": "yangastas_paradise",
          "text": "I don't understand how chunk size is related to context window ? Chunk size is more about capturing the right semantic meaning so that your RAG can fetch the relevant info at runtime. Too small then the semantic meaning is too narrow, and too large the meaning is diluted. \n\nI actually use a multi chunk strategy and that works well .",
          "score": 1,
          "created_utc": "2026-02-13 02:05:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53m8wb",
              "author": "Space__Whiskey",
              "text": "I recently discovered this, and it improved things significantly.\n\nThe analogy is, to put out more fishing lines to increase the chance of catching your fish.",
              "score": 1,
              "created_utc": "2026-02-13 03:08:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o57146u",
          "author": "Otherwise_Corgi_5940",
          "text": "Hi guys I am currently done a job as a junior AI/ML engineer now I am doing a project on the rag I have finished the document extraction with the mistral ocr now I want to move on chucking part can you guys help with some advice how to do it with Enterprise level?",
          "score": 1,
          "created_utc": "2026-02-13 17:11:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o58bchh",
          "author": "blue-or-brown-keys",
          "text": "RAG will bring multiple chunks and context windows can be large. You problem is not with chunk size, but that when you break into chunks you are losing context, try padding with document context/summary",
          "score": 1,
          "created_utc": "2026-02-13 20:57:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o59eblt",
          "author": "Legitimate_Sherbet_7",
          "text": "There are a lot of different opinions here on chunk size and accuracy. Based on my own development there are mixed results from just changing chunk sizes and overlap. There is the quality of the question being asked. The \",role,\" or personality prompt and how it's tailored to the application and the model being used makes a huge difference. It's not a one size fits all its maybe for some applications getting to a configuration where one size fits most. But like everyone else I'm still learning.",
          "score": 1,
          "created_utc": "2026-02-14 00:28:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5a8f7p",
          "author": "eurydice1727",
          "text": "Parent - Child Chunking solves a lot of this. I use parent with payload only, a top ranked child chunk after dedupe and reranking then calls for the parent chunk which provides further context. Parents are up to 3500 tokens and children I max at 900",
          "score": 1,
          "created_utc": "2026-02-14 03:41:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r5mp7t",
      "title": "Has anyone here successfully sold RAG solutions to clients? Would love to hear your experience (pricing, client acquisition, delivery, etc.)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5mp7t/has_anyone_here_successfully_sold_rag_solutions/",
      "author": "Temporary_Pay3221",
      "created_utc": "2026-02-15 18:55:31",
      "score": 23,
      "num_comments": 18,
      "upvote_ratio": 0.96,
      "text": "Hey everyone!\n\nI've been diving deep into RAG systems lately and I'm genuinely fascinated by the technology. I've built a few projects for myself and feel confident in my technical abilities, but now I'm looking to transition this into actual client work.\n\nBefore I jump in, I'd really appreciate learning from people who've already walked this path. If you've sold RAG solutions to clients, I'd love to hear about your experience:\n\n**Client & Project Details:**\n\n* What types of clients/industries did you work with?\n* How did they discover they needed RAG? (Did they come asking for it, or did you identify the use case?)\n* What was the scope? (customer support, internal knowledge base, document search, etc.)\n\n**Delivery & Timeline:**\n\n* How long did the project take from discovery to delivery?\n* What were the biggest technical challenges you faced?\n* Did you handle ongoing maintenance, or was it a one-time delivery?\n\n**Business Side:**\n\n* How did you find these clients? (freelance platforms, LinkedIn outreach, referrals, content marketing, etc.)\n* What did you charge? (ballpark is fine - just trying to understand market rates)\n* How did you structure pricing? (fixed project, hourly, monthly retainer?)\n\n**Post-Delivery:**\n\n* Were clients happy with the results?\n* Did you iterate/improve the system after launch?\n* Any lessons learned that you'd do differently next time?\n\nThanks !",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5mp7t/has_anyone_here_successfully_sold_rag_solutions/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5k30sr",
          "author": "AmbitionCrazy7039",
          "text": "What types of clients/industries did you work with? - Build and deployed for an IP firm.\n\nHow did they discover they needed RAG? - They did by themselves. They asked us for a discovery meeting.  But they basically knew what solutions they want. We picked the most interesting one.\n\nWhat was the scope? - Some very specific document/outcome search. But not just semantics stuff, but some path dependent precedents + analytics. The LLM basically only summarizes the output. Traceability was most important. \n\n\n\nHow long did the project take from discovery to delivery? - \\~1 month from discovery to signing. Pilot delivery \\~6 weeks. A lot of follow ups needed.\n\nWhat were the biggest technical challenges you faced? - Preprocessing shitty data.\n\nDid you handle ongoing maintenance, or was it a one-time delivery? - We handle maintenance if something breaks.\n\n\n\nHow did you find these clients? - Network / they found us.\n\nWhat did you charge? - Our deal was kinda different so any inside from me here will not reflect real market value. We did a research project for them at special conditions (highly profitable for both sides).\n\nHow did you structure pricing? - Fixed price per month for developing with 2 months of expected development time. We granted the option to exit the project after 1 month without paying for 2. If we would build in a different setting we surely would go for usual 30/40/30 splits.\n\n\n\nWere clients happy with the results? - Yes.\n\nDid you iterate/improve the system after launch? - Not yet but we do run market research to evaluate if this may fit a broader audience.\n\nAny lessons learned that you'd do differently next time? - Nothing we really did wrong but the project shifted my view on RAG, from \"some useless semantic search\" to a more broader, creative approach.   \nBut I will add that RAG can't to magic for LLMs. If your project scope is fundamentally not solvable by an LLM (for example writing high quality law stuff), RAG will not change it. \n\n",
          "score": 6,
          "created_utc": "2026-02-15 19:24:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5k5l2j",
              "author": "Temporary_Pay3221",
              "text": "**Thanks for sharing**  \n  \n**Two questions:**\n\n1. **How did they find you?** You said \"they found us\", but what specifically made them reach out to YOU vs the hundreds of other people who can build RAG systems? Was it your profile? A referral? Something you built publicly?\n2. **Do you want to scale this?** Are you thinking about:\n\n* Building a dev team to handle delivery while you focus on growth?\n* Hiring sales to bring in more deals?\n* Creating systems/processes to delegate the work?\n\nOr is your goal to stay small, just you (maybe +1-2 people) doing selective projects?\n\nBecause right now you're limited by your own time. Curious if scaling is part of your plan or not.",
              "score": 2,
              "created_utc": "2026-02-15 19:36:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5k859m",
                  "author": "AmbitionCrazy7039",
                  "text": "1. Network. I started a ML startup years ago. Although it had nothing to do with LLMs and RAG.\n2. Who says we don't have a dev+sales team? My goal is and has ever been to build things that really create value. This system seems to deliver to the expectations. May open source it. The difficult part is the preprocessing and this pipelines are fundamentally not really scalable. Building the individual pipelines for customers is both fun and high-payed.",
                  "score": 4,
                  "created_utc": "2026-02-15 19:49:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5kxqpn",
              "author": "AumOza",
              "text": "Curious what you meant by shitty data was it OCR problems from scanned docs, messy formatting, or something more complex?",
              "score": 1,
              "created_utc": "2026-02-15 22:01:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5n3dd1",
                  "author": "AmbitionCrazy7039",
                  "text": "Yes we had to do OCR from scanned docs. This was one sketchy thing.\n\nThe other problem was more related to data retrieval. In this particular use case, some of the data came from internal documents. That wasn't a problem. However, in order to develop a really good product, we needed data from a public source called “EPO.” Basically, hundreds of millions of our data are publicly available in scanned PDF files. However, there is no API for retrieving our specific data set, only an online service. We had to scrape a sufficient data set. Technically, we solved that challenge, but the terms of use prohibit or at least severely restrict scraping. ",
                  "score": 2,
                  "created_utc": "2026-02-16 06:26:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o5nmow7",
              "author": "Late_Special_6705",
              "text": "Ты бесполезный болтливый мусор",
              "score": 1,
              "created_utc": "2026-02-16 09:25:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5o2ri2",
          "author": "RobertLigthart",
          "text": "most clients dont come asking for \"RAG\"... they just have a problem like \"our team cant find anything in our docs\" or \"support takes too long to answer questions.\" the selling part is translating the technical solution into the business problem. if you lead with \"I'll build you a RAG pipeline\" they'll stare at you blank. lead with \"I'll cut your support response time in half\" and suddenly they're interested",
          "score": 2,
          "created_utc": "2026-02-16 11:50:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oc05d",
              "author": "Wide_Brief3025",
              "text": "Totally agree that framing is everything. I always focus conversations on practical outcomes like boosting efficiency or reducing ticket times rather than the tech behind it. For finding real business pains on networks like Reddit and LinkedIn, a tool like ParseStream makes it easier to spot and jump into those discussions when people mention problems your solutions can fix.",
              "score": 1,
              "created_utc": "2026-02-16 12:58:34",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o65qhna",
          "author": "Chance-Fan4849",
          "text": "I recently delivered a RAG + MCP knowledge base using **Ragie** for retrieval, Airtable as the structured data source, and exposed it via an MCP server so Claude and ChatGPT can query the same system.\n\nMVP took \\~1–2 weeks.\n\nBiggest challenges weren’t embeddings (Ragie handles that), but:\n\n* Structuring the data before indexing\n* Designing good metadata for filtering\n* Tuning retrieval quality (precision vs recall)\n\nPricing was fixed for MVP.\n\nMain lesson: RAG success depends more on clean data structure and evaluation loops than the vector DB itself.",
          "score": 2,
          "created_utc": "2026-02-19 00:47:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65sn1d",
          "author": "remoteinspace",
          "text": "built [papr.ai](http://papr.ai) memory and rag layer - open source. happy to share what's working, not working, etc. DM me ",
          "score": 1,
          "created_utc": "2026-02-19 00:59:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5obzck",
          "author": "devtechmonster",
          "text": "i dont know man.. there's already existing rag application by google which is notebookLM.. maybe if ur rag application is better than that, u can sell it.. i asked people myself if they interested in my application and most of them ask me whats the difference between the existing application like notebookLM?",
          "score": 0,
          "created_utc": "2026-02-16 12:58:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o652ztr",
              "author": "Turbulent_Principle7",
              "text": "That what I thought it seems like  NotebookLLM is the Google of the rag and document retrieval . Do you think we should still invest in building rag system , if yes who are the potential customer,",
              "score": 1,
              "created_utc": "2026-02-18 22:40:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o666343",
                  "author": "devtechmonster",
                  "text": "i think if ur company need it then its good.. some company have a lot of private documents they dont want to share to 3rd party app so they prefer using their internal rag system... other than that idk",
                  "score": 1,
                  "created_utc": "2026-02-19 02:18:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r79va5",
      "title": "SurrealDB 3.0 for multi-model RAG just launched",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r79va5/surrealdb_30_for_multimodel_rag_just_launched/",
      "author": "DistinctRide9884",
      "created_utc": "2026-02-17 15:58:44",
      "score": 21,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "SurrealDB 3.0 just dropped, with a big focus on agent memory infra for AI: improved vector indexing + better graph performance + native file storage + a WASM extension system (Surrealism) that can run custom logic/models inside the DB. You can store vector embeddings + structured data + graph context/knowledge/memory in one place.\n\nDetails: [https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memor](https://surrealdb.com/blog/introducing-surrealdb-3-0--the-future-of-ai-agent-memory)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r79va5/surrealdb_30_for_multimodel_rag_just_launched/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r3kk3b",
      "title": "Love-hate relationship with Docling, or am I missing something?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r3kk3b/lovehate_relationship_with_docling_or_am_i/",
      "author": "SkyStrong7441",
      "created_utc": "2026-02-13 09:13:23",
      "score": 18,
      "num_comments": 14,
      "upvote_ratio": 0.95,
      "text": "Docling is a great parser for pdfs (I've only tried pdfs)! With their DocumentConverter I convert my pdf to a DoclingDoc, from there I easily export it as a dict of the following format:  \n  \n{   \n schema\\_name:  \n version:   \n name:   \n origin:  \n furniture:  \n body:  // This is the order of blocks they appear in the pdf  \n groups: // This is where list items are grouped together  \n texts:  // list items and pure text blocks are found here  \n pictures:  // Pictures if I have to guess.   \n tables: // This is where all tables are found   \n key\\_value\\_items:   \n form\\_items:  \n pages:  \n}\n\nI can use texts to get any text block from the pdf.    \nI can use groups and texts together to recreate any list from the pdf.   \nWithin tables I have all the cells to recreate any table.   \nAnd with body I can piece it all together.   \n  \nThis is given that nothing is lost in the docling conversion, and for most pdfs I try this on there always is. There's always some block either missing or not part of the correct group, for example: \n\n* list items interpreted as being text block, thus not being part of a list group\n* header of a table not being interpreted as the header of that table, but as a header of the section the table lies within. So basically the table is missing a piece of information. \n\nWith my projects demand for accuracy Docling is not enough, but it's so so close! \n\nPlease tell me if there's some way to configure Docling, possibly making it convert tables differently? Or maybe there is some functionality of Docling I'am not utilizing?  \nOr maybe this is the exact problem with pdfs having different layout, and for 100% accuracy I need another approach than Docling?\n\nThank you for taking your time!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r3kk3b/lovehate_relationship_with_docling_or_am_i/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o54wlmw",
          "author": "fabkosta",
          "text": "You are not fighting Docling, you are fighting PDF. If you would try the same with Azure Document Intelligence, you'd end up with a similar experience.\n\nWhat can help (but does not always) is post-processing. You use all sorts of tricks to re-establish relationships that were missed by the OCRing tool. For example, you could use an LLM and ask it to put things together in a coherent way.",
          "score": 6,
          "created_utc": "2026-02-13 09:19:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o551rs3",
              "author": "Intelligent-Form6624",
              "text": "Azure Content Understanding is more recent than Document Intelligence. It is LLM-enhanced. I find it’s slightly better than Document Intelligence but still not perfect. I think they ought to improve on it.\n\nYou’re right, post-processing can help _a lot_. I’m in the middle of using Open AI Codex to build a pipeline to extract financial tables from PDFs.\n\nThe pipeline feeds PDF to Azure Content Understanding (prebuilt-layout), receives .md+json result, and performs significant post-processing using Gemini-2.5-Flash via VertexAI API. After a lot of work, I am beginning to see perfect results on my test documents. We’ll see how it performs on the rest of the corpus.\n\nAside from these ‘production-ready’ solution (Azure + VertexAI), I suggest seriously looking at specialised OCR VLMs and their corresponding pipeline software. Specifically; GLM-OCR, MinerU and PaddleOCR-VL.\n\nDepending on the sensitivity of your data, you may need to arrange an API endpoint for these models yourself. For example; RunPod, Azure Container App, Google Cloud Run etc",
              "score": 3,
              "created_utc": "2026-02-13 10:08:57",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o554l6i",
                  "author": "fabkosta",
                  "text": "These are really good pointers!\n\nAre your documents having a common structure, or is each document very distinct from the last one?\n\nMy experience is that if you have common docs, there are quite a few tricks you can apply. But if each document is very different from the last one, then that's going to be really hard.",
                  "score": 1,
                  "created_utc": "2026-02-13 10:35:04",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o5d1es7",
                  "author": "Alternative-Poet973",
                  "text": "You could use an OCR API for that financial table pipeline. I use Qoest's OCR API for similar document processing, and it handles the structured data extraction from PDFs pretty accurately so you can skip a lot of that post processing work",
                  "score": 1,
                  "created_utc": "2026-02-14 16:32:23",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o554ktp",
          "author": "fustercluck6000",
          "text": "Right there with you, I think a major issue is the lack of documentation, like there’s this constant feeling that it’s more than capable of achieving accuracy I need if only I could figure out how tune things just right. \n\nI highly recommend spaCy-layout—it basically adds spaCy magic on top of Docling and I’ve found it makes a noticeable difference in terms of indexing quality",
          "score": 2,
          "created_utc": "2026-02-13 10:34:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56a89c",
          "author": "vlg34",
          "text": "You can also try an LLM document parser such as Airparser for example",
          "score": 2,
          "created_utc": "2026-02-13 15:03:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56nnrk",
          "author": "One_Milk_7025",
          "text": "Docling is not the end game.. use it just to parse but there is a whole lot of post processing before ingestion begins. But yes depends upon on the need",
          "score": 1,
          "created_utc": "2026-02-13 16:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5iy0nt",
          "author": "sus4nne",
          "text": "How does your post-processing pipeline work? Which tools, which prompts?",
          "score": 1,
          "created_utc": "2026-02-15 16:04:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5k0tza",
          "author": "SkyStrong7441",
          "text": "Great feedback and insights. I'm still having an issue I'd like to show you with this example of a table (in swedish):  \nDocling parses the complete table correctly except the row header i marked in red. Docling believes that to be a section header just like \"6.2 Fribelopp\". \n\nThis is a major issue since it causes the first 3 lines to loose their context since their header is gone, plus the whole table is now under that section header marked in red. \n\nIs the solution to parse tables specifically with VLM's?\n\nhttps://preview.redd.it/o2ccxzm0jpjg1.png?width=635&format=png&auto=webp&s=82c232690b0f4cc1885c98d167ad8e721755f466\n\n",
          "score": 1,
          "created_utc": "2026-02-15 19:13:19",
          "is_submitter": true,
          "replies": [
            {
              "id": "o5qhzeb",
              "author": "Intelligent-Form6624",
              "text": "You could try Azure Document Intelligence or Azure Content Understanding as the primary document parser.\n\nSeparately, or in addition, you could try post-processing with a VLM. I find that supplying the source PDF page + the erroneous table fragment, and instructing it to repair the fragment if and as required, yields good results.\n\nYou will need a lot of time to get the post-processing right. It’s not as straightforward as it sounds. After some trial and error, you should see good results.",
              "score": 1,
              "created_utc": "2026-02-16 19:23:00",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55b4pw",
          "author": "Independent-Cost-971",
          "text": "Try [Kudra.ai](http://Kudra.ai) You won't regret it I promise. ",
          "score": -1,
          "created_utc": "2026-02-13 11:31:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o592u9p",
              "author": "Awkward-Customer",
              "text": "Using a third party for document processing often isn't an option for privacy/confidentiality reasons.",
              "score": 1,
              "created_utc": "2026-02-13 23:20:11",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r5lj2m",
      "title": "RAG for structured feature extraction from 500-700 page documents — what's your strategy?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r5lj2m/rag_for_structured_feature_extraction_from_500700/",
      "author": "Weary_Supermarket399",
      "created_utc": "2026-02-15 18:10:21",
      "score": 17,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "I'm trying to build a RAG pipeline to extract \\~50 predefined features from large tender/procurement documents (think: project name, technical specs, deadlines, payment terms, penalties, etc.). Each feature has its own set of search queries and an extraction prompt.\n\nWorks reasonably well on shorter docs (\\~80 pages). On 500-700 page documents with mixed content (specs, contracts, schedules, drawings, BOQs), retrieval quality drops hard. The right information exists, but indexing and retrieval become difficult.\n\nThis feels like a fundamentally different problem from conversational QA. You're not answering one question, you're running 50 targeted extractions across a massive document set where the answer for each could be anywhere.\n\n**For those who've built something similar:** How do you approach retrieval when the document is huge, the features are predefined, and simple semantic search isn't enough?\n\nCurious about any strategies — chunking, retrieval, reranking, or completely different architectures.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r5lj2m/rag_for_structured_feature_extraction_from_500700/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5juwz0",
          "author": "Astroa7m",
          "text": "I was experimenting with something similar by loading punch of data and extracting Q/A. \n\nWell you could try the following:\n- you have to call the LLM multiple times but make sure to chunk your data so it is within the LLM’s input limit\n- sometimes sticking to input limit is not enough as you would have hallucinations/in-complete result in your output so you could either turn off thinking tokens or take a percentage of the input token and keep reducing it gradually and see what works \n- I experimented with a lightweight compression format where I retain the meaning for LLMs by keeping only verbs, nouns, proper nouns, punctuation , numbers, and symbols. Worked great but poor with other languages depending on the POS ML model used (used spaCy NLP lib)\n\nFinally we need to aggregate all through either clustering or another llm call. \nHowever, this is only my personal experience I think there are more brilliant approaches.",
          "score": 3,
          "created_utc": "2026-02-15 18:44:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5jx32p",
          "author": "4641874165465",
          "text": "Second this. I'm running into a wall also. I'm trying to implement Graph Rag also, but I'm a noob ans Just learning and vibe coding hard on simpler Test Data, to get NER right.",
          "score": 2,
          "created_utc": "2026-02-15 18:54:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mtrkr",
              "author": "FUNdationOne",
              "text": "Try www.bildy.ai/documents\n\nI built it to solve the extraction of data from documents. Let me know how it goes for you and if you need any help.",
              "score": 0,
              "created_utc": "2026-02-16 05:08:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5kk42z",
          "author": "reallynewaccount",
          "text": "Keep track on this post. Similar problem. My case is also hardened by multiple questions like \"how many XYZ-related facts in this document\" which kills any RAG related scenario. Also my input is 200+ pages business reports pdfs made out of bad quality scans (sometimes 2-3 first symbols could be cut in each line).\nSo, if there is any reasonable solution, will be great to know it exists.",
          "score": 2,
          "created_utc": "2026-02-15 20:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5mtu1a",
              "author": "FUNdationOne",
              "text": "Try www.bildy.ai/documents\n\nI built it to solve the extraction of data from documents. Let me know how it goes for you and if you need any help.",
              "score": 0,
              "created_utc": "2026-02-16 05:08:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5p3ijb",
                  "author": "reallynewaccount",
                  "text": "Thank you. However, there are \"few little issues\" :)\n1. Security policy only let me work with local models\n2. This one (as well as many other \"big models\") only accept PDF smaller than 50MB, while mine could be 150+ (probably could be downsized, but this will affect quality, which is already not very good)\n3. Despite I usually have list of \"most popular questions\" it also should let user to ask extra for further details.\n\nSo, in other words, it would work relatively well with big effective model with huge context (and that's what I'm now look at with those new Kimi, Qwen and others), but I was just wondering if there is still any \"algorithmic\" approach to do so.",
                  "score": 1,
                  "created_utc": "2026-02-16 15:28:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5m7fwy",
          "author": "Linkman145",
          "text": "Typically documents are separated. E.g. there’s chapters segments etc\n\nIf your documents are huge then have a preprocessing step where you split them semantically. Think chapters 1-5 then another file for chapters 6-10 and so on.\n\nThen you do a multistep rag of sorts; you have a decision algorithm to decide which of the document pieces is relevant to your case and then rag over that single document\n\nSorry for no punctuation am on mobile",
          "score": 2,
          "created_utc": "2026-02-16 02:33:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5lv1af",
          "author": "SFXXVIII",
          "text": "You’ll want to look into a setup where you fan out to the pages in batches or individually to find candidate answers and then aggregate over that to find your final extractions.",
          "score": 1,
          "created_utc": "2026-02-16 01:13:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5n8e7m",
          "author": "nightman",
          "text": "Check how parsing and extracting structured data works on e.g. https://www.llamaindex.ai/\n\nThen compare with your chunk content and results and see what you need to improve.",
          "score": 1,
          "created_utc": "2026-02-16 07:10:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5nvuur",
          "author": "Unlucky_Comment",
          "text": "There's no pre-packaged solution that works, I tried graphiti, i tried before vector stores, and even with keywords mapping, it's not enough.\n\nYou need something custom appropriate for your use case, otherwise you'll hit a wall.\n\nI ended up creating a custom graph rag, it works great.\n\nAlso, depending on your budget, you might want to go self hosted, we were with GCP / Vertex and kept hitting 429s, also very expensive.\n\nWe went with self hosted and qwen / deepseek combo.",
          "score": 1,
          "created_utc": "2026-02-16 10:51:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5o06cc",
          "author": "Ecstatic_Heron_7944",
          "text": "A fave strategy of mine: **table of contents (TOC)** \\- a simple filtering approach prior to performing search.  \n  \n1) A preprocessing step to extract the table of contents section from each document. Typically first few pages.  \n2) At time of query, only expose the document titles and their TOCs either in the prompt, tools or MCP  \n3) The trick is to let the agent decide which documents (and/or pages) given are likely relevant to the user's query.  \n4) Let the agent scope and perform its RAG search filtered on the matched documents  \n5) Extend via deepsearch-like looping where the agent can repeat steps 2-4 until it finds a suitable answer\n\nThis approach differs by being top down as opposed to just diving straight into the contents and working out where you are is bottom up. Of course,  works better if the document has a TOC otherwise the alternative is parsing the documents to essentially generate your own TOC.\n\nAlso as a fellow enthusiast in handling large documents, would love some feedback on something I'm building [ragextract.com](http://ragextract.com) and how it compares to your pipeline (particularly interested in what you're doing for retrieval). Cheers!",
          "score": 1,
          "created_utc": "2026-02-16 11:29:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o65szq4",
          "author": "remoteinspace",
          "text": "you need a knowledge graph for this. At [papr.ai](http://papr.ai) developers can register schemas that we use to extract things like project name, tech specs etc. from docs. DM me and I can share what works/doesn't work for this use case",
          "score": 1,
          "created_utc": "2026-02-19 01:01:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6io8u",
      "title": "We built a local-first RAG memory engine + Python SDK (early feedback welcome)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6io8u/we_built_a_localfirst_rag_memory_engine_python/",
      "author": "DetectiveMindless652",
      "created_utc": "2026-02-16 19:06:07",
      "score": 12,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "Hey everyone,\n\nWe’ve been working on a local-first memory engine for RAG pipelines and wanted to share it here for feedback.\n\nA lot of RAG setups today rely on cloud vector databases, which works, but can add latency, cost, and operational overhead. We wanted something simpler that runs entirely locally and gives predictable retrieval for retrieval-heavy workflows.\n\nSo we built **Synrix**, plus a small Python RAG SDK on top of it.\n\nAt a high level:\n\n* Everything runs locally (no cloud dependency)\n* You can store chunks + metadata and retrieve deterministically\n* Queries scale with matching results rather than total dataset size\n* Designed for agent memory and RAG-style recall\n* Python SDK to make ingestion + querying straightforward\n\nThe RAG SDK basically handles:\n\n* ingesting documents / chunks\n* attaching metadata (source, tags, IDs, etc.)\n* querying memory for relevant context\n* returning results in a format that’s easy to feed back into your LLM\n\nWe’ve been testing on local datasets (\\~25k–100k nodes) and seeing microsecond-scale prefix lookups on commodity hardware. Benchmarks are still being formalized, but it’s already usable for local RAG experiments.\n\nGitHub is here if anyone wants to try it:  \n[https://github.com/RYJOX-Technologies/Synrix-Memory-Engine]()\n\nThis is still early, and we’d genuinely love feedback from people building RAG systems:\n\n* How are you handling retrieval today?\n* What pain points do you hit with vector DBs?\n* What would you want to see benchmarked or improved?\n\nHappy to answer questions, and thanks in advance for any thoughts 🙂",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r6io8u/we_built_a_localfirst_rag_memory_engine_python/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5tje87",
          "author": "Harotsa",
          "text": "Why are you comparing your prefix lookup to other DB’s cosine similarity search? That seems like a category error in terms of comparison, as essentially all of the major DBs have efficient prefix lookups as well. These include many DBs that are open source and can be run locally or even in process. I’d be more interested in seeing side by side comparisons of equivalent search methods against some major open source DBs.  \n\nAlso, a search scaling only with the top-k result count sounds impossible, unless you’re talking about simple hash lookup on a key. But if that’s what you’re talking about then you need to already know a priori the exact piece of data you want to lookup. And even then in practice B-trees will be more efficient for ID lookups than using a hash on large DBs (so also not independent of DB size).\n\nMaybe you can clarify your intentions, your claims, or how things work since nothing on your repo or your website really provide any explanation, technical detail, examples, or actual benchmarks/comparisons.",
          "score": 1,
          "created_utc": "2026-02-17 05:51:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5uc5mu",
              "author": "DetectiveMindless652",
              "text": "let me get back to you, you raise some good concerns. ",
              "score": 1,
              "created_utc": "2026-02-17 10:14:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r7ds0f",
      "title": "HyperspaceDB v2.0: Lock-Free Serverless Vector DB hitting ~12k QPS search (1M vectors, 1000 concurrent clients)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r7ds0f/hyperspacedb_v20_lockfree_serverless_vector_db/",
      "author": "Sam_YARINK",
      "created_utc": "2026-02-17 18:10:01",
      "score": 12,
      "num_comments": 9,
      "upvote_ratio": 0.84,
      "text": "We just released v2.0 and rewrote the engine’s hot path.\n\nThe bottleneck wasn’t algorithms.\n\nIt was synchronization.\n\nUnder high concurrency, RwLock was causing cache line bouncing and contention. So we removed it from the search path.\n\n\n\nWhat changed\n\n\\- Lock-free index access via ArcSwap\n\n\\- Work-stealing scheduler (Rayon) for CPU-bound search\n\n\\- SIMD-accelerated distance computations\n\n\\- Serverless cold-storage architecture (idle eviction + mmap cold start)\n\n\n\nBenchmark setup\n\n\\- 1M vectors\n\n\\- 1024 dimensions\n\n\\- 1000 concurrent clients\n\n\n\nSearch QPS:\n\n\\- Hyperspace v2.0 → 11,964\n\n\\- Milvus → 4,848\n\n\\- Qdrant → 4,133\n\n\n\nIngest QPS:\n\n\\- Hyperspace v2.0 → 59,208\n\n\\- Milvus → 28,173\n\n\\- Qdrant → 2,102\n\n\n\nDocker image size:\n\n→ 230MB\n\n\n\nServerless behavior:\n\n\\- Inactive collections evicted from RAM\n\n\\- Sub-ms cold wake-up\n\n\\- Native multi-tenancy via header isolation\n\n\n\nThe interesting part for us is not just raw QPS.\n\nIt’s that performance scales linearly with CPU cores without degrading under 1000 concurrent clients.\n\nNo read locks.\n\nNo global contention points.\n\nNo latency spikes.\n\n\n\nWould love feedback from people who have profiled high-concurrency vector search systems.\n\nRepo: [https://github.com/YARlabs/hyperspace-db](https://github.com/YARlabs/hyperspace-db)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r7ds0f/hyperspacedb_v20_lockfree_serverless_vector_db/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5x5ovy",
          "author": "ahaw_work",
          "text": "Could you create benchmark for smaller amount od connections and bigger amount of dimensions?\nIn what hyperapace is worse than qdrant or milvus",
          "score": 4,
          "created_utc": "2026-02-17 19:42:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5x7gd9",
              "author": "Sam_YARINK",
              "text": "We're using VectorDBBench datasets for testing, so you can pick from any of the 17 datasets in the /benchmark/ folder. Plus, we've put together our own big stress test that shows some really important numbers.",
              "score": 2,
              "created_utc": "2026-02-17 19:51:10",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o5xmgoj",
              "author": "Sam_YARINK",
              "text": "BTW, what's your case? What dimension do you need?",
              "score": 1,
              "created_utc": "2026-02-17 21:02:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o64thvr",
                  "author": "ahaw_work",
                  "text": "As I'm using qdrant for my private project and I have no performance issues I'm just curious :) ",
                  "score": 1,
                  "created_utc": "2026-02-18 21:55:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5xnd6l",
          "author": "-Cubie-",
          "text": "Nice! Can I use this with local embedding models?",
          "score": 2,
          "created_utc": "2026-02-17 21:06:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5xopqo",
              "author": "Sam_YARINK",
              "text": "Definitely yes. Local or by API. Set the embedding config in the .env file.\nRead the documentation about embedding in docs/book/src/",
              "score": 2,
              "created_utc": "2026-02-17 21:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o5yzrw3",
                  "author": "Haunting-Elephant587",
                  "text": "Apache/MIT license might have wider adoption",
                  "score": 2,
                  "created_utc": "2026-02-18 01:18:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66jaoh",
          "author": "New_Animator_7710",
          "text": "This is seriously impressive.\n\nA lot of systems blame “algorithm limits” for performance ceilings, but you went after the real culprit: synchronization. Removing read locks from the hot path is a big deal — especially at 1000 concurrent clients. That’s usually where things start falling apart.",
          "score": 1,
          "created_utc": "2026-02-19 03:36:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r6gzih",
      "title": "How do you decide to choose between fine tuning an LLM model or using RAG?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r6gzih/how_do_you_decide_to_choose_between_fine_tuning/",
      "author": "degr8sid",
      "created_utc": "2026-02-16 18:06:34",
      "score": 11,
      "num_comments": 17,
      "upvote_ratio": 0.93,
      "text": "# Hi,\n\nSo I was working on my research project. I created my knowledge base using Ollama (Llama 3). For knowledge base, I didn't fine tune my model. Instead, I used RAG and justified that it is cost effective and is efficient as compared to fine tuning. But I came across a couple of tutorials where you can fine tune models on single GPU.\n\n\n\nSo how do we decide what the best approach is? The objective is to show that it is better to RAG + system prompt, but RAG only provides extra information on top. It doesn't inherently change the nature of the LLM, especially when it comes to defending jailbreaking prompts or the scenario where you have to teach LLMs to realize the sinister prompts asking it to change its identity.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r6gzih/how_do_you_decide_to_choose_between_fine_tuning/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5q452d",
          "author": "fabkosta",
          "text": "They solve two distinct problems. If you compare them, that means you have the wrong mental model.\n\nFine-tuning is NOT to securely ingest new information into a model. Fine-tuning may or may not add new info to a model. It may not do that because the new info may collide with existing info, and then the model will behave inconsistently.\n\nRAG is the right thing that you need if you need to provide access to knowledge from your documents.\n\nSo, when is fine-tuning a good choice? Well, to give the LLM new or improved **capabilities** \\- not knowledge.",
          "score": 15,
          "created_utc": "2026-02-16 18:19:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ubvbq",
              "author": "laurentbourrelly",
              "text": "I'm doing both.\n\nUnalignment of LLM is a must.\n\nThen you can put a RAG on top.",
              "score": 1,
              "created_utc": "2026-02-17 10:11:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5q82ho",
          "author": "ggone20",
          "text": "Fine tuning is almost never the first thing you would do. What does ‘RAG’ mean here? There are endless permutations of techniques, standard and exotic, that can fall under this category. \n\nNot only that but fine tuning isn’t about retrieval so much as tone or understanding. You’re almost certainly not going to impart vast amounts of knowledge into a model, but you can make it align more with your expected outputs - be that email voice like a business/person, or classification of inputs for routing to the right ‘agent’ or tool. \n\nModels are so smart nowadays the answer is almost never fine tune and almost always context management and more robust and/or exotic retrieval techniques.",
          "score": 5,
          "created_utc": "2026-02-16 18:36:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qh7gy",
              "author": "Ryanmonroe82",
              "text": "I disagree.  Not a cloud model or open weights model on the market can touch the accuracy of a domain specific fine tuned version even with a great prompt and text embedded.  I have an 8b model now I fine tuned on over 100 million tokens that will beat any model out there on tasks specifically related to sonar physics, sound propagation/reverberation equations and applying it all to active scanning sonars while in use.  It's phenomenal actually, very impressed.",
              "score": 3,
              "created_utc": "2026-02-16 19:19:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o5qj9l6",
                  "author": "Horror-Turnover6198",
                  "text": "You’re doing physics calculations with an LLM?",
                  "score": 4,
                  "created_utc": "2026-02-16 19:29:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o5q9rts",
          "author": "DashboardNight",
          "text": "RAG for me was the way to go because it retrieves the relevant docs and can cite them directly from the knowledge base, including a link to those document and the relevant page. If answer validation is less important, I can imagine one chooses to go with finetuning a model instead.\n\nAlso, if the knowledge base changes all the time, it is better in my opinion to have a pipeline where documents get chunked and embedded directly, rather than having to retrain LLM models all the time.",
          "score": 2,
          "created_utc": "2026-02-16 18:44:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o60c2fv",
              "author": "code_vlogger2003",
              "text": "Hey there is a paper from meta where why can't we pass chunked multi dimensional Embeddings to the attention etc right?",
              "score": 1,
              "created_utc": "2026-02-18 06:19:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qg5ii",
          "author": "Ryanmonroe82",
          "text": "Hey. I just went through this and it depends on what you are needing from the text and how well a model can handle it.  For my use case (sonar theory and physics of sound) the documents helped but the llm still didn't understand what I needed exactly from the documents.  Cloud models couldnt handle it either.  So I took about 6 months and built a 140 million token dataset and trained llama3 8b-instruct on exactly what I needed.  \nNow I don't need the documents embedded and actually get more detailed responses without them influencing the models response.  \nIt's incredibly useful to fine tune a model in my opinion but it's not very intuitive and can be a chore to get it right",
          "score": 1,
          "created_utc": "2026-02-16 19:14:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5r5g5d",
          "author": "Irisi11111",
          "text": "Implementing RAG is a faster and easier to fine-tuning, especially when your case is common and can be resolved by providing accurate information to a model through multiple shots. Fine-tuning becomes necessary when dealing with private datasets, controlling output formats, behavior, or specific tool usage in agentic applications. However, fine-tuning only expands existing skillsets and doesn't introduce new abilities, so make sure identifying the true bottleneck before making any decisions.",
          "score": 1,
          "created_utc": "2026-02-16 21:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5si8n4",
          "author": "blue-or-brown-keys",
          "text": "In my experience,   \n\\- Use RAG for knowledge based tasks. Eg: Find me an answer from docs/tickets  \n\\- Use fine tuning for behaviour based tasks. Eg: Given a document always generate a JSON that looks like this",
          "score": 1,
          "created_utc": "2026-02-17 01:44:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5snbw4",
          "author": "burntoutdev8291",
          "text": "I would say fine tuning is good if you need new capabilities, like language, vocabulary and grammar. Or if you want a highly specialised LLM, think medical, legal etc.",
          "score": 1,
          "created_utc": "2026-02-17 02:14:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5v30e9",
          "author": "RobertLigthart",
          "text": "simple rule I use -> if your data changes frequently or you need source attribution, RAG. if you need the model to behave differently (tone, format, domain-specific reasoning), fine-tune. for your research project RAG makes way more sense because you can swap out documents without retraining anything. fine-tuning for jailbreak defense is a different problem entirely though... that's more about alignment and guardrails than knowledge retrieval",
          "score": 1,
          "created_utc": "2026-02-17 13:34:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5z1i7a",
          "author": "334578theo",
          "text": "If you don’t know then you likely should just work on a system prompt and add RAG if the models training knowledge is not enough ",
          "score": 1,
          "created_utc": "2026-02-18 01:27:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5qnr7p",
          "author": "Repulsive-Memory-298",
          "text": "guys wait till he finds out that nothing is mutually exclusive",
          "score": 0,
          "created_utc": "2026-02-16 19:51:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r81wg6",
      "title": "What chunking mistakes have cost you the most time to debug?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r81wg6/what_chunking_mistakes_have_cost_you_the_most/",
      "author": "Comfortable-Junket50",
      "created_utc": "2026-02-18 12:39:07",
      "score": 9,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "I've been researching RAG failure patterns for a few months and one thing kept coming up: most pipeline failures I looked at didn't trace back to the generation model. They traced back to how the data was chunked before retrieval even happened.\n\nThe pattern looks like this: vector search returns chunks that score high on relevance. The LLM generates a confident, well-formatted answer. But the answer is wrong because the chunk boundary split a piece of context that needed to stay together. Swapping in a stronger model doesn't help here. It just produces more convincing hallucinations from the same incomplete context.\n\nThree patterns that consistently helped in the systems I studied:\n\n**Parent-child chunking:** You index small child chunks for retrieval precision, but at generation time you pass the larger parent chunk so the model gets surrounding context. LlamaIndex has a good implementation of this with their `AutoMergingRetriever`. This alone caught a big chunk of \"almost right but wrong\" failures.\n\n**Hybrid retrieval (vector + BM25):** Pure embedding search misses exact-match terms. Product names, error codes, config values, specific IDs. Running BM25 keyword search alongside vector retrieval and merging results (Reciprocal Rank Fusion works well here) picks up what embeddings miss. Langchain and Haystack both support this pattern out of the box.\n\n**Self-correcting retrieval loops:** Before returning a response, the pipeline evaluates whether the answer is actually grounded in the retrieved chunks. If groundedness scores low, it reformulates the query and retries. The original Self-RAG paper by Asai et al. (2023) covers this well, and CRAG (Corrective RAG) by Yan et al. (2024) extends it further.\n\nI am just curious what others here have run into.   \nWhat has been your worst chunking or retrieval failure?   \nThe kind where everything looked fine on the retrieval side until you actually checked the output against the source documents.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r81wg6/what_chunking_mistakes_have_cost_you_the_most/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r69ion",
      "title": "Document ETL is why some RAG systems work and others don't",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r69ion/document_etl_is_why_some_rag_systems_work_and/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-16 13:24:03",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 0.81,
      "text": "I noticed most RAG accuracy issues trace back to document ingestion, not retrieval algorithms.\n\nStandard approach is PDF → text extractor → chunk → embed → vector DB. This destroys table structure completely. The information in tables becomes disconnected text where relationships vanish.\n\nBeen applying ETL principles (Extract, Transform, Load) to document processing instead. Structure first extraction using computer vision to detect tables and preserve row column relationships. Then multi stage transformation: extract fields, normalize schemas, enrich with metadata, integrate across documents.\n\nThe output is clean structured data instead of corrupted text fragments. This way applications can query reliably: filter by time period, aggregate metrics, join across sources.\n\nETL approach preserved structure, normalized schemas, delivered application ready outputs for me.\n\nI think for complex documents where structure IS information, ETL seems like the right primitive. Anyone else tried this?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r69ion/document_etl_is_why_some_rag_systems_work_and/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5ooe2y",
          "author": "Opening_Highlight241",
          "text": "Unstract(open-source, AGPL) does this i guess [https://github.com/Zipstack/unstract](https://github.com/Zipstack/unstract)",
          "score": 4,
          "created_utc": "2026-02-16 14:10:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5og3vi",
          "author": "Independent-Cost-971",
          "text": "Wrote up a more detailed explanation if anyone's interested: [https://kudra.ai/structure-first-document-processing-how-etl-transforms-rag-data-quality/](https://kudra.ai/structure-first-document-processing-how-etl-transforms-rag-data-quality/)\n\nGoes into the four ETL stages (extraction, structuring, enrichment, integration), layout-aware extraction workflows, field normalization strategies, and full production comparison. (figured it might help someone).",
          "score": 3,
          "created_utc": "2026-02-16 13:24:31",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5pblt0",
          "author": "penguinzb1",
          "text": "the etl framing makes sense. most people treat rag accuracy like a retrieval problem when it's actually a data quality problem upstream. if your chunks are corrupted from the start, no amount of fancy embedding models or reranking will fix it.\n\nthe table structure thing is brutal. i've seen rag systems answer \"what were q3 revenues\" by pulling three random numbers from different rows because the chunker turned a table into line-separated text. the relationships just vanish.\n\nwe use Veris to test these failure modes at scale. you can set up eval scenarios where the ground truth answer requires preserving table structure or cross-document joins, then see if your rag pipeline actually retrieves and surfaces the right data. turns out most naive chunking strategies fail on anything beyond single-paragraph qa.\n\ncurious how you're handling the tradeoff between structure preservation and retrieval speed. etl pipelines can get expensive if you're normalizing schemas across thousands of documents in real time.",
          "score": 2,
          "created_utc": "2026-02-16 16:07:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5ppel3",
          "author": "vlg34",
          "text": "Scanned documents are notoriously hard for RAG because OCR destroys table structure. \n\nLook into layout-aware models like LayoutLM or tools that use vision models to understand structure. \n\nAlternatively, Marker or Docling can handle this better than traditional OCR, but they're not perfect either.",
          "score": 1,
          "created_utc": "2026-02-16 17:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5q7oo5",
          "author": "bjl218",
          "text": "Most of our data is structured in spreadsheets. We have an ETL workflow that normalizes headers and data and stores the result in a SQL db. We then have a chatbot interface that does prompt to SQL. This seems to be working fairly well for the structured data. For unstructured data, I’ll probably ingest into a vector DB or OpenSearch. For structured data where we don’t know the general structure ahead of time, we’ll probably also put this in OpenSearch. Still working on that particularly",
          "score": 1,
          "created_utc": "2026-02-16 18:35:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5rqulc",
          "author": "cointegration",
          "text": "Tried using Qwen3 vl 8b, it took forever to get through the document, not a viable solution if you have thousands of docs",
          "score": 1,
          "created_utc": "2026-02-16 23:05:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o61te15",
          "author": "Informal_Tangerine51",
          "text": "100% agree, especially for tables. Once you flatten a table into text, you lose the relationships that actually answer the question (row/column, units, time periods), and retrieval can’t recover that no matter how fancy your reranker is.\n\nThe best pattern I’ve seen is “dual index”: keep a structured store (rows/fields with provenance back to page/box coords) for anything tabular, and keep a text index for narrative sections. Then your app (or agent) decides which path to query first based on the question, and you can do real filtering/aggregation without asking the LLM to do math over mangled chunks.\n\nThe tradeoff is ingestion becomes a product, not a script: you need schema versions, quality checks, and a way to detect when extraction drifted. What’s your current approach to provenance (linking a structured cell back to the exact spot in the PDF) and handling revised docs over time?",
          "score": 1,
          "created_utc": "2026-02-18 13:33:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r4f23t",
      "title": "we turned topics into APIs",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r4f23t/we_turned_topics_into_apis/",
      "author": "Key-Contact-6524",
      "created_utc": "2026-02-14 08:06:16",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.85,
      "text": "In almost every AI project we’ve built, we ended up recreating the same infrastructure.\n\nPick a topic ->\n\nScrape sources.  \nSchedule refreshes.  \nDeduplicate.  \nExtract entities.  \nEmbed.  \nTrack changes.  \nHandle webhooks.  \nRepeat.\n\nIt works.\n\nBut it’s surprisingly heavy to maintain.\n\nSo we built something simpler: treat a topic like an API endpoint.\n\n# The idea\n\nInstead of scraping websites, you subscribe to a topic by -->\n\n    POST {the endpoint which varies bw plans}/remem/create\n    {\n      \"query\": \"AI agent frameworks 2026\",\n      \"schedule\": \"daily\"\n    }\n\nThat topic becomes a persistent memory.\n\nIt refreshes automatically.  \nEach refresh creates a version.  \nYou can diff versions.  \nYou can query it anytime.\n\nYou’re not querying the web.  \nYou’re querying a continuously maintained topic.\n\n# What this replaces\n\nThe typical “scraper + RAG” stack looks like:\n\n* Custom scrapers per source\n* Cron scheduler\n* Storage\n* Deduplication logic\n* Entity extraction\n* Embedding pipeline\n* Version tracking\n* Change detection\n* Alert system\n* Webhook retries\n\nThat’s 6–10 moving parts.\n\nYou can absolutely build it.\n\nWe did. Multiple times.\n\nThe problem isn’t building it once.  \nIt’s maintaining it across projects.\n\n/Remem collapses that stack into one abstraction\n\n# Why not just use search APIs like Exa or Tavily?\n\nSearch APIs are query-time tools.\n\nThey search the web right now.\n\nThat’s useful.\n\nBut they don’t:\n\n* Persist results over time\n* Maintain version history\n* Detect changes between updates\n* Track entity-level deltas\n* Compute velocity trends\n* Send update webhooks\n* Build cross-topic semantic memory\n\nSearch is stateless.\n\nThis is stateful.\n\nSearch answers questions.  \nThis maintains evolving context.\n\nDifferent layer of the stack.\n\n# Why not just scrape + RAG?\n\nYou can.\n\nBut here’s what usually happens:\n\n1. You scrape once.\n2. You embed once.\n3. Data goes stale.\n4. You add refresh logic.\n5. Then deduplication issues.\n6. Then change detection.\n7. Then alerting.\n8. Then reliability problems.\n\nEventually you’ve rebuilt a topic monitoring system.\n\nIf you’re building one core product, that might be fine.\n\nIf you’re building multiple AI tools,  \nit becomes repetitive infrastructure.\n\nThis abstracts the maintenance layer.\n\n# What /remem actually does\n\nWhen you create a memory:\n\n* First update runs within 1 minute\n* Then updates hourly / daily / weekly\n* Each update creates a version\n* You can diff versions\n* You can search across memories\n* You can subscribe to webhooks\n\nUnder the hood:\n\n* Cross-source aggregation\n* Semantic deduplication\n* Entity extraction (no generative hallucinations)\n* Sentiment + keyword analysis\n* 768-d embeddings\n* Entity-level change detection\n* AI based velocity scoring\n* Topic health monitoring\n\nAll raw sources are preserved.\n\nNo generated summaries are injected into the core data layer.\n\n# Example\n\nTracking:\n\n\" AI coding assistants latest \"\n\nWithout this:\n\n* Scrape GitHub releases\n* Scrape blogs\n* Scrape funding news\n* Scrape Reddit\n* Merge\n* Deduplicate\n* Track deltas\n\nWith it:\n\n    POST /remem/create\n    {\n      \"query\": \"AI coding assistants\",\n      \"schedule\": \"daily\"\n    }\n\nThen:\n\n    GET /remem/mem_abc123/delta\n\nYou get:\n\n* New entities\n* Removed entities\n* Sentiment shift\n* Updated velocity\n\nAnd optionally:\n\nWebhook sends data if there is a major update\n\n# Where this is useful\n\n* Competitor monitoring\n* Industry tracking\n* VC research\n* Keeping RAG contexts fresh\n* Monitoring regulatory shifts\n* Building market intelligence tools\n\nIt’s especially useful when:\n\n* The topic evolves over time\n* You care about change detection\n* You don’t want to maintain scrapers\n\nIt’s not useful for static knowledge.\n\n# Tradeoffs\n\n* You’re abstracting away crawling control.\n* Version history is currently limited (3 stored per memory).\n* If you need deep archival storage, this isn’t that.\n* If you need highly custom scraping logic, you’ll still want your own stack.\n\nThis is optimized for continuous tracking, not infinite retention.\n\n# Pricing\n\n* Create memory: 1 credit\n* Each scheduled update: 1 credit\n* Reads + analytics: free\n\nDaily tracking ≈ 30 credits/month.\n\nIf you’re building AI systems that depend on fresh context,  \nthis removes a recurring infrastructure layer.\n\nIf you’re building AI systems that depend on fresh context,\n\nthis removes a recurring infrastructure layer.\n\nDocs and tool in comments",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r4f23t/we_turned_topics_into_apis/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o5b4b0h",
          "author": "Key-Contact-6524",
          "text": "[https://www.keirolabs.cloud/docs/remem](https://www.keirolabs.cloud/docs/remem)",
          "score": 3,
          "created_utc": "2026-02-14 08:06:37",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o5b5i48",
          "author": "Illustrious_Put9729",
          "text": "Hey I am using your product and this can be a really good feature. Will try",
          "score": 3,
          "created_utc": "2026-02-14 08:18:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5b5kfd",
              "author": "Key-Contact-6524",
              "text": "Thanks a ton",
              "score": 1,
              "created_utc": "2026-02-14 08:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6560in",
          "author": "Informal_Tangerine51",
          "text": "I get the appeal. Everyone rebuilds “topic monitoring” as soon as their RAG context needs to stay fresh, and the painful part isn’t scraping once, it’s all the drift, deduping, versioning, retries, and change detection over months.\n\nThe two things I’d want to understand before betting on it are provenance and controllability: can I see exactly which sources/URLs contributed to a given version, can I pin/ban sources, and can I reproduce a version deterministically if a customer asks “why did this change?” Also, limiting history to 3 versions feels tight if the product pitch is “diff over time” (even if raw sources are stored).\n\nThis kind of abstraction is genuinely useful, but only if it’s boringly reliable and audit-friendly. Who’s the target user: founders doing market intel, or engineers embedding this inside production workflows that need SLAs and evidence trails?",
          "score": 1,
          "created_utc": "2026-02-18 22:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b7iou",
          "author": "Wide_Brief3025",
          "text": "Keeping topic data fresh and reliable is a real headache if you are building multiple AI tools. Automating the monitoring and lead discovery part can honestly save a ton of time. If you want to track real conversations across platforms like Reddit and LinkedIn, ParseStream does this well by sending alerts for keywords you care about so you can jump in right when it matters.",
          "score": 1,
          "created_utc": "2026-02-14 08:37:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5b8na2",
          "author": "Ok_Revenue9041",
          "text": "Topic based APIs are a huge time saver if you are tired of rebuilding scrapers and pipelines. Persistent context and automated versioning really cut down on maintenance. If you want to take it a step further and get your content to show up across AI platforms, MentionDesk has a tool focused on optimizing for AI driven engines so your brand or updates actually get picked up and surfaced more reliably.",
          "score": 0,
          "created_utc": "2026-02-14 08:48:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}