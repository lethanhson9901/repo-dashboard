{
  "metadata": {
    "last_updated": "2026-01-04 05:29:58",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 158,
    "file_size_bytes": 245361
  },
  "items": [
    {
      "id": "1pzwot0",
      "title": "I made a fast, structured PDF extractor for RAG; 300 pages a second",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzwot0/i_made_a_fast_structured_pdf_extractor_for_rag/",
      "author": "absqroot",
      "created_utc": "2025-12-30 23:06:55",
      "score": 133,
      "num_comments": 52,
      "upvote_ratio": 0.97,
      "text": "**reposting because i've made significant changes and improvements; figured it's worth sharing the updated version. the post was vague and the quality and speed were much worse.**\n\n**context**: i'm a 15 yr old. was making a cybersecurity RAG tool w/ my dad (he's not a programmer). i got annoyed cause every time i changed the chunking and embedding pipeline, processing the PDFs took forever.\n\n## what this is\n\na fast PDF extractor in C using MuPDF, inspired by pymupdf4llm. i took many of its heuristics and approach but rewrote it in C for speed, then bound it to Python so it's easy to use. outputs structured JSON with full layout metadata: geometry, typography, tables, and document structure. designed specifically for RAG pipelines where chunking strategy matters more than automatic feature detection.\n\nspeed: ~300 pages/second on CPU. no GPU needed. 1 million pages in ~55 minutes.\n\n## the problem\n\nmost PDF extractors give you either raw text (fast but unusable) or over-engineered solutions (slow, opinionated, not built for RAG). you want structured data you can control; you want to build smart chunks based on document layout, not just word count. you want this fast, especially when processing large volumes.\n\nalso, chunking matters more than people think. i learnt that the hard way with LangChain's defaults; huge overlaps and huge chunk sizes don't fix retrieval. better document structure does.\n\n**yes, this is niche. yes, you can use paddle, deepseekocr, marker, docling. they are slow. but ok for most cases.**\n\n## what you get\n\nJSON output with metadata for every element:\n\n```json\n{\n  \"type\": \"heading\",\n  \"text\": \"Step 1. Gather threat intelligence\",\n  \"bbox\": [64.00, 173.74, 491.11, 218.00],\n  \"font_size\": 21.64,\n  \"font_weight\": \"bold\"\n}\n```\n\ninstead of splitting on word count, use bounding boxes to find semantic boundaries. detect headers and footers by y-coordinate. tables come back with cell-level structure. you control the chunking logic completely.\n\n## comparison\n\n| Tool | Speed (pps) | Quality | Tables | JSON Output | Best For |\n|------|-------------|---------|--------|-------------|----------|\n| pymupdf4llm-C | ~300 | Good | Yes | Yes (structured) | RAG, high volume |\n| pymupdf4llm | ~10 | Good | Yes| Markdown | General extraction |\n| pymupdf (alone) | ~250 | Subpar for RAG | No | No (text only) | basic text extraction\n| marker | ~0.5-1 | Excellent | Yes | Markdown | Maximum fidelity |\n| docling | ~2-5 | Excellent | Yes | JSON | Document intelligence |\n| PaddleOCR | ~20-50 | Good (OCR) | Yes | Text | Scanned documents |\n\n**the tradeoff:** speed and control over automatic extraction. marker and docling give higher fidelity if you have time; this is built for when you don't.\n\n## what it handles well\n\n- high volume PDF ingestion (millions of pages)\n- RAG pipelines where document structure matters for chunking\n- custom downstream processing; you own the logic\n- cost sensitive deployments; CPU only, no expensive inference\n- iteration speed; refine your chunking strategy in minutes\n\n## what it doesn't handle\n\n- scanned or image heavy PDFs (no OCR)\n- 99%+ accuracy on complex edge cases; this trades some precision for speed\n- figues or image extraction\n\n## why i built this\n\ni used this in my own RAG project and the difference was clear. structured chunks from layout metadata gave way better retrieval accuracy than word count splitting. model outputs improved noticeably. it's one thing to have a parser; it's another to see it actually improve downstream performance.\n\n## links\n\nrepo: [https://github.com/intercepted16/pymupdf4llm-C](https://github.com/intercepted16/pymupdf4llm-C)\n\npip: `pip install pymupdf4llm-C` ([https://pypi.org/project/pymupdf4llm-C](https://pypi.org/project/pymupdf4llm-C))\n\nnote: prebuilt wheels from 3.9 -> 3.14 (inclusive) (macOS ARM, macOS x64, Linux (glibc > 2011)). no Windows. pain to build for.\n\ndocs and examples in the repo. would love feedback from anyone using this for RAG.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1pzwot0/i_made_a_fast_structured_pdf_extractor_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwtimj3",
          "author": "Synyster328",
          "text": "What trade offs were made to get this performance?",
          "score": 6,
          "created_utc": "2025-12-30 23:17:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtjyl9",
              "author": "absqroot",
              "text": "If we're talking trade-offs in comparison to PyMuPDF4LLM:\n\nNot as much as you'd think.\n\nThe reason for PyMuPDF4LLM being so slow wasn't due to its quality. It was a terrible codebase. Inefficient O(n\\^2), looping, raw numbers in Python, pretty much just bad code and a bad language for lots of maths.\n\nThis isn't a trade-off of the project itself, but there may still be minor cases where I haven't 100% copied the heuristics.\n\nIf we're talking about trade-offs in comparison to tools like Paddle, Marker & Docling:\n\nIt does not do any fancy ML. It's just some basic geometric math. Therefore it won't handle:\n\n\\- scanned pages; no OCR  \n\\- complex tables or tables without some form of edges  \n\\- stuff like that; etc.",
              "score": 10,
              "created_utc": "2025-12-30 23:24:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwu3ir6",
                  "author": "Synyster328",
                  "text": "Gotcha, thanks for explaining it. Parsing PDFs is the bane of my existence, or at least was at one point. I certainly don't want any AI magic sitting between the original source doc and my app, since my app is the one adding the AI bs I want the source to be as pure as possible. Unfortunately with PDFs, pure doesn't mean good quality. But I digress.\n\nI would def be comparing this to something like PyMuPDF which is my usual go-to.",
                  "score": 2,
                  "created_utc": "2025-12-31 01:13:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvtvin",
          "author": "Guboken",
          "text": "This kind of thread brings in the professionals so I have to ask, what is the most accurate pipeline to read pdfs? I rather get 1%+ accuracy and it takes a minute longer than go for speed. I‚Äôm thinking vision VL secondary step to validate perhaps?",
          "score": 4,
          "created_utc": "2025-12-31 08:29:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwtjlil",
          "author": "Repulsive-Memory-298",
          "text": "How do people use bounding boxes to find semantic boundaries",
          "score": 3,
          "created_utc": "2025-12-30 23:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtka9x",
              "author": "absqroot",
              "text": "good question. basically, layout reveals structure.\n\nlarge gaps in y-coordinates often mean topic breaks. if one element ends at y=150 and the next starts at y=200, that's probably a section boundary worth chunking on.\n\nfont size changes are obvious; heading at 21pt followed by body text at 12pt usually means new section. indentation changes (x-coordinate) show hierarchy; bullet points belong with their parent, not split arbitrarily.\n\nwidth changes can indicate sidebars or different content streams. tables have consistent cell structure, so you chunk the whole table as one unit instead of breaking it apart.\n\nthe key difference from word count splitting: word count just breaks whenever you hit N words, regardless of meaning. you might split mid-sentence. bounding boxes let you chunk at natural document boundaries.",
              "score": 5,
              "created_utc": "2025-12-30 23:26:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwtqtb9",
                  "author": "Repulsive-Memory-298",
                  "text": "interesting. And what are your thoughts on something like GROBID, assuming you have suitable models for your document domains",
                  "score": 0,
                  "created_utc": "2025-12-31 00:02:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwbp9q",
          "author": "my_byte",
          "text": "Neat. The caveat is that it's only going to work on simple PDFs. The reason why we need slow stuff with OCR or VLMs is that PDF is a horrible abomination allowing for maximum contol over layout - it was designed for print. I had to write a tool to patch PDF contents once and I kid you not - whatever software they used to create these docs positioned each character on the page individually thru X/Y coords.\nThis makes document extraction at scale - where your solution would be extremely useful - very tricky. Arguably at scale, you don't really know what kind of docs you're going to encounter. And with information retrieval, people kinda care about 1%. Would be neat if some of the libraries with adaptive parsing took what you've built and used it for the first pass. That might result in much better overall throughput",
          "score": 2,
          "created_utc": "2025-12-31 11:17:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwehw9",
          "author": "EveYogaTech",
          "text": "Sounds awesome, but we cannot use it üò≠ because of GNU AFFERO General license (we don't want to force others to open-source every project or SaaS).\n\nMIT, BSD or Apache2 would fix this for us at /r/Nyno",
          "score": 2,
          "created_utc": "2025-12-31 11:42:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6rdje",
              "author": "absqroot",
              "text": "yeah, i agree, it's not great.\n\nnot saying you should for my tool, but this might help you in the future:\n\nif you really want to, you can host the GPL licensed project separately and expose the source code of that, then call it via API.\n\nor, you can just pay the company in question.",
              "score": 1,
              "created_utc": "2026-01-02 02:44:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtiif0",
          "author": "JDubbsTheDev",
          "text": "Damn those speeds are crazy, definitely checking this out",
          "score": 1,
          "created_utc": "2025-12-30 23:16:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtkccl",
              "author": "absqroot",
              "text": "thanks!",
              "score": 1,
              "created_utc": "2025-12-30 23:26:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtkupr",
          "author": "One-Claim8561",
          "text": "I am building a RAG for comparing companies annual reports. Do you think will suit my goal? For me having a structured file that also recognizes footnote is crucial",
          "score": 1,
          "created_utc": "2025-12-30 23:29:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtl9zf",
              "author": "absqroot",
              "text": "Honestly, if you‚Äôre not processing that much documents and it‚Äôs not like a chatbot where you want live responses, no. I‚Äôd recommend the higher fidelity but slower options.\n\nMy tool isn‚Äôt as good as those with stuff like footnotes, for example.",
              "score": 1,
              "created_utc": "2025-12-30 23:32:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtltxt",
          "author": "One-Claim8561",
          "text": "Thank you for the clarification!",
          "score": 1,
          "created_utc": "2025-12-30 23:35:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui8cd",
              "author": "absqroot",
              "text": "no problem",
              "score": 1,
              "created_utc": "2025-12-31 02:38:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtmw8w",
          "author": "polandtown",
          "text": "very cool - the major concern for me personally is no-ocr but if I was confident all my pdfs were properly scanned/generated beforehand I'd love to use this option. \n\nGreat work!",
          "score": 1,
          "created_utc": "2025-12-30 23:41:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwui5xx",
              "author": "absqroot",
              "text": "thanks! as for OCR, it's a design choice. i decided to keep it simple, ocr will ruin the speed benefits and so it'd just be the same as the others.",
              "score": 1,
              "created_utc": "2025-12-31 02:38:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuwwoe",
                  "author": "AffectionateCap539",
                  "text": "very rational decision. i have been obsessed with ocr for a while  and i must say that this is totally a brand new domain to evolve. complex",
                  "score": 1,
                  "created_utc": "2025-12-31 04:08:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtnymk",
          "author": "mysterymanOO7",
          "text": "Definitely excellent work, congratulations! This would have been really useful only if it had OCR support. Is there a way to use it with RapidOCR?",
          "score": 1,
          "created_utc": "2025-12-30 23:47:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuhze5",
              "author": "absqroot",
              "text": "Yeah, that could be implemented. However, that would probably compromise most of the speed benefit. it is a design choice that I did not add it. I think if you want OCR, just go with the heavier but slow libraries; they're designed for it.",
              "score": 1,
              "created_utc": "2025-12-31 02:36:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwtuyb3",
          "author": "OnyxProyectoUno",
          "text": "You built exactly what's missing. Most people are stuck with either pymupdf's raw text or waiting forever for marker to crawl through documents. 300 pages per second with structured output changes the game completely.\n\nThe JSON metadata approach is spot on. Bounding boxes let you chunk on actual document structure instead of arbitrary word counts. I see too many RAG systems failing because they're splitting mid-paragraph or breaking up tables. Your bbox-based chunking fixes that fundamental problem.\n\nOne thing I'd test is how well the font weight detection works across different PDF generators. Some tools embed fonts weirdly and \"bold\" doesn't always mean what you think. But for high-volume processing where you need good-enough extraction fast, this hits the sweet spot.\n\nThe comparison table tells the story. Marker gives you perfection at 0.5 pages per second. You're giving 80% of that quality at 600x the speed. For most RAG use cases, that math works out perfectly. When you're processing millions of pages, waiting 55 minutes instead of 55 hours matters more than catching every edge case.\n\nTesting different chunking strategies on the structured output, something vectorflow.dev handles well, would help you dial in the optimal approach for your specific documents. The metadata gives you enough control to experiment with semantic boundaries versus fixed sizes.",
          "score": 1,
          "created_utc": "2025-12-31 00:25:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwuj28q",
              "author": "absqroot",
              "text": "thanks, really appreciate this. you're right on the font weight thing, some PDFs are weird about it. it's something i've noticed but haven't fully tested across different generators yet.\n\nthe math on marker vs this (80% quality at 600x speed) is exactly the trade-off i was going for. for most RAG pipelines processing millions of pages, that's the right call.\n\nas for [vectorflow.dev](http://vectorflow.dev), i've seen it in other threads, but I don't understand what exactly it is?",
              "score": 1,
              "created_utc": "2025-12-31 02:43:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwuqieh",
                  "author": "OnyxProyectoUno",
                  "text": "VectorFlow is a RAG pipeline platform that handles the chunking, embedding, and ingestion side after you extract the structured data. Think of it as the layer between your PDF extractor and your vector database.\n\nThe font weight issue is trickier than most people realize. Adobe's PDF spec allows fonts to be embedded in about six different ways, and \"bold\" can be a font name, a weight property, or just how the renderer decides to display it. I've seen PDFs where the same visual boldness gets tagged three different ways in the metadata.\n\nYour 80/20 approach is dead right for most cases. The only time I'd reach for marker over your tool is when I'm dealing with academic papers with complex equations or heavily formatted financial documents where every table cell matters. For standard business documents, technical manuals, reports - your speed advantage wins every time.\n\nThe bbox chunking strategy you're enabling is underrated. Most people chunk on token counts and wonder why their retrieval sucks. When you can say \"everything between y-coordinates 150 and 400 is one logical section\" you get much cleaner semantic boundaries. Tables stay intact, headers don't get orphaned from their content.\n\nHave you tested how it handles PDFs with multiple column layouts? That's usually where bbox-based approaches either shine or fall apart completely.",
                  "score": 2,
                  "created_utc": "2025-12-31 03:27:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuofcx",
          "author": "Ok-Attention2882",
          "text": "I like how you tried to Un-ChatGPT-ify the post not by changing the language, but by making everything lowercase.",
          "score": 1,
          "created_utc": "2025-12-31 03:14:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwurdca",
              "author": "absqroot",
              "text": "it‚Äôs so hard NOT to get blamed for using ai man.\nBut I‚Äôd like to know how I can improve my writing then if it‚Äôs so AI like?",
              "score": 1,
              "created_utc": "2025-12-31 03:32:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwwjhz0",
                  "author": "Ok-Attention2882",
                  "text": "There's actually no way you thought that reverse psychology crap would work here.",
                  "score": 1,
                  "created_utc": "2025-12-31 12:23:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwuxda5",
          "author": "AffectionateCap539",
          "text": "can this deal with pdf with diverse layouts? like cv, research paper etc...",
          "score": 1,
          "created_utc": "2025-12-31 04:11:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvjhyj",
          "author": "Lanky-Cobbler-3349",
          "text": "No OCR? Docling excellent? lol",
          "score": 1,
          "created_utc": "2025-12-31 06:54:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvm301",
              "author": "absqroot",
              "text": "I didn‚Äôt fully test docling. I heard it was as good as the others I might be wrong.\n\nNo OCR is a design choice I chose for this tool.",
              "score": 0,
              "created_utc": "2025-12-31 07:16:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwvosms",
                  "author": "Lanky-Cobbler-3349",
                  "text": "Sorry read my comment again. It sounds kind of mean. It should be easy to fix the OCR issue. In general the whole topic is just a little frustrating as all these libraries are not satisfying. It feels wrong that LLMs are better at extracting text from pdf, docx etc. while preserving structure if you provide a decent prompt than those libraries.",
                  "score": 1,
                  "created_utc": "2025-12-31 07:41:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvkat7",
          "author": "Think-Draw6411",
          "text": "Did you run any benchmarks testing the performance ? If docling is excellent it‚Äôs worrying",
          "score": 1,
          "created_utc": "2025-12-31 07:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvlyod",
              "author": "absqroot",
              "text": "I didn‚Äôt fully test docling to be honest I was judging off what I heard.",
              "score": 1,
              "created_utc": "2025-12-31 07:15:52",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwx0spr",
                  "author": "Think-Draw6411",
                  "text": "Thanks for the reply ! Which systems did you test and which benchmark scores did you achieve with your fast system ?",
                  "score": 1,
                  "created_utc": "2025-12-31 14:17:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx28ztn",
              "author": "TaurusBlack16",
              "text": "I am currently using docling and I didn't really have any issue with it. If you don't mind can you explain why docling being good is worrying. Cuz it does a pretty good job with table extraction, and header-body detection. What other features are usually considered when testing pdf2markdown tools or similar stuff. I am sorry if this sounded rude but I am just trying to learn.",
              "score": 1,
              "created_utc": "2026-01-01 10:35:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwvno6o",
          "author": "Cladser",
          "text": "I have a project that am working on using mxbai locally for semantic splitting but it essentially prevents parallelisation.  This looks pretty interesting - is there a way to grab the preceding title as meta data?  For me I want to know if this chunck is from the introduction, rationale or somewhere else.",
          "score": 1,
          "created_utc": "2025-12-31 07:31:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6rky4",
              "author": "absqroot",
              "text": "it doesn't automatically provide that, but it's pretty easy to parse that yourself; all the data is in your hands.",
              "score": 2,
              "created_utc": "2026-01-02 02:45:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxfdbs6",
                  "author": "Cladser",
                  "text": "So I‚Äôve been testing this and for my use case (scientific reports with specific section) it fabulous. It‚Äôs is also very fast. 2700-ish 30+page pdf in 6-7 minutes with semantic sections. I said previously I was using a local model to infer sections and it was slow and less accurate than this. \nI take my hat off to you sir or ma‚Äôam (hard to tell from your username) ‚Ä¶ and honestly I‚Äôm not even a hat person",
                  "score": 1,
                  "created_utc": "2026-01-03 12:11:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwvs1rs",
          "author": "AsparagusKlutzy1817",
          "text": "PyMuPDF is by far the best PDF library out there but the underlying c-implementation mupdf has a tricky license for companies. I remember conversations with legal pushing us to replace it because the license was problematic. I wish someone would build a full apache or MIT license mupdf :)\n\nDid you have an actual throughput challenge with PDF extraction or why did you build it ?",
          "score": 1,
          "created_utc": "2025-12-31 08:12:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvu1pv",
              "author": "absqroot",
              "text": "I'll be honest here.  \nI didn't have an actual throughput challenge.  \ni was just helping my dad out, he wanted to make a cybersecurity agent for companies using RAG.  he didn't know programming but he had relations. and i was just annoyed that everything took forever.  \npymupdf was too raw, pymupdf4llm annoyed me, others were DEAD slow.\n\nthen i looked at its code and then realised this is absolute garbage code in terms of optimization.",
              "score": 1,
              "created_utc": "2025-12-31 08:31:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwwv8tz",
          "author": "drink_with_me_to_day",
          "text": "Would be nice as a duckdb extension\n\nJust grab the https://github.com/duckdb/extension-template, add your C source in a subfolder and ask Copilot to create a scalar function and register a replacement for .pdf files\n\n    TableFunction table_func(\"mupdf4llm\",\n        {LogicalType::VARCHAR},\n        extract_func,\n        extract_bind,\n        extract_init\n    );\n    ExtensionUtil::RegisterFunction(instance, table_func);\n\n    // Register the replacement scan for .pdf files\n    instance.config.replacement_scans.emplace_back(replacement_func);\n\nThen use as `SELECT * FROM mupdf4llm('s3://some-butcket/somefile.pdf')`",
          "score": 1,
          "created_utc": "2025-12-31 13:44:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwy45al",
          "author": "Straight-Gazelle-597",
          "text": "interesting. 300 pps is amazing.",
          "score": 1,
          "created_utc": "2025-12-31 17:39:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx287jb",
          "author": "PeaAccurate5302",
          "text": "This sounds amazing - I will pitch it in our team to try in a huge RAG project where we are struggling with extraction cost‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-01 10:27:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7fomt",
          "author": "Ash_It_98",
          "text": "Great work.",
          "score": 1,
          "created_utc": "2026-01-02 05:22:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxaxhwp",
          "author": "No-Neighborhood-7229",
          "text": "Sounds cool. Would be great if you also added transformation to md",
          "score": 1,
          "created_utc": "2026-01-02 19:11:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbreaj",
              "author": "absqroot",
              "text": "Yeah, I‚Äôll work on what, but should not be too difficult to convert to other formats, only like 50 lines.",
              "score": 1,
              "created_utc": "2026-01-02 21:36:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxea60r",
          "author": "stevevaius",
          "text": "Windows version could be a game changer for me at least. Congratulations",
          "score": 1,
          "created_utc": "2026-01-03 06:39:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxf27xw",
          "author": "aieatstheworld",
          "text": "This is solid. The focus on layout-level signals (bbox, font size/weight, coordinates) is exactly what most RAG pipelines miss when they jump straight to word-count chunking.\n\nThe speed numbers check out for the design choices, and CPU-only ingestion is a big deal when you‚Äôre iterating or processing at scale. Having structured, deterministic output that lets you own the chunking logic is way more useful than opinionated ‚Äúsmart‚Äù extractors.\n\nI can see this being useful to many in edge cases.\n\nNice work!",
          "score": 1,
          "created_utc": "2026-01-03 10:39:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhc5x1",
          "author": "kacisse",
          "text": "Man that looks nice, I myself had to make a crazy gas factory to use rag on pdf. I had to use huggingface models + LLM serializing.\nDoes yours handle all types of table (including nested elements) ? That was the most challenging.\nIm super interested in using your project.\nGood job !",
          "score": 1,
          "created_utc": "2026-01-03 18:27:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxhdbss",
              "author": "kacisse",
              "text": "Ah unfortunately your project won't fit my C# stack I think. But I'm curious to see how it performs",
              "score": 1,
              "created_utc": "2026-01-03 18:32:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q11yc5",
      "title": "I rebuilt my entire RAG infrastructure to be 100% EU-hosted and open-source, here's everything I changed",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q11yc5/i_rebuilt_my_entire_rag_infrastructure_to_be_100/",
      "author": "ahmadalmayahi",
      "created_utc": "2026-01-01 11:15:01",
      "score": 56,
      "num_comments": 29,
      "upvote_ratio": 0.87,
      "text": "Wanted to share my journey rebuilding a RAG-based AI chatbot platform ([chatvia.ai](https://chatvia.ai)) from scratch to be fully EU-hosted with zero US data processing. This turned out to be a much bigger undertaking than I expected, so I thought I'd document what I learned.\n\n**The catalyst**\n\nTwo separate conversations killed my original approach. A guy at a networking event asked \"where is the data stored?\" I proudly said \"OpenAI, Claude, you can pick!\" He walked away. A week later, a lawyer told me straight up: \"We will never feed client cases to ChatGPT or any US company due to privacy concerns\".\n\nThat was my wake-up call. The EU market REALLY cares about data sovereignty, and it's only getting stronger.\n\n**The full migration**\n\nHere's what I had to replace:\n\n\n\n|Component|Before|After|\n|:-|:-|:-|\n|LLMs|GPT-4, Claude, Gemini, etc...|Llama 3.3 70B, Qwen3 235B, DeepSeek R1, Mistral Nemo, Gemma 3, Holo2|\n|Embeddings|Cohere|Qwen-embedding (seriously impressed by this)|\n|Re-ranking|Cohere Rerank|RRF (Reciprocal Rank Fusion)|\n|OCR|LlamaParse|Mistral OCR|\n|Object Storage|AWS S3|Scaleway (French)|\n|Hosting|AWS|Hetzner (German)|\n|Vector DB|\\-|VectorChord (self-hosted on Hetzner)|\n|Analytics|Google Analytics|Plausible (EU)|\n|Email|Sender|Scaleway|\n\n**On ditching Cohere Rerank for RRF**\n\nThis was the hardest trade-off. Cohere's reranker is really good, but I couldn't find an EU-hosted alternative that didn't require running my own inference setup. So I went with RRF instead.\n\nFor those unfamiliar: RRF (Reciprocal Rank Fusion) merges multiple ranked lists (e.g., BM25 + vector search) into a unified ranking based on position rather than raw scores. It's not as sophisticated as a neural reranker (such as Cohere Re-reanker), but it's surprisingly effective when you're already doing hybrid search.\n\n**Embedding quality**\n\nSwitching from Cohere to Qwen-embedding was actually a pleasant surprise. The retrieval quality is comparable, and having it run on EU infrastructure without vendor lock-in is a huge win. I'm using the 8B parameter version.\n\n**What I'm still figuring out**\n\n* Better chunking strategies, currently experimenting with semantic chunking using LLMs to maintain context (I already do this with website crawling).\n* Whether to add a lightweight reranker back (maybe a distilled model I can self-host?)\n* Agentic document parsing for complex PDFs with tables/images\n\n**Try it out**\n\nIf you want to see the RAG in action:\n\n* **ChatGPT-style knowledge base:** [help.chatvia.ai](https://help.chatvia.ai)  this is our docs trained as a chatbot\n* **Embeddable widget:** [chatvia.ai](https://chatvia.ai)  check the bottom-right corner\n\n**Future plans**\n\nI'm planning to gradually open-source the entire stack:\n\n* Document parsing pipeline\n* Chat widget\n* RAG orchestration layer\n\nThe goal is to make it available for on-premise hosting.\n\nAnyone else running a fully EU-hosted RAG stack? Would love to compare notes on what's working for you.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q11yc5/i_rebuilt_my_entire_rag_infrastructure_to_be_100/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx2g6na",
          "author": "iamkucuk",
          "text": "Rrf is not a reranker. It‚Äôs a method for fusing different retrievals. Completely different things.",
          "score": 8,
          "created_utc": "2026-01-01 11:50:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxbw3d7",
              "author": "TechnicalGeologist99",
              "text": "Thank you came here to say this",
              "score": 1,
              "created_utc": "2026-01-02 21:59:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2f0zu",
          "author": "Nshx-",
          "text": "its not open source",
          "score": 5,
          "created_utc": "2026-01-01 11:38:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2g0hp",
              "author": "automata_n8n",
              "text": "Using open source models, doesn't mean it's open source.",
              "score": 10,
              "created_utc": "2026-01-01 11:48:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxf0mak",
                  "author": "LilPsychoPanda",
                  "text": "*open weights",
                  "score": 1,
                  "created_utc": "2026-01-03 10:26:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx2fuly",
              "author": "ahmadalmayahi",
              "text": "Open-source models :-)",
              "score": -3,
              "created_utc": "2026-01-01 11:46:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx2kng2",
          "author": "_xXKaiserXx_",
          "text": "Have you seen a difference on the price?",
          "score": 2,
          "created_utc": "2026-01-01 12:32:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx35gyg",
          "author": "HatEducational9965",
          "text": "Thank you for sharing! Where do you host the models, which inference engine?",
          "score": 2,
          "created_utc": "2026-01-01 15:04:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx3704f",
              "author": "ahmadalmayahi",
              "text": "I use scaleway (french cloud provider).",
              "score": 2,
              "created_utc": "2026-01-01 15:13:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx379zb",
          "author": "skibare87",
          "text": "AWS has the EU Sovereign cloud now, not really necessary to replace it",
          "score": 2,
          "created_utc": "2026-01-01 15:15:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxafhqg",
              "author": "GeroldM972",
              "text": "Physical location isn't the real problem here. Legal location is. No matter if AWS hosts in the EU or not, AWS is an US company, headquartered in the US so subject to US law. And there are laws over there that allow the US to retrieve EU data from EU-based locations without AWS needing to inform the EU company that they did.\n\nActually, that US company is forbidden to do so, else this US company will be subject to severe (administrative) punishment in the US.\n\nSo yes, replacement is becoming a necessity.\n\nGiven that Trump has shown to swivel his stance on almost anything in a moments notice in 2025, all other countries in the world deem the US government to be unstable. Dealing with an unstable US government is a headache for every and any non-US company and country. And it is deemed to be 'not worth the headache anymore\" in the EU. And I'm quite sure that many other countries on other continents feel the same way. They may not (yet) officially proclaim it, but they sure do. That is the true force behind what is called 'soft power'. \n\nResults from this EU stance will not hit the US in 2026 or 2027, but in 2028/2029 the VS will start to notice. Because the US was making a significant sum of money from cloud-services in the EU. And because levels of trust between the US and EU have been on a steady decline since Trump 1 and freefall since Trump 2 the tech industry in the US will start to hurt financially.   \n  \nIsn't industrial/political inertia a nice thing...",
              "score": 1,
              "created_utc": "2026-01-02 17:48:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxej29f",
                  "author": "Aggressive_Bed2609",
                  "text": "Legal location is definitely key. Even with EU cloud options, a lot of companies are still wary about data sovereignty due to potential US influence. Better safe than sorry when handling sensitive data!",
                  "score": 1,
                  "created_utc": "2026-01-03 07:54:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxaw20m",
                  "author": "skibare87",
                  "text": "That is not how the EU sovereign cloud works. You should look it up. All operations happen in the EU and is not connected to the US. It is completely separate and launches Jan 15. Im not talking FRA, but THF eusc-de-east-1 which is the first sovereign cloud for AWS. There are already have large partners starting to move to that partition due to the protections and EU only control.",
                  "score": 0,
                  "created_utc": "2026-01-02 19:04:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5sqgq",
          "author": "StartX007",
          "text": "OP - you are confusing people by calling what you built as Open Source. It is not open source and may be using Open Source model (open sourced by someone else). That does not make your solution open source.",
          "score": 2,
          "created_utc": "2026-01-01 23:21:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf0vu3",
              "author": "LilPsychoPanda",
              "text": "OpenAI? ü§îü§ì",
              "score": 1,
              "created_utc": "2026-01-03 10:28:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfrlun",
                  "author": "StartX007",
                  "text": "Your call - what I see is that you really built an EU based service using local (EU) hosted LLM which could be Open AI compatible models.",
                  "score": 1,
                  "created_utc": "2026-01-03 13:48:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx2jozq",
          "author": "Gondor14",
          "text": "Mistral OCR is hosted on Azure I beleive : /\n\nWhy no OVHcloud ?",
          "score": 3,
          "created_utc": "2026-01-01 12:23:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx2wfkm",
              "author": "sleepydevs",
              "text": "Yeah I'm a huge ovh fan for my personal servers. We're contemplating moving some of our backend infra to ovh (from AWS) for similar reasons to the ops.",
              "score": 1,
              "created_utc": "2026-01-01 14:04:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ji5j",
          "author": "macthom",
          "text": "Fascinating, ty for sharing üëç",
          "score": 1,
          "created_utc": "2026-01-01 16:22:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx3lix4",
          "author": "cryptoviksant",
          "text": "You RAG it's solely based on doc chunks? Or has something else that improves the response's quality?",
          "score": 1,
          "created_utc": "2026-01-01 16:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx5f0j0",
          "author": "KvAk_AKPlaysYT",
          "text": "GitHub?",
          "score": 1,
          "created_utc": "2026-01-01 22:07:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6tw0v",
          "author": "fustercluck6000",
          "text": "Was there anything in particular about switching geographies that was different from running things  locally in an air-gapped environment?",
          "score": 1,
          "created_utc": "2026-01-02 02:59:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx6vqf9",
          "author": "OnyxProyectoUno",
          "text": "Most people underestimate how much work goes into replacing the entire preprocessing stack when they migrate infrastructure. Your embedding switch working out is huge because that's where most quality degradation happens during migrations.\n\nThe chunking experiments you mentioned are where I'd focus next. Semantic chunking with LLMs can work well but gets expensive fast when you're processing volume. What I've seen work better is hybrid approaches where you do initial structural chunking based on document hierarchy, then use the LLM selectively for edge cases like tables or complex sections. Keeps costs manageable while handling the tricky parts.\n\nFor the reranker question, you might want to look at BGE or similar models you can self-host. They're not Cohere-level but significantly better than pure RRF, especially when your retrieval is pulling from heterogeneous document types. The performance hit from RRF usually shows up when you have mixed content formats where position-based fusion breaks down.\n\nYour agentic parsing mention caught my attention because that's exactly the kind of pipeline complexity that becomes a nightmare to debug when something goes wrong. Document processing pipelines fail silently and you don't discover the issues until retrieval starts returning garbage. I work on tooling for this exact problem at vectorflow.dev, specifically around making document preprocessing visible before it hits your vector store.\n\nWhat's your current approach for validating that parsed content actually looks right before embedding? Most people discover their PDF tables got mangled only after users complain about weird responses.",
          "score": 1,
          "created_utc": "2026-01-02 03:11:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx74zdo",
          "author": "Flamenverfer",
          "text": "Github where?",
          "score": 1,
          "created_utc": "2026-01-02 04:10:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx8to1p",
          "author": "ggone20",
          "text": "You can still use OpenAI with Azure. No reason to give up the best intelligence provider. I work with several EU clients and was pleasantly surprised how easy of a switch this was.",
          "score": 1,
          "created_utc": "2026-01-02 12:43:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9ynxq",
          "author": "AloneSYD",
          "text": "You can try bge m3 reranker or gte multilingual base",
          "score": 1,
          "created_utc": "2026-01-02 16:29:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx2wvci",
          "author": "Straight-Gazelle-597",
          "text": "And not sure that self-hosted Chinese models will be compliant with EU authorities. Don't get me wrong, we are all for open-source models but the compliance is the most tricky thing in the world. üò≠",
          "score": 1,
          "created_utc": "2026-01-01 14:07:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx36wg3",
              "author": "ahmadalmayahi",
              "text": "You can easily choose the model you want .. however I don‚Äôt really understand the issue here.",
              "score": 2,
              "created_utc": "2026-01-01 15:12:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pxom6i",
      "title": "[OpenSource | pip ] Built a unified PDF extraction & benchmarking tool for RAG ‚Äî PDFstract (Web UI ‚Ä¢ CLI ‚Ä¢ API)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pxom6i/opensource_pip_built_a_unified_pdf_extraction/",
      "author": "GritSar",
      "created_utc": "2025-12-28 10:52:22",
      "score": 31,
      "num_comments": 16,
      "upvote_ratio": 0.97,
      "text": "I‚Äôve been experimenting with different PDF ‚Üí text/markdown extraction libraries for RAG pipelines, and I found myself repeatedly setting up environments, testing outputs, and validating quality across tools.\n\nSo I built **PDFstract** ‚Äî a small unified toolkit that lets you:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract) \n\n* upload a PDF and run it through multiple extraction / OCR libraries\n* compare outputs side-by-side\n* benchmark quality before choosing a pipeline\n* use it via **Web UI, CLI, or API** depending on your workflow\n\n\n\nRight now it supports libraries like \n\n\\- Unstructured\n\n\\- Marker\n\n\\- Docling\n\n\\- PyMuPDF4LLM\n\n\\- Markitdown, etc., and I‚Äôm adding more over time.\n\n\n\nThe goal isn‚Äôt to ‚Äúreplace‚Äù these libraries ‚Äî but to make evaluation easier when you‚Äôre deciding which one fits your dataset or RAG use-case.\n\nIf this is useful, I‚Äôd love feedback, suggestions, or thoughts on what would make it more practical for real-world workflows.\n\nCurrently working on adding a Chunking strategies into PDFstract post conversion so that it can directly be used in your pipelines .\n",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1pxom6i/opensource_pip_built_a_unified_pdf_extraction/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwd5955",
          "author": "bonsaisushi",
          "text": "Looking great! Have you done any testing in heavy pdfs (1000+ pages) by any chance?",
          "score": 2,
          "created_utc": "2025-12-28 14:08:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdft5t",
              "author": "anashel",
              "text": "I will test it and let you know, but I previously helped a client convert a library of 20,000 books, each 200 to 300 pages. I battle tested just about everything you can imagine, from Python OCR pipelines to Grok, GPT, Claude, and others. Because the output was for audiobooks, precision and quality were non negotiable, which made this especially difficult.\n\nIn the end, no one came close to Mistral OCR. And I mean no one, by a thousand miles.\n\nPricing was $0.003 to $0.007 per page, with a batch API that can process an entire PDF library. But the real kicker is the JSON schema support.\n\nYou define exactly not only how the content is returned, but also the meta analysis you want run on each page. Meta data like; is this page part of a table of contents? Does the text continue on the next page, or is the last sentence complete?  There are no prompt gimmicks to force valid JSON. It is native, structured, and reliable. It is also massively strong in multiple languages, which mattered for us since the content was in French.\n\nWhat used to be a complex, fragile pipeline is now simple. An R2 bucket, a PDF upload, a queue event triggered on upload, a 20 line Cloudflare Worker invoking Mistral, and all content is saved back into R2 and automatically indexed by Cloudflare RAG. That is it. Nothing else. You could build the same thing on AWS in less than a day.\n\nRight now I am working on a much more challenging project: converting an archive of 500,000 printed architectural blueprints, mostly phone and fiber cable connections inside buildings, so we have a powerful search engine with an agent that can generate a first pass analysis and inventory for technicians planning a repair at a location.",
              "score": 4,
              "created_utc": "2025-12-28 15:11:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwdllkq",
                  "author": "bonsaisushi",
                  "text": "That is actually impressive, I'll surely give it a try, thanks!",
                  "score": 3,
                  "created_utc": "2025-12-28 15:42:47",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwf0rvw",
                  "author": "getarbiter",
                  "text": "That use case is actually a very clean fit.\n\nWhere keyword search breaks on technical drawings is terminology drift over time and roles. \n\nThe people querying (‚Äúfiber issue third floor east wing‚Äù) rarely use the same language as the original drafters (‚Äúoptical distribution frame,‚Äù ‚ÄúFJB,‚Äù ‚Äútelecom riser,‚Äù etc.).\n\n Zero keyword overlap, same physical thing.\n\nARBITER isn‚Äôt an OCR or extraction layer ‚Äî it sits after that. \n\nIt scores semantic coherence between a technician‚Äôs description and candidate documents.\n\nFor a blueprint archive like yours, that enables a few concrete things:\n\nCross-document retrieval\nNatural-language queries surface relevant drawings even when terminology differs completely from the query.\n\nInventory sanity checks\nScore extracted equipment lists against expected semantic patterns. Low coherence is a signal for likely extraction errors or anomalous drawings worth review.\n\nRepair triage\nA technician describes an issue in their own words; ARBITER narrows 500k documents down to a small, semantically coherent candidate set.\n\nIt‚Äôs deterministic, CPU-only, ~26MB, and runs air-gapped ‚Äî which tends to matter once building data becomes sensitive.\n\nNot competing with docling/unstructured ‚Äî complementary. Those get text out; this helps determine which documents actually mean what the technician is asking about.\n\nOut of curiosity, what are you using today to search or route those blueprints once they‚Äôre extracted?",
                  "score": 1,
                  "created_utc": "2025-12-28 19:50:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwd5qd8",
              "author": "GritSar",
              "text": "I have tried 100 pages and since pdfstract is a wrapper on top of libraries like unstructured, miner, docling, tessaract etc \n\nThe performance is subjective to the document and the system capacity \n\nBut it can be done",
              "score": 2,
              "created_utc": "2025-12-28 14:11:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwdbc7i",
          "author": "silvrrwulf",
          "text": "This sounds really great.  Have you found any to be better at one type vs another?  Say, Docling is great at unstructured legal but can't parse medical, or any generalities?  Curious if you found some do better in certain industries than others due to formatting, vocabulary, etc.  \n\nThis sounds really cool.",
          "score": 2,
          "created_utc": "2025-12-28 14:46:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdh8nu",
              "author": "GritSar",
              "text": "It is subjective to  usecases and this is what I have found in general.\n\nhttps://preview.redd.it/mlfobhrdpy9g1.png?width=1086&format=png&auto=webp&s=03b70fcda171180b1fc246b5b670a8a2652f6b54",
              "score": 3,
              "created_utc": "2025-12-28 15:19:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwe77ts",
                  "author": "silvrrwulf",
                  "text": "This is very helpful!  Thanks!!",
                  "score": 2,
                  "created_utc": "2025-12-28 17:31:12",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nwhtz4f",
                  "author": "Snoo-85117",
                  "text": "You should test out docstrange, Nanonets ocr",
                  "score": 2,
                  "created_utc": "2025-12-29 05:02:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfmplw",
          "author": "OnyxProyectoUno",
          "text": "The side-by-side comparison saves so much time over setting up each library separately.\n\nOne thing that bit me was that parser comparison is only half the story. Even when you find the best parser for your docs, chunking strategy can completely change what your RAG system actually sees. I ended up building something similar at vectorflow.dev but focused on the full preprocessing pipeline, not just extraction.\n\nThe chunking addition you mentioned sounds like the right direction. Being able to see how different chunking strategies affect the same parsed content would be huge. What's your plan for the chunking comparison UI?",
          "score": 2,
          "created_utc": "2025-12-28 21:38:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwhxivr",
              "author": "GritSar",
              "text": "In next release chunking strategies would come - it‚Äôs being added",
              "score": 1,
              "created_utc": "2025-12-29 05:27:42",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwi0ua4",
                  "author": "OnyxProyectoUno",
                  "text": "I love it. We're both addressing the same problem from different angles. Good luck to you sir!",
                  "score": 2,
                  "created_utc": "2025-12-29 05:53:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwdqjr9",
          "author": "Vegetable-Second3998",
          "text": "What makes this different from or better than https://www.docling.ai?",
          "score": 1,
          "created_utc": "2025-12-28 16:08:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwdtemo",
              "author": "GritSar",
              "text": "It‚Äôs just a wrapper for validating and using libraries like docling, unstructured etc and benchmark results and use multiple ocr libraries in your data engineering pipeline",
              "score": 2,
              "created_utc": "2025-12-28 16:22:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1d00i",
      "title": "Reaching my wit‚Äôs end with PDF ingestion",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q1d00i/reaching_my_wits_end_with_pdf_ingestion/",
      "author": "fustercluck6000",
      "created_utc": "2026-01-01 19:41:33",
      "score": 26,
      "num_comments": 55,
      "upvote_ratio": 0.91,
      "text": "Recently had a client ask me at the last minute to ingest a large corpus of highly structured PDFs into the db for this application I‚Äôm building them. Some of these docs are several hundred pages long, and this was one of those frustrating examples of needless heartache because the PDFs were clearly exported from Word, they just couldn‚Äôt track down the original docx files.\n\nRight off the jump, the existing ingestion pipeline I‚Äôd built with Docling failed miserably (up until now it‚Äôs been structured file formats with the occasional small PDF, and ingestion has been pretty flawless). I spent way too much time trying to tweak things until I resorted to parsing pages with qwen3-vl and correcting all the formatting/parsing errors manually to meet an external deadline.\n\nAfter the number of different open-source tools/libraries I‚Äôve tried at this point (including some of unstructured‚Äôs open-source pdf tools, would consider paid options if I knew they‚Äôd work *much* better), I‚Äôm having trouble comprehending how something as dumb as getting reliably correct, structured text from a PDF (that‚Äôs visually identical to a word doc, no less) can be this much of a damn headache. Even just a single missing bullet point or incorrect section index in the right spot can completely throw off chunking and create total nightmares with retrieval later on.\n\nLike am I missing something, here? I feel confident in saying I‚Äôm good at what I do, but I think the client would have second thoughts about my competency if I told them I just spent all this time manually preprocessing documents to build them an application to literally automate preprocessing documents (not billing by the hour btw). I don‚Äôt usually work with PDFs much, especially not of this size (where structural components like chapters, lists, appendixes, etc become SUPER important), so if anyone here does and has some pro tips, please please please do share üôè",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q1d00i/reaching_my_wits_end_with_pdf_ingestion/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx4paox",
          "author": "fabkosta",
          "text": "You're not missing anything - the problem is just freakingly difficult. Sure, maybe a tool like Azure Document Intelligence might be slightly better than Docling, but only marginally so.\n\nIf your documents are all structured the same way, then you could use a template-based approach, but not all OCRing tools support that. (Not sure Docling does, but Azure Document Intelligence apparently does.)\n\nGenerally speaking your client does not give you a small last minute change, but a proper change request would be in place. OCRing docs is among the most challenging and time-consuming tasks to complete when it comes to building RAG solutions.",
          "score": 13,
          "created_utc": "2026-01-01 19:54:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4s3xq",
              "author": "fustercluck6000",
              "text": "Thanks for the reply! It‚Äôs good to know I‚Äôm not alone in this, although part of me almost wishes I was since that would mean there was a good solution out there. I didn‚Äôt mention this in the post, but everything does have to run locally on their air-gapped server for security reasons, so cloud-based tools are out of the question.\n\nAs an interim solution, I‚Äôm looking at creating a template and using more regex than I already am since the docs do follow a uniform structure, or at least they‚Äôre *supposed to* (tbd how I‚Äôll parse complex tables, which Docling is actually super good at). The frustrating thing is that there‚Äôs all these little human inconsistencies that any hard-coded preprocessing logic has to account for, e.g. in one document, subsection indexes would look like this: ‚Äò(a)‚Äô, while another one has ‚Äòa)‚Äô or ‚Äòa:‚Äô. Just a massive pain more than anything else honestly, idk why the hell corporate America got in this habit of using PDFs for literally everything in the first place haha",
              "score": 1,
              "created_utc": "2026-01-01 20:08:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx53lhv",
                  "author": "fabkosta",
                  "text": "Typically, for any project that intends to process documents, my rule-of-thumb is 80% of time to prepare docs and 20% to build the system itself. Maybe this is a bit exaggerated, but definitely 60:40 at least.\n\nPeople continuously underestimate the complexity of handling text documents - and PDFs are the worst ones, yet everyone is using them. Some PDFs are also intentionally created in a way that makes every OCRing tool fail, particularly when it comes to financial reports.",
                  "score": 3,
                  "created_utc": "2026-01-01 21:08:27",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nx86nbo",
                  "author": "ai-mera",
                  "text": "\\> Azure Document Intelligence (DI) might be slightly better than Docling.\n\nWhile Azure Document Intelligence (DI) is better than nothing, it feels overpriced and doesn't deliver the expected level of performance.  \nParticularly, despite being a Microsoft product, its recognition of Word, PowerPoint, and Excel files is poor, requiring conversion to PDF before feeding them into DI.  \nUsing DI creates a black box. I believe tuning Docling according to specific use cases ultimately yields higher accuracy and lower costs.\n\nI was using DI, but I plan to start evaluating Docling after the new year.  \nI apologize for not being able to offer any specific hints. That's all for now.",
                  "score": 1,
                  "created_utc": "2026-01-02 09:20:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx57re1",
          "author": "Porespellar",
          "text": "Apache Tika or Granite Docling. Tika is super easy to get going, just a simple Docker run.",
          "score": 5,
          "created_utc": "2026-01-01 21:30:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5tnes",
              "author": "fustercluck6000",
              "text": "Awesome, always love discovering new open source tools, reading up on Tika now!",
              "score": 1,
              "created_utc": "2026-01-01 23:26:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx515ha",
          "author": "Synyster328",
          "text": "It's because PDFs are for rendering graphics, not storing data. Like using a picture taken from your phone of a computer screen to represent JSON.",
          "score": 3,
          "created_utc": "2026-01-01 20:55:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx581du",
              "author": "fustercluck6000",
              "text": "And PDFs do make sense for a lot of things like hand-signed forms, scanned receipts, etc, but why people then insist on using it as the default standard for anything else that has structured text in it is completely beyond me. Even using acrobat for basic stuff imo is a total pain",
              "score": 2,
              "created_utc": "2026-01-01 21:31:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5denx",
                  "author": "Synyster328",
                  "text": "Yeah, it's a disaster, I spent like a year building a startup dependent on good PDF parsing and never came to a satisfying conclusion or solution, it just sucked ass forever.\n\nI ended up just treating them as two separate files - There's the text content you can extract programmatically, and there's the image content from rendering it.\n\nThe extracted text content often was a complete cluster fuck, the image content was often hard to decipher with custom fonts and weird layouts, but by treating them as two sides of the same document, and giving both to the AI at all times, with some rigid prompting, was usually able to make sense of it well enough.\n\nHowever, there were always some edge cases that would break the app. I ended up just diverting my focus to make those guardrails better, making it easy for users to flag bad outputs, RLHF pipelines, auditing, etc.",
                  "score": 2,
                  "created_utc": "2026-01-01 21:58:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx8ynb8",
          "author": "TechnicalGeologist99",
          "text": "A decent bet here is to do hierarchical RAG. \n\nYou can use layout detection and parse headings to create a document hierarchy. (This is good for when you know documents are structured but don't necessarily know how, or it's too complex to fully apply true structure to all relevant docs. It will occasionally miss headings leading to an imperfect hierarchy) \n\nIt won't be 100%, you'll need to live with that.\n\nFor super structured docs like legal docs...it's really needed to build knowledge graph because it can capture more complex relationships that are needed in that domain. (You likely do not have time for this)\n\nThe lesson here is that you need to learn to say no to client. \n\nDid you agree to build such a complex system? Are they gonna pay your time to make the change? Do they understand how much of a unilateral change it is?",
          "score": 3,
          "created_utc": "2026-01-02 13:17:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxeglww",
              "author": "fustercluck6000",
              "text": "It's actually funny you mention legal docs because I've been working on a project in that area on the side for a little while now. With primary sources like statutes or case law, the structure itself is integral to how even the lawyers themselves read/interpret things (because of all the citations, definitions, precedents/priors, etc...), so I actually chose to hardcode hierarchical schemas (I guess technically hardcoding the dataclass factories but you get the idea) for chunking and adding nodes/edges to the knowledge graph before making any model calls, just because we didn't want to leave any margin for error when indexing really important, canonical materials like the U.S. Code or something (court documents like evidence and stuff are another story, though).\n\nThis definitely added a degree of complexity to the project that I didn't plan on signing up for before signing a new contract I'll be honest. And no, they really don't understand how much of a unilateral change it is, but to be fair I think a lot if not most people who aren't in aren't clued into the space would, either. I think we got CEOs promising the next model's gonna replace researchers with PhDs to thank for that lmao",
              "score": 1,
              "created_utc": "2026-01-03 07:33:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxfftnt",
                  "author": "TechnicalGeologist99",
                  "text": "I'll say that this is the biggest challenge in consulting in tech. The client doesn't see the work they see the result. \n\nMany companies agreed that they will produce a certain result for a certain price. This seems fair. But in reality many tech projects over run in costs and time by three times the expected amount.\n\nFor future projects you should aim to create a detailed PRD (product requirement document). It should capture the kind of documents you will be ingesting and what techniques will be used. It should state explicitly that changes to document type ingested will be added complexity and would require recosting of the work. \n\nThe PRD is what you point to to help you say no to the client (or rather \"yes, but more work = more cost\")\n\nEffectively we are just trying to protect ourselves and the client from scope creep mid-project. As this harms your business and creates tech debt for the client. Nobody really wins when scope creeps",
                  "score": 1,
                  "created_utc": "2026-01-03 12:30:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx581gt",
          "author": "ChapterEquivalent188",
          "text": "  \n**No you are not incompetent. You are fighting the \"Digital Paper\" fallacy**\n\n  \nFirst off: Take a deep breath. The fact that you resorted to manual fixing proves you care about data integrity. Most devs would have just ingested the garbage, delivered a hallucinating bot, and blamed the LLM.¬†**You did the job.**\n\n**Why this happens (The \"Word Export\" Trap):**  \nPDFs exported from Word are deceptively evil. Visually, they look structured  \nUnder the hood, a \"bullet point list\" in Word often gets exported as:\n\n1. A floating symbol (‚Ä¢) at coordinates X,Y.\n2. A text block at coordinates X+10, Y. Standard parsers (even Docling sometimes) read this as two separate, unrelated blocks. The semantic connection (\"This is a list item\") is lost.\n\n**The Solution: The Multi-Lane Approach**  \nI spent the last 2 years building an Enterprise RAG Architecture (V3 Core) exactly because of this headache. I found that¬†**no single tool**¬†works for 100% of pages.\n\n**My architecture uses a \"Consensus Engine\":**\n\n1. **Lane A (Text Layer):**¬†Since it's a Word export, the text layer is usually clean. Use¬†PyMuPDF¬†to extract raw text fast.\n2. **Lane B (Vision):**¬†Use a VLM (like¬†Qwen2-VL¬†or¬†Llama 3.2 Vision) ‚Äì¬†**BUT**¬†not to read everything. Use it to extract the¬†**Skeleton**¬†(Headers, List structures, Table boundaries).\n3. **Merge:**¬†Map the clean text from Lane A into the visual skeleton from Lane B.\n\n\n\n**Immediate Advice for you:**  \nSince you are on a deadline: Don't try to find the \"perfect tool\". It doesn't exist.  \nIf the docs are critical,¬†**Hybrid Parsing**¬†is the only way.  \nUse the Text Layer for content accuracy (0 hallucinations) and Vision for structural boundaries (Chunking)\n\nDon't let the client make you feel bad. Converting \"Print Layout\" back to \"Semantic Structure\" is one of the hardest problems in NLP right now",
          "score": 8,
          "created_utc": "2026-01-01 21:31:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5jcdl",
              "author": "fustercluck6000",
              "text": "I deeply appreciate this comment, super thoughtful and insightful and the kind of thing that makes me love Reddit tbh\n\nPart of the reason I stick to a fixed price policy vs. hourly is that I‚Äôll obsess over getting stuff like this right because I know full well how much boring shit like this will seriously matter in the end, so I‚Äôm happy to recoup the overtime spent on the data pipeline later. I know devs who just ignore known problems with data pipelines like that and it kinda makes me scratch my head. I think ingestion‚Äôs the single component of a RAG system with the most disproportionate impact on everything else (and that goes for accuracy and maintainability, both). Of all the things to half ass, ingestion sits at the bottom for me. And I‚Äôve half assed a few front ends in my day haha. But like c‚Äômon guys, this is literally why they say garbage in garbage out. \n\nAnd I never knew that about how Word exports PDFs but it makes total sense now (thinking about all the times sizing/scaling/page layouts get all mangled in a PDF export, which I always just assumed was *one of those things*). \n\nI actually tried working out something with PyMuPDF precisely because it extracts all the raw text correctly, but put a pin in it because I didn‚Äôt know how to combine that with structural/tabular information. Can you elaborate on how you‚Äôre merging clean text with the skeleton? I‚Äôm trying to think through the logic behind selecting text lines/paragraphs and allocating them to corresponding regions, and suddenly it makes a lot more sense how you could spend two years on this problem!\n\nAnd next meeting I have with the client‚Äôs upper management to give them a status update, I‚Äôm making sure *digital paper fallacy* is inserted at least 3 times in the conversation",
              "score": 2,
              "created_utc": "2026-01-01 22:30:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx79jwm",
                  "author": "ChapterEquivalent188",
                  "text": "I think you are already ready to understand my V3 ;) \nYou may have a look for a deeper understanding wahts awaiting you on day 3, 4 and 5........Yesterday I found Day 6 and it feels it never ends .....\n\nGlad the \"Digital Paper\" concept resonated! Use that term with your management. It shifts the blame from \"The dev is slow\" to \"The source material is flawed,\" which is the truth.\n\n\nLane B (Vision) acts as the Architect:\nIt looks at the page image and draws bounding boxes around logical sections. It says: \"There is a Header at [0, 0, 100, 50] and a Table at [0, 60, 500, 300].\" It creates the empty buckets.\n\nLane A (PyMuPDF) acts as the Miner:\nIt extracts the raw text words, but crucially, it also extracts their coordinates (fitz.Rect). It gives you the sand\n\nThe Merge (Spatial Join):\nYou write a function that checks: \"Which words from Lane A fall physically inside the box defined by Lane B?\"\n\nIf a word's center point is inside the \"Table Box\", it belongs to the table chunk.\n\nIf a word is inside the \"Header Box\", it becomes metadata\n\nHope it helpes and always enjoy what we do..... Its basic but necessary\n;)",
                  "score": 2,
                  "created_utc": "2026-01-02 04:41:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx8z0as",
              "author": "TechnicalGeologist99",
              "text": "Yep, this approach is king. Especially when you can't go to the effort to understand each specific document structure and the many edge cases.",
              "score": 2,
              "created_utc": "2026-01-02 13:20:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxbpxn6",
                  "author": "bravelogitex",
                  "text": "How do you merge both layers?",
                  "score": 2,
                  "created_utc": "2026-01-02 21:29:17",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx4p3o7",
          "author": "Snoo-85117",
          "text": "Use docstrange by Nanonets. \nYou can get the output in text format.",
          "score": 2,
          "created_utc": "2026-01-01 19:53:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4swdc",
              "author": "fustercluck6000",
              "text": "Thanks, I‚Äôll check it out!",
              "score": 1,
              "created_utc": "2026-01-01 20:12:16",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx7xqe3",
              "author": "bravelogitex",
              "text": "they are slow as hell though, like 10x compared to mistral in my exp",
              "score": 1,
              "created_utc": "2026-01-02 07:55:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4s20a",
          "author": "DeadPukka",
          "text": "The paid options have trials so you can see if it works for you. \n\nAzure Doc Intelligence has served us well, but look at the newcomers like Reducto. This shouldn‚Äôt be so painful; we support both in our platform and get reliable output.",
          "score": 2,
          "created_utc": "2026-01-01 20:08:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx52e6f",
              "author": "fustercluck6000",
              "text": "You‚Äôve just reminded me of all these Azure startup credits I have! (Marketed to ai startups but basically impossible to use for GPU time lmao) I‚Äôm going to look into both of these and running containers offline (necessary for security reasons). \n\nJust out of curiosity, how complex are the typical PDFs you‚Äôre working with? How substantially better are these than the open-source libraries out there (like if you had to estimate something like an indexing error rate)?",
              "score": 1,
              "created_utc": "2026-01-01 21:02:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5y26t",
          "author": "youre__",
          "text": "If the docs are highly structured and consistent across pages, an alternative approach is to brute-force it by cropping page images and OCRing the text. \n\nDocling and other systems can preserve page sections, but sometimes they have issues with relative placement when doing everything at once. If you do the segmentation yourself, you eliminate that as a variable. It will definitely take longer, but you only have to do it once.\n\nUse instructor or JSON response formats with qwen3-vl to ensure you grab the right properties. If it fails too many times, flag the section for human review.",
          "score": 2,
          "created_utc": "2026-01-01 23:51:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx68y1t",
              "author": "fustercluck6000",
              "text": "Cropping pages was actually one of the first steps I added in the pdf pipeline, since headers/footers are irrelevant anyways and always at the same heights. The issue I‚Äôve had with qwen3-vl (4b and 8b) is that by going page by page and exporting to markdown, without having all the previous context (like the last section‚Äôs index the previous header/indent levels), the model assumes whatever‚Äôs at the top of the page is the top-level header, and when numbered sections on that page start at, say, 3 (because section 2 was 6 pages ago), it assumes 3 is supposed to be a 1, then resets section 4 to 2, and so on‚Ä¶ Also, when sentences are broken up across pages (haven‚Äôt even gotten into multi-page tables yet), joining them back together properly is very error-prone, too. \n\nHaven‚Äôt had much luck tuning the prompt to prevent altering section indices, either, but to be fair there‚Äôs still a lot of room to experiment with that side of things. Also haven‚Äôt tried it with json output format, but now I‚Äôm wondering if there‚Äôs an elegant-ish way to enforce a pydantic output schema so I can pipe qwen3 outputs directly into a Docling document model‚Ä¶?",
              "score": 1,
              "created_utc": "2026-01-02 00:52:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx6nr3a",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's the usual story with complex PDFs. The problem isn't your competency, it's that most parsing tools treat PDFs like they're just text with some formatting when they're actually structured documents with hierarchies, relationships, and semantic meaning that gets completely destroyed during extraction.\n\nWhat's killing you is the invisibility problem. You can't see what went wrong until you're deep into chunking and retrieval, discovering that bullet points got merged into paragraphs or section headers disappeared entirely. By then you're debugging symptoms three steps removed from the actual parsing failure.\n\nFor large structured PDFs, you need to preserve document hierarchy during extraction. Most tools flatten everything into linear text, which destroys the relationships between sections, subsections, and list items that your chunking strategy depends on. The fact that these were exported from Word makes it worse because they have implicit structure that PDFs don't preserve well.\n\nA few things to try: First, use OCR even on text-based PDFs. Sometimes the text extraction is garbage but the visual layout is clean. Second, chunk by logical sections rather than arbitrary token counts. If you can identify headers and preserve that hierarchy, your retrieval will be much more accurate. Third, validate your parsed output before chunking. Most people skip this step and wonder why their chunks are nonsensical.\n\nThe real issue is you're flying blind during document processing. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this problem because you need to see what your docs actually look like after parsing, before you commit to chunking strategies.\n\nWhat does your current output look like when Docling processes one of these PDFs? Are you losing the structural elements or is the text itself getting mangled?",
          "score": 2,
          "created_utc": "2026-01-02 02:22:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx9pa0d",
              "author": "fustercluck6000",
              "text": "That invisibility problem is SO real, it‚Äôs these silent bugs that are a complete nightmare to even identify, let alone test for and fix. For now I‚Äôm just caching the parsed outputs in a test folder in my environment where I can easily look through them to see what‚Äôs going on during development.\n\nQuite a few people have suggested hybrid strategies and using OCR models to just detect the PDF‚Äôs layout and worry about text separately. Still thinking about how I want to implement that, but I‚Äôm all but certain at this point that‚Äôs how I‚Äôm going to design the pdf pipeline. Out of curiosity‚Äîwhen you say validate the parsed output, am I correct in assuming you mean Pydantic/something similar? I have a basic base model I‚Äôm using to validate simple markdown formatting syntax, but I do want to write more sophisticated checks for section indices at different depths and other structural stuff like that (which is uniform across all docs in this particular corpus).",
              "score": 1,
              "created_utc": "2026-01-02 15:45:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx4pfl9",
          "author": "AsparagusKlutzy1817",
          "text": "PDF are not structured. It‚Äôs a canvas where you put lines, boxes and letters. Some pdfs have a style guide like clear spacing between paragraphs. Orderly distance of separate elements, etc., but not necessarily.\n\nIf you have a hard requirement of keeping bullet points together this will likely not work reliably. Some solutions may put more effort in keeping structure together than others but the task is per-se non deterministic. \nWhat works for one document may not work for another second one (style varies etc)\n\nWhy does it break when you miss a bulletpoint ? Create chunks with 500 token and generously let them overlap - even if a line slips up or down this should not matter.\nCan you elaborate what you are trying to do ? Maybe it‚Äôs easier to help then but PDF parsing is a mess - if the original word is gone you just lost so much structure and information. All you can do now is to make a sincere attempt in parsing but expect no magic",
          "score": 3,
          "created_utc": "2026-01-01 19:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4w4te",
              "author": "fustercluck6000",
              "text": "The only structural component of a PDF is the pages, which are completely useless 99% of the time anyway haha. But yeah to answer your question, this client has a few specific use cases for this but a good example is retrieving detailed compliance info they might receive from one of their own clients inside of an RFP. For reference, the compliance section of an RFP in their industry can be 300-400+ pages long and it‚Äôs all hyper-detailed (think specific codes, names, numbers, etc for various rules and regulations). This application helps the people who‚Äôd write the proposal in response to that RFP to get all the miscellaneous, granular details they need without having to spend half their time hunting for them.\n\nIt‚Äôs important for the db (neo4j) to map the structural hierarchy of the document so that when a cosine similarity or whatever retrieval method turns up a specific chunk, you can walk up and down the ‚Äòtree‚Äô to add additional context based on the location. For example, what chapter/section does this chunk belong to (since those typically name specific rules/regulations explicitly, while the chunk itself might only refer to it implicitly). It‚Äôs also often useful to add the previous/next chunks as context, but only if they‚Äôre at the same anatomical ‚Äòlevel‚Äô as the original chunk (so you don‚Äôt add irrelevant stuff from the next chapter because the original chunk happened to be the very last one in its respective chapter).\n\n Also knowing exactly where chunks came from so the model can cite sources in its responses is **critical** for fact-checking and safeguarding against hallucinations, (especially in this industry where a simple clerical mistake can cost thousands in fines), not to mention just plain useful for them.",
              "score": 2,
              "created_utc": "2026-01-01 20:28:48",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx4q2fp",
              "author": "AsparagusKlutzy1817",
              "text": "Oh why not load the full text document text based on a partial match on a chunk and then pipe the full document in blocks through an LLm if the full document is too large ?",
              "score": 1,
              "created_utc": "2026-01-01 19:57:56",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx59hm8",
          "author": "trollsmurf",
          "text": "PDF is formatted for presentation not editing. E.g. Adobe Acrobat can convert back to editable formats, and my experience is that it does it well.",
          "score": 1,
          "created_utc": "2026-01-01 21:38:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5bzyp",
              "author": "fustercluck6000",
              "text": "Acrobat is so obvious that I never actually thought about using it, I guess I‚Äôve been living under a rock because I never realized until just now that they actually have a Python API for this sort of stuff, is that what you use?",
              "score": 1,
              "created_utc": "2026-01-01 21:51:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx5ptvn",
                  "author": "trollsmurf",
                  "text": "No, I convert manually. Not that many documents (yet).\n\nI also use this: [https://pypdf2.readthedocs.io/en/3.x/](https://pypdf2.readthedocs.io/en/3.x/)\n\nThis one converts to markdown, but I haven't tried it yet: [https://pypi.org/project/marker-pdf/](https://pypi.org/project/marker-pdf/)",
                  "score": 1,
                  "created_utc": "2026-01-01 23:05:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5mc05",
          "author": "davelargent",
          "text": "Have you tried https://www.docetl.org/ or LlamaParse? I‚Äôve had some success with those when others fell apart.",
          "score": 1,
          "created_utc": "2026-01-01 22:46:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx5u51m",
              "author": "fustercluck6000",
              "text": "This is the first I‚Äôm hearing of DocETL, but the fact that the first thing on their landing page is an arxiv link tells me it‚Äôs definitely worth looking into in more depth, thanks for the rec!",
              "score": 1,
              "created_utc": "2026-01-01 23:29:36",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx5x5cl",
          "author": "Lanky-Cobbler-3349",
          "text": "Just use gemini. Ask it to return a structured markfown format that preserves the original structure of the document if possible. Do it with low temperature and then apply a reasonable chunking strategy. Or use an OCR API that directly provides the structure without prompting. Docling etc. are shit. You need like 20 lines of code, you catch all the edge cases and dont have to implement some rule-based bullshit",
          "score": 1,
          "created_utc": "2026-01-01 23:46:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7xxts",
              "author": "bravelogitex",
              "text": "gemini flash 2.5 preview has worked great in my limited testing across 2 pdf's, one 18 page and another a 1 page table.\n\nsomeone said \"I work in fintech and we replaced an OCR vendor with Gemini at work for ingesting some PDFs\":¬†[https://news.ycombinator.com/item?id=42953665](https://news.ycombinator.com/item?id=42953665)",
              "score": 1,
              "created_utc": "2026-01-02 07:57:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx64llt",
          "author": "Far_Statistician1479",
          "text": "Why can‚Äôt you just convert the pages to markdown with a VLM and do whatever is needed with that?",
          "score": 1,
          "created_utc": "2026-01-02 00:27:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx69xfk",
              "author": "fustercluck6000",
              "text": "Accuracy, for smaller/less complex PDFs VLMs have worked totally fine but here, any structural parsing errors related to section indices and stuff, even minor ones effectively compound. \nI basically did what you‚Äôre describing with qwen3-vl-8b, and besides being super slow, the markdown wasn‚Äôt accurate enough on its own to chunk without making corrections, first.",
              "score": 1,
              "created_utc": "2026-01-02 00:58:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6f6th",
                  "author": "Far_Statistician1479",
                  "text": "Accuracy? I used smaller models than qwen3 and the accuracy is extremely high?",
                  "score": 1,
                  "created_utc": "2026-01-02 01:30:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6eubl",
          "author": "Flimsy_Vermicelli117",
          "text": "Did you try using Word to open these pdf files? If these are from Word, they open usually nearly flawlessly back into Word. Then save as more sane format. If you are on macOS, ChatGPT can whip out AppleScript which will convert folder of these using just Word. Word is AppleScriptable.",
          "score": 1,
          "created_utc": "2026-01-02 01:28:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx6k8yp",
              "author": "fustercluck6000",
              "text": "I don‚Äôt even know how many years it‚Äôs been since I last had a Word license haha, but I‚Äôm definitely going to try and get my hands on one now because your suggestion sounds like it could just be a perfect, simple long term solution for this client in particular (who almost exclusively uses Microsoft enterprise stuff).\n\nAnd I‚Äôve never thought of doing it, but now I‚Äôm super interested in this idea of automating word tasks. My personal laptop is a Mac, but the project itself is running on a headless Ubuntu server that belongs to the client. It would probably be easy enough to work out something with WSL, I imagine. In the meantime, I think I‚Äôm going to set up some tests locally on my Mac to see if that‚Äôs worth pursuing, got any specific noteworthy tips/tricks for generating the AppleScript (never written any code in it before)? \n\nThanks for the advice",
              "score": 1,
              "created_utc": "2026-01-02 02:01:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx6mlhw",
                  "author": "Flimsy_Vermicelli117",
                  "text": "I was never able to even phantom AppleScript as I could never find readable manual. Then I asked ChatGPT and turns out, it knows how to write AppleScripts well enough. Rarely on first attempt, but few iterations later and it's done. I asked it to help me to add right click function to convert Office documents to pdf and it wrote it fine and told me how to add it to macOS.\n\nI just asked ChatGPT and it generated macOS AppleScript as well as Windows PowerShell COM automation scripts.",
                  "score": 1,
                  "created_utc": "2026-01-02 02:15:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx6s15n",
          "author": "Flashy-Sprinkles1624",
          "text": "I don't mean to self-promote but I built a tool to specifically tackle this that I have since taken offline due to lack of uptake. Here's a like to the typescript SDK [https://github.com/Raptor-Data/ts-sdk](https://github.com/Raptor-Data/ts-sdk) . I intended to make a python SDK but haven't got that far. If it's something you think could be genuinely helpful then I'm more than happy to put it back online and chat with you to see how I can help. Please note that the current raptor data public site is for a different product so if you visit it just keep in mind the that these are two different tools.",
          "score": 1,
          "created_utc": "2026-01-02 02:48:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhmjr9",
          "author": "RolandRu",
          "text": "Yep ‚Äî you‚Äôve hit the ‚Äúlooks like Word, therefore it should parse like Word‚Äù trap. PDF is basically a rendering snapshot, so the semantic stuff you care about (lists, headings, reading order) is often implicit or just gone, even when it was exported from Word.\n\nA few things that have saved me pain in production:\n\nFirst, classify what you‚Äôre dealing with before you try to be clever. Born-digital PDFs with a clean text layer behave very differently than scanned/mixed ones, and even within born-digital you‚Äôll see ‚Äúnice text‚Äù but broken reading order because of columns, headers/footers, floating text boxes, etc. If it‚Äôs not stable, route it through a different path early instead of burning hours trying to tweak one tool.\n\nTreat layout as first-class signal, not just the text stream. Font size/weight, indentation, line spacing, and bounding boxes are what usually let you reconstruct headings and lists. Bullets and numbering are notoriously fragile if you rely on plain text output.\n\nChunk by structure rather than fixed token windows. Even an imperfect hierarchy based on detected headings/section titles beats 1k-token chunks when appendices/chapters matter, because you‚Äôll preserve intent and reduce ‚Äúwrong neighbors‚Äù in retrieval.\n\nKeep provenance for everything you store: page number and offsets at minimum, and bounding boxes when you can. It makes debugging and ‚Äúwhy did the model say this?‚Äù conversations a lot less painful.\n\nAdd a quick QC pass and a fallback. Simple checks like ‚Äúdid list item counts change?‚Äù, ‚Äúdid numbering skip?‚Äù, ‚Äúdid section headings disappear?‚Äù catch the cases that will later destroy chunking. When it fails, automatically re-run just those pages/sections with a more expensive extractor (layout-aware or vision-based) instead of paying that cost everywhere.\n\nAnd if the client can‚Äôt find the DOCX, ask them to re-export a tagged/accessible PDF if they possibly can. When headings and lists actually exist in the document structure tree, life gets dramatically easier ‚Äî it‚Äôs the closest you‚Äôll get to ‚ÄúDOCX semantics‚Äù coming out of a PDF.",
          "score": 1,
          "created_utc": "2026-01-03 19:14:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4okoo",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-01-01 19:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4oyzx",
              "author": "fustercluck6000",
              "text": "Thanks for the reply! And you definitely have my attention haha, is there a repo I can check out? I didn‚Äôt mention this in the OP, but list of the docs are technical and everything has to be on-prem too for security reasons, so what you‚Äôre describing sounds highly relevant",
              "score": 1,
              "created_utc": "2026-01-01 19:52:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx4pqyd",
                  "author": "Impossible-Table6121",
                  "text": "I'm also interested in anything you are happy to share",
                  "score": 1,
                  "created_utc": "2026-01-01 19:56:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx5uums",
          "author": "Both-Number-7319",
          "text": "So interesting \nI can see with you what approach can do",
          "score": 0,
          "created_utc": "2026-01-01 23:33:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4vzhf",
          "author": "ampancha",
          "text": "PDFs are the final boss of RAG. Standard libraries like Docling usually fail on 100+ page docs because they treat the file as a stream of text and ignore the spatial layout. If you lose the document hierarchy, your retrieval precision will never recover. I sent a DM with the specific architecture I use to handle these enterprise-scale PDFs without manual correction.",
          "score": -1,
          "created_utc": "2026-01-01 20:28:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0bzyg",
      "title": "Those running RAG in production, what's your document parsing pipeline?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q0bzyg/those_running_rag_in_production_whats_your/",
      "author": "Hour-Entertainer-478",
      "created_utc": "2025-12-31 12:34:22",
      "score": 22,
      "num_comments": 20,
      "upvote_ratio": 1.0,
      "text": "Following up on my previous post about hardware specs for RAG. Now I'm trying to nail down the document parsing side of things.\n\n**Background:**¬†I'm working on a fully self hosted RAG system.\n\nCurrently I'm using docling for parsing PDFs, docx files and images, combined with rapidocr for scanned pdfs. I have my custom chunking algorithm that chunks the parsed content in the way i want. It works pretty well for the most part, but I get the occasional hiccup with messy scanned documents or weird layouts. I just wanna make sure that I haven't made the wrong call, since there are lots of tools out there.\n\nMy use case involves handling a mix of everything really. Clean digital PDFs, scanned documents, Word files, the lot. Users upload whatever they have and expect it to just work.\n\nFor those of you running document parsing in production for your RAG systems:\n\n* What are you using for your parsing pipeline?\n* How do you handle the scanned vs native digital document split?\n* Any specific tools or combinations that have proven reliable at scale ?\n\nI've looked into things like¬†unstructured, pypdf, marker, etc but there's so many options and I'd rather hear from people who've¬†**actually**¬†battle tested these in real deployments rather than just going off benchmarks.\n\nWould be great to hear what's actually working for people in the wild.\n\nI've already looked into deepseekocr after i saw people hyping it, but it's too memory intensive for my use case and kinda slow.\n\nI understand that i'm looking for a self hosted solution, but even if you have something that works pretty well tho it's not self hosted, please feel free to share. I plan on connecting cloud apis for potential customers that wont care if its self hosted.\n\nBig thanks in advance for you help ‚ù§Ô∏è. The last post here, gave me some really good insights.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q0bzyg/those_running_rag_in_production_whats_your/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwxf8go",
          "author": "funkspiel56",
          "text": "Im using Mistral to get the job done. Its not perfect but its pretty damn cheap and quick. It fumbles on really tough handwriting as well as documents with a ton of diagrams in it but other than that it gets forms correct and roughly maintains the documents structure.",
          "score": 4,
          "created_utc": "2025-12-31 15:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwxo0mp",
              "author": "dmx007",
              "text": "It's amazing until it isn't. Crazy fast. Inexpensive. Very accurate ocr. But sometimes, it drops content even on clear doc scans because of slight changes in font style or size in the middle of documents. It's hard to recover when the content is just gone at the ocr stage with post processing.",
              "score": 1,
              "created_utc": "2025-12-31 16:18:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwzpiip",
                  "author": "funkspiel56",
                  "text": "yeah the sad part to is that I've also sent them a bit of samples and they are like yeah nothing we can do currently. Which I get but I know its possible. \n\nLlamaparse which in my experience reigns king on the leaderboard of ocr tools nails everything I feed it. Only the most obtuse items trip it up.   \n  \nIssue is llamaparse is really really pricey. Not affordable at all for a POC project where mistrals accuracy is good enough.  \n\nI tried the homegrown solutions where you run models on rtx cards but these are finicky and almost all of them require a bit of gluing together.",
                  "score": 2,
                  "created_utc": "2025-12-31 22:43:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwwscdb",
          "author": "Snoo-85117",
          "text": "Using docstrange by Nanonets for my extraction and parsing pipeline.\n\nWorks phenomenally well.",
          "score": 2,
          "created_utc": "2025-12-31 13:26:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwwu9z3",
              "author": "Hour-Entertainer-478",
              "text": "Thank you. Ill try that out.",
              "score": 1,
              "created_utc": "2025-12-31 13:38:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nx0zy9t",
              "author": "bravelogitex",
              "text": "how's it compare to others?",
              "score": 1,
              "created_utc": "2026-01-01 03:39:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxty4r",
          "author": "ValueOk4740",
          "text": "deformable-detr-doclaynet for layout understanding + paddleocr for text extraction. Here's our library that implements these things together if you're trying to run self-hosted: [sycamore](https://sycamore.readthedocs.io/en/stable/sycamore/transforms/partition.html) but we also have a cloud api (which tends to be faster and has a better layout model that the open source one - free for the first 10k pages) [aryn docparse](https://docs.aryn.ai/docparse/introduction)\n\nThe cloud offering can handle to-pdf conversion; on your own you're gonna need headless libreoffice or something.",
          "score": 2,
          "created_utc": "2025-12-31 16:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwyrwlh",
          "author": "drfritz2",
          "text": "You may need more than one pipeline. One for text, another another kind of text, other for OCR, another for images, another for conversions. \n\nMineru , paddle OCR, docling, and many others.",
          "score": 2,
          "created_utc": "2025-12-31 19:39:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx65o0x",
          "author": "fustercluck6000",
          "text": "The more you build things around a particular library/tool, the harder it gets to change your mind later, so it‚Äôs really good that you‚Äôre thinking carefully about this now instead of just winging it.\n\nJust a word of caution from my recent experience with Docling‚Äîit‚Äôs a really great tool until it isn‚Äôt. I had such great luck initially using it for html, xml, docx, and other structured files that I assumed I could expect the same with PDFs if the need ever arose, then made the mistake of building a lot of my data pipeline around the DoclingDocument class. I‚Äôll just say I was very disappointed when I needed it to process large, complex PDFs. \n\nThe whole DoclingDocument ‚Äòecosystem‚Äô with Pydantic is super tempting as a general purpose solution for your project, but imho the documentation‚Äôs pretty bad (and there are quite a few different bugs, though to their credit they were very quick to push an update to one open issue I was asking about on GitHub). That becomes a major hassle when you need to tweak/tune the pipeline for your data instead of rewriting it, but can‚Äôt easily determine the scope of your options in the first place without taking the time to dig through source code. Idk I‚Äôm always left with this feeling there‚Äôs probably way more Docling can do than I‚Äôm aware of‚Äîbut I‚Äôll only find out about those things by getting lucky and reading the right Reddit post or spending more time reading through the source code and model json files to familiarize myself.\n\nI‚Äôm still torn about how much/whether to use Docling going forward because certain constructs/methods are incredibly useful, but without being confident I can quickly work through bugs in the future or knowing more conclusively how much I can scale my project with it gives me serious pause. Just make sure to keep things very modular and dependencies loosely coupled.",
          "score": 2,
          "created_utc": "2026-01-02 00:34:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7iu5f",
          "author": "ChapterEquivalent188",
          "text": "you might wanna sneak peak here https://github.com/2dogsandanerd/RAG_enterprise_core \nno ad but it might give you some idea for the future ;)",
          "score": 2,
          "created_utc": "2026-01-02 05:46:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf8t8o",
              "author": "Think-Draw6411",
              "text": "What‚Äôs the benchmark results ?",
              "score": 1,
              "created_utc": "2026-01-03 11:35:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nxfgqkj",
                  "author": "ChapterEquivalent188",
                  "text": "Im pretty sure you understand my platform ;) what would you measure for a benchmark ?",
                  "score": 1,
                  "created_utc": "2026-01-03 12:37:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx3265t",
          "author": "Hungry-Style-2158",
          "text": "I just use Wetrocloud‚Äôs end to end data extraction API.\n\nI spent a lot of time trying to create my own pipeline. That took so much effort from my team and the infrastructure was costing us financially. \n\nWe just decided to go with existing built solutions like Wetrocloud. It was our preferred choice cause we didn‚Äôt have to build and manage the infrastructure ourselves.",
          "score": 1,
          "created_utc": "2026-01-01 14:43:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx7i3pl",
          "author": "beallg",
          "text": "MinerU 2.5",
          "score": 1,
          "created_utc": "2026-01-02 05:41:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxgfrsm",
          "author": "franzel_ka",
          "text": "I made a combined workflow that is working well. ImageMagick to extract, Qwen3 VL model to recognise, even 2b is doing well and GPT 20b OSS for processing. I let CC write an orchestration combining the steps. The prompt for OCR is just ‚Äúconvert image to text‚Äù, GPT 20b prompt to extract invoice information must be somehow tailored to your use case. Medium reasoning is often required to get good results.\n\nFor best OCR I can recommend Chandra. Also make sure that your document really need OCR. Almost all computer-generated PDFs have perfectly extractable text embedded.\n\nHardware: MB Air M3 24 GB -> some patience required, up to 5 minutes per document",
          "score": 1,
          "created_utc": "2026-01-03 15:57:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwreed",
          "author": "patbhakta",
          "text": "You're looking for a silver bullet that doesn't exist.\n\n1) self hosted on limited hardware or free tier SaaS\n2) user can upload whatever and expect it to just work\n\nTrust me those are wonderful things but not achievable with what you're asking. \n\ntake a lesson from the GOAT chatGPT, a free tier limits to only a handful of documents and under a certain size too. it works ok as long as the documents are clean. Clean meaning all computer text, not scanned, no handwriting, charts, tables, pics, diagrams, etc. otherwise your results goes down severely.\n\nBest free tier IMO is Google's notebook llm, this does fine for most users.\n\nDocing is good in principle but take a hard look at the number of github issues...and ask yourself would your users be satisfied with the 2nd criteria.\n\nIf you want something production kinda and cheap you have to do it yourself along with open source projects such as docling. But your 2 requirements are out of scope, you need more constraints or pay up.",
          "score": 1,
          "created_utc": "2025-12-31 13:20:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwws6jw",
              "author": "Hour-Entertainer-478",
              "text": "thanks for the answer. I'm willing to upgrade the hardware. im sure you can understand that i had to ask just be sure that i'm not missing anything.   \nfor 2 i may have been rather vague with my description, the idea was to upload documents, and get a descent parsing (not perfect). \n\nDocling's been working good so far, but i was curious incase someone else has been using something that's better. \n\ni would reckon that you worked on something similar, may i ask what did you use ?",
              "score": 3,
              "created_utc": "2025-12-31 13:25:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwx1od4",
                  "author": "patbhakta",
                  "text": "Before you upgrade hardware you need a clear scope before architecture. \nWhat's your current use case and projection for 2026 and 2027, etc. If the users want everything to be on premise then good luck to you. \n\nBut odds are the staff at the company already uses chatGPT, perplexity, gemini, etc. So information is already being potentially compromised.\n\nI used docling for a bit but it just has too many issues, sure it's fine for testing, even hundreds of documents but it's not for me. I still have it running for testing also use notebookllm and chatGPT for testing (this is what your users see or do)\nIn general they're fine, most people don't even read the whole document much less understand it. So it's \"better than nothing\" which is great from a user standpoint. Problems arise when the information isn't accurate. \n\nMy use case is in fintech, so I don't need good, I need precise,  My architecture is hybrid, some stuff is in house, some stuff doesn't matter like SEC filings that are public knowledge. I use gemini, openai, openrouter, etc for various things like most people. They all have hiccups that's the nature of using LLMs. To reduce these hiccups you need quite a few things depending on your use case. \n\nI'm assuming you need an internal knowledge base that's why you're looking at rag, but knowledge bases have been around even before the internet, searching came soon after. So I'm assuming you need to incorporate LLMs for some reason because searching isn't good enough. This is where things get complicated because exact searches aren't good enough now you need semantic searches and that's where LLMs shine and that's also where things go wrong. \n\nI can go on and on and do deep dives but point is you need to scope first before you architect this...\n1) hardware / budget constraints\n2) privacy vs acceptable sharing\n3) personalized vs company-wide or both\n4) how many users and what speed is acceptable\n5) and so on...\n\nThen you can worry about the fun stuff architecture...\nIngestion, parsers, dedups, embeddings, vectordb, graphdb, tabulardb, agents, tools, orchestration, and which llm to use for what purpose and at what cost/time or and I almost forgot security and privileges...",
                  "score": 1,
                  "created_utc": "2025-12-31 14:22:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwxn4ff",
          "author": "ejpusa",
          "text": "My Parsing Pipeline Plan: GPT-5.2 writes the Python code. Nano Banana + Midjourney for infographics. VEO 3 for animations. Bare metal Linux box on Liquid Web to host it all.\n\nSTEP BY STEP. Do not go onto the next step until the first is done. My Agentic Agenct project, first Agent up this week. AI first drug discovery. \n\nSTEP 1: What is the Mission Statement\n\nThe Parkinson‚Äôs Evidence Atlas is an open, agentic AI research platform designed to continuously ingest, organize, and audit biomedical evidence at scale. By preserving raw data, exposing uncertainty and failure, and systematically surfacing neglected biological signals‚Äîespecially those overlooked for economic rather than scientific reasons‚Äîthe Atlas addresses structural blind spots in drug discovery. By treating AI as transparent research infrastructure rather than a black-box predictor, the project has a real opportunity to reshape how early-stage discoveries are identified, evaluated, and trusted, creating a new, more rigorous pathway for tackling complex neurodegenerative diseases such as Parkinson‚Äôs and Alzheimer‚Äôs.\n\nAnd so it begins.\n\n:-)\n\nThis is a family of Agents, we call them \"Harvesters.\" You can spin these up in minutes. Send them out in the wilds. Sample code:\n\n```\nimport requests\nimport psycopg2\nimport json\nfrom datetime import date\nfrom config import DATABASE_URL, DISEASE_QUERIES\nimport openai\n\nopenai.api_key = None  # set via env\n\nEUROPE_PMC_API = \"https://www.ebi.ac.uk/europepmc/webservices/rest/search\"\n\ndef fetch_updates(query):\n    params = {\n        \"query\": query,\n        \"format\": \"json\",\n        \"pageSize\": 20,\n        \"sort\": \"P_PDATE_D\"\n    }\n    r = requests.get(EUROPE_PMC_API, params=params)\n    r.raise_for_status()\n    return r.json().get(\"resultList\", {}).get(\"result\", [])\n\ndef store_updates(conn, disease, records):\n    cur = conn.cursor()\n    for r in records:\n        cur.execute(\"\"\"\n            INSERT INTO research_updates\n            (source, disease, title, abstract, url, published_date, raw_json)\n            VALUES (%s, %s, %s, %s, %s, %s, %s)\n            ON CONFLICT DO NOTHING\n        \"\"\", (\n            \"EuropePMC\",\n            disease,\n            r.get(\"title\"),\n            r.get(\"abstractText\"),\n            r.get(\"fullTextUrlList\", {}).get(\"fullTextUrl\", [{}])[0].get(\"url\"),\n            r.get(\"firstPublicationDate\"),\n            json.dumps(r)\n        ))\n    conn.commit()\n\ndef generate_daily_summary(conn):\n    cur = conn.cursor()\n    cur.execute(\"\"\"\n        SELECT title, abstract\n        FROM research_updates\n        WHERE fetched_at::date = CURRENT_DATE\n    \"\"\")\n    rows = cur.fetchall()\n\n    if not rows:\n        return None\n\n    text_block = \"\\n\\n\".join(\n        f\"Title: {t}\\nAbstract: {a}\" for t, a in rows if a\n    )\n\n    prompt = f\"\"\"\nSummarize the following Parkinson‚Äôs and Alzheimer‚Äôs research updates.\nBe descriptive, not evaluative.\nDo not speculate on efficacy or importance.\nGroup by general topic if possible.\n\n{text_block}\n\"\"\"\n\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4o-mini\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0.2\n    )\n\n    summary = response.choices[0].message.content\n\n    cur.execute(\"\"\"\n        INSERT INTO daily_summaries (summary_date, summary_text)\n        VALUES (%s, %s)\n        ON CONFLICT (summary_date) DO NOTHING\n    \"\"\", (date.today(), summary))\n    conn.commit()\n\ndef run():\n    conn = psycopg2.connect(DATABASE_URL)\n\n    for disease, query in DISEASE_QUERIES.items():\n        records = fetch_updates(query)\n        store_updates(conn, disease, records)\n\n    generate_daily_summary(conn)\n    conn.close()\n\nif __name__ == \"__main__\":\n    run()\n```",
          "score": -1,
          "created_utc": "2025-12-31 16:14:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q02chq",
      "title": "Looking for someone to collaborate on an ML + RAG + Agentic LLM side project",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q02chq/looking_for_someone_to_collaborate_on_an_ml_rag/",
      "author": "Far-Palpitation4482",
      "created_utc": "2025-12-31 03:17:47",
      "score": 21,
      "num_comments": 22,
      "upvote_ratio": 0.96,
      "text": "Hey! Is anyone here interested in building a side project together involving RAG + LLMs (agentic workflows) + ML?\n\nI‚Äôm not looking for anything commercial right now, just learning + building with someone who‚Äôs serious and consistent.\nIf interested, drop a comment or DM,happy to discuss ideas and skill sets",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q02chq/looking_for_someone_to_collaborate_on_an_ml_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwwda59",
          "author": "lundrog",
          "text": "Discord server for everyone ? Im interested in the concept.",
          "score": 3,
          "created_utc": "2025-12-31 11:31:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwurfr3",
          "author": "liy8",
          "text": "Yep, I'm interested.",
          "score": 2,
          "created_utc": "2025-12-31 03:33:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuu89g",
          "author": "Nivedh2004",
          "text": "Interested",
          "score": 2,
          "created_utc": "2025-12-31 03:50:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuw999",
          "author": "Maleficent_Repair359",
          "text": "interested",
          "score": 2,
          "created_utc": "2025-12-31 04:03:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwuwa8h",
          "author": "remoteinspace",
          "text": "Up for contributing to open source?",
          "score": 2,
          "created_utc": "2025-12-31 04:04:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwv988i",
          "author": "rkpandey20",
          "text": "I am game.¬†",
          "score": 2,
          "created_utc": "2025-12-31 05:32:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvpdck",
          "author": "Powerful-Teacher-188",
          "text": "Yup, interested",
          "score": 2,
          "created_utc": "2025-12-31 07:47:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwvu70p",
          "author": "Ok-Development-9420",
          "text": "Let‚Äôs go!",
          "score": 2,
          "created_utc": "2025-12-31 08:32:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww7faf",
          "author": "glitch1080",
          "text": "interested",
          "score": 2,
          "created_utc": "2025-12-31 10:37:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwnhe2",
          "author": "Critical-Set1190",
          "text": "Instrested right now building a legaltech product that includes agentic rag",
          "score": 2,
          "created_utc": "2025-12-31 12:53:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwri3n",
          "author": "Ai_dl_folks",
          "text": "I'm interested",
          "score": 2,
          "created_utc": "2025-12-31 13:20:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwxncks",
          "author": "Radio-Time",
          "text": "need an idea and teammates",
          "score": 2,
          "created_utc": "2025-12-31 16:15:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysuxx",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2025-12-31 19:44:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwxuv2h",
          "author": "Tricky_Progress_3606",
          "text": "Hey man im interesterd currently building aswell",
          "score": 2,
          "created_utc": "2025-12-31 16:52:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwysw8n",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2025-12-31 19:44:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ss5i",
          "author": "kiran-187",
          "text": "Yes I'm interested",
          "score": 1,
          "created_utc": "2026-01-01 17:11:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4ho10",
              "author": "Far-Palpitation4482",
              "text": "DM me",
              "score": 1,
              "created_utc": "2026-01-01 19:16:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx96khz",
          "author": "legaltextai",
          "text": "i'd be interested if it involves legal area",
          "score": 1,
          "created_utc": "2026-01-02 14:05:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx9b4j6",
          "author": "Altruistic_Leek6283",
          "text": " I'm down.",
          "score": 1,
          "created_utc": "2026-01-02 14:32:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxcbrd8",
          "author": "SubtlyOnTheNose",
          "text": "DOWN, lets get a discord going ya?",
          "score": 1,
          "created_utc": "2026-01-02 23:20:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxhcbro",
          "author": "hlama26",
          "text": "Why not to use the fine tuning concept instead of ML ?",
          "score": 1,
          "created_utc": "2026-01-03 18:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjex0k",
          "author": "distspace",
          "text": "Dm me I‚Äôm current working on one right now\n\nI don‚Äôt mind sharing some ideas",
          "score": 1,
          "created_utc": "2026-01-04 00:37:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pzuerp",
      "title": "Lessons learned from building hybrid search in production (Weaviate, Qdrant, Postgres + pgvector) [OC]",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzuerp/lessons_learned_from_building_hybrid_search_in/",
      "author": "ElBargainout",
      "created_utc": "2025-12-30 21:32:09",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "After shipping hybrid search into multiple production systems (RAG/chatbots, product search, and support search) over the last 18 months, here's a practical playbook of what actually mattered. Full disclosure: we build retrieval/RAG systems for customers, so these are lessons we learned on real traffic, not toy benchmarks.\n\n**Why hybrid search**\n\nVector search finds semantics but misses exact matches (SKUs, IDs, proper nouns). BM25/TF-IDF finds exact tokens but misses paraphrases. Hybrid = pragmatic: combine both and tune for your user needs.\n\n**Quick decision flow (how to pick an approach)**\n\n\\- Need fastest time-to-market + minimal ops? Try a vector DB with built-in hybrid (Weaviate-style) if it fits your scale.  \n\\- Need tight control over scoring, advanced reranking, or best-effort accuracy? Use vector DB (Qdrant/FAISS) + a separate BM25 engine (Postgres full-text or Elasticsearch) and fuse results.  \n\\- Need transactional consistency, joins, or want a single source of truth for metadata and embeddings? Use Postgres + pgvector.\n\n**Patterns & code snippets**  \n  \nBuilt-in hybrid (example: Weaviate-style)  \nPros: simple API, single service, alpha knob for weighting. Cons: less control, black-box internals, possible limits on scale/tuning.\n\nPython pseudo-example:\n\n\\`\\`\\`python  \n\\# high-level example (client API varies by vendor)  \nresults = client.query.get(\"Document\", \\[\"content\"\\]) \\\\  \n.with\\_hybrid(query=\"how to cancel subscription\", alpha=0.7) \\\\  \n.with\\_limit(10) \\\\  \n.do()  \n\\`\\`\\`text\n\nTuning knobs: alpha (0..1), limit, semantic model version, chunking strategy.\n\nWhen to pick: small team, want fewer moving parts, need quick prototype, acceptable to trade some control for speed.\n\n\\---\n\n2) Multi-engine: Qdrant (vectors) + BM25 (Postgres/Elasticsearch)\n\nPattern A: Fuse scores from two full-retrievals  \n\\- Vector DB: get top-N semantic candidates  \n\\- BM25: get top-N lexical candidates  \n\\- Normalize scores and combine (alpha weighting or RRF)\n\nPattern B: Two-stage rerank (fast, often better tail quality)  \n\\- Stage 1: vector search to get \\~100 candidates  \n\\- Stage 2: BM25 (or cross-encoder) reranks those candidates\n\nExample normalization + fusion (Python sketch):\n\n\\`\\`\\`python  \n\\# vector\\_results = \\[{'id':id, 'score':v\\_score}, ...\\]  \n\\# bm25\\_scores = {doc\\_id: raw\\_score}\n\ndef normalize(scores):  \nvals = list(scores.values())  \nmx, mn = max(vals), min(vals)  \nif mx == mn: return {k: 1.0 for k in scores}  \nreturn {k: (v - mn) / (mx - mn) for k, v in scores.items()}\n\nvec = {r\\['id'\\]: r\\['score'\\] for r in vector\\_results}  \nvec\\_n = normalize(vec)  \nbm25\\_n = normalize(bm25\\_scores)\n\nalpha = 0.7  \ncombined = {}  \nfor doc in set(vec\\_n) | set(bm25\\_n):  \ncombined\\[doc\\] = alpha \\* vec\\_n.get(doc, 0) + (1 - alpha) \\* bm25\\_n.get(doc, 0)\n\nranked = sorted(combined.items(), key=lambda x: x\\[1\\], reverse=True)  \n\\`\\`\\`text\n\nTrade-offs: more infra and operational complexity, but more control over scoring, reranking, and caching. Two-stage rerank gives best cost/quality trade-off in many cases.\n\n\\---\n\n3) Postgres + pgvector (single-system hybrid)\n\nWhy choose this: transactional writes, rich joins (user/profile metadata), ability to keep embeddings in the same DB as your authoritative rows.\n\nExample schema and query (Postgres 14+ with pgvector extension):\n\n\\`\\`\\`sql  \n\\-- table: documents(id serial, content text, embedding vector(1536), ts tsvector)  \n\\-- create index on vector  \nCREATE INDEX ON documents USING ivfflat (embedding vector\\_cosine\\_ops) WITH (lists = 100);  \n\\-- create full-text index  \nCREATE INDEX documents\\_ts\\_idx ON documents USING GIN (ts);\n\n\\-- hybrid query: weight vector distance and text rank  \nSELECT id, content,  \n(1 - (embedding <#> :query\\_embedding)) AS vec\\_sim,    -- cosine distance -> similarity  \nts\\_rank\\_cd(ts, plainto\\_tsquery(:q)) AS ft\\_rank,  \n0.7 \\* (1 - (embedding <#> :query\\_embedding)) + 0.3 \\* ts\\_rank\\_cd(ts, plainto\\_tsquery(:q)) AS hybrid\\_score  \nFROM documents  \nWHERE ts @@ plainto\\_tsquery(:q)  \nORDER BY hybrid\\_score DESC  \nLIMIT 20;  \n\\`\\`\\`text\n\nNotes:  \n\\- pgvector uses \\`<->\\` or \\`<#>\\` operators depending on metric; check your version.  \n\\- You can include rows that don't match ts query by using LEFT JOIN or removing the WHERE clause and controlling ft\\_rank nulls.  \n\\- Use \\`lists\\` (IVF lists) and \\`probes\\` tuning for ivfflat; \\`lists\\` affects index size and build time.\n\nTrade-offs: single system simplicity and ACID guarantees vs scaling limits (need to shard or read-replicate for very high throughput). Maintenance (VACUUM, ANALYZE) matters.\n\n**Tuning knobs that actually moved metrics for us**\n\n\\- Chunking: semantic-aware chunks (paragraph boundaries) beat fixed token windows for recall.  \n\\- Alpha (vector vs BM25): tune per use-case. FAQ/support: favor vector (\\~0.7). SKU/product-id exact-match: lower alpha.  \n\\- Candidate set size for rerank: retrieving 100-500 candidates and reranking often beats smaller sets.  \n\\- Normalization: min-max per-query or rank-based fusion (RRF) is safer than naive raw-score mixing.  \n\\- Embedding model: pick one and be consistent; differences matter less than chunking and reranking.  \n\\- Index params: nlist/nprobe (FAISS), ef/search\\_k (HNSW), \\`lists\\`/\\`probes\\` (pgvector ivfflat) ‚Äî tune for your latency/recall curve.\n\n**Evaluation checklist (offline + online)**\n\nOffline:  \n\\- recall@k (k = 10, 50)  \n\\- MRR (mean reciprocal rank)  \n\\- Precision@k if relevance labels exist  \n\\- Diversity / redundancy checks\n\nOnline:  \n\\- latency p50/p95 (target SLA)  \n\\- synthetic query coverage (tokenized vs paraphrase)  \n\\- task success (e.g., issue resolved, product clicked)  \n\\- cost per Q (compute + index storage)\n\nExperimentation:  \n\\- A/B test different alphas and reranker thresholds  \n\\- Log and sample failures for manual review\n\n**Ops & deployment tips**\n\n\\- Version embeddings: store model name + version to allow reindexing safely.  \n\\- Incremental reindex: prefer small, transactional updates rather than bulk rebuilds where possible.  \n\\- Cache hot queries and pre-warm frequently used embeddings.  \n\\- Monitor drift: embeddings/models change over time; schedule periodic re-evaluation.  \n\\- Fallbacks: if vector DB fails, use BM25-only fallback instead of returning an error.  \n\\- Attribution: always include source IDs/snippets in generated responses to avoid hallucination.\n\n**Common failure modes**\n\n\\- Mixing raw scores without normalization ‚Äî one engine dominates.  \n\\- Using too-small candidate set and missing correct docs.  \n\\- Not accounting for metadata (date, user region) in ranking ‚Äî causes irrelevant hits.  \n\\- Treating hybrid as a silver bullet: some queries need exact filters before retrieval (e.g., rate-limited or region-restricted docs).\n\n\\---\n\n**Example test queries to validate hybrid behavior**\n\n\\- \"how to cancel subscription\"  \n\\- \"SKU-12345 warranty\"  \n\\- \"refund policy for order 9876\"  \n\\- \"best GPU for training transformer models\"\n\nFor each query, inspect: top-10 results, source IDs, whether exact-match tokens rank up, and whether paraphrase matches appear.\n\n\\---\n\n**TL;DR**\n\n\\- Hybrid = vectors + lexical. Pick the approach based on control vs speed-to-market vs transactional needs.  \n\\- Weaviate-style built-in hybrid is fastest to ship; multi-engine (Qdrant + BM25) gives most control and best quality with reranking; Postgres+pgvector gives transactional simplicity and joins.  \n\\- Chunking, candidate set size, and normalization/reranking matter more than small differences in embedding models.  \n\\- Always evaluate with recall@k, MRR, and online KPIs; version embeddings and plan for incremental reindexing.\n\nI'd love to hear how others fuse scores in production: do you prefer normalization, rank fusion (RRF), or two-stage rerank? What failure modes surprised you?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1pzuerp/lessons_learned_from_building_hybrid_search_in/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx231el",
          "author": "getarbiter",
          "text": "Solid playbook. One thing we've been experimenting with that complements hybrid retrieval: adding a coherence scoring stage after fusion.\n\nThe problem with alpha tuning is you're still combining two \"how close is this?\" signals. Neither actually answers \"does this candidate resolve the query under its constraints?\"\n\nExample: \"refund policy for order 9876\" ‚Äî hybrid might rank a general refund FAQ highly (good semantic match, maybe BM25 hit on \"refund\"). But it doesn't satisfy the constraint field (specific order). A coherence check catches that.\n\nWe run it as a lightweight post-retrieval filter: retrieve 50-100 candidates with hybrid ‚Üí score coherence against query constraints ‚Üí pass top-k to generation.\n\nThe nice part is it's orthogonal to your retrieval stack. Works with Weaviate, Qdrant, pgvector, whatever. 26MB engine, deterministic, runs locally.\n\nCurious if you've experimented with anything similar ‚Äî rerankers partially address this but they're still similarity-based.",
          "score": 1,
          "created_utc": "2026-01-01 09:32:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nww6ojl",
          "author": "ElBargainout",
          "text": "To kick things off, I'm really curious about everyone's experience with RRF (Reciprocal Rank Fusion) vs. Score Normalization.\n\nWe settled on normalization because it felt more predictable for our specific dataset, but I hear a lot of praise for RRF being more robust out-of-the-box. Has anyone here switched from one to the other and noticed a significant jump in MRR?",
          "score": 0,
          "created_utc": "2025-12-31 10:31:01",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nwtm3p6",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -1,
          "created_utc": "2025-12-30 23:36:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nww6rq9",
              "author": "ElBargainout",
              "text": "This is a fantastic addition. The point about checksumming reference embeddings to catch silent API updates is something I hadn‚Äôt implemented, but I‚Äôm definitely stealing that idea. It‚Äôs a silent killer for long-running systems.\n\nRegarding temporal relevance, it‚Äôs tricky because you don't want recency to override semantic relevance entirely (e.g., a relevant doc from 2022 vs. an irrelevant one from today).\n\nHere is how we usually handle it in production:\n\n\\- Decay Functions (The \"Soft\" Way): In systems like Elasticsearch or Weaviate, we apply a Gaussian decay function based on the timestamp. It acts as a multiplier on the hybrid score. The key is tuning the scale so the decay isn't too aggressive. Usually, we just want to break ties or give a slight edge to newer content, not bury older foundational docs.\n\n\\- Hard Filters (The \"Hard\" Way): For things like news or urgent support issues, we sometimes enforce a hard filter (e.g., date > now - 1 year) before the vector search, but this hurts recall for \"legacy\" problems.\n\n\\- Reranking Signal: If we use a cross-encoder (Stage 2), we sometimes inject the date into the text passed to the reranker (e.g., \\[Date: 2024-01-01\\] Content...) so the model explicitly sees the freshness, though this relies heavily on the model's training.\n\nDo you use the decay approach, or have you found a way to incorporate recency directly into the RRF logic?",
              "score": 1,
              "created_utc": "2025-12-31 10:31:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1py8l8f",
      "title": "I built a Python library that translates embeddings from MiniLM to OpenAI ‚Äî and it actually works!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
      "author": "Interesting-Town-433",
      "created_utc": "2025-12-29 01:23:16",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "*I built a Python library called* ***EmbeddingAdapters*** *that* ***provides multiple pre-trained adapters for translating embeddings from one model space into another***:\n\n[https://github.com/PotentiallyARobot/EmbeddingAdapters/](https://github.com/PotentiallyARobot/EmbeddingAdapters/)\n\n\\`\\`\\`  \n`pip install embedding-adapters`\n\n`embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text \"Where can I get a hamburger near me?\"`  \n\\`\\`\\`\n\n*This works because* ***each adapter is trained on a restrictive domain*** allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity.¬† ***A quality endpoint then lets you determine how well the adapter will perform*** *on a given input.*\n\nThis has been super useful to me, and I'm quickly iterating on it.\n\nUses for ***EmbeddingAdapters*** so far:\n\n1. You want to **use an existing vector index built with one embedding model and query it with another** \\- if it's expensive or problematic to re-embed your entire corpus, this is the package for you.\n2. You can also **operate mixed vector indexes** and map to the embedding space that works best for different questions.\n3. You can **save cost on questions that are easily adapted**, \"What's the nearest restaurant that has a Hamburger?\" no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.\n\nIt also lets you experiment with provider embeddings you may not have access to.¬† By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.\n\nThis makes it practical to:  \n\\- **sample providers you don't have direct access to**  \n\\- **migrate or experiment with embedding models gradually** instead of re-embedding everything at once,  \n\\- ***evaluate multiple providers side by side*** in a consistent retrieval setup,  \n\\- ***handle provider outages or rate limits*** without breaking retrieval,  \n\\- ***run RAG in air-gapped or restricted environments*** with no outbound embedding calls,  \n\\- ***keep a stable ‚Äúcanonical‚Äù embedding space*** while changing what runs at the edge.\n\nThe adapters aren't perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 98% of the openai embedding and dramatically outperforms minilm -> minilm RAG setups\n\nIt's still early¬†in this project. I‚Äôm actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the¬†models and improving evaluation and quality tooling.\n\nI‚Äôd love feedback from anyone who might be interested in using this:  \n*- What data would you like to see these adapters trained on?*  \n*- What domains would be most helpful to target?*  \n*- Which model pairs would you like me¬†to add next?*  \n*- How could¬†I make this more useful for you to use?*\n\nSo far the¬†library supports:  \n*minilm <-> openai*¬†  \n*openai <-> gemini*  \n*e5 <-> minilm*  \n*e5 <-> openai*  \n*e5 <-> gemini*  \n*minilm <-> gemini*\n\nHappy to answer questions and if anyone has any ideas please let me know.  \nI could use any support you can give, especially if anyone wants to chip in to help cover the training cost.\n\nPlease upvote if you can, thanks!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1py8l8f/i_built_a_python_library_that_translates/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwii84p",
          "author": "-Cubie-",
          "text": "Nice work! This is cool. How does it train the adapter, and what is the network for the adapter? A single Linear with the correct input/output dimensionality trained with distillation?\n\nIt reminds me a bit of model distillation to finetune a small local embedding model to match a bigger one: https://sbert.net/examples/sentence_transformer/training/distillation/README.html Such s fascinating strategy.",
          "score": 2,
          "created_utc": "2025-12-29 08:24:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwjv4le",
              "author": "Mysterious_Robot_476",
              "text": "Thanks! Really appreciate the support. Yeah, I'm super excited to share!\n\nRe architecture, I tried purely linear ( there are a couple in the registry ) but actually the mapping between embedding spaces is only mostly linear, not entirely, so higher accuracy models do benefit from non linearity. The balance is really how much size you want to add to the model. Still trying to figure out the best architecture for capturing, right now I'm leaning more toward a MOE of MLPs with residual connections, but still very flexible here.  The v2 models I'm training are more aligned with this.\n\nIt does seem to point to something interesting though, how can a small llm like minilm capture so much of a massive provider model ( even in a restricted domain ). Something deeper going on here perhaps?\n\nI'm training the v2 models atm expanding the training set size and providers, next version of embedding-adapters I plan to add fine tuning scripts so people can experiment here and upload their own adapters easier.  You can add to the registry as is if you have a working adapter as well, use the cli add  functionality or make a pr to the embedding-adapters registry\n\nhttps://github.com/PotentiallyARobot/embedding-adapters-registry",
              "score": 2,
              "created_utc": "2025-12-29 14:43:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nx3ht22",
          "author": "Last-Application-558",
          "text": "Wow cool! I will check in details",
          "score": 1,
          "created_utc": "2026-01-01 16:13:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pxzo7t",
      "title": "Vector DBs for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pxzo7t/vector_dbs_for_rag/",
      "author": "hackdev001",
      "created_utc": "2025-12-28 19:13:14",
      "score": 16,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\n  \nI am working on a rag application and was confused on which vector db should I go ahead with? I have currently integrated Qdrant as it is open source I can deploy it to my own servers.\n\n  \nHowever, I dont really know how to judge the accuracy of the application. Does different vector dbs give different results in terms of accuracy?  \nIf yes, then which ones are the most accurate and SOTA?\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pxzo7t/vector_dbs_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nweunin",
          "author": "Spursdy",
          "text": "Accuracy won't change by changing the database. The embedding model , number of dimensions and distance algorithm will change the accuracy.\n\nI use postgres with ph_vector.",
          "score": 10,
          "created_utc": "2025-12-28 19:21:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwff3cw",
              "author": "EveYogaTech",
              "text": "üíØ We also use pg_vector, for most use-cases it seems to be enough.",
              "score": 4,
              "created_utc": "2025-12-28 21:00:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwntgmb",
                  "author": "344lancherway",
                  "text": "Have you experimented with different distance metrics in pg_vector? Sometimes tweaking those can make a big difference in how well your model performs.",
                  "score": 1,
                  "created_utc": "2025-12-30 02:38:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf199r",
              "author": "hackdev001",
              "text": "I am using latest google embedding model. So that should be good. Distance algorithm is something with which I can compare and verify",
              "score": 1,
              "created_utc": "2025-12-28 19:53:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwfw5vl",
          "author": "Valeria_Xenakis",
          "text": "Yes, different vector databases can return different results ‚Äî but it‚Äôs usually not because one is ‚Äúsmarter.‚Äù It‚Äôs because most production vector search is approximate and each system (and even each config) trades speed vs retrieval recall (how often you actually get the true nearest neighbours).\n\nPeople also mix up two kinds of ‚Äúaccuracy‚Äù:\n\n1) Retrieval accuracy (are you fetching the right chunks?)\n\nMost vector databases use Approximate Nearest Neighbour search instead of exact nearest neighbour search, because exact is too slow once you have lots of vectors.\n\nSo results can differ due to:\n\nA. Index type (for example, Hierarchical Navigable Small World graphs)\n\nB. Different defaults (some default ‚Äúfast but lower recall‚Äù)\n\nC. Different tuning knobs (how hard the search works)\n\nSo yes: two databases can produce different top-k chunks if one is more aggressively approximate than the other.\n\n2) End-to-end Retrieval-Augmented Generation accuracy (is the final answer good?)\n\nIn practice, the bigger drivers for this are:\n\nA. Embedding model quality\n\nB. Chunking strategy\n\nC. Query rewriting\n\nD. Reranking (second stage re-ordering with a stronger model)\n\nDatabase choice matters, but it‚Äôs rarely the main reason your Retrieval-Augmented Generation answers are good/bad ‚Äî unless you‚Äôre under-tuned or using heavy compression.\n\nHow to test it:\n\nDo it in two layers:\n\nStep 1: Measure retrieval recall\n\nMake a small eval set (like 50‚Äì200 real queries).\n\nThen:\n\n1. Run exact search (full scan) to get ground truth neighbours\n\n2. Run your normal approximate search\n\n3. Compute Recall at K: ‚ÄúHow many of the true top-K did the approximate search retrieve?‚Äù\n\nThis tells you if you‚Äôre losing too much recall for speed.\n\nStep 2: Measure answer quality\n\nOn the same queries, score final answers using:\n\nA. Ground truth answers (if you have them)\n\nOR\n\nB. a mix of human review + automated judging\n\nThis tells you whether retrieval differences actually matter for your use case.\n\nFor the question which vector database is most accurate / state of the art?‚Äù\n\nIf you mean ‚Äúmost accurate possible‚Äù: exact search is the most accurate, but expensive.\n\nPS: The answer has been formatted and structured using AI. Protect our community against a dead internet due to overuse of AI",
          "score": 7,
          "created_utc": "2025-12-28 22:25:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwexmst",
          "author": "bzImage",
          "text": "want accuracy ? .. use qdrant metadata and save the chunk of data \"keywords\".. .. now.. you can filter and search by keywords.. \n\nkeywords > vector search for accuracy",
          "score": 5,
          "created_utc": "2025-12-28 19:35:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf0vw2",
              "author": "hackdev001",
              "text": "Thats interesting but in a long document, a few keywords can sometimes give inaccurate results. Maybe hybrid (keyword + vector) is the way to go here",
              "score": 1,
              "created_utc": "2025-12-28 19:51:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwji395",
                  "author": "bzImage",
                  "text": "Use llm to generate good size chunks and extract keywords..",
                  "score": 1,
                  "created_utc": "2025-12-29 13:27:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nwf1zrt",
              "author": "websinthe",
              "text": "Yeah, keywords/metadata, chunking strategy, and graph storage are all orders of magnitude more important but they're kinda treated like afterthoughts.",
              "score": 1,
              "created_utc": "2025-12-28 19:56:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nweuo0m",
          "author": "DressMetal",
          "text": "I use Chroma for now, works fine.",
          "score": 2,
          "created_utc": "2025-12-28 19:21:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwezg1z",
          "author": "RolandRu",
          "text": "In my case I‚Äôm doing RAG for code, so I evaluate retrieval directly. I run a fixed set of questions against a test repo and check whether the expected files/functions/fragments show up in the top-k retrieved chunks. Only after that I look at the final LLM answer. This keeps the evaluation focused on retrieval quality instead of being fooled by a fluent answer that isn‚Äôt grounded in the right context.",
          "score": 2,
          "created_utc": "2025-12-28 19:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwf1kck",
          "author": "websinthe",
          "text": "The best results I've had came from just using Polars. It's a little more work but not a huge amount, it's significantly more performant, and vector DBs are such a small part of a RAG that it's not worth taking on the constant breakages the named VDBs have on updates.\n\nOtherwise I'd just use chromadb.\n\nSpend 5% of your time choosing the vector database and 95% perfecting your chunking strategy. That's the important part.\n\nSorry if this is preachy.",
          "score": 1,
          "created_utc": "2025-12-28 19:54:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwf2z9l",
              "author": "hackdev001",
              "text": "How are you using Polars for a Rag application?\nAnd how is it a substitute for a vector db?",
              "score": 2,
              "created_utc": "2025-12-28 20:01:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx48xeh",
                  "author": "websinthe",
                  "text": "A vector db is just a high-dimensional store of vector embeddings. Polars is kinda like Numpy and can have arbitrarily-dimensional storage of vectors, and if you have code that takes tokens and turns them into embeddings, there's nit a lot that a vector db does that isn't just a big multi-dimensional array.",
                  "score": 2,
                  "created_utc": "2026-01-01 18:32:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwfen1a",
          "author": "hrishikamath",
          "text": "Unless you really have huge amount or data, pg vector w Postgres is more than enough",
          "score": 1,
          "created_utc": "2025-12-28 20:58:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwlbk68",
          "author": "Potential-Buy-4267",
          "text": "Chromadb, pgvector,...",
          "score": 1,
          "created_utc": "2025-12-29 18:54:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pyn4hj",
      "title": "Why is there no opinionated all in one RAG platform?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pyn4hj/why_is_there_no_opinionated_all_in_one_rag/",
      "author": "Pl8tinium",
      "created_utc": "2025-12-29 14:05:40",
      "score": 15,
      "num_comments": 26,
      "upvote_ratio": 0.86,
      "text": "Im skimming through the web and unfortunately cannot find a SOTA maintained FOSS platform for RAG. I identified some platforms like\n\nQuivr but they dont seem to be maintained anymore.\n\n[https://github.com/QuivrHQ/quivr](https://github.com/QuivrHQ/quivr)\n\nI also identified a lot of frameworks that make it easier to build RAG apps like llamaindex, RAGflow, dify etc., but they dont provide the opinionated blackbox experience im searching for.\n\nSure there are also those \"all in one\" platforms like openwebui or localGPT that provide RAG capabilities and have an opinionated pipeline. But often times their primary focus is not just RAG and they thereby often do not incorporate SOTA techniques into their products. Also they are often built so that you could use them in conjunction with the rest of the package, not to just deliver the RAG results to another frontend.\n\n[https://github.com/PromtEngineer/localGPT](https://github.com/PromtEngineer/localGPT)\n\n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nThat being said i do think that it most definetly makes sense that there would be one giant FOSS project always keeping track of the latest and greatest techniques and would just provide a set of valves to tweak functionality and individualize the experience. Other proprietary vendors also try to provide this like Microsoft 365 Copilot Agent or Snowflake cortex. In these cases there is often a sophisticated RAG pipeline in place that does things like\n\n\\- broad chunk search, then narrow down when it identified focus on a specific document\n\n\\- expanding the context of found chunks\n\n\\- intermediate summarizations of docs when working with a large amount of docs simultaneously\n\n\\- ....\n\nAll of those things help to provide a great experience, but for me as a \"one engineer in the ai team\" cannot build, maintain and keep a self built RAG solution up to date to the latest and greatest additions to the space.\n\nJust to note one could say that an \"one size fits all\" solution is not possible, especially because data is so different from system to system, but i'd argue that many proprietary platforms like Microsoft 365 Copilot have perfected this already and can easily be plugged in to any arbitrary form of data and  work relatively well (atleast if the data is in one of the basic formats of data like txt, pdf, pptx, docx ...)\n\nIdeally i would want a RAG platform that always is relatively close behind SOTA, there would be (community) created adapters for enterprise data stores like sharepoint, SAP etc and allows for simple integration into other systems. I'd also pay for this, its not like i just want FOSS, but I would think that the community would also have identified a need for this..\n\nIs what im thinking about valid or is my department just too small to do any meaningful RAG and i should upgrade to more personal so i have the capability to build and maintain RAG pipelines from the ground up or am i just not noticing some development in the space?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pyn4hj/why_is_there_no_opinionated_all_in_one_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwjwuld",
          "author": "fabkosta",
          "text": "Why is there no single database system but many? Same reasons apply to RAG.",
          "score": 7,
          "created_utc": "2025-12-29 14:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx4ar6a",
              "author": "hhussain-",
              "text": "One difference worth mentioning here: databases have solid definitions (DBRM) and well searched mechanism to achieve those definitions in reality. RAG, like the rest of AI arena, is well defined but mechanisms are still and R&D race. This is why all those $ billions are burned into it without really reaching a solid optimized mechanisms.\n\nCompanies are racing, cash is burnt, we enjoy what we can enjoy!",
              "score": 1,
              "created_utc": "2026-01-01 18:41:58",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwjxjsu",
              "author": "Pl8tinium",
              "text": "theres opinionated systems for many things though, eg docusaurus is an opinionated markdown documentation platform with the purpose to make it easy for people to bootstrap a general docu platform without glueing all things together themselves. Why not for RAG?",
              "score": -1,
              "created_utc": "2025-12-29 14:56:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwjzabk",
                  "author": "stingraycharles",
                  "text": "Because RAG is always just a component in a larger system rather than a system itself.",
                  "score": 6,
                  "created_utc": "2025-12-29 15:05:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwjy42i",
          "author": "ampancha",
          "text": "The reason a \"One Size Fits All\" FOSS platform doesn't exist is that SOTA retrieval techniques (like Parent-Document or Late Interaction) are highly dependent on your specific data topology. A \"Black Box\" that works for PDFs will fail on SQL tables.\n\nThe closest you get to \"maintained SOTA\" without building from scratch is adopting an **Opinionated Reference Architecture** rather than a framework. I maintain a **Standard RAG** repo that implements the exact pipeline you described (Broad Search -> Narrow Focus -> Context Expansion) as a deployable microservice, specifically for teams that don't have the bandwidth to reinvent the wheel. I sent you a DM",
          "score": 3,
          "created_utc": "2025-12-29 14:59:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk1doi",
              "author": "Pl8tinium",
              "text": "sure that would be something i could use but i want to have that maintained not by a single individual but some kind of company or FOSS contributors, otherwise i dont see the steady maintenance and incorporation of new ideas.\n\n\nAlso, sure you could say the described techniques quickly fail on eg sql tables but tbh the majority of rag use cases is standard txt/ pdf or something that is converted to pdf, thereby allowing some assumptions on the data formats",
              "score": 1,
              "created_utc": "2025-12-29 15:16:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwks3a7",
          "author": "TrustGraph",
          "text": "TrustGraph is not only open source, but is maintained (1.7 was just released and 1.8 is already in testing) and in production with users. TrustGraph has been pioneering the concept of context graphs (funny that term is just now catching on) for nearly 2 years. Best I can tell, we were also the first to use the term \"TemporalRAG\" as well. Our temporal features likely won't but coming until 2.0 though.\n\nStill open source & in active development: [https://github.com/trustgraph-ai/trustgraph](https://github.com/trustgraph-ai/trustgraph)",
          "score": 3,
          "created_utc": "2025-12-29 17:24:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjyc4l",
          "author": "OnyxProyectoUno",
          "text": "Every all-in-one platform I've seen makes the same mistake. They focus on retrieval orchestration while the real problems happen upstream during document processing.\n\nYou can have the most sophisticated reranking and context expansion in the world, but if your PDFs got mangled during parsing or your chunking strategy is splitting sentences mid-thought, you're building on quicksand. Most teams discover their chunking is broken only after they've already embedded everything and are debugging weird responses three conversations deep.\n\nThe platforms you mentioned treat document preprocessing as solved when it's actually where most RAG systems break. Tables get scrambled, metadata gets dropped, section hierarchy gets flattened. By the time you're looking at similarity scores you're three steps removed from the root cause.\n\nWhat you need isn't another orchestration layer. You need visibility into what your documents actually look like after parsing and chunking, before they hit the vector store. That's the angle I've been taking with [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_c), focusing specifically on the preprocessing pipeline rather than trying to be everything to everyone.\n\nThe enterprise platforms work because they control the entire stack and can see their preprocessing output. You can't debug what you can't see.",
          "score": 4,
          "created_utc": "2025-12-29 15:00:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk0pg3",
              "author": "Pl8tinium",
              "text": "I dont need to debug these platforms because they just work. if i would build something myself, i agree, a monitoring platform for the input data may be helpful.\n\n\nBut the core of my requirement is not monitoring because i dont want to build myself and there is no inherit rule that says that what the enterprise platforms do is not feasible for a FOSS project. They all start their pipelines with the same input eg a pdf",
              "score": 2,
              "created_utc": "2025-12-29 15:13:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkvchk",
          "author": "ChapterEquivalent188",
          "text": "Everyone is building \"Chat with PDF\" wrappers, but nobody is solving the deep ingestion engineering required for enterprise-grade reliability.\n\nI'm a solo dev and I just released the architecture manifest for RAG Enterprise Core V3.0. It‚Äôs an opinionated, \"batteries-included\" platform designed to solve the exact problems you mentioned (complex docs, tables, hallucinations).\n\nMy approach is different: Instead of trusting one ingestion method, I built a \"Multi-Lane Consensus Engine\" (Solomon).\n\nIt runs parallel extraction lanes:\n\nFast Lane: PyMuPDF (Text) \n\nSmart Lane: Docling (Structure/Markdown) \n\nVision Lane: VLM/Ollama (Charts & Images)\n\nso\n\nThe engine then votes on the \"Ground Truth\" and reconciles conflicts before indexing into a Neo4j Graph + ChromaDB Vector store. It also includes a \"Surgical HITL\" UI where you only verify the specific tokens the AI disagreed on.\n\nI haven't open-sourced the full code yet (IP reasons), but I just published the V3 Architecture Manifest and a Live Demo Video showing the consensus engine in action. It might give you some ideas for your own stack, or at least validate that you aren't crazy for wanting an \"all-in-one\" solution.\n\nCheck the Manifest & Demo here:\n\n[https://github.com/2dogsandanerd/RAG\\_enterprise\\_core](https://github.com/2dogsandanerd/RAG_enterprise_core)",
          "score": 2,
          "created_utc": "2025-12-29 17:39:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp32fh",
              "author": "Pl8tinium",
              "text": "damn that atleast sounds pretty cool, ill check it out!",
              "score": 2,
              "created_utc": "2025-12-30 07:56:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwk67sh",
          "author": "Horror-Turnover6198",
          "text": "I agree with OP and think it‚Äôs odd that there isn‚Äôt a single plug and play RAG box that would handle PDFs, text and markdown, with a standard ingestion and retrieval API. I know it‚Äôs not prohibitively hard to roll that but I know that I sure had to do a ton of reading and trial and error to even get started. \n\nMy guess is there just hasn‚Äôt been enough time for a standard framework to develop. For example, before Symfony, and later Laravel, PHP was a mishmash solving the same problems on every project. I don‚Äôt love every design decision of those frameworks but holy crap do they do a lot of heavy lifting to get new devs started. Why not have that sort of thing for RAG?",
          "score": 1,
          "created_utc": "2025-12-29 15:40:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwl5j6i",
          "author": "remoteinspace",
          "text": "Have you tried mem0 or papr?",
          "score": 1,
          "created_utc": "2025-12-29 18:26:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp3n4l",
              "author": "Pl8tinium",
              "text": "mem0 sounds cool and i may consider it in addition to my RAG capabilities i wanna provide, thx!",
              "score": 1,
              "created_utc": "2025-12-30 08:02:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwn4iu5",
          "author": "GP_103",
          "text": "Proprietary platforms like MS 365 Copilot certainly does not work well with dense PDFs.",
          "score": 1,
          "created_utc": "2025-12-30 00:21:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwp49he",
              "author": "Pl8tinium",
              "text": "ive had good experiences with docs that had atleast 200 pages, what are your experiences about its limits?",
              "score": 1,
              "created_utc": "2025-12-30 08:07:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwqj9ni",
          "author": "RolandRu",
          "text": "There‚Äôs a real demand for an ‚Äúopinionated RAG black box,‚Äù but the reason it rarely exists (and stays SOTA) is that RAG is *mostly integration + evaluation*, not just a pipeline recipe.\n\nA platform has to pick defaults for: parsing, chunking, embedding model, hybrid retrieval, reranking, query routing, caching, ACLs, connectors, observability, and (hardest) **how you measure ‚Äúgood‚Äù** across wildly different corpora. The moment it‚Äôs opinionated, it breaks for someone‚Äôs data shape, compliance rules, latency budget, or cost ceiling ‚Äî and now the maintainer is on the hook.\n\nWhat tends to work in practice is ‚Äúopinionated core + pluggable edges‚Äù:  \na solid ingestion + ACL story\n\nhybrid retrieval + reranking as a default\n\nstrong eval harness (golden Q/A sets, regression tests, drift monitoring)\n\nconnectors as community modules\n\nan API-first design so you can pipe results into any frontend.\n\nIf you‚Äôre a small team, I‚Äôd aim for a maintained base stack + a thin layer of your own opinions (connectors + eval + guardrails). The eval layer is the part that keeps you ‚Äúnear SOTA‚Äù longer than chasing the newest chunking trick every month.",
          "score": 1,
          "created_utc": "2025-12-30 14:42:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwvz2sf",
              "author": "Pl8tinium",
              "text": "this is the best reply so far IMO, thank you very much! Do you have oncrete examples what a maintained base layer may be/ look like? can you drop names?",
              "score": 2,
              "created_utc": "2025-12-31 09:19:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nww0qqf",
                  "author": "RolandRu",
                  "text": "Maintained base layer = the ‚Äúboring but critical‚Äù foundations you don‚Äôt want to hand-roll: storage/search, ingestion/parsing, and eval/observability.\n\nConcrete names people commonly use:  \nQdrant / Weaviate / Milvus (vector DB)  \nHaystack / LlamaIndex / LangChain (RAG framework)  \nUnstructured (document parsing/ingestion)  \nLangfuse or Arize Phoenix (tracing/observability)  \nRagas / TruLens / DeepEval + promptfoo in CI (evaluation/regression)\n\nA typical maintained stack looks like:  \nUnstructured ‚Üí Qdrant (or Weaviate/Milvus) ‚Üí Haystack (or LlamaIndex) + Langfuse + Ragas/promptfoo.",
                  "score": 1,
                  "created_utc": "2025-12-31 09:35:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrl7mc",
          "author": "coderarun",
          "text": "This is a data centric view. The first step towards such an opinionated RAG is to have one database that does keyword/vector/graph searches well, runs embedded (no server to run) and gives you 80% of what you need. The focus in r/RAG tends to be on the python/TS package that runs on top of the database and these packages support 5-10 databases with vast differences in capability.\n\nWith coding models becoming more capable, developers can generate their own python/TS package to suit their needs. They're nowhere near writing their own database, indexing or query optimizations though.",
          "score": 1,
          "created_utc": "2025-12-30 17:44:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwrxf1s",
          "author": "digital_legacy",
          "text": "What could we add to make our solution what you need? We have a local Docker setup, UI and plugable models and it's open source. [https://www.reddit.com/r/eMediaLibrary/](https://www.reddit.com/r/eMediaLibrary/)\n\nWe currently have drivers for two RAG approaches. LlamaIndex and [ThoughtFrame.ai](http://ThoughtFrame.ai)",
          "score": 1,
          "created_utc": "2025-12-30 18:40:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwwekbz",
          "author": "Powerful-Ad-7237",
          "text": "Check out Piragi: https://github.com/hemanth/piragi\n\nFeatures\n\nZero Config - Works with free local models out of the box\nAll Formats - PDF, Word, Excel, Markdown, Code, URLs, Images, Audio\nRemote Storage - Read from S3, GCS, Azure, HDFS, SFTP with glob patterns\nWeb Crawling - Recursively crawl websites with /** syntax\nAuto-Updates - Background refresh, queries never blocked\nSmart Citations - Every answer includes sources\nPluggable Stores - LanceDB, PostgreSQL, Pinecone, Supabase, or custom\nAdvanced Retrieval - HyDE, hybrid search, cross-encoder reranking\nSemantic Chunking - Context-aware and hierarchical chunking\nKnowledge Graph - Entity/relationship extraction for better answers\nAsync Support - Non-blocking API for web frameworks",
          "score": 1,
          "created_utc": "2025-12-31 11:42:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nx4b7sw",
          "author": "hhussain-",
          "text": "I believe this is because of maturity level in RAG and AI in general. There are many missing keys, and community don't have that leverage of putting the R&D time without ROI which is main concern in FOSS. For sure FOSS have the required skillset, but you can imagine what would happen if a key is released as FOSS. Motivation is hard.",
          "score": 1,
          "created_utc": "2026-01-01 18:44:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q0v8jp",
      "title": "GraphQLite - Embedded graph database for building GraphRAG with SQLite",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q0v8jp/graphqlite_embedded_graph_database_for_building/",
      "author": "Fit-Presentation-591",
      "created_utc": "2026-01-01 04:03:30",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "For anyone building GraphRAG systems who doesn't want to run Neo4j just to store a knowledge graph, I've been working on something that might help.\n\nGraphQLite is an SQLite extension that adds Cypher query support. The idea is that you can store your extracted entities and relationships in a graph structure, then use Cypher to traverse and expand context during retrieval. Combined with sqlite-vec for the vector search component, you get a fully embedded RAG stack in a single database file.\n\nIt includes graph algorithms like PageRank and community detection, which are useful for identifying important entities or clustering related concepts. There's an example in the repo using the HotpotQA multi-hop reasoning dataset if you want to see how the pieces fit together.\n\n\\`pip install graphqlite\\`\n\nHope someone finds this useful.\n\nGitHub: [https://github.com/colliery-io/graphqlite](https://github.com/colliery-io/graphqlite)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q0v8jp/graphqlite_embedded_graph_database_for_building/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx2lxa4",
          "author": "ubiquae",
          "text": "This is great news, specially after kudu being discontinue.\n\nI will try it out asap.",
          "score": 1,
          "created_utc": "2026-01-01 12:44:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q2gq6z",
      "title": "RAG, Knowledge Graphs, and LLMs in Knowledge-Heavy Industries - Open Questions from an Insurance Practitioner",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q2gq6z/rag_knowledge_graphs_and_llms_in_knowledgeheavy/",
      "author": "PlanktonPika",
      "created_utc": "2026-01-03 01:15:28",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.83,
      "text": "RAG, knowledge graphs (KG), LLMs, and \"AI\" more broadly are increasingly being applied in knowledge-heavy industries such as healthcare, law, insurance, and banking.\n\nI‚Äôve worked in the insurance domain since the mainframe era, and I‚Äôve been deep-diving into modern approaches: RAG systems, knowledge graphs, LLM fine-tuning, knowledge extraction pipelines, and LLM-assisted underwriting workflows. I‚Äôve built and tested a number of prototypes across these areas.\n\nWhat I‚Äôm still grappling with is this: **from an enterprise, production-grade perspective, how do these systems realistically earn trust and adoption from the business?**\n\nTwo concrete scenarios I keep coming back to:\n\n# Scenario 1: Knowledge Management\n\nInsurance organisations sit on enormous volumes of internal and external documents - guidelines, standards, regulatory texts, technical papers, and market materials.\n\nMuch of this ‚Äúknowledge‚Äù is:\n\n* High-level and ambiguous\n* Not formalised enough to live in a traditional rules engine\n* Hard to search reliably with keyword systems\n\nThe goal here isn‚Äôt just faster search, but **answers the business can trust,** answers that are accurate, grounded, and defensible.\n\nQuestions I‚Äôm wrestling with:\n\n* Is a pure RAG approach sufficient, or should it be combined with explicit structure such as ontologies or knowledge graphs?\n* How can fluent but subtly incorrect answers be detected and prevented from undermining trust?\n* From an enterprise perspective, what constitutes ‚Äúgood enough‚Äù performance for adoption and sustained use?\n\n# Scenario 2: Underwriting\n\nMany insurance products are non-standardised or only loosely standardised.\n\nUnderwriting in these cases is:\n\n* Highly manual\n* Knowledge- and experience-heavy\n* Inconsistent across underwriters\n* Slow and expensive\n\nThe goal is not full automation, but to **shorten the underwriting cycle** while producing outputs that are:\n\n* Reliable\n* Reasonable\n* Consistent\n* Traceable\n\nHere, the questions include:\n\n* Where should LLMs sit in the underwriting workflow?\n* How can consistency and correctness be assured across cases?\n* What level of risk control should be incorporated?\n\nI‚Äôm interested in hearing from others who are building, deploying, or evaluating RAG/KG/LLM systems in regulated or knowledge-intensive domains:\n\n* What has worked in practice?\n* Where have things broken down?\n* What do you see as the real blockers to enterprise adoption?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q2gq6z/rag_knowledge_graphs_and_llms_in_knowledgeheavy/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1pz4l71",
      "title": "Do you need a better BeautifulSoup; for RAG?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pz4l71/do_you_need_a_better_beautifulsoup_for_rag/",
      "author": "absqroot",
      "created_utc": "2025-12-30 01:31:06",
      "score": 12,
      "num_comments": 12,
      "upvote_ratio": 0.79,
      "text": "Hi all,\n\n  \nI'm currently developing 'rich-soup', an alternative to BS, and \"raw\" Playwright.\n\n  \nFor RAG, I found that there weren't many options for parsing HTML pages easily; i.e: content-extraction, getting the actual 'meaty' content from the page, cleanly.\n\nBeautifulSoup is the standard, but it's static only (doesn‚Äôt execute JS). Most sites use JS to dynamically populate content, React and jQuery being common examples. So it's not very useful. Unless you write a lot of boilerplate and use extensions.\n\n  \nYes, Playwright solves this. In fact, my tool uses Playwright under the hood. But, it doesn't give you easy-to-use blocks, the actual content. My tool, Rich Soup intends to give you the DX of Beautiful Soup, but work on dynamic pages.\n\n  \nI've got an MVP. It doesn't handle some edge cases, but it seems OK at the moment.\n\nRich Soup uses Playwright to render the page (JS, CSS, everything), then uses visual semantics to understand what you're actually looking at. It analyzes font sizes, spacing, hierarchy, and visual grouping; the same cues humans use to read, and reconstructs the page into clean blocks.\n    \n    \n Instead of this:\n    ```html\n    <div class=\"_container\"><div class=\"_text _2P8zR\">...</div><div class=\"_text _3k9mL2\">...</div>...\n    ```\n    \n    \nYou get this:\n```json\n  {\n    ¬† \"blocks\": [\n    ¬† ¬† {\"type\": \"paragraph\", \"spans\": [\"News article about \", \"New JavaScript Framework\", \"**Written in RUST!!!**\"]},\n    ¬† ¬† {\"type\": \"image\", \"src\": \"...\", \"alt\": \"Lab photo\"},\n    ¬† ¬† {\"type\": \"paragraph\", \"spans\": [\"Researchers say...\", \" *significant progress*\", \"...\"]}\n    ¬† ]\n  }\n```\n    \n    \n  Clean blocks instead of markup soup. Now you can actually use the content‚Äîfeed it to an LLM, chunk it for search, build a knowledge base, generate summaries.\n    \n  \n    \n**Rich Soup extracts**:\n- Paragraph blocks - (items: list[Span])\n- Table blocks- (rows: list[list[str]])\n- Image blocks - (src, alt)\n- List blocks - (prefix: str, items: list[Span])\n    \n  \n> Note: A 'span' isn't `<span>`. It represents a logical group of styling.\n> E.g: `ParagraphBlock.spans = [\"hi\", \"*my*\", \"**name**\", \"is\", \"**John**\", \".\"]`\n\nBefore I develop further, I just want to see if there's any demand. Personally, I think you can do it without this tool, but it takes a lot of extra logic. If you're parsing only a few sites, I reckon it's not that useful. But if you want something a bit more generically useful, maybe it's good?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pz4l71/do_you_need_a_better_beautifulsoup_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwq94iq",
          "author": "Oshden",
          "text": "This is freaking amazing OP. You‚Äôve solved an issue that I‚Äôve been beating my head against a wall on for weeks. I‚Äôd love to try out your code, as right now I‚Äôm working on a pipeline that will compile and download online manuals that contain hundreds of articles/sections, the trying to convert those HTML files to markdown but struggling with digging into the code of the web page to get only the info I need. I cobbled/brute-forced something into existence but nowhere near as cleanly as it seems that you did. If you‚Äôre willing to have someone try out your program and give you feedback, I would love to try it out as soon as you let me lol",
          "score": 3,
          "created_utc": "2025-12-30 13:44:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwqb856",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2025-12-30 13:56:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwr22dy",
                  "author": "Oshden",
                  "text": "OP, I apologize if my message came off as snarky, honestly. I am legitimately interested in your project. The lol at the end of my message was in reference to me not being sure if you‚Äôd let a stranger like me on the internet have access to your program. What I wrote about all the crap I‚Äôve been trying which you seem to have solved already is genuine. I‚Äôve been banging my head against a digital wall for weeks now trying to cobble together something that your code seems to do already. I‚Äôd still love to try out your code if you‚Äôd be willing to share it. Regardless, great work on getting as far as you have with your project. I wish you nothing but success with it. Seriously.\n\nedit: p.s. had I found the GitHub repo with your program I‚Äôd likely already be trying to figure out how to integrate it into the pipeline I‚Äôve come up with to make it work even better.",
                  "score": 1,
                  "created_utc": "2025-12-30 16:15:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwzio5z",
          "author": "Xacius",
          "text": "Interesting. Out of curiosity what edge cases are you running into? Also, how are you parsing the html?",
          "score": 2,
          "created_utc": "2025-12-31 22:04:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwzr3jq",
          "author": "Due-Project-7507",
          "text": "Such a library would be very useful. I have a knowledgebase with very old and ugly HTML code (e.g. a lot of nested tables for placing elements). Docling can e.g. not handle it directly. At the moment, I have custom BeautifulSoup code to remove unnecessary tables, but your solution would be more robust.",
          "score": 2,
          "created_utc": "2025-12-31 22:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwp9inq",
          "author": "AsparagusKlutzy1817",
          "text": "Help me understand the tool a bit better. Are you also dealing with accessing and retrieving potentially dynamic websites? This is currently the biggest pain point I would say with web crawling. More and more dynamically rendered website which you cannot get() without having a GUI-like environment running?\n\nOnce you have a website representation you pre-select typical structural elements? Is this right?",
          "score": 1,
          "created_utc": "2025-12-30 08:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwpap28",
              "author": "absqroot",
              "text": "Hi, thanks for asking\n\nYes, it uses playwright, which basically launches an entire browser (Chromium), you are right. That means it does execute JavaScript and all the odd dynamic hacks.\n\nYeah, it does select elements but it‚Äôs got a lot of additional heuristics to handle websites that aren‚Äôt semantic html (like they don‚Äôt use heading tags and paragraph tags when they should have been used). \n\nAnd it filters sidebar contents, headers, footers, language selectors, de duplicates. Basically it cleans it up so you just get nice text you can chunk and embed for RAG.",
              "score": 1,
              "created_utc": "2025-12-30 09:07:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7gsnk",
          "author": "Ok-Adhesiveness-4141",
          "text": "Yeah, this makes a lot of sense to me. \nAre you planning to make something open source or is this a product?",
          "score": 1,
          "created_utc": "2026-01-02 05:31:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx7rlwk",
              "author": "absqroot",
              "text": "Ofc it will be open source. ATM it‚Äôs not that good tho so I will work on it for a few months",
              "score": 2,
              "created_utc": "2026-01-02 06:59:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwnjrhm",
          "author": "RetiredApostle",
          "text": ">stuff like jQuery or PHP, still dynamic\n\nThe new generation of devs arrived...",
          "score": 1,
          "created_utc": "2025-12-30 01:45:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwomcdh",
              "author": "absqroot",
              "text": "sorry, made a mistake. i type fast and write random stuff sometimes. mb.",
              "score": 1,
              "created_utc": "2025-12-30 05:36:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzai7x",
      "title": "Metadata extraction from unstructured documents for RAG use cases",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzai7x/metadata_extraction_from_unstructured_documents/",
      "author": "Serious-Barber-2829",
      "created_utc": "2025-12-30 06:14:14",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "I'm an engineer at Aryn (aryn.ai) and I work in document parsing and extraction and help customers build RAG solutions.  We recently launched a new metadata extraction feature that allows you to extract metadata/properties of interest from unstructured documents using JSON schemas.  I know this community is really big on various ways of dealing with unstructured documents (PDFs, docx, etc) for the purpose of getting them ready for RAG and LLMs.  Most of the use cases I see talked about here are around pulling out text and chunking and embedding and ingesting into a vector database with a heavy emphasis on self-hosting.  We believe that metadata extraction is going to provide a differentiation for RAG because the process of imposing structure on the data using schemas opens the door for many existing data analytics tools that work on structured data (think relational databases with catalogs).  Anyone actively looking into or working on this for their RAG projects?  Are you already using something for metadata extraction.  If so, how has your experience been using it?  What's working well and what's lacking?  I'd love to hear your experience!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pzai7x/metadata_extraction_from_unstructured_documents/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwp3a2f",
          "author": "AsparagusKlutzy1817",
          "text": "What metadata do you have in mind? The document structure itself like headings, subheadings?\n\nI have been building a text extraction library over christmas: https://github.com/Horsmann/sharepoint-to-text. This one also picks up metadata it finds in the source document. This is currently limited to author, creation date etc. I don't call them metadata but for .docx for instance I also separate the tables to work on them afterwards if any table-processing is desired (caller needs to implement this - i just pull the tables)",
          "score": 1,
          "created_utc": "2025-12-30 07:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrbndg",
              "author": "Serious-Barber-2829",
              "text": "Yes, things like title, authors would be metadata.  But it can be any pieces of information you are interested in pulling out of a document.  Think invoices (invoice number, address, total amount), contracts, tax forms, etc.",
              "score": 1,
              "created_utc": "2025-12-30 16:59:52",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwpawga",
          "author": "Extreme-Brick6151",
          "text": "Metadata is the unsexy part of RAG that actually moves the needle. Once teams enforce schema-level metadata, retrieval quality, filtering, and access control improve way more than just tuning chunk sizes. Curious how you‚Äôre handling schema drift and messy edge cases across mixed doc types.",
          "score": 1,
          "created_utc": "2025-12-30 09:09:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwrb9vq",
              "author": "Serious-Barber-2829",
              "text": "\\> Metadata is the unsexy part of RAG that actually moves the needle. Once teams enforce schema-level metadata, retrieval quality, filtering, and access control improve way more than just tuning chunk sizes.\n\nI couldn't agree more!\n\nWe are not yet tackling use cases where schema drift would be an issue.  We are dealing with documents like contracts, invoices, forms, etc.  But there are some \"standard\" practices in streaming/PubSub where you use schema registries and schema validation to deal with schema evolution.",
              "score": 1,
              "created_utc": "2025-12-30 16:58:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nws9o4a",
          "author": "valuechase",
          "text": "In my experience working with complex PDFs with unstructured data, the limitations of RAG are less on retrieval and much more at the parsing step. I‚Äôm working with Financial documents and even the best vision based parsers make mistakes when parsing tables from a pdf. You can mitigate this to an extent by using traditional RAG for narrative and maybe for table related queries, routing those using an index (and metadata extraction of full document) to an LLM, providing the LLM with the full document. This is maybe expensive path but probably more reliable.",
          "score": 1,
          "created_utc": "2025-12-30 19:38:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtpwby",
              "author": "Serious-Barber-2829",
              "text": "Do you have something working reliably enough in production?",
              "score": 1,
              "created_utc": "2025-12-30 23:57:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwuh5s0",
          "author": "absqroot",
          "text": "Sorry, I don't quite get it. What do you mean by metadata? Basic metadata about the page, metadata like bounding boxes, font sizes & weights, or metadata like numerical data and tables?",
          "score": 1,
          "created_utc": "2025-12-31 02:32:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwun119",
              "author": "Serious-Barber-2829",
              "text": "Metadata or \"property\" as in any piece of interest.  It can be any of the things you mentioned, but it can be specific values found on a page (invoice number, address, e.g.)",
              "score": 1,
              "created_utc": "2025-12-31 03:06:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwz7wql",
          "author": "drfritz2",
          "text": "Metadata is needed if working with a lot of data. It would be SQL and linked with the vector or graph.\n\nSo if you want to have filters to search, it would be possible.\n\nOf course the metadata template should be very flexible. The model itself should extract and create/adapt the fields. \n\nThe issue os that the hole system should be \"hot\". It's metamorphic, because of the fast pace of the tech development",
          "score": 1,
          "created_utc": "2025-12-31 21:05:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1pym1sz",
      "title": "How would you build a RAG system over a large codebase",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pym1sz/how_would_you_build_a_rag_system_over_a_large/",
      "author": "Creepy_Page566",
      "created_utc": "2025-12-29 13:17:14",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "I want to build a tool that helps automate IT support in companies by using a multi-agent system. The tool takes a ticket number related to an incident in a project, then multiple agents with different roles (backend developer, frontend developer, team lead, etc.) analyze the issue together and provide insights such as what needs to be done, how long it might take, and which technologies or tools are required.\n\nTo make this work, the system needs a RAG pipeline that can analyze the ticket and retrieve relevant information directly from the project‚Äôs codebase. While I have experience building RAG systems for PDF documents, I‚Äôm unsure how to adapt this approach to source code, especially in terms of code-specific chunking, embeddings, and intelligent file selection similar to how tools like GitHub Copilot determine which files are relevant.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pym1sz/how_would_you_build_a_rag_system_over_a_large/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwk2cn5",
          "author": "ampancha",
          "text": "The biggest mistake in Code RAG is treating source files like PDFs. Code is a graph, not a narrative. If you just chunk by token count, you slice functions in half and lose the import context.\n\nTo get \"Copilot-like\" selection, you need:\n\n1. **AST Chunking:** Use tree-sitter to split code by logical scope (Function/Class) rather than lines.\n2. **Dependency Graphing:** Your retrieval needs to understand that if `auth.ts` is retrieved, the agent probably also needs `user_model.ts` because it's imported.\n3. **Repo Map:** Build a compressed \"skeleton\" of the codebase (signatures only) to fit in the context window first, letting the agent decide which full files to read.\n\nI sent you a DM with some patterns on how to architect this \"Graph + Vector\" retrieval.",
          "score": 4,
          "created_utc": "2025-12-29 15:21:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk7666",
              "author": "Creepy_Page566",
              "text": "Thanks I appreciate it, I will definetly look into this",
              "score": 1,
              "created_utc": "2025-12-29 15:45:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nwq7dtr",
              "author": "munkymead",
              "text": "Hey if you wouldn't mind forwarding this to me I'd really appreciate it.",
              "score": 1,
              "created_utc": "2025-12-30 13:34:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwkhwup",
          "author": "foobarrister",
          "text": "I did the GraphRag route but it was a pain to keep up to date.¬†\n\n\nPlan now is to pivot towards Roslyn MCP to gain insight into c# code and use Claude code MCP for the rest",
          "score": 2,
          "created_utc": "2025-12-29 16:36:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwq04mt",
          "author": "Hungry-Style-2158",
          "text": "Rag itself can be its own micro service. It‚Äôs easy to set up in small codebases but when it‚Äôs time to scale, it can cause a huge problem in your existing code base. This is why I always recommend using a micro service architecture to handle RAG. \n\nI spent all of this past year learning that the hard way. \n\nAnother thing I would recommend is to use existing RAG services than building yours from scratch. That‚Äôs because you will end up managing the entire RAG infrastructure yourself, which is totally different from your business requirements. Another reason I recommend the RAG Saas than building yourself is cause most of the libraries that are used to build RAG software are always getting updated, it can be a pain trying to keep up in this AI space. All these take time from you while you are trying to keep up with business requirements.\n\nSome of the best RAG tools I can recommend are \n - FireCrawl for data extraction\n - supermemory ai for agent memory \n- reducto ai \n- Wetrocloud for data extraction and end to end rag\n\n\n\nI do not intend to help make a choice on which tool to use, but you can take a look at each. I have worked with Firecrawl, but eventually switched to Wetrocloud cause of the full RAG end to end support.",
          "score": 2,
          "created_utc": "2025-12-30 12:47:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwqkdzi",
          "author": "RolandRu",
          "text": "I‚Äôm actually building a RAG setup specifically for code, and I‚Äôm trying to specialize it around .NET + MS SQL patterns (typical enterprise layering, stored procs, migrations, EF, etc.), so code-aware retrieval matters a lot.\n\nOne approach that‚Äôs worked well for me is a two-stage pipeline:\n\n1. Cheap file-level candidate selection (high recall): BM25/keyword + repo heuristics + symbol index (imports/usings, namespaces, table/proc names, etc.).\n2. Precision retrieval inside the shortlisted files: chunk by semantic units (class/method/function + docstring/comments + nearby types/constants), store metadata (path, namespace, symbol name, language, dependencies), then rerank hard before final context assembly.\n\nThen do an iterative ‚ÄúCopilot-style‚Äù loop: start narrow, answer with file:line citations, and if confidence is low expand to neighbors (tests, configs, migrations, related SQL objects) rather than pulling the whole repo.\n\nSeparate note: I‚Äôm looking for a decent-sized public codebase to test on (I‚Äôm doing this privately and don‚Äôt want to use my work repo for obvious reasons). If you know a good .NET-heavy repo (or a couple) that‚Äôs realistic in structure, I‚Äôm all ears. Also, if you‚Äôre interested, feel free to DM ‚Äî I‚Äôm not trying to self-promote on the thread.",
          "score": 2,
          "created_utc": "2025-12-30 14:48:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwjwhzy",
          "author": "Crafty_Disk_7026",
          "text": "You want to convert the codebase to ast then try to load as much of it as possible into context",
          "score": 1,
          "created_utc": "2025-12-29 14:51:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwk5iyb",
              "author": "Creepy_Page566",
              "text": "This won't work either on code, What I am trying to achieve is something similar to how tools like github copilot, from the prompt it identifies what files to consider and read them all without chunking",
              "score": 1,
              "created_utc": "2025-12-29 15:37:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwkfi5a",
                  "author": "Crafty_Disk_7026",
                  "text": "Well this is how they do it... they parse the code into an ast tree of relationships then crawl that to get specific code fragments.  One example library https://github.com/cedricrupb/code_ast",
                  "score": 2,
                  "created_utc": "2025-12-29 16:24:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwrvyy2",
          "author": "Total_Prize4858",
          "text": "You don‚Äôt need rag, you need a good mcp server for your source code management (btw. Github mcp is not good)",
          "score": 1,
          "created_utc": "2025-12-30 18:34:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1q92k",
      "title": "Scraping text from websites + PDFs for profile matching: seeking best tools & pipeline design",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q1q92k/scraping_text_from_websites_pdfs_for_profile/",
      "author": "Ash_It_98",
      "created_utc": "2026-01-02 05:29:09",
      "score": 10,
      "num_comments": 24,
      "upvote_ratio": 1.0,
      "text": "Hi guys, I‚Äôm brainstorming a project that needs to pull textual data from a set of websites ‚Äî some pages contain plain HTML text, others have PDFs (some with extractable text, others scanned/image-based).\nThe goal is to use the extracted text with user preferences to determine relevance/match. I‚Äôm trying to keep the idea general, but I‚Äôm stuck on two key parts:\n\n1. Extraction speed & accuracy ‚Äî What‚Äôs the most reliable way to scrape and extract text at scale, especially for mixed content (HTML + various PDF types, including scanned ones)?\n2. Profile matching pipeline ‚Äî Once I have clean text, what‚Äôs an efficient way to compare it against user profiles/preferences? Any RAG-friendly methods or embeddings/models that work well for matching without heavy fine-tuning?\n\nIdeally, I‚Äôd like a setup that‚Äôs fast for near-real-time matching but doesn‚Äôt sacrifice accuracy on harder-to-parse PDFs. Would appreciate any tips on tools (e.g., for OCR on scanned PDFs), text preprocessing steps, or architectural pointers you‚Äôve used in similar projects.\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q1q92k/scraping_text_from_websites_pdfs_for_profile/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nx84521",
          "author": "cryptoviksant",
          "text": "1. To scrape you can use crawl4ai, firecrawl via their API or Jina reader API  \n2. You don't need to finetune anything. Go with openAI embeddings or any other model, such as jina, qwen or mxbai-embed-large (the list is infinite)",
          "score": 3,
          "created_utc": "2026-01-02 08:56:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx867d1",
              "author": "Ash_It_98",
              "text": "Yeah I will definitely check. Thanks.",
              "score": 1,
              "created_utc": "2026-01-02 09:16:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nx8h15m",
                  "author": "cryptoviksant",
                  "text": "Anytime.",
                  "score": 2,
                  "created_utc": "2026-01-02 10:58:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nx88108",
              "author": "Joy_Boy_12",
              "text": "But how should he chunk the data from the website Before sending it to the model?\n\ngetting data from websites it‚Äôs easy, create from them good chunks is hard",
              "score": 1,
              "created_utc": "2026-01-02 09:34:01",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nx8gz6r",
                  "author": "cryptoviksant",
                  "text": "It's really not. At the end of the day, chunking means splitting the data into smaller pieces. Hence you can temporarily save the scraped content as a file (or even chunk it in memory) and then send it to the chunking function.",
                  "score": 1,
                  "created_utc": "2026-01-02 10:57:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nx7nhzq",
          "author": "AsparagusKlutzy1817",
          "text": "Scanned PDF means you need OCR as the page is just an image and not actual text. \n\nYou will not get a clean PDF parse across different formats or layouts. Some are notoriously hard to parse as the tool needs to guess what is a paragraph or unit of high cohesion. This works sometimes better sometimes worse. Old pdf = worse as a rule of thumb.\n\nJust getting text out of them usually works but don‚Äôt expect deeper parses beyond this level of granularity.\n\nWeb content is likewise not so easy. As long it‚Äôs plain html you are able to extract many structures if you are willing to write html parsing code.\n\nWithout an LLM to ask if a retrieval candidate is a match this will likely not work reliably. Embedding will score high for similar content but not reliably enough to say a profile matches exactly to a candidate- unless they are 1:1 identical",
          "score": 2,
          "created_utc": "2026-01-02 06:24:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx863uo",
              "author": "Ash_It_98",
              "text": "Okay thanks I will check for that.",
              "score": 1,
              "created_utc": "2026-01-02 09:15:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx7qryo",
          "author": "Both-Number-7319",
          "text": "I can contribute with you to implement your rag when you clear your data",
          "score": 2,
          "created_utc": "2026-01-02 06:52:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx864yf",
              "author": "Ash_It_98",
              "text": "Sure thank you.",
              "score": 1,
              "created_utc": "2026-01-02 09:15:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx82zo4",
          "author": "Upset-Pop1136",
          "text": "Dify is the best, and we have worked with it in multiple projects. That works well across all the PDFs, across websites. You can try it.",
          "score": 2,
          "created_utc": "2026-01-02 08:45:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nx8663v",
              "author": "Ash_It_98",
              "text": "Sure thank you.",
              "score": 1,
              "created_utc": "2026-01-02 09:16:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nx8lr3k",
          "author": "teroknor92",
          "text": "you can try ParseExtract APIs for both HTML and scanned PDFs, if this works this will be most effective option. Other options for HTML is Firecrawl and for PDFs is Llamaparse.",
          "score": 2,
          "created_utc": "2026-01-02 11:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdwia6",
              "author": "Ash_It_98",
              "text": "Thanks.",
              "score": 1,
              "created_utc": "2026-01-03 04:55:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nxd1tcd",
          "author": "OnyxProyectoUno",
          "text": "The mixed content problem usually comes down to having different parsers for different document types, then normalizing their output into a consistent format. For PDFs, you'll want something like Unstructured or Docling for the extractable text ones, and they both handle OCR for scanned documents reasonably well. The tricky part isn't the individual parsers but orchestrating them and seeing what actually comes out.\n\nYour bigger issue is going to be chunking strategy after extraction. Website content has different structure than PDF content, and scanned PDFs often have weird artifacts that mess up semantic chunking. You need to see what your text looks like after each processing step before you start building embeddings on top of it. I work on document processing pipelines at vectorflow.dev and this visibility problem is exactly why most RAG projects fail quietly.\n\nFor the matching part, standard embedding models like text-embedding-3-small work fine for profile matching without fine-tuning. The real bottleneck is usually upstream in your extraction pipeline where context gets lost or mangled. What does your target content actually look like? Are these technical documents, marketing pages, research papers?",
          "score": 2,
          "created_utc": "2026-01-03 01:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxdy4t5",
              "author": "Ash_It_98",
              "text": "Well honestly regarding the profile matching, chunking, and embedding, I am learning that part so I am not sure what can be the best practice. But for the scraping and parsing, the thing is that each website has tables and each table has columns like simple details that are important but the main focus is the PDF in the last columns. The scraper will have to check for the filtered niche then will check the tables and then download the PDFs. Then it will perform parsing or OCR. And accuracy is really important here. And after extraction then I will perform the profile matching. Basically every user will have their preference and requirements uploaded while they are creating a profile during sign up so it's basis I will match with the PDFs content to check if the user is eligible for it or not and I will show credit scores to users according to the user's eligibility. That's the basic concept but the thing is I haven't started working on it, because I am learning a few things like currently I am learning NLP. And yes when I start working on it I will ask more if I face any blocker.",
              "score": 1,
              "created_utc": "2026-01-03 05:06:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1q1vqc1",
      "title": "RAG with visual docs: I compared multimodal vs text embeddings",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q1vqc1/rag_with_visual_docs_i_compared_multimodal_vs/",
      "author": "midamurat",
      "created_utc": "2026-01-02 10:56:06",
      "score": 9,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "When you run RAG on visual docs (tables, charts, diagrams), the big decision is: do you embed the images directly, or do you first convert them to text and embed that?\n\nI tested both in a controlled setup.\n\nSetup (quick):  \nText pipeline = image/table ‚Üí text description ‚Üí text embeddings  \nMultimodal pipeline = keep it as an image ‚Üí multimodal embedding  \nTested on query sets(150) from DocVQA (text + tables), ChartQA (charts), and AI2D (diagrams). Metrics were Recall@1 / Recall@5 / MRR.\n\nHere are some findings:\n\n* On visual docs, multimodal embeddings work better.\n   * Tables: big gap (88% vs 76% Recall@1)\n   * Charts: small but consistent edge (92% vs 90%)\n* On pure text, text embeddings are slightly better (96% vs 92%).\n* Recall@5 is high for both - the real difference is whether the *right* page shows up at rank #1.\n\nSo, multimodal embeddings seem to be the better default if your corpus has real visual structure (especially tables).\n\n(if interested, feel free to check out detailed setup and results here: [https://agentset.ai/blog/multimodal-vs-text-embeddings](https://agentset.ai/blog/multimodal-vs-text-embeddings) )",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1q1vqc1/rag_with_visual_docs_i_compared_multimodal_vs/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxdxmfs",
          "author": "silvrrwulf",
          "text": "Thanks for this",
          "score": 1,
          "created_utc": "2026-01-03 05:03:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxf3t6w",
              "author": "midamurat",
              "text": "glad if it is helpful",
              "score": 1,
              "created_utc": "2026-01-03 10:53:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pyu7si",
      "title": "Semantic Coherence in RAG: Why I Stopped Optimizing Tokens",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pyu7si/semantic_coherence_in_rag_why_i_stopped/",
      "author": "getarbiter",
      "created_utc": "2025-12-29 18:37:29",
      "score": 9,
      "num_comments": 21,
      "upvote_ratio": 0.8,
      "text": "I‚Äôve been following a lot of RAG optimization threads lately (compression, chunking, caching, reranking). After fighting token costs for a while, I ended up questioning the assumption underneath most of these pipelines.\n\nThe underlying issue:\nMost RAG systems use cosine similarity as a proxy for meaning. Similarity ‚â† semantic coherence.\n\nThat mismatch shows up downstream as:\n‚ÄîOver-retrieval of context that‚Äôs ‚Äúrelated‚Äù but not actually relevant\n‚ÄîAggressive compression that destroys logical structure\n‚ÄîComplex chunking heuristics to compensate for bad boundaries\n‚ÄîLarge token bills spent fixing retrieval mistakes later in the pipeline\n\nWhat I‚Äôve been experimenting with instead:\nConstraint-based semantic filtering ‚Äî measuring whether retrieved content actually coheres with the query‚Äôs intent, rather than how close vectors are in embedding space.\n\nPractically, this changes a few things:\n‚ÄîNo arbitrary similarity thresholds (0.6, 0.7, etc.)\n‚ÄîChunk boundaries align with semantic shifts, not token limits\n‚ÄîCompression becomes selection, not rewriting\n‚ÄîRetrieval rejects semantically conflicting content explicitly\n\nEarly results (across a few RAG setups):\n‚Äî~60‚Äì80% token reduction without compression artifacts\n‚ÄîMuch cleaner retrieved context (fewer false positives)\n‚ÄîFewer pipeline stages overall\n‚ÄîMore stable answers under ambiguity\n\nThe biggest shift wasn‚Äôt cost savings ‚Äî it was deleting entire optimization steps.\n\nQuestions for the community:\nHas anyone measured semantic coherence directly rather than relying on vector similarity?\n\nHave you experimented with constraint satisfaction at retrieval time?\n\nWould be interested in comparing approaches if others are exploring this direction.\n\nHappy to go deeper if there‚Äôs interest ‚Äî especially with concrete examples.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pyu7si/semantic_coherence_in_rag_why_i_stopped/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwlgeca",
          "author": "Horror-Turnover6198",
          "text": "Isn‚Äôt this what a reranker does?",
          "score": 2,
          "created_utc": "2025-12-29 19:16:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmindy",
              "author": "getarbiter",
              "text": "Different mechanism. Rerankers still rely on similarity scoring between query and candidates. This approach measures semantic constraint satisfaction directly - whether the candidate actually fulfills the logical requirements of the query rather than just being textually similar.\n\nYou can have high similarity with zero coherence (like finding documents about 'bank' the financial institution when you meant 'river bank'). Constraint satisfaction catches those cases that similarity-based reranking misses.\"",
              "score": -1,
              "created_utc": "2025-12-29 22:23:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nwmjf4g",
                  "author": "Horror-Turnover6198",
                  "text": "I am totally ready to be called out as being wrong here, but I thought rerankers (or cross-transformers at least) were specifically looking at relevance, and you use them post-retrieval because they‚Äôre more intensive.",
                  "score": 2,
                  "created_utc": "2025-12-29 22:27:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwld9yx",
          "author": "private_donkey",
          "text": "Interesting! How, specifically, are you doing the compression and retrieval now?",
          "score": 1,
          "created_utc": "2025-12-29 19:02:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmh4ji",
              "author": "mysterymanOO7",
              "text": "Exactly, what I was something! Lots of words and nothing in terms of what he actually did!",
              "score": 2,
              "created_utc": "2025-12-29 22:15:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nwmiaae",
              "author": "getarbiter",
              "text": "Currently using constraint-based geometric analysis to measure semantic coherence directly. The approach works by mapping content into a 72-dimensional semantic space and measuring coherence gaps between query intent and retrieved content.\n\nFor compression: Instead of arbitrary similarity thresholds (0.7, etc.), I identify which parts of documents maintain the highest coherence with the query context, then compress based on those semantic boundaries. Getting 60-80% size reduction while maintaining retrieval quality.\n\nFor retrieval: Skip the cosine similarity step entirely. Measure whether candidate documents actually satisfy the semantic constraints of the query rather than just having similar embeddings.\n\nThe key insight is that similarity ‚â† coherence. Two documents can be highly similar in embedding space but completely incoherent when trying to answer a specific query.\n\nThe geometric approach lets you compress based on actual meaning preservation rather than token counting or prompt-based summarization. You're essentially asking 'what are the minimum semantic components needed to maintain coherence with this specific use case' rather than 'what are the most similar vectors.'\nHappy to share some comparative results if you're interested in testing approaches. What's your current retrieval+compression pipeline looking like?",
              "score": 1,
              "created_utc": "2025-12-29 22:21:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwmh89a",
          "author": "OnyxProyectoUno",
          "text": "Your constraint-based approach cuts through a lot of noise. The similarity threshold guessing game is exhausting.\n\nWhat strikes me is how much of this traces back to whether your chunks actually contain coherent semantic units to begin with. If your document processing is splitting mid-thought or losing logical structure during parsing, even perfect constraint satisfaction won't fix the underlying fragmentation.\n\nThe \"chunk boundaries align with semantic shifts\" piece is where most pipelines break down. People end up with arbitrary token limits because they can't see what their parsing and chunking actually produces. You're measuring coherence at retrieval time, but the coherence was already destroyed upstream during document processing.\n\nI've been working on this problem from the preprocessing angle with [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_g) because you can't optimize what you can't see. Most teams discover their chunking preserves zero semantic structure only after they've embedded everything and are debugging weird retrievals.\n\nHow are you handling the boundary detection in practice? Are you working with structured documents where semantic shifts are more obvious, or have you found ways to identify them reliably in unstructured content?\n\nThe constraint satisfaction angle is compelling but I suspect it's fighting symptoms if the chunks themselves are semantically broken from the start.",
          "score": 1,
          "created_utc": "2025-12-29 22:16:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmit38",
              "author": "getarbiter",
              "text": "Exactly - you've identified the core issue. Most people are trying to fix retrieval problems with better embeddings, when the real issue is that document chunking destroys semantic boundaries before you even get to retrieval.\n\nThe constraint satisfaction approach works because it can identify coherent semantic units regardless of how the original parsing split things up. It's measuring logical consistency rather than token proximity.",
              "score": 1,
              "created_utc": "2025-12-29 22:24:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nwms1l0",
          "author": "notAllBits",
          "text": "sounds like you found a sweet spot on the range from dense- to mixed opinionated indexes for your use case. I have written similar text-interpretation-indexing > filtering-ranking-generation pipelines for narrow use cases. I briefly went on a tangent with fully idiosyncratic indexes that relied on spectral indexes with static compression of local sub-semantics per topic, but the hard-coded reduction and hydration (on retrieval) of core business logic is only applicable for the tidiest of data pipelines.\n\nThe challenge lies in aligning your \"indexing perspective\" with your use case over projected future requirements. Use cases may drift over time, which limits the lifetime of your indexing strategy.",
          "score": 1,
          "created_utc": "2025-12-29 23:13:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwmt9dc",
              "author": "getarbiter",
              "text": "Thanks - though this isn't really about finding a sweet spot for specific use cases. The constraint satisfaction approach works better across the board because it's measuring actual semantic relationships rather than learned patterns. It's more of a foundational shift in how semantic analysis works.",
              "score": 0,
              "created_utc": "2025-12-29 23:19:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1pzsmxd",
      "title": "LLMs + SQL Databases",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1pzsmxd/llms_sql_databases/",
      "author": "oddhvdfscuyg",
      "created_utc": "2025-12-30 20:20:23",
      "score": 8,
      "num_comments": 10,
      "upvote_ratio": 0.84,
      "text": "How do you use LLMs with databases?\n\nI wonder what is the best approach to make LLMs generate a correct query with correct field names and conditins?\n\nDo you just pass the full db schema in each prompt? this works for me but very inefficient\n\nAny better ideas?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1pzsmxd/llms_sql_databases/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nwsvwgd",
          "author": "Low-Efficiency-9756",
          "text": "Don‚Äôt make the LLM write raw SQL. Wrap it in typed \ntools\n\nfile_to_cabinet({ project_id, file_path, summary }). \n\nThe tool interface is your schema. Wrong input fails with a useful error.",
          "score": 5,
          "created_utc": "2025-12-30 21:24:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtdd24",
              "author": "EveYogaTech",
              "text": "Yes, this is usually much safer too. Raw SQL is definitely a risk even if using other measures like read-only accounts, because it could still exhaust too many resources (no limit/heavy joins/latteral) + potential zero days attacks.",
              "score": 1,
              "created_utc": "2025-12-30 22:49:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nwslmdo",
          "author": "CapitalShake3085",
          "text": "Without the schema, the model cannot generate a precise query since it does not know the table structure. So there are not any workaround",
          "score": 2,
          "created_utc": "2025-12-30 20:35:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsorni",
          "author": "tifa_cloud0",
          "text": "i have tried generating an sql query with google gemini flash model. it was surprisingly very good but also do note that my table schema was simple like name, address and id. it generated accurately all the queries like join queries, select queries etc. \nin prompt i just passed the two sample rows of table as an example and then llm model was accurate in generating the queries.\n\ni think the same gemini flash model is an ocr model. do verify it first because when i tried the example i was playing around with an ocr.",
          "score": 2,
          "created_utc": "2025-12-30 20:50:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwsljqw",
          "author": "SimplyRemainUnseen",
          "text": "There are plenty of MCP servers for sql databases. Have you tried any?",
          "score": 3,
          "created_utc": "2025-12-30 20:35:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nwstond",
          "author": "hrishikamath",
          "text": "I had built a stock screener that is a text to SQL llm workflow, check it out: https://github.com/kamathhrishi/stratalens-ai/blob/main/agent/screener/main_duckdb.py. Although my first question would be, do you really need sql? In my case I later realized just querying pandas data frames would have sufficed given llms are better at Python than sql. I split the it into two separate steps of choosing: tables and then having the llm choose the columns. Then based on the outputs from previous steps it generated the query.",
          "score": 1,
          "created_utc": "2025-12-30 21:14:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nwtdrwu",
              "author": "charming-hummingbird",
              "text": "I tried it both using text to sql and the pandas dataframe as an executable script. I found the pd worked better.",
              "score": 1,
              "created_utc": "2025-12-30 22:51:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nwtedbu",
                  "author": "hrishikamath",
                  "text": "Yep commercial llms are way better at python than SQL, so worked better for me too. Lots of incorrect syntax errors were in my case. (These were the older models btw)",
                  "score": 1,
                  "created_utc": "2025-12-30 22:54:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nwtdo2n",
          "author": "Esseratecades",
          "text": "Like others mentioned, there are MCP servers capable of giving LLMs the context necessary to understand SQL.\n\n\nHowever with LLMs being LLMs, your mileage may vary on how safely they actually perform with that context. If you are in a low trust scenario, then tool use should be fine for read operations. Define functions that pull the specific info from the db and let the LLM decide when to call a given function.\n\n\nFor write operations, have the LLM use structured output to render a pydantic model of what it would like to upsert or delete and then you take the params of that model and execute the old fashioned way.\n\n\nFor VERY low trust situations, have a human review the content of the structured output first.",
          "score": 1,
          "created_utc": "2025-12-30 22:51:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nxjwimy",
          "author": "OkAlternative2260",
          "text": "I've built something ( demo only ) very simple and efficient using Vectordb and Graph store.\n\nVector Store holds some good NL/SQL pairs for the target database. ( This allows for semantics search on the user prompt) .\nYou can go next level by even fine tuning your own Vector DB to make it more accurate for business jargon related to your domain. But generally not needed imo.\n\nGraph store holds only the schema information ( tables names, columns , relationship, constraints etc)\n\nFlow :\n\nUser prompt -> vector store ( return top n pairs)-> graph store -> LLM -> SQL\n\nGraph store will only return metadata for tables involved in NL/SQL pairs returned from vector DB.\n\nHere's a demo - https://app.schemawhisper.com/\n\n\nIt's actually quite simple behind the scene.",
          "score": 1,
          "created_utc": "2026-01-04 02:13:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q1rfvn",
      "title": "Source code GraphRAG builder for C/C++ development",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q1rfvn/source_code_graphrag_builder_for_cc_development/",
      "author": "Barronli",
      "created_utc": "2026-01-02 06:33:25",
      "score": 7,
      "num_comments": 5,
      "upvote_ratio": 0.9,
      "text": "Probably there are already some similar projects. Hopefully this one brings something new.\n\n[https://github.com/2015xli/clangd-graph-rag](https://github.com/2015xli/clangd-graph-rag)\n\n**1. Overview**\n\nThis project enables deep code analysis with Large Language Models. By constructing a Neo4j-based Graph RAG, it enables developers and AI agents to perform complex, multi-layered queries on C/C++ codebases that traditional search tools simply can't handle. With only a few MCP APIs and a vanilla agent, it is already able to accomplish complex tasks efficiently related to the codebases.\n\n**2. How it works**\n\nUsing clangd and clang, the system parses and indices your source files to create a high-fidelity code graph. It captures everything from high-level folder structures to granular relationships, including entities like Folders, Files, Namespaces, Classes/Structs, Variables, Methods, etc.; relationships like: CALLS, INCLUDES, INHERITS, OVERRIDES, and more.\n\nThe system generates summaries and embeddings for every level of the codebase (from functions up to entire folders) using a bottom-up approach. This structured context helps AI agents understand the \"big picture\" without getting lost in the syntax.\n\nTo get you started easily, the project includes an example MCP (Model Context Protocol) server, and a demonstration AI agent to showcase the graph‚Äôs power. You can easily build your own custom agents and servers on top of the graph RAG.\n\n**3. Efficiency & Performance**\n\nIncremental Updates: The system detects changes between commits and updates only what‚Äôs necessary. \n\nParallel Processing: Parsing and summary generation are distributed across worker processes with optimized data sharing. \n\nSmart Caching: Results are cached to minimize redundant computations, saving you both time and LLM costs.\n\n**4. A benchmark: The Linux Kernel**\n\nWhen building a code graph for the Linux kernel (WSL2 release) on a workstation (12 cores, 64GB RAM), it takes about \\~4 hours using 10 parallel worker processes, with peak memory usage at \\~36GB. Note this process does not include the summary generation, and the total time (and cost) may vary based on your LLM provider. \n\n**5. Note**, this is an independent project and is not affiliated with the official Clang or clangd projects.\n\nThis project is by no means a replacement for the clangd language server (LSP) used in IDEs. Instead, it is designed to complement it by enabling LLMs to perform deep architectural analysis, like mapping project workflows, tracing complex call paths, and understanding system-wide architecture.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q1rfvn/source_code_graphrag_builder_for_cc_development/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nxbjaey",
          "author": "remotigent",
          "text": "Can it support code refactoring?",
          "score": 1,
          "created_utc": "2026-01-02 20:57:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "nxczq7i",
              "author": "Barronli",
              "text": "Yes but not directly. \n\nRefactoring is an **agent capability** built on top of the graphRAG ‚Äî the source code graphRAG provides the agent with accurate architectural understanding and impact analysis, while an agent can plan refactoring and generate code/patches using that context.   \n  \nThe graphRAG itself does not include original source code in it, but with path/location info pointing to the original source. The agent can manipulate the source code as needed. Once the code has significant changes, it is better to update the graphRAG to reflect the latest status of the codebase, i.e., keeping the graphRAG and the codebase in sync.\n\nI have been using the graphRAG to support my own project refactoring.",
              "score": 1,
              "created_utc": "2026-01-03 01:34:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nxdeort",
                  "author": "remotigent",
                  "text": "Thanks for your response. That helps. Will check it out.\n\n\nBtw, does a code refactoring requires the graphRAG to be rebuilt for content consistency?",
                  "score": 1,
                  "created_utc": "2026-01-03 03:01:54",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nxkr9t5",
                  "author": "remotigent",
                  "text": "Thanks.¬†I just had a look into your project. It seems to do exactly what I need. When does it access llm API? I want to understand if it is absolutely necessary for my projects. And how's your experience with local models via ollama?¬†",
                  "score": 1,
                  "created_utc": "2026-01-04 05:19:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}