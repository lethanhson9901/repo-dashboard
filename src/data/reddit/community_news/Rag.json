{
  "metadata": {
    "last_updated": "2026-01-29 02:49:02",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 136,
    "file_size_bytes": 174235
  },
  "items": [
    {
      "id": "1qocxu9",
      "title": "Ran 30 RAG chunking experiments - found that chunk SIZE matters more than chunking STRATEGY",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "author": "ManufacturerIll6406",
      "created_utc": "2026-01-27 12:49:14",
      "score": 63,
      "num_comments": 19,
      "upvote_ratio": 0.98,
      "text": "I kept seeing recommendations that sentence chunking is best for RAG because it \"respects grammatical boundaries.\"\n\nDecided to test it systematically: 4 strategies, 2 datasets, 1,200 retrieval evaluations.\n\nWriteup with methodology and open source code:¬†[Link](https://theprincipledengineer.substack.com/p/its-a-chunking-lie?r=2ivi0a)\n\n**Sentence chunking did dominate initially ‚Äî 96.7% recall vs 80-83% for others.**\n\nThen I noticed something most benchmarks don't report: actual chunk sizes produced.\n\nWhen I configured all strategies with chunk\\_size=1024:\n\n\\- Token: 934 chars (0.91x)\n\n\\- Recursive: 667 chars (0.65x)\n\n\\- Semantic: 1117 chars (1.09x)\n\n\\- Sentence: 3677 chars (3.59x) ‚Üê\n\nSentence chunking was producing chunks 3.6x larger than requested. Larger chunks = more context = better recall. That's a size effect, not a strategy effect.\n\n**When I controlled for actual chunk size (\\~3000 chars across strategies), token chunking matched or beat sentence chunking.**\n\n**Correlation between chunk size and recall: r=0.74 (HotpotQA), r=0.92 (Natural Questions).**\n\nCurious if others have seen similar results or if this breaks down on different datasets.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qocxu9/ran_30_rag_chunking_experiments_found_that_chunk/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o20c64u",
          "author": "fixitchris",
          "text": "Depends what you are chunking.",
          "score": 10,
          "created_utc": "2026-01-27 13:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20jhim",
              "author": "ManufacturerIll6406",
              "text": "Exactly and this fw lets you test that on your own strategy and your own documents \n\n[https://github.com/somasays/rag-experiments/tree/main/chunking](https://github.com/somasays/rag-experiments/tree/main/chunking)",
              "score": 1,
              "created_utc": "2026-01-27 13:47:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o23k80i",
          "author": "durable-racoon",
          "text": "smaller chunks are usually better recall. larger chunks often better for generation. but nothing beats measuring for your specific use case, which you've done.",
          "score": 3,
          "created_utc": "2026-01-27 21:54:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20wha5",
          "author": "irodov4030",
          "text": "\"Larger chunks = more context = better recall.\"\n\nHow did you measure recall here?",
          "score": 3,
          "created_utc": "2026-01-27 14:52:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o272ar6",
              "author": "mrFunkyFireWizard",
              "text": "This is also false, your vector points are not accurate if they contain too much context. If it's within the same semantic meaning it's fine but just adding more context is poor design",
              "score": 1,
              "created_utc": "2026-01-28 11:22:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o21cwv3",
          "author": "TechnicalGeologist99",
          "text": "There's many steps in retrieval. I find that the main disadvantage of having short chunks is that occasioanly they knock it out the park in terms of relevance and they take up a space in the top K even though they are a useless chunk. \n\nIt may be that some strategies make many such useless chunks. (That are also small). That raises the probability of filling up top K with crap. \n\nYou could repeat this with varying top K to try and measure if that is occurring here",
          "score": 2,
          "created_utc": "2026-01-27 16:07:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21ewvq",
              "author": "[deleted]",
              "text": "[removed]",
              "score": 1,
              "created_utc": "2026-01-27 16:16:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21f5km",
                  "author": "ManufacturerIll6406",
                  "text": "The article shows the results in detail if you are interested\n\n[https://theprincipledengineer.substack.com/p/its-a-chunking-lie](https://theprincipledengineer.substack.com/p/its-a-chunking-lie)",
                  "score": 2,
                  "created_utc": "2026-01-27 16:17:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o26q28j",
          "author": "HMM0012",
          "text": "Makes sense; chunk size often drives context retention more than strategy. Controlling for size usually levels performance, so reported strategy wins can be misleading without normalizing chunk length.",
          "score": 2,
          "created_utc": "2026-01-28 09:35:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zyrl",
          "author": "notAllBits",
          "text": "I would abandon tokenization into chunks. Stream if you can, but cutting any text produces bias from discontinuity. I would follow syntactic and semantic structure when indexing.",
          "score": 1,
          "created_utc": "2026-01-27 15:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o218c9v",
              "author": "Jords13xx",
              "text": "Streaming is definitely an interesting approach, but it can be tricky with context retention. If you maintain some syntactic and semantic boundaries while still chunking, you might strike a balance between continuity and performance. Have you experimented with any hybrid methods?",
              "score": 1,
              "created_utc": "2026-01-27 15:47:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o21cnnp",
                  "author": "ManufacturerIll6406",
                  "text": "Recursive chunking is essentially a hybrid, it tries paragraphs first, falls back to sentences, then words. In my experiments it landed in the middle: better size control than sentence, slightly worse recall than token at equivalent sizes.\n\nThe interesting question is whether there's a \"best of both worlds\" approach: target a specific size but snap to the nearest sentence boundary. You'd get predictable chunk sizes without mid-sentence cuts.\n\nDidn't test that explicitly, but the framework is extensible & would be a straightforward strategy to add - [https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)\n\nMight be worth exploring in a follow-up.",
                  "score": 1,
                  "created_utc": "2026-01-27 16:06:37",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o223w9s",
          "author": "jrochkind",
          "text": "If you have text that has em, i'd try paragraph chunking (up to certain max size paragraph anyway)",
          "score": 1,
          "created_utc": "2026-01-27 18:04:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o22ip3o",
          "author": "blue-or-brown-keys",
          "text": "Curious if this may be trying to justify method based on outcome. Whats the intution here.",
          "score": 1,
          "created_utc": "2026-01-27 19:07:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o230ewt",
              "author": "ManufacturerIll6406",
              "text": "Intuition: More text = more semantic information encoded = better chance of matching the query. Also, answers rarely live in a single sentence, larger chunks capture the full context.\n\nThe \"justifying outcome\" concern would apply if I cherry-picked one result. But the correlation held across 30 configs, two datasets, and all four strategies landed on the same trendline (r=0.74 and r=0.92).\n\nCode's open if you want to test on a different dataset.",
              "score": 1,
              "created_utc": "2026-01-27 20:26:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o29pm8l",
          "author": "lyonsclay",
          "text": "Wouldn't the optimal chunk size be contingent on your vector size assuming you are using vector similarity to select the chunks? If your chunk is smaller than your vector size then your system is being wasteful, if your chunk is larger than your vector size you are losing information.",
          "score": 1,
          "created_utc": "2026-01-28 19:15:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2aikqp",
              "author": "ManufacturerIll6406",
              "text": "There probably is a sweet spot! In my experiments, recall kept improving up to \\~3000 chars with text-embedding-3-small (1536 dims). Didn't test beyond that. Could be interesting to check.\n\nhttps://preview.redd.it/arrmfswmq5gg1.png?width=1500&format=png&auto=webp&s=98470fd0c77c983075e9e1807d121e82f47fd5bd",
              "score": 1,
              "created_utc": "2026-01-28 21:23:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qjvqd4",
      "title": "Vector dbs aren't memory (learned this the hard way building a coding agent)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjvqd4/vector_dbs_arent_memory_learned_this_the_hard_way/",
      "author": "Sweet121",
      "created_utc": "2026-01-22 14:19:15",
      "score": 43,
      "num_comments": 22,
      "upvote_ratio": 0.82,
      "text": "So i spent the last month losing my mind building a personal coding tutor agent.\n\nWhe problem:\n\nWhe goal was simple: an agent that remembers my skill level, current project, and coding style (like 'i hate list comprehensions, just give me loops'). i did the standard thing: pinecone, chunking, and a RAG pipeline\n\nIt worked for like an hour. by day 3, the agent was a complete mess. it would retrieve code snippets from a project i finished two weeks ago. or worse, i'd tell it 'im switching to rust now', and it would still pull python examples because they were 'semantically similar' to my query. its honestly such a pain.\n\nWhat i tried:\n\ni tried everything to fix this:\n\n* crammed the context window: got expensive fast, and the model got 'lost in the middle'.\n* summarization chains: tried summarizing old convos, but it lost the specific details (like WHY i chose a specific library).\n* metadata filtering: helps, but managing that manually is a nightmare.\n\nthe breakthrough:\n\ni realized i was treating memory like a static library, but human memory is dynamic. we dont remember everything with equal weight. some things need to be forgotten, some need to be merged, and some only matter when they're actually relevant.\n\nthis might sound obvious in hindsight, but i realized i wasn‚Äôt missing a better database ‚Äî i was missing an operating system for memory.\n\nwhat i‚Äôm experimenting with:\n\nSo i built (and open sourced) something called MemOS.\n\nits a memory management layer that sits between your LLM and your storage. instead of just dumping text into a vector store, it treats memory with a lifecycle:\n\n* generated: raw info comes in\n* activated: relevant stuff is pulled into 'Working Memory' (RAM)\n* merged: repeated or evolving preferences get collapsed instead of duplicated.\n* activated: only stuff that actually matters *now* gets pulled into working memory.\n\nit also separates Facts (what happened) from Preferences (what i like). this was the real game changer for me. now, when i ask for a hotel recommendation, it checks my PREFERENCES (cheap, clean) and searches FACTS (hotels in the area).\n\nIm really trying to make this solid for production use, not just a toy demo\n\nthe repo is here: [https://github.com/MemTensor/MemOS](https://github.com/MemTensor/MemOS)\n\ndocs/cloud trial if you dont want to self-host: [https://memos-docs.openmem.net/cn](https://memos-docs.openmem.net/cn)\n\nWould love to hear how you guys are handling long-term user state vs static RAG. is anyone else trying to build an 'OS' layer for this?\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qjvqd4/vector_dbs_arent_memory_learned_this_the_hard_way/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o12b91h",
          "author": "Expensive_Culture_46",
          "text": "Oh look an ad",
          "score": -39,
          "created_utc": "2026-01-22 15:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12am84",
          "author": "Inner_Possibility310",
          "text": "context pollution is exactly what killed my last project. i hated that the bot would bring up 'User likes pizza' when i was asking about a 'Python pizza library'. does this actually filter that out?",
          "score": 3,
          "created_utc": "2026-01-22 15:43:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cehp",
              "author": "No-Living-8429",
              "text": "MemOS doesn‚Äôt treat all memory as global context. It separates facts, preferences, tools, and knowledge into different memory types, and only activates what‚Äôs relevant to the current task.\nSo ‚ÄúUser likes pizza‚Äù won‚Äôt leak into a ‚ÄúPython pizza library‚Äù query unless the retrieval policy explicitly says it should. Memory is filtered, scoped, and gated, not blindly stuffed back into the prompt.\n\nIn other words: MemOS is built to stop context pollution by design, not by prompt hacks.",
              "score": 1,
              "created_utc": "2026-01-22 15:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o13q7rp",
          "author": "New_Animator_7710",
          "text": "This hits hard because it‚Äôs *exactly* the failure mode of‚Äô ‚Äújust throw it in a vector DB‚Äù memory. You nailed it: the problem isn‚Äôt retrieval, it‚Äôs **state management**‚Äîforgetting, merging, and prioritizing like a real system, not a library. MemOS feels way closer to how humans (and good dev tools) actually work than most RAG stacks I‚Äôve seen",
          "score": 3,
          "created_utc": "2026-01-22 19:33:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11zga1",
          "author": "Crafty_Disk_7026",
          "text": "These tools never work for me.  What works is having a completely clean session and only giving the llm the specific info it needs.   There's no point to storing memory of 2 weeks ago when your app was a Python app if it's a rust app now....\n\nI would encourage you to do some benchmarking.  Try a task with your memory layer and without it and you may be surprised that it's worse with memory since it's just overloading the context with unnecessary info.",
          "score": 5,
          "created_utc": "2026-01-22 14:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o128cvy",
              "author": "Academic_Track_2765",
              "text": "This! Exactly this!",
              "score": 1,
              "created_utc": "2026-01-22 15:32:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12bmdq",
              "author": "Sweet121",
              "text": "This! You nailed the exact problem I hit.\n\nWhen I switched from Python to Rust, my previous RAG setup kept pulling Python snippets because they were 'semantically similar' to the coding task. It was worse than useless‚Äîit was confusing the model.\n\nThat's actually why I'm trying to treat memory with a lifecycle (forgetting mechanism) rather than a static dump. A 'clean session' is great, but I hated re-prompting 'I prefer concise code' or 'don't use unwrap()' every single time.\n\nI'm accepting your challenge on the 'with vs without' benchmark. I want to prove (to myself mostly) that selective memory > zero memory > bad memory. Will update the repo when I have those numbers.",
              "score": 1,
              "created_utc": "2026-01-22 15:47:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1eb6gl",
              "author": "LettuceEfficient7170",
              "text": "I feel the same way",
              "score": 1,
              "created_utc": "2026-01-24 08:58:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12b57s",
          "author": "not_-ram",
          "text": "interesting. so if i'm building a travel agent, and the user changes their mind from 'Budget' to 'Luxury' halfway through, does MemOS catch that? or do i get conflicting memories?",
          "score": 2,
          "created_utc": "2026-01-22 15:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cv88",
              "author": "No-Living-8429",
              "text": "yep, preferences are mutable memories in MemOS.\n\nwhen a user switches from Budget ‚Üí Luxury, the old preference is updated/archived, not kept active. Only the latest state is recalled ‚Äî no conflicting context, no stale decisions.",
              "score": 2,
              "created_utc": "2026-01-22 15:53:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ew04",
              "author": "Sweet121",
              "text": "thats the 'Conflict Resolution' part of the lifecycle. when new info conflicts with old info (high confidence), MemOS updates the state instead of just appending a new row",
              "score": 1,
              "created_utc": "2026-01-22 16:02:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bjqq",
          "author": "Aslymcrumptionpenis",
          "text": "yeah, distinguishing between semantic similarity and actual relevance is hard. how are you handling the 'Merging' part? is it an LLM call in the background?",
          "score": 1,
          "created_utc": "2026-01-22 15:47:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12d43k",
              "author": "No-Living-8429",
              "text": "merging is LLM-assisted, but gated.\n\nwe first use embeddings to detect potential duplicates, then call an LLM only to decide merge vs keep (and how to rewrite). If merged, the old memory is archived so it won‚Äôt steer future decisions.",
              "score": 1,
              "created_utc": "2026-01-22 15:54:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12by6n",
          "author": "IncreaseWilling6396",
          "text": "how is this different from MemGPT? or just using LangChain's EntityMemory? feels like we are reinventing the wheel.",
          "score": 1,
          "created_utc": "2026-01-22 15:49:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12disw",
              "author": "No-Living-8429",
              "text": "the difference is scope and lifecycle.\n\nMemGPT / EntityMemory mainly store + retrieve text inside an agent loop. MemOS treats memory as a system layer: multi-type (facts, prefs, tools), merge/archive over time, scheduled recall, and cross-agent reuse ‚Äî so you don‚Äôt just remember, you prevent stale or conflicting memory from steering decisions.",
              "score": 1,
              "created_utc": "2026-01-22 15:56:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14pm08",
          "author": "toothpastespiders",
          "text": ">Would love to hear how you guys are handling long-term user state vs static RAG. is anyone else trying to build an 'OS' layer for this?\n\nSadly, that's essentially my view of the only fully effective way to handle things. I think effective memory needs to be either custom made from the ground up to fit individual needs and styles or existing projects heavily modified to do the same.",
          "score": 1,
          "created_utc": "2026-01-22 22:21:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eaypc",
          "author": "LettuceEfficient7170",
          "text": "That's so cool, I want to try it!",
          "score": 1,
          "created_utc": "2026-01-24 08:56:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eg94d",
              "author": "Sweet121",
              "text": "yeah,  pls",
              "score": 1,
              "created_utc": "2026-01-24 09:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11y0yi",
          "author": "Academic_Track_2765",
          "text": "Ok bro, what are you selling?",
          "score": -35,
          "created_utc": "2026-01-22 14:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o125eou",
              "author": "Sweet121",
              "text": "This is an open-source project, and the community allows limited promotion. What are you yelling about here?",
              "score": 8,
              "created_utc": "2026-01-22 15:18:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o12842b",
                  "author": "Academic_Track_2765",
                  "text": "Just sick of people selling stuff, when there are production grade systems available. RAG is not rocket science. Install FAISS and be on your way if you want something basic. What really bothers me is that people say use this use that, without any benchmarks e.g., How do you know if weaviate is better than pinecone? If you aren‚Äôt in production than why aren‚Äôt you just using chromadb / faiss? There is already Mem0 and Zep and few other frameworks for memory. If you are going to advertise please share how it compares with other similar frameworks. Besides that I have no problem.",
                  "score": -2,
                  "created_utc": "2026-01-22 15:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlftqz",
      "title": "Which Vector DB should I use for production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "author": "Cheriya_Manushyan",
      "created_utc": "2026-01-24 06:42:26",
      "score": 42,
      "num_comments": 84,
      "upvote_ratio": 0.96,
      "text": "I see many enterprises using Pinecone, Weaviate, Milvus, Qdrant etc. Based on your experience, which one is best  for production and why? Help a friend out...üôÇ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1ep896",
          "author": "ampancha",
          "text": "If you're already on Postgres, pgvector is underrated. One less system to secure and operate, and recent benchmarks show it's competitive with the dedicated options at moderate scale.\n\nIf you want a purpose-built vector DB, Qdrant. Best latency performance in most independent tests, and the open-source version is production-ready.\n\nEither works. What usually breaks is the stuff around the DB: missing per-user query limits, no spend caps on embedding calls, no alerting when retrieval patterns drift. Sent you a DM",
          "score": 19,
          "created_utc": "2026-01-24 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o08hm",
              "author": "Appropriate_Ant_4629",
              "text": "Came here to describe a pretty extensive test from my workplace that benchmarked many (qdrant, milvus, chroma, pgvector, databricks's vector search feature, building our own with faiss, building our own with hnswlib, etc) ...    \n... but our findings exactly matched yours.  \n\nWhen we reach a billion vectors we'll re-visit Qdrant - because it is impressive technologically and was easy to shard across compute nodes; but at our modest tens-of-millions, pgvector shined.",
              "score": 3,
              "created_utc": "2026-01-25 18:41:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1et06u",
              "author": "Cheriya_Manushyan",
              "text": "Informative, thanks.",
              "score": 2,
              "created_utc": "2026-01-24 11:41:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1i65yu",
                  "author": "LightShadow",
                  "text": "We're using Qdrant for performance and continuing feature development. We didn't want the vector operations mixed with our normal postgres load in case they slowed each other down.",
                  "score": 2,
                  "created_utc": "2026-01-24 21:51:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1nr3j6",
              "author": "licjon",
              "text": "Both also work. You can do a 2-stage search starting with Qdrant and then narrowing even more with pgvector. Works well with cross-document search and large corpora.",
              "score": 2,
              "created_utc": "2026-01-25 18:04:33",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e2jwz",
          "author": "hrishikamath",
          "text": "Postgres w pgvector lol, I serve like 100k documents and it takes like milli seconds for hybrid search. Edit: repo: https://github.com/kamathhrishi/stratalens-ai and blogpost: https://substack.com/home/post/p-181608263",
          "score": 34,
          "created_utc": "2026-01-24 07:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eemh2",
              "author": "ZiKyooc",
              "text": "That is a lot of seconds",
              "score": 11,
              "created_utc": "2026-01-24 09:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fifrm",
                  "author": "hrishikamath",
                  "text": "Sorry I meant milli seconds loll (editing my comment)",
                  "score": 4,
                  "created_utc": "2026-01-24 14:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1e4j11",
              "author": "Ok-Adhesiveness-4141",
              "text": "Right choice.",
              "score": 3,
              "created_utc": "2026-01-24 07:58:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1e5hci",
              "author": "debauch3ry",
              "text": "~~Are you saying pgvector is bad?~~ (confusion resolved after edit!) I found the tech very reliable with 500k vectors indexed with HNSW giving very fast knn searches. As a general DB it's also fairly high tier.",
              "score": 5,
              "created_utc": "2026-01-24 08:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fijjb",
                  "author": "hrishikamath",
                  "text": "I meant milli seconds I edited my comment",
                  "score": 3,
                  "created_utc": "2026-01-24 14:32:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1foxph",
                  "author": "hrishikamath",
                  "text": "Repo: https://github.com/kamathhrishi/stratalens-ai and blogpost how I did it: https://substack.com/home/post/p-181608263",
                  "score": 2,
                  "created_utc": "2026-01-24 15:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ertbv",
              "author": "psanilp",
              "text": "How do you handle chunking? We use the Azure AI search pipeline and thinking of going local RAG. Do you have a ready product or licensable deployment?",
              "score": 2,
              "created_utc": "2026-01-24 11:30:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1hgho7",
                  "author": "hrishikamath",
                  "text": "Chunking: [https://substack.com/home/post/p-181608263](https://substack.com/home/post/p-181608263) Yes, a kind of ready product is online. also open source: [https://github.com/kamathhrishi/stratalens-ai](https://github.com/kamathhrishi/stratalens-ai)",
                  "score": 2,
                  "created_utc": "2026-01-24 19:50:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1f35x9",
              "author": "Straight-Gazelle-597",
              "text": "solid choice to cover at least 90% of the biz needs;-)",
              "score": 2,
              "created_utc": "2026-01-24 12:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fw61c",
              "author": "virgilash",
              "text": "Yeah, op, give PostgreSQL with pgvector a try üòâ",
              "score": 2,
              "created_utc": "2026-01-24 15:42:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1gigea",
                  "author": "Cheriya_Manushyan",
                  "text": "Yeah, definitely!",
                  "score": 1,
                  "created_utc": "2026-01-24 17:22:52",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1gil92",
              "author": "Cheriya_Manushyan",
              "text": "Thanks a lot for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 17:23:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e29ow",
          "author": "instantlybanned",
          "text": "I use milvus in production and it works extremely well for me.¬†\n\n\nPostgreSQL with pgvector is not an option for me because of the recall that I need and the speed with which I need the results at the scale that I'm working at. With milvus, I have control over the in memory index that's being used as well as the parameters for the index and the query parameters for the approximate search, allowing me to tune it to have high recall at still very fast speeds.¬†\n\n\nEdit: just for context, I'm working with around 200 million vectors.¬†",
          "score": 7,
          "created_utc": "2026-01-24 07:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6gs5",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing",
              "score": 1,
              "created_utc": "2026-01-24 08:15:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1ilmze",
              "author": "MeringueInformal7670",
              "text": "I am using Milvus in production as a consulting gig i did for a startup pretty solid perf so far during internal runs currently running on a single node standalone setup do you mind sharing how does your Milvus infra look like for 200 mil vectors. You can DM me as well thanks.",
              "score": 1,
              "created_utc": "2026-01-24 23:07:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1lrktx",
                  "author": "ChoiceEmpty8485",
                  "text": "Sure! For my Milvus setup with 200 million vectors, I use a distributed architecture with multiple nodes to handle the load. We have optimized the index parameters for both recall and speed, and utilize GPU acceleration for faster queries. Happy to share more specific details if you need!",
                  "score": 1,
                  "created_utc": "2026-01-25 11:57:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e7ex8",
          "author": "pberck",
          "text": "I used lancedb in Rust in my last project. It worked well, but it wasn't a huge amount of data.",
          "score": 5,
          "created_utc": "2026-01-24 08:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1etlhw",
              "author": "Cheriya_Manushyan",
              "text": "Is it open source or like Pinecone?",
              "score": 1,
              "created_utc": "2026-01-24 11:46:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1etvny",
                  "author": "pberck",
                  "text": "It is dual license, open source with apache 2.0 and a cloud version which has a commercial licence. I used the OS version.",
                  "score": 3,
                  "created_utc": "2026-01-24 11:48:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fmb3m",
          "author": "ComputationalPoet",
          "text": "ill add opensearch,  several variations of semantic search (approximate knn, exact cosine similarity), hybrid search and obviously bm25.   Scales well and performs great, though ill admit i dont know some of these other options that well.",
          "score": 4,
          "created_utc": "2026-01-24 14:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1dyeel",
          "author": "IdeaAffectionate945",
          "text": "Roll your own, way faster and more flexible. Preferably something that's a \"plugin\" to SQL, allowing you to parametrise your vector retrieval saying stuff such as \"select \\* from rag where x, and distance(...)\"\n\nIt's a 100 times more flexible than whatever Pinecone even \\*can\\* give you in theory.\n\nI'm using SQLite and sqliteai-vector ...",
          "score": 11,
          "created_utc": "2026-01-24 07:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dz14n",
              "author": "Cheriya_Manushyan",
              "text": "I‚Äôve personally been using PostgreSQL with pgvector, but I notice many enterprises prefer databases like Pinecone. I‚Äôm trying to understand the real reasons behind this choice.",
              "score": 7,
              "created_utc": "2026-01-24 07:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1e2dnr",
                  "author": "IdeaAffectionate945",
                  "text": "*\"I notice many enterprises prefer databases like Pinecone\"*\n\nMarketing bs. The best filtering you can do is on meta fields. With integrated into the core DB, you've got a bajillion times the speed, and a bajillion times the flexibility on querying the thing.",
                  "score": 6,
                  "created_utc": "2026-01-24 07:39:02",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1e188g",
                  "author": "Chucki_e",
                  "text": "I also use pgvector and I don't think you need to use any third-party vector database unless you have some special requirements that force you to. As with so many other architectural/technology choices, it's easier to start simple and then scale up when you actually need to - of course with a plan in mind, but I don't imagine migrating your vector database isn't that big of an issue?",
                  "score": 6,
                  "created_utc": "2026-01-24 07:28:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1q151b",
              "author": "Appropriate_Ant_4629",
              "text": "For roll-you-own, do you prefer faiss, hnswlib, or something else?\n\nIn our benchmarking hnswlib vastly outperformed faiss; though this may have been user error.\n\nBut in the end, for a few million vectors it didn't really matter; and pgvector was more convenient; and for a billion vector test, sharding across nodes became the hard part and qdrant seemed to handle it best.",
              "score": 2,
              "created_utc": "2026-01-26 00:05:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rtauk",
                  "author": "IdeaAffectionate945",
                  "text": "*\"do you prefer faiss, hnswlib\"*\n\nsqliteai-vector isn't using any indexing as far as I know, only HW optimisation and *\"quantisation\"*, which makes a 1,580 dimensional vector become like 250 if you wish, almost without loosing quality ...\n\nCheck it out on GitHub if you wish. Technically, it's years ahead of *\"everything else\"* in the space ...\n\nHowever, for me it's just a lib, and I don't care about its implementations. I know it's capable of serving my sjit faster than anything else I've tried for SQLite, and I know it's doesn't leak memory like everything else I've tried for SQLite. And I know it returns results in sub seconds even through a DB with 100,000 items, and it gives me *\"memory guarantees\"* which I love like crazy.\n\nIt might not be as scalable as some of the other solutions (for PostgreSQL, MySQL, etc), but then again I'm not building Twitter or Instagram either ...",
                  "score": 2,
                  "created_utc": "2026-01-26 06:07:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ec1id",
          "author": "Suspicious-Bite6107",
          "text": "People that say pgvector is slow are usually DEVs that want \"to simplify their life\" and dont look or make any benchmark, also because they don't read they still think you have to use postgres  with pgvector only, and forget about pgvectorscale or diskann indexes....thank you for still providing work, I think that even with AI there is enough stupidity in this world to allow to have work for the next 50 years :) \n\nPS - your app isn't special, you are not going to have to handle 100k concurent transactions unless as usuall you keep not using pooling :p",
          "score": 6,
          "created_utc": "2026-01-24 09:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eu7ei",
              "author": "Cheriya_Manushyan",
              "text": "Well, you have a fair point.",
              "score": 1,
              "created_utc": "2026-01-24 11:51:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1errhw",
          "author": "Useful-Disk3725",
          "text": "I had qdrant for some time, really fast. Then switched to MariaDB 11.8. Vector search is really fast, but insert is slow. I think qdrant had a buffering system, and building indexes from time to time (standing 100% cpu spikes regularly). I mad a similar thing, getting vectors to a buffer table without index and in a separate cronned process moving in batches.\n\nThe key is, never mix vector index search with regular searches, that‚Äôs a known limitation in almost all databases. Though overcome is easy (technologies advertised as hybrid search are only hack solutions)",
          "score": 3,
          "created_utc": "2026-01-24 11:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1et5tn",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1la8xl",
          "author": "HealthOk5149",
          "text": "Postgres + pgvector with HNSW indexing or + pgvectorscale with DiskANN indexing when RAM becomes an issue at scale because HNSW needs a lot of RAM",
          "score": 3,
          "created_utc": "2026-01-25 09:26:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e3jfr",
          "author": "Academic_Track_2765",
          "text": "Many options, if you guys use azure use azure search, I have deployed solutions with chromadb, neo4j, weaviate. See what costs the least for your dimensions X documents and use that. I think it mostly comes down to what your company / IT dept is ok with and the costs. You can even build your own if you like.",
          "score": 2,
          "created_utc": "2026-01-24 07:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e8uj9",
              "author": "Cheriya_Manushyan",
              "text": "From your experience, which option do you prefer if the goal is good performance at a low cost?",
              "score": 1,
              "created_utc": "2026-01-24 08:37:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3v7x",
          "author": "Effective-Ad2060",
          "text": "We use qdrant (supports hybrid search out of the box)  \nFor reference:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)",
          "score": 2,
          "created_utc": "2026-01-24 07:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1egvk3",
              "author": "KYDLE2089",
              "text": "+1 we do too work really good",
              "score": 2,
              "created_utc": "2026-01-24 09:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1emf8s",
              "author": "Cheriya_Manushyan",
              "text": "Noted.",
              "score": 0,
              "created_utc": "2026-01-24 10:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e4vaf",
          "author": "nborwankar",
          "text": "Depends on corpus size, whether there is need for joining with relational data and whether there is need to scale up massively",
          "score": 2,
          "created_utc": "2026-01-24 08:01:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eseds",
              "author": "Cheriya_Manushyan",
              "text": "That's an interesting case you shared.",
              "score": 0,
              "created_utc": "2026-01-24 11:36:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1gwlhy",
                  "author": "nborwankar",
                  "text": "My default suggestion for workgroup or departmental size enterprise applications is just use Postgres w pgvector which gives you a hybrid relational vector db with ability to join across relational and vector data using just SQL syntax.\n\nIt is not as performant as the pure vector databases but it is very familiar and it is the easiest way to get started while you figure out what you want to do. This is very important for onboarding DB developers.\n\nNot to blow my own horn but I have a book called ‚ÄúVector Databases‚Äù from OReilly coming out in a few months - available on Amazon.\n\nIf you want to read parts of it for free - sign up for the OReilly Platform - it‚Äôs free for 7 days.\n\nThe first chapter deals with these issues. You can also DM me if you have specific questions.",
                  "score": 1,
                  "created_utc": "2026-01-24 18:24:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1eit8q",
          "author": "RolandRu",
          "text": "In my opinion there is no single ‚Äúbest‚Äù vector DB for production.\n\nI‚Äôm building a code-focused RAG. For now FAISS is enough for me, but I also added BM25 search, hybrid search and a dependency graph between code chunks.\n\nAfter some time I realized that new requirements will only make my custom code more complicated. In practice it feels like I‚Äôm rebuilding features that Weaviate already has (BM25 + hybrid + graph/relations).\n\nQdrant can be faster, but for me the difference like 25ms vs 35ms doesn‚Äôt really matter. Native support for everything I need matters more, so the next step will be migrating to Weaviate and testing it in real use.\n\nAt the same time I will keep FAISS as a nice option for people who want to run the project quickly without setting up a container and configuring Weaviate.\n\nSo Weaviate ‚Äî **if someone thinks this is a mistake, please let me know** üôÇ",
          "score": 2,
          "created_utc": "2026-01-24 10:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esoxa",
              "author": "Cheriya_Manushyan",
              "text": "Noted",
              "score": 1,
              "created_utc": "2026-01-24 11:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1erp4p",
          "author": "psanilp",
          "text": "PostGres with some plugin seems to be the preferred route. Having said that, does anyone here have a 'rag in a box' i can install and deploy at a legal firm?",
          "score": 2,
          "created_utc": "2026-01-24 11:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1exwig",
              "author": "seomonstar",
              "text": "anything for legal firms needs heavy skills and lots of custom work. Thats why there is so much money in it. from parsing to chunking to retrieval to context and session management.  There are some open source rag things on github, but I wouldnt be deploying them in a law firm ‚Ä¶",
              "score": 3,
              "created_utc": "2026-01-24 12:21:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1etdu0",
              "author": "Cheriya_Manushyan",
              "text": "By 'rag in a box', you mean low code/no code solution?",
              "score": 1,
              "created_utc": "2026-01-24 11:44:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g34yq",
                  "author": "psanilp",
                  "text": "I understand RAG pipeline and we currently use Azure AI Search with inbuilt vectorisation/hybrid/semantic search . a) There are some issues related to chunking longer docs. b) Some firms who are willing to pay, would like a setup where the RAG is done locally so everything stays within the firewall. Hence no external api calls. So my query was if someone has an end to end solution we can buy/license and deploy on a hardware that is physically placed in the client's premises.",
                  "score": 2,
                  "created_utc": "2026-01-24 16:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1f3g4n",
          "author": "a_developer_2025",
          "text": "Doesn‚Äôt pgvector apply metadata filtering only after searching by vectors? If that‚Äôs still true, it is a big issue if you have small datasets per metadata.",
          "score": 2,
          "created_utc": "2026-01-24 13:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fphel",
          "author": "bzImage",
          "text": "Qdrant.",
          "score": 2,
          "created_utc": "2026-01-24 15:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ftsfa",
          "author": "seomonstar",
          "text": "Postgre scaled to 800 million users with openai, still at their core, so anyone having problems with pgvector should bear in mind its likely a skill issue unless they have 800 million users or more . https://openai.com/index/scaling-postgresql/",
          "score": 2,
          "created_utc": "2026-01-24 15:30:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1gzv2h",
          "author": "Leather-Departure-38",
          "text": "Align with your current cloud infra and db",
          "score": 2,
          "created_utc": "2026-01-24 18:38:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ixl6d",
          "author": "purposefulCA",
          "text": "Our org has mssql. We just pushed all our vectors there. Working fine so far",
          "score": 2,
          "created_utc": "2026-01-25 00:10:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1l8ufi",
          "author": "my_byte",
          "text": "Lots of considerations. Personally, my go to if working with a python or nodejs codebase is mongodb. Postgres is also a good choice. Big fan of not adding multiple db's & search engines to your stack. If you end up scaling a ton, migrating to a dedicated search engine isn't terribly hard.",
          "score": 2,
          "created_utc": "2026-01-25 09:13:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1llcpd",
          "author": "TemporaryMaybe2163",
          "text": "Genuine question: I wonder why nobody commented about Oracle 26ai. \nIs it bad?",
          "score": 2,
          "created_utc": "2026-01-25 11:04:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1mp3in",
          "author": "martinschaer",
          "text": "SurrealDB. It scales without costing a fortune, runs in single node or distributed, and it is multi model in case you want to do hybrid search with BM25 or graph",
          "score": 2,
          "created_utc": "2026-01-25 15:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1shv08",
          "author": "jerrysyw",
          "text": "I had used Milvus for embedding store and chunk text with PG wiht  my product",
          "score": 2,
          "created_utc": "2026-01-26 09:37:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1e6g4b",
          "author": "debauch3ry",
          "text": "I bet those enterprises using Pinecone also write their backend software in python or node.js.\n\nI use PaaS cloud postgres DB + pgvector, as I find it a good mix of having control vs delegating the infrastructure to a cloud provider.\n\nQdrant you can run locally via docker which is testament to their confidence in their tech. They're my next-to-try.",
          "score": 3,
          "created_utc": "2026-01-24 08:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eskjy",
              "author": "Cheriya_Manushyan",
              "text": "For most of them, a python based stack is used.",
              "score": 2,
              "created_utc": "2026-01-24 11:37:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dw4nh",
          "author": "PiaRedDragon",
          "text": "Qdrant or neo4j.",
          "score": 2,
          "created_utc": "2026-01-24 06:44:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dwhad",
              "author": "Cheriya_Manushyan",
              "text": "Can you give reasons, focusing on performance and cost compared to other databases?",
              "score": 2,
              "created_utc": "2026-01-24 06:47:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1enimk",
          "author": "crishoj",
          "text": "Are you certain you even need vectors? Have you tried regular keyword search? Have the agent come up with relevant search terms",
          "score": 2,
          "created_utc": "2026-01-24 10:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esw2l",
              "author": "Cheriya_Manushyan",
              "text": "I haven't tried keyword only search, will checkout.",
              "score": 1,
              "created_utc": "2026-01-24 11:40:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1etdxu",
          "author": "Professional_Cup6629",
          "text": "might be a good read: https://agentset.ai/blog/best-vector-db-for-rag",
          "score": 1,
          "created_utc": "2026-01-24 11:44:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ey7jg",
              "author": "Cheriya_Manushyan",
              "text": "Will check.",
              "score": 1,
              "created_utc": "2026-01-24 12:23:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f35tp",
          "author": "Live-Guitar-8661",
          "text": "Postgres",
          "score": 1,
          "created_utc": "2026-01-24 12:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jd3lw",
          "author": "Grocery_Odd",
          "text": "Went through making this decision for a separate project, developed and applied this framework to get an eval-driven solution: [https://github.com/conclude-ai/rag-select](https://github.com/conclude-ai/rag-select)",
          "score": 1,
          "created_utc": "2026-01-25 01:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1vt3gt",
          "author": "Green_Crab_9726",
          "text": "FalkorDB",
          "score": 1,
          "created_utc": "2026-01-26 20:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o286yq6",
          "author": "CutPuzzleheaded5884",
          "text": "Turopuffer",
          "score": 1,
          "created_utc": "2026-01-28 15:18:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjp5vy",
      "title": "Turn documents into an interactive mind map + chat (RAG) üß†üìÑ",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjp5vy/turn_documents_into_an_interactive_mind_map_chat/",
      "author": "sAI_Innovator",
      "created_utc": "2026-01-22 08:29:10",
      "score": 37,
      "num_comments": 9,
      "upvote_ratio": 0.95,
      "text": "Built an app that converts any PDF/DOCX into an interactive mind map (NotebookLM-style).\n\n‚Ä¢ Click a node ‚Üí summary + keywords + ask questions\n\n‚Ä¢ Chat with the whole document (RAG + sources)\n\n‚Ä¢ Document history saved\n\nStack: React + FastAPI, LlamaIndex (parent‚Äìchild), optional Docling parsing.\n\nRepo: https://github.com/SaiDev1617/mindmap\n\nWould love feedback!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qjp5vy/turn_documents_into_an_interactive_mind_map_chat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o11uwuz",
          "author": "CommercialComputer15",
          "text": "How does it organize and recognise relationships between documents? Semantically? Is it a graph?",
          "score": 2,
          "created_utc": "2026-01-22 14:26:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14up4d",
              "author": "sAI_Innovator",
              "text": "Using Hierarchical Llamaindex node parser üëç",
              "score": 1,
              "created_utc": "2026-01-22 22:50:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bfp6",
          "author": "Aslymcrumptionpenis",
          "text": "oh wow thats helpful",
          "score": 1,
          "created_utc": "2026-01-22 15:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14us0k",
              "author": "sAI_Innovator",
              "text": "Thank you! Please check out the repo.",
              "score": 1,
              "created_utc": "2026-01-22 22:51:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14mnnb",
          "author": "Unique-Temperature17",
          "text": "Great stuff, congrats on shipping this! The mind map visualisation approach is a nice twist on the usual RAG chat interface. Will definitely clone and check it out over the weekend. Always cool to see LlamaIndex projects in the wild.",
          "score": 1,
          "created_utc": "2026-01-22 22:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14u1ei",
              "author": "sAI_Innovator",
              "text": "cool. Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 22:47:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o264l25",
          "author": "PlanetMercurial",
          "text": "Will this work with local llm's. I mean open ai compatible supported?",
          "score": 1,
          "created_utc": "2026-01-28 06:27:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2658iu",
              "author": "sAI_Innovator",
              "text": "Yes It is openai compatible supported design. But may not be SLM‚Äôs due to the context window limits.",
              "score": 1,
              "created_utc": "2026-01-28 06:32:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o286xb6",
                  "author": "PlanetMercurial",
                  "text": "Ok thanks.. good to know that will try it out with either GLM4.7 Flash or Qwen3.  \nThanks again.",
                  "score": 1,
                  "created_utc": "2026-01-28 15:18:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qmlxfw",
      "title": "Looking for testers: 100% local RAG system with one-command setup",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "author": "primoco",
      "created_utc": "2026-01-25 15:23:30",
      "score": 29,
      "num_comments": 30,
      "upvote_ratio": 0.94,
      "text": "Hey everyone! üëã\n\nI've been working on an open-source RAG system and I'm looking for people willing to test it and give honest feedback.\n\n\\*\\*What it does:\\*\\*\n\nA document processing and Q&A system that runs 100% locally on your machine. No API keys needed, no data leaving your computer.\n\n\\*\\*Why I built it:\\*\\*\n\nI was frustrated with RAG solutions that either required cloud services, complex Docker configurations, or hours of setup. I wanted something that \"just works\" out of the box for businesses that need complete data privacy.\n\n\\*\\*Tech stack:\\*\\*\n\n\\- FastAPI backend\n\n\\- React + Vite frontend\n\n\\- Qdrant for vector storage\n\n\\- Ollama for local LLMs (Qwen2.5 or Mistral 7B)\n\n\\- BAAI/bge-m3 embeddings\n\n\\*\\*Key features:\\*\\*\n\n\\- Single command to start everything (\\`./setup.sh standard\\`)\n\n\\- Completely offline after initial setup\n\n\\- Supports 29 languages\n\n\\- Multi-user with JWT auth and role-based access\n\n\\- OCR support (Apache Tika + Tesseract)\n\n\\- Handles PDF, DOCX, PPTX, XLSX, TXT, MD and more\n\n\\- Tested with 10,000+ documents\n\n\\*\\*What I'm looking for:\\*\\*\n\n\\- Feedback on installation experience (Ubuntu 20.04+)\n\n\\- Real-world testing with your own documents\n\n\\- Bug reports and edge cases\n\n\\- Suggestions for improvements\n\n\\*\\*Repo:\\*\\* [https://github.com/I3K-IT/RAG-Enterprise](https://github.com/I3K-IT/RAG-Enterprise)\n\nI've stress-tested it with large documents (including the Mueller Report) but I'd love to see how it handles different use cases and languages.\n\nHappy to answer any questions!\n\nEDIT: Benchmark script and real-world results now published! See Community Benchmarks in the README.\n\n\\---\n\n\\*Fully open-source under AGPL-3.0. No paid tiers, no telemetry, no external calls.\\*  \n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmlxfw/looking_for_testers_100_local_rag_system_with/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1o03id",
          "author": "ampancha",
          "text": "Nice work on the local-first setup. One thing worth stress-testing before enterprise users hit it: retrieval-augmented systems are vulnerable to prompt injection via document content, and multi-user setups without per-user rate limits or query attribution can get abused fast. Both failure modes are invisible until production. Sent you a DM with more detail.",
          "score": 3,
          "created_utc": "2026-01-25 18:41:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o66iq",
              "author": "primoco",
              "text": "Really appreciate this feedback ‚Äî you‚Äôre raising exactly the kind of security considerations that matter for production deployments.\nYou‚Äôre right that both prompt injection via document content and multi-user abuse patterns are often invisible until they hit you in production. These are definitely on my radar for hardening the system.\nChecked your DM ‚Äî thanks for the detailed insights. I‚Äôll follow up there.\nFor anyone else reading: this kind of security-focused feedback is gold. If you spot potential vulnerabilities or have suggestions, Issues and DMs are always welcome.",
              "score": 1,
              "created_utc": "2026-01-25 19:06:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nbwin",
          "author": "Chrys",
          "text": "I could give it a try but I have a Mac mini. What do you think? It will be too slow?",
          "score": 1,
          "created_utc": "2026-01-25 17:00:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1nu1eo",
              "author": "primoco",
              "text": "Hi Chris, it depends on which Mac Mini you have.\nApple Silicon (M1/M2/M4) with 16GB+ RAM: Should work well! Ollama runs natively on Apple Silicon and uses the integrated GPU. Performance won‚Äôt match a dedicated NVIDIA card, but it‚Äôs definitely usable.\nIntel Mac Mini: Will be slower since it runs on CPU only.\nThe catch: The automated setup.sh script is designed for Ubuntu + NVIDIA. On Mac, you‚Äôd need to set things up manually:\n\t‚àô\tInstall Docker Desktop\n\t‚àô\tInstall Ollama for Mac\n\t‚àô\tRun Qdrant via Docker\n\t‚àô\tStart the backend/frontend manually\nIf you share your Mac Mini specs (chip + RAM), I can give you a better estimate and maybe help with Mac-specific instructions. It would actually be great to have Mac compatibility documented!",
              "score": 1,
              "created_utc": "2026-01-25 18:16:29",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1ogig8",
                  "author": "Chrys",
                  "text": "Mac mini M4 10-core CPU 10-core GPU 16GB/256GB. \nWhich local LLM do you suggest?",
                  "score": 1,
                  "created_utc": "2026-01-25 19:52:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ou98e",
          "author": "fredastere",
          "text": "Def check it out ty",
          "score": 1,
          "created_utc": "2026-01-25 20:52:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwajj",
          "author": "Character_Pie_5368",
          "text": "Is there a mcp server interface? I have some pentesting frameworks that I‚Äôd like to try and interface with thisz",
          "score": 1,
          "created_utc": "2026-01-26 02:41:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rs2va",
              "author": "primoco",
              "text": "Not yet ‚Äî MCP server interface isn‚Äôt implemented at the moment.\nCurrently the system exposes a REST API (FastAPI backend on port 8000) that you could target for pentesting. The main endpoints are:\n\t‚àô\t/api/v1/query ‚Äî RAG queries\n\t‚àô\t/api/v1/documents ‚Äî document upload/management\n\t‚àô\t/api/v1/auth ‚Äî JWT authentication\nThat said, MCP support is an interesting idea for the roadmap ‚Äî would make integration with Claude and other tools much smoother.\nIf you run your pentesting frameworks against it and find vulnerabilities, I‚Äôd genuinely appreciate the feedback! There‚Äôs a SECURITY.md for responsible disclosure.\nWhat frameworks are you planning to use? Curious what you‚Äôre testing for.",
              "score": 1,
              "created_utc": "2026-01-26 05:58:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ratzb",
          "author": "floating___around",
          "text": "I am interested. Would a 3090 run that program?",
          "score": 1,
          "created_utc": "2026-01-26 04:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ru7re",
              "author": "primoco",
              "text": "Absolutely! A 3090 with 24GB VRAM is perfect ‚Äî even more than my test setup (RTX 5070 Ti, 16GB).\n\nThe setup script has three profiles:\n Profile   |GPU VRAM|LLM Model  |RAM  |\n |----------|--------|-----------|-----|\n |`minimal` |8-12GB  |Mistral 7B |16GB |\n |`standard`|12-16GB |Qwen2.5 14B|32GB |\n |`advanced`|16-24GB |Qwen2.5 32B|128GB|\n \nWith your 3090 (24GB), you could run:\n \n ```\n ./setup.sh advanced\n ```\n \nThis installs `qwen2.5:32b-instruct-q4_K_M` ‚Äî the most capable model available.\n \nNote: The advanced profile expects 128GB system RAM. If you have less, you can still run `./setup.sh advanced` but consider switching to the 14B model in `docker-compose.yml` if you experience issues.\n \nAlternatively, `./setup.sh standard` with the 14B model is a safe choice that will run great on your hardware.\nLet me know how it goes!\n\n-----\n\nCos√¨ diamo info complete e accurate. Vuoi modificare qualcosa?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã",
              "score": 1,
              "created_utc": "2026-01-26 06:14:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rj89p",
          "author": "ggone20",
          "text": "\nSearch performance metrics? ‚ÄòTested on 10,000+ documents‚Äô with just a Qdrant db and an old small models? Idk‚Ä¶",
          "score": 1,
          "created_utc": "2026-01-26 04:56:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rv6ux",
              "author": "primoco",
              "text": "Qdrant handles vector search efficiently ‚Äî at 10K docs the bottleneck is typically LLM inference, not retrieval. Query times stay in the 2-4 second range in my testing.\nI don‚Äôt have formal benchmark charts published yet, but it‚Äôs something I‚Äôd like to add to the documentation. If you run stress tests on your setup, I‚Äôd be happy to include real-world metrics from the community!",
              "score": 1,
              "created_utc": "2026-01-26 06:22:02",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1rwkiv",
                  "author": "ggone20",
                  "text": "I really mean retrieval performance for non-trivial queries.",
                  "score": 1,
                  "created_utc": "2026-01-26 06:32:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1rrwi1",
          "author": "AwayLuck7875",
          "text": "Rag ollama vulkan??",
          "score": 1,
          "created_utc": "2026-01-26 05:56:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rvm6p",
              "author": "primoco",
              "text": "Are you asking about Vulkan support for AMD/Intel GPUs?\nRAG Enterprise is currently tested with NVIDIA GPUs (CUDA). Ollama does have experimental Vulkan support for AMD cards, but I haven‚Äôt tested that combination yet.\nIn theory it should work ‚Äî the backend just calls Ollama‚Äôs API, it doesn‚Äôt care what‚Äôs running underneath. But I can‚Äôt guarantee performance or stability on Vulkan.\nIf you have an AMD GPU and want to try it:\n\t1.\tInstall Ollama with Vulkan support manually\n\t2.\tSkip the automated setup and configure services individually\n\t3.\tPoint the backend to your Ollama instance\nLet me know your GPU ‚Äî happy to help figure out if it‚Äôs worth trying!",
              "score": 1,
              "created_utc": "2026-01-26 06:25:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rxwbk",
          "author": "AwayLuck7875",
          "text": "Maybe byt work",
          "score": 1,
          "created_utc": "2026-01-26 06:43:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s4h4j",
              "author": "primoco",
              "text": "Sorry, didn't quite catch that ‚Äî could you clarify? Happy to help if you have questions!",
              "score": 1,
              "created_utc": "2026-01-26 07:37:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1rzws6",
          "author": "arxdit",
          "text": "Definitely something that many of us are working on.\n\nI have built my own, and it supports both ollama and openai api key.\n\non ollama I rely mostly on qwen3-coder:30b which is nothing short of AMAZING.\n\nI made my own curated indexing & retrieval algos focusing on handling ever expanding knowledge - including an endless conversation manager that does not compact - it's here \\[[https://github.com/andreirx/FRAKTAG](https://github.com/andreirx/FRAKTAG)\\] if you want to check it out\n\nOh and having a CLI makes it usable by claude code",
          "score": 1,
          "created_utc": "2026-01-26 06:59:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1s5csz",
              "author": "primoco",
              "text": "Nice! Just checked out FRAKTAG ‚Äî interesting approach with the non-compacting conversation manager. Different philosophy from what I'm doing but I can see the value for ever-expanding knowledge bases.\n\nThe documentation is really thorough ‚Äî that's not easy to maintain, kudos for that.\n\nThe CLI + Claude Code integration is a smart move ‚Äî that's actually something I should consider adding to RAG Enterprise.\n\nqwen3-coder:30b is impressive, though it needs serious VRAM. I've kept the default models smaller (7B-14B) to lower the entry barrier, but power users can definitely swap in larger models.\n\nCool to see others building in this space ‚Äî plenty of room for different approaches! ü§ù",
              "score": 2,
              "created_utc": "2026-01-26 07:45:05",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1s7eiw",
                  "author": "arxdit",
                  "text": "I wanted to have the capability of targeting enterprise level private deployments - like a law firm",
                  "score": 1,
                  "created_utc": "2026-01-26 08:02:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1zgjsq",
              "author": "GP_103",
              "text": "Pretty cool. Did you actually Claude code that in 13 days. Props.",
              "score": 1,
              "created_utc": "2026-01-27 08:52:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1ziacp",
                  "author": "arxdit",
                  "text": "Thank you!\n\nYes - first planned with gemini pro + opus, then passed to claude code. Still using this approach on major features / refactors.\n\nPeppered my codebase with MAP files and now my main problem is babysitting claude code... can't fully trust an AI with code but I'm getting used to its failure modes and I can smell the mistakes much faster",
                  "score": 1,
                  "created_utc": "2026-01-27 09:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zz39u",
          "author": "primoco",
          "text": "UPDATE: Benchmarks are live!\n\nAdded a complete benchmarking suite to the repo:\n\nBenchmark script: python benchmark/rag\\_benchmark.py ‚Äî run it on your hardware\n\nTest documents: Mueller Report, 9/11 Commission Report, Bitcoin Whitepaper, \"Attention Is All You Need\"\n\nReal metrics: Upload times, query latency (mean/median/p95), similarity scores\n\nResults from my setup (Ryzen 9 5950X, 64GB RAM, RTX 5070 Ti):\n\nQuery response: \\~4.3s mean, \\~3.6s median\n\nUpload: 0.6s - 24s depending on document size\n\nFull details in the README: Community Benchmarks section\n\nIf you run the benchmark on your hardware, I'd love to add your results to the comparison table. Open an issue or comment here!",
          "score": 1,
          "created_utc": "2026-01-27 11:37:05",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qnplpb",
      "title": "Built a tool for visualizing text chunking strategies",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "author": "Crazy-Plan8697",
      "created_utc": "2026-01-26 19:09:49",
      "score": 27,
      "num_comments": 0,
      "upvote_ratio": 0.97,
      "text": "Hey :)\n\nSome time ago I wanted to learn the fundamentals of RAG, and I started with chunking. Greg Kamradt‚Äôs tool ([https://chunkviz.up.railway.app/](https://chunkviz.up.railway.app/)) helped me understand the basics, and it was a great starting point.\n\nWhile digging deeper, especially when reading papers like [https://arxiv.org/pdf/2312.10997](https://arxiv.org/pdf/2312.10997), I noticed that **semantic** and **agentic** chunking are showing up more often and are getting adopted in new research. But I couldn‚Äôt find any visualizers that supported those methods, so I tried to build one myself.\n\nI put together **Chunking-Vis**, a small web tool for anyone who wants to explore and learn how different chunking strategies behave. I think it can be especially helpful if you‚Äôre new to RAG and want to see how chunks are formed before they‚Äôre sent to an embedding model or retrieval pipeline.\n\n# What Chunking-Vis supports\n\n* Character-level chunking\n* Word-level chunking\n* Token-level chunking (GPT-4o tokenizer)\n* Recursive chunking\n* Semantic chunking\n* Agentic chunking (Phi-3 powered, available locally on CPU)\n\nThere‚Äôs also a **Snapshot** feature that lets you save and compare different chunking configurations side-by-side, which can make experimentation easier.\n\n# Live Demo\n\n[https://chunkingvis-production.up.railway.app](https://chunkingvis-production.up.railway.app)  \n(Agentic chunking is disabled in the demo due to compute limits.)\n\n# Github Repository\n\n[https://github.com/MichalZnalezniak/Chunking-Vis](https://github.com/MichalZnalezniak/Chunking-Vis)\n\nHope some of you find it useful ‚Äî and if you have ideas, feedback, or suggestions, please let me know. Thank you!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qnplpb/built_a_tool_for_visualizing_text_chunking/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qjx9yy",
      "title": "üöÄ We designed a white-box RAG framework with a built-in AI developer assistant ‚Äî feel free to give it a try!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjx9yy/we_designed_a_whitebox_rag_framework_with_a/",
      "author": "Relevant_Abroad_6614",
      "created_utc": "2026-01-22 15:20:04",
      "score": 24,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "üöÄ **Introducing UltraRAG 3.0: Reject \"Black Box\" Development. Make Every Line of Inference Logic Visible!**\n\nüèÅ **UltraRAG 3.0** solves the \"Last Mile\" problem in RAG development, developed by THUNLP, NEUIR, OpenBMB & AI9Stars.\n\nüêô **GitHub:** [https://github.com/OpenBMB/UltraRAG](https://github.com/OpenBMB/UltraRAG)\n\nüìö **Tutorial:** [https://ultrarag.openbmb.cn/pages/en/getting\\_started/introduction](https://ultrarag.openbmb.cn/pages/en/getting_started/introduction)\n\n**Key Highlights:**\n\n‚ö° **WYSIWYG Pipeline Builder**\n\nFrom logic to prototype in seconds. Our dual-mode builder (Canvas + Code) syncs in real-time. Click \"Build\" and your static logic instantly becomes an interactive UI. No more boilerplate code!\n\nüîç **Pixel-Level \"White-Box\" Visualization**\n\nStop guessing. The \"Show Thinking\" panel visualizes the entire inference trajectory‚Äîloops, branches, and tool calls. Debug bad cases instantly by comparing retrieval chunks vs. model hallucinations.\n\nü§ñ **Built-in AI Developer Assistant**\n\nStuck on config? The embedded AI Assistant knows the framework inside out. Just use natural language to generate Pipeline configurations, optimize Prompts, or explain parameters.\n\nüî¨ **DeepResearch Engine**\n\nPowered by **AgentCPM-Report**, it supports \"Writing-as-Reasoning.\" The system dynamically plans, retrieves, and deepens content to generate professional, cited reports automatically.\n\n**Check out the demo video on the GitHub pageÔºÅ**",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qjx9yy/we_designed_a_whitebox_rag_framework_with_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17j8td",
          "author": "Relevant_Abroad_6614",
          "text": "https://preview.redd.it/knov7unxf2fg1.png?width=3686&format=png&auto=webp&s=d173ebf383e8371cd9f08f851e7473c9b3f93981",
          "score": 1,
          "created_utc": "2026-01-23 09:14:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o17j9b8",
          "author": "Relevant_Abroad_6614",
          "text": "https://preview.redd.it/1wvuoz51g2fg1.png?width=3734&format=png&auto=webp&s=304ef394c007ea63f4b2792b772b3b5924716bbf",
          "score": 1,
          "created_utc": "2026-01-23 09:14:40",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkupw7",
      "title": "We did RAG on the r/Rag Reddit channel - Free To Use",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkupw7/we_did_rag_on_the_rrag_reddit_channel_free_to_use/",
      "author": "jannemansonh",
      "created_utc": "2026-01-23 16:02:28",
      "score": 20,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hey, creator of Needle here. This channel is one of the reasons we started Needle as a RAG API in the first place! So many real insights from people actually building things.\n\nSo we made a free, public search tool that lets you explore everything discussed here in 2025.\n\nUseful if you're:\n\n* A dev looking for RAG advice\n* A business person exploring how RAG can help\n* A founder researching trends and common problems\n\nWould love your feedback and curious: what questions would you find most interesting to explore?\n\nCompletely free, no signup required: [https://needle.app/featured-collections/reddit-rag-2025](https://needle.app/featured-collections/reddit-rag-2025)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qkupw7/we_did_rag_on_the_rrag_reddit_channel_free_to_use/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o19sp9g",
          "author": "Popular_Sand2773",
          "text": "Without any attribution it's hard to tell what's just world knowledge vs. actually driven by your retrieval so the value add is really unclear.",
          "score": 3,
          "created_utc": "2026-01-23 17:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bpwwt",
              "author": "jannemansonh",
              "text": "You can see the references that are given back in the chat.",
              "score": 1,
              "created_utc": "2026-01-23 22:43:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1der3y",
                  "author": "Popular_Sand2773",
                  "text": "Sorry if it was there it wasn‚Äôt obvious.",
                  "score": 1,
                  "created_utc": "2026-01-24 04:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dp6ac",
          "author": "Key-Contact-6524",
          "text": "Lovely bro , Thanks a ton",
          "score": 1,
          "created_utc": "2026-01-24 05:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecmhn",
              "author": "jannemansonh",
              "text": "Sure! Happy you find it helpful!",
              "score": 1,
              "created_utc": "2026-01-24 09:12:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cako",
          "author": "lucido_dio",
          "text": "RAG-ception :D one step closer to singularity",
          "score": 1,
          "created_utc": "2026-01-23 16:06:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19diah",
              "author": "jannemansonh",
              "text": "Haha, geeky jokes always make me laugh",
              "score": 1,
              "created_utc": "2026-01-23 16:11:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qpce5s",
      "title": "You can now train embedding models 1.8-3.3x faster!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "author": "yoracale",
      "created_utc": "2026-01-28 14:11:25",
      "score": 19,
      "num_comments": 7,
      "upvote_ratio": 0.96,
      "text": "Hey RAG folks! We collaborated with Hugging Face to enable 1.8-3.3x faster embedding model training with 20% less VRAM, 2x longer context & no accuracy loss vs. FA2 setups.\n\nFull finetuning, LoRA (16bit) and QLoRA (4bit) are all faster by default! You can deploy your fine-tuned model anywhere: transformers, LangChain, Ollama, vLLM, llama.cpp etc.\n\nFine-tuning embedding models can improve retrieval & RAG by aligning vectors to your domain-specific notion of similarity, improving search, clustering, and recommendations on your data.\n\nWe provided many free notebooks with 3 main use-cases to utilize. \n\n* Try the [EmbeddingGemma notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/EmbeddingGemma_(300M).ipynb) in a free Colab T4 instance\n* We support ModernBERT, Qwen Embedding, Embedding Gemma, MiniLM-L6-v2, mpnet, BGE and all other models are supported automatically!\n\n‚≠ê Guide + notebooks: [https://unsloth.ai/docs/new/embedding-finetuning](https://unsloth.ai/docs/new/embedding-finetuning)\n\nGitHub repo: [https://github.com/unslothai/unsloth](https://github.com/unslothai/unsloth)\n\nThanks so much guys! :)\n\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qpce5s/you_can_now_train_embedding_models_1833x_faster/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o287p7m",
          "author": "z0han4eg",
          "text": "Thanks mate, this a rly big news",
          "score": 2,
          "created_utc": "2026-01-28 15:21:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27wj9w",
          "author": "Popular_Sand2773",
          "text": "This is very cool. Quick question if we are using these encoders for the base of something else is this still valuable or is it only really for classic fine tuning? If I understand correctly the main speedup came from a new fused kernel correct?",
          "score": 1,
          "created_utc": "2026-01-28 14:28:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o284gga",
              "author": "yoracale",
              "text": "Apologies could you elaborate your first question?\n\nOur main optimizations includes gradient checkpointing, kernels yes and more. You can see gradient checkpointing here: https://unsloth.ai/docs/new/500k-context-length-fine-tuning#unsloth-gradient-checkpointing-enhancements",
              "score": 1,
              "created_utc": "2026-01-28 15:06:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o28gjpf",
          "author": "TechySpecky",
          "text": "Why do people fine tune models and when do these lead to superior performance than large well-known embedding models like gemini / qwen ones? For example if I am doing RAG for archeology would it make sense to have a custom embedding model?",
          "score": 1,
          "created_utc": "2026-01-28 16:00:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o294cnv",
              "author": "Financial-Bank2756",
              "text": "yes, a custom embedding model could help if you have enough domain text and evaluation pairs. Otherwise, a strong general model plus better chunking, metadata filters, hybrid search, and rerankers often beats premature fine-tuning.",
              "score": 1,
              "created_utc": "2026-01-28 17:44:07",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o294nzx",
                  "author": "TechySpecky",
                  "text": "Yea makes sense, what do you mean by evaluation pairs?",
                  "score": 1,
                  "created_utc": "2026-01-28 17:45:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qp9pmy",
      "title": "How to handle extremely large extracted document data in an agentic system? (RAG / alternatives?)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "author": "Complex-Time-4287",
      "created_utc": "2026-01-28 12:13:17",
      "score": 15,
      "num_comments": 22,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm building an agentic system where users can upload documents. These documents can be *very* large ‚Äî for example, up to **15 documents at once**, where some are **\\~1500 pages** and others **300‚Äì400 pages**. Most of these are financial documents (e.g., tax forms), though not exclusively.\n\nWe have a document extraction service that works well and produces structured layout + document data.  \nHowever, the extracted data itself is also huge, so we **can‚Äôt fit it into the chat context**.\n\n  \n**Current approach**\n\n* The extracted structured data is stored as a **JSON file in cloud storage**\n* We store a **reference/ID in the DB**\n* Tools can fetch the data using this reference when needed\n\n  \n**The Problem**\n\nBecause the agent never directly ‚Äúsees‚Äù or understands the extracted data:\n\n* If a user asks questions about the document content,\n* The agent often can‚Äôt answer correctly, since the data is not in its context or memory\n\n  \n**What we‚Äôre considering**\n\nWe‚Äôre thinking about applying **RAG on the extracted data**, but we have a few concerns:\n\n* Agents run in a chat loop ‚Üí **creation + retrieval must be fast**\n* The data is deeply nested and very large\n* We want minimal latency and good accuracy\n\n**Questions**\n\n1. What are **practical solutions** to this problem?\n2. Which **RAG systems / architectures** would work best for this kind of use-case?\n3. Are there **alternative approaches** (non-RAG) that might work better for large documents?\n4. Any best practices for handling **very large documents** in agentic systems?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qp9pmy/how_to_handle_extremely_large_extracted_document/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o27bksd",
          "author": "Mishuri",
          "text": "You must do semantic chunking on the document. Split it up according to logical partitions. Ask LLM to enhance with descriptions metadata. Explicit relationships to other sections. Then embed those. 90% and the expensive part is LLM preprocessing of this. Then you gather context with vector rag and details with agentic rag + subagents to manage context.",
          "score": 4,
          "created_utc": "2026-01-28 12:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27dy9y",
              "author": "rshah4",
              "text": "Yes, and then you can have a Table of Contents and make sure each of those sections understand their role in the hierarchical structure. This is what we do and it works well.  \nAlso a database can be useful here as well.",
              "score": 1,
              "created_utc": "2026-01-28 12:46:07",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27onir",
              "author": "Complex-Time-4287",
              "text": "this it totally possible, but I'm concerned about the time it is likely to take, in a chat it'll feel kind of blocking until the chuking and embedding is complete",
              "score": 1,
              "created_utc": "2026-01-28 13:47:25",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o28i5j8",
              "author": "usernotfoundo",
              "text": "Are there any particular resources you would suggest that go into detail in this process? Currently I have been simply using an llm to process my large paragraph into a list of (observation,recommendation), embedding the observations and retrieving it based on similarity with a query. I feel this is too simplified, and breaking it down into multiple steps like you described could be the way to go, no idea how to start.",
              "score": 1,
              "created_utc": "2026-01-28 16:07:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bc02",
          "author": "patbhakta",
          "text": "For financial data, skip the traditional RAG, skip the vector databases, perhaps skip graph rag too. Go with trees, you'll incur more cost but at least your data will be sound.",
          "score": 1,
          "created_utc": "2026-01-28 12:28:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27fxm7",
              "author": "yelling-at-clouds-40",
              "text": "Trees are just subset of graphs, but curious: what kind of trees do you suggest (as node hierarchy)?",
              "score": 1,
              "created_utc": "2026-01-28 12:58:22",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27nv20",
              "author": "ajay-c",
              "text": "Interesting do you know any tree techniques?",
              "score": 1,
              "created_utc": "2026-01-28 13:43:17",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27oswt",
              "author": "Complex-Time-4287",
              "text": "can you please provide some details on this?",
              "score": 1,
              "created_utc": "2026-01-28 13:48:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27bjew",
          "author": "arxdit",
          "text": "I‚Äôm handling this via knowledge trees and human supervised document ingestion (you supervise proper slicing and where the document belongs in the knowledge tree - though the AI does make suggestions)\n\nThe AI by itself is very bad at organizing information with no clear rules and will fail spectacularly \n\nSlowly learning through this\n\nYou can check out my solution [FRAKTAG on github](https://github.com/andreirx/FRAKTAG)",
          "score": 1,
          "created_utc": "2026-01-28 12:30:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27nxzr",
              "author": "ajay-c",
              "text": "Interesting",
              "score": 1,
              "created_utc": "2026-01-28 13:43:43",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o27p3tn",
              "author": "Complex-Time-4287",
              "text": "Looks interesting, I'll check this  \nFor my use-case, we cannot really have a human in the loop, agents are completely autonomous and must proceed on their own",
              "score": 1,
              "created_utc": "2026-01-28 13:49:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27q1w8",
                  "author": "arxdit",
                  "text": "I want to get there too and I‚Äôm using my human decisions to hopefully ‚Äúteach‚Äù the ai how to do it by itself, and I am gathering data",
                  "score": 1,
                  "created_utc": "2026-01-28 13:54:40",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27btme",
          "author": "proxima_centauri05",
          "text": "You‚Äôre not doing anything ‚Äúwrong‚Äù. This is the natural failure mode when the agent only has a pointer to the data instead of an understanding of it. If the model never sees even a compressed view of the document, it‚Äôll confidently answer based on vibes.\n\nWhat‚Äôs worked for me is separating understanding from storage. On ingestion, I generate a thin semantic layer, section summaries, key entities, numbers, obligations, relationships. That layer is small, fast, and always available to the agent. The heavy JSON stays out of the loop unless the agent explicitly needs to verify something.\nTrying to RAG directly over deeply nested extracted data is usually a dead end. It‚Äôs slow, and the signal to noise ratio is awful. Hierarchical retrieval helps a lot, first decide where to look, then pull only that slice, then answr. Latency stays low because most questions never touch the raw data.\n\nFor financial or forms heavy docs, I often skip RAG entirely and just query normalized fields. It‚Äôs boring, but it‚Äôs correct. RAG is great for ‚Äúexplain‚Äù questions, terrible for ‚Äúcalculate‚Äù ones.\n\nI‚Äôm building something in this space too, and the big unlock was treating documents like evolving knowledge objects, not blobs you fetch. Once the agent has a map of the document, it stops hallucinating and starts reasoning.",
          "score": 1,
          "created_utc": "2026-01-28 12:32:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27tqal",
              "author": "Complex-Time-4287",
              "text": "In my case, the questions are much more likely to be *‚Äúfind‚Äù* questions rather than *‚Äúcalculate‚Äù* ones. For extremely large documents say a 1,500-page PDF containing multiple tax forms summaries or key-entity layers won‚Äôt realistically capture all the essential details.\n\nAlso, I‚Äôm not entirely sure what you mean by ‚Äújust query normalized fields‚Äù in this context.",
              "score": 1,
              "created_utc": "2026-01-28 14:13:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o27mwi8",
          "author": "KnightCodin",
          "text": "There are few gaps in the problem summary that might help  \n1. Operational flow :   \nWhen you say \"tools can fetch the data using this reference when needed\" and \"gent never directly ‚Äúsees‚Äù or understands the extracted data\",   \n\\- Where is data going to - directly to the user and not to the Agent/LLM?  \n\\- What is stopping you from presenting the \"summary\" data (Chain of Density compressed) to the Agent so follow up questions can be answered\n\n2. Do you need to answer questions on documents by other users? - Meaning is it a departmental segmentation with multiple users but same overall context or user/documents are isolated?   \n\\- This will provide type of KG and scale   \n\\- Summary \"Contextual Map\"",
          "score": 1,
          "created_utc": "2026-01-28 13:38:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o27riay",
              "author": "Complex-Time-4287",
              "text": "In my agentic system, users can connect third-party MCP tools. If a tool requires access to the extracted data, the agent can pass that data to the specific tool the user has attached, but only when it‚Äôs actually needed.\n\nThe main issue with relying on summaries is that the extracted data itself is already very large and deeply nested JSON. Generating a meaningful summary from it is hard, and even a compressed (Chain-of-Density‚Äìstyle) summary would still fail to answer very specific questions‚Äîfor example, ‚ÄúWhat was the annual income in 2023?‚Äù\n\nRegarding document access and isolation: documents are scoped strictly to the current conversation. Conversations are not user-specific, and there can be multiple conversations, but within each conversation we only reference the documents uploaded in that same context.\n\nDocuments are uploaded dynamically as part of the conversation flow, and only those on-the-go uploads are considered when answering questions or invoking tools.",
              "score": 1,
              "created_utc": "2026-01-28 14:02:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o27twm2",
                  "author": "KnightCodin",
                  "text": "Better :)  \nSimplistic and practical solution (not to be confused with simple) is   \nMulti-tier retrieval:   \n  \nEg : ‚ÄúSummary doc map‚Äù ‚Üí ‚ÄúTargeted Sub-Node‚Äù ‚Üí \"Drill-Down Deep fetch\"\n\nThis will be the most latency-effective for massive bundles.\n\n**SPECIFICITY :**   \n**Tier A: coarse index**\n\n* Embed¬†full **page summaries**,¬†**section headers**, and¬†**table captions**\n* Or **one chunk per page**¬†: Fully summarized (Will say normalized but that will open a whole new can of worms)\n* Path: identify *which pages/sections matter -> use deep fetch to grab that JSON*\n\n**Tier B: targeted extraction retrieval**\n\n* Once you know relevant pages/sections, fetch only that slice from cloud storage:\n   * e.g., pages 210‚Äì218 JSON\n   * or the section subtree for¬†`Income >` What was the annual income in 2023",
                  "score": 1,
                  "created_utc": "2026-01-28 14:14:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o27oiym",
          "author": "ajay-c",
          "text": "I do have same issues",
          "score": 1,
          "created_utc": "2026-01-28 13:46:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27swp4",
          "author": "Crafty_Disk_7026",
          "text": "Try using a retrieval MCP https://github.com/imran31415/codemode-sqlite-mcp/tree/main. Here's one I made you can try.  This won't require embedding",
          "score": 1,
          "created_utc": "2026-01-28 14:09:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27u005",
          "author": "Popular_Sand2773",
          "text": "Your instinct that you can't just shove these in the context window is correct and that you need some sort of RAG but what and why depend on the answers to these questions.\n\nWhat questions are your users asking?  \nGiven it is financial documents if it is mainly numbers and tables they care about then you should think about a SQL db and retrieval. Regular semantic embeddings are not very good at highly detailed math. If it's contract minutia then maybe a vector db and semantic embeddings. Likely you'll need both.\n\nHow much of this is noise?  \nYou mention huge documents and tax forms as an example. If a lot of this is stuff your users are never going to query you are paying both in quality and cost for things you won't use and don't need. Figure out what you can prune. \n\nIs there clear structure you can leverage?  \nJust because it's called unstructured text doesn't mean there is no structure at all. If you can narrow down where in the documents you are looking for a specific query based on the inherent structure like sections etc then you can narrow the search space and increase your top-k odds.\n\nAll this to say. It's not about what RAG is best etc it's what problem are you actually trying to solve and why. If you just want a flat quality bump without further thought try knowledge graph embeddings.",
          "score": 1,
          "created_utc": "2026-01-28 14:15:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o28dreo",
          "author": "Det-Nick-Valentine",
          "text": "I have the same problem.\n\nI'm working on an in-company solution like NotebookLM.\n\nIt works very well for small and medium-sized documents, but when the user uploads something large, like legal documents, it doesn't give good responses.\n\nI'm thinking of going for a summary by N chunks and working with re-ranking.\n\nWhat do you think of this approach?",
          "score": 1,
          "created_utc": "2026-01-28 15:48:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2a9g7o",
          "author": "pl201",
          "text": "Take a look of open source LightRAG. Per my research and trying, it has the best potential to be used for the requirements you have described in the post. I am working on enhancements so it can be used on a company setting (multi users, workspace, separate embedding LLM and chat LLM, speed the query for a larger knowledge base.  Etc). PM me if you are interested to make it work for your case.",
          "score": 1,
          "created_utc": "2026-01-28 20:43:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qohzz3",
      "title": "Multilingual RAG for Legal Documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "author": "mathrb",
      "created_utc": "2026-01-27 16:06:51",
      "score": 14,
      "num_comments": 9,
      "upvote_ratio": 0.89,
      "text": "Hey all,\n\nWe're a small team (not many engineers) building a RAG system for legal documents(contracts, NDAs, terms of service, compliance docs, etc.).\n\nThe multilingual challenge:\n\nOur documents span multiple languages (EN, FR, DE, ES, IT, etc.).\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have docs in a single language (e.g., all French)\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some tenants have mixed-language corpora\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Some individual documents are bilingual\n\n¬†\n\nFor legal docs, hybrid search (FT search and dense vectors with re rank) seems to be a good candidate for retrieval. One issue I saw is that most implementations relies on language dependent solutions for FT search.\n\nApproaches I've seen discussed:\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Per-language BM25 indexes: Detect language, route to the right index with proper stemmer. Seems correct but adds complexity. How do you handle bilingual documents?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Language-agnostic tokenization: Skip stemming, just split on whitespace. Loses morphological matching but works across languages.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† BGE-M3 sparse vectors: Supposedly handles 100+ languages natively for both dense and sparse. But does it require GPU? What's the cost/perf tradeoff vs traditional BM25?\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Translate everything to English: Normalize the knowledge base. Feels wrong for legal where original wording matters and adds a translation failure mode.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Dense-only + reranker : Skip BM25 entirely, use strong multilingual embeddings (BGE-M3, multilingual-e5) and rerank. Loses exact keyword matching.\n\n¬∑¬†¬†¬†¬†¬†¬†¬†¬† Qdrant's native BM25 : Qdrant now has built-in BM25 with language configs. Anyone using this for multilingual? How does it compare to dedicated solutions?\n\n¬†\n\nWe‚Äôd rather use managed services when available in the cloud provider we chose (scaleway).\n\nOur constraints:\n\n* Managed PostgreSQL for app data : only supports pgvector, not pg\\_search/ParadeDB. Would require to self-host a postgres for additional extensions.\n* Prefer simplicity: Leaning toward Qdrant over Milvus since it seems easier to operate.\n* Cost-conscious: GPU-heavy solutions for embeddings are a concern.\n* Multi-tenant: Each tenant's documents are typically in one consistent language, but not always.\n\nAnyone would like to share their experience or thoughts on this challenge?  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qohzz3/multilingual_rag_for_legal_documents/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o21ijp5",
          "author": "patbhakta",
          "text": "1) translate everything to English is a bad solution for legal reasons as the verbage needs to stay in tact. Translation is another can of worms you don't want to go down.\n\n2) you could have multiple vector\\graph\\sql databases for each language, querying them all could be a nightmare though.\n\n3) embeddings, chunking, parsing, are all an issue that needs to be addressed with every RAG.\n\n4) legal documents are like a git repo, multiple versions and annotations, how are going to handle edits?\n\n5) cross referencing legal jurisdiction that periodically changes as well and in multiple countries you'll need a system for that knowledge base\n\n6) the larger your documents grow the worse your AI will get and will hallucinate beyond use. \n\n7) dedups and gardening can help but that's another wrench in the system\n\nPersonally I wouldn't use RAG on your use case, you need a truthful symantec search with citation. You really should seek consultation before going forward on ideas.",
          "score": 6,
          "created_utc": "2026-01-27 16:32:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o21nzh2",
              "author": "mathrb",
              "text": "Thanks for your reply\n\n1. Agree, feels wrong in this domain\n2. Agree with the nightmare\n3. As of today, only last version will be handled\n4. Not in scope\n\n\\`truthful symantec search with citation\\`, what kind of systems are you referring to?  \nEdit: I assume you meant semantic search.  \nEven though, we still face some of the same the challenges (minus the generative part at the end), right?",
              "score": 1,
              "created_utc": "2026-01-27 16:55:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o21wv03",
                  "author": "patbhakta",
                  "text": "Yes semantic hybrid approach. \nWithout knowing your document size, document length, who the users are, what do they want done, etc it's hard to provide a solution. \n\nIf the scenario was like a legal SaaS where documents needed to be translated then AI is great at that or a summary to avoid reading hundreds of pages. A tuned AI with case logic can also research as a paralegal or even a college level lawyer.\n\nIf you're building an AI tailored to the firm based on its documents, procedures and cases then it's a bit more complex but doable.\n\nIf you're building an AI for automation even that's doable but a different approach. \n\nHave to break down the scope, what it's roadmap is like, budget, legal compliance, security access (who has access to what documents) etc...",
                  "score": 1,
                  "created_utc": "2026-01-27 17:34:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o269emw",
          "author": "jael_m",
          "text": "You can still do the hybrid search combining dense vector search and text match like BM25. There are some special tokenizers for multilingual text. For example, milvus supports language identifier to automatically detect and apply the proper tokenizer, and the multi-language analyzer for text retrieval.",
          "score": 1,
          "created_utc": "2026-01-28 07:06:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26e9yg",
              "author": "explodedgiraffe",
              "text": "But sparse will still fail on cross language queries right? It would match a query ‚Äúv√©lo‚Äù (French) with ‚Äúbike‚Äù in English ?",
              "score": 1,
              "created_utc": "2026-01-28 07:47:45",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o2blzit",
          "author": "Repulsive-Memory-298",
          "text": "Interesting. I mean bm25 is beyond FT right? You could start with FT key word maybe? Where you could milk that by sorting by vector similarity? \n\nI wonder if term frequency would really struggle with combined languages. \n\nAnyways there are lots of future directions. Definitely start with more basic solutions imo, it‚Äôll give you a footing and then you can explore and test more complex solutions. BM25 is pretty advanced FT compared to meat and potato‚Äôs.",
          "score": 1,
          "created_utc": "2026-01-29 00:34:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkeiss",
      "title": "I built a RAG as a second brain",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkeiss/i_built_a_rag_as_a_second_brain/",
      "author": "mapt0nik",
      "created_utc": "2026-01-23 02:30:16",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.87,
      "text": "I work in software development. On my typical day I go from meeting to meeting and review lots of API contracts and design diagrams. The other day I simply couldn't recall something in a group discussion (I guess my brain was overloaded).\n\nThen it hit me: Why don't I build an app to house all stuff I have gone through so I could ask it for anything whenever? Works like the Pensieve in Harry Potter Dumbledore stores and retrieves memories. üßô‚Äç‚ôÇÔ∏è ü™Ñ  It is like my second brain.\n\nSo I vibe-coded [Second Brain](https://second-brain-484821.web.app/). A private RAG to serve as a digital twin for your personal and/or professional knowledge base.\n\nIf you are interested, here is the [blog](https://maptonik.hashnode.dev/second-brain-enters-public-preview) on my story behind it. Check it out. All comments are welcome.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qkeiss/i_built_a_rag_as_a_second_brain/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1bepnf",
          "author": "Smart_MoneyTor",
          "text": "I am not particularly against one trying to productize their vibe coded app, but there is a surge in these private RAG 2nd brain kind of apps. \n\nPersonally, I‚Äôd be more interested in the approach, the knowledge representation, the retrieval strategy, the performance metrics, the architectural design .. the novelty really. If it‚Äôs just stitching off-the-shelf tooling, I could just make use of any of the more privacy-friendly alternatives. That being said, I get that not everyone is fond of open sourcing their stuff, but good luck to you OP!",
          "score": 6,
          "created_utc": "2026-01-23 21:48:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qngww9",
      "title": "GraphRAG D√©j√† Vu: Freezing Edges = Graph DB Repeat? (Prod Trade-offs)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "author": "dqj1998",
      "created_utc": "2026-01-26 14:05:21",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 0.86,
      "text": "**Update (Jan 27, 2026)**:  \nThanks for the great discussion here in r/RAG! Some highlights from related threads:\n- Determinism & reproducibility as key to relational DB win (echoing why Graph DBs struggled).\n- Real prod experiences: keep graphs deterministic/auditable (e.g., calls/imports/FKs), avoid LLM-guessed edges clutter.\n- Links shared: [DDG preprint](https://zenodo.org/records/18373053) and [RoslynIndexer repo](https://github.com/RusieckiRoland/RoslynIndexer) for deterministic code RAG.\n\n---\n\nr/RAG ‚Äî GraphRAG hype (explicit graphs over vector RAG) feels like 70s graph DBs (IMS/CODASYL): explicit relations won benchmarks but lost to relational cuz upfront assumptions brittle.\n\n**Hype vs Reality**\nLLM infers entities/relations ‚Üí persist edges ‚Üí query traversal. Cool for global search, but edges = ingestion-time guesses ‚Üí bias for new intents.\n\n**Core Brittleness**\nFrom my [r/programming post](https://www.reddit.com/r/programming/comments/1pz6pj3/graphrag_is_just_graph_databases_all_over_again/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button): Nodes=facts, edges=guesses. Scoped query-time inference (BM25+vectors+rerank) often better for ambiguous RAG (no freeze).\n\n**Pushback & Predictability**\nComments nailed it: auditable edges > opaque LLMs (prod win). Dynamic rebuilds? Viable, but maintenance cost high vs simple hybrid RAG.\n\nShines: stable domains (regs/code deps). Fails: intent-shifting queries.\n\nMedium breakdown:[Medium friend link](https://medium.com/sisai/graphrags-deja-vu-why-are-we-repeating-the-same-mistakes-f6852f54bde0?sk=2692c642e7dfb19e9d552162462384c4)\n\n\nProd experiences? GraphRAG beat baseline RAG on your corpus (e.g., multi-hop QA, latency)? Hybrid + dynamic graphs? Or stick to rerank?\n\nShare benchmarks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qngww9/graphrag_d√©j√†_vu_freezing_edges_graph_db_repeat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1uoi8i",
          "author": "RolandRu",
          "text": "Really happy I found this Reddit btw ‚Äî the topics here are genuinely interesting and they kind of force you to think things through.\n\nAnd yeah, you‚Äôre right: it really depends.\n\nGraphs *can* be brittle when the edges are basically guessed during ingestion (LLM-inferred relations). You‚Äôre sort of freezing assumptions that may not match what people will ask later.\n\nBut it‚Äôs very use-case dependent. For code, I honestly think a dependency graph is pretty much **non-optional**. Calls/imports/inheritance aren‚Äôt opinions ‚Äî they‚Äôre real structure. Without graph expansion you often end up with random snippets, and vanilla RAG struggles badly with questions like ‚Äúwhere does this start?‚Äù or ‚Äúwhat does this change affect?‚Äù, because you‚Äôre missing the whole call chain.",
          "score": 4,
          "created_utc": "2026-01-26 17:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1xa5pj",
              "author": "dqj1998",
              "text": "I totally agree! Seeing your comment about code dependencies resonated with me, I also believe that code is essentially a set of pre-defined dependency chains, and debugging is about constantly patching the gap between these pre-defined chains and reality.\n\nWhile my thinking is still quite rudimentary, that's precisely why I wanted to discuss it further here.\n\nThank you for sharing your real-world experience! In this era where AI is coding faster and faster, this kind of discussion is really interesting‚Äîdo you think AI might represent a spiral of dependency reduction?",
              "score": 2,
              "created_utc": "2026-01-27 00:17:07",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xg881",
                  "author": "RolandRu",
                  "text": "I see it like this: dependencies aren‚Äôt going away, because they‚Äôre basically a consequence of architecture (boundaries, responsibilities, contracts). But AI lowers the cost of doing things ‚Äúthe right way‚Äù ‚Äî it‚Äôs easier to add an adapter, an interface, a test, validation, or split a big chunk into smaller parts, without feeling like you‚Äôre wasting time on repetitive stuff. So you‚Äôre less tempted to cram everything into one file/method ‚Äújust because it‚Äôs faster.‚Äù",
                  "score": 2,
                  "created_utc": "2026-01-27 00:48:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1upwsy",
          "author": "hhussain-",
          "text": "Determinism is what really makes this paradigm *shake*.  \nYour Reddit post and Medium article are excellent ‚Äî they pinpoint both the possibilities *and* the limitations very clearly.\n\nWhat ultimately made relational databases win wasn‚Äôt just performance, but **determinism and reproducibility enabled by a new computing model**.\n\nI had a similar discussion on [r/Rag post](https://www.reddit.com/r/Rag/comments/1qg2h8f/why_is_codebase_awareness_shifting_toward_vector/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) about deterministic graphs vs probabilistic vectors/embeddings. That thread, along with related discussions, helped isolate the issue to what seems like a missing graph category.\n\nInteresting to see this resurfacing now. I‚Äôve just published a timestamped preprint defining a *Deterministic Domain Graph (DDG)* category:  \n[https://zenodo.org/records/18373053](https://zenodo.org/records/18373053)\n\nI‚Äôm currently working on a framework to construct DDGs in practice, and early experiments suggest this is feasible even for large real-world codebases.",
          "score": 3,
          "created_utc": "2026-01-26 17:20:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1vrjfq",
              "author": "RolandRu",
              "text": "Thanks for sharing the article ‚Äî honestly pretty interesting read.\n\nThis is actually close to where I ended up while building a **code RAG** system. I‚Äôm trying to keep the *edges* deterministic + auditable (calls/imports/inheritance, ReadsFrom/WritesTo, FKs etc.), and I‚Äôm really trying to avoid freezing ‚ÄúLLM-guessed‚Äù relations during ingestion.\n\nI kind of treat vectors as *ranking / fuzzy recall*, but the graph as the closed-world structure that should rebuild the same way every time. For example I force stable outputs (sorted nodes/edges) and I also add missing TABLE nodes so the SQL graph is actually closed (nodes + edges), not half implicit.\n\nOne thing I‚Äôd highlight though: **heuristics ‚â† inference**. I‚Äôm fine with fixed, testable heuristics (like inline SQL detection) ‚Äî even if it‚Äôs not perfect, it‚Äôs still deterministic and you can regression-test it. What I‚Äôm trying to avoid is context-dependent enrichment that changes depending on the model/prompt or whatever the ‚Äúbest guess‚Äù is this week.\n\nIf you‚Äôre curious, this repo is just the **indexing part** (Roslyn/.NET side). The actual RAG pipeline / retrieval is in a separate project:  \n[https://github.com/RusieckiRoland/RoslynIndexer](https://github.com/RusieckiRoland/RoslynIndexer)\n\nAlso curious how you want to handle schema evolution / versioning for DDGs on big real repos ‚Äî do you version the domain spec per build, kind of like a compiler?",
              "score": 3,
              "created_utc": "2026-01-26 20:01:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xcvpz",
                  "author": "dqj1998",
                  "text": "Wow, thank you both‚Äîthis thread is gold! \n\nu/hhussain, your DDG preprint looks fascinating‚Äîdeterminism + reproducibility as the key to why relational won makes total sense, and early experiments on large codebases sound promising. Will dive into it right away.\n\nu/RolandRu, super appreciate you sharing your setup and the RoslynIndexer repo! Exactly aligns with what I've been thinking: keep the graph as closed-world, deterministic structure (hard facts like calls/imports/inheritance/ReadsFrom/WritesTo/FKs), and treat vectors as fuzzy recall/ranking. Love the emphasis on heuristics (fixed, testable, regression-friendly) vs. context-dependent LLM guesses‚Äîthat's the brittleness killer.Your approach to forcing stable outputs (sorted nodes/edges) and adding missing nodes for closed structure is smart‚Äîavoids the \"half implicit\" mess. Curious on a couple things:\n\n\\* How do you handle schema evolution/versioning in big repos? Per-build domain spec like a compiler, or something else?\n\n\\* Have you seen measurable wins on those chain-tracing queries (e.g., impact analysis) vs. vanilla RAG?\n\n\\* Any thoughts on hybrid with dynamic rebuilds for evolving code, or is pure deterministic the way?\n\nThis ties perfectly into what I'm exploring next: code itself as frozen presuppositions/dependencies, and debug as closing the gap to reality. If you're open, I'd love to reference/link your repo/preprint in upcoming posts (with credit, of course).\n\nThanks again‚Äîthreads like this are why I love posting here!",
                  "score": 3,
                  "created_utc": "2026-01-27 00:30:58",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkjetz",
      "title": "What RAG topics would you actually read a deep-dive on?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkjetz/what_rag_topics_would_you_actually_read_a/",
      "author": "Hungry-Amount-2730",
      "created_utc": "2026-01-23 06:26:47",
      "score": 10,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "Hey folks, been diving into RAG implementations lately and wondering what specific areas you'd actually want to see covered in depth? I'm thinking about writing something more technical/methodical but want to make sure it's stuff people actually care about.\n\nAre there any particular challenges or sub-topics around retrieval that you find interesting? Like chunking strategies, hybrid search approaches, reranking methods, that kind of thing? Or maybe semantic code search since that seems to be picking up lately?\n\nJust curious what gaps you see in the current content out there. What would make you actually sit down and read through a longer piece?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qkjetz/what_rag_topics_would_you_actually_read_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17si8c",
          "author": "TannerTot69",
          "text": "Dive into chunking strategies and hybrid search approaches especially in how they handle different types of data. Also reranking methods are something I feel doesn‚Äôt get covered enough, so a detailed breakdown on how to improve retrieval quality with those would be valuable.",
          "score": 7,
          "created_utc": "2026-01-23 10:39:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17dxec",
          "author": "NoBlueberry6793",
          "text": "check out this discord server \"context engineers\" it's all about reranking, new rag approaches etc..lots of smart & helpful folks\n\nhttps://discord.gg/HxBBDN3Q",
          "score": 5,
          "created_utc": "2026-01-23 08:25:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17mdzv",
              "author": "Hungry-Amount-2730",
              "text": "thank you very much for valuable info :)",
              "score": 2,
              "created_utc": "2026-01-23 09:44:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17r95x",
          "author": "monikaTechCuriosity",
          "text": "What about Knowledge Graph?",
          "score": 2,
          "created_utc": "2026-01-23 10:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19oxwd",
              "author": "prodigy_ai",
              "text": "Agree! graph could be a valuable solution",
              "score": 1,
              "created_utc": "2026-01-23 17:03:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1b58jv",
          "author": "patbhakta",
          "text": "RLM + Graph",
          "score": 2,
          "created_utc": "2026-01-23 21:04:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mwdka",
              "author": "Jords13xx",
              "text": "RLM with graph structures sounds intriguing! Would love to see a deep dive on how those two can enhance retrieval accuracy and efficiency. What specific use cases are you thinking about?",
              "score": 1,
              "created_utc": "2026-01-25 15:52:51",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1b3z6o",
          "author": "drfritz2",
          "text": "Multimodal, hybrid (SQL), parsing, metadata and open source",
          "score": 1,
          "created_utc": "2026-01-23 20:58:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1p9kdy",
              "author": "maigpy",
              "text": "hybrid (SQL) what does that mean",
              "score": 1,
              "created_utc": "2026-01-25 21:59:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1xac8x",
                  "author": "drfritz2",
                  "text": "It means that its possible to search SQL and Embeddings \n\nLets say you have tons of documents with the same \"subject\", you can have SQL metadata and then \"filter\" by other criteria do find what you are looking for.",
                  "score": 1,
                  "created_utc": "2026-01-27 00:18:03",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1gwf28",
          "author": "ajay-c",
          "text": "I need to know more about rag",
          "score": 1,
          "created_utc": "2026-01-24 18:23:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmoxj2",
      "title": "Looking for RAG Engineer / AI Partner ‚Äî Real Estate + SMB Automation (Paid Contract, Long-Term Potential)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "author": "TheGloomWalker",
      "created_utc": "2026-01-25 17:12:28",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Hey everyone, I‚Äôm building a small AI services company focused on deploying custom RAG-based systems and internal AI tools for small and mid-sized businesses (starting with real estate automation and an industrial services company).\n\nI currently have infrastructure running (servers, cloud resources, deployment environment) and guaranteed business interest, but I‚Äôm looking to bring on a technical partner or contractor who can help design and implement production-grade RAG pipelines.\n\nWhat I‚Äôm building initially:\n\nReal estate automation use cases:\n\n\t‚Ä¢\tScraping + ingesting foreclosure / distressed property listings\n\n\t‚Ä¢\tStructured document ingestion (county data, CSVs, PDFs)\n\n\t‚Ä¢\tSearch + semantic querying over listings and owner data\n\n\t‚Ä¢\tEmail / outreach workflow integration (CRM-style pipelines)\n\nEnterprise pilot project (industrial company):\n\n\t‚Ä¢\tInternal document RAG (finance, operations, SOPs, contracts)\n\n\t‚Ä¢\tSecure knowledge base assistant for staff\n\n\t‚Ä¢\tRole-based access control\n\n\t‚Ä¢\tData isolation + security-first architecture\n\nThis company i‚Äôm working initial contract with is real (50m valuation), has active operations, and is willing to deploy AI internally across departments. Their IT team is security-focused, so experience with data isolation, permissioning, private vector DBs, and secure API practices is important.\n\nWhat I‚Äôm looking for:\n\nSomeone with experience in:\n\n\t‚Ä¢\tRAG pipelines (LangChain, LlamaIndex, custom pipelines, etc.)\n\n\t‚Ä¢\tVector DBs (Pinecone, Weaviate, Qdrant, FAISS, Chroma)\n\n\t‚Ä¢\tEmbeddings + chunking strategies\n\n\t‚Ä¢\tAPI integration\n\n\t‚Ä¢\tAuth / security best practices\n\n\t‚Ä¢\tCloud deployment (Docker, VPS, AWS/GCP/Hetzner/etc.)\n\n\t‚Ä¢\tBonus: web scraping + ETL pipelines\n\nCompensation:\n\n\t‚Ä¢\tPaid contract work (budget available)\n\n\t‚Ä¢\tOpportunity for ongoing partnership if things go well\n\n\t‚Ä¢\tOpen to milestone-based payments\n\nWhat I‚Äôd like to see from you:\n\n\t‚Ä¢\tBrief background / experience\n\n\t‚Ä¢\tAny demos, repos, or projects you‚Äôve built\n\n\t‚Ä¢\tWhat stack you prefer working with\n\n\t‚Ä¢\tAvailability\n\nIf you‚Äôre interested, DM me or reply here and we can talk details.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qmoxj2/looking_for_rag_engineer_ai_partner_real_estate/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1nnecp",
          "author": "DeadPukka",
          "text": "Have a look at [Graphlit](https://www.graphlit.com). We do this as a platform and can help with custom services. \n\nNo need to build this yourself in 2026.",
          "score": 2,
          "created_utc": "2026-01-25 17:49:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqnhe",
          "author": "ampancha",
          "text": "The enterprise pilot is where this gets interesting. Role-based access control in RAG isn't a UI toggle; it has to happen at retrieval time, or users can still surface documents they shouldn't see through indirect queries. Their IT team will ask how you verify that isolation actually holds under adversarial prompts. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-01-25 18:02:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1odwl7",
          "author": "pk13055",
          "text": "Sounds interesting [pk13055.com](https://pk13055.com)",
          "score": 1,
          "created_utc": "2026-01-25 19:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1qwvna",
          "author": "radicalpeaceandlove",
          "text": "I am an AI/ML engineer leaving Optum, would be open to chatting",
          "score": 1,
          "created_utc": "2026-01-26 02:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1rnp1d",
          "author": "Fear_ltself",
          "text": "I‚Äôd love to give it a shot, I‚Äôve been working exactly with these programs for over 2 years and think we could get something up and running with what you‚Äôre looking for very quickly.",
          "score": 1,
          "created_utc": "2026-01-26 05:26:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nqkah",
          "author": "BallinwithPaint",
          "text": "Hey, saw your post. This is right up my alley. I specialize in building the exact kind of autonomous RAG and automation pipelines you're describing. My experience includes projects involving live web scraping for data ingestion and building secure, production-grade agentic systems.\n\n\n\nSending you a DM with my portfolio and more details now.",
          "score": 0,
          "created_utc": "2026-01-25 18:02:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qk5ctc",
      "title": "Building RAG systems: the hard parts aren‚Äôt the models",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qk5ctc/building_rag_systems_the_hard_parts_arent_the/",
      "author": "nuvintaillc",
      "created_utc": "2026-01-22 20:12:04",
      "score": 9,
      "num_comments": 9,
      "upvote_ratio": 1.0,
      "text": "Working on RAG systems recently.\n\nThe biggest challenges haven‚Äôt been the models ‚Äî they‚Äôve been:\n\n* structuring data for retrieval\n* managing context windows\n* handling latency and evaluation\n\nFeels like a good example of where AI is moving: away from demos and toward real systems engineering.\n\nCurious how others are handling RAG evaluation in production.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qk5ctc/building_rag_systems_the_hard_parts_arent_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o19txbg",
          "author": "Popular_Sand2773",
          "text": "I think the big one that get's overlooked is what I call the CEO test. \n\nWe had spent a year building this application etc etc. Finally launch it and CEOs Son tries to use it and hits an error. Fast forward to everyone being called on in on the weekend to solve this one guy's problem. After that the whole project lost momentum. It worked fine for 99% of people but that didn't matter because the wrong executive had the wrong experience.\n\nAll that to say reliability and consistency. You've got to track and manage drift.",
          "score": 4,
          "created_utc": "2026-01-23 17:26:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ah0gf",
          "author": "ggone20",
          "text": "Go deep. I was literally just talking about this in another thread. Orchstrators orchestrating orchestrators before any work gets done. Adds slight latency (Cerebras is nice if you can use it), but managing context windows is the name of the game. \n\nOf course that‚Äôs after data ingestion pipes. I recommend doing it multiple ways. For PDFs, for example, we break it into 3 page chunks, have an LLM do semantic chunking of the extracted text, we turn those same 3 page chunks into images and have a vLM create semantic chunks, a few other tricks‚Ä¶ then we use a small model to coalesce them (same with PowerPoints, word docs, architectural/engineering drawings), and then vectorize those semantic chunks after it‚Äôs been been viewed a few different ways along with a set of questions that might be asked of those chunks based on the industry/business/workflows you have‚Ä¶ then graph them‚Ä¶ we also use KBLaM and a few other custom models with dynamic knowledge base loading based on the query (the KBs are built dynamically also)\n\nThis is gold. Complicated gold, but gold nonetheless. Write it down ü§£üôÉ. Otherwise it‚Äôs magic. Works better than anything else we‚Äôve tested. I‚Äôm leaving a lot out but there‚Äôs a lot there also.\n\nDiffMem and MemVid are interesting. RAG at scale is one of the few things in life where more complicated is better. Sigh‚Ä¶ lol so hard.",
          "score": 2,
          "created_utc": "2026-01-23 19:10:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1g9p9h",
              "author": "JustSentYourMomHome",
              "text": "I'm trying to build a RAG that ingests ASME code books to help me look things up. This sounds like what I need to be doing.",
              "score": 2,
              "created_utc": "2026-01-24 16:43:58",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1h33ne",
                  "author": "ggone20",
                  "text": "Oh yea for advanced reasoning over large artifacts like standards and regulatory principles, definitely need multiple paths. I did some extensive work in hydrogen and had to create a field ‚Äòhydrogen expert‚Äô chatbot for construction and engineering teams on the ground to reference things like NFPA2 and others. \n\nKnowledge graphs, at minimum, are needed for multi-hop reasoning. Most standards have tons of cross referencing also. Find a few items, hop around to references, you can get good answers if you handle ingestion correctly.",
                  "score": 2,
                  "created_utc": "2026-01-24 18:51:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1ug90f",
              "author": "nuvintaillc",
              "text": "\\+1 to the idea that RAG complexity lives outside the model.\n\nIn practice, what‚Äôs worked for us is treating RAG as a *systems problem*, not a retrieval problem:\n\n‚Ä¢ **Multi-path ingestion** (semantic chunks + structural units like sections, tables, references)  \n‚Ä¢ **Query-time KB assembly** instead of static context windows  \n‚Ä¢ **Evaluation tied to user workflows**, not just answer correctness (the ‚ÄúCEO test‚Äù resonates hard)\n\nFor large artifacts (standards, policies, email threads), we‚Äôve found that **reference-aware chunking + lightweight graph traversal** beats naive vector search every time.\n\nThe interesting shift for me: RAG quality improves more from **better data topology and runtime orchestration** than from swapping models. Models are becoming commodities; *context engineering* isn‚Äôt.\n\nCurious if others are measuring success with task completion / decision confidence rather than traditional IR metrics?",
              "score": 2,
              "created_utc": "2026-01-26 16:39:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v9l2z",
                  "author": "ggone20",
                  "text": "You got it!",
                  "score": 1,
                  "created_utc": "2026-01-26 18:44:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1t0ztf",
          "author": "EnoughNinja",
          "text": "The data structuring piece is an issue, especially when you're dealing with email threads where the actual context is scattered.\n\nWe ended up building an API that just handles the email context assembly part (thread reconstruction, role detection, that whole mess). Turns out \\~200ms retrieval with proper citations is possible when you're not fighting with generic RAG pipelines.",
          "score": 2,
          "created_utc": "2026-01-26 12:18:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlmttf",
      "title": "built a Local RAG System That Works Without API Keys - Is This Actually Useful?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qlmttf/built_a_local_rag_system_that_works_without_api/",
      "author": "DetectiveMindless652",
      "created_utc": "2026-01-24 13:21:25",
      "score": 8,
      "num_comments": 14,
      "upvote_ratio": 0.79,
      "text": "Spent a couple days putting together¬†this local RAG SDK¬†thing. Basically lets you do document search and context retrieval entirely offline, no cloud APIs required by default.\n\nIt¬†uses local embeddings so you don't have to pay OpenAI or Cohere, and it's built on this fast storage engine that does O(1) lookups. Performance seems decent - about 5x faster¬†than what I was getting with cloud alternatives when I tested it.\n\nCompared to cloud services like Pinecone that charge $70-200/month, this is completely¬†free. Plus¬†you¬†get better¬†speed since¬†everything runs locally¬†and your data never¬†leaves¬†your machine. + no cloud cost \n\nThe main issue I was¬†trying to solve is that¬†if you want privacy with your RAG setup, you usually end up with something¬†slow or complicated. This keeps everything local¬†but still performs pretty well.\n\nWould people¬†actually use something like this? I feel¬†like most RAG tutorials just assume¬†you'll use Pinecone + OpenAI, but¬†maybe there's demand for a local option¬†that doesn't suck.\n\nDownsides: local¬†embeddings aren't as good as the latest¬†OpenAI models, and it depends¬†on your hardware. But no monthly bills and your data¬†stays private.\n\nWhat do you think¬†- overkill or actually useful? Have you¬†run into the privacy vs¬†performance trade-off with RAG?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qlmttf/built_a_local_rag_system_that_works_without_api/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1fzyqo",
          "author": "hrishikamath",
          "text": "I use Postgres with pg vector, it takes milli seconds for retrieval over 100k documents. I spend like a handful dollars in a deployed version. It‚Äôs simple. You don‚Äôt need pinecone unless you have scale.",
          "score": 5,
          "created_utc": "2026-01-24 15:59:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1k6mjx",
          "author": "voycey",
          "text": "Anyone can do it, it's literally how most RAG systems work, what you have to do is differentiate, having basic retrieval gives you no edge, vector search has been around much longer than the current AI hype, it's not a new concept.\n\nHell all the hyperscalers offer this out of the box, why would someone choose to use your system over that? \n\nI think the future is in private AI solutions but they have to be different",
          "score": 3,
          "created_utc": "2026-01-25 04:21:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1kdnob",
              "author": "No-Consequence-1779",
              "text": "Can you list some downloadable consumer products please?¬†",
              "score": 1,
              "created_utc": "2026-01-25 05:05:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1sf451",
              "author": "DetectiveMindless652",
              "text": "i see what you are saying, but in theory what we have built is different because it is not vector based, predictable performance, memory efficient works beyond ram, local, and acid,wal crash recovery etc.\n\nWould be cool to get your thoughts on that",
              "score": 0,
              "created_utc": "2026-01-26 09:11:56",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1zg74n",
                  "author": "voycey",
                  "text": "Any RAG system that doesnt in some way use Vectors or provide similar context so that it can be fetched by the LLM isnt going to perform well.  \n  \nTake Langextract by Google as an example, it essentially does away with vectors but instead you have to provide an example for NER, it does a great job of locating answers in the corpus where what you are searching for matches whats in the document but ultimately you are just pushing the pipeline to a different part, in production this puts much more strain on the query segment of Retrieval vs the pre-processing that RAG does.\n\nI could go into a bunch of different reasons why LangExtract isnt a good solution by itself for enterprise retrieval - but I will also say its an incredible product that when used in tandem with other methods can significantly increase accuracy and / or speed.\n\nhttps://preview.redd.it/des5fhkmuufg1.png?width=2484&format=png&auto=webp&s=92afd14d45037ea81ee8f6e68b43fc495dd4aa85\n\nOverall I think it's a good idea to go ahead and build - it sounds like you are focusing on robustness too (incredibly important as most of the options out there are incredibly brittle) but you really must get some production data. The \"non-vector\" retrieval options (Langextract, PageIndex etc), right now frequently fail on production data (complex tables are a good example of something thats still a very hard problem and something I have spent a non-trivial amount of time solving for my system).",
                  "score": 1,
                  "created_utc": "2026-01-27 08:49:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1fma0m",
          "author": "Pale_Reputation_511",
          "text": "I've been working on this and refining it for the last two months. I built it from scratch, without a framework or anything. I want it to be simple and without external dependencies, if possible, just what's necessary for the job. It's a tailor-made solution for the things I do; the process has been difficult, but I've learned a lot.",
          "score": 2,
          "created_utc": "2026-01-24 14:52:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1h1sv7",
              "author": "DetectiveMindless652",
              "text": "We‚Äôve built this bro production ready dm me",
              "score": -2,
              "created_utc": "2026-01-24 18:46:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1j7jsl",
          "author": "burntoutdev8291",
          "text": "Jina and bge is usually good enough. We use this for airgapped environments, so everything is local.",
          "score": 2,
          "created_utc": "2026-01-25 01:02:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1yvfft",
          "author": "SkyFeistyLlama8",
          "text": "Don't reinvent the wheel. I've made local RAG setups using local embedding models and local LLMs, Postgres with pgvector or a big CSV file for vector and keyword search. Vector search isn't the bottleneck! You're limited by the speed of creating and maintaining vector indexes (it can take days to index a million chunks) and by local LLM inference.",
          "score": 1,
          "created_utc": "2026-01-27 05:52:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1jk5no",
          "author": "wurzelbrunft",
          "text": "It's certainly useful if you don't want to upload private data to an AI. I'm currently developing such a concept myself.",
          "score": 0,
          "created_utc": "2026-01-25 02:11:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1sf51u",
              "author": "DetectiveMindless652",
              "text": "would be cool to hear about it.",
              "score": 2,
              "created_utc": "2026-01-26 09:12:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qo9u4g",
      "title": "I'm looking for an OCR for my RAG.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "author": "AdministrationPure45",
      "created_utc": "2026-01-27 10:04:44",
      "score": 8,
      "num_comments": 27,
      "upvote_ratio": 0.91,
      "text": "Which one do you think is the best among:   \nMistral OCR, LightOnOCR-2, Chandra, OlmOCR 2, Dolphin-v2, LlamaParse, Reducto, Qwen2.5-VL-8B, or DeepSeekOCR?\n\nWhich one do you use? Thanks ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qo9u4g/im_looking_for_an_ocr_for_my_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2009jp",
          "author": "CapitalShake3085",
          "text": "Qwen3 vl",
          "score": 4,
          "created_utc": "2026-01-27 11:46:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o201f6p",
          "author": "roydotai",
          "text": "DeepSeekOCR v2 came out recently. I haven‚Äôt gotten around to test it myself yet, but it looked interesting",
          "score": 4,
          "created_utc": "2026-01-27 11:54:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr0q1",
          "author": "zzriyansh",
          "text": "use pymupdf4llm + layout + pro",
          "score": 3,
          "created_utc": "2026-01-27 10:29:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zzn59",
          "author": "hashiromer",
          "text": "Go with LlamaParse or Reducto if budget allows. I benchmark various PDF parsing solutions for work and LlamaParse and Reducto are far ahead of everything else. However, i only tested at highest compute so ymmv.\n\nYou can also directly use Gemini 3 Flash to convert PDFs to markdown directly with a prompt but you will need to write some basic logic like splitting pages and convert each separately. Gemini on its own beats specialized pipeline based OCRs solutions easily on my internal benchmarks.\n\nIt also depends on complexity of layout by the way, if you are dealing with simple layouts, simpler pipeline based tools like docling,minerU,marker could also work very well.",
          "score": 3,
          "created_utc": "2026-01-27 11:41:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o20aoiw",
              "author": "nofuture09",
              "text": "why reducto? isnt llamaparse better and cheaper?",
              "score": 1,
              "created_utc": "2026-01-27 12:58:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o20r6yd",
                  "author": "hashiromer",
                  "text": "Reducto was cheaper last i tested and faster but LlamaParse was slightly more accurate.",
                  "score": 1,
                  "created_utc": "2026-01-27 14:26:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1zt402",
          "author": "Live-Guitar-8661",
          "text": "We use Llama-4-17b with Poppler, probably not the popular choice out there but works for us. Give it a shot if you want to see how well it works: https://orchata.ai\n\nPS- not giving you the link as a plug, just saying if you want to see how they work, it‚Äôs free and you can get a sense of the output in the dashboard. Hope that helps.",
          "score": 2,
          "created_utc": "2026-01-27 10:47:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o202dir",
          "author": "SiebenZwerg",
          "text": "We use Mistral OCR due to high accuracy.",
          "score": 2,
          "created_utc": "2026-01-27 12:01:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20gus1",
          "author": "AloneSYD",
          "text": "I like a very underrated OCR model nanonets/Nanonets-OCR2-3B FP8 quantized",
          "score": 2,
          "created_utc": "2026-01-27 13:33:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20zbll",
          "author": "it_and_webdev",
          "text": "Docling",
          "score": 2,
          "created_utc": "2026-01-27 15:06:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o269e9z",
          "author": "sirebral",
          "text": "Smallest Qwen 3 VL variant is wonderful at this task, even with a four bit quant.",
          "score": 2,
          "created_utc": "2026-01-28 07:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zr1pz",
          "author": "fabkosta",
          "text": "‚ÄúBest‚Äù depends on your data. You need to do a PoC to find out. Also look at Docling, it‚Äôs free.",
          "score": 1,
          "created_utc": "2026-01-27 10:29:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zu2u4",
          "author": "instantlybanned",
          "text": "Depends a little on what you need it for (document ocr, read text in images etc.) but for just general purpose OCR on images, Paddle's PP-OCRv5 is probably the state-of-the-art model out there at the moment.",
          "score": 1,
          "created_utc": "2026-01-27 10:55:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o200kzs",
          "author": "teroknor92",
          "text": "ParseExtract works well for complex documents with tables, handwriting etc. The pricing is very friendly with good ocr and data extraction accuracy.",
          "score": 1,
          "created_utc": "2026-01-27 11:48:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o21fnkx",
          "author": "Rich-Emu-1561",
          "text": "An OCR API could help that. I have been using qoest for developers platform for similar document processing. You can check their site to see if it fit your setup.",
          "score": 1,
          "created_utc": "2026-01-27 16:19:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o224zfc",
          "author": "maniac_runner",
          "text": "llmwhisperer if you want to parse complex tables in documents",
          "score": 1,
          "created_utc": "2026-01-27 18:09:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o228kmn",
          "author": "zmanning",
          "text": "[https://99franklin.github.io/ocrbench\\_v2/](https://99franklin.github.io/ocrbench_v2/)",
          "score": 1,
          "created_utc": "2026-01-27 18:24:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o23qx01",
          "author": "No_Thing8294",
          "text": "What do you want to achieve? Qwen-VL is totally different to Deepseek-OCR. Totally different approaches.",
          "score": 1,
          "created_utc": "2026-01-27 22:25:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23r5r4",
              "author": "No_Thing8294",
              "text": "‚Ä¶and it depends on the kind of input data. Text? Tables, complex PDFs etc.?",
              "score": 1,
              "created_utc": "2026-01-27 22:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o25bgby",
          "author": "FrozenBuffalo25",
          "text": "What‚Äôs your hardware?",
          "score": 1,
          "created_utc": "2026-01-28 03:16:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o264vwh",
          "author": "DressMetal",
          "text": "Do you need a local model? Otherwise, Gemini 2.5 flash lite is great at this and it costs next to nothing.",
          "score": 1,
          "created_utc": "2026-01-28 06:29:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27jawc",
          "author": "Ch3mCat",
          "text": "Colqwen 2.5 ?",
          "score": 1,
          "created_utc": "2026-01-28 13:18:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o27vumt",
          "author": "Independent-Cost-971",
          "text": "Try using Kudra AI it is very good at complex doc extraction you can even pick what you want to extract",
          "score": 1,
          "created_utc": "2026-01-28 14:24:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2bsbnr",
          "author": "nnamfuak",
          "text": "Mistral OCR 3 (mistral-ocr-2512)",
          "score": 1,
          "created_utc": "2026-01-29 01:08:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2295sc",
          "author": "Fresh_Refuse_4987",
          "text": "You can check Reseek. It's an AI tool that automatically extract text from images and PDFs for your RAG pipeline, and uses semantic search so you can find your information easily.",
          "score": 0,
          "created_utc": "2026-01-27 18:26:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qoyyjm",
      "title": "We built a knowledge graph from code using AST extractors. Now we're drowning in edge cases. Roast our approach.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "author": "TraditionalDegree333",
      "created_utc": "2026-01-28 02:39:08",
      "score": 8,
      "num_comments": 5,
      "upvote_ratio": 0.84,
      "text": "I'm building a code intelligence platform that answers questions like¬†*\"who owns this service?\"*¬†and¬†*\"what breaks if I change this event format?\"*¬†across 30+ repos.\n\nOur approach: Parse code with tree-sitter AST ‚Üí Extract nodes and relationships ‚Üí Populate Neo4j knowledge graph ‚Üí Query with natural language.\n\nHow It Works:\n\n    Code File\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ tree-sitter AST parse\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Extractors (per file type):\n        ‚îÇ   ‚îú‚îÄ‚îÄ CodeNodeExtractor     ‚Üí File, Class, Function nodes\n        ‚îÇ   ‚îú‚îÄ‚îÄ CommitNodeExtractor   ‚Üí Commit, Person nodes + TOUCHED relationships  \n        ‚îÇ   ‚îú‚îÄ‚îÄ DiExtractor           ‚Üí Spring  ‚Üí INJECTS relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ MessageBrokerExtractor‚Üí Kafka listeners ‚Üí CONSUMES_FROM relationships\n        ‚îÇ   ‚îú‚îÄ‚îÄ HttpClientExtractor   ‚Üí RestTemplate calls ‚Üí CALLS_SERVICE\n        ‚îÇ   ‚îî‚îÄ‚îÄ ... 15+ more extractors\n        ‚îÇ\n        ‚îú‚îÄ‚îÄ Enrichers (add context):\n        ‚îÇ   ‚îú‚îÄ‚îÄ JavaSemanticEnricher  ‚Üí Classify: Service? Controller? Repository?\n        ‚îÇ   ‚îî‚îÄ‚îÄ ConfigPropertyEnricher‚Üí Link (\"${prop}\") to config files\n        ‚îÇ\n        ‚îî‚îÄ‚îÄ Neo4j batch write (MERGE nodes + relationships)\n\n**The graph we build:**\n\n    (:Person)-[:TOUCHED]->(:Commit)-[:TOUCHED]->(:File)\n    (:File)-[:CONTAINS_CLASS]->(:Class)-[:HAS_METHOD]->(:Function)\n    (:Class)-[:INJECTS]->(:Class)\n    (:Class)-[:PUBLISHES_TO]->(:EventChannel)\n    (:Class)-[:CONSUMES_FROM]->(:EventChannel)\n    (:ConfigFile)-[:DEFINES_PROPERTY]->(:ConfigProperty)\n    (:File)-[:USES_PROPERTY]->(:ConfigProperty)\n\n  \n**The problem we're hitting:**\n\nEvery new framework or pattern = new extractor.\n\n* Customer uses Feign clients? Write FeignExtractor.\n* Uses AWS SQS instead of Kafka? Write SqsExtractor.\n* Uses custom DI framework? Write another extractor.\n* Spring Boot 2 vs 3 annotations differ? Handle both.\n\nWe have 40+ node types and 60+ relationship types now. Each extractor is imperative pattern-matching on AST nodes. It works, but:\n\n1. Maintenance nightmare¬†- Every framework version bump can break extractors\n2. Doesn't generalize¬†- Works for our POC customer, but what about the next customer with different stack?\n3. No semantic understanding¬†- We can extract¬†\\`@KafkaListener\\`but can't answer¬†\"what's our messaging strategy?\"\n\n**Questions:**\n\n1. Anyone built something similar and found a better abstraction?\n2. How do you handle cross-repo relationships? (Config in repo A, code in repo B, deployment values in repo C)\n\n\n\n  \nHappy to share more details or jump on a call. DMs open.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qoyyjm/we_built_a_knowledge_graph_from_code_using_ast/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o26gra0",
          "author": "Sure_Host_4255",
          "text": "That's what I was exactly was thinking about for spring projects, because mostly you don't need general knowledge, but framework specific. Maybe another step would be to create detailed skill or rule for agent how to use this graph, because for LLM it is just graph, it doesn't know what to do with it and still reads the files to context for it's tasks.\nAs for me even better results was when I wrote spring documentation MCP, then even weak models become stronger.\n\nAnother though, you don't need to support All frameworks, java specific would be enough for commercial product, just choose this niche and keep going, enterprise clients will pay for it.\n\nFor patterns you need to ask LLM to write breaf reviews for graph chains, it could cost 20-100$, depending on repo and model. Also write custom wights algorithms, but it's rather doubtful, because it could work great for 1 project and can harm for another.\nI was participating in similar project for Cobol, and can say you are in the right direction and have similar problems ‚ò∫Ô∏è",
          "score": 1,
          "created_utc": "2026-01-28 08:09:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o26inxl",
              "author": "Sure_Host_4255",
              "text": "So for conclusion:\n1. Write spring docs MCP, it can fetch documentation from GitHub md docs for each version upgrade \n2. Focus just on java frameworks\n3. Ask LLM to create summary for nodes relationship chain",
              "score": 1,
              "created_utc": "2026-01-28 08:26:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o26lewq",
          "author": "TraditionalDegree333",
          "text": "Thank you for the advice, I will explore",
          "score": 1,
          "created_utc": "2026-01-28 08:52:06",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o28aaig",
          "author": "devopstoday",
          "text": "Sounds good approach. Is your project opensource?",
          "score": 1,
          "created_utc": "2026-01-28 15:33:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o28fvkv",
              "author": "TraditionalDegree333",
              "text": "No, it‚Äôs not",
              "score": 1,
              "created_utc": "2026-01-28 15:57:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qomak1",
      "title": "Chunking strategies for contracts",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "author": "cat47b",
      "created_utc": "2026-01-27 18:34:39",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all, after seeing a few posts I'm curious as to if anyone has a tried and true favourite method for chunking.\n\nIf I set the scene as we're making an agentic RAG for contracts, ingesting with OCR, carrying out hybrid search (vector+BM25) and a re-ranker. My main concern is the ingestion/chunking process (garbage in, garbage out).\n\nLooking at OCR results I can see that a paragraph may have 1 line that lands on the next page, or that a bullet list goes across 2 pages or that structurally few headings are used.\n\nFor no good reason semantic chunking has stood out as something that makes logical sense as I just want chunks that in isolation make sense, and hopefully side-step the need for overlap etc. \n\nOne downside to this I can imagine however is that a chunk can be semantically complete, but could otherwise refer to an exception clause. Without both you could end up with poor context to feed an LLM when answering a user query.\n\n  \nSo back to the original question, for the stated use case does anyone have a pattern they would use here?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qomak1/chunking_strategies_for_contracts/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o234f9p",
          "author": "ManufacturerIll6406",
          "text": "I just published an article today on this subject, also included lightweight framework for evaluating various strategies with statistical rigor\n\n[https://theprincipledengineer.substack.com/i/184904124/methodology-and-code](https://theprincipledengineer.substack.com/i/184904124/methodology-and-code)",
          "score": 5,
          "created_utc": "2026-01-27 20:44:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o23j8hk",
              "author": "cat47b",
              "text": "Your post was excellent and I did read it, would I be right in saying if you had to pick one it'd be sentence based chunking?\n\nIn a sense I don't really care about chunk size as accuracy is more important to me",
              "score": 1,
              "created_utc": "2026-01-27 21:50:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o23lt8e",
                  "author": "ManufacturerIll6406",
                  "text": "Thanks! And yes. If you just want max recall and don't care about chunk size, sentence chunking is a safe default. It naturally produces larger chunks, which is exactly why it wins.\n\nThe caveat is cost and latency. Sentence chunking at 1024 config gives you \\~3500 char chunks. At top-k=5, that's 17,500 chars in your LLM context before you've added the prompt. If that's fine for your use case, go for it.\n\nTL;DR: Sentence chunking works. Now you know \\*why\\* it works.",
                  "score": 1,
                  "created_utc": "2026-01-27 22:01:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o292824",
          "author": "ampancha",
          "text": "Chunking matters, but with an agentic RAG the higher-order risk is what happens when the agent acts on a hallucinated clause or when contract text contains embedded instructions that hijack the workflow. Contracts are untrusted input at scale. Before optimizing retrieval, I'd map the action boundary: what can this agent actually do, and what stops it from doing something wrong? Sent you a DM.",
          "score": 1,
          "created_utc": "2026-01-28 17:34:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}