{
  "metadata": {
    "last_updated": "2026-01-24 16:49:57",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 124,
    "file_size_bytes": 154880
  },
  "items": [
    {
      "id": "1qhzs5i",
      "title": "DeepResearch is finally localized! The 8B on-device writing agent AgentCPM-Report is now open-sourced!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhzs5i/deepresearch_is_finally_localized_the_8b_ondevice/",
      "author": "Relevant_Abroad_6614",
      "created_utc": "2026-01-20 12:27:26",
      "score": 47,
      "num_comments": 6,
      "upvote_ratio": 0.98,
      "text": "In an era where **Deep Research** is surging, we all long for a ‚Äúsuper writing assistant‚Äù capable of automatically producing **tens of thousands of words**.\n\nBut‚Äîwhen you‚Äôre holding **corporate strategic plans, unpublished financial reports, or core research data**, would you really dare to upload them to the cloud ‚òÅÔ∏è?\n\nToday, we bring a **game-changing solution**: **AgentCPM-Report** ‚Äî a **localized, private, yet top-tier** deep research agent.\n\nJointly developed by **Tsinghua University NLP Lab**, **Renmin University of China**, **ModelBest**, and the **OpenBMB open-source community**, it is now **open-sourced** on **GitHub, Hugging Face**, and more.\n\n**What does this mean?**\n\nNo expensive compute. No data uploads.\n\nYou can run an **expert-level research assistant entirely on your local machine** üîß\n\nüîç **Why choose AgentCPM-Report?**\n\n‚úÖ **Extreme efficiency ‚Äî doing more with less**\n\nWith only **8B parameters**, it achieves **40+ rounds of deep retrieval** and **nearly 100 steps of chain-of-thought reasoning**, generating logically rigorous, insight-rich **long-form reports** comparable to top closed-source systems.\n\n‚≠êÔ∏è DeepResearch Bench\n\n|Model|Overall|Comprehensiveness|Insight|Instruction Following|Readability|\n|:-|:-|:-|:-|:-|:-|\n|Doubao-research|44.34|44.84|40.56|47.95|44.69|\n|Claude-research|45.00|45.34|42.79|47.58|44.66|\n|OpenAI-deepresearch|46.45|46.46|43.73|49.39|47.22|\n|Gemini-2.5-Pro-deepresearch|49.71|49.51|49.45|50.12|50.00|\n|WebWeaver (Qwen3-30B-A3B)|46.77|45.15|45.78|49.21|47.34|\n|WebWeaver (Claude-Sonnet-4)|50.58|51.45|50.02|50.81|49.79|\n|Enterprise-DR (Gemini-2.5-Pro)|49.86|49.01|50.28|50.03|49.98|\n|RhinoInsigh (Gemini-2.5-Pro)|50.92|50.51|51.45|51.72|50.00|\n|**AgentCPM-Report**|**50.11**|**50.54**|**52.64**|**48.87**|**44.17**|\n\n‚≠êÔ∏è DeepResearch Gym\n\n|Model|Avg.|Clarity|Depth|Balance|Breadth|Support|Insightfulness|\n|:-|:-|:-|:-|:-|:-|:-|:-|\n|Doubao-research|84.46|68.85|93.12|83.96|93.33|84.38|83.12|\n|Claude-research|80.25|86.67|96.88|84.41|96.56|26.77|90.22|\n|OpenAI-deepresearch|91.27|84.90|98.10|89.80|97.40|88.40|89.00|\n|Gemini-2.5-pro-deepresearch|96.02|90.71|99.90|93.37|99.69|95.00|97.45|\n|WebWeaver (Qwen3-30b-a3b)|77.27|71.88|85.51|75.80|84.78|63.77|81.88|\n|WebWeaver (Claude-sonnet-4)|96.77|90.50|99.87|94.30|100.00|98.73|97.22|\n|**AgentCPM-Report**|**98.48**|**95.10**|**100.00**|**98.50**|**100.00**|**97.30**|**100.00**|\n\n‚≠êÔ∏è DeepConsult\n\n|Model|Avg.|Win|Tie|Lose|\n|:-|:-|:-|:-|:-|\n|Doubao-research|5.42|29.95|40.35|29.70|\n|Claude-research|4.60|25.00|38.89|36.11|\n|OpenAI-deepresearch|5.00|0.00|100.00|0.00|\n|Gemini-2.5-Pro-deepresearch|6.70|61.27|31.13|7.60|\n|WebWeaver (Qwen3-30B-A3B)|4.57|28.65|34.90|36.46|\n|WebWeaver (Claude-Sonnet-4)|6.96|66.86|10.47|22.67|\n|Enterprise-DR (Gemini-2.5-Pro)|6.82|71.57|19.12|9.31|\n|RhinoInsigh (Gemini-2.5-Pro)|6.82|68.51|11.02|20.47|\n|**AgentCPM-Report**|**6.60**|**57.60**|**13.73**|**28.68**|\n\n‚úÖ **Physical isolation, true local security**\n\nDesigned for **high-privacy scenarios**, it supports **fully offline deployment**, eliminating cloud data leakage risks.\n\nYou can mount **local knowledge bases**, ensuring sensitive data **never leaves your domain** while still producing professional-grade reports.\n\nüòé **Try it now: put DeepResearch on your hard drive**\n\n**AgentCPM-Report** is now available on **GitHub | Hugging Face | ModelScope | GitCode | Modelers**, and we warmly invite developers to try it out and co-build the ecosystem!\n\n**GitHubÔºöüîó**¬†[https://github.com/OpenBMB/AgentCPM](https://github.com/OpenBMB/AgentCPM)\n\n**HuggingFaceÔºö üîó**¬†[https://huggingface.co/openbmb/AgentCPM-Report](https://huggingface.co/openbmb/AgentCPM-Report)\n\nIf you find our work helpful, please consider giving us a ‚≠ê **Star** & üíñ **Like**\\~",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qhzs5i/deepresearch_is_finally_localized_the_8b_ondevice/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0o2fw3",
          "author": "New_Wear4177",
          "text": "Fantastic Work! Put the DeepResearch in edges.",
          "score": 3,
          "created_utc": "2026-01-20 14:05:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0r1vb7",
          "author": "Unique-Temperature17",
          "text": "This is really cool stuff, thanks for sharing! The privacy-first approach with full offline deployment is exactly what's needed for handling sensitive docs. Will definitely check out the GitHub repo soon. Would love to see this integrated into apps - having an 8B model that punches above its weight like this would be a game-changer for local document workflows.",
          "score": 2,
          "created_utc": "2026-01-20 22:30:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13rtvh",
          "author": "New_Animator_7710",
          "text": "Impressive results for 8B, no doubt‚Äîbut I think the more interesting question is *how brittle this is outside the benchmark*. DeepResearch-style agents often look amazing on structured evals, but the real test is messy, underspecified prompts and evolving goals over long sessions.",
          "score": 2,
          "created_utc": "2026-01-22 19:41:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o17j2yf",
              "author": "Relevant_Abroad_6614",
              "text": "You‚Äôre absolutely right ‚Äî we also place a strong emphasis on user experience.\n\nDeepResearch is relatively easier in this regard, because its evaluation is done via LLM-as-a-Judge on long-form text, so it‚Äôs not as easy to hack.\n\nYou can also give this model a try. From what we‚Äôve seen in the open-source community, some people have used it for web-agent‚Äìlike tasks with pretty solid results ‚Äî which honestly surprised us as well.\n\nThanks again for your interest and support!",
              "score": 1,
              "created_utc": "2026-01-23 09:13:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o19c4e0",
                  "author": "New_Animator_7710",
                  "text": "Sure",
                  "score": 1,
                  "created_utc": "2026-01-23 16:05:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e33n4",
          "author": "BarrenLandslide",
          "text": "Very nice. I am going to test it next week and share my results. Thank you for providing this.",
          "score": 2,
          "created_utc": "2026-01-24 07:45:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjvqd4",
      "title": "Vector dbs aren't memory (learned this the hard way building a coding agent)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjvqd4/vector_dbs_arent_memory_learned_this_the_hard_way/",
      "author": "Sweet121",
      "created_utc": "2026-01-22 14:19:15",
      "score": 38,
      "num_comments": 22,
      "upvote_ratio": 0.8,
      "text": "So i spent the last month losing my mind building a personal coding tutor agent.\n\nWhe problem:\n\nWhe goal was simple: an agent that remembers my skill level, current project, and coding style (like 'i hate list comprehensions, just give me loops'). i did the standard thing: pinecone, chunking, and a RAG pipeline\n\nIt worked for like an hour. by day 3, the agent was a complete mess. it would retrieve code snippets from a project i finished two weeks ago. or worse, i'd tell it 'im switching to rust now', and it would still pull python examples because they were 'semantically similar' to my query. its honestly such a pain.\n\nWhat i tried:\n\ni tried everything to fix this:\n\n* crammed the context window: got expensive fast, and the model got 'lost in the middle'.\n* summarization chains: tried summarizing old convos, but it lost the specific details (like WHY i chose a specific library).\n* metadata filtering: helps, but managing that manually is a nightmare.\n\nthe breakthrough:\n\ni realized i was treating memory like a static library, but human memory is dynamic. we dont remember everything with equal weight. some things need to be forgotten, some need to be merged, and some only matter when they're actually relevant.\n\nthis might sound obvious in hindsight, but i realized i wasn‚Äôt missing a better database ‚Äî i was missing an operating system for memory.\n\nwhat i‚Äôm experimenting with:\n\nSo i built (and open sourced) something called MemOS.\n\nits a memory management layer that sits between your LLM and your storage. instead of just dumping text into a vector store, it treats memory with a lifecycle:\n\n* generated: raw info comes in\n* activated: relevant stuff is pulled into 'Working Memory' (RAM)\n* merged: repeated or evolving preferences get collapsed instead of duplicated.\n* activated: only stuff that actually matters *now* gets pulled into working memory.\n\nit also separates Facts (what happened) from Preferences (what i like). this was the real game changer for me. now, when i ask for a hotel recommendation, it checks my PREFERENCES (cheap, clean) and searches FACTS (hotels in the area).\n\nIm really trying to make this solid for production use, not just a toy demo\n\nthe repo is here: [https://github.com/MemTensor/MemOS](https://github.com/MemTensor/MemOS)\n\ndocs/cloud trial if you dont want to self-host: [https://memos-docs.openmem.net/cn](https://memos-docs.openmem.net/cn)\n\nWould love to hear how you guys are handling long-term user state vs static RAG. is anyone else trying to build an 'OS' layer for this?\n\nCheers!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qjvqd4/vector_dbs_arent_memory_learned_this_the_hard_way/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o12b91h",
          "author": "Expensive_Culture_46",
          "text": "Oh look an ad",
          "score": -38,
          "created_utc": "2026-01-22 15:46:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o12am84",
          "author": "Inner_Possibility310",
          "text": "context pollution is exactly what killed my last project. i hated that the bot would bring up 'User likes pizza' when i was asking about a 'Python pizza library'. does this actually filter that out?",
          "score": 3,
          "created_utc": "2026-01-22 15:43:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cehp",
              "author": "No-Living-8429",
              "text": "MemOS doesn‚Äôt treat all memory as global context. It separates facts, preferences, tools, and knowledge into different memory types, and only activates what‚Äôs relevant to the current task.\nSo ‚ÄúUser likes pizza‚Äù won‚Äôt leak into a ‚ÄúPython pizza library‚Äù query unless the retrieval policy explicitly says it should. Memory is filtered, scoped, and gated, not blindly stuffed back into the prompt.\n\nIn other words: MemOS is built to stop context pollution by design, not by prompt hacks.",
              "score": 1,
              "created_utc": "2026-01-22 15:51:15",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12b57s",
          "author": "not_-ram",
          "text": "interesting. so if i'm building a travel agent, and the user changes their mind from 'Budget' to 'Luxury' halfway through, does MemOS catch that? or do i get conflicting memories?",
          "score": 2,
          "created_utc": "2026-01-22 15:45:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12cv88",
              "author": "No-Living-8429",
              "text": "yep, preferences are mutable memories in MemOS.\n\nwhen a user switches from Budget ‚Üí Luxury, the old preference is updated/archived, not kept active. Only the latest state is recalled ‚Äî no conflicting context, no stale decisions.",
              "score": 2,
              "created_utc": "2026-01-22 15:53:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12ew04",
              "author": "Sweet121",
              "text": "thats the 'Conflict Resolution' part of the lifecycle. when new info conflicts with old info (high confidence), MemOS updates the state instead of just appending a new row",
              "score": 1,
              "created_utc": "2026-01-22 16:02:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o13q7rp",
          "author": "New_Animator_7710",
          "text": "This hits hard because it‚Äôs *exactly* the failure mode of‚Äô ‚Äújust throw it in a vector DB‚Äù memory. You nailed it: the problem isn‚Äôt retrieval, it‚Äôs **state management**‚Äîforgetting, merging, and prioritizing like a real system, not a library. MemOS feels way closer to how humans (and good dev tools) actually work than most RAG stacks I‚Äôve seen",
          "score": 2,
          "created_utc": "2026-01-22 19:33:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11zga1",
          "author": "Crafty_Disk_7026",
          "text": "These tools never work for me.  What works is having a completely clean session and only giving the llm the specific info it needs.   There's no point to storing memory of 2 weeks ago when your app was a Python app if it's a rust app now....\n\nI would encourage you to do some benchmarking.  Try a task with your memory layer and without it and you may be surprised that it's worse with memory since it's just overloading the context with unnecessary info.",
          "score": 2,
          "created_utc": "2026-01-22 14:49:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "o128cvy",
              "author": "Academic_Track_2765",
              "text": "This! Exactly this!",
              "score": 1,
              "created_utc": "2026-01-22 15:32:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o12bmdq",
              "author": "Sweet121",
              "text": "This! You nailed the exact problem I hit.\n\nWhen I switched from Python to Rust, my previous RAG setup kept pulling Python snippets because they were 'semantically similar' to the coding task. It was worse than useless‚Äîit was confusing the model.\n\nThat's actually why I'm trying to treat memory with a lifecycle (forgetting mechanism) rather than a static dump. A 'clean session' is great, but I hated re-prompting 'I prefer concise code' or 'don't use unwrap()' every single time.\n\nI'm accepting your challenge on the 'with vs without' benchmark. I want to prove (to myself mostly) that selective memory > zero memory > bad memory. Will update the repo when I have those numbers.",
              "score": 1,
              "created_utc": "2026-01-22 15:47:42",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1eb6gl",
              "author": "LettuceEfficient7170",
              "text": "I feel the same way",
              "score": 1,
              "created_utc": "2026-01-24 08:58:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bjqq",
          "author": "Aslymcrumptionpenis",
          "text": "yeah, distinguishing between semantic similarity and actual relevance is hard. how are you handling the 'Merging' part? is it an LLM call in the background?",
          "score": 1,
          "created_utc": "2026-01-22 15:47:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12d43k",
              "author": "No-Living-8429",
              "text": "merging is LLM-assisted, but gated.\n\nwe first use embeddings to detect potential duplicates, then call an LLM only to decide merge vs keep (and how to rewrite). If merged, the old memory is archived so it won‚Äôt steer future decisions.",
              "score": 1,
              "created_utc": "2026-01-22 15:54:28",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o12by6n",
          "author": "IncreaseWilling6396",
          "text": "how is this different from MemGPT? or just using LangChain's EntityMemory? feels like we are reinventing the wheel.",
          "score": 1,
          "created_utc": "2026-01-22 15:49:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12disw",
              "author": "No-Living-8429",
              "text": "the difference is scope and lifecycle.\n\nMemGPT / EntityMemory mainly store + retrieve text inside an agent loop. MemOS treats memory as a system layer: multi-type (facts, prefs, tools), merge/archive over time, scheduled recall, and cross-agent reuse ‚Äî so you don‚Äôt just remember, you prevent stale or conflicting memory from steering decisions.",
              "score": 1,
              "created_utc": "2026-01-22 15:56:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o14pm08",
          "author": "toothpastespiders",
          "text": ">Would love to hear how you guys are handling long-term user state vs static RAG. is anyone else trying to build an 'OS' layer for this?\n\nSadly, that's essentially my view of the only fully effective way to handle things. I think effective memory needs to be either custom made from the ground up to fit individual needs and styles or existing projects heavily modified to do the same.",
          "score": 1,
          "created_utc": "2026-01-22 22:21:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1eaypc",
          "author": "LettuceEfficient7170",
          "text": "That's so cool, I want to try it!",
          "score": 1,
          "created_utc": "2026-01-24 08:56:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eg94d",
              "author": "Sweet121",
              "text": "yeah,  pls",
              "score": 1,
              "created_utc": "2026-01-24 09:45:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o11y0yi",
          "author": "Academic_Track_2765",
          "text": "Ok bro, what are you selling?",
          "score": -35,
          "created_utc": "2026-01-22 14:42:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o125eou",
              "author": "Sweet121",
              "text": "This is an open-source project, and the community allows limited promotion. What are you yelling about here?",
              "score": 7,
              "created_utc": "2026-01-22 15:18:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o12842b",
                  "author": "Academic_Track_2765",
                  "text": "Just sick of people selling stuff, when there are production grade systems available. RAG is not rocket science. Install FAISS and be on your way if you want something basic. What really bothers me is that people say use this use that, without any benchmarks e.g., How do you know if weaviate is better than pinecone? If you aren‚Äôt in production than why aren‚Äôt you just using chromadb / faiss? There is already Mem0 and Zep and few other frameworks for memory. If you are going to advertise please share how it compares with other similar frameworks. Besides that I have no problem.",
                  "score": -2,
                  "created_utc": "2026-01-22 15:31:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qjp5vy",
      "title": "Turn documents into an interactive mind map + chat (RAG) üß†üìÑ",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjp5vy/turn_documents_into_an_interactive_mind_map_chat/",
      "author": "sAI_Innovator",
      "created_utc": "2026-01-22 08:29:10",
      "score": 35,
      "num_comments": 6,
      "upvote_ratio": 0.95,
      "text": "Built an app that converts any PDF/DOCX into an interactive mind map (NotebookLM-style).\n\n‚Ä¢ Click a node ‚Üí summary + keywords + ask questions\n\n‚Ä¢ Chat with the whole document (RAG + sources)\n\n‚Ä¢ Document history saved\n\nStack: React + FastAPI, LlamaIndex (parent‚Äìchild), optional Docling parsing.\n\nRepo: https://github.com/SaiDev1617/mindmap\n\nWould love feedback!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qjp5vy/turn_documents_into_an_interactive_mind_map_chat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o11uwuz",
          "author": "CommercialComputer15",
          "text": "How does it organize and recognise relationships between documents? Semantically? Is it a graph?",
          "score": 2,
          "created_utc": "2026-01-22 14:26:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14up4d",
              "author": "sAI_Innovator",
              "text": "Using Hierarchical Llamaindex node parser üëç",
              "score": 1,
              "created_utc": "2026-01-22 22:50:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o12bfp6",
          "author": "Aslymcrumptionpenis",
          "text": "oh wow thats helpful",
          "score": 1,
          "created_utc": "2026-01-22 15:46:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14us0k",
              "author": "sAI_Innovator",
              "text": "Thank you! Please check out the repo.",
              "score": 1,
              "created_utc": "2026-01-22 22:51:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o14mnnb",
          "author": "Unique-Temperature17",
          "text": "Great stuff, congrats on shipping this! The mind map visualisation approach is a nice twist on the usual RAG chat interface. Will definitely clone and check it out over the weekend. Always cool to see LlamaIndex projects in the wild.",
          "score": 1,
          "created_utc": "2026-01-22 22:07:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o14u1ei",
              "author": "sAI_Innovator",
              "text": "cool. Thank you!",
              "score": 1,
              "created_utc": "2026-01-22 22:47:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qispty",
      "title": "Best production-ready RAG framework",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qispty/best_productionready_rag_framework/",
      "author": "marcusaureliusN",
      "created_utc": "2026-01-21 08:43:54",
      "score": 31,
      "num_comments": 20,
      "upvote_ratio": 0.97,
      "text": "Best open-source RAG framework for production?\n\nWe are building a RAG service for an insurance company. Given a query about medical history, the goal is to retrieve relevant medical literature and maybe give some short summary.\n\nService will run on internal server with no access to Internet. Local LLM will be self-hosted with GPU. Is there any production(not research) focused RAG framework? Must-have feature is retrieval of relevant evidences. It will be great if the framework handles most of the backend stuff.\n\nMy quick research gives me LlamaIndex, Haystack, R2R. Any suggestions/advice would be great!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qispty/best_productionready_rag_framework/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17d2a3",
          "author": "Intelligent_Push7935",
          "text": "If your main requirement is strong evidence retrieval, I would start from the retrieval layer.\n\nZeroEntropy has an end to end retrieval stack (document ingestion/parsing + hybrid search + reranking), and they also offer an on-prem option so it can run inside your own network.\n\nA simple production pattern is:\n\n- do a broad first pass retrieval to collect candidates\n\n- rerank the candidates so the top results are the best evidence\n\n- only summarize from the top reranked chunks and return those chunks as citations\n\nIf you need something you can run fully offline with open weights, zerank-1-small is available on Hugging Face. If you want instruction following in the reranker,  zerank-2 is built for that.\n\nThey also have embeddings (zembed-1), but it is still early preview / private beta based on their own site, so I would treat it as optional for now.",
          "score": 8,
          "created_utc": "2026-01-23 08:17:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0toefv",
          "author": "bravelogitex",
          "text": "I would try [https://ragflow.io/](https://ragflow.io/) since it's a complete solution\n\nIf it is lacking I'd go with haystack, my research showed that to be robust. R2R is unsupported and the repo is a ghost town.",
          "score": 4,
          "created_utc": "2026-01-21 08:48:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0udsll",
          "author": "Effective-Ad2060",
          "text": "You should give PipesHub a try.\n\nPipesHub can answer any queries from your existing knowledge base, provides Visual Citations and supports direct integration with File uploads, Google Drive,¬†Gmail, OneDrive, SharePoint Online, Outlook, Dropbox and more. Our implementation (Multimodal Agentic Graph RAG) says Information not found rather than hallucinating.¬†You can self-host, choose any AI model including local inferencing models of your choice.  \nOur AI accuracy is best in class\n\nGitHub Link :  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)\n\nDemo Video:  \n[https://www.youtube.com/watch?v=xA9m3pwOgz8](https://www.youtube.com/watch?v=xA9m3pwOgz8)",
          "score": 7,
          "created_utc": "2026-01-21 12:27:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vrbjq",
              "author": "Clay_Ferguson",
              "text": "pipeshub looks cool, but many people (including me) don't want to touch anything that's not \\`MIT License\\`. Every other license is trying to limit you in some way.",
              "score": 7,
              "created_utc": "2026-01-21 16:42:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0wc3ft",
          "author": "OnyxProyectoUno",
          "text": "The framework choice matters less than getting your document processing pipeline right. Medical literature has complex structures that most parsing approaches butcher. Tables, references, nested sections all get scrambled during ingestion, and you won't discover this until retrieval returns garbage.\n\nLlamaIndex and Haystack handle orchestration well enough, but they won't fix upstream problems. If your parsing mangles a critical study methodology or splits dosage information across chunks, no amount of sophisticated retrieval will recover that context. You need visibility into what your documents actually look like after processing.\n\nR2R has decent observability features, which helps with debugging retrieval issues. But the real problems usually trace back to chunking strategy and how you're handling document structure. Medical papers aren't just text blocks. They have hierarchies, cross-references, and metadata that needs to survive the processing pipeline.\n\nI've been building vectorflow.dev to tackle exactly this visibility problem, letting teams preview their processed documents before committing to a pipeline configuration. For your use case, I'd focus on getting document processing right first, then layer whichever orchestration framework fits your infrastructure constraints.\n\nWhat does your medical literature look like? PDFs with complex formatting, or cleaner structured documents?",
          "score": 3,
          "created_utc": "2026-01-21 18:14:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0trtgh",
          "author": "No_Kick7086",
          "text": "How much data are you embedding, how is it structured,  that matters, a lot and what format is it. Also this is an art form in my experience, a lot depends on what document formats are being embedded, are they all different formats by different authors. Data prep and parsing is one of the hardest things to get right in a commercial setting in my experience. I built a quite advanced customer service rag saas for small businesses and I get new edge cases all the time.\n\n Having only one customer can simplify things but it all depends on the scale of the work and it sounds like you need sources citing etc. Why not use a good opensource model on its own locally, unless your planning on adding medical texts as rag",
          "score": 2,
          "created_utc": "2026-01-21 09:21:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vrzl8",
          "author": "Clay_Ferguson",
          "text": "I have the same question myself, but I'd phrase it as best LangChain-based RAG framework that's MIT License. You can then use LangChain Openwork as the GUI if you want.",
          "score": 2,
          "created_utc": "2026-01-21 16:45:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wfa4s",
          "author": "ampancha",
          "text": "All three frameworks can handle the retrieval mechanics, but for insurance and medical data the harder problem is what sits around them: audit trails for every retrieval, PII redaction before anything hits the LLM context, and strict filtering so the system only surfaces evidence from approved document sets.  \nFramework choice matters less than whether you can prove to compliance that a query about Patient A never leaked context from Patient B. Sending you a DM with more specifics",
          "score": 2,
          "created_utc": "2026-01-21 18:28:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1b26bq",
              "author": "Ok-Durian8329",
              "text": "You nailed it.",
              "score": 1,
              "created_utc": "2026-01-23 20:50:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0x835w",
          "author": "CarefulDeer84",
          "text": "I'd say LlamaIndex or R2R depending on how much control you want. LlamaIndex abstracts a lot which is nice, but R2R gives you more flexibility for custom retrieval logic.\n\nFor medical literature though, retrieval quality matters way more than framework choice. We had Lexis Solutions set up a system with Voyage embeddings and proper chunking strategies that actually understood medical context instead of just semantic similarity. Made a huge difference in precision for our healthcare client. If you're doing production insurance stuff, getting the embedding model and chunk strategy right is probably more important than which framework you pick.",
          "score": 2,
          "created_utc": "2026-01-21 20:37:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0v3e8z",
          "author": "Live-Guitar-8661",
          "text": "If you want to try something early, shoot me a DM.",
          "score": 1,
          "created_utc": "2026-01-21 14:52:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vvu9p",
          "author": "PurpleCollar415",
          "text": "Although not \"production ready\" or a framework, my RAG system using Qdrant + Voyage AI embedding models is supremely optimized for accuracy retrieval. You can ingest any corpora, I just used agent framework documentation.\n\n[https://github.com/MattMagg/agentic-rag-sdk](https://github.com/MattMagg/agentic-rag-sdk)",
          "score": 1,
          "created_utc": "2026-01-21 17:02:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0w86o9",
          "author": "Legitimate-Leek4235",
          "text": "You can use the framework which built this : https://github.com/traversaal-ai/lennyhub-rag",
          "score": 1,
          "created_utc": "2026-01-21 17:57:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wguu4",
          "author": "Academic_Track_2765",
          "text": "Azure search, its built for large scale production systems.",
          "score": 1,
          "created_utc": "2026-01-21 18:35:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10swmq",
          "author": "Ch3mCat",
          "text": "ColPali (Layra or else...) ?",
          "score": 1,
          "created_utc": "2026-01-22 10:09:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11yv6j",
          "author": "vinoonovino26",
          "text": "Nexa.ai has a product called hyperlink, might wanna try it",
          "score": 1,
          "created_utc": "2026-01-22 14:46:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o14sgt3",
          "author": "primateprime_",
          "text": "There isn't a best. You have to figure out what performance (answer quality, response time, general user experience) and weigh that against how much time and money you are willing to allocate. \nSo, how smart, how fast, and how much dakka do you want to throw at it?",
          "score": 1,
          "created_utc": "2026-01-22 22:37:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o17fr7x",
          "author": "Cool_Drive_2090",
          "text": "i would try zeroentropy.dev",
          "score": 1,
          "created_utc": "2026-01-23 08:42:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18z5w4",
          "author": "prodigy_ai",
          "text": "We‚Äôre going with enhanced GraphRAG, especially because we‚Äôre targeting healthcare and legal use cases. In research and academic contexts, GraphRAG consistently outperforms standard RAG, so it‚Äôs the better fit for what we‚Äôre building.",
          "score": 1,
          "created_utc": "2026-01-23 15:05:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhxtt2",
      "title": "Chunking without document hierarchy breaks RAG quality",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhxtt2/chunking_without_document_hierarchy_breaks_rag/",
      "author": "Upset-Pop1136",
      "created_utc": "2026-01-20 10:41:45",
      "score": 27,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "I tested a few AI agent builders (Dify, Langflow, n8n, LyZR). Most of them chunk documents by size, but they ignore document hierarchy (doc name, section titles, headings).  \n\n\nSo each chunk loses context and doesn‚Äôt ‚Äúknow‚Äù what topic it belongs to.  \n\n\nSimple fix: **Contextual Prefixing**\n\nBefore embedding, prepend hierarchy like this:\n\n`Document: Admin Guide`\n\n`Section: Security > SSL Configuration`\n\n`[chunk content]`  \n\n\nThis adds a few tokens but improves retrieval a lot.\n\nSurprised this isn‚Äôt common. Does anyone know a builder that already supports hierarchy-aware chunking?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qhxtt2/chunking_without_document_hierarchy_breaks_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0ng5eg",
          "author": "janus2527",
          "text": "Docling",
          "score": 5,
          "created_utc": "2026-01-20 11:43:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nit6k",
          "author": "Live-Guitar-8661",
          "text": "Totally agree. Also just breaking for chunks wherever.\n\nWe do hierarchy, smart chunks, expanded context, etc. I would love some beta users to test\n\nhttps://orchata.ai",
          "score": 3,
          "created_utc": "2026-01-20 12:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0njsv5",
              "author": "Upset-Pop1136",
              "text": "Thanks I would love to check it.",
              "score": 2,
              "created_utc": "2026-01-20 12:11:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0nnqsc",
                  "author": "Live-Guitar-8661",
                  "text": "Free to sign up, just go to https://app.orchata.ai/signup, let me know what you think!",
                  "score": 1,
                  "created_utc": "2026-01-20 12:39:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0no99r",
          "author": "SiebenZwerg",
          "text": "why before embedding?  \nI thought of this as well but I would have saved the document and section as metadata and provided it as additional context during retrieval so that i don't have 1000 chunks with similar lines at the start.",
          "score": 3,
          "created_utc": "2026-01-20 12:42:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ntdfy",
              "author": "DotPhysical1282",
              "text": "Agree, what are the benefits of running through it before embedding?",
              "score": 2,
              "created_utc": "2026-01-20 13:15:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0q29ld",
              "author": "Clay_Ferguson",
              "text": "But those similar lines of text at the start ARE part of the context of that chunk. What would be interesting is a hybrid approach where the 'category/metadata' for each 'chunk' is still linked (by relational DB field) to the chunk, but where the 'category/metadata' has IT'S OWN vector generated. So this means would be like having two semantic searches. First you identify things matching the high level category, and then once you narrow down you do semantic search on just the chunks (that don't have the metadata, or the duplicate lines at the start)\n\nI haven't yet done RAG myself, so I may be missing something.",
              "score": 1,
              "created_utc": "2026-01-20 19:44:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0naasd",
          "author": "One_Milk_7025",
          "text": "Yes this is not common but this does improve the retrieval quality a lot.. I use this but not sure any library support this or not.\nYou should keep this recursive depth limit so that the breadcumbs doesn't overflow.. good to see more people using this ‚úåÔ∏è",
          "score": 2,
          "created_utc": "2026-01-20 10:54:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0nad4n",
          "author": "Final_Special_7457",
          "text": "I saw someone on YouTube talk about this",
          "score": 2,
          "created_utc": "2026-01-20 10:55:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ogzaa",
              "author": "rshah4",
              "text": "Maybe me, over at Contextual AI we do this and I have shared/shown this technique.",
              "score": 2,
              "created_utc": "2026-01-20 15:20:36",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0ndpq5",
          "author": "Code-Axion",
          "text": "Https://hierarchychunker.codeaxion.com \n\nSee it in action\nhttps://youtu.be/czO39PaAERI?si=1t_J4NZYUcFU1m1E\n\nCheck this out",
          "score": 2,
          "created_utc": "2026-01-20 11:24:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0no4ae",
          "author": "seomonstar",
          "text": "any open source solutions . it sounds a good idea but a lot of promos in this thread.",
          "score": 2,
          "created_utc": "2026-01-20 12:41:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0oqm4u",
          "author": "Ecstatic_Heron_7944",
          "text": "Yep, this is the same idea behind [https://www.anthropic.com/engineering/contextual-retrieval](https://www.anthropic.com/engineering/contextual-retrieval) (Sep 2024).  \nFrom the article:\n\n    original_chunk = \"The company's revenue grew by 3% over the previous quarter.\"\n    \n    contextualized_chunk = \"This chunk is from an SEC filing on ACME corp's performance in Q2 2023; the previous quarter's revenue was $314 million. The company's revenue grew by 3% over the previous quarter.\"",
          "score": 2,
          "created_utc": "2026-01-20 16:06:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tln01",
          "author": "coderarun",
          "text": "Related: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-01-21 08:22:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vi5e5",
          "author": "Important_Proof5480",
          "text": "Yeah, totally agree with the idea. Hierarchy-aware chunking makes a huge difference in retrieval quality.\n\nIn my case I ended up with a slightly lighter version of that. I only prepend the direct header (the immediate parent), not the full path. Once paths get deep and headings are generic, you shift the embedding away from the chunk‚Äôs true meaning. \n\nI would usually do:\n\nHeader: SSL Configuration ¬†\n\n\\[chunk content\\]\n\nAnd then store the full hierarchy separately as structured metadata. At query time I can still surface or inject the full path if the LLM needs grounding or references, without polluting the embedding itself.\n\nBut every use case is different of course.\n\nI just published my chunker at [https://www.docslicer.ai/](https://www.docslicer.ai/) feel free to give it a try and let me know what you think.",
          "score": 2,
          "created_utc": "2026-01-21 16:01:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zpmu9",
          "author": "Academic_Track_2765",
          "text": "This is not true, there are many ways to do this. You can embed metadata, you can do semantic chunking, you can do topic modeling and then chunks, you can do KG. There is no one way to do it.",
          "score": 2,
          "created_utc": "2026-01-22 04:37:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qj13kb",
      "title": "I built my own hierarchical document chunker, sharing it in case it helps anyone else.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qj13kb/i_built_my_own_hierarchical_document_chunker/",
      "author": "Important_Proof5480",
      "created_utc": "2026-01-21 15:36:02",
      "score": 25,
      "num_comments": 8,
      "upvote_ratio": 0.96,
      "text": "A while back I was working on a RAG pipeline that needed to extract structured clauses from dense legal and financial documents. I tried tools like Docling, which worked okay to parse the data, but were too slow for my use case, and tended to flatten the hierarchy. Everything ended up on the same level, which killed context for citations and retrieval.\n\nI needed something which could track deep nesting like this:\n\n* \\# Article II THE MERGER\n* \\## 2.7 Effect on Capital Stock ¬†\n* \\### (b) Statutory Rights of Appraisal ¬†\n* \\#### (i) Notwithstanding anything to the contrary‚Ä¶\n\nAfter a bunch of tweaking, I ended up writing my own parsing + chunking logic that:\n\n* Traverses the document hierarchy tree and attaches the complete heading path to every chunk (so you can feed the full path to the LLM for precise citations)\n* Links chunks by chunk\\_id and parent\\_chunk\\_id ‚Äî at inference time you can easily pull parent chunks or siblings for extra context\n* Only splits on structural boundaries, so each chunk is semantically clean and there are basically 0 mid-sentence cuts\n\nIt worked really well for my project, so I wrapped it in a small frontend and published it as DocSlicer.\n\nTry it here: [https://www.docslicer.ai/](https://www.docslicer.ai/)\n\nJust drop in a PDF or URL, no sign-up needed. Export to json or parquet.\n\nIt's still early and I'm actively improving it, but it already works nicely for long financial or legal docs. Would love to hear real feedback.\n\nHappy to chat in the comments or DMs!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qj13kb/i_built_my_own_hierarchical_document_chunker/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0xtfcz",
          "author": "Clay_Ferguson",
          "text": "Thanks for sharing the project, although I'm probably going to try Docling first to see what kind of performance I get. You might need to be sure your setup is using VRAM correctly and also turn off the OCR flag, which are two things I just now learned from this conversation with Gemini:\n\n[https://gemini.google.com/share/ff2b793b8617](https://gemini.google.com/share/ff2b793b8617)",
          "score": 4,
          "created_utc": "2026-01-21 22:15:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0vl0c6",
          "author": "Viqqo",
          "text": "Just tried it for Annex 11: Computerised Systems (edit: 2025), and the results are‚Ä¶ not that good. The document regions are not accurate, the headings are a mess and so are the resulting document hierarchy.",
          "score": 3,
          "created_utc": "2026-01-21 16:13:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0vq24h",
              "author": "Important_Proof5480",
              "text": "Thanks for trying it out and for the feedback, that‚Äôs really helpful.\n\nThe doc you mention is actually a great example of a case that‚Äôs still tricky. The line numbers interfere with heading detection, so the structure can get pretty messy right now.\n\nI'm already working on handling line-numbered documents better (filtering them from the actual content), and it‚Äôs part of the next iteration of the parser.\n\nIf you‚Äôre up for it, feel free to try a few other documents as well and let me know if you see similar issues or anything else that looks off. Real-world examples like this are super valuable for improving it.",
              "score": 1,
              "created_utc": "2026-01-21 16:36:44",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0x6e07",
              "author": "cat47b",
              "text": "Know of any libraries that are better?",
              "score": 1,
              "created_utc": "2026-01-21 20:30:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0w9hdc",
          "author": "Weird-Investment9522",
          "text": "Very cool project, congrats on building it all yourself! I tested this on some SEC filings and the results look solid, hierarchy is definitely captured better than other pdf to md tools I've tried. I'll try it on a larger doc set and send you a dm with feedback.\n\nHow are you identifying the layouts and hierarchy? Is it mostly heuristic-based, or are you using any ML/LLMs?",
          "score": 2,
          "created_utc": "2026-01-21 18:03:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0wdyvc",
              "author": "Important_Proof5480",
              "text": "Appreciate you trying it out! It‚Äôs fully heuristic-based right now, with no LLMs in the parsing step. That keeps it deterministic, fast, and easier to debug.\n\nThe logic is mostly driven by bounding box geometry and typography signals like font size, weight, alignment, vertical spacing, ...\n\nCurious to hear your thoughts after you‚Äôve tried it on a few more documents.",
              "score": 1,
              "created_utc": "2026-01-21 18:22:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xe0jl",
          "author": "Unique-Temperature17",
          "text": "This looks really smart - the hierarchical path tracking is exactly what's missing from most chunkers. I've run into the same flattening issue with legal docs where you lose all the section context. Will definitely give it a spin this weekend with some contracts I've been working with. Nice work shipping this!",
          "score": 2,
          "created_utc": "2026-01-21 21:04:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o11x85t",
              "author": "Important_Proof5480",
              "text": "Great, curious to know how it went and if you have any feedback!",
              "score": 1,
              "created_utc": "2026-01-22 14:38:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qg6iqv",
      "title": "RAG for excel/CSV",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg6iqv/rag_for_excelcsv/",
      "author": "user_rituraj",
      "created_utc": "2026-01-18 11:42:15",
      "score": 22,
      "num_comments": 23,
      "upvote_ratio": 1.0,
      "text": "I have been working on a personal project with AI. Majorly, it involves reading financial documents(more specifically, DCF models, MIS in Excel).\n\n\n\n\n\nI am using the Claude/GPT 5.1 models for my extraction agent (LLMS running in a Loop) and have in place chunking and indexing with Azure OCR and Azure Search (which provide indexing and searching).\n\n\n\nMeanwhile, PDF extraction is working better, but with Excel I am facing many issues where LLMs mix data, such as saying data is for FY21 when it is for FY22(after getting the chunk data) or not able to find the exact related chunks.\n\n\n\nThe problem is that, in Excel, it is very number-heavy (like a 100\\* 50 type table). Also, structurally, it is a finance document and is created by different individuals, so I really do not control the structures, so lots of spaces or themes, so it is really not like CSV, where columns and rows are well defined.\n\n\n\nMajor Problem:\n\n1. By chunking the data, it loses a lot of context, like headers or information is missing if a single table is divided into multiple chunks, and hence, the context is missing, like what that column is like, the year, and the type.\n\n2. If I keep the table big, it is not going to fit sometimes in context as well.\n\n\n\n3. Also, as these tables are mostly number-heavy, creating chunks really does not make sense much (based on my understanding, as in vector embedding, the number itself does not have much context with text).\n\n\n\n\n\nPlease suggest if someone has worked with Excel and what has helped them to get the data in the best possible way.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qg6iqv/rag_for_excelcsv/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0a0s9n",
          "author": "Sunchax",
          "text": "I usually make a code \"sub-agent\" that can use pandas or such libraries to interact with the excel/csv.\n\nThis means that it can generate exact queries to aggregate, look up, or otherwise extract and manipulate excel data that is even in large files.",
          "score": 10,
          "created_utc": "2026-01-18 12:02:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0arutp",
              "author": "babygrenade",
              "text": "Second this. Llms can understand text but not really data files.",
              "score": 6,
              "created_utc": "2026-01-18 14:57:33",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0b1fp7",
              "author": "SuedeBandit",
              "text": "Yup. Subagents.\nPlanner / prompted analyst\nFunction caller with mcp\nGuardians all around\nVL validator\nDeterministic inputs and outputs",
              "score": 2,
              "created_utc": "2026-01-18 15:45:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0b8t6c",
                  "author": "Sunchax",
                  "text": "Agree, except I usually try to avoid MCP.",
                  "score": 1,
                  "created_utc": "2026-01-18 16:20:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o0bchij",
                  "author": "user_rituraj",
                  "text": "I would love to do this but the challenge is to have the proper data in a structured way in excel. \n\n\nBut unfortunately, table and header detection is an endless rabbit hole of a problem. There are an infinite number of ways a human can muck up an Excel document to make it troublesome to ingest. Common examples are multi-row headers, merged cells within headers, multiple tables in a single sheet separated by blank rows/columns, etc....\n\n\nSo the issue with this is:\n\nI cannot make this directly into tabular data and use agent to.find it and at the same time creating chunks for indexing by ocr has its problem of context loss as data is number heavy so its embedding does not make sense in itself.\n\n\nProbably i will have to create a structured context out of it with proper headers (a processing layer to convert it but i am really not sure how hard this is going to be)so that we can pass headers to read the data using some excel agent.",
                  "score": 1,
                  "created_utc": "2026-01-18 16:38:12",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0b8d6g",
              "author": "Sunchax",
              "text": "Might add that during ingestion one can uses similar sub-agents to explore such data to write a description of what's in it. This makes it possible to fetch such descriptors during search and let the orchestrator know if the right call is to ask a sub-agent to further explore the file.",
              "score": 2,
              "created_utc": "2026-01-18 16:18:35",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0cwvgp",
              "author": "lupin-the-third",
              "text": "This is what I do do as well, but when indexing the files initially I have a data threshhold where if there are less then like 50 rows or something I will index it rather then include it in the code subnet.",
              "score": 2,
              "created_utc": "2026-01-18 21:08:06",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ebfbw",
                  "author": "Sunchax",
                  "text": "Smart! I like that idea, might test it out =)",
                  "score": 1,
                  "created_utc": "2026-01-19 01:29:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0a1d9h",
          "author": "jesus_was_rasta",
          "text": "Worked a lot last year on the matter. Reading Excel is a fucking nightmare. Ones that are basically flat tables: you can arrange something that converts them as a database tables then implement text to SQL to get correct data. You have to enrich data with context, as many times column names doesn't say much. \n\nThen, you have to work on other Excels, those that are more fancy with graphics, informations boxes in many tables per sheet here and there. Those ones better to scan as a whole, like concerting them to PDFs than use Docling",
          "score": 3,
          "created_utc": "2026-01-18 12:07:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0begde",
              "author": "user_rituraj",
              "text": "Most of my exercises are going to be the second type with graphics, spaces and what not.\n\nScanning was my first step to go but as excel is very heavy and tables are also huge so after scanning, i am dividing it into chunks but i see lots of issues with this.\n\n1. Because of number embedding search or semantic search is really not working.\n2.Also with chunking, if we split, usually column names are at the top so that context gets lost in the table.",
              "score": 1,
              "created_utc": "2026-01-18 16:47:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0gan3o",
          "author": "lost_soul1995",
          "text": "I had similar project. \n- chunks per table\n- summary per table\n- embed summary\n- retrieve results based on summary. Feed the retrieved summary and table together.\n- context should mention about quarterly, yearly, monthly terminologies.\n- Use reranking model\n- Hybrid retrieval (B25 plus vector) introduced dirty context as B25 would bring in irrelevant chunks such as revenue repeated in multiple chunks.",
          "score": 2,
          "created_utc": "2026-01-19 10:03:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mdnwt",
              "author": "user_rituraj",
              "text": "This is exactly what i'm doing as of now and i believe this is good at the early stage of the solution. Eventually, i will have to do the pre processing of data to make it like csv (with proper columns in a plane in a nice table) and then give the column structure to the agent to work on csv.\n\nAs these tables can be huge and passing all these numbers to agent context is going to worsen the agent itself.",
              "score": 1,
              "created_utc": "2026-01-20 06:02:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a68nj",
          "author": "Durovilla",
          "text": "How many CSVs do you have? is there an inherent structure to them? if so, you could use DuckDB for text2SQL across all your files without having to embed them",
          "score": 1,
          "created_utc": "2026-01-18 12:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bfwj5",
              "author": "user_rituraj",
              "text": "Usually 4-5 excels with multiple sheets into it.\n\nSo it's more like a MIS document.\n\nBut excels are graphic heavy and also have multiple columns name and there are an infinite number of ways a human can muck up an Excel document to make it troublesome to ingest. Common examples are multi-row headers, merged cells within headers, multiple tables in a single sheet separated by blank rows/columns, etc....",
              "score": 1,
              "created_utc": "2026-01-18 16:54:27",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0a9v1f",
          "author": "IdeaAffectionate945",
          "text": "[Magic Cloud](https://ainiro.io) (my project) has strong support for both PDF files and CSV files. You might want to check it out ...",
          "score": 1,
          "created_utc": "2026-01-18 13:12:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0aqcw6",
          "author": "Anth-Virtus",
          "text": "Llama Cloud offers specifically an OCR/document parsing pipeline especially for spreadsheets.",
          "score": 1,
          "created_utc": "2026-01-18 14:49:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0bf7sl",
              "author": "user_rituraj",
              "text": "Interesting, will check.\n\nAlso if you have already tried it out, do let me know your experience.",
              "score": 1,
              "created_utc": "2026-01-18 16:51:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0atu2q",
          "author": "Much-Researcher6135",
          "text": "1. A DCF model's output will be derived from raw financial report data which is highly structured. Import that, then rerun the DCF model in Python, inserting both into the database.\n\n2. You should NOT push tabular data into a tabular database as a chunked strings for vector search. You should push it *as tabular data*, then give his agent the schema (table layouts in the db) and let it author and run queries. LLMs are great at SQL once they understand your schema!",
          "score": 1,
          "created_utc": "2026-01-18 15:07:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ben1k",
              "author": "user_rituraj",
              "text": "https://www.reddit.com/r/Rag/s/VajxL7WS47",
              "score": 1,
              "created_utc": "2026-01-18 16:48:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0cqnzh",
          "author": "College_student_444",
          "text": "Turn excel data into sentences. Then split and index.",
          "score": 1,
          "created_utc": "2026-01-18 20:35:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhvzy7",
      "title": "Compiled a list of ùêöùê∞ùêûùê¨ùê®ùê¶ùêû ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhvzy7/compiled_a_list_of_ùêöùê∞ùêûùê¨ùê®ùê¶ùêû_ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨/",
      "author": "midamurat",
      "created_utc": "2026-01-20 08:52:51",
      "score": 22,
      "num_comments": 7,
      "upvote_ratio": 0.93,
      "text": "Been working on reranking for a while and kept finding info all over the place - different docs, papers, blog posts. Put together what I found in case it helps someone else.\n\n**What it includes:** \n\n* Code to get started quickly (both API and self-hosted)\n* Which models to use for different situations\n* About 20 papers - older foundational ones and recent stuff from 2024-2025\n* How to plug into LangChain, LlamaIndex, etc.\n* Benchmarks and how to measure performance\n* Live leaderboard for comparing models\n\nSome of the recent papers cover interesting approaches like test-time compute for reranking, KV-cache optimizations for throughput, and RL-based dynamic document selection.\n\nStill adding to it as I find more useful stuff. If you've come across resources I missed, feel free to contribute or drop suggestions.\n\n  \nGitHub: [https://github.com/agentset-ai/awesome-rerankers](https://github.com/agentset-ai/awesome-rerankers)\n\nHappy to answer questions about specific models or implementations if anyone's working on similar stuff!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qhvzy7/compiled_a_list_of_ùêöùê∞ùêûùê¨ùê®ùê¶ùêû_ùê´ùêûùê´ùêöùêßùê§ùêûùê´ùê¨/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0n2y7y",
          "author": "Professional_Cup6629",
          "text": "thanks for the list!",
          "score": 1,
          "created_utc": "2026-01-20 09:47:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0n32br",
              "author": "midamurat",
              "text": "glad if it helps :)",
              "score": 1,
              "created_utc": "2026-01-20 09:48:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0neqyl",
          "author": "No_Kick7086",
          "text": "Nice work",
          "score": 1,
          "created_utc": "2026-01-20 11:32:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nt2t8",
              "author": "midamurat",
              "text": "üôå üôå",
              "score": 1,
              "created_utc": "2026-01-20 13:13:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0qibjv",
          "author": "McNiiby",
          "text": "How does this compare to MTEB/RTEB? Why are some of the more popular STOA models missing from the Agentset leaderboard like Llama, gemini, and Qwen3?",
          "score": 1,
          "created_utc": "2026-01-20 20:58:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ua3rd",
              "author": "midamurat",
              "text": "hugging face leaderboards do not contain rerankers and you're right many other models are missing from Agentset leaderboard. We plan to add models as we test so the list isn't complete yet",
              "score": 1,
              "created_utc": "2026-01-21 12:00:58",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o17581v",
          "author": "Commercial_Range_957",
          "text": "Thanks for the list and if possible could you please let me know if minilm-l6 on cpu(azure aks) for 10k requests per day will workout ??",
          "score": 1,
          "created_utc": "2026-01-23 07:07:45",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qga7m6",
      "title": "Claude RAG Skills : 4 open-source tools to optimize your RAG pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qga7m6/claude_rag_skills_4_opensource_tools_to_optimize/",
      "author": "Responsible-Radish65",
      "created_utc": "2026-01-18 14:38:44",
      "score": 20,
      "num_comments": 0,
      "upvote_ratio": 0.92,
      "text": "I've been using these internally for 3 months while building our RAG platform. Just cleaned them up for public release.\n\n**The 4 skills:**\n\n* `/rag-audit`¬†‚Üí Scans your codebase, flags anti-patterns, gives you a score out of 100\n* `/rag-scaffold`¬†‚Üí Generates 800+ lines of production-ready boilerplate in seconds\n* `/chunking-advisor`¬†‚Üí Decision tree for optimal chunk size based on your document types\n* `/rag-eval`¬†‚Üí Retrieval metrics (recall, MRR, NDCG) + optional benchmark against our API\n\n**Concrete results:**\n\n* Debugging sessions cut from 2h to 30min (the audit catches recurring mistakes)\n* Scaffold saves \\~15k tokens per new project setup\n* Chunking advisor prevented me from using 512 tokens on legal documents (bad idea)\n\nMIT licensed, no signup required:¬†[https://github.com/floflo777/claude-rag-skills](https://github.com/floflo777/claude-rag-skills)\n\nFeedback welcome, especially if you spot missing anti-patterns.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qga7m6/claude_rag_skills_4_opensource_tools_to_optimize/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qh5kls",
      "title": "Very confused on the optimal approach for generating knowledge-graphs for use with RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qh5kls/very_confused_on_the_optimal_approach_for/",
      "author": "boombox_8",
      "created_utc": "2026-01-19 14:27:40",
      "score": 20,
      "num_comments": 16,
      "upvote_ratio": 0.92,
      "text": "Hey guys! I am new to the world of knowledge graphs and RAGs, and am very interested in exploring it!\n\n  \nI am currently looking at using property graphs (neo4j to be specific) as the 'knowledge base' for RAG implementations since I've read that they're more powerful than the alternative of RDFs\n\n  \nWhat confuses me is about how one should go about generating the knowledge graph in the first place. neo4j's own blog and various others propose using LLMs to extract the data for you, and construct a JSON/csv-esque format which is then ingested to create the knowledge graph\n\n  \nExcept it feels like I am poisoning the well here so to speak? If I have tons of text-based documents as my corpora, won't using LLMs to do the job of data extraction and graph generation have issues?\n\nOff the top of my head, I can think of the following issues:\n\n1) The LLM could generate duplicates of entities across documents/chunks (For example, the word \"White House\" is present in a bunch of various documents in various levels of described detail? The LLM could very well extract out multiple such 'White House' entities\n\nI did have an idea of pre-defining all entity types and relations and forcing he LLM to stick with that, as well do an NLP-based deduplication technique, though I am not sure if it'll work well\n\n  \n2) The LLM could just up and hallucinate up data. Bad for obvious reasons, since I don't want a garbage in = garbage out problem for the resultant rag\n\n  \n3) It could just generate wonky results with incorrect 'syntax'. Bad for obvious reasons\n\n  \n4) Manually extracting data and writing the appropriate CYPHER queries? Yeah, won't work out feasibly\n\n5) Using an NLP-based entity and relation extractor? Faster and cheaper compute-wise, but the duplication issue still remains. It does solve issue 3)\n\nWith all these issues comes the extra issue of validating the output graph. Feels like I'm biting off more than I can chew, since all of this is VERY hard to pack into a pipeline unless I make my own bespoke one for the domain I am focusing on. Is there a better way of doing things?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qh5kls/very_confused_on_the_optimal_approach_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0hkqew",
          "author": "Popular_Sand2773",
          "text": "Knowledge graphs definitely aren't for the faint of heart. The reason everyone recommends using llms to generate the graph is because the alternatives involve significantly more effort. Stanford maintains the CoreNLP library which you can use and is deterministic so no hallucination risk. It's very easy to try out and once you see it's default hot mess you'll see why people risk it with llms.  \n  \nNowadays the risk of improper extraction is fairly low with the llm but the cost is obviously very high. For many RAG setups retrieval corpus's are large and queries are sparse. This can mean you are paying to have a llm generate a mostly unused graph which can hurt at scale.  \n[](https://alb.reddit.com/cr?za=ierAezmqakjkPaVDWKzCA3tCEIV4aCCC0bCRBCF5670AiSndpd2SMNGrxPD7pIs6QF_4_YGz223SK1eDI9NQaF3Bknih3JiBtO6-hq5KNJJm-M01GGJkGTYSSFvK9a--Ar68SJTzwWNl4P85wKLFZdf16OlAlcuEW3bQ8uXOYDazxjJfRvbl32NUYNgzANollM-JxDrxA9w5Y12eNTUmO27Ryo87KWbIfQIfJGpcpepJNJAFeDTsF2g3zJADNX9DoiMdVapRNgCzoQJ7qVHCxGrTGszYBwTzOhewPPUg97EFuriNOiSlKB8mgqfSm7Y6VBddMqH2m3j8H5TviyJQqm9pkBWo1-bQzRQvzseTDrb640e94Gk5GLcfiwM2rq0HtTVmaCwYyVGZ3xCSnsAe9uGOTYk9_rGGRR-AacQb5Mt_CAbs4yf7K4tOwLzW0Gf54Qf2tO_sJQF39wH8Ze8qdBof95qtUiZTkvswn5SQCJ_mUXzAHEUmOh-XA2Sl598ax2cyd-oM1T3oPMMf7m85Q99cXlzhY_Fp23SrxgpqwblimxUIWyD84KxrrgRU2ZpYBN3gxww2U4FwY6Rij4hIWQ7s-xTHsBcaSUSThgeUcd4XuZj5OATt&zp=EhBBaNqSZnW-BP4gOIznz82pQlrRYlfvaCNcyBIhs7wTVfWQDkGmlXivkNxEMtVXgXrymn3Wp2-145A4MY8bevLzVp1T2C9RxQn5ZonalUJpLtqYJENT7XBECPofmqZoSYN6dRcQs05mbKpeONZg3gNhIuibc7o_A9zb46C_7iccXXMOH2yC0cwG6bnT-efJDN_tqMYjKd2etZfrO3Y8GCROkZrxPUnmC--M3iUkc79mq0cOkoRo2W8LVfPMGq6mhq1oAyQ_hbSV6sOcq946_PtUYjPmN3kr4meSycYg_NXUQQfmxr99tsjBBv25tnzoWhw3hJHyQf_N6XQMXqpOhRCRm55i8xSv3PhZYuijg7qfIfRZ5LPyr2gvl21sBitjPlEmA2UuICjDB7Kypxrts7lNzRc-eCtDEuPDpGBHV3KpsXB30tBvlbVpNyhx2uMiFpeS4c3pn-Dii9nkkcviACMwawKXj0mue16rI3_gKxMWYnnStQ&a=598359&b=565262&be=383828&c=382514&d=568613&e=378211&ea=379178&eb=353934&f=352953&r=10&g=1&i=1768835127842&t=1768835729084&o=1&q=1&h=212&w=732&sh=864&sw=1536)  \nYou also identified the other issue which is canonicalization. Assuming your many white houses are spelled the same then they are a shared node with multiple edges. This is desired behavior and they aren't true duplicates just shared entities. The problem is even a small typo without post processing will faultily create a new node.   \n  \nIf you really want knowledge graph like quality without all the extra hassle then knowledge graph embeddings are a great place to look.",
          "score": 6,
          "created_utc": "2026-01-19 15:16:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0hwe31",
              "author": "callmedevilthebad",
              "text": "You are spot on about the cost. Interested to know if you found any low cost, good-quality solutions.",
              "score": 1,
              "created_utc": "2026-01-19 16:09:32",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0ish77",
                  "author": "Popular_Sand2773",
                  "text": "I can probably point you in the right direction/to the right model family but would need to know a little more about the use case. For example if you have a reasonable closed world/taxonomy you can collapse the problem space. Knowledge graphs certainly have their place and purpose but it's more scalpel to embeddings machete. \n\nWithout more context for a case like this I would suggest knowledge graph embeddings as the sweet spot between cost quality and latency. DM me and I can get you access to an early model that's working well for me.",
                  "score": 1,
                  "created_utc": "2026-01-19 18:33:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0j11s2",
              "author": "boombox_8",
              "text": ">If you really want knowledge graph like quality without all the extra hassle then knowledge graph embeddings are a great place to look\n\n  \nI am planning on using the KG for a problem with a very particular domain. Would it be right to assume that the constrained domain/problem space makes it more suitable to NOT go for a knowledge graph embedding and instead just go directly for an LLM-generated KG followed by manual validation and cleaning up from my side?\n\n  \nOr can the two methods be combined and used in a hybrid fashion?",
              "score": 1,
              "created_utc": "2026-01-19 19:11:09",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jaje0",
                  "author": "Popular_Sand2773",
                  "text": "That's actually a great question. There is no reason you can't do both as complements and if you are maintaining your own graph you can fine tune the embeddings on the graph for some really fun combo moves.\n\nFor a constrained domain/problem space you are in the best place knowledge graph wise because you can potentially handroll a cheaper and more reliable system than a pure llm with something like langextract. CIE closed information extraction models take a predefined schema + set of relationships and reliably pull what you need at much lower cost than an llm. For example Gliner2, UIE, REBEL(ish) can likely do the job cleanly without the coreNLP noise. You have some options.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:54:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0hrj27",
          "author": "ggone20",
          "text": "Depends on the data. Or if you build it right it dynamically creates new nodes, edges, and properties. Entities need to deduped so you‚Äôll manage an index. You also don‚Äôt want similar edges -> is_part_of, has_part‚Ä¶ same same. Feed the LLM your index of nodes and edges and properties‚Ä¶ tell her to use those when it makes sense or create new ones when it doesn‚Äôt. You‚Äôll also use the index as filter, which helps you search and traverse much faster as well.",
          "score": 1,
          "created_utc": "2026-01-19 15:47:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0j1od4",
              "author": "boombox_8",
              "text": "By index, are you referring to having a pre-defined list of 'valid' relation names and entities, and feed it to the LLM, forcing it to stick to just those values unless it can't (in which case it creates new ones)?",
              "score": 2,
              "created_utc": "2026-01-19 19:14:01",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0jbi7d",
                  "author": "Popular_Sand2773",
                  "text": "They are talking about canonicalization. You can constrain the llm but most people just map things back using heuristics like cosine or jaccard similarity as post processing cleanup before you move it into the graph.",
                  "score": 1,
                  "created_utc": "2026-01-19 19:59:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qlftqz",
      "title": "Which Vector DB should I use for production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "author": "Cheriya_Manushyan",
      "created_utc": "2026-01-24 06:42:26",
      "score": 20,
      "num_comments": 60,
      "upvote_ratio": 0.92,
      "text": "I see many enterprises using Pinecone, Weaviate, Milvus, Qdrant etc. Based on your experience, which one is best  for production and why? Help a friend out...üôÇ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qlftqz/which_vector_db_should_i_use_for_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o1e2jwz",
          "author": "hrishikamath",
          "text": "Postgres w pgvector lol, I serve like 100k documents and it takes like milli seconds for hybrid search. Edit: repo: https://github.com/kamathhrishi/stratalens-ai and blogpost: https://substack.com/home/post/p-181608263",
          "score": 21,
          "created_utc": "2026-01-24 07:40:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eemh2",
              "author": "ZiKyooc",
              "text": "That is a lot of seconds",
              "score": 11,
              "created_utc": "2026-01-24 09:30:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fifrm",
                  "author": "hrishikamath",
                  "text": "Sorry I meant milli seconds loll (editing my comment)",
                  "score": 2,
                  "created_utc": "2026-01-24 14:31:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1e5hci",
              "author": "debauch3ry",
              "text": "~~Are you saying pgvector is bad?~~ (confusion resolved after edit!) I found the tech very reliable with 500k vectors indexed with HNSW giving very fast knn searches. As a general DB it's also fairly high tier.",
              "score": 6,
              "created_utc": "2026-01-24 08:06:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1fijjb",
                  "author": "hrishikamath",
                  "text": "I meant milli seconds I edited my comment",
                  "score": 2,
                  "created_utc": "2026-01-24 14:32:25",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1foxph",
                  "author": "hrishikamath",
                  "text": "Repo: https://github.com/kamathhrishi/stratalens-ai and blogpost how I did it: https://substack.com/home/post/p-181608263",
                  "score": 2,
                  "created_utc": "2026-01-24 15:06:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1e4j11",
              "author": "Ok-Adhesiveness-4141",
              "text": "Right choice.",
              "score": 2,
              "created_utc": "2026-01-24 07:58:14",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1ertbv",
              "author": "psanilp",
              "text": "How do you handle chunking? We use the Azure AI search pipeline and thinking of going local RAG. Do you have a ready product or licensable deployment?",
              "score": 2,
              "created_utc": "2026-01-24 11:30:49",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1f35x9",
              "author": "Straight-Gazelle-597",
              "text": "solid choice to cover at least 90% of the biz needs;-)",
              "score": 1,
              "created_utc": "2026-01-24 12:59:52",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1fw61c",
              "author": "virgilash",
              "text": "Yeah, op, give PostgreSQL with pgvector a try üòâ",
              "score": 1,
              "created_utc": "2026-01-24 15:42:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ep896",
          "author": "ampancha",
          "text": "If you're already on Postgres, pgvector is underrated. One less system to secure and operate, and recent benchmarks show it's competitive with the dedicated options at moderate scale.\n\nIf you want a purpose-built vector DB, Qdrant. Best latency performance in most independent tests, and the open-source version is production-ready.\n\nEither works. What usually breaks is the stuff around the DB: missing per-user query limits, no spend caps on embedding calls, no alerting when retrieval patterns drift. Sent you a DM",
          "score": 8,
          "created_utc": "2026-01-24 11:07:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1et06u",
              "author": "Cheriya_Manushyan",
              "text": "Informative, thanks.",
              "score": 1,
              "created_utc": "2026-01-24 11:41:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e29ow",
          "author": "instantlybanned",
          "text": "I use milvus in production and it works extremely well for me.¬†\n\n\nPostgreSQL with pgvector is not an option for me because of the recall that I need and the speed with which I need the results at the scale that I'm working at. With milvus, I have control over the in memory index that's being used as well as the parameters for the index and the query parameters for the approximate search, allowing me to tune it to have high recall at still very fast speeds.¬†\n\n\nEdit: just for context, I'm working with around 200 million vectors.¬†",
          "score": 8,
          "created_utc": "2026-01-24 07:38:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e6gs5",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing",
              "score": 1,
              "created_utc": "2026-01-24 08:15:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e6g4b",
          "author": "debauch3ry",
          "text": "I bet those enterprises using Pinecone also write their backend software in python or node.js.\n\nI use PaaS cloud postgres DB + pgvector, as I find it a good mix of having control vs delegating the infrastructure to a cloud provider.\n\nQdrant you can run locally via docker which is testament to their confidence in their tech. They're my next-to-try.",
          "score": 4,
          "created_utc": "2026-01-24 08:15:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eskjy",
              "author": "Cheriya_Manushyan",
              "text": "For most of them, a python based stack is used.",
              "score": 2,
              "created_utc": "2026-01-24 11:37:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e7ex8",
          "author": "pberck",
          "text": "I used lancedb in Rust in my last project. It worked well, but it wasn't a huge amount of data.",
          "score": 5,
          "created_utc": "2026-01-24 08:24:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1etlhw",
              "author": "Cheriya_Manushyan",
              "text": "Is it open source or like Pinecone?",
              "score": 1,
              "created_utc": "2026-01-24 11:46:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1etvny",
                  "author": "pberck",
                  "text": "It is dual license, open source with apache 2.0 and a cloud version which has a commercial licence. I used the OS version.",
                  "score": 1,
                  "created_utc": "2026-01-24 11:48:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1ec1id",
          "author": "Suspicious-Bite6107",
          "text": "People that say pgvector is slow are usually DEVs that want \"to simplify their life\" and dont look or make any benchmark, also because they don't read they still think you have to use postgres  with pgvector only, and forget about pgvectorscale or diskann indexes....thank you for still providing work, I think that even with AI there is enough stupidity in this world to allow to have work for the next 50 years :) \n\nPS - your app isn't special, you are not going to have to handle 100k concurent transactions unless as usuall you keep not using pooling :p",
          "score": 5,
          "created_utc": "2026-01-24 09:06:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eu7ei",
              "author": "Cheriya_Manushyan",
              "text": "Well, you have a fair point.",
              "score": 1,
              "created_utc": "2026-01-24 11:51:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dyeel",
          "author": "IdeaAffectionate945",
          "text": "Roll your own, way faster and more flexible. Preferably something that's a \"plugin\" to SQL, allowing you to parametrise your vector retrieval saying stuff such as \"select \\* from rag where x, and distance(...)\"\n\nIt's a 100 times more flexible than whatever Pinecone even \\*can\\* give you in theory.\n\nI'm using SQLite and sqliteai-vector ...",
          "score": 7,
          "created_utc": "2026-01-24 07:03:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dz14n",
              "author": "Cheriya_Manushyan",
              "text": "I‚Äôve personally been using PostgreSQL with pgvector, but I notice many enterprises prefer databases like Pinecone. I‚Äôm trying to understand the real reasons behind this choice.",
              "score": 8,
              "created_utc": "2026-01-24 07:09:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1e188g",
                  "author": "Chucki_e",
                  "text": "I also use pgvector and I don't think you need to use any third-party vector database unless you have some special requirements that force you to. As with so many other architectural/technology choices, it's easier to start simple and then scale up when you actually need to - of course with a plan in mind, but I don't imagine migrating your vector database isn't that big of an issue?",
                  "score": 4,
                  "created_utc": "2026-01-24 07:28:42",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1e2dnr",
                  "author": "IdeaAffectionate945",
                  "text": "*\"I notice many enterprises prefer databases like Pinecone\"*\n\nMarketing bs. The best filtering you can do is on meta fields. With integrated into the core DB, you've got a bajillion times the speed, and a bajillion times the flexibility on querying the thing.",
                  "score": 2,
                  "created_utc": "2026-01-24 07:39:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e3jfr",
          "author": "Academic_Track_2765",
          "text": "Many options, if you guys use azure use azure search, I have deployed solutions with chromadb, neo4j, weaviate. See what costs the least for your dimensions X documents and use that. I think it mostly comes down to what your company / IT dept is ok with and the costs. You can even build your own if you like.",
          "score": 2,
          "created_utc": "2026-01-24 07:49:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1e8uj9",
              "author": "Cheriya_Manushyan",
              "text": "From your experience, which option do you prefer if the goal is good performance at a low cost?",
              "score": 1,
              "created_utc": "2026-01-24 08:37:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e3v7x",
          "author": "Effective-Ad2060",
          "text": "We use qdrant (supports hybrid search out of the box)  \nFor reference:  \n[https://github.com/pipeshub-ai/pipeshub-ai](https://github.com/pipeshub-ai/pipeshub-ai)",
          "score": 2,
          "created_utc": "2026-01-24 07:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1egvk3",
              "author": "KYDLE2089",
              "text": "+1 we do too work really good",
              "score": 2,
              "created_utc": "2026-01-24 09:51:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o1emf8s",
              "author": "Cheriya_Manushyan",
              "text": "Noted.",
              "score": 0,
              "created_utc": "2026-01-24 10:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e4vaf",
          "author": "nborwankar",
          "text": "Depends on corpus size, whether there is need for joining with relational data and whether there is need to scale up massively",
          "score": 2,
          "created_utc": "2026-01-24 08:01:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1eseds",
              "author": "Cheriya_Manushyan",
              "text": "That's an interesting case you shared.",
              "score": 0,
              "created_utc": "2026-01-24 11:36:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1eit8q",
          "author": "RolandRu",
          "text": "In my opinion there is no single ‚Äúbest‚Äù vector DB for production.\n\nI‚Äôm building a code-focused RAG. For now FAISS is enough for me, but I also added BM25 search, hybrid search and a dependency graph between code chunks.\n\nAfter some time I realized that new requirements will only make my custom code more complicated. In practice it feels like I‚Äôm rebuilding features that Weaviate already has (BM25 + hybrid + graph/relations).\n\nQdrant can be faster, but for me the difference like 25ms vs 35ms doesn‚Äôt really matter. Native support for everything I need matters more, so the next step will be migrating to Weaviate and testing it in real use.\n\nAt the same time I will keep FAISS as a nice option for people who want to run the project quickly without setting up a container and configuring Weaviate.\n\nSo Weaviate ‚Äî **if someone thinks this is a mistake, please let me know** üôÇ",
          "score": 2,
          "created_utc": "2026-01-24 10:09:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esoxa",
              "author": "Cheriya_Manushyan",
              "text": "Noted",
              "score": 1,
              "created_utc": "2026-01-24 11:38:34",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1erp4p",
          "author": "psanilp",
          "text": "PostGres with some plugin seems to be the preferred route. Having said that, does anyone here have a 'rag in a box' i can install and deploy at a legal firm?",
          "score": 2,
          "created_utc": "2026-01-24 11:29:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1etdu0",
              "author": "Cheriya_Manushyan",
              "text": "By 'rag in a box', you mean low code/no code solution?",
              "score": 1,
              "created_utc": "2026-01-24 11:44:34",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1g34yq",
                  "author": "psanilp",
                  "text": "I understand RAG pipeline and we currently use Azure AI Search with inbuilt vectorisation/hybrid/semantic search . a) There are some issues related to chunking longer docs. b) Some firms who are willing to pay, would like a setup where the RAG is done locally so everything stays within the firewall. Hence no external api calls. So my query was if someone has an end to end solution we can buy/license and deploy on a hardware that is physically placed in the client's premises.",
                  "score": 1,
                  "created_utc": "2026-01-24 16:14:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1exwig",
              "author": "seomonstar",
              "text": "anything for legal firms needs heavy skills and lots of custom work. Thats why there is so much money in it. from parsing to chunking to retrieval to context and session management.  There are some open source rag things on github, but I wouldnt be deploying them in a law firm ‚Ä¶",
              "score": 1,
              "created_utc": "2026-01-24 12:21:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1errhw",
          "author": "Useful-Disk3725",
          "text": "I had qdrant for some time, really fast. Then switched to MariaDB 11.8. Vector search is really fast, but insert is slow. I think qdrant had a buffering system, and building indexes from time to time (standing 100% cpu spikes regularly). I mad a similar thing, getting vectors to a buffer table without index and in a separate cronned process moving in batches.\n\nThe key is, never mix vector index search with regular searches, that‚Äôs a known limitation in almost all databases. Though overcome is easy (technologies advertised as hybrid search are only hack solutions)",
          "score": 2,
          "created_utc": "2026-01-24 11:30:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1et5tn",
              "author": "Cheriya_Manushyan",
              "text": "Thanks for sharing.",
              "score": 1,
              "created_utc": "2026-01-24 11:42:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1dw4nh",
          "author": "PiaRedDragon",
          "text": "Qdrant or neo4j.",
          "score": 2,
          "created_utc": "2026-01-24 06:44:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dwhad",
              "author": "Cheriya_Manushyan",
              "text": "Can you give reasons, focusing on performance and cost compared to other databases?",
              "score": 2,
              "created_utc": "2026-01-24 06:47:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1enimk",
          "author": "crishoj",
          "text": "Are you certain you even need vectors? Have you tried regular keyword search? Have the agent come up with relevant search terms",
          "score": 2,
          "created_utc": "2026-01-24 10:52:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1esw2l",
              "author": "Cheriya_Manushyan",
              "text": "I haven't tried keyword only search, will checkout.",
              "score": 1,
              "created_utc": "2026-01-24 11:40:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1etdxu",
          "author": "Professional_Cup6629",
          "text": "might be a good read: https://agentset.ai/blog/best-vector-db-for-rag",
          "score": 1,
          "created_utc": "2026-01-24 11:44:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ey7jg",
              "author": "Cheriya_Manushyan",
              "text": "Will check.",
              "score": 1,
              "created_utc": "2026-01-24 12:23:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1f35tp",
          "author": "Live-Guitar-8661",
          "text": "Postgres",
          "score": 1,
          "created_utc": "2026-01-24 12:59:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1f3g4n",
          "author": "a_developer_2025",
          "text": "Doesn‚Äôt pgvector apply metadata filtering only after searching by vectors? If that‚Äôs still true, it is a big issue if you have small datasets per metadata.",
          "score": 1,
          "created_utc": "2026-01-24 13:01:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fmb3m",
          "author": "ComputationalPoet",
          "text": "ill add opensearch,  several variations of semantic search (approximate knn, exact cosine similarity), hybrid search and obviously bm25.   Scales well and performs great, though ill admit i dont know some of these other options that well.",
          "score": 1,
          "created_utc": "2026-01-24 14:52:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1fphel",
          "author": "bzImage",
          "text": "Qdrant.",
          "score": 1,
          "created_utc": "2026-01-24 15:09:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ftsfa",
          "author": "seomonstar",
          "text": "Postgre scaled to 800 million users with openai, still at their core, so anyone having problems with pgvector should bear in mind its likely a skill issue unless they have 800 million users or more . https://openai.com/index/scaling-postgresql/",
          "score": 1,
          "created_utc": "2026-01-24 15:30:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjx9yy",
      "title": "üöÄ We designed a white-box RAG framework with a built-in AI developer assistant ‚Äî feel free to give it a try!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjx9yy/we_designed_a_whitebox_rag_framework_with_a/",
      "author": "Relevant_Abroad_6614",
      "created_utc": "2026-01-22 15:20:04",
      "score": 19,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "üöÄ **Introducing UltraRAG 3.0: Reject \"Black Box\" Development. Make Every Line of Inference Logic Visible!**\n\nüèÅ **UltraRAG 3.0** solves the \"Last Mile\" problem in RAG development, developed by THUNLP, NEUIR, OpenBMB & AI9Stars.\n\nüêô **GitHub:** [https://github.com/OpenBMB/UltraRAG](https://github.com/OpenBMB/UltraRAG)\n\nüìö **Tutorial:** [https://ultrarag.openbmb.cn/pages/en/getting\\_started/introduction](https://ultrarag.openbmb.cn/pages/en/getting_started/introduction)\n\n**Key Highlights:**\n\n‚ö° **WYSIWYG Pipeline Builder**\n\nFrom logic to prototype in seconds. Our dual-mode builder (Canvas + Code) syncs in real-time. Click \"Build\" and your static logic instantly becomes an interactive UI. No more boilerplate code!\n\nüîç **Pixel-Level \"White-Box\" Visualization**\n\nStop guessing. The \"Show Thinking\" panel visualizes the entire inference trajectory‚Äîloops, branches, and tool calls. Debug bad cases instantly by comparing retrieval chunks vs. model hallucinations.\n\nü§ñ **Built-in AI Developer Assistant**\n\nStuck on config? The embedded AI Assistant knows the framework inside out. Just use natural language to generate Pipeline configurations, optimize Prompts, or explain parameters.\n\nüî¨ **DeepResearch Engine**\n\nPowered by **AgentCPM-Report**, it supports \"Writing-as-Reasoning.\" The system dynamically plans, retrieves, and deepens content to generate professional, cited reports automatically.\n\n**Check out the demo video on the GitHub pageÔºÅ**",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qjx9yy/we_designed_a_whitebox_rag_framework_with_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o17j8td",
          "author": "Relevant_Abroad_6614",
          "text": "https://preview.redd.it/knov7unxf2fg1.png?width=3686&format=png&auto=webp&s=d173ebf383e8371cd9f08f851e7473c9b3f93981",
          "score": 1,
          "created_utc": "2026-01-23 09:14:32",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o17j9b8",
          "author": "Relevant_Abroad_6614",
          "text": "https://preview.redd.it/1wvuoz51g2fg1.png?width=3734&format=png&auto=webp&s=304ef394c007ea63f4b2792b772b3b5924716bbf",
          "score": 1,
          "created_utc": "2026-01-23 09:14:40",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg2hxo",
      "title": "PyTelos - Agentic RAG powered by Postgres pg_vector and pg_textsearch extensions",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg2hxo/pytelos_agentic_rag_powered_by_postgres_pg_vector/",
      "author": "Feisty-Assignment393",
      "created_utc": "2026-01-18 07:45:20",
      "score": 17,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "I wanna introduce my side project \n\n[PyTelos](https://github.com/richinex/pytelos)\n\nIt is an Agentic RAG App with support for the major LLM providers. I built it as a side project to show a friend how Agentic RAG works. It uses the Postgres pg\\_vector and newly released pg\\_textsearch extensions. The pg\\_textsearch extension allows for BM25 relevance-ranked full-text search.\n\nIt also uses a durable execution library I wrote for distributed indexing.\n\nI'd like to hear your thoughts and feedback.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qg2hxo/pytelos_agentic_rag_powered_by_postgres_pg_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o09c14b",
          "author": "mtbMo",
          "text": "Nice one, might check it out. Any plans for a web-ui?",
          "score": 1,
          "created_utc": "2026-01-18 08:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o09cgtw",
              "author": "Feisty-Assignment393",
              "text": "My web design skills are still sub-par so I can't say for now",
              "score": 1,
              "created_utc": "2026-01-18 08:21:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o09cwph",
                  "author": "mtbMo",
                  "text": "Once my coding agents are up and running, i will give them a try to get something working",
                  "score": 2,
                  "created_utc": "2026-01-18 08:25:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkupw7",
      "title": "We did RAG on the r/Rag Reddit channel - Free To Use",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qkupw7/we_did_rag_on_the_rrag_reddit_channel_free_to_use/",
      "author": "jannemansonh",
      "created_utc": "2026-01-23 16:02:28",
      "score": 16,
      "num_comments": 8,
      "upvote_ratio": 0.86,
      "text": "Hey, creator of Needle here. This channel is one of the reasons we started Needle as a RAG API in the first place! So many real insights from people actually building things.\n\nSo we made a free, public search tool that lets you explore everything discussed here in 2025.\n\nUseful if you're:\n\n* A dev looking for RAG advice\n* A business person exploring how RAG can help\n* A founder researching trends and common problems\n\nWould love your feedback and curious: what questions would you find most interesting to explore?\n\nCompletely free, no signup required: [https://needle.app/featured-collections/reddit-rag-2025](https://needle.app/featured-collections/reddit-rag-2025)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qkupw7/we_did_rag_on_the_rrag_reddit_channel_free_to_use/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o19sp9g",
          "author": "Popular_Sand2773",
          "text": "Without any attribution it's hard to tell what's just world knowledge vs. actually driven by your retrieval so the value add is really unclear.",
          "score": 3,
          "created_utc": "2026-01-23 17:20:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1bpwwt",
              "author": "jannemansonh",
              "text": "You can see the references that are given back in the chat.",
              "score": 1,
              "created_utc": "2026-01-23 22:43:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1der3y",
                  "author": "Popular_Sand2773",
                  "text": "Sorry if it was there it wasn‚Äôt obvious.",
                  "score": 1,
                  "created_utc": "2026-01-24 04:33:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1dp6ac",
          "author": "Key-Contact-6524",
          "text": "Lovely bro , Thanks a ton",
          "score": 1,
          "created_utc": "2026-01-24 05:47:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ecmhn",
              "author": "jannemansonh",
              "text": "Sure! Happy you find it helpful!",
              "score": 1,
              "created_utc": "2026-01-24 09:12:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o19cako",
          "author": "lucido_dio",
          "text": "RAG-ception :D one step closer to singularity",
          "score": 1,
          "created_utc": "2026-01-23 16:06:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o19diah",
              "author": "jannemansonh",
              "text": "Haha, geeky jokes always make me laugh",
              "score": 1,
              "created_utc": "2026-01-23 16:11:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qgbm8d",
      "title": "Which one is better for GraphRAG?: Cognee vs Graphiti vs Mem0",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qgbm8d/which_one_is_better_for_graphrag_cognee_vs/",
      "author": "Imaginary-Bee-8770",
      "created_utc": "2026-01-18 15:35:07",
      "score": 15,
      "num_comments": 6,
      "upvote_ratio": 0.9,
      "text": "Hello everybody, appreciate any insights you may have on this\n\nIn my team we are trying to evolve from traditional RAG into a more comprehensive and robust approach: GraphRAG. We have a extensive corpus of deep technical documents such as  manuals and datasheets that we want to use to feed customer support agents.  \n  \nWe've seen there are a lot of OSS tools out there to work with, however, we don't know the limitations, ease-of-use, scalability and overall information about them. So, if you have a personal opinion about them and you've tried any of them before, we would be glad if you could share it with us.   \n  \nThanks a lot!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qgbm8d/which_one_is_better_for_graphrag_cognee_vs/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0bivda",
          "author": "astronomikal",
          "text": "Before jumping to tools, it might help to clarify what RAG is failing at for you. A few concrete questions that usually separate ‚Äúbetter RAG‚Äù from ‚Äúdifferent architecture‚Äù\n\nAre you mostly struggling with retrieval quality, or with reasoning across multiple documents?\n\nDo agents need to answer single-fact questions, or questions that require combining constraints across specs, revisions, and edge cases?\n\n When a customer issue is resolved, do you want the system to learn from that resolution, or is memory strictly static?\n\nDo answers need to be explainable/traceable beyond citations (e.g. why one constraint overrode another)?\n\nHow often do documents contradict or partially overlap (datasheet vs manual vs errata)?\n\nDo you need to model relationships (dependencies, incompatibilities, versions), or is chunk-level retrieval sufficient?\n\nIs latency predictability important (e.g. local-first, deterministic), or is cloud-scale recall the priority?",
          "score": 6,
          "created_utc": "2026-01-18 17:08:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0b6cxn",
          "author": "ubiquae",
          "text": "Hey, looking for similar feedback.\n\nMy feedback regarding graphiti is that it is a good starting point but it still lacks core capabilities.\n\nI am waiting for a PR to be reviewed to be able to work with custom entities and relationships since it does not work out of the box. And of course quality depends a lot on how the graph is being modelled so I can't understand why they haven't solved it yet.\n\nAlso, there is no evaluation ready, so there is no way to test it and have at least a basic idea about how it performs.\n\nFinally, the MCP server is pretty basic. Useful for demos but not leveraging all their capabilities.\n\nCognee sounds nice but it seems even more half baked. They probably are pushing their cloud offering. So ideas are great, implementation is not still there.",
          "score": 2,
          "created_utc": "2026-01-18 16:09:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0cz066",
              "author": "Unlucky_Comment",
              "text": "Also used graphiti and its a good starting point, but you grow out of it, which isn't a bad thing. Depending on your use case, you'll want to do something custom.",
              "score": 1,
              "created_utc": "2026-01-18 21:20:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0hoesw",
          "author": "Popular_Sand2773",
          "text": "As others mentioned it's not always a flat upgrade just a different way of doing things that excels in certain use cases. One of the reasons I ended up moving away from these guys was latency. For example if your customer support use case is using voice with a dense graph the latency can spike aggressively leading to awkward pauses that make people scream \"human\". You'll see they all dance around the problem of graph latency at scale. \n\nIf you are looking for something with graph like quality/behavior but actual guaranteed low latency and ANN vector db speeds then I would recommend checking out knowledge graph embeddings.",
          "score": 2,
          "created_utc": "2026-01-19 15:33:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0mx9v7",
          "author": "334578theo",
          "text": "If you can‚Äôt make an objective decision on this yourself then you probably don‚Äôt need GraphRAG",
          "score": 2,
          "created_utc": "2026-01-20 08:53:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0fvx7v",
          "author": "OnyxProyectoUno",
          "text": "Technical manuals and datasheets are tricky for graph approaches because the relationships you care about aren't always explicit in the text. Part numbers reference other parts, specs depend on operating conditions, procedures assume prior steps. Before picking a tool, worth mapping out what relationships actually matter for your support use case.\n\nCognee handles entity extraction and relationship building pretty well out of the box. Graphiti is more focused on temporal/conversational memory, less suited for static technical docs. Mem0 is really about user-level personalization, probably not what you need here.\n\nHave you validated that graph structure actually helps your retrieval? Sometimes dense technical docs benefit more from better chunking that preserves table structure and cross-references than from full knowledge graphs. What's failing with your current RAG setup that's pushing you toward graphs?",
          "score": 1,
          "created_utc": "2026-01-19 07:45:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qjf80l",
      "title": "Recommendations for cheaper alternatives to ElasticSearch",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qjf80l/recommendations_for_cheaper_alternatives_to/",
      "author": "shanukag",
      "created_utc": "2026-01-22 00:18:42",
      "score": 14,
      "num_comments": 24,
      "upvote_ratio": 0.94,
      "text": "Hi everyone,\n\nI‚Äôm building an AI-assisted search feature for an early-stage legal-tech platform and I‚Äôm looking for recommendations for cheaper alternatives to Elasticsearch that still work well for hybrid search use cases.\n\n# The challenge\n\nWe‚Äôre not doing traditional full-text search only. The system needs to support:\n\n* Keyword search\n* Vector similarity search (embeddings)\n* Filtering on metadata (jurisdiction, document type, status, etc.)\n* Reasonable relevance out of the box (I‚Äôd rather not hand-roll ranking logic)\n\nThe content itself is mostly static (guides and reference documents), and traffic is currently low since this is still early days  - but the search quality matters because it feeds into an LLM for AI-assisted answers.\n\n# What we‚Äôve implemented so far\n\n* Elasticsearch as the search layer\n* Hybrid search (keyword + vector)\n* Semantic-style retrieval for RAG workflows\n* Minimal custom scoring or tuning - mostly relying on built-in capabilities\n\nFrom a technical perspective, Elasticsearch works well. From a **cost perspective**, it feels hard to justify right now.\n\n# The problem\n\nEven at low usage, the baseline pricing and add-ons start to add up quickly. I‚Äôm trying to keep infrastructure spend sensible until there‚Äôs clearer traction, without completely downgrading search quality.\n\n# What I‚Äôm hoping to find\n\n* A more startup-friendly alternative to Elasticsearch\n* Supports keyword + vector search (or a realistic hybrid approach)\n* Can handle filters and structured metadata cleanly\n* Prefer managed or low-ops solutions\n* Not looking to fully custom-build a search engine unless there‚Äôs a strong reason\n\nIf you‚Äôve built something similar (hybrid search feeding LLMs) and had to balance cost vs relevance, I‚Äôd really appreciate any recommendations.\n\nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qjf80l/recommendations_for_cheaper_alternatives_to/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0yhjex",
          "author": "mwon",
          "text": "I assume you are using cloud and not community edition. Leave the cloud and rent a VM somewhere and use opensearch or milvus.",
          "score": 11,
          "created_utc": "2026-01-22 00:21:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o12ick1",
              "author": "vangapr",
              "text": "You can self host elasticsearch also, no licensing issues.",
              "score": 3,
              "created_utc": "2026-01-22 16:18:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yqkwo",
          "author": "raul3820",
          "text": "https://turbopuffer.com/",
          "score": 2,
          "created_utc": "2026-01-22 01:11:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0z3j5x",
          "author": "madbuda",
          "text": "Open search?? Foss fork if ES",
          "score": 2,
          "created_utc": "2026-01-22 02:24:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zcj47",
          "author": "anthony_doan",
          "text": "> We‚Äôre not doing traditional full-text search only.\n\nThen use postgres.\n\nElasticsearch and database that base on lucene are using a Trie data structure that is optimize for doc text search. \n\nIf you're not doing that then you shouldn't be using ES. The overhead of using is an undertaking. You either pay for managed services (cloud) or manage it yourself.\n\nI did RAG on Postgres with PGvector. It's fine and plus I also needed a RMDB which reduce the tech stack and complication.\n\nThe PGvector extension will vectorize for you and store vector and vector your query to compare the stored vectors.\n\nYou can either have Postgresql compare the vector or you can pull out the vector and use your programming language to compare it.",
          "score": 2,
          "created_utc": "2026-01-22 03:15:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0zzg5d",
              "author": "ContentSecret1203",
              "text": "Geez, pg and elasticsearch have completely different use cases. Pg only works if you have less than 100 million rows. We do that in less than a month",
              "score": 0,
              "created_utc": "2026-01-22 05:47:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0zzsw2",
                  "author": "anthony_doan",
                  "text": "> Geez, pg and elasticsearch have completely different use cases.\n\nYeah... I believe I alluded to that with the datastructure that Elasticsearch uses.\n\n> Pg only works if you have less than 100 million rows.\n\nThat doesn't mean anything if I don't know anything about your data. (edit: What type of data, how big it is, how it's structure)\n\n> We do that in less than a month\n\nCool story.\n\n---\n\nedit/update:\n\nI'm here trying to help and address OP's use case. I also gave my reasonings to back it up.\n\nHere come along some rando bragging about his accomplishment without any details nor does it address or attempt to address the user's situation. There's no alternative solution to mine. Just random vague stuff. There's no real counterpoint at all. \n\nIt screams me-me-me-me and the lack of technical and leadership skill.",
                  "score": 3,
                  "created_utc": "2026-01-22 05:50:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0zrr80",
          "author": "Curious-Sample6113",
          "text": "Manticore",
          "score": 2,
          "created_utc": "2026-01-22 04:52:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10rv34",
          "author": "Present-Reaction4344",
          "text": "Apache Solr",
          "score": 2,
          "created_utc": "2026-01-22 10:00:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o134zh8",
              "author": "crewone",
              "text": "Solr in every way is not up to date with opensearch or elastic. It doesn't make it bad per se,  but it is lagging in terms of features compared to the well maintained opensearch and elasticsearch.",
              "score": 1,
              "created_utc": "2026-01-22 17:59:40",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0z5l55",
          "author": "Infamous_Ad5702",
          "text": "I do semantic search, deterministic.\nTokens are too much so I built this\n\n\n* No gpu \n* No hallucination \n* Works offline.\n\n\nI got sick of embedding and chunking. My defence client needed to be offline. So I built Leonata. Happy to go through my process with you or anyone keen..",
          "score": 2,
          "created_utc": "2026-01-22 02:36:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10p6o2",
              "author": "cisspstupid",
              "text": "I would like to learn from you.",
              "score": 1,
              "created_utc": "2026-01-22 09:34:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o16iez7",
                  "author": "Infamous_Ad5702",
                  "text": "Great. I‚Äôll send an invite",
                  "score": 2,
                  "created_utc": "2026-01-23 04:20:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0yi1cg",
          "author": "SerDetestable",
          "text": "pgvector?",
          "score": 1,
          "created_utc": "2026-01-22 00:24:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ykxp7",
              "author": "Expensive_Culture_46",
              "text": "I audibly laughed because I had a baby data scientist try to tell me you can‚Äôt do vector in PG and this is the third time since then someone has mentioned it.",
              "score": 2,
              "created_utc": "2026-01-22 00:39:57",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0yp6lh",
          "author": "jba1224a",
          "text": "Does s3 vectors not work for your use case?\n\nIt is a little slower, but vastly cheaper. \n\nCaveat is you would need to write your own code for lookups, but it would be lightweight and easily done in python.",
          "score": 1,
          "created_utc": "2026-01-22 01:03:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yyxqf",
          "author": "hrishikamath",
          "text": "I used Postgres for finance rag over sec filings and earnings calls. Like 100k docs. Check it out: https://github.com/kamathhrishi/stratalens-ai",
          "score": 1,
          "created_utc": "2026-01-22 01:58:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zro93",
          "author": "bzImage",
          "text": "Qdrant",
          "score": 1,
          "created_utc": "2026-01-22 04:51:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1005yh",
          "author": "IdeaAffectionate945",
          "text": "How many records are we talking about? Thousands or millions?",
          "score": 1,
          "created_utc": "2026-01-22 05:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o103ggq",
          "author": "Party-Cartographer11",
          "text": "A bit on terminology...\n\n\nElasticsearch is usually used to indicate the free opensource Elasticsearch. I know there have been changes in the licensing, but pretty sure there a free versions.\n\n\nElastic is the paid version.\n\n\nSo maybe look at the OSS version.¬† Also, as others have stated, I have used Postgres with PG vector.¬† It works well and has some math built in (e.g. cosine for similarity).",
          "score": 1,
          "created_utc": "2026-01-22 06:19:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11x89t",
          "author": "digital_legacy",
          "text": "We built our own Java Vector engine on top of an older version of Elasticsearch\n\n[https://github.com/entermedia-community/entermedia-server/tree/main/src/org/entermediadb/ai/knn](https://github.com/entermedia-community/entermedia-server/tree/main/src/org/entermediadb/ai/knn)",
          "score": 1,
          "created_utc": "2026-01-22 14:38:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o126i2j",
          "author": "Past-Grapefruit488",
          "text": "What is teh issue with cost..  Hordware ? Hosting ? License ?",
          "score": 1,
          "created_utc": "2026-01-22 15:23:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o13s7vi",
          "author": "New_Animator_7710",
          "text": "One alternative perspective: if your corpus is mostly static and low-traffic, you might be over-indexing on ‚Äúsearch infra‚Äù too early. For legal RAG, **retrieval accuracy often comes more from parsing quality, chunking strategy, and reranking** than from the search engine itself.\n\nIn early stages, a simpler stack (Postgres + pgvector or even file-level indexes) plus a strong cross-encoder reranker can outperform Elastic at a fraction of the cost‚Äîand gives you flexibility before committing to heavyweight infra. Elastic shines at scale, but it‚Äôs not always the best *learning-phase* tool.",
          "score": 1,
          "created_utc": "2026-01-22 19:43:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0yl3mc",
          "author": "Expensive_Culture_46",
          "text": "What‚Äôs your expected user load? Maybe azure blob with AI search enabled. Then set for pay as you go.",
          "score": 0,
          "created_utc": "2026-01-22 00:40:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qg17pf",
      "title": "I cut my Claude Code costs by ~70% by routing it through local & cheaper models",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qg17pf/i_cut_my_claude_code_costs_by_70_by_routing_it/",
      "author": "Dangerous-Dingo-5169",
      "created_utc": "2026-01-18 06:31:55",
      "score": 14,
      "num_comments": 6,
      "upvote_ratio": 0.89,
      "text": "I love Claude Code, but using it full-time was getting expensive.\n\nSo I built **Lynkr**, a proxy that lets me:\n\n* Route some prompts to local models\n* Fall back to stronger models only when needed\n* Cache repeated prompts automatically\n\nResult: \\~60‚Äì80% lower costs depending on workload.\n\nIt‚Äôs open source and self-hosted:\n\n[https://github.com/Fast-Editor/Lynkr](https://github.com/Fast-Editor/Lynkr?utm_source=chatgpt.com)  \nIf you‚Äôre juggling multiple LLM providers, this might be useful ‚Äî feedback welcome.\n\nIt also supports Codex cli, [continue.dev](http://continue.dev), cursor pro, Cline etc",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qg17pf/i_cut_my_claude_code_costs_by_70_by_routing_it/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0e09ms",
          "author": "Economy-Manager5556",
          "text": "I spend less on Claude code by using it less... \nI gotta try that",
          "score": 5,
          "created_utc": "2026-01-19 00:28:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0f389j",
              "author": "Dangerous-Dingo-5169",
              "text": "Lol üòÇ",
              "score": 1,
              "created_utc": "2026-01-19 04:05:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0nhzuk",
          "author": "alokin_09",
          "text": "What local models are you running? And if I'm getting this right, you're still in the CC CLI but using local models?\n\nI've been using Kilo Code for most of my coding stuff (actually contributing to the project and working with their team), and it supports local models through Ollama and LM Studio. I use that sometimes when I wanna keep costs down. So how does your tool differ from that setup? Also curious if it could work with Kilo.\n\nCool idea btw :)",
          "score": 1,
          "created_utc": "2026-01-20 11:57:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p4mf2",
              "author": "Dangerous-Dingo-5169",
              "text": "Thanks alokin_09\nThis project also supports models hosted via ollama, llama.cpp and LM studio \nYes you would still be using cc cli or any other supoort ai tool ui itself\nAnd this project has a feature where the request automatically reroutes to a better model besides local llms based on the complexity of the question or if the local llms failed to answer it \nTo answer your question about kilocode \nI have not personally looked into it\n If its open ai compatible \nThen yes lynkr should support it otherwise we need to make the corresponding code changes \nCan you please raise a feature request issue on the repository \nThank you üòä",
              "score": 1,
              "created_utc": "2026-01-20 17:10:54",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o0plw2n",
              "author": "Dangerous-Dingo-5169",
              "text": "I just checked the documentation of kilocode and from the looks of it its openai compatible so lynkr should be able to work with kilocode",
              "score": 1,
              "created_utc": "2026-01-20 18:29:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0rgq0z",
          "author": "Ok-Responsibility734",
          "text": "Have you tried Headroom? https://github.com/chopratejas/headroom ?",
          "score": 1,
          "created_utc": "2026-01-20 23:49:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgh5nw",
      "title": "RAG Discovery Framework. It's a checklist of what to ask the client before writing any code",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qgh5nw/rag_discovery_framework_its_a_checklist_of_what/",
      "author": "not-so-boring",
      "created_utc": "2026-01-18 19:02:15",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.85,
      "text": "I've found this on Linkedin and it goes on about the importance of business understanding before building any project.\n\nIt's 42 items that you can map out to a decision matrix for the architecture.\n\nThe article with more details:¬†[https://thehyperplane.substack.com/p/why-the-hell-should-i-build-this](https://thehyperplane.substack.com/p/why-the-hell-should-i-build-this)",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qgh5nw/rag_discovery_framework_its_a_checklist_of_what/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qj38u8",
      "title": "Limits of File System Search (and Why you need RAG)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qj38u8/limits_of_file_system_search_and_why_you_need_rag/",
      "author": "rshah4",
      "created_utc": "2026-01-21 16:53:15",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Nice analysis comparing file system search (think Claude Code) or RAG (chunks). Filesystem works great with a small number of files, but as you get to adding more documents it doesn't scale as well.\n\n* 100 docs: FS takes 11.8 seconds compared to 9.9 for RAG \n* 1000 docs: FS takes 33 seconds compared to 8.4 for RAG\n\nIt's good reminder and data point for why we use RAG. Check our the post for lots more details: [https://www.llamaindex.ai/blog/did-filesystem-tools-kill-vector-search](https://www.llamaindex.ai/blog/did-filesystem-tools-kill-vector-search)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qj38u8/limits_of_file_system_search_and_why_you_need_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0w3rrm",
          "author": "No-Marionberry-772",
          "text": "you're cherry picking.\n\n\nTheir answer isnt use RAG, its, as it usually is, \"it depends\"\n\n\ntheir tests showed file system to be more accurate, so if you care more about accuracy than speed then FS wins.\n\n\nRAG also requires more setup, where FS just exists and is already supported by many tools, RAG requires the installation, integration and configuration of not 1 but many pieces of software, for performance gains that appear fairly marginal at a glance, but at massive scales that could matter.\n\n\nThey dont state the kind of hardware they tested on.¬† An HDD and an SSD are going to have completely different performance profiles, and they seem to assume that you cant optimize file system access performance, which may or may not be a valid depending on where the performance gaps come from.\n\n\nIts still good research however, they just needed to add a lot more detail, and people should read the whole article rather than cherry pick.",
          "score": 2,
          "created_utc": "2026-01-21 17:38:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0w93ws",
              "author": "rshah4",
              "text": "Fair enough, this is just one data point, so not saying we should only do RAG.  (The results did show RAG was better than file system, but its close and not a hill I would die on).   \nI do think there is a strong generalization though as your number of files increase (and size of files increase), you are going to need to move to some sort of search/retrieval approach like RAG.    \nI just liked this because it actually gave me a data point.  Now we just need more data points üòÄ",
              "score": 2,
              "created_utc": "2026-01-21 18:01:32",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xmr6i",
          "author": "seomonstar",
          "text": "Interesting. Fs is also severely hampered by file size. not to mention early context bloat from many files and potential issues with agents.",
          "score": 1,
          "created_utc": "2026-01-21 21:44:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ddzmp",
          "author": "Infamous_Ad5702",
          "text": "I haven‚Äôt pushed my tool to the edge yet on file size‚Ä¶I‚Äôll keep going",
          "score": 1,
          "created_utc": "2026-01-24 04:28:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhcvfp",
      "title": "Best practice for semantic/vector search",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qhcvfp/best_practice_for_semanticvector_search/",
      "author": "justrandombuddy",
      "created_utc": "2026-01-19 18:47:59",
      "score": 11,
      "num_comments": 15,
      "upvote_ratio": 0.87,
      "text": "I am very new to RAG & AI search in general. I‚Äôm building a **semantic (vector) search system**, not a RAG or answer-generation system.\n\nMy goal is **only to retrieve the correct article ID/title** from a fixed set of articles based on a user query. I do **not** need passage retrieval, summaries, or generated answers. Once I get the article ID, I fetch the full article from my primary database.\n\nEach article represents a single topic (e.g. driver‚Äôs license, banking, immigration, housing) and is scoped by metadata such as city, state, language, and immigration status (country-wide content).\n\nTypical article titles look like:\n\n* *Using Your Driver‚Äôs Licence in {some city}*\n* *Senior Support Services in {some city} for Citizens*\n* *Financial Help for Refugee Claimants in {some state}*\n\nTypical user queries look like:\n\n* ‚Äúdrivers license in {some city}‚Äù\n* ‚Äúhow to open bank account‚Äù\n* ‚Äúdocuments to become student‚Äù\n\nI‚Äôm currently deciding **what exactly should be embedded** in the vector database:\n\n**Option A:** Embed only the article **title**  \n**Option B:** Embed the **title + structured metadata** (city, state, status)  \n**Option C:** Embed the **full article text + metadata**\n\nKey constraints:\n\n* This is **pure semantic search**, not RAG\n* One result should map to **one article ID**\n* Articles are authoritative and static\n* Precision matters more than generating answers\n* Queries are often short and loosely phrased\n\nI‚Äôd love to hear:\n\n* What tends to work best in practice for this kind of lookup?\n* Is embedding full article content overkill if I only need ID-level retrieval?\n* Are there proven patterns for ‚Äúsemantic title search‚Äù with metadata?\n* Any gotchas with similarity thresholds or false positives?\n\nI have around 55k articles in total.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qhcvfp/best_practice_for_semanticvector_search/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o0j55wm",
          "author": "AsparagusKlutzy1817",
          "text": "The retrieval part works on chunks. You will need to chop down your documents into smaller unit to benefit from semantic similarity matching. On full documents this does not work. Usually people aim at 300 tokens leaning onto LLM terminology here.\n\nYou will get the topk best matches if you do not limit the topk you would essentially retrieve the entire database.\nYou can try to define some thresholds where you throw away results. This would work with cosine similarity which is in the range of 0..1\nFor the largest part of cases if you allow for 50 chunks you will get 50. There is no notion of ‚Äúnot similar enough anymore‚Äù\nThis is the LLM part you want to leave out.\n\nUse Postgres And pgvector this works also on local machines with 50k documents. You will need a chunker and also an embedder. Huggingface has both as tools to explore",
          "score": 3,
          "created_utc": "2026-01-19 19:29:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0mmgdh",
              "author": "Dan6erbond2",
              "text": "Why fetch topK when you can just LIMIT 1 and ORDER BY score? OP clearly said they just want the best match so once they fetch the chunk they can fetch its corresponding article, or JOIN it in the query. \n\nUnless you want to reduce an article's chunk scores to an average, there's no need for a topK at all.",
              "score": 1,
              "created_utc": "2026-01-20 07:14:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o0p530m",
                  "author": "justrandombuddy",
                  "text": "Actually, I do not need just the first result. I require topK articles back along with their ids",
                  "score": 1,
                  "created_utc": "2026-01-20 17:13:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o0p5ttg",
              "author": "justrandombuddy",
              "text": "I tried the chunking part along with the full article embedding. The results were not great and it seemed a bit of an overkill for my simple system. I am thinking of generating a summary of the article and embedding them with the metadata of the article for more context",
              "score": 1,
              "created_utc": "2026-01-20 17:16:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0paxi7",
                  "author": "AsparagusKlutzy1817",
                  "text": "A Single embedding for an article won‚Äôt work as you figured out. The summaries and embed approach is actually fine if the details users would search for are picked up in this summary. The additional LLM calls makes this more complicated than necessary but if this is what you need than this is absolutely fine.\nAre you going to LLM summarize everything and then embed it? This can become costly depending on the number of reruns.\n\nThe chunk on document blocks is less cost aggressive why this is the most spread one",
                  "score": 1,
                  "created_utc": "2026-01-20 17:40:10",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0jbvzo",
          "author": "Clay_Ferguson",
          "text": "the approach you would use depends on first whether or not you would consider using a small, local LLM to be able to simply ask the AI in real time , what the answer is to the multiple choice problem , of taking a block of text and then identifying all of the categories from a predefined list that you send into the prompt . \n\nalso, if you make your categories a hierarchy , then you could let the AI sort of drill down into that hierarchy , by picking the best multiple choice match at any level of the hierarchy based on what matches the text in the document .\n\nso, in other words, you can do this whole solution without any vector database at all , assuming that SLMs can at least excel in this simple task of matching a body of text to categories . it seems like a class of problem that small language models can do , although I've never done it . but it would be a simple and beautiful solution if it worked . you could run some test cases on small language models and see if they're able to do the multiple choice matching of categories against blocks of text .",
          "score": 1,
          "created_utc": "2026-01-19 20:00:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jokmz",
          "author": "Ecstatic_Heron_7944",
          "text": "It sounds like you just want to build a recommendation feature for articles.  \n  \nI would go with option C but forego chunking; literally just generate a vector using the whole article or otherwise use an LLM to summarize the article succinctly then generate from that. Chunking would work against you in this scenario as (1) likely search would be dominated by one or two articles narrowing the range of results and (2) many false positives due to not having enough article context.  \n  \nTo improve precision, using a reranker may make a lot of sense even though this is not your typical RAG Q&A use-case. This approach would probably require you generate short summaries for all articles however.",
          "score": 1,
          "created_utc": "2026-01-19 21:00:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p5g16",
              "author": "justrandombuddy",
              "text": "This seems to be a good approach. Basically, generate summary of the articles and embed it with metadata and title for proper context. Will try it out and let you know the results",
              "score": 1,
              "created_utc": "2026-01-20 17:14:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0k1vma",
          "author": "Fulgren09",
          "text": "Your approach sounds like it‚Äôs looking to retrieve the ID accurately more than the knowledge.¬†\n\nFor each document, when doing chunking, manually add the article ID as a text labelled ‚ÄúarticleID: _____‚Äù¬†\n\nThis way all your chunks have your ID¬†\n\nIdk if vector out of the box can handle this but sounds like you are building something custom. Good luck!",
          "score": 1,
          "created_utc": "2026-01-19 22:05:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0klfc1",
          "author": "Wimiam1",
          "text": "I‚Äôm actually working on a similar thing right now, although with less opportunity to use metadata. I may be able to help. How long are your articles and how are they generally structured?",
          "score": 1,
          "created_utc": "2026-01-19 23:46:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0p4n4f",
              "author": "justrandombuddy",
              "text": "My articles are \\~1500 characters long. They are generally structured by city/state/status or some permutations of these options. I am leaning a bit towards generating a summary of the articles and also provide the metadata inside the vector. Will try that out and see how the results go\n\nWhat's working for you?",
              "score": 1,
              "created_utc": "2026-01-20 17:10:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0r6est",
                  "author": "Wimiam1",
                  "text": "Ok 1.5K tokens is small enough that you might be able to get away without chunking at all, especially if each article is exclusively about a single topic. \n\nI‚Äôm jealous of your nice clear metadata situation lol. In my project, topics are a lot fuzzier. Since you have precise metadata, I‚Äôd try hard filtering results with a basic keyword search between the query and your metadata fields. Inserting them into your article before embedding *might* help find the correct article, but it will not prevent finding the wrong one. \n\nIn my experience, reranking is essential. You‚Äôll have to do the math on cost here though. Something like Zerank-2 is on the affordable end of the spectrum but still performs very well as far as I can see. It costs $0.025 per million tokens. So if passing 10 full 1.5K token articles to be reranked is an acceptable cost to you, then you‚Äôre good to go. If you need to cut costs, then you could see about chunking down so that the reranker needs to see less text. Zerank-2 can be run locally, so you can test all you want before setting up a paid API.\n\nI think a quick and cheap first test for you would be to embed each article as a whole with the title and everything. On query, filter by your metadata fields and then retrieve by vector search and BM25. Combine with RRF and then rerank the top 10 and see how that works for you. \n\nBefore sending stuff to the reranker, you could even prioritize that metadata by including it and the article title inside <context> ‚Ä¶ </context> tags at the beginning if you‚Äôre using Zerank-2. It‚Äôs also instruction following, so you could experiment with  instructions like ‚ÄúPrioritize geographical relevance‚Äù or something like that. \n\nOn second thought, if you do end up chunking, absolutely include the relevant metadata and article title at the beginning of each chunk before embedding. Anthropic calls this contextual chunking and it might help a lot to improve both precision and recall since I suspect you‚Äôll have a lot of cases where individual chunks look relevant but might not restate the location the parent article is about. You could even look into Late Chunking, but that‚Äôs a whole other ball of wax. \n\nJust start with a simple vector + bm25 search and reranking and metadata filtering and see how that works for you\n\nEDIT: I promise Zerank isn‚Äôt paying me lol. It‚Äôs just that new API rerankers that are also available open source are hard to find",
                  "score": 1,
                  "created_utc": "2026-01-20 22:53:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0lb10e",
          "author": "bzImage",
          "text": "Use qdrant.. save the metadata... filter by metadata..",
          "score": 1,
          "created_utc": "2026-01-20 02:05:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}