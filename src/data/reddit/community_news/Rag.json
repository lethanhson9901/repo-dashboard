{
  "metadata": {
    "last_updated": "2026-02-13 03:16:08",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 148,
    "file_size_bytes": 210527
  },
  "items": [
    {
      "id": "1r1o9qz",
      "title": "EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r1o9qz/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-11 05:01:24",
      "score": 211,
      "num_comments": 41,
      "upvote_ratio": 0.97,
      "text": "I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?\n\nTook the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) â€“ 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.\n\nWhat I built:\n\n\\- Full RAG pipeline with optimized data processing\n\n\\- Processed 2M+ pages (cleaning, chunking, vectorization)\n\n\\- Semantic search & Q&A over massive dataset\n\n\\- Constantly tweaking for better retrieval & performance\n\n\\- Python, MIT Licensed, open source\n\nWhy I built this:\n\nItâ€™s trending, real-world data at scale, the perfect playground.\n\nWhen you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.\n\nRepo: [https://github.com/AnkitNayak-eth/EpsteinFiles-RAG](https://github.com/AnkitNayak-eth/EpsteinFiles-RAG)\n\nOpen to ideas, optimizations, and technical discussions!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1r1o9qz/epsteinfilesrag_building_a_rag_pipeline_on_2m/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4s40uy",
          "author": "Negative-Age-4566",
          "text": "Cool project !   \nAs an improvement I'd suggest doing not only semantic search bu hybrid search using dense embeddings for semantic search + BM25 for keyword search.   \nHeres a tutorial on how to do it using qdrant [https://qdrant.tech/documentation/tutorials-search-engineering/reranking-hybrid-search/](https://qdrant.tech/documentation/tutorials-search-engineering/reranking-hybrid-search/)\n\n",
          "score": 21,
          "created_utc": "2026-02-11 11:11:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sdoe4",
              "author": "Cod3Conjurer",
              "text": "Appreciate it\n\nYeah, hybrid search (dense + BM25) makes a lot of sense, especially for exact entity lookups.\n\nI'll definitely explore that direction combining semantic retrieval with lexical scoring could improve precision quite a bit.",
              "score": 6,
              "created_utc": "2026-02-11 12:26:17",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rpukb",
          "author": "ShadowStormDrift",
          "text": "Recommend adding reviewer+corrector agents. \n\nReviewer double checks answers grounded in context and penalizes omissions. \n\nCorrector does what you think it does.\n\nIncreases latency but good when you need to be extra sure you aren't seeing bullshit.\n\nOtherwise providing a source list alongside responses for manual review",
          "score": 9,
          "created_utc": "2026-02-11 09:01:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tf1fk",
              "author": "Cod3Conjurer",
              "text": "never thought of it that way definitely gonna try",
              "score": 1,
              "created_utc": "2026-02-11 15:51:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o50yg30",
                  "author": "MrDetectiveSir",
                  "text": "How can you think of this? This is literally bare minimum",
                  "score": 1,
                  "created_utc": "2026-02-12 18:38:58",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4qzc2h",
          "author": "redditorialy_retard",
          "text": "Epstein bot incomingÂ ",
          "score": 9,
          "created_utc": "2026-02-11 05:07:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s6zr0",
              "author": "GreenHell",
              "text": "You mean [MechaEpstein-8000](https://www.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/)? Someone at /r/LocalLLaMA already did that.",
              "score": 6,
              "created_utc": "2026-02-11 11:36:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4qzijv",
              "author": "Cod3Conjurer",
              "text": "what?",
              "score": 0,
              "created_utc": "2026-02-11 05:08:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4ro0g1",
                  "author": "nikita2206",
                  "text": "As in chatbot with epstein personality, grounded in facts",
                  "score": 6,
                  "created_utc": "2026-02-11 08:44:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4t8di8",
          "author": "Tight-Actuary-3369",
          "text": "It's incredible, man. Keep it up, excellent work.",
          "score": 3,
          "created_utc": "2026-02-11 15:19:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4teczl",
              "author": "Cod3Conjurer",
              "text": "thanks man",
              "score": 1,
              "created_utc": "2026-02-11 15:48:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4vzz3x",
          "author": "Legitimate_Sherbet_7",
          "text": "I have built rag pipelines commercial and open source that can also do this . But from what I understand the most important information in these files is graphical in nature. The challenge is to sift through and draw conclusions on millions of pages of video graphical and text media. RAG alone is  not going to help do that on its own .",
          "score": 3,
          "created_utc": "2026-02-11 23:16:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xk95e",
              "author": "Cod3Conjurer",
              "text": "RAG is a retrieval layer, not a full investigative system. Multimodal indexing + structured extraction would be the next level.",
              "score": 1,
              "created_utc": "2026-02-12 05:05:58",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4z50ei",
                  "author": "Legitimate_Sherbet_7",
                  "text": "Yes very nice work. I did not know the dataset was available in txt format. It would be interesting to experiment with different persona layer prompts to look for certain types of patterns. Good luck with your continued development.",
                  "score": 2,
                  "created_utc": "2026-02-12 13:16:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rhhbj",
          "author": "FortiCore",
          "text": "Vibe coded ? Looks like result of a one or two prompts  \nsame for CrawlAI-RAG you posted yesterday.\n\nEven this post is AI generated",
          "score": 9,
          "created_utc": "2026-02-11 07:42:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ro5zy",
              "author": "Cod3Conjurer",
              "text": "Once you build a solid RAG pipeline, you can plug in any large dataset, it's mostly about preprocessing and tuning retrieval, not rewriting everything.\n\nSo no, it's not just \"two prompts.\" The architecture and pipeline matter.\n\nAnd yes, I use Al to help write code and posts, but I understand the full process and control the system end-to-end",
              "score": 4,
              "created_utc": "2026-02-11 08:45:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4uyf50",
                  "author": "UseMoreBandwith",
                  "text": "except, it is not 'solid'.   \nChunking is not correct. (no variable size, no chunking by paragraph, wordlength or sentence...)  \nThe cleaning is silly.  \nNo reranking.  \netc...etc..",
                  "score": 5,
                  "created_utc": "2026-02-11 20:10:45",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4rs3bm",
                  "author": "FortiCore",
                  "text": "Rudimentary cleaning, chunking, dedups and embedding is a basic functional RAG\nWouldnt call it a \"Solid RAG pipeline\"\n\nThe thing is AI would do this in 5 prompts, if not three (but it will do in three actually).\n\n**There isnt anything special the way the post makes it look like.**\n\nAll it does for \"**Constantly tweaking for better retrieval & performance**\" is : cleanup new lines, and spaces, 80 chars overlap embed in batch.Â \n\n\nTbvh, it not even at the level of a good RAG POC, its a demo for how to do rudimentary RAG with langchain, would fail to give satisfactory result for almost any real world usecase.\n\nThe post it self is clearly AI written to make it look like a worthy opensource RAG project\n\nStill helpful for some one who want to see RAG in its most simplest form.",
                  "score": 7,
                  "created_utc": "2026-02-11 09:23:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4rdzv3",
          "author": "-Cubie-",
          "text": "Very cool! What embedding model are you using?",
          "score": 2,
          "created_utc": "2026-02-11 07:09:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4redml",
              "author": "Cod3Conjurer",
              "text": "I'm using all-MiniLM-L6-v2 from SentenceTransformers, lightweight, fast, and solid for large-scale semantic retrieval.",
              "score": 5,
              "created_utc": "2026-02-11 07:13:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4rhxx3",
                  "author": "-Cubie-",
                  "text": "Nice, thanks",
                  "score": 2,
                  "created_utc": "2026-02-11 07:46:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xrt67",
          "author": "Otherwise-Platypus38",
          "text": "The source code does not have the proper layout of a FastAPI project. To have all the routes in main is the worst way to structure a FastAPI project. Vibe-coded I assume. Main reason is the format of the comments.",
          "score": 2,
          "created_utc": "2026-02-12 06:07:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zirqi",
              "author": "Cod3Conjurer",
              "text": "The current structure is more prototype-oriented than production-grade.  \nsome parts were AI-assisted, but the architecture decisions and debugging were mine",
              "score": 0,
              "created_utc": "2026-02-12 14:33:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4yyzlc",
          "author": "agilek",
          "text": "Deploy it somewhere, male it accessible for normal people and youâ€™ll get massive traffic! JMail lovers will go nuts.",
          "score": 2,
          "created_utc": "2026-02-12 12:37:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zosnt",
              "author": "Cod3Conjurer",
              "text": "Yeah, thatâ€™s the plan.  \nIf it gets enough traction and consistent usage, Iâ€™ll deploy it online and make it publicly accessible.",
              "score": 1,
              "created_utc": "2026-02-12 15:04:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4rd0zw",
          "author": "Extension_Armadillo3",
          "text": "I recommend BM25 to find passages with e.g Elon or something like that. And in addition, a re-ranker, but I think that could be hard to find without Zensurship.",
          "score": 4,
          "created_utc": "2026-02-11 07:01:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rducs",
              "author": "-Cubie-",
              "text": "Normal cross-encoder rerankers shouldn't censor, but they might be bad at parsing the e.g. email formats. I agree that BM25/Hybrid search would be cool, perhaps even with a slider denoting how much weight to give to the semantic vs lexical side (sometimes you might just want full lexical).",
              "score": 2,
              "created_utc": "2026-02-11 07:08:28",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4re823",
              "author": "Cod3Conjurer",
              "text": "I'm not using BM25 right now, it's pure semantic retrieval using embeddings (SentenceTransformers + Chroma similarity search).\n\n\nHybrid search with BM25 + vector reranking would definitely be an interesting improvement though.",
              "score": 1,
              "created_utc": "2026-02-11 07:11:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ryy75",
          "author": "nirmalyamisra",
          "text": "whats your hardware and tech stack",
          "score": 1,
          "created_utc": "2026-02-11 10:26:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s18z0",
              "author": "Cod3Conjurer",
              "text": "5060 8gb vram\n24 gb ram\nI7 14gen \n\nPython, LangChain, fastapi",
              "score": 4,
              "created_utc": "2026-02-11 10:47:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4s1ska",
                  "author": "nirmalyamisra",
                  "text": "thanks",
                  "score": 2,
                  "created_utc": "2026-02-11 10:51:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4uk9dt",
          "author": "degr8sid",
          "text": "Quick question! I'm a newbie with RAG and was wondering why didn't you finetune the model instead of RAG?",
          "score": 1,
          "created_utc": "2026-02-11 19:03:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o53fv25",
          "author": "PayMelodic3377",
          "text": "I think itâ€™s awesome youâ€™re open-sourcing this. Having spent 13 years at Google Legal Discovery and 16 years as a computer forensic examiner, I can tell you from experience: raw email data is messy, and AI safety guardrails make it dangerously polite.\n\nMost AI models are trained to be agreeable, not skeptical. They won't naturally assume a group sounds criminal or is hiding somethingâ€”they tend to treat document content as a \"fact\" by default. But in high-stakes truth-seeking, the text is often the smoke screen.\nIn my opinion, to make this truly groundbreaking, the RAG pipeline needs to incorporate a Dynamic Truth Table (a live Chronology), a people index that tracks people as objects so the system can do a better job of knowing J.E=Jeffery Epstein and also jevacation@gmail.com=Jeffery Epstein etc. \n\nThere are reports from back in the 90s that Trump would call into the news rooms under a fake name (always the same fake name) to spread rumors about himself and keep his name in the news cycle - something a living rolodex would help to normalize  \n\n\nWeâ€™ve seen this play out in real-time with the 2026 investigations in the UK and Belarus. For years, people like Prince Andrew claimed they had no contact with Epstein after 2009, but the latest document trickles are proving that wasn't true. Storing facts like Prince Andrew claims he has not contacted Epstein since 2009 would hopefully be able to identify where the document trove conflicts with Prince Andrews claims.  \n\nIf you can build a pipeline that is \"living and breathing,\" you could feed it those new jurisdictions' findings, estate records, and community-sourced photos as they surface. By grounding the AI in a constantly updating chronology rather than just a static vector database, you give the agent a way to \"cross-examine\" the documents.\nWhen the machine has a verified timeline to push back against the raw text, it moves from being a simple search tool to a forensic powerhouse.\n\nKeep it upâ€”this has the potential to be a world-class truth-seeking engine.",
          "score": 1,
          "created_utc": "2026-02-13 02:28:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4r03pl",
          "author": "eslove24",
          "text": "Whats your system prompt?",
          "score": 1,
          "created_utc": "2026-02-11 05:13:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4r7sjf",
              "author": "Cod3Conjurer",
              "text": "System prompt:\n\n\n\"You are a retrieval-based assistant. Answer ONLY using the provided context. If the answer is not present, say: 'I could not find this information in the documents.' Limit the answer to 5-6 lines.\"",
              "score": 3,
              "created_utc": "2026-02-11 06:15:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qxct95",
      "title": "My weekend project just got a $1,500 buyout offer.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "author": "Physical_Badger1281",
      "created_utc": "2026-02-06 09:08:40",
      "score": 75,
      "num_comments": 47,
      "upvote_ratio": 0.89,
      "text": "I built a simple RAG (AI) starter kit 2 months ago.\n\nThe goal was just to help devs scrape websites and PDFs for their AI chatbots without hitting anti-bot walls.\n\nProgress:\n- 10+ Sales (Organic)\n- $0 Ad Spend\n- $1,500 Acquisition Offer received yesterday.\n\nI see a lot of people overthinking their startup ideas. This is just a reminder that \"boring\" developer tools still work. I solved a scraping problem, put up a landing page, and the market responded.\n\nI'm likely going to reject the offer and keep building, but it feels good to know the asset has value.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3vl7fc",
          "author": "Available-Appeal-173",
          "text": "Did you open source it?",
          "score": 4,
          "created_utc": "2026-02-06 10:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vri0e",
              "author": "Physical_Badger1281",
              "text": "Itâ€™s built on open-source tech (Next.js, LangChain, Pinecone), but the repo itself is a paid boilerplate.\n\n\nI decided to sell it as a 'Source-Available' product because the real value isn't the stack, but the 100+ hours of glue-code (specifically the Puppeteer scraping config and Auth setup) that saves you from debugging for weeks.\nI kept the price super low ($5 range) so itâ€™s accessible to almost anyone who wants to skip the setup.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 5,
              "created_utc": "2026-02-06 11:02:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xrwyp",
                  "author": "Playwithme408",
                  "text": "Where did you list it.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:43:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vsick",
          "author": "Joy_Boy_12",
          "text": "I didn't understand the use case.\nI have a chatbot but I scrape the website with docling and insert the data to vector db so it will be available for my chatbot.\n\n\nWhere's the need for your product?",
          "score": 3,
          "created_utc": "2026-02-06 11:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vuh35",
              "author": "Physical_Badger1281",
              "text": "Great question. If you're scraping static documentation or PDFs, tools like Docling or standard parsers work perfectly.\n\nThe specific use case FastRAG solves is Client-Side Rendered (SPA) websites. A lot of modern React/Next.js sites return empty HTML until the JavaScript runs. Standard scrapers often fail there.\n\nFastRAG uses a headless browser instance (Puppeteer) to fully hydrate the DOM before scraping, so you catch the content that only appears after JS execution. Itâ€™s overkill for simple docs, but necessary for a robust 'Chat with Any Website' SaaS.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 6,
              "created_utc": "2026-02-06 11:27:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vy3ns",
              "author": "motorsportlife",
              "text": "Curious if you are you hashing files to ensure you don't scrape the same document twice and duplicate entries in the db",
              "score": 2,
              "created_utc": "2026-02-06 11:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w1kej",
                  "author": "Physical_Badger1281",
                  "text": "Currently, we rely on URL-based upserts to prevent database duplication (same URL = overwrite existing vectors)\n\nHowever, actual Content Hashing is on the roadmap for v1.5. It would definitely save on embedding costs for pages that haven't been updated. \n\nIs there a specific library you prefer for that flow?",
                  "score": 1,
                  "created_utc": "2026-02-06 12:20:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4cmf9j",
          "author": "Mary_Avocados",
          "text": "Who and how did you get the offer? Just random email?",
          "score": 3,
          "created_utc": "2026-02-09 00:49:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dh7tl",
              "author": "Physical_Badger1281",
              "text": "From indiemaker, Fastrag is listed there.",
              "score": 1,
              "created_utc": "2026-02-09 03:37:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4o6cei",
          "author": "Fragrant-Dark5656",
          "text": "Looks great âœ¨ How do you get buyers ? Do you do email marketing or social media ads or something else ?",
          "score": 3,
          "created_utc": "2026-02-10 19:43:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ourdb",
              "author": "Key-Boat-7519",
              "text": "Main thing is hanging where devs already complain about scraping: Reddit, Hacker News, specific Discords. Share quick demos, answer questions, link only when relevant. I use Gumroad for checkout, SimpleAnalytics for tracking, and Pulse for Reddit alerts to jump into fresh scraping threads fast.",
              "score": 2,
              "created_utc": "2026-02-10 21:37:10",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o407acm",
          "author": "finnomo",
          "text": "Launched projects cost at least 10 times more than that. Don't sell.",
          "score": 2,
          "created_utc": "2026-02-07 01:15:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40t1bl",
              "author": "Physical_Badger1281",
              "text": "Thanks for the sanity check! ðŸ™Œ\n\nSometimes when you're deep in the code, you forget how much effort it actually took to get from 'localhost' to 'live URL with Gumroad attached.'\n\nI definitely decided to hold. The validation of the offer was nice, but the asset is worth way more to me as a business.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-07 03:31:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hgu67",
          "author": "Dmoneybaby23",
          "text": "No offense but $1500 is honestly nothing, i get paid more than that to do nothing while working on projects, you should hold out and wait for your project to get bigger",
          "score": 2,
          "created_utc": "2026-02-09 19:26:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jwxx0",
              "author": "Physical_Badger1281",
              "text": "Yessss!!!\n\nGive it a try https://www.fastrag.live/",
              "score": 1,
              "created_utc": "2026-02-10 03:21:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iz2pt",
          "author": "SharpRule4025",
          "text": "The scraping part is honestly where most RAG projects hit the wall first. Everyone assumes you already have clean text but getting through Cloudflare and anti-bot stuff on modern sites is half the battle.\n\nSmart to build around that pain point. The people buying probably just want content for their vector store without fighting browser fingerprinting for a week.",
          "score": 2,
          "created_utc": "2026-02-10 00:03:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jx97o",
              "author": "Physical_Badger1281",
              "text": "Thanks!!!\nGive it a try [Fastrag](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-10 03:23:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lmdbh",
          "author": "AwayUnderstanding701",
          "text": "Congrats!!! Keep the good work and don't stop building!!",
          "score": 2,
          "created_utc": "2026-02-10 11:55:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lmgta",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!!!",
              "score": 1,
              "created_utc": "2026-02-10 11:56:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o50aczf",
          "author": "MaxMcregor",
          "text": "It is good to see that your Project is getting good fame even if it is primary. This is proof that simple, useful tools win. You solved a real problem, shipped fast, and the market validated and valued it.",
          "score": 2,
          "created_utc": "2026-02-12 16:46:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o50ckqz",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!!!\n\nGive it a try [Fastrag](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-12 16:56:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3z0dlp",
          "author": "AnxietyPrudent1425",
          "text": "You can go bigger. $1500 is barely worth the AI credits and food",
          "score": 2,
          "created_utc": "2026-02-06 21:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40sn10",
              "author": "Physical_Badger1281",
              "text": "100%. Thatâ€™s exactly why I turned it down.\n\nIf I just sell a couple of licenses a week, I beat that offer in a few months. Iâ€™d rather own a cash-flowing asset than take a quick $1.5k exit. I'm betting on the long game here!\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 03:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3w8289",
          "author": "StatusFoundation5472",
          "text": "True story",
          "score": 1,
          "created_utc": "2026-02-06 13:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w8fdz",
              "author": "Physical_Badger1281",
              "text": "Facts. Just gotta keep building! ðŸš€",
              "score": 1,
              "created_utc": "2026-02-06 13:05:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wcqtf",
          "author": "StatusFoundation5472",
          "text": "I checked out your landing page. A question please. What's your experience with gumroad?",
          "score": 1,
          "created_utc": "2026-02-06 13:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z0uxk",
              "author": "AnxietyPrudent1425",
              "text": "Lemon Squeezy is the way to go. Especially if you sell licenses.",
              "score": 2,
              "created_utc": "2026-02-06 21:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40tefk",
                  "author": "Physical_Badger1281",
                  "text": "Lemon Squeezy, okay I'll surely give it a try.\nThanks!",
                  "score": 2,
                  "created_utc": "2026-02-07 03:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wdpah",
          "author": "Interesting-Town-433",
          "text": "How are you able to run an llm on your site without getting crushed by gpu cost?",
          "score": 1,
          "created_utc": "2026-02-06 13:35:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3weh0l",
              "author": "Physical_Badger1281",
              "text": "Great question. Since this is utilizing OpenAI's API, there are no fixed GPU costs - it's purely pay-as-you-go.\n\nFor the live demo, I default to gpt-4o-mini, which is incredibly cheap. I also have rate limiting set up on Vercel to ensure no one user drains the API credits.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-06 13:40:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wg082",
          "author": "jennylane29",
          "text": "Great tool, wish you good luck!\nHow did you market your solution/landing page? I find getting it out there is particularly tricky.",
          "score": 1,
          "created_utc": "2026-02-06 13:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wj8l7",
              "author": "Physical_Badger1281",
              "text": "It really is the hardest part! For this project, I stuck to a 100% organic 'Build in Public' approach:\n- Reddit: Posting deep-dives on the tech stack (like why I chose Puppeteer over Cheerio) in subreddits like r/Nextjs, r/rag and r/SaaS.\n- Twitter/X: Sharing revenue milestones and 'behind the scenes' screenshots.\nI haven't spent a dime on ads. I think developers just appreciate seeing the code/process rather than a flashy marketing video",
              "score": 3,
              "created_utc": "2026-02-06 14:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xj3wf",
          "author": "TalosStalioux",
          "text": "Congrats dude",
          "score": 1,
          "created_utc": "2026-02-06 17:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xjoq4",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-06 17:03:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o434hzl",
          "author": "Top_Yogurtcloset_258",
          "text": "I might actually buy this, I've been trying to scrape client loaded data for a while using Firecrawl, and it's annoying. Have you used Firecrawl before?",
          "score": 1,
          "created_utc": "2026-02-07 14:43:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o434rek",
              "author": "Physical_Badger1281",
              "text": "No, but you can try the demo\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 14:45:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o436526",
                  "author": "Top_Yogurtcloset_258",
                  "text": "It gave an error when I put the link in the demo\n\nWebsite link I put: [https://partners.naeem.cg.sa/book/eleven-wishes-salon](https://partners.naeem.cg.sa/book/eleven-wishes-salon)  \n\n\nError scraping URL.\n\nError message:  \nAn error occurred with your deployment\n\nFUNCTION\\_INVOCATION\\_TIMEOUT\n\ndxb1::7fw5m-1770475864803-9e9fcb110063",
                  "score": 1,
                  "created_utc": "2026-02-07 14:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4h1vxu",
          "author": "NayanCat009",
          "text": "Please close the deal and get the money. Someone is going to build it in hours. Not demotivating or pulling down, this is the harsh reality of software development in the AI era.",
          "score": 1,
          "created_utc": "2026-02-09 18:16:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r129pa",
      "title": "Knowledge Distillation for RAG (Why Ingestion Pipeline Matters More Than Retrieval Algorithm)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r129pa/knowledge_distillation_for_rag_why_ingestion/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-10 14:31:03",
      "score": 57,
      "num_comments": 30,
      "upvote_ratio": 0.95,
      "text": "Been spending way too much time debugging RAG systems that \"should work\" but don't, and wanted to share something that's been bothering me about how we collectively approach this problem.\n\nWe obsess over retrieval algorithms (hybrid search, reranking, HyDE, query decomposition) while completely ignoring that retrieval operates over fundamentally broken representations of knowledge.\n\nI started using a new approach that is working pretty well so far : Instead of chunking, use LLMs at ingestion time to extract and restructure knowledge into forms optimized for retrieval:\n\nLevel 1: Extract facts as explicit SVO sentences\n\nLevel 2 : Synthesize relationships spanning multiple insights\n\nLevel 3 : Document-level summaries for broad queries\n\nLevel 4 : Patterns learned across the entire corpus\n\nEach level serves different query granularities. Precision queries hit insights. Exploratory queries hit concepts/abstracts.\n\nI assume this works well beacuse LLMs during ingestion can spendÂ *minutes*Â analyzing a document that gets used thousands of times. The upfront cost amortizes completely. And they're genuinely good at:\n\n* Disambiguating structure\n* Resolving implicit context\n* Normalizing varied phrasings into consistent forms\n* Cross-referencing\n\nTested this on a few projects involving financial document corpus : agent with distillation correctly identified which DOW companies were financial institutions, attributed specific risks with page-level citations, and supported claims with concrete figures. Naive chunking agent failed to even identify the companies reliably.\n\nThis is fully automatable with workflow-based pipelines:\n\n1. Table extraction (preserve structure via CV models)\n2. Text generation 1: insights from tables + text\n3. Text generation 2: concepts from insights\n4. Text generation 3: abstracts from concepts\n5. Text generation 4: table schema analysis for SQL generation\n\nEach component receives previous component's output. Final JSON contains original data + all distillation layers.\n\nAnyway figure this is one of those things where the industry is converging on the wrong abstraction and we should probably talk about it more.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r129pa/knowledge_distillation_for_rag_why_ingestion/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4miuy5",
          "author": "charlesrwest0",
          "text": "I have been using a similar approach. I also find it useful to have the model try to predict who would read it/what it would be used for and what associated questions are likely to be asked. Q/A pairs play very nicely with vector databases.",
          "score": 5,
          "created_utc": "2026-02-10 15:07:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ydr06",
              "author": "Independent-Cost-971",
              "text": "Totally agree. Q/A generation is basically intent distillation at ingestion time. Youâ€™re not just storing content,  youâ€™re storing how itâ€™s likely to be queried.",
              "score": 2,
              "created_utc": "2026-02-12 09:36:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mhk0p",
          "author": "penguinzb1",
          "text": "this resonates. the problem with chunking is it destroys the semantic boundaries that actually matter for retrieval. structuring at ingestion makes sense but the hard part is validating that your distillation pipeline is actually extracting what you need without hallucinating connections",
          "score": 3,
          "created_utc": "2026-02-10 15:01:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ydvof",
              "author": "Independent-Cost-971",
              "text": "Exactly. Chunking optimizes for token limits, not semantic coherence. And yeah validation is the hard part. Iâ€™ve been thinking about grounding each distillation layer back to source spans + enforcing traceability so nothing gets synthesized without citations.",
              "score": 2,
              "created_utc": "2026-02-12 09:37:23",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4myuxh",
              "author": "ButterflyEconomist",
              "text": "For the chunking problem, I overlay them.\n\nIf I want to process data abcd, I chunk it into ab, bc, cd",
              "score": 1,
              "created_utc": "2026-02-10 16:24:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mbw5i",
          "author": "Independent-Cost-971",
          "text": "Anyway, wrote this up in more detail if anyone's interested :Â [https://kudra.ai/knowledge-distillation-for-ai-agents-and-rag-building-hierarchical-knowledge-from-raw-documents/](https://kudra.ai/knowledge-distillation-for-ai-agents-and-rag-building-hierarchical-knowledge-from-raw-documents/)\n\n( shameless self promotion I know but worth a read )",
          "score": 6,
          "created_utc": "2026-02-10 14:31:40",
          "is_submitter": true,
          "replies": [
            {
              "id": "o4stcxg",
              "author": "True_Context_6852",
              "text": "Good read thank youÂ ",
              "score": 2,
              "created_utc": "2026-02-11 14:01:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4ydsij",
                  "author": "Independent-Cost-971",
                  "text": "Appreciate it! glad it resonated.",
                  "score": 1,
                  "created_utc": "2026-02-12 09:36:29",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mj0ww",
          "author": "minaminotenmangu",
          "text": "The trouble is cost. all knowledge now has to go through an llm + embedding model. I guess its not too bad for some, it might be intetesting to find cheaper models that could do the rewriting of knowledge.\n\ni feel we need a disconnect between what we embed which is a shorter simpler text to what we retrieve.",
          "score": 3,
          "created_utc": "2026-02-10 15:08:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4mk1nn",
              "author": "Popular_Sand2773",
              "text": "You just need to do knowledge distillation. You can get llm behavior at a fraction of the price on narrow tasks although the real savings is if you can move off of generative approaches all together while preserving the behavior. ",
              "score": 2,
              "created_utc": "2026-02-10 15:13:48",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4mkfjb",
                  "author": "minaminotenmangu",
                  "text": "What exactly do you mean by knowledge distillation? Ots never clear to me what this actually entails for a corpus.",
                  "score": 1,
                  "created_utc": "2026-02-10 15:15:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4nlp2e",
              "author": "isthatashark",
              "text": "100% on your suggestion to use cheaper models. I've been doing a lot of research into this lately and you don't need a frontier model to get good results. \n\nWe use this technique for memory consolidation in Hindsight. Smaller models do a surprisingly good job. I mostly use the ones on Groq because the performance is so fast and the cost is low, but Ollama is also an option if you want something local and free (but slower).",
              "score": 2,
              "created_utc": "2026-02-10 18:09:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4modds",
          "author": "DeepWiseau",
          "text": "How does this work with a growing document library? How would it handle several thousand pages being added a week?",
          "score": 2,
          "created_utc": "2026-02-10 15:34:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n1qwh",
              "author": "Krommander",
              "text": "Probably have to push towards recursive indexing and knowledge anchoring to earlier knowledge graphs already in store for smaller increments. Reset for the whole mapping layer in regular intervals if there needs to be pruning of older info or a redraw of conceptual attractors.",
              "score": 1,
              "created_utc": "2026-02-10 16:37:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n5psc",
          "author": "Informal_Tangerine51",
          "text": "Distillation at ingestion helps retrieval quality but creates a new debugging problem: when retrieval fails, which distillation layer broke?\n\nYour 4-level approach works until precision query returns wrong info. Now you need to trace: was SVO extraction wrong, relationship synthesis hallucinated, summary incomplete, or pattern recognition overgeneralized? Without evidence of what each distillation step produced, debugging is guesswork. LLMs spent minutes analyzing doc, but you don't have artifacts showing what they extracted at each level.\n\nFinancial doc example: agent identified wrong risk attribution. Is it because Level 1 extracted wrong facts, Level 2 synthesized wrong relationships, or retrieval chose wrong layer? Chunking is simpler to debug - you see exactly what text was retrieved. Multi-layer distillation optimizes for quality but trades debuggability. Production systems need both retrieval quality and incident traceability.",
          "score": 2,
          "created_utc": "2026-02-10 16:55:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye2dj",
              "author": "Independent-Cost-971",
              "text": "Totally fair. Thatâ€™s why I treat each distillation layer as a first-class artifact, persisted, versioned, and fully traceable back to source spans. If retrieval fails, you can inspect exactly what Level 1 extracted, what Level 2 synthesized, etc.\n\nIâ€™d argue chunking feels easier to debug because itâ€™s shallow, not because itâ€™s better, you see the text, but you donâ€™t see the reasoning gap. Distillation just makes the abstraction explicit, so you have to engineer observability into it.",
              "score": 1,
              "created_utc": "2026-02-12 09:39:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4meyku",
          "author": "fabkosta",
          "text": "Whatâ€™s a CV model in the context of table extraction?",
          "score": 1,
          "created_utc": "2026-02-10 14:47:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye40a",
              "author": "Independent-Cost-971",
              "text": "CV = Computer Vision. In table extraction it usually refers to vision models (like layout detection or table structure recognition models) that identify table boundaries, rows, columns, and cell relationships from PDFs or scanned docs before OCR/LLM processing.",
              "score": 1,
              "created_utc": "2026-02-12 09:39:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4n0uqg",
          "author": "Krommander",
          "text": "Very nice. I have found distillation to condense well into a recursive semantic hypergraph, as a map of the knowledge and interrelations.",
          "score": 1,
          "created_utc": "2026-02-10 16:33:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye6up",
              "author": "Independent-Cost-971",
              "text": "Thatâ€™s a great way to frame it. recursive semantic hypergraph is exactly the right abstraction. Once you move beyond flat chunks, knowledge naturally becomes nodes + higher-order relations. Distillation is basically constructing that graph explicitly instead of hoping embeddings infer it implicitly.",
              "score": 2,
              "created_utc": "2026-02-12 09:40:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4yyy13",
                  "author": "Krommander",
                  "text": "Yes exactly, by building your recursive semantic hypergraphs based on validated synthesis of a subject, you are able to drop hallucinations and shorten latency.\n\n\nMemory modules with this architecture are compact but deliver a high quality restoration of facts.Â \n\n\nThere are some interesting publications on HyperRAG I stumbled upon this summer, that demonstrate the quality and speed of recalls when using the hypergraphs to index the complete sources, instead of flat pair embeddings.Â ",
                  "score": 1,
                  "created_utc": "2026-02-12 12:37:38",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4zt7b2",
                  "author": "Krommander",
                  "text": "Feng, Y., Hu, H., Hou, X., et al. (2025). Hyper-RAG: Combating LLM Hallucinations using Hypergraph-Driven Retrieval-Augmented Generation. arXiv preprint arXiv:2504.08758.\n\n[https://doi.org/10.48550/arXiv.2504.08758](https://doi.org/10.48550/arXiv.2504.08758)\n\nGarcÃ­a, F. G., Shi, Q., & Feng, Z. (2025). Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification. arXiv preprint arXiv:2509.05741.\n\n[https://arxiv.org/pdf/2509.05741](https://arxiv.org/pdf/2509.05741)\n\nHan, H., Wang, Y., Shomer, H., Guo, K., Ding, J., Lei, Y., Halappanavar, M., Rossi, R. A., Mukherjee, S., Tang, X., He, Q., Hua, Z., Long, B., Zhao, T., Shah, N., Javari, A., Xia, Y., & Tang, J. (2025). Retrieval-Augmented Generation with Graphs (GraphRAG). arXiv preprint arXiv:2501.00309.\n\n[https://arxiv.org/pdf/2501.00309](https://arxiv.org/pdf/2501.00309)\n\nHuang, C., Huang, H., Yu, T., Xie, K., Wu, J., Zhang, S., Mcauley, J., Jannach, D., & Yao, L. (2025). A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms. arXiv preprint arXiv:2504.16420.\n\n[https://arxiv.org/pdf/2504.16420](https://arxiv.org/pdf/2504.16420)\n\nLuo, H., Chen, G., Zheng, Y., et al. (2025). HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation. arXiv preprint arXiv:2503.21322.\n\n[https://doi.org/10.48550/arXiv.2503.21322](https://doi.org/10.48550/arXiv.2503.21322) \n\nLuo, H., E, H., Chen, G., Lin, Q., Guo, Y., Xu, F., Kuang, Z., Song, M., Wu, X., Zhu, Y., & Tuan, L. A. (2025). Graph-R1: Towards Agentic Graphrag Frame-Work Via End-To-End Reinforcement Learning. arXiv preprint arXiv:2507.21892.\n\n[https://arxiv.org/pdf/2507.21892](https://arxiv.org/pdf/2507.21892)\n\nSharma, K., Kumar, P., & Li, Y. (2024). OG-RAG: Ontology-grounded retrieval-augmented generation for large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025).\n\n2025.emnlp-main.1674.pdf\n\nWang, C., Deng, W., Guan, W., Lu, Q., & Jiang, N. (2025). Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering. arXiv preprint arXiv:2508.11247.\n\n[https://arxiv.org/pdf/2508.11247](https://arxiv.org/pdf/2508.11247)\n\nZhu, Z., Huang, T., Wang, K., Ye, J., Chen, X., & Luo, S. (2025). Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey. arXiv preprint arXiv:2504.10499.\n\n[https://arxiv.org/pdf/2504.10499](https://arxiv.org/pdf/2504.10499)",
                  "score": 1,
                  "created_utc": "2026-02-12 15:26:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4n8j4q",
          "author": "Forsaken-Cod-4944",
          "text": "I'm new to this but wouldnt it take significantly more time to produce chunks with this (depends on llm i guess?)\n\n",
          "score": 1,
          "created_utc": "2026-02-10 17:08:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4njusw",
              "author": "UBIAI",
              "text": "I think the distillation happens at the document level, not the chunk level.",
              "score": 2,
              "created_utc": "2026-02-10 18:00:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4njxj3",
          "author": "isthatashark",
          "text": "We had to tackle a similar problem in Hindsight. I just published a blog post about it yesterday on how we do memory consolidation to handle this: https://hindsight.vectorize.io/blog/2026/02/09/resolving-memory-conflicts",
          "score": 1,
          "created_utc": "2026-02-10 18:01:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xw71l",
          "author": "SharpRule4025",
          "text": "This matches what I've been seeing. Spent weeks tuning rerankers and hybrid search, then realized the chunks themselves were garbage because the source extraction was throwing navigation, sidebars, and footer text into the same chunks as actual content.\n\nSwitched to extracting content into structured fields (headings, paragraphs, lists, metadata) before chunking and the retrieval quality jumped immediately. No reranker changes needed. The hierarchy gives you natural chunk boundaries instead of arbitrary token windows.\n\nThe part about knowledge distillation at ingestion time is interesting. Basically front-loading the intelligence into the pipeline instead of hoping retrieval figures it out. Feels obvious in hindsight but most RAG tutorials skip this entirely and jump straight to vector search tuning.",
          "score": 1,
          "created_utc": "2026-02-12 06:46:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ye8ui",
              "author": "Independent-Cost-971",
              "text": "Exactly. Most RAG issues Iâ€™ve seen werenâ€™t retrieval problems, they were representation problems. If the chunks mix navigation noise with signal, no reranker will save you.\n\nOnce you respect document structure, retrieval suddenly looks â€œsmartâ€ without changing the algorithm. Distillation just pushes that idea further; fix the knowledge representation first, then worry about search.",
              "score": 1,
              "created_utc": "2026-02-12 09:41:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qzxttv",
      "title": "I feel semantic search is overused",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzxttv/i_feel_semantic_search_is_overused/",
      "author": "pskd73",
      "created_utc": "2026-02-09 07:24:05",
      "score": 37,
      "num_comments": 43,
      "upvote_ratio": 0.93,
      "text": "I understand semantic search was the basis for RAG along with graph search. That's how we started making the search possible with AI. \n\nBut as the LLMs got so better, I feel we should take a step back and appreciate the simple text search tools as well. In one of my projects, it was doing good with just semantic search. Recently got an issue where it could not find information about code related question.\n\nThe text was right there, but you know the semantic search stuff, it doesn't pick it up if there is no enough information even though it is just there. That's when I took a step back and just did a regular text base regex search tools.\n\nPoint it, the LLMs are a lot better and they try multiple combinations and eventually they find out as we expect them to. This for sure helps in codebases because there it is more of keywords. I immediately made this improvement live and its giving amazing results.\n\nAnybody did this already? any thoughts?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzxttv/i_feel_semantic_search_is_overused/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4edaae",
          "author": "lupin-the-third",
          "text": "Most people are using a combo of semantic and keyword search and then reranking.",
          "score": 18,
          "created_utc": "2026-02-09 07:45:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edi66",
              "author": "pskd73",
              "text": "I see. Are they just tools for the LLMs? And is the regex based search on text?",
              "score": 2,
              "created_utc": "2026-02-09 07:47:15",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4eekw5",
                  "author": "lupin-the-third",
                  "text": "You can make a simple react agent, that will have access to a semantic search and a keyword search tool. Depending on what kind of input comes in, you'll probably need an LLM to reshape the input sentences/keywords for regex to get better output.\n\nFor code, it's not very semantically friendly - comments are just about the only thing that might get some hits - so interacting with it is fine with just grep/find. You can create a few different approaches to searching:   \n \\- Use find to get a directory structure, then ask it to use targeted greps using this directorry structure to find valid files and get surround chunks.   \n \\- Simple regex grep on everything in a shotgun blast approach with multiple combinations of what it could be: For example if you ask \"How does auth work here\" You'll want it to break down into stuff like \"auth\" \"authorization\" \"login\" \"token\", to get a wide breadth of information.",
                  "score": 2,
                  "created_utc": "2026-02-09 07:57:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4efhfe",
          "author": "ComputeLanguage",
          "text": "This is why HyDe became a popular paper :)",
          "score": 7,
          "created_utc": "2026-02-09 08:06:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ed5ex",
          "author": "lez566",
          "text": "I do a combo of semantic search and keyword search and combine them.",
          "score": 3,
          "created_utc": "2026-02-09 07:43:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4edft6",
              "author": "pskd73",
              "text": "Good to know that. How exactly do you combine them? They are passed as tools to the LLMs?",
              "score": 1,
              "created_utc": "2026-02-09 07:46:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4edl73",
                  "author": "lez566",
                  "text": "Yes it alls gets merged into one context and then passed to the LLM",
                  "score": 3,
                  "created_utc": "2026-02-09 07:48:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4fe825",
          "author": "Informal_Tangerine51",
          "text": "Hybrid search (semantic + keyword) is standard now but you're hitting the real production issue: when retrieval fails, debugging is archaeology.\n\nSemantic search missed code reference. Regex found it. Great, but when this happens at scale across different query types, how do you know which retrieval method to trust? Users don't tell you \"use keyword search\" - they ask questions and expect answers.\n\nThe problem compounds: query X needs semantic, query Y needs keyword, query Z needs both. LLM tries combinations and \"eventually finds it\" means wasted retrieval calls and latency. More importantly, when it still fails after trying combinations, you can't debug why without logging what each method returned and why the LLM chose path A over B.\n\nWe saw this exact issue. Added BM25 alongside vector search. Results improved but incidents became harder to debug. \"It found the wrong doc\" doesn't tell you which retrieval method failed or why fusion chose incorrectly.\n\nFor production RAG: are you capturing retrieval results from both methods plus fusion decisions? Or just measuring end result quality?\n\nMulti-method retrieval helps accuracy. Evidence capture makes failures debuggable.",
          "score": 3,
          "created_utc": "2026-02-09 13:11:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fjop2",
              "author": "pskd73",
              "text": "True that. Yes, https://github.com/crawlchat/crawlchat records almost all details like, the search queries, search types, scores, pages retrieved and other things. They help in fine tuning the process",
              "score": 1,
              "created_utc": "2026-02-09 13:44:45",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4fly79",
              "author": "Wide_Brief3025",
              "text": "Capturing detailed logs for both retrieval methods and their fusion outputs is essential for debugging RAG in production. Setting up structured evidence capture lets you see where breakdowns happen. If alerting on specific keywords and tracking conversations across forums helps, ParseStream has tools that could surface those scenarios in real time to help you intervene faster.",
              "score": 1,
              "created_utc": "2026-02-09 13:57:51",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4mfs9r",
              "author": "code_vlogger2003",
              "text": "Have you tried a mixture of encoder's concept of super linked where the high level statement is that if the user search depends on multiple search fields then the routing slm decides how much weight i need to put on each encoder result.",
              "score": 1,
              "created_utc": "2026-02-10 14:52:08",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ega37",
          "author": "xeraa-net",
          "text": "Definitely infuriating for users if they know something exists and it's simply not found on a direct match.\n\nEither regex (maybe more so on the code side) or BM25 will give a solid baseline and there's a reason why most of the discussion has moved on to hybrid search.",
          "score": 2,
          "created_utc": "2026-02-09 08:13:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eh0pt",
              "author": "pskd73",
              "text": "Yeah. Good that I am pretty hybrid now :)",
              "score": 1,
              "created_utc": "2026-02-09 08:21:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4eop6m",
          "author": "chungyeung",
          "text": "Welcome to the big o notation engineering world.",
          "score": 2,
          "created_utc": "2026-02-09 09:37:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4eppmz",
              "author": "pskd73",
              "text": "I didnâ€™t get it haha",
              "score": 1,
              "created_utc": "2026-02-09 09:47:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4eqsc1",
          "author": "exaknight21",
          "text": "BM25 + Lexical + knowledge graphs is de wei.",
          "score": 2,
          "created_utc": "2026-02-09 09:58:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4er9e4",
              "author": "pskd73",
              "text": "Nice, how do you handle the BM25? What tools to do this? Also, how do you make the knowledge graphs? I have experimented with Knowledge Graphs my way and found it doesn't add much value next to semantic + regex based text search",
              "score": 1,
              "created_utc": "2026-02-09 10:03:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ewbpv",
          "author": "RobertLigthart",
          "text": "yea honestly hybrid is the move but even then people overcomplicate it. for most use cases just doing BM25 + vector search with a reranker on top gets you like 90% of the way there... seen too many people jump straight into knowledge graphs and complex pipelines when their actual problem was just bad chunking",
          "score": 2,
          "created_utc": "2026-02-09 10:51:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ey0j7",
              "author": "pskd73",
              "text": "True that. Chunking is critical. At least I have made sure the headings, table headers are carried over.\n\nIn my experience knowledge graphs add very little improvement and a lot of cost involved.\n\nI have reranker too!",
              "score": 1,
              "created_utc": "2026-02-09 11:07:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ewrs4",
          "author": "AICodeSmith",
          "text": "strongly relate, in our BlackSmith multi-agent research platform, we also found that pure semantic search can miss obvious matches, especially in code or structured financial data. We now let LLMs orchestrate *when* to use semantic, graph, or simple keyword/regex search, which has been far more reliable curious if others are moving toward LLM-driven hybrid retrieval as well.",
          "score": 2,
          "created_utc": "2026-02-09 10:55:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ey7dm",
              "author": "pskd73",
              "text": "True that. Thats what I do it in [crawlchat](https://github.com/crawlchat/crawlchat)",
              "score": 1,
              "created_utc": "2026-02-09 11:08:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4f4ezz",
          "author": "joey2scoops",
          "text": "One size does NOT fit all.",
          "score": 2,
          "created_utc": "2026-02-09 12:01:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f4ofe",
              "author": "pskd73",
              "text": "Yup, for sure",
              "score": 1,
              "created_utc": "2026-02-09 12:03:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ffs2o",
          "author": "Live-Guitar-8661",
          "text": "That's exactly what we are doing, and the results have been great. Vector DBs just feel too heavy at this point. I'm focused more on giving the agents the tools they need to find the information, instead of try and have some heavy compute figure it out.",
          "score": 2,
          "created_utc": "2026-02-09 13:21:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fjted",
              "author": "pskd73",
              "text": "Yes, thats the way to go. I found this is thr bedt workflow",
              "score": 2,
              "created_utc": "2026-02-09 13:45:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4fouba",
          "author": "hrishikamath",
          "text": "I mean even Claude code uses pure regex/keyword search but it takes quite a while, semantic search with re ranking works well for my use case.",
          "score": 2,
          "created_utc": "2026-02-09 14:14:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4fp6c4",
              "author": "pskd73",
              "text": "It works but not the best. You can make it better by providing it more ways of exploring the KB as mentioned above",
              "score": 1,
              "created_utc": "2026-02-09 14:16:27",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o51zjnj",
              "author": "Free-Ferret7135",
              "text": "Regex is easier and more deterministic when you are just dealing with code.",
              "score": 1,
              "created_utc": "2026-02-12 21:35:22",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4kh01p",
          "author": "ApprehensiveYak7722",
          "text": "I am currently using docling for processing pdf and could see that it was extracting all headings in level 2 (##) only and I overcome that with a solution somehow and the next part is even when I say max-tokens = 1024 still few tables or few chunks have crossed that limit.\n\nFew chunks are having both tables and text in the chunk.\n\nFurther, I donâ€™t understand how to proceed further. Is there any best method to overcome where using that tool should easily find tables and chunk them and differentiate text, tables.",
          "score": 2,
          "created_utc": "2026-02-10 05:40:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ki265",
              "author": "pskd73",
              "text": "I have done something very similar on https://github.com/crawlchat/crawlchat\n\nYou may check it out here - https://github.com/crawlchat/crawlchat/blob/main/source-sync/src/scrape/markdown-splitter.ts",
              "score": 1,
              "created_utc": "2026-02-10 05:49:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4tvd5g",
          "author": "BarrenLandslide",
          "text": "Just use something like Qdrant. It is going to boost your retrieval a lot.",
          "score": 1,
          "created_utc": "2026-02-11 17:07:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxm3z3",
      "title": "I tested Opus 4.6 for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "author": "midamurat",
      "created_utc": "2026-02-06 16:21:47",
      "score": 37,
      "num_comments": 12,
      "upvote_ratio": 0.95,
      "text": "I just finished comparing the new Opus 4.6 in a RAG setup against 11 other models.\n\n  \nThe TL;DR results I saw:\n\n* **Factual QA** king: It hit an 81.2% win rate on factual queries\n* **vs. Opus 4.5:** Massive jump in synthesis capabilities (+387 ELO), it no longer degrades as badly on multi-doc queries\n* **vs. GPT-5.1:** 4.6 is more consistent across the board, but GPT-5.1 still wins on deep, long-form synthesis.\n\nVerdict: I'm making this my default for source-critical RAG where accuracy is more imprtant than verbosity.\n\nHappy to answer questions on the data or methodology!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3xv27r",
          "author": "chearmstrong",
          "text": "Using a top-tier model for answer generation in RAG is often unnecessary.\n\nOnce retrieval quality is high and you have post-retrieval steps (relevance filtering / re-ranking, dedupe, metadata filters, chunk stitching), the generatorâ€™s job is mostly summarise, structure, and stay grounded. A cheaper/faster model is usually sufficient.\n\nCommon best practice weâ€™ve seen work well:\n\n- Spend compute on retrieval quality (chunking, filters, re-ranking), not the final generator.\n- Use a fast default generator, and only escalate to a stronger model when signals suggest itâ€™s needed (low relevance scores, sparse matches, high ambiguity, multi-doc synthesis).\n- Treat generation as a formatting + synthesis step, not the place to â€œfixâ€ weak retrieval.\n\nIn other words: if you need a very powerful model to get good answers, thatâ€™s often a retrieval problem, not a generation one.",
          "score": 39,
          "created_utc": "2026-02-06 17:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yswzx",
              "author": "midamurat",
              "text": "that's right, i agree. and in this comparison , models were under fixed retrieval + reranking to keep it fair",
              "score": 0,
              "created_utc": "2026-02-06 20:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xrmzb",
          "author": "One_Milk_7025",
          "text": "What is the factual rag means? If the retrieval is perfect any llm can answer that right? How opus is better ? Multi loop or making hyde method to create hypothetical question? For factual rag llm is not needed if the fact checking and rag pipeline is good enough..",
          "score": 3,
          "created_utc": "2026-02-06 17:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ytrk6",
              "author": "midamurat",
              "text": "for factual rag, scifact dataset was used. in theory, what you say might work but in practice, even with the same docs, models differ (like, some over generalize or hide uncertainty). Opus 4.6 was more conservative meaning it actually sticked closer to the source than others",
              "score": -1,
              "created_utc": "2026-02-06 20:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xwfue",
          "author": "Informal-Resolve-831",
          "text": "So how much better it is really?",
          "score": 2,
          "created_utc": "2026-02-06 18:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ysg1n",
              "author": "midamurat",
              "text": "main gain is multi-doc synthesis which is about +387 ELO vs 4.5, much less degradation when sources overlap. or disagree. \n\n(and elo is score from pairwise model-vs-model comparisons using an LLM judge)",
              "score": 1,
              "created_utc": "2026-02-06 20:40:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43s9dj",
          "author": "my_byte",
          "text": "Imagine using the most expensive, premium frontier model for RAG",
          "score": 1,
          "created_utc": "2026-02-07 16:43:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xg60a",
          "author": "Legitimate-Leek4235",
          "text": "How about Gemini 3.0 flash ? Want to keep costs low and sacrifice bit on quality. How did you measure this ?",
          "score": 1,
          "created_utc": "2026-02-06 16:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xoyhe",
              "author": "midamurat",
              "text": "gemini 3 flash was also very good when I tested especially in terms of being strong in factual RAG. wrote about that too a while ago: [https://agentset.ai/blog/gemini-3-flash](https://agentset.ai/blog/gemini-3-flash) \n\n  \n",
              "score": 0,
              "created_utc": "2026-02-06 17:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xau2g",
          "author": "midamurat",
          "text": "if interested in detail writeup: [https://agentset.ai/blog/opus-4.6-in-rag](https://agentset.ai/blog/opus-4.6-in-rag)",
          "score": 0,
          "created_utc": "2026-02-06 16:22:29",
          "is_submitter": true,
          "replies": [
            {
              "id": "o44wiyl",
              "author": "Single-Constant9518",
              "text": "Nice find! That writeup looks detailed. What specific aspects of the new Opus 4.6 did you find most impressive in your testing?",
              "score": 2,
              "created_utc": "2026-02-07 20:05:27",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o48u6a6",
                  "author": "midamurat",
                  "text": "Thank you!  \nI was impressed by how big of an upgrade there was from Opus 4.5 in multi doc queries. Opus 4.6 is much better when reasoning across many docs. Also, it is noticeably better at *not* over-answering. ",
                  "score": 1,
                  "created_utc": "2026-02-08 12:56:22",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qzj9j4",
      "title": "From RAG-powered LLMs to autonomous agents: the real AI production stack in 2026",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qzj9j4/from_ragpowered_llms_to_autonomous_agents_the/",
      "author": "devasheesh_07",
      "created_utc": "2026-02-08 20:06:03",
      "score": 34,
      "num_comments": 7,
      "upvote_ratio": 0.82,
      "text": "Over the last few years, most AI conversations revolved around bigger models, benchmarks, and chatbots.\n\nBut looking ahead to 2026, the real shift isnâ€™t just smarter LLMs â€” itâ€™s how AI is actually being used in production. AI is moving from assistive software to something closer to a digital workforce.\n\nHereâ€™s whatâ€™s standing out to me ðŸ‘‡\n\n1. AI is executing work, not just answering prompts\n\nEnterprises are already running AI-powered, end-to-end workflows for billing, customer support, document processing, reporting, and internal ops.\n\nThis isnâ€™t experimentation or demos anymore â€” itâ€™s workflow automation at scale.\n\n\n\n2. RAG is becoming the default architecture\n\nRetrieval-Augmented Generation isnâ€™t optional if you care about accuracy, grounding, or compliance.\n\nConnecting LLMs to internal documents, databases, and vector stores significantly reduces hallucinations and makes AI usable in regulated environments.\n\n\n\n3. LLM progress is about reasoning, not raw size\n\nModel scale mattered early on, but real value now comes from context management, domain awareness, and multi-step reasoning â€” not just parameter count.\n\n\n\n4. AI agents are the real inflection point\n\nAgentic systems donâ€™t just generate text â€” they plan, use tools, call APIs, and execute multi-step tasks across systems.\n\nLess â€œchatbotâ€, more junior employee with supervision and guardrails.\n\n\n\n5. Enterprise AI has moved into production\n\nWeâ€™re past pilots and proofs-of-concept. Teams are measuring latency, cost, reliability, and productivity gains â€” and AI is starting to affect real KPIs.\n\n\n\n6. Multimodal and domain-specific models are winning\n\nText-only AI is limiting. The future is multimodal systems that understand documents, images, audio, and structured data â€” often fine-tuned or adapted for specific industries.\n\n7. Governance, trust, and compliance are first-class concerns\n\nAudit trails, explainability, bias mitigation, access controls, and data residency are no longer â€œnice to haveâ€ â€” theyâ€™re required for production AI.\n\n8. Edge AI is quietly becoming important\n\nOn-device and local inference unlock lower latency, better privacy, and reduced cloud costs â€” especially for personal and enterprise use cases.\n\nTo me, the takeaway is simple:\n\nThe AI revolution isnâ€™t coming â€” itâ€™s already operational.\n\nThe winners wonâ€™t be the ones chasing hype, but the ones building reliable, grounded, agent-driven systems that actually work.\n\nI wrote a longer breakdown here:\n\n [https://www.loghunts.com/rag-llm-agentic-ai-guide-2026](https://www.loghunts.com/rag-llm-agentic-ai-guide-2026)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qzj9j4/from_ragpowered_llms_to_autonomous_agents_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4b8h4j",
          "author": "penguinzb1",
          "text": "The agent production gap is real. The jump from \"this works in my demo\" to \"this runs reliably for 10k users\" is where most teams hit the wall.\n\nWhat you said about measuring latency, cost, and reliability is spot on. The problem is most teams only find out their agents are brittle after users start hitting edge cases in production. By then you're firefighting instead of building.\n\nThe governance piece (point 7) ties directly into thisâ€”you can't just deploy an agent that \"mostly works\" when it's handling regulated data or making consequential decisions. You need to know how it behaves across thousands of scenarios before it touches real users.\n\nWe've been working on simulating agent workflows at scale before deployment to catch these issues early. Helps surface the difference between \"works on my machine\" and \"works in production under load with real user chaos.\"",
          "score": 6,
          "created_utc": "2026-02-08 20:21:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4cbhrs",
          "author": "ArmOk3290",
          "text": "Latency is also a real constraint once you add retrieval, rerank, and tool calls. People talk about multi step agents, but if each step is 2 to 4 seconds, nobody will use it.",
          "score": 2,
          "created_utc": "2026-02-08 23:48:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4dxnzn",
          "author": "Kind_Temperature_701",
          "text": "Absolutely agree(point 6) - we have already developed a Gen AI-powered solution for a major media company, which gave us firsthand experience with the capabilities of Multimodal LLMs.",
          "score": 1,
          "created_utc": "2026-02-09 05:29:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ew6fr",
          "author": "jannemansonh",
          "text": "the point about rag + workflow automation being the real shift is spot on... moved our doc processing workflows to needle app since you just describe what you want and it builds it (has rag built in). way easier than wiring together vector stores + n8n nodes, especially for non-technical folks on our team",
          "score": 1,
          "created_utc": "2026-02-09 10:50:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4eyt1j",
          "author": "AICodeSmith",
          "text": "This aligns closely with what weâ€™re seeing in BlackSmith AI as well. Our LLM stack focuses on agent-driven workflows, strong retrieval, and domain-specific reasoning so AI can operate reliably in real production environments. If you wants more details feel free to ask.",
          "score": 1,
          "created_utc": "2026-02-09 11:14:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4mv62i",
          "author": "Little-Appearance-28",
          "text": "This is a solid take. Iâ€™ve also noticed the shift from just bigger LLMs to **real production use cases** where RAG + agents actually deliver value. Grounding an LLM with retrieval cuts hallucinations and makes answers reliable, and agents that *plan, use tools, and act* are way closer to what businesses actually deploy today than simple chatbots. The future isnâ€™t just smarter models â€” itâ€™s workflows that run reliably, measure latency/cost/accuracy, and integrate RAG deeply into real systems.",
          "score": 1,
          "created_utc": "2026-02-10 16:06:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r05za6",
      "title": "Rerankers in RAG: when you need them + the main approaches (no fluff)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r05za6/rerankers_in_rag_when_you_need_them_the_main/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-09 14:42:35",
      "score": 32,
      "num_comments": 16,
      "upvote_ratio": 0.81,
      "text": "If your RAG feels like itâ€™s â€œalmost thereâ€ â€” rerankers are usually the missing piece.\n\nA reranker sits between retrieval and the LLM:\n\n1. Retrieve a larger candidate set (e.g., top-50)\n2. Rerank those candidates by relevance\n3. Send only top-5/top-10 to the model\n\nThe point: stop feeding the LLM garbage context.\n\n# When rerankers are actually worth it\n\nYou likely need a reranker if:\n\n* The correct chunk is often in top-50, but not in top-5\n* Your corpus has near-duplicates (policy versions, templates, â€œsame doc but updatedâ€)\n* Queries are long / multi-intent (â€œcompare A vs B, cite the latest policy, exclude legacyâ€)\n* Dense retrieval returns â€œrelatedâ€ chunks but not the *answer-bearing* chunk\n* Increasing k makes answers worse (more context â†’ more confusion)\n\nIf your data is small and clean and top-5 is already precise, rerankers can be extra latency for little gain.\n\n# The main reranker approaches (practical overview)\n\n# 1) Cross-encoder rerankers (most common â€œquality winâ€)\n\nScores each *(query, chunk)* pair by reading them together.\n\n* âœ… Best precision (biggest improvement to answer quality)\n* âŒ More compute than embeddings\n* Best pattern: retrieve top-50 â†’ cross-encoder â†’ keep top-5\n\n# 2) Embedding similarity (bi-encoder) (baseline, not really â€œrerankingâ€)\n\nThis is what most people mean by â€œvector searchâ€.\n\n* âœ… Fast, scales well\n* âŒ Weaker at fine-grained intent (â€œwhich chunk actually answers?â€)\n* Best use: candidate generator before a stronger reranker\n\n# 3) Hybrid (BM25 + dense)\n\nCombine lexical matching with embeddings.\n\n* âœ… Great for IDs, error codes, names, exact terms\n* âœ… More robust across weird queries\n* âŒ Requires tuning weights / mixing logic\n\n# 4) LLM-as-reranker (works, but donâ€™t start here)\n\nAsk an LLM to rank chunks (sometimes with custom rules).\n\n* âœ… Very flexible (â€œprefer newest docâ€, â€œmust include citationâ€, etc.)\n* âŒ Slower + expensive + can be inconsistent unless tightly constrained\n* Use when you need domain rules that models donâ€™t capture well\n\n# 5) Domain-tuned rerankers\n\nFine-tune a reranker on your own relevance data.\n\n* âœ… Big gains in specialized corpora\n* âŒ Needs training data + evaluation discipline\n* Worth it when retrieval quality is core to your product\n\n# A simple setup that usually works\n\n* Retrieve top-50\n* Cross-encoder rerank â†’ pick top-5\n* Apply filters **before reranking** when possible (permissions, time, doc type)\n* Track metrics like: â€œanswer present in top-Nâ€ and final answer accuracy\n\nThatâ€™s it. Reranking isnâ€™t a silver bullet â€” itâ€™s just the cleanest way to convert â€œthe answer is somewhere in thereâ€ into â€œthe model actually sees it.â€\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1r05za6/rerankers_in_rag_when_you_need_them_the_main/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4hc902",
          "author": "Xacius",
          "text": "Thanks ChstGPT",
          "score": 16,
          "created_utc": "2026-02-09 19:04:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4jhos5",
              "author": "stingraycharles",
              "text": "â€œno fluffâ€ \n\n*proceeds to put in shitloads of prose which could have been summed up in one paragraph*",
              "score": 10,
              "created_utc": "2026-02-10 01:51:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4k0zgd",
                  "author": "MullingMulianto",
                  "text": "do you think openai just sets every model's temperature at 100 even especially after everyone shits on them for slop content\n\nbecause I really am just starting to think so",
                  "score": 1,
                  "created_utc": "2026-02-10 03:47:19",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4l1fe2",
                  "author": "Donkit_AI",
                  "text": "You're welcome to sum it up in one paragraph. ðŸ˜",
                  "score": 0,
                  "created_utc": "2026-02-10 08:44:49",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4hn9pq",
          "author": "Space__Whiskey",
          "text": "This sounds great.  \nWhat reranker are we talking about here, and which one is straightforward to fine-tune/train?",
          "score": 3,
          "created_utc": "2026-02-09 19:58:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l35op",
              "author": "Donkit_AI",
              "text": "There's a nice link from u/Comfortable-Fan-580 a few comments below to a nice short post about \"what rerankers are\". In real life though it depends heavily on the use case and the constrains. We're using LLM as a reranker in some of our cases. It is expensive and requires more tinkering to make it right but works better with messy datasets and complex rules.\n\nAs for fine-tuning, cross-encoders are the easiest. You can pick BAAI/bge-reranker family or e.g. cross-encoder/ms-marco-MiniLM-L-6-v2 if we're talking open-source.",
              "score": 1,
              "created_utc": "2026-02-10 09:01:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4i4ksh",
          "author": "drink_with_me_to_day",
          "text": "> Apply filters before reranking when possible (permissions, time, doc type)\n\nPermissions check should happen way before this step",
          "score": 2,
          "created_utc": "2026-02-09 21:24:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l253j",
              "author": "Donkit_AI",
              "text": "Filters are on retrieval. That's usually the previous step. I wouldn't say it's \"way before\".",
              "score": 1,
              "created_utc": "2026-02-10 08:51:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4iduz0",
          "author": "Comfortable-Fan-580",
          "text": "Simply explained here - https://pradyumnachippigiri.dev/til/ai/rag-reranking",
          "score": 2,
          "created_utc": "2026-02-09 22:10:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l1xip",
              "author": "Donkit_AI",
              "text": "Nice explanation of what reranker is at all. Thank you!\n\nMaybe I should have started with something like this rather than jumping to Step2 (what reranker to use) right away.",
              "score": 1,
              "created_utc": "2026-02-10 08:49:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ggx1b",
          "author": "jrochkind",
          "text": "This is very helpful, thank you!",
          "score": 2,
          "created_utc": "2026-02-09 16:36:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4j2ri2",
          "author": "RoCkyGlum",
          "text": "Rerank is shit",
          "score": 0,
          "created_utc": "2026-02-10 00:24:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyck5p",
      "title": "My RAG pipeline costs 3x what I budgeted...",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qyck5p/my_rag_pipeline_costs_3x_what_i_budgeted/",
      "author": "Potential-Jicama-335",
      "created_utc": "2026-02-07 12:28:11",
      "score": 31,
      "num_comments": 43,
      "upvote_ratio": 0.91,
      "text": "BuiltÂ a RAG systemÂ overÂ internalÂ docs. PickedÂ ClaudeÂ Sonnet becauseÂ it seemedÂ like the best quality-to-price ratio basedÂ on what I readÂ online. EverythingÂ workedÂ great in testing.\n\nThenÂ I lookedÂ at the billÂ afterÂ aÂ weekÂ ofÂ productionÂ traffic. WayÂ overÂ budget. TurnsÂ out the actualÂ costÂ perÂ queryÂ isÂ wayÂ higher than what I estimatedÂ fromÂ the pricingÂ page. SomethingÂ aboutÂ howÂ differentÂ models tokenize theÂ sameÂ contextÂ differently, soÂ myÂ 8kÂ tokenÂ retrieval chunksÂ costÂ more onÂ someÂ models than others.\n\nNowÂ I needÂ to findÂ a model that givesÂ similarÂ quality but actually fitsÂ my budget.\n\nAnyoneÂ dealtÂ with this?\n\nEdit: Thanks everyone for your suggestions ! I'm so grateful for this community's help.  \nI ended up trying a few solutions with my team, and we finally tried [openmark.ai](https://www.openmark.ai) like someone mentioned for automated testing, we managed to find models that perform better for every step of the agentic flows, and that are much more cost efficient, with fallbacks if necessary. Hopefully, we don't get any surprises anymore. ðŸ™",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qyck5p/my_rag_pipeline_costs_3x_what_i_budgeted/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o43inv4",
          "author": "pl201",
          "text": "Bottom line, You picked the most expensive model to process your doc. Put this way, you paid CEO salary to do a delivery job. There are many models that is 5x to 10x cheaper but are fully capable to process doc for your RAG.",
          "score": 8,
          "created_utc": "2026-02-07 15:56:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44iu75",
              "author": "Potential-Jicama-335",
              "text": "Thanks for your input. Yes I'm learning this the hard way. Now I'm trying to find a quick way to find the most appropriate model(s) for the job, as you say, not the ceo job, but the most cost efficient 'person' for the job.  \nI'm on it, I found a few services that could be better than manual testing, I'm in the process of trying them out.\n\nThanks again, nice way to put this.",
              "score": 2,
              "created_utc": "2026-02-07 18:54:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42jplo",
          "author": "Fantastic_suit143",
          "text": "I built mine for free",
          "score": 4,
          "created_utc": "2026-02-07 12:35:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42v9by",
              "author": "Joy_Boy_12",
              "text": "Can you explain your pipeline?",
              "score": 1,
              "created_utc": "2026-02-07 13:51:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o42ygls",
                  "author": "Fantastic_suit143",
                  "text": "1. The Input (Vectorization)  \n\nMy Flask backend captures the user's text question.  \nBGE-M3 Hugging face (Local Embedding Model).  \nwhat happens: The model converts the English sentence into a 1024-dimensional vector, which is a list of numbers. It does not comprehend the text; it transforms it into a mathematical point in space.  \nWhy this choice? Running this locally guarantees under 100 milliseconds latency and no cost since it's on hugging face backend (free plan).  \n\n2. The Retrieval\nThe system searches my database for relevant information.  \nFAISS.  \nFAISS calculates the Cosine Similarity between the user's question vector and the 130,000+ chunks of text in mine database. It retrieves the Top 3-5 chunks that are mathematically closest to the query. FAISS is designed for speed. It can scan millions of vectors in milliseconds, even on a CPU which is available on hugging face free plan.  \n\n3. The Prompt Construction  \nThe system creates a large prompt for the AI.  \nThe Logic: It combines three components:  \nRole: \"You are a strict Singapore Law Expertâ€¦â€  \nContext: The 3-5 text chunks retrieved by FAISS.  \nQuery: The user's original question.  \n\n4. The Generation (Te \"triplefailover\" brain)  \n The prompt is sent to a Large Language Model (LLM) to generate a human-like answer.  \nA reliable \"Chain of Command\":  \nPrimary: Gemini 2.0 Flash (Fastest, High Context).  \nFailover 1: If Gemini fails due to rate limit or error, it switches to Arcee AI trinity large preview(via OpenRouter).  \nFailover 2: If that fails, it switches to Llama 3.3 70B (via Groq).  \nThis guarantees 99.9% uptime.\n\n5. The Output\nAction: The text response streams back to the frontend.  \nReact + Framer Motion.\n The answer appears in  my chat window and thats it I hope this enough explanation also I didn't even invest a single cent in this\nIf you found this helpful thank you very much â˜ºï¸ðŸ˜",
                  "score": 12,
                  "created_utc": "2026-02-07 14:10:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o42z85o",
          "author": "ChapterEquivalent188",
          "text": "why not taking your time and go local ;) https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit",
          "score": 5,
          "created_utc": "2026-02-07 14:14:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44j84j",
              "author": "Potential-Jicama-335",
              "text": "Thats a nice suggestion thank you. My company is set on using cloud API services though.",
              "score": 1,
              "created_utc": "2026-02-07 18:56:42",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42kiru",
          "author": "Kathane37",
          "text": "There is token variation between models + language can have an impact + pay a lot of attention to caching.\nAlso sonnet for a 8k context is overkill, you can check fiction bench or the blog post context rot from chroma db to choose a model according to the context you have to deal with.",
          "score": 3,
          "created_utc": "2026-02-07 12:41:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44kbsp",
              "author": "Potential-Jicama-335",
              "text": "Interesting take. Thank you for your input. ",
              "score": 1,
              "created_utc": "2026-02-07 19:02:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44t80w",
          "author": "primoco",
          "text": "I went full local to avoid exactly this problem\nI built a RAG system (RAG Enterprise) and decided early on to keep everything local, both embeddings and inference, no API costs, no surprises.\nMy setup: local embeddings with EmbeddingGemma, local LLM inference running on my own hardware, zero per-query costs once set up.\nTrade-offs I accepted: upfront hardware cost (I run this on an RTX 5070 Ti), quality might not match top-tier API models, slower inference than API calls, need to manage infrastructure yourself.\nBut the benefits: completely predictable costs, no tokenization surprises, full privacy (important for internal docs), scales with hardware not with usage.\nIf your budget is tight and you have the technical capability, going local might be worth considering, the initial investment pays off quickly if you have decent traffic volume.\nThat said, if you need API-level quality, others here have mentioned GPT-4o-mini and Haiku as cheaper alternatives worth testing, just make sure you test with the actual tokenizer before committing.",
          "score": 2,
          "created_utc": "2026-02-07 19:48:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42k7k7",
          "author": "Educational_Cup9809",
          "text": "gemini 2.5",
          "score": 1,
          "created_utc": "2026-02-07 12:38:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42om47",
          "author": "Infamous_Ad5702",
          "text": "Yes. I built a tool to fix my problem. Tokens were expensive and gpu was out of reach.\n\nI build an index first and then each new query I auto build a fresh knowledge graph.\n\nI can add docs whenever I like.\n\nIt is offline\nZero hallucinations.\n\nMy defence client needed it this way. \nCheap for me, offline security for them.",
          "score": 1,
          "created_utc": "2026-02-07 13:10:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o42ymnc",
              "author": "_dakota__",
              "text": "Can you please elaborate how it works offline?",
              "score": 1,
              "created_utc": "2026-02-07 14:11:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o44kldd",
              "author": "Potential-Jicama-335",
              "text": "Fancy way to solve that. My team wouldn't create our own solution though, we don't have time to do that unfortunately. Interesting take though. Thank you. ",
              "score": 1,
              "created_utc": "2026-02-07 19:03:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o42vsip",
          "author": "Joy_Boy_12",
          "text": "How is your pipeline works?",
          "score": 1,
          "created_utc": "2026-02-07 13:54:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44kwfy",
              "author": "Potential-Jicama-335",
              "text": "Its a multi step compression/semantic retrieval flow with several agentic steps to achieve a good compression rate. Not easy to describe in a few words, I assume like for lots rag pipelines. ",
              "score": 1,
              "created_utc": "2026-02-07 19:05:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o431fy7",
          "author": "hrishikamath",
          "text": "Your biggest culprit is sonnet lol. Itâ€™s probably the most expensive LLM, I use like qwen253B for my agent RAG and itâ€™s fast and fine. Itâ€™s even a agentic rag. Link: https://github.com/kamathhrishi/stratalens-ai",
          "score": 1,
          "created_utc": "2026-02-07 14:27:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jdys",
              "author": "Potential-Jicama-335",
              "text": "Yes I'm learning this the hard way thank you. Now I'm trying to find a service to find the most cost efficient models as soon as possible, in the process of trying some out. \n\nThanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-07 18:57:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44wne1",
                  "author": "hrishikamath",
                  "text": "I think its better to not use a service like openrouter, but rather you benchmark your RAG with different LLM's. If use some service with that you will have unexpected issues in prod. ",
                  "score": 1,
                  "created_utc": "2026-02-07 20:06:07",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o45yxpc",
                  "author": "Superb_Plane2497",
                  "text": "[together.ai](http://together.ai) is good although focused on opensource/openweight models, which are good enough. I have a much smaller corpus but three layers of authoritative source (canonical, expert opinion, user organisation override). I got better retrieval results by splitting the parent chunks into child chunks with a small token count, and embedding those. Being smaller, a cheap encoding model can be used, and they work better with specific queries anyway. But embedding is a one time cost, and this approach uses the child chunks to find and retrieve a parent chunk so it doesn't save actual tokens when answering queries, except that perhaps it does better retrieval which is also an optimisation (my retrieval is a BM25/vectorisation hybrid which some hand-coded domain-specific assistance provided to BM25). [together.ai](http://together.ai) has a list of recommended models which do well. I allow the user to swap to kimi-2.5 which lets them paste/upload images to their prompt ... however, each user has a quota so if they do more expensive things, they exhaust their quota more quickly, which is a thinking outside the box solution, perhaps. ",
                  "score": 1,
                  "created_utc": "2026-02-07 23:40:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o433olw",
          "author": "st0ut717",
          "text": "Local llm is the way to go.",
          "score": 1,
          "created_utc": "2026-02-07 14:39:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o43b9gs",
          "author": "ArturoNereu",
          "text": "Hi.\n\nIs the cost you mention only for requests to Opus? \n\nI want to understand if that's the case, because depending on your requirements, you can offload the cost by: using a less capable LLM to provide the answer, but use a better embedding model to perform the search.",
          "score": 1,
          "created_utc": "2026-02-07 15:20:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jidy",
              "author": "Potential-Jicama-335",
              "text": "This is exactly the kind of 'two step' solution I'm looking into currently. You're right ahead of me ! ",
              "score": 1,
              "created_utc": "2026-02-07 18:58:07",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o43t8yl",
          "author": "ampancha",
          "text": "Model swapping treats the symptom. The actual failure mode here is missing spend controls: per-user caps, token limits, and attribution so you can see which queries and users drive cost. Without those, any model will eventually surprise you. Also worth auditing your 8k chunk strategy; retrieval filtering and smarter chunking often cut costs 30 to 50 percent without touching quality. Sent you a DM with more detail.",
          "score": 1,
          "created_utc": "2026-02-07 16:48:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4462cb",
          "author": "tillemetry",
          "text": "What about local hardware costs?",
          "score": 1,
          "created_utc": "2026-02-07 17:52:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44cf87",
          "author": "Rent_South",
          "text": "TheÂ tokenizationÂ costÂ thingÂ is somethingÂ mostÂ people don't realizeÂ untilÂ theÂ bill hits. TheÂ sameÂ retrieval contextÂ costsÂ differentÂ amounts onÂ different providersÂ becauseÂ theyÂ tokenize differently. TheÂ \"priceÂ per millionÂ tokens\" onÂ pricingÂ pagesÂ is misleading forÂ exactlyÂ this reason.\n\nBeforeÂ switchingÂ modelsÂ inÂ aÂ RAG pipeline, runÂ yourÂ actualÂ retrieval prompts throughÂ [openmark.ai](https://www.openmark.ai)Â andÂ compareÂ realÂ costÂ perÂ queryÂ alongsideÂ accuracy. YouÂ mightÂ find aÂ modelÂ that matchesÂ qualityÂ atÂ a fractionÂ of the priceÂ justÂ becauseÂ it tokenizes yourÂ documentÂ chunksÂ moreÂ efficiently.",
          "score": 1,
          "created_utc": "2026-02-07 18:23:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44jybi",
              "author": "Potential-Jicama-335",
              "text": "The tokenization angle is interesting. I looked it up quickly and f it can save me some time I'll actually try the tool, because testing everything manually has been a nightmare and really unpractical. I'll speak about it to my supervisor tomorrow. I've got a few options to consider.  \nThanks for your  input.",
              "score": 1,
              "created_utc": "2026-02-07 19:00:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o44ll3e",
          "author": "TechnicalGeologist99",
          "text": "We still building wrappers? \n\nIf you don't own your AI inference stack then what will you do when market pressure forces prices up.",
          "score": 1,
          "created_utc": "2026-02-07 19:08:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44mg7t",
              "author": "Potential-Jicama-335",
              "text": "Hopefully I'd have fallbacks to other APIs, different provider brands, and competition will drive the costs down, and if not, I'd just switch provider. \n\nMy team likes the flexibility of cloud services, we're also not a huge company that can afford localized solution easily, not to mention the time to spend on it. \n\nSo Ideally we'd have a few fallback models, and switch if 1 is not cost efficient anymore.   \nI'm actually looking into some solutions I found thanks to this thread to find optimal models to do that on a schedule. \n\nTHank you for your input. ",
              "score": 1,
              "created_utc": "2026-02-07 19:12:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44wywx",
                  "author": "TechnicalGeologist99",
                  "text": "Competition does drive costs down. \n\nBut costs are artificially low and inference providers are not profitable. The costs will be passed on to you. The issue isn't lack of competition, it's the cost to manufacture and maintain GPUs.",
                  "score": 1,
                  "created_utc": "2026-02-07 20:07:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45c5qf",
          "author": "[deleted]",
          "text": "I built mine for free! I do my entire RAG pipeline with a local model. It's beautiful. ",
          "score": 1,
          "created_utc": "2026-02-07 21:29:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46ze63",
          "author": "New_Advance5606",
          "text": "Welcome to the club. Depends the scale.Â  You can RAG a document or a thousand for free.Â  When you push the magnitude of order to a million, it costs money.Â Â ",
          "score": 1,
          "created_utc": "2026-02-08 03:27:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o48i8mo",
          "author": "KyjenYes",
          "text": "worst ad I have seen",
          "score": 1,
          "created_utc": "2026-02-08 11:17:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ei6pu",
          "author": "SubstantialTea707",
          "text": "You need to rerank with a cross-encoder before the Hybrid search. This is the key to a successful reranking.",
          "score": 1,
          "created_utc": "2026-02-09 08:32:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zc0vi",
          "author": "Little-Appearance-28",
          "text": "Dude, this is *such* a classic RAG trap ðŸ˜…  \nOn paper Claude Sonnet looks like the perfect quality/price sweet spotâ€¦ then prod traffic shows up and tokenization absolutely wrecks your estimates.\n\nYouâ€™re 100% right on the root cause: **pricing pages lie by omission**. Different models tokenize the *same* context very differently, so an â€œ8k tokenâ€ chunk in your mental math can quietly turn into a much more expensive prompt depending on the model family. At scale, that gap explodes your bill.\n\nThis happens to a ton of teams who only benchmark on:\n\n* price per 1M tokens\n* synthetic prompts\n* happy-path tests\n\nand not on their *actual* RAG pipeline.\n\nNice move pivoting to real-world evaluation. Running automated benchmarks on your **real prompts + docs + agentic flows** is pretty much the only reliable way to balance quality vs cost. The fact that you also set up fallbacks is huge â€” thatâ€™s how you avoid future surprises when traffic spikes or providers change behavior.\n\nFor anyone else reading this:  \ndonâ€™t trust theoretical costs. Measure **effective cost per query** on your own data, or you *will* get burned.\n\nOut of curiosity, which models ended up being your main winners in prod? Always looking to compare notes for RAG stacks ðŸ‘€",
          "score": 1,
          "created_utc": "2026-02-12 13:57:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qypa0t",
      "title": "Building a Fully Local RAG Pipeline with Qwen 2.5 and ChromaDB",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qypa0t/building_a_fully_local_rag_pipeline_with_qwen_25/",
      "author": "The_Visionary_Grimmy",
      "created_utc": "2026-02-07 21:00:23",
      "score": 26,
      "num_comments": 3,
      "upvote_ratio": 0.88,
      "text": "I recently wrote a short technical walkthrough on building a **fully local Retrieval-Augmented Generation (RAG) pipeline** using **Qwen-2.5** and **ChromaDB**. The focus is on keeping everything self-hosted (no cloud APIs) and explaining the design choices around embeddings, retrieval, and generation.\n\nArticle:  \n[https://medium.com/@mostaphaelansari/building-a-fully-local-rag-pipeline-with-qwen-2-5-and-chromadb-968eb6abd708](https://medium.com/@mostaphaelansari/building-a-fully-local-rag-pipeline-with-qwen-2-5-and-chromadb-968eb6abd708)\n\nI also put the reference implementation here in case itâ€™s useful to anyone experimenting with local RAG setups:  \n[https://github.com/mostaphaelansari/Optimization-and-Deployment-of-a-Retrieval-Augmented-Generation-RAG-System-](https://github.com/mostaphaelansari/Optimization-and-Deployment-of-a-Retrieval-Augmented-Generation-RAG-System-)\n\nHappy to hear feedback or discuss trade-offs (latency, embedding choice, scaling, etc.).",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qypa0t/building_a_fully_local_rag_pipeline_with_qwen_25/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o474i1c",
          "author": "the-vague-blur",
          "text": "Super cool! I'll try this out! \nOut of curiosity, what were the specs of your computer?",
          "score": 2,
          "created_utc": "2026-02-08 04:01:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48icdd",
              "author": "The_Visionary_Grimmy",
              "text": "**RTX 4070 desktop GPU** with **12 GB of GDDR6X VRAM**",
              "score": 1,
              "created_utc": "2026-02-08 11:18:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o464gho",
          "author": "Academic_Track_2765",
          "text": "Excellent work! yes this is the way to do it if you have GPU / memory available. ",
          "score": 1,
          "created_utc": "2026-02-08 00:14:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qynrqv",
      "title": "\" Hierarchical Agentic RAG (Knowledge Graph + Vector) & JSON RAG \" running fully offline on GTX 1650 (Scale Vs Speed)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qynrqv/hierarchical_agentic_rag_knowledge_graph_vector/",
      "author": "D_E_V_25",
      "created_utc": "2026-02-07 20:01:06",
      "score": 24,
      "num_comments": 4,
      "upvote_ratio": 0.97,
      "text": "Hi everyone, Iâ€™m a 1st-year CSE student. Iâ€™ve been obsessing over how to run decent RAG pipelines on my consumer laptop (GTX 1650, 4GB VRAM) without relying on any cloud APIs.\n\nâ€‹I quickly realized that \"one size fits all\" doesn't work when you have limited VRAM. So I ended up building two completely different RAG architectures for my projects, and Iâ€™d love to get some feedback on them.\n\nâ€‹1. The \" HIERARCHICAL AGENTIC RAG WITH HYBRID SEARCH (VRCTOR SEARCH + KNOWLEDGE GRAPH)\" (WiredBrain)::\n\nâ€‹The Goal: Handle massive scale (693k chunks) without crashing my RAM.\n\nâ€‹The Problem: Standard HNSW indexes were too RAM-heavy and got slow as the dataset grew.\n\nâ€‹My Solution: I built a Hierarchical 3-Address Router. Instead of searching everything, it uses a lightweight classifier to route the query to a specific \"Cluster\" (Domain -> Topic -> Entity) before doing the vector search.\n\nâ€‹The Result: It cuts the search space by ~99% instantly. Iâ€™m using pgvector to keep the index on system RAM so my GPU is free for generation.\n\nâ€‹Repo: https://github.com/pheonix-delta/WiredBrain-Hierarchical-Rag\n\nâ€‹2. The \"Speed Demon\" (Axiom Voice Agent)\n\nâ€‹The Goal: <400ms latency for a real-time voice assistant.\nâ€‹The Problem: Even the optimized Graph RAG was too slow for a fluid conversation.\n\nâ€‹My Solution: I built a pure JSON-based RAG. It bypasses the complex graph lookups and loads a smaller, highly specific context directly into memory for immediate \"reflex\" answers. Itâ€™s strictly for the voice agent where speed > depth.\n\nâ€‹Repo: https://github.com/pheonix-delta/axiom-voice-agent",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qynrqv/hierarchical_agentic_rag_knowledge_graph_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4646g8",
          "author": "Top_Locksmith_9695",
          "text": "Super interesting! Thanks for sharing! Can you give more details on how you did the domain, topic entity?",
          "score": 1,
          "created_utc": "2026-02-08 00:12:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46x6lv",
              "author": "D_E_V_25",
              "text": "Thanks a lot for giving time to this ðŸ˜ðŸ˜\n\nNow as for the topic...\n\nMy architectural is divided into gates in the paper I had posted u will see .. domain tagging and 13 gates named with different topics inside that I first tool 2k+ place holders with different names..\n \nSay if maths then inside that chapters and say each chapter with headings and each headings with a sub headings and problems ..\n\nAlthough maths in rag is always a bad idea but in my case it's working with decent percentage of performance also  and the reasons are because of the architecture...\n\nAs u know with such heavy rag local models can't take it up as they will themselves get out of context ...\n\nSo current working whole state I have installed in the laboratory which i can't publicly disclose to the web .. Sorry for that.. But if I get good response this time I plan to move from 250 gb of data inputs to 1tb one as well....\n\nI have open sources the backend with few files missing because I am in the line of getting pre prints acceptance  and journals as well..\n\nBut even if I don't get a valuable feedback... \n\nComments like yours truly motivates me and I feel truly happy to opensource my whole projects ...\n\nPls give a moment to share that and starred the repo it will boost the visibility...\n\nCurrently I have 500+ clones of my projects but sad to see the visibility algorithm works only when u have stars...\nBut no worries .. I am not hungry for stars or fame...\n\nI know a true community and ppl are loving these.. and that was the whole point",
              "score": 2,
              "created_utc": "2026-02-08 03:12:38",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4mw17c",
          "author": "Little-Appearance-28",
          "text": "Running a hierarchical Agentic RAG fully offline on a GTX 1650? Thatâ€™s impressive! Combining knowledge graphs with vectors and JSON for RAG really shows the trade-off between scale and speed. Itâ€™s cool to see fully offline setups actually performing well without massive hardware.",
          "score": 1,
          "created_utc": "2026-02-10 16:11:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n50b0",
              "author": "D_E_V_25",
              "text": "Thank you ðŸ˜Š\n\nAnd more importantly I have added reasoning to rag which is unheard of anywhere and also I have put the paper over my repo u can check that out ... \n\nI am looking forward for contributions \n\nCurrently i upgraded systems to get retrieval under 50ms and Its not false .. but with a whole model on my without tensor cores gpu also I have been able to get a eye blink fast re retrieval and now the system is not Hybrid but Tri-search..\n\nThats why I had mentioned I am currently improving thing...\n\nI don't know I did right to put things early onto GitHub but I just want to contribute to the open source world as I also learn a lot from them...\n\nHappy to see u recognising the work !!",
              "score": 1,
              "created_utc": "2026-02-10 16:52:14",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz1zad",
      "title": "Do companies actually use internal RAG / doc-chat systems in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qz1zad/do_companies_actually_use_internal_rag_docchat/",
      "author": "NetInternational313",
      "created_utc": "2026-02-08 06:49:42",
      "score": 21,
      "num_comments": 50,
      "upvote_ratio": 0.92,
      "text": "Iâ€™m curious how common internal RAG or doc-chat tools really are in practice.\n\nDoes your org have something like:\n\n* chat over internal docs / wikis / tickets\n* an internal knowledge assistant\n* or any RAG-based system beyond a small pilot?\n\nIf yes, is it widely deployed or limited to a few teams?  \nIf no, did it stall at POC due to security, compliance, or other concerns?\n\nGenuinely interested in real-world adoption",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qz1zad/do_companies_actually_use_internal_rag_docchat/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o48yy8v",
          "author": "SpectralCoding",
          "text": "We deployed one against our entire internal quality and R&D documentation. 2.2M pages converted to markdown and stored in an Azure AI Search Index. Deployed to ~7,000 users and gets about 250 questions per day. Only a subset of those 7000 work with the data we indexed daily. GPT-5.2 Medium for answer generation. My comment history has some information we got from users about how they use the tool and how itâ€™s helping them in their daily work. Very interesting responses.\n\nItâ€™s costing us about $1k/mo to maintain (OCRing new docs, Search index cost, LLM calls, etc). Thatâ€™s more expensive than it probably needs to be but the value we get is more than $0.18/answer on average.",
          "score": 13,
          "created_utc": "2026-02-08 13:28:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4917x7",
              "author": "NetInternational313",
              "text": "Is this tool strictly for internal use, and how did you handle security and access control for such a large doc set?",
              "score": 2,
              "created_utc": "2026-02-08 13:43:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o496vw0",
                  "author": "SpectralCoding",
                  "text": "Yes internal only. Most of the document set is open to all users. However our tool uses Azure AI Search where you can do effectively â€œrecord levelâ€ security with groups. So when you do the retrieval segment they are only searching what they have access to.",
                  "score": 4,
                  "created_utc": "2026-02-08 14:17:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o491wyp",
              "author": "Thenuges",
              "text": "Are they not worried about the confidential data being sent out to OpenAI? Curious what their thoughts or policies are around this.",
              "score": 2,
              "created_utc": "2026-02-08 13:47:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o496n67",
                  "author": "SpectralCoding",
                  "text": "No. We use Azure OpenAI and are secured with Microsoftâ€™s Data Processing Addendum. More of a concern is the tool is so useful and effective it would make an internal bad actors job way easier. A year ago someone would have had to ask around to find the right documents and piece together the entire process for a specific chemical coating or whatever. Now they just ask and it coalesces info from 20 docs and gives them a top tier answer in about 1min. So good for most use cases but also concerning how much data is now immediately available. But thatâ€™s our organization immaturity with document categorization and restricting data access.",
                  "score": 3,
                  "created_utc": "2026-02-08 14:15:59",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49xfqq",
              "author": "horserino",
              "text": "(disclaimer: I'm mostly a noob on this kind of setup)\n\nHow does the model interface with Azure? Does the model \"call\" a DB search somehow? Or does the search happen first and the results sent in the model's prompt ?",
              "score": 1,
              "created_utc": "2026-02-08 16:36:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49ygjf",
                  "author": "SpectralCoding",
                  "text": "This is the base project we use and have extended. Lots of docs and videos on it too.\n\nhttps://github.com/Azure-Samples/azure-search-openai-demo\n\nYou answer is here:\n\nhttps://github.com/Azure-Samples/azure-search-openai-demo/releases/tag/2025-11-18\n\nThere is an API layer that uses an LLM to come up with search terms, it searches the index, then creates a new prompt with the data from the search and the users question. Classic RAG pattern. This project is more interesting because it uses some Azure AI Search features to agentically search the index until it either finds relevant info or gives up. Not just a one shot search.",
                  "score": 2,
                  "created_utc": "2026-02-08 16:41:01",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o49ybpb",
              "author": "No-Contribution8248",
              "text": "What's the avg latency per question?",
              "score": 1,
              "created_utc": "2026-02-08 16:40:22",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49z6fn",
                  "author": "SpectralCoding",
                  "text": "Longer since we built or enabled the agentic search capabilities where it will make multiple searches if it doesnâ€™t find relevant things the first time. Best case for a simple answer is about 2sec-ish? Longer complex multi-faceted answers might take 10sec or so since we have reasoning turned up. But we value completeness of answer and research over speed. We could (and have) turn everything down and generate sub-second streaming response but for our type of data this leads to better accuracy.",
                  "score": 3,
                  "created_utc": "2026-02-08 16:44:31",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o4an39c",
                  "author": "c4cheeku",
                  "text": "I think youâ€™re taking about similarity search latency. How is this achieved? Given, youâ€™d do some reasoning and routing followed by retrieval and then answer generation and I am sure you wonâ€™t be able to do so under 10 seconds!!",
                  "score": 1,
                  "created_utc": "2026-02-08 18:37:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4e3m4u",
              "author": "Vvictor88",
              "text": "Wondering how accurate is this? Did data cleaning a part of the Rag setup? Or the document itself is very good to do the rag directly",
              "score": 1,
              "created_utc": "2026-02-09 06:17:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4flgtl",
                  "author": "SpectralCoding",
                  "text": "Data cleaning? The indexing pipeline is complicated. Most of them are source Microsoft Word documents, convert them to PDF with Microsoft Word (via pywin32/COM calls), then OCRs them with Azure Document Intelligence (Layout mode) and gets the Markdown from that. Then chunks that to \\~1,000 token chunks with some other things in there like a look-back for the most recent header to enhance the accuracy of the vector.\n\nThe docs themselves were not good to do RAG on directly because many were scanned images, many were Word docs which the \"easy\" Markdown conversion tools do a terrible job on. There are still some flaws like literally entire Word docs are 20 page tables... It's yuck, but the answers are still pretty good.",
                  "score": 1,
                  "created_utc": "2026-02-09 13:55:06",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4tc5ar",
              "author": "Fabulous_Radish8162",
              "text": "How are you managing relationships between data. Will sales teams for example get specific intelligence from a rag?",
              "score": 1,
              "created_utc": "2026-02-11 15:37:55",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o47uwl5",
          "author": "mr_dudo",
          "text": "we use one internally for onboarding - mostly hitting our confluence and notion exports. biggest win was honestly just getting the docs into a format the model could actually use. most wikis export as garbage html. ended up using a crawler to convert everything to markdown first, then chunking by headers. way cleaner retrieval than trying to parse the raw exports",
          "score": 4,
          "created_utc": "2026-02-08 07:37:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47yl5x",
              "author": "NetInternational313",
              "text": "That makes sense.\n\nDid you have to put any guardrails or access controls in place (team-based access, doc-level permissions), or is it mostly open internally since itâ€™s onboarding-focused?\n\n  \nDid the model internally hosted",
              "score": 1,
              "created_utc": "2026-02-08 08:11:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o496mp5",
                  "author": "mr_dudo",
                  "text": "nah we just don't index anything sensitive. onboarding stuff is all public internally anyway - process docs, how to set up your dev env, that kind of thing. way easier than building actual permissions into it. if you need doc-level access control you're gonna have a bad time lol",
                  "score": 1,
                  "created_utc": "2026-02-08 14:15:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o4dih2b",
              "author": "AnxietyPrudent1425",
              "text": "Onboarding clients. Not employees I assume",
              "score": 1,
              "created_utc": "2026-02-09 03:44:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o48p6p2",
          "author": "RobertLigthart",
          "text": "from what ive seen most companies have at least tried a POC but the ones that actually make it to production are usually narrow scope... like internal docs search or customer support kb, not the \"ask anything about our company\" thing that everyone demos\n\nthe biggest killer is usually data quality not the RAG pipeline itself. if your internal docs are a mess going in, no amount of fancy chunking or reranking is gonna save it",
          "score": 3,
          "created_utc": "2026-02-08 12:18:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ya8v",
              "author": "NetInternational313",
              "text": "For the RAGs that made it to prod, did teams have to add doc-level permissions or audit controls, or were those systems usually trusted because of the narrow scope?",
              "score": 1,
              "created_utc": "2026-02-08 13:24:26",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o49yo1a",
          "author": "Educational_Cup9809",
          "text": "I built the whole Custom Assistants as a service internally for my org. Itâ€™s not have 600+ assistants that teams and people created in last few months. This is just beta mode. Some features i built:\n\nData sources: Sharepoint, file uploads (10 formats), confluence, azure blob, zendesk, service now, salesforce.\n\nFeatures:  draft publish mode, auto sync(delta only) file level tracking and metadata tagging. \n\nsecurity: Automated pinecone project and index creation per assistant with. Encryption of chunks.\n\nRBAC: access control using internal AD security groups and roles per assistant (automated)\n\nMCP server and answer generation: Lot of custom filter prompts and assistant level instructions features which are configurable. I also inject Users AD data  for region and other useful user level information for context.\n\nAnother team had already build a model agnostic api layer with openai spec transformation. So i use that for completions and embedding models.\n\nFeedback and insights dashboard. And a conversation history management for analytics.\n\nNow I am diving into graph RAG and structured (database) and Agentic workflows. \n\nHuge success internally.\n\nSecurity and compliance, and also encryption, was discussed heavily 2 years ago before I begin. This should be first layer you properly design and build everything else on top it at all layers",
          "score": 3,
          "created_utc": "2026-02-08 16:42:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4a5ouz",
              "author": "NetInternational313",
              "text": "Looking back, what assumptions you had early on about security, access control, or usage patterns turned out to be wrong once adoption scaled to hundreds of assistants?",
              "score": 1,
              "created_utc": "2026-02-08 17:16:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o47w2t3",
          "author": "Amazing_Alarm6130",
          "text": "We do. Scaled to serve hundreds of employees. Around 10 millions of curated scientific documents.",
          "score": 2,
          "created_utc": "2026-02-08 07:48:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o47yexb",
              "author": "NetInternational313",
              "text": "Did you implement any access-control or governance policies to ensure data isolation across teams (e.g., preventing cross-team document access), especially at that scale?",
              "score": 1,
              "created_utc": "2026-02-08 08:09:51",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4djotj",
                  "author": "Amazing_Alarm6130",
                  "text": "not for the specific use case I worked. but I know they implemented access-control in another one, using AWS RDS to achieve that ",
                  "score": 1,
                  "created_utc": "2026-02-09 03:52:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o47ygks",
              "author": "NetInternational313",
              "text": "do you use internal hosted LLM",
              "score": 1,
              "created_utc": "2026-02-08 08:10:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4dk1jj",
                  "author": "Amazing_Alarm6130",
                  "text": "nope",
                  "score": 1,
                  "created_utc": "2026-02-09 03:54:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o48fqss",
              "author": "hashiromer",
              "text": "Do you use any evals?\n\nIn practice, relevance is trivial to check with citations but evaluating completeness of answers/recall seems next to impossible.",
              "score": 1,
              "created_utc": "2026-02-08 10:53:59",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o4dji9p",
              "author": "AnxietyPrudent1425",
              "text": "Interesting. Curious if you have any methods to curate context. Curious if youâ€™d be interested in beta testing an app weâ€™re building designed for the human to scope context window so the AI has weighted and targeted context.",
              "score": 1,
              "created_utc": "2026-02-09 03:51:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4isjjh",
          "author": "the_olivenbaum",
          "text": "We have a large production system for a customer built on our own software (Curiosity Workspace) and operating over 10+ TB of data.\nThe system combines NLP/NER, entity linking, and an in-memory knowledge graph, with RAG + similarity search built on top. Itâ€™s used daily by thousands of users as a real internal knowledge tool and assistant over their legacy and live data, not just a chat interface.\nWhat we found is that pure RAG didnâ€™t scale well at this size and the added structure (entities + graph) was critical, especially for grounding and navigating relationships across documents. Access control uses a ReBAC model with each document having permissions attached in the graph. Enforcing permissions before retrieval and showing clear source attribution were also key to adoption and the customer is planning to expand this system further.\nIn practice, the systems that work tend to look more like search + structured knowledge + LLM, rather than a simple doc-chat layer.",
          "score": 2,
          "created_utc": "2026-02-09 23:27:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4km5k0",
              "author": "NetInternational313",
              "text": "At what point did you realize pure RAG wouldnâ€™t scale and what specific failure signals or incidents forced the move toward entities and a knowledge graph? When something goes wrong in production, what evidence do security or compliance teams expect you to produce, and how does the graph-based approach make that possible? What was the biggest ongoing cost or operational burden introduced by adding entity extraction and a graph layer? For the ReBAC model, what level of granularity turned out to be necessary in practice document, section, or entity-level permissions?",
              "score": 2,
              "created_utc": "2026-02-10 06:23:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4knnsg",
                  "author": "the_olivenbaum",
                  "text": "The data is very technical, full of jargon and identifiers - vector embeddings could only get so far with capturing meaning, so structuring the data was key to the success. The problem is that embeddings don't capture well identifiers - so a query for something just a digit away that meant something completely different would have the same vector. For audit: there's an append only log of all data accessed by the user, directly or via search or chat, and logs of all chat interactions. \nThe biggest challenge on building the graph was data acquisition and mapping: we've over 50 data sources integrated in this project from all sorts of internal databases, and building a cohesive view of the data took some time. But it is also done incrementally, continuously throughout the project development and ongoing production usage.\nIt's all using traditional NLP approaches, we don't use LLMs for building the graph in this project both due to cost limitations (traditional NLP handles 100,000s of files/s and enables very quick reprocessing once new datasets are added).\nThe access model is one of the big challenges: it's at the 3 levels (data type, entity level and field level access enforcement - last one we're just adding to the product). And yes the use-case requires it (many data repositories each with it's own rules), strict export control requirements, etc...",
                  "score": 2,
                  "created_utc": "2026-02-10 06:36:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4agc15",
          "author": "No_Direction_7168",
          "text": "Yes - Many internal docs across tens of thousands.  Then, individual departments make their own to share with tens to hundreds.  I donâ€™t know their inner workings but I use NotebookLM as a RAG for my personal notes/documents.  The company has also given us personal RAG spaces, but they donâ€™t work nearly as good as NotebookLM.",
          "score": 1,
          "created_utc": "2026-02-08 18:07:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ait8e",
              "author": "NetInternational313",
              "text": "When you say the internal RAG doesnâ€™t work as well, is that mostly about relevance/recall, latency, or how answers are synthesized?",
              "score": 1,
              "created_utc": "2026-02-08 18:18:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4c4cgn",
                  "author": "No_Direction_7168",
                  "text": "Mostly about the quality of the answers.  The responses lack depth and feel like they are sticking close to the actual words of the ingested documents.  The company provided one was more clinical and while it technically contains a correct response.  e.g. I asked it to explain the history and development of a requirement and it responded with the history of the previous revisions of the document.  NLM not only gave me that information but also went on to describe the backstory of the document.. Not sure if it is a backend LLM difference, ingesting/chunking, or Quantization issue but the work one is always a bit lacking.",
                  "score": 2,
                  "created_utc": "2026-02-08 23:05:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4d8pdk",
          "author": "Ecanem",
          "text": "I work for one of the largest professional services firms in the world.  We have many deployed to 10,000-100,000 users.",
          "score": 1,
          "created_utc": "2026-02-09 02:50:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4esjiw",
              "author": "NetInternational313",
              "text": "What were the key controls or guarantees required to get these approved at that scale  especially around access control and auditability? Were there any early failures or rollout lessons before reaching that level of adoption? How do you handle document-level permissions and prevent cross-team data leakage at that user count?",
              "score": 1,
              "created_utc": "2026-02-09 10:15:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4di86o",
          "author": "AnxietyPrudent1425",
          "text": "I assume companies are not buying anything so much that i feel sorry for anyone currently employed planning finance. Lol. Youâ€™re going to lose everything too. Youâ€™re not special because youâ€™re currently employed. A smart person would shore up any assets they have and prepare for 1930 all over again.",
          "score": 1,
          "created_utc": "2026-02-09 03:43:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ff6h6",
          "author": "Informal_Tangerine51",
          "text": "Most orgs have pilots, few have production deployments. The gap is usually debuggability and compliance, not capability.\n\nRAG works great in demos. Production question: when it returns wrong answer, can you prove what docs it retrieved, how fresh they were, and whether retrieval logic has regressed since last week? Legal and compliance need verifiable evidence, not \"the model probably used the right docs.\"\n\nSecurity teams block deployment because there's no runtime policy layer. User asks sensitive question, RAG retrieves confidential docs, returns answer with data user shouldn't see. Without enforcement at retrieval time plus audit trails, it's a compliance nightmare waiting to happen.",
          "score": 1,
          "created_utc": "2026-02-09 13:17:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4kjcta",
              "author": "NetInternational313",
              "text": "From your experience, whatâ€™s the minimum set of instrumentation or controls a RAG system needs before security and compliance will even consider approving it for production? When a RAG system returns a wrong or sensitive answer, what evidence do legal or compliance teams actually expect to see during review or incident response? What does a â€œruntime policy layerâ€ look like in practice for RAG  is it mostly retrieval-time RBAC, query classification, or something else?In orgs where youâ€™ve seen RAG make it to production, what was the turning point that closed the gap between pilot and approval?",
              "score": 1,
              "created_utc": "2026-02-10 05:59:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sumyd",
          "author": "EnoughNinja",
          "text": "Most places I've seen get stuck going from POC to production hit the same wall whereby the easy stuff works fine (confluence pages, handbooks, product specs), but the questions people actually care about live in conversations, not documents.\n\n\"What did we decide about the pricing change?\" That answer is spread across a 15-message email thread, a half-written meeting recap, and a Jira comment. \n\nBasic RAG treats each chunk independently so it loses thread structure, who said what, and how decisions evolved.\n\nThe other killer is permissions. The moment you index internal comms, you need per-user access controls. Most POCs skip this and hit a wall when legal reviews it.\n\nThe orgs that make it to production either scope very narrowly to one doc type, or invest in a proper context layer that handles threading and permissions before anything hits the LLM. \n\nWe've been building infrastructure for the email side of this at iGPT, treating email as conversational graphs instead of flat docs. \n\nHappy to share more if it helps",
          "score": 1,
          "created_utc": "2026-02-11 14:08:15",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r235rh",
      "title": "What do you use for scraping data from URLs?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r235rh/what_do_you_use_for_scraping_data_from_urls/",
      "author": "Physical_Badger1281",
      "created_utc": "2026-02-11 17:11:42",
      "score": 21,
      "num_comments": 13,
      "upvote_ratio": 1.0,
      "text": "Hey all,\n\nQuick question â€” whatâ€™s your go-to setup for scraping data from websites?\n\nIâ€™ve used Python (requests + BeautifulSoup) and Puppeteer, but Iâ€™m seeing more people recommend Playwright, Scrapy, etc.\n\nWhat are you using in 2026 and why?\nDo you bother with proxies / rotation, or keep it simple?\n\nCurious whatâ€™s working best for you.\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r235rh/what_do_you_use_for_scraping_data_from_urls/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4txdnd",
          "author": "RoyalTitan333",
          "text": "I prefer Firecrawl(self hosted). \n\nHereâ€™s what I like about it. You point it at a site and it handles crawling, rendering, structured extraction, and even markdown output without forcing you to stitch together five different tools. For projects where the goal is usable data fast, that matters more than having ultimate low level control.\n\nI still reach for Playwright when a site is heavily interactive or guarded, and Scrapy is hard to beat for very large, rule driven crawls. But for a huge middle ground, especially content heavy sites.",
          "score": 8,
          "created_utc": "2026-02-11 17:17:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4uc54l",
              "author": "laurentbourrelly",
              "text": "I am old school, but gotta admit Firecrawl is awesome.",
              "score": 1,
              "created_utc": "2026-02-11 18:25:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4txcv5",
          "author": "Cod3Conjurer",
          "text": "A few days before this, I built a similar project using BeautifulSoup4 + Playwright + RAG for dynamic website crawling and retrieval.\n\nRepo: https://github.com/AnkitNayak-eth/CrawlAI-RAG",
          "score": 4,
          "created_utc": "2026-02-11 17:16:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uptd5",
          "author": "johnrock001",
          "text": "playwright or selenium",
          "score": 2,
          "created_utc": "2026-02-11 19:29:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4uxxot",
          "author": "bigahuna",
          "text": "Scrapy https://www.scrapy.org/ with faker.",
          "score": 1,
          "created_utc": "2026-02-11 20:08:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4v3hex",
          "author": "One_Milk_7025",
          "text": "Dude for bulk and stable scalable scraping use @crawl4ai they are the best.. it's open source and blazing fast for various task but the bulk extraction is their niche",
          "score": 1,
          "created_utc": "2026-02-11 20:35:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xw31a",
          "author": "SharpRule4025",
          "text": "Depends on what you're feeding the data into. If it's going into a RAG pipeline, the scraping part is only half the problem. The real pain is getting clean, structured content out of whatever HTML you pulled.\n\nI was using Playwright plus a bunch of custom extraction logic for a while. Worked fine until I had to deal with sites behind Cloudflare or DataDome, then it turned into a proxy rotation mess on top of everything else.\n\nLately I've been sending URLs through a scraping API that handles the rendering and anti-bot stuff, then returns structured JSON with headings, paragraphs, links separated out. Saves me from writing extraction code per site and the chunks map way better to embeddings than raw markdown does.\n\nFor simple static pages though, requests plus BeautifulSoup is still hard to beat. No reason to overcomplicate it if the site cooperates.",
          "score": 1,
          "created_utc": "2026-02-12 06:45:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xwfgg",
          "author": "Physical_Badger1281",
          "text": "I worked on a project called [Fastrag](https://www.fastrag.live), where I used Puppeteer for data scraping, and itâ€™s working well.",
          "score": 1,
          "created_utc": "2026-02-12 06:48:50",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4ygss5",
          "author": "CapMonster1",
          "text": "For me it really depends on the target. If itâ€™s a simple/static site, I still go with Python & requests & BeautifulSoup, super fast and low overhead. For anything JS-heavy, Playwright has been my go-to lately, feels more stable than Puppeteer and handles modern frontends better.\n\nScrapy is great when you need structure and scale, but for small side projects it can feel like overkill. Proxies depend on how aggressive the site ismsometimes you donâ€™t need them, but once you hit rate limits or bot protection, rotation becomes mandatory.",
          "score": 1,
          "created_utc": "2026-02-12 10:05:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2hlzd",
      "title": "RAG for AI memory: why is everyone indexing databases instead of markdown files?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2hlzd/rag_for_ai_memory_why_is_everyone_indexing/",
      "author": "ProfessionalLaugh354",
      "created_utc": "2026-02-12 02:38:26",
      "score": 21,
      "num_comments": 7,
      "upvote_ratio": 0.89,
      "text": "I've been building memory systems for agents and noticed something weird. Most memory solutions follow this pattern:\n\n**Standard RAG approach (Mem0, Zep, etc.):**\n\n* Store memories in database (PostgreSQL, MongoDB, whatever)\n* Query through APIs\n* To inspect: write code to query DB\n* To edit: call update endpoints\n* To migrate: export â†’ transform â†’ reimport\n\n**Alternative approach (inspired by OpenClaw):**\n\n* Store memories in markdown files\n* Embed and index in vector store (same as above)\n* Query through APIs (same as above)\n* To inspect: `cat memory/MEMORY.md`\n* To edit: vim/VSCode the file, auto-reindexes\n* To migrate: `cp -r memory/ new-system/`\n\nThe retrieval layer is identical - both use vector search + reranking. The only difference is the source of truth.\n\n**Why markdown seems better for memory:**\n\n**Debuggability** \\- When retrieval returns wrong context, you can grep through source files instead of writing DB queries. `rg \"Redis config\" memory/` beats SQL any day.\n\n**Version control** \\- `git log memory/MEMORY.md` shows you exactly when bad info entered the system. Database audit logs? Painful.\n\n**Chunk inspection** \\- See the actual document structure. Databases flatten everything into rows. Markdown preserves semantic boundaries (headings, paragraphs).\n\n**Hybrid search** \\- BM25 keyword search works naturally on markdown. On JSON in databases? Need full-text indexes and special config.\n\n**Cold start** \\- New developer? `git clone`, read the markdown, understand what the AI knows. Database? Need credentials, connection, schema knowledge.\n\n**The RAG perspective:**\n\nFrom a pure retrieval standpoint, markdown has advantages:\n\n* Semantic chunking is easier (split by headings/paragraphs)\n* Context preservation (you can read surrounding text naturally)\n* Deduplication is straightforward (content hash)\n* A/B testing embeddings is trivial (reindex from source)\n\n  \n==========================\n\n**So,,,, What I built:**\n\nGot so convinced by this I built \\`memsearch\\` , a memory package ready for agent memory usage.\n\nhttps://github.com/zilliztech/memsearch\n\nbasically proper RAG over markdown files with:\n\n* Hybrid search (vector + BM25, weighted fusion)\n* File watching + auto-indexing\n* Chunk deduplication (saves 20-30% on embedding costs)\n* Framework agnostic\n\n**My question to the community:**\n\nIs there a technical reason database-first is better that I'm missing? Or is it just convention?\n\nThe only argument I hear is \"scale\" but most agent memory is < 100MB even after months. That's nothing for modern RAG systems.\n\nWould love to hear from people who've built production RAG systems. What breaks when you use files instead of databases?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1r2hlzd/rag_for_ai_memory_why_is_everyone_indexing/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4x2y1f",
          "author": "Ok_Signature_6030",
          "text": "you're right that files work great for the debugging and versioning use cases. we actually use markdown-based memory for a few internal tools and git log on the memory files has saved us more than once.\n\nthe place it breaks down for us is concurrent access. when you have multiple agents writing to the same memory store at the same time, file locking becomes a real headache. databases handle that natively. the other thing is structured queries... like if you need \"show me all memories tagged with customer X from the last 30 days\" that's trivial in sql but painful to grep through markdown.\n\nfor single-agent setups under 100mb though, yeah honestly files are probably underrated as a source of truth.",
          "score": 12,
          "created_utc": "2026-02-12 03:06:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4xydfk",
              "author": "ProfessionalLaugh354",
              "text": "Thatâ€™s exactly why we use a vector database to asynchronously sync content from Markdownâ€”only reindexing changed chunks. We then use ANN or hybrid search for queries, which solves both concurrency and structured search neatly.",
              "score": 1,
              "created_utc": "2026-02-12 07:06:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4xx4et",
          "author": "-Cubie-",
          "text": "Very cool! Is there also CrossEncoder reranking possible? I find it helps retrieval nicely, e.g.ms-marco-MiniLM-L6-v2",
          "score": 2,
          "created_utc": "2026-02-12 06:55:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4ys4q5",
          "author": "RobertLigthart",
          "text": "100% agree on this. I use markdown files for all my project context and agent memory and its just so much simpler to debug and maintain\n\n  \nthe concurrency argument is valid for multi-agent setups but honestly most people arent running multiple agents writing to the same memory simultaneously. for single agent workflows (which is like 90% of use cases) files work perfectly\n\n  \nbiggest win for me is the git versioning. being able to git diff your AI's memory and see exactly what changed is incredibly useful for debugging weird behavior. try doing that with a postgres table",
          "score": 2,
          "created_utc": "2026-02-12 11:47:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o52b6eo",
          "author": "jcheroske",
          "text": "What do you think of this: [https://github.com/basicmachines-co/basic-memory](https://github.com/basicmachines-co/basic-memory)",
          "score": 2,
          "created_utc": "2026-02-12 22:32:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4y521y",
          "author": "TeeRKee",
          "text": "Did you consider the hardware req to locally embed and manage the whole vector db pipeline?",
          "score": 1,
          "created_utc": "2026-02-12 08:09:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4zwtis",
          "author": "owlpellet",
          "text": "Initial thoughts:\n\n1. I'm curious how often you're updating embedding. Embedding is expensive at scale.\n2. putting user data into git is a non-starter for production systems in my org. If agent memory has PII in it, you're putting that into a database, with access controls.\n\nMy guess is that once you start covering edge cases, you are now a database developer. Postgres is pretty good. \n\nAgree that optimizing for inspectability is a good idea. Dev environment vs test/stage/prod environment maybe.",
          "score": 1,
          "created_utc": "2026-02-12 15:43:23",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qy7dhz",
      "title": "Legaltech for Singapore with RAG (version 2)(open source â­)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qy7dhz/legaltech_for_singapore_with_rag_version_2open/",
      "author": "Fantastic_suit143",
      "created_utc": "2026-02-07 07:19:06",
      "score": 18,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "Hey everyone,\n\nA few Days back, I talked about my pet project, which is a RAG-based search engine on Singaporean laws and acts (Scraping 20,000 pages/sec) with an Apple-inspired user interface.\n\nThis project is open source meaning anyone can use my backend logic but do read the license provided in the GitHub.(Star the repo if you liked it.)\n\nThe community posed some fantastic and challenging questions on answer consistency, complex multi-law queries, and hallucinations. These questions were just incredible. Rather than addressing these questions or issues with patches and fixing them superficially, I decided to revisit the code and refactor significant architectural changes.This version also includes reference to page number of the pdf while answering i have achieved that using metadata while I also building the vector database.\n\nI look forward to sharing with you Version 2.\n\nThe following are specific feedback that I received, and how I went about engineering the solutions:\n\nThe Problem: \"How do you ensure answer quality doesn't drop when the failover switches models?\"\n\nThe Feedback: My back-end has a \"Triple Failover\" system (three models, triple the backups!). I was concerned that moving from a high-end model to a backup model would change the \"answer structure\" or \"personality,\" giving a \"jarring\" effect to the user. The V2 Fix: Model-Specific System Instructions. I have no ability to alter the underlying intelligence of my backup models, so I had to normalize the output of my back-end models. I implemented a dynamic instruction set. If the back-end should fail over to Model B, I inject a specific \"system prompt\" to encourage Model B to conform to the same structure as Model A.\n\n2. The Problem: \"Single queries miss the bigger picture (e.g., 'Starting a business' involves Tax, Labor, AND Banking laws).\"\n\nThe Feedback: A simple semantic search for â€œstarting a businessâ€ could yield the Companies Act but completely overlook the Employment Act or Income Tax Act. The V2 Fix: Multi-Query Retrieval (MQR). I decided the cost of computation for MQR was worth it. What we now do is, when you pose an open-ended question, an LLM catches the question and essentially breaks it down into sub-questions that could be â€œBusiness Registration,â€ â€œCorporate Taxation,â€ â€œHiring Regulations,â€ etc. It's a more computationally intensive process, but the depth of the answer is virtually night and day from V1.\n\n3. The Problem: \"Garbage In, Garbage Out (Hallucinations)\n\nThe Feedback: If the search results contain an irrelevant document, the LLM has two choices: either hallucinate an answer or say \"I don't know.\"\nThe V2 Fix: Re-Ranking with Cross-Encoders: I decided to introduce an additional validation layer. Once the initial vector search yields the primary results, the Cross-Encoder model \"reads\" them to ensure that they're indeed relevant to the query before passing them along to the LLM. If they're irrelevant, the results are discarded immediately, greatly reducing the incidence of hallucinations and \"confidently wrong\" answers.\n\n4. The Problem: Agentic Capabilities\n\nAgentic Behavior: Iâ€™ve improved the backend logic so that it is less passive. It is moving towards becoming an agent that can interpret the â€œintentâ€ behind the search terms, not just match words.\n\nVersioning: This is the hardest nut to crack, but I've begun to organize the data to enable versioning in subsequent updates.\n\nTech Stack Recap\n\nFrontend: Apple-inspired minimalist design.\n\nUsing: BGE-M3 as text embedder\n\nBackend: Triple Failover System - 3 AI Models\n\nNew in V2: FAISS + Cross-Encoder Re-ranking + Multi-Query Retrieval. I'm still just a student and learning every day. The project is open source, and I would love it if you could tear it apart again so that I could create Version 3. Links:\n\nGitHub Repo: [https://github.com/adityaprasad-sudo/Explore-Singapore/\n](https://github.com/adityaprasad-sudo/Explore-Singapore/)\nThanks for the user who asked those questionsâ€”you literally shaped this update!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qy7dhz/legaltech_for_singapore_with_rag_version_2open/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4am8io",
          "author": "Academic_Track_2765",
          "text": "nice! I like that you actually used cross encoder reranking with multi query retrieval. I don't have much time, but I will take a look, you can also implement an llm based classifier with the cross-encoder reranking stage to further improve document routing. ",
          "score": 2,
          "created_utc": "2026-02-08 18:34:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4906sa",
          "author": "FakruddinBaba",
          "text": "Great work, my friend ðŸ‘ Really impressive execution.\n\nIâ€™m also a student currently learning AI agents and RAG systems using the LangChain ecosystem, and Iâ€™m at a stage where exploration feels a bit overwhelmingâ€”especially deciding what skills truly matter for industry-grade systems and how to translate learning into meaningful projects.\nMy goal is to move beyond toy demos and build real-world, production-oriented agents: systems that are reliable, optimistic in decision-making, fault-tolerant, and resilient to errors. Iâ€™m particularly curious about designing agents that handle failures, retries, evaluation, logging/observability, and long-term maintainabilityâ€”even if the project itself is conceptually simple.\n\n\nIâ€™d love to learn from your experience:\n\nHow did you structure your learning journey?\nWhich skills or concepts gave you the biggest leap toward production-level thinking?\n\n\nWhere did you learn theseâ€”resources, projects, or real-world exposure?\nAnd could you suggest a clear roadmap or realistic industry-style project ideas that actually run end-to-end?\n\n\nYour work is genuinely inspiring, and any guidance would mean a lot. Thanks for sharing your knowledge.",
          "score": 1,
          "created_utc": "2026-02-08 13:36:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ans3r",
              "author": "Fantastic_suit143",
              "text": "Wonderful comment mate well I was always interested in coding and building something out of it so that it's helpful for anyone and can learn by looking at my code \n1. No I didn't structure my learning journey actually I have a time slot like 10pm to 12am and in that I use reddit and basically just watch some lectures and build my knowledge and share it too!\n2.Well learning python and a little bit of how llm's work what is rag gave me a good boost in my productivity \n3.well the biggest material is youtube it basically learned everything from that and sometimes I used claude and gemini to help me with error in my code.for the real world exposure part for now I just exploring the world as I am still learning and new to these things\n4.i think like building something usefull like instead of just using ai in everything we should prioritize it more on fixing our lives like whose gonna use ai button on a fridge when I just want some food \nSo my suggestion is making something that makes ai not hallucinate and helps out daily lives.\n\nThanks God bless you for the journey ahead!â˜ºï¸",
              "score": 1,
              "created_utc": "2026-02-08 18:41:00",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r16mrx",
      "title": "A Practical RAG Roadmap to Stop LLM Apps from Failing in Production",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r16mrx/a_practical_rag_roadmap_to_stop_llm_apps_from/",
      "author": "devasheesh_07",
      "created_utc": "2026-02-10 17:11:32",
      "score": 18,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "After building and breaking a few LLM-based systems, Iâ€™ve realized something uncomfortable:\n\nMost failures donâ€™t come from â€œbad models.â€  \nThey come from **bad retrieval**.\n\nRAG (Retrieval-Augmented Generation) isnâ€™t a framework or a prompt trick â€” itâ€™s a systems problem. Hereâ€™s the **short roadmap** that finally made it click for me:\n\n**1. Start with embeddings**  \nIf you donâ€™t understand how text becomes vectors and how similarity search works, nothing else matters.\n\n**2. Use a vector database properly**  \nStore embeddings, not raw text. Learn indexing, filtering, and why top-k retrieval isnâ€™t magic.\n\n**3. Chunking is everything**  \nMost vague or wrong answers come from poor chunking, not the LLM. This step is underrated.\n\n**4. Retrieval > model choice**  \nA smaller model with clean, relevant context beats a larger model with noisy retrieval.\n\n**5. Treat RAG like a system, not a demo**  \nLogging, evaluation, failure cases, and feedback loops matter more than fancy prompts.\n\nThis mindset shift helped me understand why many â€œLLM appsâ€ look great in demos but fall apart in real use.\n\nI wrote a **fully detailed article** breaking this down step by step   \n[https://www.loghunts.com/rag-in-ai-ml-practical-learning-roadmap](https://www.loghunts.com/rag-in-ai-ml-practical-learning-roadmap)  \nIf anything here is wrong or oversimplified, Iâ€™m very open to corrections â€” happy to update the article.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r16mrx/a_practical_rag_roadmap_to_stop_llm_apps_from/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4nj0o0",
          "author": "HandSuspicious1579",
          "text": "This is something i would not prefer",
          "score": 1,
          "created_utc": "2026-02-10 17:56:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4olojx",
          "author": "Late_Special_6705",
          "text": "404",
          "score": 1,
          "created_utc": "2026-02-10 20:55:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz41dq",
      "title": "Addressing a fundamental flaw in hybrid search by introducing a Log-Odds Conjunction framework in Bayesian BM25",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qz41dq/addressing_a_fundamental_flaw_in_hybrid_search_by/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-08 08:53:42",
      "score": 16,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "[https://github.com/instructkr/bb25/pull/1](https://github.com/instructkr/bb25/pull/1)\n\nTo the Information Retrieval Community:  \nA significant update has been merged into the Bayesian BM25 (bb25) repository today!\n\nThis update addresses a fundamental flaw in hybrid search known as Conjunction Shrinkage by introducing a Log-Odds Conjunction framework.\n\nIn traditional probabilistic retrieval, calculating the probability that multiple signals are simultaneously satisfied typically relies on the Naive Product Rule.\n\nFor instance, if a document is relevant based on keyword search with a probability of 0.7 and also relevant based on vector semantic search with a probability of 0.7, the standard approach multiplies these to yield 0.49.\n\nIntuitively, however, if two independent pieces of evidence both suggest a document is relevant, our confidence should increase beyond 0.7.\n\nThe product rule causes the final score to decrease toward zero as more signals are added, violating the intuition that corroborating evidence should amplify confidence.\n\nThe solution implemented in this PR resolves this by shifting the calculation from probability space to log-odds space. The mechanism operates in three stages: first, it computes the geometric mean to find the baseline tendency; second, it performs a Log-Odds Transformation to map the bounded probability space to the unbounded log-odds space; and third, it adds a bonus proportional to the logarithm of the number of signals.\n\nThis works because probability space is bounded by 1.0, preventing simple addition. By transforming to log-odds space, we remove this ceiling. Instead of the score shrinking to 0.49, the logic applies an additive bonus for agreeing signals, resulting in amplification where the final score becomes roughly 0.83.\n\nThis implementation is the proof that this structure is not merely a heuristic. The paper demonstrates that rigorous Bayesian inference over multiple signals produces a computational structure formally isomorphic to a feedforward neural network.\n\nThis work proves that the Sigmoid activation function is a mathematical necessity that emerges when converting Bayesian evidence into probability, rather than an arbitrary design choice. Consequently, this implementation demonstrates that a neural network is the natural structure of correct probabilistic reasoning.\n\nThe introduction of Log-Odds Conjunction has yielded measurable improvements on the SQuAD v2.0 benchmark compared to the standard Hybrid OR approach marking a +1.2% improvement.\n\nThis confirms that properly modeling the agreement between text and vector signals yields better ranking performance than simple score summation or probabilistic multiplication. I would like to extend our gratitude to Jaepil for deriving these proofs and contributing the code to bb25.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qz41dq/addressing_a_fundamental_flaw_in_hybrid_search_by/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o48e0fh",
          "author": "Professional-Fox4161",
          "text": "I have a question. I previously understood that BB25 was order-preserving relative to BM25, but the differing performance on several metrics seems to show that it's not true. Could you help me understand ?",
          "score": 3,
          "created_utc": "2026-02-08 10:37:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4dfyib",
              "author": "jaepil",
              "text": "I'm the author of the paper. That is an excellent question and shows youâ€™ve read the theorem carefully.\n\nYou are correct that **Theorem 4.3.1** guarantees monotonicity (order-preservation) relative to BM25, but this holds strictly **'for a fixed prior p'**.\n\nHowever, in practice (as detailed in **Section 4.2**), we often apply a **Composite Prior** that incorporates term frequency and document length signals. Because this prior varies dynamically per document, it introduces a Bayesian re-ranking effect that can slightly alter the order compared to raw BM25.\n\nFurthermore, even if the text-only order were identical, the **non-linear sigmoid transformation** changes the *relative distribution* of scores. In a hybrid setting, this calibrated distribution interacts differently with vector scores compared to unbounded BM25 scores, which naturally leads to different (and often improved) ranking metrics.",
              "score": 2,
              "created_utc": "2026-02-09 03:29:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4dhr3r",
                  "author": "jaepil",
                  "text": "Most importantly, regardless of slight ranking shifts, theÂ **engineering efficiency remains intact**.\n\nAs proven inÂ **Theorem 6.1.2**Â andÂ **Theorem 6.2.1**, the Bayesian transformation is strictly monotonic. This means we can directly utilize existingÂ **WAND**Â andÂ **Block-Max WAND (BMW)**Â dynamic pruning algorithms without any modification to the inverted index structure.\n\nIn practice, this ensures that Bayesian BM25 incursÂ **O(1) overhead**Â per document (Theorem 9.1.1) and maintains the same query latency profile as standard BM25, making it immediately deployable in production systems like Vespa or Lucene.",
                  "score": 0,
                  "created_utc": "2026-02-09 03:40:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o48rbxy",
          "author": "RobertLigthart",
          "text": "the core intuition makes total sense... if keyword and semantic search both agree a doc is relevant, multiplying probabilities and getting a lower score than either signal alone is obviously wrong. nice to see someone formalize it instead of just hacking around it with weighted sums",
          "score": 2,
          "created_utc": "2026-02-08 12:35:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyvjt1",
      "title": "Lightrag Graph RAG performing worse than open web UI semantic search with similar setup.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qyvjt1/lightrag_graph_rag_performing_worse_than_open_web/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-02-08 01:27:42",
      "score": 16,
      "num_comments": 14,
      "upvote_ratio": 0.95,
      "text": "Hi folks,\nIâ€™ve set up a Graph RAG pipeline using LightRAG with:\nbge-m3 embeddings\nphi-4 / gpt-oss-20B as LLM\nNeo4j for knowledge graph\nMilvus for semantic search\n\nIn parallel, Iâ€™m also running a simpler setup:\nOpen WebUI\nDocling for document parsing\nbge-m3 + vector search (no graph)\n\nSurprisingly, the plain RAG + Docling setup consistently gives better answers for:\ndocument Q&A\nexplanations\nclause lookup\nsummaries\n\nThe Graph RAG feels more brittle and often misses relevant context.\n\nMy questions:\nIs this a known/expected behavior with Graph RAG?\nDoes Graph RAG usually underperform flat RAG unless the use case is explicitly relational?\nAre people using graphs mainly as rerankers / secondary reasoning layers rather than primary retrieval?\n\nWould love to hear real-world experiences before I invest more time tuning the graph side.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qyvjt1/lightrag_graph_rag_performing_worse_than_open_web/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o46k6l7",
          "author": "sp3d2orbit",
          "text": "This is pretty normal from what Iâ€™ve seen.\n\nFor document Q&A, flat RAG tends to outperform Graph RAG. Clause lookup, summaries, and explanations depend on preserving local context, and graphs often lose that unless the schema is very deliberately designed. Missing or imperfect edges hurt recall fast.\n\nIn our work we still start with graph-style retrieval, but itâ€™s ontology-driven rather than document-driven. The graph narrows the candidate space or enforces constraints, and then we fall back to text retrieval for the actual answer. Treating the graph as structure and control, not as the main text retriever, has worked better for us.",
          "score": 4,
          "created_utc": "2026-02-08 01:50:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o46lsz9",
              "author": "wahnsinnwanscene",
              "text": "Do you auto generate the ontology or hand design?",
              "score": 1,
              "created_utc": "2026-02-08 02:00:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o46o4jr",
                  "author": "sp3d2orbit",
                  "text": "Always programmatically generated from the input documents and it can be grounded in a well known ontology like ICD-10 or SNOMED. Or sometimes generate a grounding ontology from a specialized domain. ",
                  "score": 2,
                  "created_utc": "2026-02-08 02:15:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4899l0",
          "author": "Visual_Algae_1429",
          "text": "20b is nothing for graph rag, use 120b minimum",
          "score": 2,
          "created_utc": "2026-02-08 09:52:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46qxxg",
          "author": "penguinzb1",
          "text": "yeah docling's chunking is really good for local context. graphs shine when you need to traverse relationships or enforce constraints, but for straight document retrieval they add more overhead than value unless your schema is dialed in",
          "score": 1,
          "created_utc": "2026-02-08 02:32:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46z7cp",
          "author": "D_E_V_25",
          "text": "Hi buddy!! \nI was one there with the same issue as well.. \n\nhttps://www.reddit.com/r/Rag/s/1ttwNuIlVy \n\nAbove is the post I made here yesterday about hierchial agentic rag with hybrid search ( Knowledge graph + vector search )  and another json rag as well u can choose based on your usage \n..\n\nGive a visit u will get the whole pictur and it might help u set up your own as well  \n\n..\n\nAs for your question I feel ... U might be using graph rag in a wrong and treating it as a king...\n\nIn my 693k stored chunks which is 10% of what raw I had .. 250gb of raw data .. and I stored and things are working as well with a score of 0.878 on scale of 1..\n\nI have made things open source u give a visit and star the repo as very soon I will teach the folks to how to implement them into your own projects ...\n\nCurrently what I made was for my university and sorry but I can't share the whole thing but no worries.. If community loves this idea and projects..\n\nThis time I feel we will together make a 500gb+ thing and whole another level of thing .. ðŸ˜ŽðŸ˜Ž",
          "score": 1,
          "created_utc": "2026-02-08 03:25:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o47bn4v",
          "author": "Curious-Sample6113",
          "text": "Not surprised. Saw similar results along the same lines.",
          "score": 1,
          "created_utc": "2026-02-08 04:52:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49qrkf",
          "author": "No_Wrongdoer41",
          "text": "Me and a small team have built a platform to automatically build a graph for rag out of your documents. We find it's way better than literag. Would love for you to try it (for free) and compare!",
          "score": 1,
          "created_utc": "2026-02-08 16:03:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4d0wf3",
              "author": "No_Presence4293",
              "text": "Can i get more info?",
              "score": 1,
              "created_utc": "2026-02-09 02:10:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4d11u0",
                  "author": "No_Wrongdoer41",
                  "text": "yep dming you",
                  "score": 1,
                  "created_utc": "2026-02-09 02:11:12",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4eizvc",
          "author": "Equivalent_Tie4071",
          "text": "There is no one size fits all solution in the RAG world. Your doc type, doc quialty, doc relationship, data curation, model used for data process, all play a role on the quality of the retrival. \n\nDoc relationship is the key for using Graph RAG. â€œWhat companies compete with our suppliers in Europe?â€, your plain RAG will struggle to find an answer but Graph RAG will easily give you the list.",
          "score": 1,
          "created_utc": "2026-02-09 08:40:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4f9dav",
          "author": "AICodeSmith",
          "text": "This makes sense honestly. Weâ€™ve seen flat RAG outperform graph setups for straight document Q and A and summaries too. Graphs seem to help more when the question really needs relationships or multi step reasoning, otherwise they can feel a bit fragile.\n\n",
          "score": 1,
          "created_utc": "2026-02-09 12:38:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxa4fq",
      "title": "Built a Website Crawler + RAG (fixed it last night ðŸ˜…)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:25:37",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "Iâ€™m **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff ðŸ’»), I thought:  \nEveryone is feeding PDFsâ€¦ **why not try something thatâ€™s not PDF ingestion?**\n\nSo I focused on fixing the **real problem â€” crawling quality**.\n\nðŸ”— GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**Whatâ€™s better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well ðŸ‘€  \nFeedback, suggestions, and â­s are welcome!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3uy709",
          "author": "Legitimate-Fun7608",
          "text": "the crawler quality insight is spot on. most people underestimate how much bad extraction ruins everything downstream.\n\n  \ncurious - how are you handling duplicate content across pages? (like shared headers/footers that make it through, or pages with similar structure). playwright helps but usually need some fuzzy matching too.",
          "score": 2,
          "created_utc": "2026-02-06 06:32:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v3m2h",
              "author": "Cod3Conjurer",
              "text": "    # Deduplication check\n\n                    import hashlib\n\n                    content_hash = hashlib.md5(copied_text.encode('utf-8')).hexdigest()\n\n                    if content_hash in content_hashes:\n\n                        print(f\"Skipping duplicate content: {clean_url}\")\n\n                        continue\n\n                    content_hashes.add(content_hash)\n\nRight now, I just strict content hashing (MD5) to catch identical pages and URL normalization. I haven't implemented footer-stripping or MinHash yet because the vector store is surprisingly creating good separation between the 'content' chunks and the 'boilerplate' chunks on its own ðŸ˜‚  \n",
              "score": 1,
              "created_utc": "2026-02-06 07:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2st4j",
      "title": "Vectorless RAG (Why Document Trees Beat Embeddings for Structured Documents)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r2st4j/vectorless_rag_why_document_trees_beat_embeddings/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-12 13:01:07",
      "score": 15,
      "num_comments": 18,
      "upvote_ratio": 0.81,
      "text": "I've been messing around with vectorless RAG lately and honestly it's kind of ridiculous how much we're leaving on the table by not using it properly.\n\nThe basic idea makes sense on paper. Just build document trees instead of chunking everything into embedded fragments, let LLMs navigate structure instead of guessing at similarity. But the way people actually implement this is usually pretty half baked. They'll extract some headers, maybe preserve a table or two, call it \"structured\" and wonder why it's not dramatically better than their old vector setup.\n\nThink about how humans actually navigate documents. We don't just ctrl-f for similar sounding phrases. We navigate structure. We know the details we want live in a specific section. We know footnotes reference specific line items. We follow the table of contents, understand hierarchical relationships, cross reference between sections.\n\nIf you want to build a vectorless system you need to keep all that in mind and go deeper than just preserving headers. Layout analysis to detect visual hierarchy (font size, indentation, positioning), table extraction that preserves row-column relationships and knows which section contains which table, hierarchical metadata that maps the entire document structure, and semantic labeling so the LLM understands what each section actually contains.\"\n\nTested this on a financial document RAG pipeline and the performance difference isn't marginal. Vector approach wastes tokens processing noise and produces low confidence answers that need manual follow up. Structure approach retrieves exactly what's needed and answers with actual citations you can verify.\n\nI think this matters more as documents get complex. The industry converged on vector embeddings because it seemed like the only scalable approach. But production systems are showing us it's not actually working. We keep optimizing embedding models and rerankers instead of questioning whether semantic similarity is even the right primitive for document retrieval.\n\nAnyway feels like one of those things where we all just accepted the vector search without questioning if it actually maps to how structured documents work.\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r2st4j/vectorless_rag_why_document_trees_beat_embeddings/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4zo9de",
          "author": "Distinct-Target7503",
          "text": "Just a question... how do you build the tree? do you feed each page of a document to a VLM and rely on its ocrd text? isn't that quite expensive in term of tokens?\n\nI ask because, even without tabs/images, usually headers extraction from PDFs is not really reliable",
          "score": 1,
          "created_utc": "2026-02-12 15:01:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o53kiyt",
              "author": "Clipbeam",
              "text": "This. It would either be super expensive or slow and error prone if you try do use local models.",
              "score": 1,
              "created_utc": "2026-02-13 02:57:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o50pwzw",
          "author": "bac2qh",
          "text": "I am going to test out pageindex for my openclaw soon because the idea makes sense to me and embeddings feel underwhelming for my use case. I have a feeling that itâ€™s going to consume a lot of tokens though",
          "score": 1,
          "created_utc": "2026-02-12 17:59:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o51691m",
              "author": "Single-Constant9518",
              "text": "Pageindex sounds like an interesting approach! Token consumption can be a concern, but if it helps structure your data more effectively, it might be worth the trade-off. Just keep an eye on how it handles complex documents; thatâ€™s where the real benefits could show.",
              "score": 1,
              "created_utc": "2026-02-12 19:15:38",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o517ina",
                  "author": "bac2qh",
                  "text": "Yeah hopefully it can turn to a POC at work too.",
                  "score": 1,
                  "created_utc": "2026-02-12 19:21:43",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o51z34r",
          "author": "Intrepid-Scale2052",
          "text": "Do you mean the RAG first searches by document metadata, and then searches inside the document. Instead of the contents? \n\nI'm interested, I'm trying to build a searchable archive. What if the header does not say enough? What if you want to search, \"can you find me historical accounts of xxx?\"",
          "score": 1,
          "created_utc": "2026-02-12 21:33:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o538kcc",
          "author": "licjon",
          "text": "I think it depends on file formats, domain, and purpose. I think a layered approach is the way to go. I prefer to filter with FTS, then do a semantic search on the filtered candidates. ",
          "score": 1,
          "created_utc": "2026-02-13 01:43:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4z2stx",
          "author": "Independent-Cost-971",
          "text": "\n\nWrote this up in more detail if anyone's interested :Â [https://kudra.ai/vectorless-rag-why-document-tree-navigation-outperforms-semantic-search/](https://kudra.ai/vectorless-rag-why-document-tree-navigation-outperforms-semantic-search/)\n\n( shameless plug I know but worth a read )",
          "score": 0,
          "created_utc": "2026-02-12 13:03:09",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4z86co",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -3,
          "created_utc": "2026-02-12 13:35:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4zf26g",
              "author": "exaknight21",
              "text": "I use knowledge graphs + hybrid search, i donâ€™t have this issue? My use case requires semantic relationship trees, technical requirements in this document -> section page 10 section 2.10 Manufacturers item a. Benjamin Moore (specified) paint. Or in a spec table page 67 item 13 requires 200 SF of abatemet in room 201 library.\n\nThis is scalable per project and is working. I am baffled why it wonâ€™t work for you.",
              "score": 5,
              "created_utc": "2026-02-12 14:13:40",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4znp49",
                  "author": "Distinct-Target7503",
                  "text": ">knowledge graphs\n\nhow do you build the knowledge graph?\nit is usually really expensive in terms of tokens, or am I doing something wrong?",
                  "score": 1,
                  "created_utc": "2026-02-12 14:58:53",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r0u2in",
      "title": "Need help with chunking and embedding strategies for my rag model",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1r0u2in/need_help_with_chunking_and_embedding_strategies/",
      "author": "Forsaken-Cod-4944",
      "created_utc": "2026-02-10 07:08:26",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m fairly new to this field, and after reading a few posts here, I realized that to get accurate and reliable results, I need to carefully design a chunking and embedding strategy. The problem is, Iâ€™m not really sure how to approach that yet.\n\nHereâ€™s what Iâ€™ve built so far:\n\nI created a RAG-based model for companies that can be integrated into their app or website. The idea is that employees or anyone interested can use it to research and ask questions about the company. The data it handles includes:\n\n* Company policies (long, formal documents)\n* HR documents (rule-based and structured)\n* FAQs (short Q&A format)\n* Financial summaries (number-heavy content)\n* Product documentation (technical text and details about previous projects)\n* CSV structured data (tabular format)\n* Website content (marketing and general information)\n\nSince Iâ€™m a second-year student, I canâ€™t afford paid services, so Iâ€™m planning to run everything locally. Right now, Iâ€™m considering using **baai/bge-m3** for embeddings and retrieval, and **Qwen3â€“1.7B-MLX-8bit** for answer generation.\n\nI also have a question about deployment. Iâ€™m planning to dockerize the entire system for a live demo on my resume. If I run the LLM locally, will it still work for someone accessing it from another device? Or would they also need the model running on their own machine?\n\nIâ€™d really appreciate any guidance or suggestions. Thanks!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1r0u2in/need_help_with_chunking_and_embedding_strategies/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o4l63n3",
          "author": "RobertLigthart",
          "text": "for the chunking honestly dont overthink it at the start. use something like recursive character splitting with \\~500-800 tokens and some overlap (\\~100 tokens). the key thing most people miss is that different doc types need different chunk sizes -> FAQs should stay as complete Q&A pairs, policies can be chunked by section headers, and CSV data you probably want to convert to natural language sentences before embedding\n\n  \nbge-m3 is a solid choice for embeddings especially since its multilingual. for deployment if you dockerize it and host it on a VPS or even a free tier cloud instance the LLM runs server-side so anyone can access it through the API... they dont need anything on their machine. just be aware that Qwen 1.7B is going to be pretty limited for complex questions tho",
          "score": 2,
          "created_utc": "2026-02-10 09:30:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4l8s85",
              "author": "Forsaken-Cod-4944",
              "text": "Thanks for the clarity but i couldnt understand the llm part, im planning to dockerize and upload my project on [render.com](http://render.com) so my question was will the query generation part work on other devices even if my device is switch off or not running?\n\n",
              "score": 1,
              "created_utc": "2026-02-10 09:56:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4lz01b",
          "author": "notsarthaxx",
          "text": "What are your gpu specs?",
          "score": 1,
          "created_utc": "2026-02-10 13:20:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4m7rb5",
              "author": "Forsaken-Cod-4944",
              "text": "4050 6gb, 55W (Laptop)",
              "score": 1,
              "created_utc": "2026-02-10 14:09:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4n8q9c",
                  "author": "notsarthaxx",
                  "text": "damn does running these models lag?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-10 17:09:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4mp9xh",
          "author": "Popular_Sand2773",
          "text": "You are describing a highly diverse and often large dataset i.e. an entire corporate knowledge base. If you one size fits all this you'll certainly have something but with such a weak llm 1.7B params its going to be hard to cover up ugly/messy retrieval. \n\nIf you want to tackle all of these hierarchical search and provenance are your friends. Honestly though I would take these one at a time and do what's right for each piece then have the agent use the appropriate tool for each job rather than a monolithic retriever. ",
          "score": 1,
          "created_utc": "2026-02-10 15:39:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4n6uys",
              "author": "Forsaken-Cod-4944",
              "text": "I just need the basics since i'm pretty new to this , when i get the logic and build something that can atleast work on low-medium dataset , I'll try switching to 8-70B parameter llms.\n\nand about provenance , is it a type of reranker? if not then can it be used with it",
              "score": 1,
              "created_utc": "2026-02-10 17:00:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4osozg",
                  "author": "Popular_Sand2773",
                  "text": "That makes sense every time I dip my toes in models that size it ends up being rough. Provenance is keeping track of where things come from. So instead of putting everything into a db you add metadata like faq vs HR vs policy. Then an agent can do something like a filtered search. Oh this question is likely about policy let me check policy specific documents. Lots of ways to execute that's just a basic example of using metadata to limit the search space and improve results. These filters are usually applied pre-re-ranker so go nuts!",
                  "score": 1,
                  "created_utc": "2026-02-10 21:27:41",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4xw7j8",
          "author": "SharpRule4025",
          "text": "For corporate knowledge bases with mixed document types, the chunking strategy matters a lot more than the embedding model. A few things that helped me when I was working on something similar.\n\nPDFs and Word docs, convert to markdown first so you have consistent formatting. For web content, strip everything except the actual body content before chunking. Navigation, headers, footers, all of that pollutes your embeddings.\n\nFor chunk size, 500 to 800 tokens with 100 token overlap is a safe starting point. But the real win comes from respecting document structure. Split on headings and section boundaries instead of arbitrary character counts. A chunk that starts mid-paragraph and ends mid-sentence will retrieve poorly regardless of your embedding model.\n\nAlso, storing metadata alongside chunks (source document, section heading, document type) lets you filter at query time which makes a bigger difference than most retrieval tricks.",
          "score": 1,
          "created_utc": "2026-02-12 06:46:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}