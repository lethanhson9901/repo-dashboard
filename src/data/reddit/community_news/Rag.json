{
  "metadata": {
    "last_updated": "2026-01-17 16:48:01",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 162,
    "file_size_bytes": 210841
  },
  "items": [
    {
      "id": "1q9srmh",
      "title": "Announcing Kreuzberg v4",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q9srmh/announcing_kreuzberg_v4/",
      "author": "Goldziher",
      "created_utc": "2026-01-11 07:04:47",
      "score": 55,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Hi Peeps,\n\nI'm excited to announce [Kreuzberg](https://github.com/kreuzberg-dev/kreuzberg) v4.0.0. \n\n## What is Kreuzberg:\n\nKreuzberg is a document intelligence library that extracts structured data from 56+ formats, including PDFs, Office docs, HTML, emails, images and many more. Built for RAG/LLM pipelines with OCR, semantic chunking, embeddings, and metadata extraction. \n\nThe new v4 is a ground-up rewrite in Rust with a bindings for 9 other languages! \n\n## What changed:\n\n- **Rust core**: Significantly faster extraction and lower memory usage. No more Python GIL bottlenecks.\n- **Pandoc is gone**: Native Rust parsers for all formats. One less system dependency to manage.\n- **10 language bindings**: Python, TypeScript/Node.js, Java, Go, C#, Ruby, PHP, Elixir, Rust, and WASM for browsers. Same API, same behavior, pick your stack.\n- **Plugin system**: Register custom document extractors, swap OCR backends (Tesseract, EasyOCR, PaddleOCR), add post-processors for cleaning/normalization, and hook in validators for content verification.\n- **Production-ready**: REST API, MCP server, Docker images, async-first throughout.\n- **ML pipeline features**: ONNX embeddings on CPU (requires ONNX Runtime 1.22.x), streaming parsers for large docs, batch processing, byte-accurate offsets for chunking.\n\n## Why polyglot matters:\n\nDocument processing shouldn't force your language choice. Your Python ML pipeline, Go microservice, and TypeScript frontend can all use the same extraction engine with identical results. The Rust core is the single source of truth; bindings are thin wrappers that expose idiomatic APIs for each language.\n\n## Why the Rust rewrite:\n\nThe Python implementation hit a ceiling, and it also prevented us from offering the library in other languages. Rust gives us predictable performance, lower memory, and a clean path to multi-language support through FFI.\n\n## Is Kreuzberg Open-Source?:\n\nYes! Kreuzberg is MIT-licensed and will stay that way. \n\n## Links\n\n- [Star us on GitHub](https://github.com/kreuzberg-dev/kreuzberg)\n- [Read the Docs](https://kreuzberg.dev/)\n- [Join our Discord Server](https://discord.gg/38pF6qGpYD)\n",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q9srmh/announcing_kreuzberg_v4/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyxoxt2",
          "author": "butwhol",
          "text": "This is amazing. Thanks. Will definitely try out. I wonder how it compares to docling  esp on complex pdf’s. I am using docling and looking to replace it as its too slow and huge. Is this with one of ocr plugin comparable to docling?",
          "score": 5,
          "created_utc": "2026-01-11 07:24:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyxpxmq",
              "author": "Goldziher",
              "text": "its x50 times faster than docling on a CPU (not suprising, since docling is GPU orientated), and it works very well. Docling is superior in terms of complex layout extraction. So, test and see how it works for your use case.",
              "score": 7,
              "created_utc": "2026-01-11 07:33:06",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nyxpr9z",
              "author": "Anth-Virtus",
              "text": "Would love to know this, too",
              "score": 2,
              "created_utc": "2026-01-11 07:31:31",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz0jkqv",
          "author": "delapria",
          "text": "Are there any plans to support labeled bounding boxes in the future? That probably requires a layout/reading order model. \n\nFor me, that is the missing piece and gets you to the next level of performance for two main reasons: the labels make it possible to treat tables, figures etc differently. Second, the bounding boxes itself are useful for a lot of downstream tasks",
          "score": 2,
          "created_utc": "2026-01-11 18:32:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyz0vxa",
          "author": "CMPUTX486",
          "text": "Thx for sharing.  I want to try it with C#, but the installation guide page returns 404.. could you help me with that please?",
          "score": 1,
          "created_utc": "2026-01-11 14:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyz2daj",
              "author": "Goldziher",
              "text": "I'll check",
              "score": 1,
              "created_utc": "2026-01-11 14:13:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nyz7qf5",
          "author": "silenceimpaired",
          "text": "My gut feel is I’m not technically literate enough to integrate this into KOBOLDCPP, Text Gen UI (by Oobabooga) or Silly Tavern. \n\nIs it in any platform like those?",
          "score": 1,
          "created_utc": "2026-01-11 14:43:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nyzwzfw",
              "author": "Goldziher",
              "text": "Well, its not for the text generation side, its for getting text out of different documents and preparing it for processing in ML or RAG, or other such use cases. I would guess you could take docs you want to feed into any of these tools, and use Kreuzberg, yes. It shouldnt be difficult to do - you can ask an AI agent tool such as Claude Code or Gemini to install Kreuzberg as a CLI tool or using docker and even manually extract text from documents. Its very easy to do and the agent will do this quickly. Point it at the kreuzberg docs.",
              "score": 1,
              "created_utc": "2026-01-11 16:47:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nyzz5qg",
                  "author": "silenceimpaired",
                  "text": "I get this ends up in the prompt sent to the LLM… good idea asking AI for integration.",
                  "score": 2,
                  "created_utc": "2026-01-11 16:57:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz17a0g",
          "author": "Lanky-Cobbler-3349",
          "text": "Why do people still use text parsers?",
          "score": 1,
          "created_utc": "2026-01-11 20:17:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nyxxx23",
          "author": "drm00",
          "text": "Looks great! Will it be possible to use docling as an OCR plugin in the future?",
          "score": 0,
          "created_utc": "2026-01-11 08:46:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qd2e2b",
      "title": "RAG at scale still underperforming for large policy/legal docs – what actually works in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "author": "Flashy-Damage9034",
      "created_utc": "2026-01-14 23:00:50",
      "score": 53,
      "num_comments": 33,
      "upvote_ratio": 0.95,
      "text": "I’m running RAG fairly strong on-prem setup, but quality still degrades badly with large policy / regulatory documents and multi-document corpora. Looking for practical architectural advice, not beginner tips.\n\nCurrent stack:\n-Open WebUI (self-hosted)\n-Docling for parsing (structured output)\n-Token-based chunking\n-bge-m3 embeddings\n-bge-m3-v2 reranker\n-Milvus (COSINE + HNSW)\n-Hybrid retrieval (BM25 + vector)\n-LLM: gpt-oss-20B\n-Context window: 64k\n-Corpus: large policy / legal docs, 20+ documents\n-Infra: RTX 6000 ADA 48GB, 256GB DDR5 ECC\n\nObserved issues:\nCross-section and cross-document reasoning is weak\nIncreasing context window doesn’t materially help\nReranking helps slightly but doesn’t fix missed clauses\nWorks “okay” for academic projects, but not enterprise-grade\n\nI’m thinking of trying:\nGraph RAG (Neo4j for clause/definition relationships)\nAgentic RAG (controlled, not free-form agents)\n\nQuestions for people running this in production:\nHave you moved beyond flat chunk-based retrieval in Open WebUI? If yes, how?\nHow are you handling definitions, exceptions, overrides in policy docs?\nDoes Graph RAG actually improve answer correctness, or mainly traceability?\nAny proven patterns for RAG specifically (pipelines, filters, custom retrievers)?\nAt what point did you stop relying purely on embeddings?\n\nI’m starting to feel that naive RAG has hit a ceiling, and the remaining gains are in retrieval logic, structure, and constraints—not models or hardware.\nWould really appreciate insights from anyone who has pushed RAG system beyond demos into real-world, compliance-heavy use cases.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qd2e2b/rag_at_scale_still_underperforming_for_large/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzqkpdf",
          "author": "Soft-Speaker6195",
          "text": "You’re not wrong - flat chunk RAG hits a wall fast on policy/legal corpora because “correctness” is mostly about scope, definitions, exceptions, and precedence, not semantic similarity. The teams I’ve seen get this working stop treating retrieval as “top-k chunks” and start treating it as a constrained legal reasoning pipeline: definition expansion > scope filters > precedence rules > targeted clause fetch > answer with mandatory citations. Graph adds value when it’s used for precedence + definition binding, not as a generic knowledge graph. If you want a concrete example of how this looks in practice (especially definition/exception handling + audit-friendly outputs), AI Lawyer has some useful patterns you can mirror: strict citation gating, clause-level retrieval, and override/exception tracing.",
          "score": 15,
          "created_utc": "2026-01-15 15:00:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmphjq",
          "author": "Chemical_Orange_8963",
          "text": "Basically you are making GLEAN, research more on that on how it works",
          "score": 10,
          "created_utc": "2026-01-14 23:06:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolbyp",
          "author": "OnyxProyectoUno",
          "text": "Yeah, you've hit the ceiling that most people hit. Token-based chunking on legal docs is basically guaranteed to break cross-reference reasoning because it has no awareness of document structure.\n\nThe issue isn't your retrieval stack, it's what you're feeding it. Legal docs have explicit hierarchical relationships: definitions that apply to specific sections, exceptions that modify clauses, cross-references that span documents. Flat chunks destroy all of that before your embeddings ever see it. Doesn't matter how good your reranker is if the chunk boundaries cut through a definition-to-usage relationship.\n\nGraph RAG can help with traceability but it won't fix the upstream problem. You're still building the graph from chunks that already lost the structure. Same with agentic approaches, they're working with degraded inputs.\n\nWhat's actually worked in production for policy docs: semantic chunking that respects section boundaries, explicit metadata for document hierarchy (section > subsection > clause), and preserving cross-reference relationships as first-class data. You want chunks that know what section they belong to and what other sections they reference.\n\nDocling gives you structured output but are you actually using that structure for chunking decisions? Most people parse structured and then chunk flat anyway. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) specifically for this kind of pipeline configuration, where you can see how your docs look after each transformation before committing.\n\nFor the cross-document reasoning specifically, you probably need document-level metadata propagation so chunks know which policy they came from and what other policies they relate to. That's not a retrieval problem, it's an enrichment problem that happens way before Milvus sees anything.",
          "score": 6,
          "created_utc": "2026-01-15 05:57:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzrtoxv",
              "author": "stevevaius",
              "text": "Vectorflow still not available to test. Waiting for access",
              "score": 1,
              "created_utc": "2026-01-15 18:24:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzrxwzu",
                  "author": "OnyxProyectoUno",
                  "text": "Apologies, the next limited early access will be in the very near future. You should receive an email a week or two before launch. \n\nIn the interim, is there something we can help you with, perhaps related to your setup or feature requests/pain points in your current setup, that you would like us to take into account as we refine the VectorFlow experience?",
                  "score": 2,
                  "created_utc": "2026-01-15 18:42:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o00dcmg",
              "author": "bigshit123",
              "text": "Can you explain how you got structured output from docling? I’m parsing to markdown but docling seems to make every title a second-level heading (##).",
              "score": 1,
              "created_utc": "2026-01-16 22:53:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o026cwg",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, the default markdown export flattens everything to h2. You want to use the JSON output instead, which preserves the actual document structure. When you call docling, set the output format to JSON and you'll get the hierarchical layout with proper nesting levels, section types, and element relationships.\n\nThe JSON gives you stuff like section numbers, whether something is a definition block, table structure, footnote relationships. That's what you actually want to use for chunking decisions. I chunk based on the JSON structure first, then convert relevant parts to markdown only for the final chunk content. Most people do it backwards and lose all the structural info that docling worked to extract.",
                  "score": 1,
                  "created_utc": "2026-01-17 05:45:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzn3o33",
          "author": "ggone20",
          "text": "Graphs are the answer. You need to rethink chunking strategies though - semantic and section based chunking. All the questions you have are indeed things you need to work through. \n\nYou can (and should) approach retrieval from both ends: start with keyword and vector search, use the findings to traverse graph relationships - you’ll need agentic search here so it can intelligently ‘loop’ and refine queries as needed. You can come from the graph side as well if you know certain nodes on specific edges you’re looking for to filter (keep an index). \n\nHave your agent use a ‘scratchpad’ during search and keep each search branch’s context clean and focused - what I’ve found so far, what information I still need, search terms used).  There are a hundred more things but yea…\n\nI built an engineering assistant for the hydrogen mobility industry (think hydrogen fueling stations and generation facilities) that is used to validate plans and even day to day work packages against required standards, protocols, and regulations. It provides detailed report with citations of ‘why’ for go/no-go decisions.  So yes, to answer your final questions, this is the way. Prompting and intelligent search is more art than science though. Expanding user queries, prompting the user intelligently for search clarity, cacheing and vectorizing queries to store them with ‘what worked’ provenance so the agent can find like or similar answers later. \n\nA simple ranking system that users can ‘thumbs up/down’ responses (and potentially provide feedback) helps a lot for refining the system down the road. Good luck!",
          "score": 6,
          "created_utc": "2026-01-15 00:22:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzov0wj",
              "author": "cisspstupid",
              "text": "I agree to this approach. I'm myself starting to look into building knowledge graph and how to use them. If u/ggone20 have any good references for learning or tips. Those will be highly appreciated.",
              "score": 2,
              "created_utc": "2026-01-15 07:19:29",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzpcwac",
          "author": "TechnicalGeologist99",
          "text": "Legal documents are not really semantic. \n\nThe semantics of the text help get us in the correct postcode...but it doesn't help us to reason or extract full threads of information. \n\nThis is because legal documents are actually hiding a great deal of latent structure. \n\nThis is why people use knowledge graphs for high stakes documents like this. \n\nYou need to hire someone with research expertise in processing legal text. \n\nBuilding a useful knowledge graph is very difficult.\n\nAnyone who says otherwise is a hype gremlin that's never had to evaluate something with genuinely high risk outputs.\n\nYou should also be aware that KGs usually run in memory and are  memory hungry. This will be a major consideration for deployment. Either you already own lots of RAM (you lucky boy) or you're about to find out how much AWS charge per GB",
          "score": 4,
          "created_utc": "2026-01-15 10:10:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzmrcb1",
          "author": "DeadPukka",
          "text": "Any links to the type of docs you’re working with? (If public)\n\nAnd what types of prompts are you using?  \n\nAre you doing prompt rewriting? Reranking?\n\nAre you locked into only on-prem?",
          "score": 2,
          "created_utc": "2026-01-14 23:16:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4etw",
          "author": "FormalAd7367",
          "text": "in my experiences, law/statues/policy doc are hardest because there are so many variables that agentic ai can’t read like sec 7(x)(78) of law -> sec 7(x7(8) of law\n\nthen there are tables and long winded text.. and law that was superseded by another law like 59 times",
          "score": 2,
          "created_utc": "2026-01-15 00:26:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzn4sah",
          "author": "hrishikamath",
          "text": "I have a feeling you should use agentic retrieval. Also do more of metadata engineering than anything. From my personal experience building an agent for sec filings. You can look at the readme for inspiration: https://github.com/kamathhrishi/stratalens-ai/blob/main/agent/README.md I am writing an elaborate blogpost on this too.",
          "score": 2,
          "created_utc": "2026-01-15 00:28:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobqkz",
          "author": "tony10000",
          "text": "I would use a dense model rather than OSS20B.  I am not sure that a MoE model is up to the task.  If you must use MoE, try: [https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-30B-A3B-Instruct-2507)",
          "score": 2,
          "created_utc": "2026-01-15 04:46:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzogl1v",
          "author": "Past-Grapefruit488",
          "text": "Consider : \n\n1. Full text search via Elastic Search or similar (in addition to vecros and graph store)  \n2. Agentic RAG taht uses all these and evaluates it in context of inputs (initial as well as subsequent)",
          "score": 2,
          "created_utc": "2026-01-15 05:20:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzolkqm",
          "author": "Rokpiy",
          "text": "hierarchical chunking helps but doesn't solve cross-references. legal docs have section 7.2.3 referencing \"as defined in section 2.1\" which has exceptions in section 5.4. flat chunks break these chains, hierarchical chunks reduce it but don't eliminate it\n\nwhat worked for us: dual-layer retrieval. first pass gets relevant sections via embeddings, second pass explicitly searches for cross-reference patterns in those sections and fetches the referenced content. regex + heuristics after semantic search\n\nyou can't solve this with better embeddings because legal language encodes relationships through explicit references, not semantic similarity. \"section 7.2.3\" and \"section 2.1\" have zero semantic overlap but maximum logical dependency\n\ngraph rag helps if you pre-extract \"section X references section Y\" during ingestion. but that's a parsing problem, not retrieval. most teams skip it because the parsing is fragile and breaks on updates\n\nfor the 64k context issue: either accept incomplete context or implement multi-hop retrieval where the model asks for missing definitions when it hits a reference",
          "score": 2,
          "created_utc": "2026-01-15 05:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzom65c",
          "author": "RecommendationFit374",
          "text": "Have you tried papr.ai we have document ingestion  u can use reducto or other providers, define your custom schema and auto build graph we combine vector + graph + prediction models it works well at scale. See our docs at platform.papr.ai",
          "score": 2,
          "created_utc": "2026-01-15 06:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzometk",
              "author": "RecommendationFit374",
              "text": "Our open source repo here https://github.com/Papr-ai/memory-opensource \n\nWe will bring doc ingestion to open source soon",
              "score": 2,
              "created_utc": "2026-01-15 06:06:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzqkbvr",
          "author": "DistinctRide9884",
          "text": "Check out SurrealDB, which is multi-model and has support for graph, vectors, documents and can be updated in real time (vs. other graph DBs where you have to rebuild the cache each time time you update the graph).\n\nThen for the documenting parsing/extraction something like [https://cocoindex.io/](https://cocoindex.io/) might be worth exploring, their core value prop is real-time updates and full traceability from origin into source. A CocoIndex and SurrealDB integration is in the works.",
          "score": 2,
          "created_utc": "2026-01-15 14:58:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzqvuzd",
              "author": "Early_Interest_5768",
              "text": "CocoIndex looks great thanks.",
              "score": 2,
              "created_utc": "2026-01-15 15:52:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzo6491",
          "author": "Recursive_Boomerang",
          "text": "https://medium.com/enterprise-rag/deterministic-document-structure-based-retrieval-472682f9629a\n\nMight help you out. PS I'm not affiliated with them",
          "score": 1,
          "created_utc": "2026-01-15 04:07:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzobiin",
          "author": "charlesthayer",
          "text": "Please tell us a little bit more about your current setup. I'm wondering how many agents or sub agents you are currently using? \n\nEg. Do you have a search assistant agent that focuses just on finding docs (without full context)? \nAre you using a memory system like mem0 or a skill system like ACE?\nDo your prompts include both negative and positive examples? \nDo you have a set of evals that provides precision and recall (relevance)\n\nMy first thought is that it would probably help to extract a fair amount of metadata for each document into a more structured database. So this would be a separate pipeline that understands key important things that you're looking for in these docs. Eg. Which compliance or audit standards are discussed, etc.\n\nAdding the graph DB should be a great help. Doing multiple levels of chunking so that you include whole paragraphs and sections will also be a help for ranking. I'm not familiar with law or legal documents, but I imagine there may be some models fine-tuned for your legal domain.\n\nSounds interesting!",
          "score": 1,
          "created_utc": "2026-01-15 04:44:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzoc0kd",
          "author": "tony10000",
          "text": "You may also want to check out Anything LLM.  It has excellent RAG capabilities.",
          "score": 1,
          "created_utc": "2026-01-15 04:48:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzojqrq",
          "author": "AloneSYD",
          "text": "First you need to work on you chunking for splitting you need to optimize splitting for the your kind of documents and specially cross pages, section\n\nLook up contextual retrieval, basically you need to add metadata to each chunk. CR can help in two ways either like a first stage retrieval or it can embedded with each chunk to enhance relevant chunks\n\n If you don't have enough context length to take a full document, make a custom one that will take n #pages before and after to create the context for each chunk instead of the whole document.\n\nI would highly recommend is to use a reAct agent. Because the reflection step help it in many situations to requery until it reaches a satisfiying state and you can specify the criteria on the answer is complete",
          "score": 1,
          "created_utc": "2026-01-15 05:44:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp0q11",
          "author": "cat47b",
          "text": "Can you share an anonymised example of the exceptions and overrides text. As others have said a chunk may need metadata that refers to others which need to be retrieved as part of that overall context",
          "score": 1,
          "created_utc": "2026-01-15 08:11:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzpp9nl",
          "author": "HonestoJago",
          "text": "Law firms are hard. As soon as there’s one slight error, or one response that isn’t “complete”, everyone will stop using it. It’s probably malpractice just to rely on the RAG, anyway, and not review the full underlying docs, so I’m not really sure there’s a lot of value there. I think tools for email management and retrieval are easier to build and are more attractive to attorneys, who are constantly overwhelmed by emails and can’t keep track of project status, etc…",
          "score": 1,
          "created_utc": "2026-01-15 11:56:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzqciya",
          "author": "my_byte",
          "text": "Hard disagree with folks screaming graph. If you do A/B with GraphRAG vs agentic, multi turn search and normalize for tokens - the latter will have similar results, without the operational headache that comes from graph updates, disambiguation and such.\nI'm not sure if I like gpt-oss tbh. Have you tried other models?\nGenerally speaking - what's your approach to measure recall and accuracy in your system?",
          "score": 1,
          "created_utc": "2026-01-15 14:19:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzs2ta0",
          "author": "ClearApartment2627",
          "text": "20+ documents? Did you mean 20k+ or 20m+ docs? Just asking because 20 does not sound right, and 20m might need a different architecture than 20k.",
          "score": 1,
          "created_utc": "2026-01-15 19:04:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzttohd",
          "author": "MrTechnoScotty",
          "text": "Perhaps RLM is worth a look?",
          "score": 1,
          "created_utc": "2026-01-16 00:07:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuw0ip",
          "author": "One_Milk_7025",
          "text": "Your chunking strategy is not perfect i feel. token based basic chunking wont do much when you need interlinking cross document reference. Docling is already good choice..   \nPick a AST based chunker like markdown-it if you can convert you existing doc to a MD file , extensively extract metadata from the chunks and atttach back to it(Qdrant support it natively). Optionally use a NER model like Gliner to extract the entities from those chunk text and header , this gives a common Concept registry which can be very helpful to create the graphDB. Chunking is the most important part of this, you need to extract parent/neighbour chunk relation, line count, section header , optionally token count etc etc.. work on the chunking pipeline and find what is more suitable for you.  \nfor graphdb its not necessary to use Neo4j but it does have its perks.. but for start you can use postgres +qdrant . it gives both hard graph from postgres which contains the file structure and hierarchy and semantic graph from qdrant.. but to have actual graph like structure its the Concept registry where things get really connected..  \nnow for the retrieval part it will be now much more easy to hop around those concepts, expanding the neighbor..",
          "score": 1,
          "created_utc": "2026-01-16 03:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwv5t8",
          "author": "Alternative-Edge-149",
          "text": "I think you should try the graphiti mcp server with ollama and use qwen 8b embedding + qwen 32b vlm or qwen 32b llm or something similar. Graphiti works for this usecase precisely since it is a temporal graph. You can connect it with Neo4j or Falkordb. It might not work out of the box with ollama as it requires the new open ai \"/v1/responses\" endpoint instead of chat completions endpoint but it can be done by something like LiteLLM. This should be accurate enough",
          "score": 1,
          "created_utc": "2026-01-16 12:59:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0326ic",
          "author": "andreasdotorg",
          "text": "20+ documents? Is that a typo or the actual number?\n\nI'm working with corporate policy, about 100 documents, plus legal texts relevant to them. This is purely agentic, I use Claude Code with Opus 4.5, with a lot of subagents.\n\nAll data storage is on disk, using ordinary file tools, no RAG in sight. And I don't think it's needed. What's a policy document, 5k tokens? I can have 20 in context and still have 100k tokens headroom in context.\n\nHere's a high level overview of what I do.\n\nOne important subagent is source intake. There's an agent fetching the original source, extracting full text and images from it (for PDF, .docx, web sites, whatever), creating a summary in a standardized format, including source location, link to full text, ready made citation in Chicago Style Manual format, and relevance to us (there's a high level background on jurisdiction, legal status, company size etc. in the global context that all agents get). Subagent has some CLI prompts for processing, it knows how to call lynx -dump or pdftotext. \n\nAnother one is the legal researcher. It knows how to call another subagent doing research on our legal database for case law, cross references to other relevant laws, etc. It provides a list of sources (then to be processed by source intake) , and a preliminary answer to the legal question.\n\nThere's a subagent for actually writing the text answering a research question. It has an extensive style guide: every sentence either a premise or a conclusion. Every premise has a citation. Conclusions depending on interpretation need a citation too backing up this conclusion. \n\nThen, a subagent for validation. Does the cited document exist? Is the citation precise enough? Does the citation actually support our statements? Do conclusions follow? Any language imprecise? Any claim unsubstantiated? Anything we missed?\n\nWorks pretty well for me, got compliments from actual lawyers.",
          "score": 1,
          "created_utc": "2026-01-17 10:34:35",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbjr5n",
      "title": "We built a semantic highlighting model for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
      "author": "ethanchen20250322",
      "created_utc": "2026-01-13 06:09:28",
      "score": 47,
      "num_comments": 15,
      "upvote_ratio": 0.99,
      "text": "We kept running into this problem: when we retrieve documents in our RAG system, **users can't find where the relevant info actually is**. Keyword highlighting is useless – if someone searches \"iPhone performance\" and the text says \"A15 Bionic chip, smooth with no lag,\" nothing gets highlighted.\n\nWe looked at existing semantic highlighting models:\n\n* OpenSearch's model: 512 token limit, too small for real docs\n* Provence: English-only\n* XProvence: supports Chinese but performance isn't great + NC license\n* Open Provence: solid but English/Japanese only\n\nNone fit our needs, **so we trained our own bilingual (EN/CH) model (Hugging Face: https://huggingface.co/zilliz/semantic-highlight-bilingual-v1)**. Used LLMs to generate 5M training samples where they explain their reasoning before labeling highlights. This made the data way more consistent.\n\n**Quick example of why it matters:**\n\nQuery: \"Who wrote the film The Killing of a Sacred Deer?\"\n\nContext mentions:\n\n1. The screenplay writers (correct)\n2. Euripides who wrote the Greek play it's based on (trap)\n\nOur model: 0.915 for #1, 0.719 for #2 → correct\n\nXProvence: 0.133 for #1, 0.947 for #2 → wrong, fooled by keyword \"wrote\"\n\nWe're using it in Milvus and open-sourced it (MIT license), covers EN/CH right now.\n\nWould be interested to hear if this solves similar problems for others or if we're missing something obvious.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzb4gde",
          "author": "ethanchen20250322",
          "text": "Know more: [https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md](https://milvus.io/blog/zilliz-trained-and-open-sourced-bilingual-semantic-highlighting-model-for-production-ai.md)",
          "score": 2,
          "created_utc": "2026-01-13 06:12:20",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "nzb83es",
          "author": "Rokpiy",
          "text": "the keyword vs semantic highlighting gap is real. especially when the answer is conceptually there but zero keyword overlap\n\nthe training approach is interesting, having LLMs explain reasoning before labeling probably helps with edge cases where multiple spans could be \"correct\" but with different confidence levels\n\ncurious about the 512 token limit you mentioned with opensearch. did you end up with a higher context window for your model or just better semantic understanding within similar limits?",
          "score": 2,
          "created_utc": "2026-01-13 06:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbif1j",
              "author": "ProfessionalLaugh354",
              "text": "Thanks. Data labeling with reasoning leads to higher-quality data.\n\nWe’ve moved to a larger 8k context window model—it’s much more aligned with real-world use cases in RAG/Agent scenarios.",
              "score": 1,
              "created_utc": "2026-01-13 08:16:17",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzcfuog",
                  "author": "DeliciousWalk9535",
                  "text": "An 8k context window is definitely a game changer! It should really help with capturing the nuances in longer documents. Have you noticed any specific improvements in user satisfaction since making that switch?",
                  "score": 0,
                  "created_utc": "2026-01-13 13:02:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzbcb9k",
          "author": "-Cubie-",
          "text": "Cool work!",
          "score": 1,
          "created_utc": "2026-01-13 07:19:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzbs9tt",
          "author": "jerrysyw",
          "text": "THE problem want to solve was how to give the right reference when llm generated answers?",
          "score": 1,
          "created_utc": "2026-01-13 09:52:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgt4o2",
              "author": "ProfessionalLaugh354",
              "text": "Split the context by sentences, then assign a serial number to each sentence, and let the LLM select the number of the sentence that should be highlighted. And use a thinking model to enable the thinking mode.",
              "score": 1,
              "created_utc": "2026-01-14 02:10:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzccj7k",
          "author": "Wimiam1",
          "text": "This is great! Excuse my ignorance, but how does this compare to using something like ColBERTv2 as a reranker and pooling token vector scores into sentence scores?",
          "score": 1,
          "created_utc": "2026-01-13 12:40:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgwd3o",
              "author": "ProfessionalLaugh354",
              "text": "A good point. As far as I know, ColBERT’s training objective is to use the average of the maximum similarity scores across the entire context as the overall context score. While it can also output token-level scores, its training objective may not be perfectly aligned with tasks like semantic highlighting or context pruning. We haven’t conducted an evaluation yet, but I suspect there might be a slight mismatch. We welcome more tests and insights from the community.",
              "score": 1,
              "created_utc": "2026-01-14 02:28:59",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh0pc2",
                  "author": "Wimiam1",
                  "text": "Interesting! I only thought of it because one of my first introductions to ColBERT was [this website](https://colbert.aiserv.cloud) where you could run little demos in browser and it would highlight the relevant parts of the document. I tried it with some of the demos on your GitHub, but it didn’t perform as well. I suspect this is v1 using BERT, which would explain its poor performance on specific technical jargon like in the iPhone example I tried.",
                  "score": 1,
                  "created_utc": "2026-01-14 02:53:21",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdkdjq",
          "author": "OrbMan99",
          "text": "This looks great. It's unclear to me, though, how to relate the sentence output back to the original text (e.g, how do I locate \"Sentence13\"). I do not know how the text is being split. What is the algorithm you recommend for locating the sentences for highlighting?",
          "score": 1,
          "created_utc": "2026-01-13 16:28:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgzj4b",
              "author": "ProfessionalLaugh354",
              "text": "This \\`process()\\` function can directly return the sentences that need to be highlighted.\n\nThe sentence splitting logic is right here in this code. [https://huggingface.co/zilliz/semantic-highlight-bilingual-v1/blob/main/modeling\\_open\\_provence\\_standalone.py](https://huggingface.co/zilliz/semantic-highlight-bilingual-v1/blob/main/modeling_open_provence_standalone.py)   \n It works a little differently for each language, and you can also override to customize it.",
              "score": 1,
              "created_utc": "2026-01-14 02:46:49",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzhbvyf",
                  "author": "OrbMan99",
                  "text": "Thanks! I thought the model was splitting the sentences for some reason.",
                  "score": 1,
                  "created_utc": "2026-01-14 04:00:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzp5gfb",
          "author": "jerrysyw",
          "text": "which means when recall chunks filters the most related sentence with this model ,and then give it to llm for summary?",
          "score": 1,
          "created_utc": "2026-01-15 08:57:21",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdy998",
      "title": "Job wants me to develop RAG search engine for internal documents",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qdy998/job_wants_me_to_develop_rag_search_engine_for/",
      "author": "Next-Self-184",
      "created_utc": "2026-01-15 22:39:17",
      "score": 42,
      "num_comments": 76,
      "upvote_ratio": 0.96,
      "text": "this would be the first time I develop a RAG tool that searches through 2-4 million documents (mainly PDFs and many of those needing OCR). I was wondering what sort of approach I should take with this and whether it makes more sense to develop a local or cloud tool. Also the information needs to be secured so that's why I was leading toward local. Have software exp in other things but not working with LLMs or RAG systems so looking for pointers. Also turnkey tools are out of the picture unless they're close to 100k.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qdy998/job_wants_me_to_develop_rag_search_engine_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nztftcm",
          "author": "GroundbreakingEmu450",
          "text": "My two cents, while open for more experts to chime in: 2/4 million is a lot of documents and pdf is the worst format to start with. I would start by identifying the most valuable subset of documents, define a conversion approach (pref. towards markdown files) and then experiment with different chunking strategies (esp. parsing) to test retrieval. Embedding and Reranking should be considered. Those models are usually tiny and can run locally. For the reasoning part (responding to the user query by formulating an answer starting from the retrieved chunks) you can think about using a cloud LLM to have more advanced reasoning and potentially less hallucinations.",
          "score": 28,
          "created_utc": "2026-01-15 22:53:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztpvvl",
              "author": "OnyxProyectoUno",
              "text": "Parsing tools will already output markdown. That's the whole job of parsing, converting unstructured data like PDFs to more structured data in an LLM friendly document format like Markdown (or worse but acceptable, JSON).",
              "score": 5,
              "created_utc": "2026-01-15 23:47:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztsn6m",
                  "author": "ggone20",
                  "text": "It isn’t this trivial… parsing pdfs sucks still today in 2026. If it’s just words with one column, it’s awesome. Two columns? You need to babysit it or develop more robust processes. Tables and charts? Lmao yea… LLM/vLM parsing (expensive and/or slow). \n\nYou can rapid parse any corpus… is what you get out useful and retrievable afterwards? Not likely…",
                  "score": 6,
                  "created_utc": "2026-01-16 00:02:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztz1ep",
              "author": "MrTechnoScotty",
              "text": "I agree in principle:  Start with a bite size chunk that focused on some subset of docs that can offer biggest outcomes when transformed.  Learn, iterate, it will begin to get clearer how scaling should occur…",
              "score": 2,
              "created_utc": "2026-01-16 00:36:50",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztifap",
          "author": "nicoracarlo",
          "text": "What u/GroundbreakingEmu450 said it true. When you start handling such large amount of data, a simple RAG is going to get very messy and starts hallucinating (or missing important facts). You should look at graph rags and drift techniques, but those are not things you learn in a day or a week, as they require solid foundations (and analysing 2-4m documents for a graph rag is going to be very expensive even using specific models with low CPM) and it will take a lot of time.  \nI build a graph rag with drift and it takes between 10 to 60 seconds (depending on size and complexity)  to properly analyse a document. do the math...  \nMaybe at this point an off the shelf solution as u/BigNoseEnergyRI suggested may be less painful",
          "score": 9,
          "created_utc": "2026-01-15 23:07:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvy5wz",
              "author": "TechnicalGeologist99",
              "text": "Not to mention 2-4 million docs is looking to be 100 million nodes atleast. That's gonna be one expensive bill in terms of memory/ hosting fees",
              "score": 3,
              "created_utc": "2026-01-16 08:28:00",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nztuxlx",
              "author": "BigNoseEnergyRI",
              "text": "Great advice. I wasn’t trying to be glib. The request is so general. Everything needs to be secure? They are describing every business out there. What industry? Compliance? Search to do what? Do they need to index/extract/enrich 2-4M documents or is there any curation needed?",
              "score": 1,
              "created_utc": "2026-01-16 00:14:41",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzxfqv1",
              "author": "Popular_Sand2773",
              "text": "For a graph at that scale have you considered knowledge graph embeddings. Sounds like it strikes the right balance you’d be looking for.",
              "score": 1,
              "created_utc": "2026-01-16 14:49:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztq8ui",
          "author": "exaknight21",
          "text": "What resources have they made available to you? I would suggest do it locally. Get your hands on a 32 GB+ GPU, 3 if you can, and here is why:\n\n1. Run an 8b model (i prefer qwen3 family, and personally use qwen3:4b for my construction documents, I do government contracts only). This is your main LLM so serve with vLLM. If budget is low, go Tesla V100 32 GB. If budget is good, go RTX Pro 6000 Blackwell.\n\n2. Get another GPU, this I think should be a V100 32GB and will process all documents for embedding. You don’t need vLLM for this, Hugging Face Transformers works perfectly fine because most requests are batch processed and you want it to be like that.\n\n3. Get another GPU for OCR and use PaddleOCR-VL - it is genuinely good. \n\n\nOnce you’re good, deploy your gateway, I use dokploy, so get a container going for your gateway, a web app, and you have an entire pipeline.\n\nFor RAG, use lanceDB + S3 to store vector data directly into the S3, saving on storage otherwise you’ll need excessive amount of data/drive management. I use an older 0.17.0 version of lanceDB + Backblaze B2. Latency is 400-500 ms for me. S3 would be 200-300, I ain’t dying.\n\nQuality of OCR matters, but so does organization of documents, and knowledge graphs. Use dgraph for knowledge graph, and call it a day.\n\nIf you’re not dealing with Handwritten Text, then you can use something like OCRMyPDF (i wrapped an API around it, it’s at https://github.com/ikantkode/exaOCR - completely free and dockerized). This runs CPU only and is very effective for me since I don’t work with HTR (handwritten text recognition). I mainly deal with scanned/text recognizable PDF, and it works like a charm.\n\n\nYou’d wanna create a gateway",
          "score": 6,
          "created_utc": "2026-01-15 23:49:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o03u4t6",
              "author": "NeoNix888",
              "text": "https://preview.redd.it/pthygm472xdg1.png?width=2146&format=png&auto=webp&s=6b78b2d26b288f22b9983872cd082bf7f6fbc4d1\n\nThat's kewl you are using Gwen3 because they are on the top ten on [hugginghugh.com](http://hugginghugh.com) AI model scores!\n\nI have not use it yet, still learning.",
              "score": 1,
              "created_utc": "2026-01-17 14:04:36",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o03vx1k",
              "author": "NeoNix888",
              "text": "https://preview.redd.it/nwqukah73xdg1.png?width=2130&format=png&auto=webp&s=66ee02aaee8d6d391a2206be92354c5934cb7753\n\none more thing u/exaknight21 \\- there is 1 vulnerability in the exaOCR in case you didn't know. I got the report from [sbomly.com](http://sbomly.com) for free basically, it took less than a minute to get the full report.\n\nyou would get the below in the report and can click to go directly to the GHSA and CVE. they even provide SBOMQS scores!\n\nCheers!\n\n# 📚 python-multipart\n\nv0.0.9 purl: pkg:pypi/python-multipart@0.0.9\n\nLIBRARYAPACHE-2.01 VULN (HIGH)\n\n**🔐**Hashes:**ⓘ**0\n\n**🔗**References:**ⓘ**0\n\n**⚙️**Properties:**ⓘ**5\n\n**⚠️**Vulnerabilities:**ⓘ**1\n\n⚠️ Vulnerabilities (1)\n\n**GHSA-59g5-xgcq-4qw3 CVE-2024-53981** \n\nHIGH\n\nDenial of service (DoS) via deformation \\`multipart/form-data\\` boundary\n\n**✅ Fix Available:** Upgrade to version **0.0.18**\n\n**EPSS:** **0.1%** chance of exploitation (Top 68.9% most likely)\n\n**Risk Score:** **0.09** (combines CVSS + EPSS + KEV)\n\nCVSS: 7.5CWE-770\n\n[🔗 ](https://nvd.nist.gov/vuln/detail/CVE-2024-53981)NVD\n\n**Risk Score:** 62/100 (MEDIUM)\n\n**🎯 Recommendations:**\n\n* Update python-multipart to fix 1 vulnerability\n\n📖 View Technical Details",
              "score": 1,
              "created_utc": "2026-01-17 14:14:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztjgx0",
          "author": "Hour-Entertainer-478",
          "text": "For response generation: \nYou need to find a nice local llm that would hallucinate less, findinfo buried in lots of chunks, handle follow up conversations, and offer good tool calling. \nI have been doing pretty much that for different companies so ill say got oss 20b qwen3:30ba3b and gemma3:27b are good options. \n\nFor parsing docs: docling with rapidocr is a good start. \n\nFor 2 million docs, id suggest qdrant. It is generally faster than weaviate. (Vector db)\n\nOpinions are my own, hope that helps",
          "score": 4,
          "created_utc": "2026-01-15 23:12:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztkmyt",
              "author": "Next-Self-184",
              "text": "Thanks for responding - I'm assuming you'd need a pretty decent hardware setup?",
              "score": 1,
              "created_utc": "2026-01-15 23:19:02",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "nzts37u",
              "author": "ggone20",
              "text": "I really like got-oss also. I would layer with Qwen-vl…\n\nDocling is OK but the chunk strategy needs to be a lot more robust, it’ll never work out if the box for this many documents. \n\nStorage explodes since you’ll definitely need blob/object/artifact storage (s3), vector store, traditional db (Postgres-like is preferable), and knowledge graph… local is ideal to ‘keep costs down’, specifically ongoing costs, but the hardware here isn’t trivial if you want anything done in a reasonable amount of time and HA durable with backups. If you’ve never done infra before, bad time to start…\n\nThis is not a learn on the job endeavor…",
              "score": 1,
              "created_utc": "2026-01-15 23:59:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuvdcl",
          "author": "One_Milk_7025",
          "text": "everythign can be ran locally..  \nUse docling to parse those pdf with page number included..  \nonce you get the markdown then the actual play starts. Pick a AST based chunker like markdown-it , extensively extract metadata from the chunks and atttach back to it(Qdrant support it natively). Optionally use a NER model like Gliner to extract the entities from those chunk text and header , this gives a common Concept registry which can be very helpful to create the graphDB. Chunking is the most important part of this, you need to extract parent/neighbour chunk relation, line count, section header , optionally token count etc etc.. you can use local bge-m3 quantized model or even the embedding-gemma 300m model.. which works fine for me.  \nfor graphdb its not necessary to use Neo4j but it does have its perks.. but for start you can use postgres +qdrant . it gives both hard graph from postgres which contains the file structure and hierarchy and semantic graph from qdrant.. but to have actual graph like structure its the Concept registry where things get really connected..  \nnow for the retrieval part it will be now much more easy to hop around those concepts, expanding the neighbor.. i hope this helps..",
          "score": 4,
          "created_utc": "2026-01-16 03:37:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvrjia",
          "author": "Much-Researcher6135",
          "text": "For document prep, there are several solid libraries out there (`marker`, `docling`, `unstructured`, `pymupdf`), depending on what type of layout and information are in the PDFs, but really good results may only come with neural net layout detection *followed* by OCR, not just OCR alone.\n\nBy all means go the non-ML route first to see if something like `ocrmypdf` will be sufficient. If so, that's a big win, because the really good stuff requires GPU acceleration.\n\nSo yeah, experiment. I am building RAG for a lot of funky stuff like [math-heavy PDFs](https://linear.axler.net/). For that, [marker](https://github.com/datalab-to/marker) outclassed everything else I listed above, but it's also the slowest., Guess you get what you pay for.\n\nEven that isn't always good enough, so in my workflow, I typically take marker's JSON output and run it through a cleanup XGBoost model I hand-tuned to omit stuff that annoys me. In other words, I burn a lot of GPU cycles to let marker do most of the work, and the library comes with some absolutely brilliant pre-trained neural nets, but there's still a bit of cleanup to do.\n\nAnyway running marker to convert a folder of files is dead simple code-wise (python or CLI). The following is done serially because the box in question has only one GPU, an RTX 3090, and marker will saturate that GPU while using maybe 15/24 GB VRAM on certain files ([this two-column technical textbook was brutal](https://networksciencebook.com/)):\n\n    # Convert a folder of books (epub, pdf) to markdown + JSON\n    from pathlib import Path\n    import os\n    import json\n    from marker.converters.pdf import PdfConverter\n    from marker.models import create_model_dict\n    from marker.renderers.markdown import MarkdownRenderer\n    from marker.renderers.json import JSONRenderer\n    \n    path_input = Path(\"input\")\n    path_output = Path(\"output\")\n    \n    for fn in os.listdir(path_input):\n        fp_in = path_input / fn\n        stem = fp_in.stem\n        fp_out_md = path_output / f\"{stem}.md\"\n        fp_out_json = path_output / f\"{stem}.json\"\n    \n        print(f\"Converting {stem}\")\n        if fp_out_md.exists() or fp_out_json.exists():\n            print(\"   already exists, skipping.\")\n        else:\n            converter = PdfConverter(artifact_dict=create_model_dict())\n            doc = converter.build_document(str(fp_in))\n            md_out = MarkdownRenderer()(doc)      # pydantic model\n            js_out = JSONRenderer()(doc)          # pydantic model\n    \n            fp_out_md.write_text(md_out.markdown, encoding=\"utf-8\")\n            fp_out_json.write_text(json.dumps(js_out.model_dump(mode='json'), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n            print(f\"   {stem} converted\")",
          "score": 4,
          "created_utc": "2026-01-16 07:28:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzvsk2q",
          "author": "fabkosta",
          "text": "We built search engines for 600m docs. But even with 2 - 4m things are really complicated, you need a lot of infra.\n\nSteps:\nFirst, build an OCRing system that scales. This is its own application that runs independently from the entire ingestion.\n\nSecond, build scalable ingestion and search infrastructure. That’s complicated. You will probably need mass storage, Kafka, back propagation pressure, and Elasticsearch.\n\nThird, build the RAG system. Use hybrid search, it’s almost always superior than RAG alone. But you must take a measuring approach, each change must be applied to eg 100 sample docs, and you measure whether a change improves something or not.\n\nYou see, these are really 3 distinct projects. There is a lot more here, eg OCRing will yield bad results, and you’ll need a language model to correct OCRing results. (Disclaimer: I am selling consulting for such stuff.)",
          "score": 5,
          "created_utc": "2026-01-16 07:37:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzuzn6y",
          "author": "GP_103",
          "text": "Missing from this conversation and really the starting point is what’s the use case?\n\nIf you need ground truth and linked citation, then none of the aforementioned solutions may work.",
          "score": 3,
          "created_utc": "2026-01-16 04:03:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzy8ncx",
              "author": "Next-Self-184",
              "text": "when you say linked citations, do you mean like retrieving the documents with the keywords?",
              "score": 1,
              "created_utc": "2026-01-16 16:58:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzz0yer",
                  "author": "GP_103",
                  "text": "I mean whatever natural language /text AI wraps around the answer, you include a link or citation to the actual content block in the document.\n\nThat and evaluation suite, validation and visibility/traceability across the pipeline and back to the actual source.\n\nBasic for Healthcare, finance, legal, engineering and now enterprise in general",
                  "score": 1,
                  "created_utc": "2026-01-16 19:03:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nztkc6a",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's a classic enterprise RAG problem. The scale isn't the hard part, it's getting consistent results across 2-4 million documents when you can't see what went wrong during processing.\n\nStart with your document processing pipeline first. OCR quality varies wildly between tools, and bad OCR creates garbage chunks that'll haunt your retrieval later. Test Tesseract vs cloud OCR on a sample of your PDFs to see what you're working with. For parsing, Unstructured handles PDFs reasonably well, but you'll want to preview what your documents actually look like after each transformation step.\n\nChunking strategy matters more than your vector store choice here. Recursive chunking works for most enterprise docs, but test different chunk sizes on your actual content. 500 tokens might work great for technical docs but fail on contracts or reports.\n\nLocal makes sense for security, but don't underestimate the infrastructure overhead. You'll need serious compute for embedding 2-4 million docs, plus vector storage that can handle the scale. Qdrant or Chroma can work locally, but plan your hardware accordingly.\n\nThe real killer is iteration speed. Every config change usually means reprocessing everything, which gets expensive fast at your scale. That's what I've been building around at vectorflow.dev, but honestly any approach that lets you test configurations before committing to the full pipeline will save you weeks.\n\nWhat type of PDFs are you dealing with? Scanned documents vs native PDFs change your whole preprocessing approach.",
          "score": 4,
          "created_utc": "2026-01-15 23:17:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztqg5m",
              "author": "ggone20",
              "text": "This is good advice. The part to really pay attention here is iteration as mentioned. \n\nYou can’t test your pipe against 1000 docs and assume it’s good because your evals hit hard. If you’re not doing the full ingestion and testing against the full corpus, you have no idea what performance is actually going to be like. \n\nHow many users you’re serving, how many updates per day/week/month are needed (re-indexing is a pain in the ass) and what downtime is acceptable while that happens? Are you serving users in different time zones where you can only do re-indexes on weekends? Rolling indexes? How will you do change management? Lol the list goes on.",
              "score": 3,
              "created_utc": "2026-01-15 23:50:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01tx46",
                  "author": "OnyxProyectoUno",
                  "text": "Yeah, the rolling index problem is brutal at that scale. Most people don't think about it until they're staring at 8 hour reprocessing times for a single config change.\n\nChange management gets weird too because you can't really do proper A/B testing when your index is that large. You end up having to commit to architectural decisions based on smaller samples and hope they hold up. The timezone constraint is real, especially if you're dealing with global teams who need the system during your maintenance windows.\n\nHave you dealt with incremental indexing before? I'm wondering if OP could get away with treating document updates as deletes plus inserts rather than trying to build something more sophisticated for their first pass.",
                  "score": 2,
                  "created_utc": "2026-01-17 04:14:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nztzoom",
              "author": "MrTechnoScotty",
              "text": "I think ocr should be a separate process from the ingesting.  OCR using known quality tool(s), likely NOT AI, then work on converting/ingesting the output docs that have been tested to equal = “good”…",
              "score": 1,
              "created_utc": "2026-01-16 00:40:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o01y75a",
                  "author": "OnyxProyectoUno",
                  "text": "That's the right call. OCR as a preprocessing step keeps your pipeline cleaner and lets you validate quality before anything hits your vector store. Tesseract with some postprocessing usually gets you there for most enterprise docs, though you might need to tune confidence thresholds per document type.\n\nThe tricky part becomes managing that two-stage workflow at scale. You'll want to track which source documents failed OCR, what your confidence scores look like across batches, and probably build in some manual review process for borderline cases. Once you've got clean text output though, the rest of your RAG pipeline becomes much more predictable since you're not debugging OCR artifacts mixed in with chunking or retrieval issues.\n\nAre you planning to reprocess the OCR if it fails quality checks, or just flag those documents for manual handling?",
                  "score": 1,
                  "created_utc": "2026-01-17 04:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzvgpo9",
          "author": "Select-Spirit-6726",
          "text": "It’s great that your team recognizes the scale early — millions of PDFs with OCR is a hard engineering problem on its own before you even get to an LLM.\n\nA few folks here are pointing you toward Elastic + pgvector or similar solutions, and that’s not accidental. Doing reliable vector search at that scale *first* will make whatever generative layer you add later much more accurate and cost-effective.\n\nAlso, starting with a strict chunking + OCR pipeline and validating extraction quality at smaller scale will save you from scaling bad vectors later. Even a well-tuned vector store alone can answer a lot of semantic queries without invoking a language model on every request.\n\nIf security and local deployment are priorities, open-source vector stores and on-prem OCR tools are totally reasonable to start with. Just be sure to invest as much in ingestion and indexing quality as you do in the LLM layer.",
          "score": 2,
          "created_utc": "2026-01-16 05:59:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx1lev",
          "author": "voycey",
          "text": "So I have just spent the last 6 months building exactly this, unfortunately you are in for a rough time.\nAs others have said, PDF parsing isn't a solved thing, it requires a LOT of steps to get right.\n\nUltimately you will have to contend with a few different things:\n\n1. Sovereignty issue - unlikely you can use public APIs with company / PII data \n2. Contention issues - local LLMs are literally the \"Fast, Cheap, Good - Pick 2\" paradox\n3. Machines reading documents is not the same as people reading documents.\n4. Infrastructure - you better be happy deploying some specific tooling\n5. Speed - Good RAG is S L O W - are your users going to be ok with a 20-40 second round trip?\n\nShortcuts for you:.\n1. Use Bedrock and Nova Pro 2 - this is probably the fastest way to get something that will work at scale up and running. The RAG isn't great but you can build a simple chatbot in Lex or use Open WebUI and hook it up to your Bedrock pipeline, you then basically just upload all of the docs to the Bedrock KB S3. Also keeps it somewhat sovereign (although watch out for CRIS on Bedrock models).  Other clouds have something similar for this!\n\n2. Buy something readymade that does this and focus instead on the data engineering challenges that surround this like the pre RAG and post RAG pipelines to actually derive value from those documents! I can help here!\n\nTons of trade-offs DIYing it, really comes down to what you want to get out of it at the end!",
          "score": 2,
          "created_utc": "2026-01-16 13:36:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyadvn",
              "author": "Next-Self-184",
              "text": "yea 20-40 second round trip is fine compared to hours of searching. been seeing a lot of feedback recommending to really focus on the OCR pipeline and then get elastic search to deal with the vectors.",
              "score": 1,
              "created_utc": "2026-01-16 17:06:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o02r47w",
                  "author": "voycey",
                  "text": "I would personally avoid ElasticSearch - if you are deploying a service for this then just start with Postgres for everything. The only time you would want to use ES is if you are planning on indexing the full documents in there (reasons for and against this).\n\nThe OCR pipeline has many solutions - Comprehend is very good from AWS and you can pipeline that many documents easily, however, your chunking strategy / solution is the most important thing to get right and there are no easy / ready made solutions out there that solve this for you, it will be different on the document type, the document content and several other things!",
                  "score": 1,
                  "created_utc": "2026-01-17 08:49:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzxbfdq",
          "author": "Majinsei",
          "text": "2 million is quite a large number~\n\nYou need to do a classification analysis of exactly what you have...\n\nProcessing invoices is not the same as processing laws, processing minutes, processing a survey, etc.~\n\nIf necessary, each of these will surely have a custom extraction method, although creating about 20 different extraction methods is easier than creating several hundred.~\n\nThere are several ways to do it, but a popular strategy is to extract the metadata. You create a metadata table, and your agents search using that metadata as the first filtering step. Then you use the embedding of those filters to get the relevant data from there.\n\nThe metadata, for example, involves adding an Entity extractor step:\n\n{\"Type\": City, \"name\": Los Angeles, \"id\": 5}\n\nAnother for dates, and so on, depending on the data model you need.\n\nSo, when the user asks for the city of \"Los Angeles,\" an Entity extractor is used in the query, allowing you to filter by the city dimension (this has its pros and cons). And so on with each data point. You can optimize it later with a custom model that does everything at once.\n\nAnd these filters based on your metadata require filtering by ranking the N most similar.\n\nThis is a more or less effective method.\n\n\nRegarding OCR, you have to consider two different cases. One is pure images to be OCRed; there are many good cloud-based models for this, and you can even customize them yourself according to your needs. And you also have to consider when you have the explanatory context; a graph requires knowing which line goes up and which line goes down.\n\n\nAlthough this depends on how granular you want the information to be.",
          "score": 2,
          "created_utc": "2026-01-16 14:27:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzz8nwg",
          "author": "patbhakta",
          "text": "2-4mill documents and mostly OCR... You're in for a treat. I wish you well. \n\nYou don't need RAG just yet you need an architect. Let's say you ignore my advice so you proceed...\n\nLet's say you find a YouTuber follow some clever selling n8n\\rag implementation and it might even work or so you think on the first document. Maybe even the 10th or 100th document. So you think. \n\nThen reality sets in your users who actually retrieve the stuff find the information garbage when researching a bit more. Or worse your AI hallucinates information based on your embeddings. \n\nThen you work on 2.0\nOnly to find chunking large documents suck\nOnly to find OCR sucks\nOnly to find charts, tables, diagrams, media is lost in ingestion\nYou come to find data cleansing is hard on just a few different documents let alone hundreds or even thousands.\n\nBut let's say you figured it all out now you need to work on 3.0\nThis involves scaling vector databases, graph data, traditional data, etc. \nCongrats you managed to embed 4 million documents! \nYou get a meeting with the CFO about why the database cost too much monthly, users are also complaining about it being slow at times, constantly getting unrelatable documents, and it's just clunky in general\n\nNow it's time for 4.0\nYou discover the databases are littered with duplicate entries from ingesting similar template documents, you discover dedup methods and implement that, potentially breaking your beautiful vectors. \nNow your CEO is happy about the speed optimization and reduced cost, however the users still think it's shit and unreliable. Congrats you spent months on something nobody can use or trust... \n\nThen you still have 5.0, 6.0, 7.0, 8.0 problems too such as  data security, everyone in the damn company has access to all 4 million documents and it's crappy info. CEO doesn't want proprietary data going to openAI or others. New documents are made constantly and it's mixing with old documents and confusing the shit about what's newer. Users find your system to be shit, they end up manually getting relevant docs uploading them to ChatGPT so it can give a seemingly intelligent answer to the dummy who knows well whatever you built is worse. \n\nCEO cans the project because AI can't be trusted, data is flawed, staff is lazy and inept now thanks to your RAG.",
          "score": 2,
          "created_utc": "2026-01-16 19:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztrd1y",
          "author": "Infamous_Ad5702",
          "text": "I did it.\nI do it via an index. And build a new KG on the fly for each new query. \nIt’s offline.\nDeterministic \nNot AI.\nI can show you.\nI do reddit webinars now apparently",
          "score": 2,
          "created_utc": "2026-01-15 23:55:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztpinp",
          "author": "TrevorHikes",
          "text": "Talk to https://aicamp.so/\n\nAnd no I’m not affiliated but I have used their platform and they have been doing on premise solutions for high compliance industries and aren’t so big that a small to medium biz customer wouldn’t be attractive. \n\nIf you want to see an ideal solution in my opinion for productivity check out Juma.ai",
          "score": 1,
          "created_utc": "2026-01-15 23:45:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzujwv2",
          "author": "zapaljeniulicar",
          "text": "A huge chunk of the advice lies in what are you actually going to talk to, like what is in those PDFs. What information are you going to deal with? I’ve seen somebody here talking about RAG for CAD PDFs, like why? If you are searching for an image, RAG is not the way to do it.",
          "score": 1,
          "created_utc": "2026-01-16 02:33:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzv3x1o",
          "author": "IdeaAffectionate945",
          "text": "2/4 million documents might be too much for our SQLite VSS database, but if you want to perform tests to check if you could get away with it, you can search for AINIRO Magic Cloud (open source!)\n\nIt can automatically convert PDF (with **text**) to text ...",
          "score": 1,
          "created_utc": "2026-01-16 04:30:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxhc1g",
          "author": "sippin-jesus-juice",
          "text": "They want to vectorize 4 million documents but only are prepared to spend 100k in total for the project?",
          "score": 1,
          "created_utc": "2026-01-16 14:56:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxvv3f",
          "author": "seomonstar",
          "text": "best approach get a new job",
          "score": 1,
          "created_utc": "2026-01-16 16:02:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzyk425",
          "author": "kbash9",
          "text": "Just use a managed platform that offers agentic RAG: rather than a single lookup, an agentic RAG system uses a planner to find the answer. Much better accuracy",
          "score": 1,
          "created_utc": "2026-01-16 17:49:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzykwnl",
          "author": "Wise_Reward6165",
          "text": "IMHO you would need a good server to use or 9-series cpu/gpu for local at minimum. Servers are recommended for continuous duty. If money is the primary concern (and not time), consider using a CPU server with a lot of RAM. GPUs are great for acceleration but unnecessary for building the initial database. A standard PC with a 7900 or 5080 could serve the actual AI model once the database is fully assembled. Since security is an issue you will have to be technical btw. Use air gapped protection without wifi-bluetooth-eth0 initially then when the database is assembled serve it as a reverse proxy (with apache or nginx) to a second machine in a VM with monitoring.\n\nTools:\nChromadb transformers torch langchain\nFaiss and llama.cpp are optional. Custom binding a llama.cpp fork for security is extra work but necessary for security.\n\nFrom HF.io:\ndeepseek-ocr\nQwen3-embedding \nNomic-text-embedding (is highly rated)\n\nIt would be a multi-model setup to make the initial database but after that just serve it with apache or nginx and a reverse proxy to your intranet.\n\nAlso, you ‘might’ be able to use traefik as a web scraper arranged locally on the initial data and then feed it to the Chromadb setup. (Custom programming for sure)\n\nAlso, Also, you could look into using vision models for the PDFs like llama scout but most I think are heavy on ram and vision models still struggle with setup and accuracy.",
          "score": 1,
          "created_utc": "2026-01-16 17:53:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzypb2t",
          "author": "Hungry-Amount-2730",
          "text": "I'm curious - Why building it from scratch, not just buying already existing software?",
          "score": 1,
          "created_utc": "2026-01-16 18:12:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzyrcgu",
              "author": "Next-Self-184",
              "text": "yea this might be the way, focus on the OCR pipeline and get something like elastic search",
              "score": 1,
              "created_utc": "2026-01-16 18:21:50",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzys3pg",
                  "author": "Hungry-Amount-2730",
                  "text": "to be completely honest - we have built semantic code search tool and we have PoC for documentation (and gonna go further, its on ou roadmap) , that's why it was my business curiosity to have better understanding :)",
                  "score": 1,
                  "created_utc": "2026-01-16 18:25:11",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o01mja4",
          "author": "Previous-Ad5318",
          "text": "we use Ziqara internally for document search. Its like google drive for Rag, we kinda love it as they outperformed glean and other rag solutions",
          "score": 1,
          "created_utc": "2026-01-17 03:24:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02xiuy",
          "author": "techie_boy69",
          "text": "IBM Docling project on GitHub",
          "score": 1,
          "created_utc": "2026-01-17 09:50:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o03gfsv",
          "author": "jannemansonh",
          "text": "Worth checking out Needle. Production-ready RAG API, handles large document volumes including PDFs with OCR. Might save you from building everything from scratch.",
          "score": 1,
          "created_utc": "2026-01-17 12:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztezp5",
          "author": "BigNoseEnergyRI",
          "text": "Every off the shelf enterprise search solution can do this without having to build it. Coveo, Elastic, Lucid, serachblox, lucid, Lucy, Glean, OpenText, Google, etc.",
          "score": 0,
          "created_utc": "2026-01-15 22:49:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "nztni1h",
              "author": "ggone20",
              "text": "Someone has never built anything before lol. Come back when you’re grown up, kid 😎😂\n\nThere literally doesn’t exist an out of box solution that is reliable for several hundred thousand documents much less millions.\n\nSure, you COULD throw them in these services, you’d never get anything useful out.",
              "score": -2,
              "created_utc": "2026-01-15 23:34:26",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nztvomo",
                  "author": "BigNoseEnergyRI",
                  "text": "We don’t know what they are even trying to get out of it and how many documents they actually need. \n\nBut you said I’m young, so muwah!!!",
                  "score": 2,
                  "created_utc": "2026-01-16 00:18:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzu3dya",
          "author": "SerDetestable",
          "text": "u don’t have 4M pdfs. nobody does",
          "score": -1,
          "created_utc": "2026-01-16 01:00:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztrbwn",
          "author": "abhi91",
          "text": "Just use contextual AI. This is a standard use case, they power enterprises with this scale and it's affordable. Most importantly you'll be able to have enterprise grade security. If needed it can be deployed on your own VPC",
          "score": -1,
          "created_utc": "2026-01-15 23:55:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzu5lhy",
              "author": "Wikileaks_2412",
              "text": "ContextualAI is affordable ? How much do they charge for the parsing part ?",
              "score": 1,
              "created_utc": "2026-01-16 01:13:04",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzuzsoz",
          "author": "Responsible_Type_",
          "text": "Bro just use aws bedrock. U don't need to worry about anything other than cost. \n\nJust learn what is what thorugh youtube.",
          "score": -1,
          "created_utc": "2026-01-16 04:04:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzvt9cr",
              "author": "Much-Researcher6135",
              "text": "\"just throw money at the problem\" lol",
              "score": 2,
              "created_utc": "2026-01-16 07:43:59",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nztmnb9",
          "author": "ggone20",
          "text": "This will be extremely difficult. \n\nI’ve discussed this in many other threads but if you’re here looking for advice to build a RAG system at this scale you’ve already failed. \n\nHire someone. Hire ME, I can be available. #shameless-pitch\n\nFor real though… hire someone who demonstrably has done this before. You’ll never succeed. Not putting you down, it’s just that hard and there STILL doesn’t yet exist a service that you can just plug and play into. Further, even if you do everything RIGHT, it will be very expensive just to do the ingestion and processing of this many documents - you/your employer likely won’t expect actual costs. \n\nIf you do things wrong or need to trial/error, costs balloon. There isn’t really a way to test if things work until you scale the ingestion as well. Lots of processes work at smaller document numbers and fall apart in the tens or hundreds of thousands. Nevermind millions.  \n\nFor the record I’ve led building systems for utility-scale capital construction projects where 5-10 million artifacts are produced over the course of the project. The utility portfolio has hundreds of millions. In addition to the artifacts created, the actual management of the project requires accurate retrieval to give go/no-go decisions as well as adherence to standards, regulations, etc where reporting decisions with provenance is critical. Good times. \n\nGood luck. \nDMs open 🫡🙃🤷🏽‍♂️",
          "score": -3,
          "created_utc": "2026-01-15 23:29:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9brpp",
      "title": "🚀 Master RAG from Zero to Production: I’m building a curated \"Ultimate RAG Roadmap\" playlist. What are your \"must-watch\" tutorials?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q9brpp/master_rag_from_zero_to_production_im_building_a/",
      "author": "Dev-it-with-me",
      "created_utc": "2026-01-10 18:37:10",
      "score": 41,
      "num_comments": 6,
      "upvote_ratio": 0.96,
      "text": "Hey everyone,\n\nRetrieval-Augmented Generation (RAG) is moving at light speed. While there are a million \"Chat with PDF\" tutorials, it's becoming harder to find deep dives into the advanced stuff that actually makes RAG work in production (Evaluation, Agentic flows, GraphRAG, etc.).\n\nI’ve started a curated YouTube playlist: [**RAG - How To / All You Need To Know / Tutorials**](https://www.youtube.com/watch?v=0kVT1B1yrMc&list=PLIjrFhaJBqA4PYmO_16489unuYLfHY-CQ&index=2).\n\nMy goal is to build a playlist that goes from the basic \"What is RAG?\" to advanced enterprise-grade architectures.\n\n**Current topics covered:**\n\n* **Foundations:** High-level conceptual overviews.\n* **GraphRAG:** Visual guides and comparisons vs. traditional RAG.\n* **Local RAG:** Private setups using Ollama & local models.\n* **Frameworks:** LangChain Masterclasses & Hybrid Search strategies.\n\n**I’m the creator of the GraphRAG and Local RAG videos in the list,** but I know I can't cover everything alone. I want this to be a \"best-of-the-best\" resource featuring creators who actually explain the why behind the code.\n\n**I’m looking for your recommendations!** Specifically, do you know of high-quality videos on:\n\n1. **Evaluation:** RAGAS, TruLens, or DeepEval deep dives?\n2. **Chunking:** Beyond just recursive splitting - semantic or agentic chunking?\n3. **Agentic RAG:** Self-RAG, Corrective RAG (CRAG), or Adaptive RAG tutorials?\n4. **Production:** Real-world deployment, latency optimization, or CI/CD for RAG?\n5. **Multimodal RAG:** Tutorials on handling images, complex PDF tables, or charts using vision models?\n\nIf there’s a creator you think is underrated or a specific video that gave you an \"Aha!\" moment, please drop the link below. I'll be updating the playlist regularly.\n\nThanks for helping build a better roadmap for the community! 🛠️",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1q9brpp/master_rag_from_zero_to_production_im_building_a/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nyvlmsp",
          "author": "Fear_ltself",
          "text": "Embeddings are in my opinion the most important aspect of a great RAG. Research stuff like EmbeddingGemma:300m or the new qwenVL embeddor (I think it embeds photos for example)… they have all kinds of neat tricks. Also be aware the model has to be the same lineage as the embedding (qwen VL- qwen 14b, embeddingGemma:300m - Gemma3 27B) or the embedding will be gibberish to the model",
          "score": 3,
          "created_utc": "2026-01-10 23:47:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "nywl0sj",
              "author": "sqm_prout",
              "text": "Great point about embeddings! They really are the backbone of RAG. Have you found any specific tutorials that dive deep into those new models like qwenVL or embeddingGemma? I'd love to add some solid resources on that.",
              "score": 1,
              "created_utc": "2026-01-11 02:55:42",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nywlcyf",
                  "author": "Fear_ltself",
                  "text": "https://youtu.be/Xu1X-J-r5Xk\n\nI just watch the release vids and read the documentation for the capabilities, then try to code an example to get the feature working locally",
                  "score": 1,
                  "created_utc": "2026-01-11 02:57:39",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "nyyiwip",
                  "author": "Dev-it-with-me",
                  "text": "You can checkout my guide on Naive RAG and it is already in the playlist - I used embeddingGemma for embeddings and Gemma3 for inference.  \n  \n[https://www.youtube.com/watch?v=TqeOznAcXXU&list=PLIjrFhaJBqA4PYmO\\_16489unuYLfHY-CQ&index=3](https://www.youtube.com/watch?v=TqeOznAcXXU&list=PLIjrFhaJBqA4PYmO_16489unuYLfHY-CQ&index=3)",
                  "score": 1,
                  "created_utc": "2026-01-11 11:58:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nza58z0",
          "author": "Drj_dev411",
          "text": "Would love to explore Evaluation of RAGs and what each evaluation library has to offer and how it can be helpful",
          "score": 1,
          "created_utc": "2026-01-13 02:32:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nytyika",
          "author": "macromind",
          "text": "This playlist idea is great, the gap between \"chat with PDF\" and actual production RAG is huge.\n\nIf youre adding agentic RAG content, Id vote for resources that show: retrieval eval, self critique loops, and how they stop tool calls from spiraling (timeouts, budgets, fallbacks). Those bits matter way more than the happy path demo.\n\nAlso, Ive bookmarked this as a quick reference for agentic AI + automation patterns and guardrails: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-01-10 18:50:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qdziof",
      "title": "New Chapter on \"Chunking Strategies\" - 21 RAG Strategies Book",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qdziof/new_chapter_on_chunking_strategies_21_rag/",
      "author": "blue-or-brown-keys",
      "created_utc": "2026-01-15 23:29:31",
      "score": 31,
      "num_comments": 5,
      "upvote_ratio": 0.97,
      "text": "I have added a new Chapter on Chunking the \"21 RAG Strategies\" Book. I am looking for feedback, Which of these strategies do you use in production? Also do you use a strategy you like thats not mentioned here?\n\n[Download \"21 RAG Strategies\" Ebook here](https://www.twig.so/#DownloadEbookSection)\n\n* **Chapter 22 — Chunking Strategies for Retrieval-Augmented Generation**\n      1. Chunking as a Core RAG Primitive\n   * 1.1 Definition of a Chunk\n   * 1.2 Chunking vs. Text Splitting\n   * 1.3 Chunking and Retrieval Semantics\n* 2. Why Chunking Determines RAG Accuracy\n   * 2.1 Context Window and Model Constraints\n   * 2.2 Retrieval Precision and Recall\n   * 2.3 Cost, Latency, and Token Efficiency\n   * 2.4 Chunking as an Information Architecture Problem\n* 3. Baseline Chunking Approaches\n   * 3.1 Fixed-Size Token Windowing\n   * 3.2 Sentence-Aligned Chunk Construction\n   * 3.3 Paragraph-Aligned Chunk Construction\n* 4. Structure-Driven Chunking\n   * 4.1 Section- and Heading-Scoped Chunking\n   * 4.2 Document Markup–Aware Chunking\n   * 4.3 Code- and Clause-Scoped Chunking\n* 5. Semantic Boundary Detection\n   * 5.1 Topic Shift–Based Chunk Segmentation\n   * 5.2 Embedding Similarity Thresholding\n   * 5.3 Discourse-Level Chunk Formation\n* 6. Context Preservation Techniques\n   * 6.1 Controlled Overlap and Window Expansion\n   * 6.2 Sentence-Window Retrieval Models\n   * 6.3 Contextual Header Injection\n   * 6.4 Pre- and Post-Context Buffering\n* 7. Hierarchical and Multi-Resolution Chunking\n   * 7.1 Fine-Grained vs. Coarse-Grained Retrieval Units\n   * 7.2 Parent–Child Chunk Hierarchies\n   * 7.3 Recursive and Outline-Derived Chunking\n* 8. Question-Centric Chunk Design\n   * 8.1 Generating Retrieval-Aligned Questions\n   * 8.2 Answer-Complete Chunk Construction\n   * 8.3 Context-Buffered Question Anchoring\n* 9. Dual-Index and Retrieval-First Architectures\n   * 9.1 Question-First Retrieval Models\n   * 9.2 Canonical Chunk Grounding\n   * 9.3 Deduplication, Reranking, and Stitching\n* 10. Domain-Aware Chunking Patterns\n   * 10.1 API and Reference Documentation\n   * 10.2 Support Tickets and Conversation Threads\n   * 10.3 Policy, Compliance, and Versioned Knowledge\n* 11. Evaluation-Driven Chunk Optimization\n   * 11.1 Measuring Chunk Quality\n   * 11.2 Retrieval Accuracy and Citation Fidelity\n   * 11.3 Iterative Chunking Refinement\n* 12. Practical Guidance and Trade-Offs\n   * 12.1 Choosing the Right Strategy per Data Source\n   * 12.2 Combining Multiple Chunking Strategies\n   * 12.3 Common Failure Modes and Anti-Patterns\n* 13. Summary: Chunking as the Foundation of RAG\n* 13.1 Why Models Fail When Chunking Fails\n* 13.2 Recommended Production Defaults",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qdziof/new_chapter_on_chunking_strategies_21_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzwjarp",
          "author": "GroundbreakingEmu450",
          "text": "I can already see it is missing anything related to AST based chunking for indexing codebases for example",
          "score": 3,
          "created_utc": "2026-01-16 11:36:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzwjg41",
          "author": "GroundbreakingEmu450",
          "text": "Also the structure you posted here does not match the one of the book, wtf?",
          "score": 2,
          "created_utc": "2026-01-16 11:37:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzxon9h",
              "author": "blue-or-brown-keys",
              "text": "Let me check, this is only one chapter of the book.",
              "score": 1,
              "created_utc": "2026-01-16 15:30:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwpith",
          "author": "Able-Let-1399",
          "text": "Thanks. I will when I've read it 👍",
          "score": 2,
          "created_utc": "2026-01-16 12:22:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzumx9s",
          "author": "blue-or-brown-keys",
          "text": "Please share your feedback and how I can improve on the book, what strategies are missing and which ones you have used in production.",
          "score": 1,
          "created_utc": "2026-01-16 02:50:09",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qaxwi5",
      "title": "We tested Vector RAG on a real production codebase (~1,300 files), and it didn’t work",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qaxwi5/we_tested_vector_rag_on_a_real_production/",
      "author": "Julianna_Faddy",
      "created_utc": "2026-01-12 15:30:12",
      "score": 30,
      "num_comments": 29,
      "upvote_ratio": 0.78,
      "text": "Vector RAG has become the default pattern for coding agents: embed the code, store it in a vector DB, retrieve top-k chunks. It feels obvious.\n\nWe tested this on a real production codebase (\\~1,300 files) and it mostly… didn’t work.\n\nThe issue isn’t embeddings or models. It’s that similarity is a bad proxy for relevance in code.\n\nIn practice, vector RAG kept pulling:\n\n* test files instead of implementations\n* deprecated backups alongside the current code\n* unrelated files that just happened to share keywords\n\nSo, the agent’s context window filled up with noise. Reasoning got worse, not better.\n\nWe compared this against an agentic search approach using context trees (structured, intent-aware navigation instead of similarity search). We won’t dump all the numbers here, but a few highlights:\n\n* Orders of magnitude fewer tokens per query\n* Much higher precision on “where is X implemented?” questions\n* More consistent answers for refactors and feature changes\n\nVector RAG did slightly better on recall in some cases, but that mostly came from dumping more files into context, which turned out to be actively harmful for reasoning.\n\nThe takeaway for me:\n\nCode isn’t documentation. It’s a graph with structure, boundaries, and dependencies. Treating it like a bag of words breaks down fast once the repo gets large.\n\nI wrote a [detailed breakdown](https://github.com/RyanNg1403/agentic-search-vs-rag?tab=readme-ov-file) of the experiment, failure modes, and why context trees work better for code (with full setup and metrics) here if you want the full take.\n\nCurious if others here have hit similar issues with vector RAG for code, or if you’ve found ways to make it behave at scale.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qaxwi5/we_tested_vector_rag_on_a_real_production/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz6orvk",
          "author": "hrishikamath",
          "text": "You are not supposed to chunk codebase directly! Rather take functions/classes and then have a LLM summarize them and do RAG on that! Read Greptile engineering blog they describe this well",
          "score": 23,
          "created_utc": "2026-01-12 16:26:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz9hnu6",
              "author": "technicalpickles",
              "text": "https://www.greptile.com/blog/semantic-codebase-search",
              "score": 2,
              "created_utc": "2026-01-13 00:25:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzaqp1m",
                  "author": "hrishikamath",
                  "text": "Thanks for the exact link",
                  "score": 1,
                  "created_utc": "2026-01-13 04:32:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nzbbhoz",
              "author": "Diligent-Builder7762",
              "text": "*separate LLM does rag, summarizes them to the main one. and ripgrep. Yummy.",
              "score": 1,
              "created_utc": "2026-01-13 07:12:13",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "nzl2fbg",
              "author": "Julianna_Faddy",
              "text": "Hey, thanks for the Greptile tip, summarizing functions/classes first is smart. We tried method-level chunking, but vectors still missed code deps like imports, leading to noisy context. Worth benchmarking their approach  tho!",
              "score": 1,
              "created_utc": "2026-01-14 18:34:48",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzl8yyv",
                  "author": "hrishikamath",
                  "text": "I mean it’s not a complete solution by itself. You will have to make changes around that. I was telling more about the chunking part.",
                  "score": 1,
                  "created_utc": "2026-01-14 19:03:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6no0j",
          "author": "ggone20",
          "text": "Why downvoted to hell?\n\nLarge codebases need knowledge graphs, at the very least. Advanced RAG is a major driver of consulting business. On one hand I’m surprised there aren’t more write-ups but on the other… I don’t want to give my secrets when commercially intelligent retrieval and inference over data stores is by far the biggest market right now in AI since every company can use it in some degree. \n\nYou mentioned in the larger piece the ‘museum problem’ - this is a function of poor architecture AND another area where graphs would help… so long as you keep them updated. For large codebases, change management/hooks are critical to keep things up to date. It’s easy enough to automate but work does need to be done to this end. Just setup on-change scripts and reprocess files. 🙃\n\nSeems like overall you’re happy with the system you ended up with, any other major headaches you haven’t figured out a solution for? Are the results good enough for your use case? I feel like the numbers, while dramatically better, are still rough… no?",
          "score": 6,
          "created_utc": "2026-01-12 16:21:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl2t88",
              "author": "Julianna_Faddy",
              "text": "Haha, no clue on the downvotes, Reddit gonna Reddit. Totally agree on knowledge graphs for big codebases; that's why agentic search via context trees felt like a win because it basically builds a dynamic graph on the fly, respecting deps without the full KG overhead. On the museum problem, yeah, poor arch is part of it, but graphs + hooks for updates are key, we're automating re-indexing with git hooks now, keeps it fresh.\n\nWe're stoked with the setup; results are solid for our prod use (e.g., quick impl lookups), but yeah, numbers aren't perfect, hallucinations creep in on super ambiguous queries.",
              "score": 1,
              "created_utc": "2026-01-14 18:36:31",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzn7uk2",
                  "author": "ggone20",
                  "text": "Nice. It’s fun when there’s no ‘right’ answer I development. In prod it’s a bit different through haha. Keep us updated!",
                  "score": 2,
                  "created_utc": "2026-01-15 00:45:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6npz3",
          "author": "dyeusyt",
          "text": "we ran into pretty much the same problem when we got into this codebase-knowledge bog\n\nbut in our case we wanted to create a system that could evaluate GitHub repos. i.e only index the codebase once; not after every update.\n\nas we started, we didn’t find any resources on this other than videos showing graphical implementation of semantic search using Euclidian distance and so after looking at a bunch of open-source projects and VS Code’s way of managing things, we made our own Python library for it. our main goal was actually to build semantic search functionality out of it (note we had never done this kind of stuff earlier)\n\nso after some trial and error, we built the whole system. it chunks the codebase with the help of tree-sitter and builds a parent–child relationship between chunks to reduce extra chunks and noise.\n\nalthough the semantic search started working, we soon realized the same problem as you did: “similarity is a bad proxy for relevance in code.”\n\nLater on, we realized how some CLI-based coding agents like AWS Q work:\n\n\\* they read the project structure a lot, they do a lot of \\`cat\\` and \\`grep\\` instead of semantic searches, and still outperform agents that rely heavily on semantic search.\n\n\\* this gave us the idea to build more tools like \\`grep\\`, \\`cat\\`, \\`folder\\_structure\\`, etc (thanks to \\`chromadb\\` for these though)\n\n\\* so instead of solely relying on semantic search, we distributed the load.\n\n\\* now the agent gets the repo folder structure and is easily able to get a gist of the codebase from the structure itself\n\n\\* for files it considers important, it automatically \\`cat\\`s them; for patterns, it calls the pattern-matching tool.\n\n\\* and only for more generalized queries like “authentication implementation” does the agent currently do semantic tool calling.\n\n\\* this way, we were able to still use semantic tool calling, but in a much more efficient way.\n\na few days ago, I also posted a thread on this sub regarding too much noise in semantic search results. one better fix for that turned out to be using a dual-vector index mechanism and analyzing the intent of the semantic query each time, then redirecting it accordingly.\n\nwe’re still learning about these things, but this is basically how we’ve monkey-patched our way till here.\n\nhere’s the thread mentioned above: [https://www.reddit.com/r/Rag/comments/1q3ksvy/how\\_do\\_you\\_tackle\\_semantic\\_search\\_ranking\\_issues/](https://www.reddit.com/r/Rag/comments/1q3ksvy/how_do_you_tackle_semantic_search_ranking_issues/)\n\nEdit: can you share your repo link (that 1300 files one), will probably see how our semantic search performs lol",
          "score": 3,
          "created_utc": "2026-01-12 16:21:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6owd9",
              "author": "Julianna_Faddy",
              "text": "the repo link [Experimental validation: Agentic Search (context trees) vs traditional RAG for code retrieval. Agentic wins with 99% fewer tokens and 2× better accuracy. Fully reproducible with automated pipelines.](https://github.com/RyanNg1403/agentic-search-vs-rag) this repo link is also included in the detailed breakdown blog",
              "score": 1,
              "created_utc": "2026-01-12 16:27:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7bur8",
          "author": "OnyxProyectoUno",
          "text": "Yeah, code as a bag of words is fundamentally broken. You're spot on about similarity being a terrible proxy for relevance in structured codebases.\n\nThe test files vs implementation thing is classic. Embeddings see similar function signatures and think \"this looks relevant\" when it's actually noise. Same with deprecated code that shares naming patterns with current implementations. Your context window fills up with garbage before you even get to reasoning.\n\nContext trees make way more sense for code. You're working with actual relationships instead of word soup. Function calls, imports, class hierarchies. That's the real structure that matters for understanding how code works.\n\nI've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) for document processing pipelines, and while that's different from code search, the core insight is the same. Structure matters more than similarity. With documents, I see people losing hierarchy, metadata, and relationships because they're chunking without considering document structure. Same problem, different domain.\n\nFor code specifically, have you looked at tree-sitter for parsing? It gives you the AST structure you need for proper context trees. Combined with dependency graphs from static analysis, you can build much smarter traversal than vector similarity.\n\nThe token efficiency gain you're seeing makes sense too. When you're pulling relevant code based on actual relationships instead of keyword matching, you need way fewer examples to give the model proper context.\n\nWhat does your context tree traversal look like? Are you doing breadth-first from entry points or something more sophisticated based on the query type?",
          "score": 2,
          "created_utc": "2026-01-12 18:11:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl3jms",
              "author": "Julianna_Faddy",
              "text": "Dude, this is spot on, we hit the exact same wall with pure semantic search being noisy AF for code. Love how you monkey-patched it with tree-sitter chunking, parent-child relations, and tools like grep/cat/folder\\_structure to offload the heavy lifting. That's basically agentic search: let the agent decide when to grep patterns or cat files vs semantic for broader stuff. In our tests, context trees did that graph traversal dynamically. \n\nWe're still tweaking too\n\nSure, here's the repo (no surprise, it's gemini-cli repo, a really big prod-ready repo):https://github.com/google-gemini/gemini-cli",
              "score": 1,
              "created_utc": "2026-01-14 18:39:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz7fued",
          "author": "notAllBits",
          "text": "Code is structured. Use to operationalize your data. You turn structure into poison using just vector indexing",
          "score": 2,
          "created_utc": "2026-01-12 18:29:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl429y",
              "author": "Julianna_Faddy",
              "text": "That's right code's all structure, and straight vector indexing turns that gold into garbage by ignoring deps and hierarchies.",
              "score": 1,
              "created_utc": "2026-01-14 18:42:03",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6eeep",
          "author": "Creative-Chance514",
          "text": "I did not read your full post but the first half suggests that its a problem of indexing, if you will just index everything in the vector db you will get chunks from everything. In the very first class of computers when I was a kid I was taught about a concept called GIGO",
          "score": 4,
          "created_utc": "2026-01-12 15:38:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz6ihcf",
              "author": "Technical-Will-2862",
              "text": "Your comment didn’t even answer their question so try reading more. Indexing is great but it doesn’t solve contextual relevance. ",
              "score": 4,
              "created_utc": "2026-01-12 15:57:42",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz6msmt",
          "author": "irodov4030",
          "text": "which specific embedding model did you use to embed codebase?",
          "score": 1,
          "created_utc": "2026-01-12 16:17:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl18l7",
              "author": "Julianna_Faddy",
              "text": "I used text-embedding-3-small from OpenAI",
              "score": 1,
              "created_utc": "2026-01-14 18:29:33",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzl99jx",
                  "author": "irodov4030",
                  "text": "This is a model for embedding texts and more for general purpose tasks\n\n  \nIf you want to embed codebase try using specific models like CodeBERT. There are models available specific to your coding language too.\n\n [https://github.com/microsoft/CodeBERT](https://github.com/microsoft/CodeBERT)",
                  "score": 1,
                  "created_utc": "2026-01-14 19:05:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz6o9gt",
          "author": "Medium_Chemist_4032",
          "text": "If you have access to a model with long context (like the unsloth extended nemotron 1M), try using it instead of RAG for filtering out relevant parts in the first stage of processing.  \nEDIT: I'd love to see a comparision to RAG of that",
          "score": 1,
          "created_utc": "2026-01-12 16:24:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6vju3",
          "author": "CallOfBurger",
          "text": "Have you added metadata on your embeddings after the chunking ? If you only get deprecated stuff and test file it might retrieve the wrong stuff. You could add title, tags, dates etc. you could filter out of your search",
          "score": 1,
          "created_utc": "2026-01-12 16:57:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdmm3d",
          "author": "TechnicalGeologist99",
          "text": "Code isn't semantic!\n\nIt's already structured and highly queriable/navigable\n\nWhy would we want to search for semantically similar text? \n\n\nUnless you have fine-tuned some embedding models to align with the semantic meaning of discrete pieces of code, and you are trying to find similar code that isn't syntactically similar but is semantically similar \n\nI.e. two separate implementations of one algorithm.\n\nThen I don't really see any need to be embedding code.",
          "score": 1,
          "created_utc": "2026-01-13 16:39:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdz2ao",
          "author": "Whole-Assignment6240",
          "text": "It really comes down to what metadata you collect and how you handle the codebase—for example, respecting semantic breaks at code boundaries by tree-sitter.  \ntake a look at [https://cocoindex.io/examples/code\\_index](https://cocoindex.io/examples/code_index) (i'm the maintainer) we also plan to do graph version for this as well! lmk what you think!",
          "score": 1,
          "created_utc": "2026-01-13 17:48:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzh0jzl",
          "author": "Academic_Track_2765",
          "text": "lol what are you doing!!!! This should be a case study on poor implementation. Code by design is not semantic! And this should be enough to understand why this failed.",
          "score": 1,
          "created_utc": "2026-01-14 02:52:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzl5cuc",
              "author": "Julianna_Faddy",
              "text": "Agreed, that’s exactly why treating it as “just embed + top-k” fails in practice. This wasn’t a toy setup: clean chunking, good embeddings, metadata, the usual best practices — and it still pulled irrelevant tests/backups because similarity ≠ intent",
              "score": 1,
              "created_utc": "2026-01-14 18:47:44",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzmo161",
                  "author": "Academic_Track_2765",
                  "text": "You can solve this issue, we have done it, but it adds latency which is expected.",
                  "score": 1,
                  "created_utc": "2026-01-14 22:58:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nz722md",
          "author": "jba1224a",
          "text": "“We used the wrong approach and got a bad result”\n\n*scrolls to bottom*\n\nYep someone else trying to sell something.\n\nIt would be so much more effective if you just stated the problem up front, offer up your solution - put a link to the raw data so people can comment and make a conclusion.\n\nAt the risk of sounding rude - no want wants to read your long winded pedantic Reddit post which is light on facts and heavy on generalizations.\n\nPs - code isn’t semantic.  Literally anyone in this space could explain in less than a paragraph why just straight embedding code using a non specialized model and then running pure semantic search against it would fail miserably.",
          "score": 0,
          "created_utc": "2026-01-12 17:27:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6ld9p",
          "author": "nineelevglen",
          "text": "I see zero reasoning on chunking strategy like AST using something like tree sitter. or picking an embedding model for code like Jina, Voyage etc. and performance between those. did you spend any time on that?",
          "score": 0,
          "created_utc": "2026-01-12 16:11:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qed97y",
      "title": "A user shared to me this complete RAG guide",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/",
      "author": "Real-Turnover9685",
      "created_utc": "2026-01-16 11:12:01",
      "score": 28,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "Someone juste shared to me this complete RAG guide with everything from parsing to reranking. Really easy to follow through.  \nLink : [https://app.ailog.fr/en/blog](https://app.ailog.fr/en/blog)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzwsx76",
          "author": "Much-Researcher6135",
          "text": "Fancy seeing this at the top of /r/rag. I just asked Gemini for RAG knowledge hubs and it [recommended this sick repo](https://github.com/NirDiamant/RAG_Techniques). I just found out about it 2 minutes ago, but 24k github stars can't be wrong. It also recommended the llamaindex and langchain docs, plus this subreddit!",
          "score": 3,
          "created_utc": "2026-01-16 12:44:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwtwqi",
              "author": "Real-Turnover9685",
              "text": "Oh that's nice, I'll take a look at it, thank you.",
              "score": 2,
              "created_utc": "2026-01-16 12:51:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzwl8mh",
          "author": "tchikss",
          "text": "Thanks for sharing !",
          "score": 1,
          "created_utc": "2026-01-16 11:51:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzwnjsz",
              "author": "Real-Turnover9685",
              "text": "Glad you liked it :)",
              "score": 1,
              "created_utc": "2026-01-16 12:08:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o02alm5",
          "author": "George_David_S",
          "text": "Thanks so much for sharing would love to connect with you",
          "score": 1,
          "created_utc": "2026-01-17 06:20:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzx0ajk",
          "author": "atultrp",
          "text": "[fastrag.live](http://fastrag.live)",
          "score": 0,
          "created_utc": "2026-01-16 13:29:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qafa53",
      "title": "Best practices for running a CPU-only RAG chatbot in production?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qafa53/best_practices_for_running_a_cpuonly_rag_chatbot/",
      "author": "Acceptable_Young_167",
      "created_utc": "2026-01-11 23:55:37",
      "score": 18,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "Hi r/LocalLLaMA 👋\n\nMy company is planning to deploy a **production RAG-based chatbot that must run entirely on CPU** (no GPUs available in deployment). I’m looking for **general guidance and best practices** from people who’ve done this in real-world setups.\n\n# What we’re trying to solve\n\n* Question-answering chatbot over internal documents\n* Retrieval-Augmented Generation (RAG) pipeline\n* Focus on **reliability, grounded answers, and reasonable latency**\n\n\n\n# Key questions\n\n**1️⃣ LLM inference on CPU**\n\n* What size range tends to be the sweet spot for CPU-only inference?\n* Is aggressive quantization (int8 / int4) generally enough for production use?\n* Any tips to balance latency vs answer quality?\n\n**2️⃣ Embeddings for retrieval**\n\n* What characteristics matter most for CPU-based semantic search?\n   * Model size vs embedding dimension\n   * Throughput vs recall\n* Any advice on multilingual setups (English + another language)?\n\n**3️⃣ Reranking on CPU**\n\n* In practice, is cross-encoder reranking worth the extra latency on CPU?\n* Do people prefer:\n   * Strong embeddings + higher `top_k`, or\n   * Lightweight reranking with small candidate sets?\n\n**4️⃣ System-level optimizations**\n\n* Chunk sizes and overlap that work well on CPU\n* Caching strategies (embeddings, reranker outputs, answers)\n* Threading / batch size tricks for Transformers on CPU\n\n\n\n# Constraints\n\n* CPU-only deployment (cloud VM)\n* Python + Hugging Face stack\n* Latency matters, but correctness matters more than speed\n\nWould love to hear **real deployment stories, lessons learned, or pitfalls to avoid**.  \nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qafa53/best_practices_for_running_a_cpuonly_rag_chatbot/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz2jqca",
          "author": "Altruistic_Leek6283",
          "text": "First:  Decide what are you want to delivery.   \nIf you don't mind latency go with the LLM with CPU. God have mercy in your soul. \n\nPlease, understand that the stack you will use, is defined by the data, not the hardware. Never the hardware, if you have issue with hardware you need to upgrade it. \n\nAll AI Systems you need to think first in the product, the architecture comes second, the third is the data. You need to see the corpus to understand the first stack and guess what? Will change. You will change the stack, because you don't know how the data will behave with the chuck and embedding process. \n\nDeploy a MVP of your system, and update here.",
          "score": 5,
          "created_utc": "2026-01-12 00:12:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8fb2c",
          "author": "Ok_Pomelo_5761",
          "text": "CPU only RAG can work if you keep the pipeline tight.\n\nLLM on CPU: pick a small model (3B to 8B) and quantize hard (int4). llama.cpp or GGUF usually gives better latency than raw HF on CPU.\n\nEmbeddings: use a small embedder, precompute everything, and keep top\\_k modest.\n\nReranking on CPU: yes it is worth it if you rerank a small set. Grab top 30 to 50 from embeddings, then rerank down to 5 to 8. This is where I would plug zeroentropy reranker, it is a clean drop in to boost precision without needing GPU if you keep candidates low.\n\nOps stuff: cache query embeddings + reranker results, keep chunks around 300 to 800 tokens with a bit of overlap, and force the bot to say I dont know when sources do not support the answer.",
          "score": 3,
          "created_utc": "2026-01-12 21:13:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz37cv7",
          "author": "raiffuvar",
          "text": "Metrics. Everything is solved by metrics. \nI've tried reranker on game specific slang and quality went down. \nSo, just try and search for your performance.",
          "score": 1,
          "created_utc": "2026-01-12 02:14:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3esia",
          "author": "Ok_Mirror7112",
          "text": "With your current requirements use pymupdf4llm for parsing. \n\nEmbeddings you will have to check which one has free tier.\n\nQuantisation works only if use over fetch 3-5x with RRF\n\nTop-k = 8-12 or 20\n\nChunk size 520 or 1028 depending on your goal.",
          "score": 1,
          "created_utc": "2026-01-12 02:54:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3g3z6",
          "author": "hrishikamath",
          "text": "I run completely on CPU, my embedding is ~300 dimension and latency is fine. I even use re ranker in prod on cpu. No quantization nothing. I think you are asking this question too early. First make a setup, benchmark it and then think of making it production ready.",
          "score": 1,
          "created_utc": "2026-01-12 03:01:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz3zq4w",
          "author": "Giedi-Prime",
          "text": "need to something similar for our company, want to learn more, can anyone recommend a good starting point?",
          "score": 1,
          "created_utc": "2026-01-12 04:56:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4ytj1",
          "author": "tony10000",
          "text": "4B-8B models.  Look at AnythingLLM coupled with LM Studio or Ollama as the LLM server.",
          "score": 1,
          "created_utc": "2026-01-12 10:01:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz539zj",
          "author": "ConcertTechnical25",
          "text": "Since you are on a Python + HF stack, look into Intel Extension for PyTorch (IPEX) or OpenVINO—the speedup on CPU is non-trivial compared to vanilla Transformers.\nThe real performance killer in CPU-RAG isn't the inference, it's the context bloat. If your chunks are too large, the self-attention mechanism will eat your CPU cycles for breakfast. Instead of long chunks, try \"Small-to-Big\" retrieval: index small 256-token chunks for better recall, but only feed the parent context to the LLM. This keeps your KV-cache small and your CPU latency predictable.",
          "score": 1,
          "created_utc": "2026-01-12 10:43:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5glzk",
          "author": "Rokpiy",
          "text": "the reranking tradeoff seems like the key question here. if correctness matters more than speed you probably want it, but cross-encoder on CPU adds up fast\n\nbetter embeddings + higher top\\_k might be the move? avoids the extra model call entirely. also curious what your retrieval recall looks like without reranking - might not even need it depending on your data",
          "score": 1,
          "created_utc": "2026-01-12 12:30:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6cmpb",
          "author": "Fun-Purple-7737",
          "text": "Best practices for running a CPU-only RAG chatbot:\n\ndon't",
          "score": 1,
          "created_utc": "2026-01-12 15:30:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6zpo3",
          "author": "OnyxProyectoUno",
          "text": "The chunking and caching side is where you'll probably get the biggest wins on CPU.\n\nFor chunking, smaller is usually better on CPU since you're already latency-constrained. I'd start around 256-512 tokens with 50-100 token overlap. Larger chunks mean more tokens to process during reranking, which hurts when you can't parallelize well. The tradeoff is you might need higher top_k to catch relevant info spread across multiple small chunks.\n\nAggressive embedding caching is crucial. Cache at the chunk level, not just query level. If your docs don't change much, precompute all chunk embeddings and store them. For queries, implement semantic similarity caching so similar questions hit cached results. Even fuzzy matching on query embeddings can save you inference cycles.\n\nOn the reranking question, cross-encoders are usually worth it but keep the candidate set small. Retrieve maybe 20-30 chunks, rerank to 5-8. The quality jump is significant enough to justify the latency hit, especially when correctness matters more than speed.\n\nOne gotcha: watch your memory usage with quantized models. int4 can be unstable under load, and int8 is often the better production choice even if it's slightly slower. Also, if you're doing multilingual, make sure your chunking strategy doesn't break on non-English text boundaries.\n\nWhat's your target response time looking like? That'll change the chunking math quite a bit.",
          "score": 1,
          "created_utc": "2026-01-12 17:16:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz77ylt",
          "author": "vinoonovino26",
          "text": "Try hyperlink.ai . It has done miracles for my rag setup",
          "score": 1,
          "created_utc": "2026-01-12 17:54:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze2kkp",
          "author": "Whole-Assignment6240",
          "text": "in most of the cases, in production, if you don't have enough resources and wants decent performance, Gemini embedding is pretty cost effective. a lot of our users use it.",
          "score": 1,
          "created_utc": "2026-01-13 18:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz4qdjz",
          "author": "Both-Number-7319",
          "text": "Up",
          "score": 1,
          "created_utc": "2026-01-12 08:40:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcjvn2",
      "title": "RAG BUT WITHOUT LLM (RULE-BASED)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "author": "adrjan13",
      "created_utc": "2026-01-14 10:27:43",
      "score": 12,
      "num_comments": 13,
      "upvote_ratio": 0.84,
      "text": "Hello, has anyone here created a scripted chatbot (without using LLM)? \n\nI would like to implement such a solution in my company, e.g., for complaints, so that the chatbot guides the customer from A to Z. I don't see the need to use LLM here (unless you have a different opinion—feel free to discuss). \n\nHas anyone built such rule-based chatbots? Do you have any useful links? Any advice? ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcjvn2/rag_but_without_llm_rulebased/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzimms7",
          "author": "redditorialy_retard",
          "text": "that's building the retrieval but replace LLM with scripts. IE having relevant article 1, 2, 3 (retrieval results) and having the chat bot recommend those. \n\n\nIe chatbot ask what is the problem, user input then is parsed to use as a query to the RAG.\n\n\nBut honestly it's just easier to put an LLM in the middle if you don't want to expose it to the end user.",
          "score": 5,
          "created_utc": "2026-01-14 10:36:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjegum",
          "author": "PrepperDisk",
          "text": "Tested this out with Haystack just doing retrieval, but then feeding into an LLM to rephrase the chunks.\n\nMy issue with this was user expectation.  With a simple search box, users know to search on keywords.  With a conversational interface, users increasingly expect to be able to phrase requests like they do with Gemini or ChatGPT  (with conversations and context from last request).\n\nMy solution was in a kind of \"uncanny valley\" where it was neither and users got stuck.",
          "score": 3,
          "created_utc": "2026-01-14 13:52:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzk7hvg",
          "author": "Elses_pels",
          "text": "Have you tried RASA ?",
          "score": 3,
          "created_utc": "2026-01-14 16:15:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjnesz",
          "author": "irodov4030",
          "text": "there are 100s of applications where you can use retrieved chunks in a workflow and do not need LLM to package the response. Your solution will be more deterministic than a typical RAG with LLM\n\nI have built a similar custom solution.\n\nYou are going in the right direction.\n\nLet me know if you have any specific questions.",
          "score": 2,
          "created_utc": "2026-01-14 14:40:34",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzq2cf7",
              "author": "ghaaribkhurshid",
              "text": "Hello, I'm a fresher in CS, I want to build career in AI, could you please guide me more on this?",
              "score": 1,
              "created_utc": "2026-01-15 13:24:17",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzio6rc",
          "author": "Necessary-Dot-8101",
          "text": "compression-aware intelligence (CAI) is useful bc it treats hallucinations, identity drift, and reasoning collapse not as output errors but as structural consequences of compression strain within intermediate representations. it provides instrumentation to detect where representations are conflicting and routing strategies that stabilize reasoning rather than patch outputs",
          "score": 1,
          "created_utc": "2026-01-14 10:50:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzipyni",
          "author": "trollsmurf",
          "text": "How would you build consistent responses from chopped up content chunks otherwise?\n\nIf you consistently chunk on chapters/sections and provide that whole section as a response it would work, but it's not quite the same thing.\n\nInterested in knowing how commercial support-related chatbots handle this.",
          "score": 1,
          "created_utc": "2026-01-14 11:05:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjn5h4",
          "author": "vdharankar",
          "text": "So basically you just want to pull the chunks and show user ? Base idea of RAG is generation with augmentation",
          "score": 1,
          "created_utc": "2026-01-14 14:39:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzjq12n",
          "author": "cubixy2k",
          "text": "So the standard intent based chat bots like Alexa skills?",
          "score": 1,
          "created_utc": "2026-01-14 14:53:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkj4cl",
          "author": "Alternative_Nose_874",
          "text": "You may consider [botpress.com](http://botpress.com) or similar open source platform as the backend for easy setup.",
          "score": 1,
          "created_utc": "2026-01-14 17:08:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzw8el0",
          "author": "TechnicalGeologist99",
          "text": "This is a classification problem. \n\nIngest text to something like XLM Roberta. Fine tune to classify the failure modes (or modes of complaint) that you have identified in your taxonomy.\n\nAt run time, the model predicts the label and the label triggers whatever text is associated with that problem",
          "score": 1,
          "created_utc": "2026-01-16 10:03:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o01bos7",
          "author": "HealthyCommunicat",
          "text": "I mean isnt this just classic route of “assign keywords to docs, use query keywords to mass scan and find matching docs?”\n\nSo you mean a knowledgebase?\n\nYou must be thinking of RAG as the object that the “knowledgebase/library” part that the LLM is attached to - but RAG actually means the process of taking information, usually a large number of separate docs, and then having the LLM read and use those tokens to generate an output. You’re overthinking it. You just want a knowledgebase.\n\nEasily put, RAG is a process not an object, the G for generation should tell you that much.\n\nHow are you going to make a chatbot that can possibly really detect every possible combination of keywords and make sure that it pulls up the right docs? Are you sure its not just the fact that you’re really really overthinking LLM’s and you would much rather not just download LM Studio + Anything LLM and drag and drop all your docs? Or is it because you can’t afford it? Solutions are usually made entirely to fix a problem, and noone can help solve your problem unless we know what  it even is.\n\n122 days ago you posted a thread related to RAGs. During that 122 days you have not still even understood what a RAG even is. It’s pretty easy to tell you just gave up when if you were to go and ask Gemini, it would give you exact step by step instructions on how to set this up, you can even really use like a 1-4b model on a 10 year old laptop, and Gemini can put the steps in a way that even a child can understand if you simply say “explain the steps like you’re explaining to a kid”.\n\nIt doesn’t matter what answer here someone gives you. You’ve shown that you will pretend to care and want to know something but still won’t make any progress 122 days later. The biggest group of people I despise are those that say they “want to learn something” when they really really don’t care in the slightest - it just makes people who actually want to learn get taken unseriously.",
          "score": 1,
          "created_utc": "2026-01-17 02:16:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzin0qd",
          "author": "Bozo32",
          "text": "I used a combination of cosine similarity and bm25 to filter the obviously irrelevant -> reranker -> NLI and then presented results to the user...have them say yes or no. This could iterate where you use the 'no' answers as a filter to rerank results  \nover time you would accumulate an evidence base of   \n'query' 'rejected resources' 'chosen resource'  \nthat would be useful for future searches  \nthe work done by folks at Utrecht university on screening abstracts for systematic review may be helpful  \n[https://asreview.nl/install/](https://asreview.nl/install/)",
          "score": -1,
          "created_utc": "2026-01-14 10:40:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1q9x0gk",
      "title": "RAG without a Python pipeline: A Go-embeddable Vector+Graph database with an internal RAG pipeline",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1q9x0gk/rag_without_a_python_pipeline_a_goembeddable/",
      "author": "sd_cips",
      "created_utc": "2026-01-11 11:22:51",
      "score": 12,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "Hi everyone,\n\n(English is not my first language, so please excuse any errors).\n\nFor the past few months, I've been working on **KektorDB**, an in-memory, embeddable vector database.\n\nInitially, it was just a storage engine. However, I wanted to run RAG locally on my documents, but I admit I'm lazy and I didn't love the idea of manually managing the whole pipeline with Python/LangChain just to chat with a few docs. So, I decided to move the retrieval logic directly inside the database binary.\n\n**How it works**\n\nIt acts as an OpenAI-compatible **middleware between your client** (like Open WebUI) **and your LLM** (Ollama/LocalAI). You configure it via two YAML files:\n\n*  **vectorizers.yaml:** Defines folders to watch. It handles ingestion, chunking, and uses a local LLM to extract entities and link documents (Graph).\n* **proxy.yaml:** Defines the inference pipeline settings (models for rewriting, generation, and search thresholds).\n\n\n\n**The Retrieval Logic (v0.4)**\n\nI implemented a specific pipeline and I’d love your feedback on it:\n\n* **CQR (Contextual Query Rewriting):** It intercepts chat messages and rewrites the last query based on history to fix missing context.\n* **Grounded HyDe:** Instead of standard HyDe (which can hallucinate), it performs a preliminary lookup to find real context snippets, generates a hypothetical answer based on that context, and finally embeds that answer for the search.\n* **Hybrid Search (Vector + BM25):** The final search combines dense vector similarity with sparse keyword matching (BM25) to ensure specific terms aren't lost.\n* **Graph Traversal:** It fetches the context window by traversing prev/next chunks and mentions links (entities) found during ingestion.\n\n**Note:** All pipeline steps are configurable via YAML, so you can toggle HyDe/Hybrid search and other on or off.\n\n\n\n**My questions for you**\n\nSince you folks build RAG pipelines daily:\n\nIs this \"Grounded HyDe + Hybrid\" approach robust enough for general purpose use cases?\n\nDo you find Entity Linking (Graph) actually useful for reducing hallucinations in local setups compared to standard window retrieval?\n\nShould I make more use of graph capabilities during ingestion and retrieval?Should I make more use of graph capabilities during ingestion and retrieval?\n\n**Disclaimer:** The goal isn't to replace manual pipelines for complex enterprise needs. **The goal is to provide a solid baseline for generic situations where you want RAG quickly without spinning up complex infrastructure.**\n\n\n\n**Current Limitations (That I'm aware of):**\n\n* **PDF Parsing:** It handles images via Vision models decently, but table interpretation needs improvement.\n* **Splitting:** Currently uses basic strategies; I need to dive deeper into semantic chunking.\n* **Storage:** It is currently RAM-bound. A hybrid disk-storage engine is already on the roadmap for v0.5.0.\n\n\n\nThe project compiles to a single binary and supports OpenAI/Ollama \"out of the box\".\n\nRepo: [https://github.com/sanonone/kektordb](https://github.com/sanonone/kektordb)\n\nGuide: [https://github.com/sanonone/kektordb/blob/main/docs/guides/zero\\_code\\_rag.md](https://github.com/sanonone/kektordb/blob/main/docs/guides/zero_code_rag.md)\n\nAny feedback or roasting is appreciated!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1q9x0gk/rag_without_a_python_pipeline_a_goembeddable/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz2eblc",
          "author": "OnyxProyectoUno",
          "text": "Your retrieval pipeline looks solid, but I'd focus on those parsing limitations you mentioned. PDF table interpretation and basic chunking strategies will bite you more than retrieval tweaks. When tables get mangled or chunks lose semantic boundaries, even perfect retrieval can't save you. I've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) because I kept hitting these upstream issues - needed to see what documents actually looked like after parsing before they hit the vector store.\n\nEntity linking can be useful, but it depends on your documents. If you're dealing with contracts or technical docs where entities reference each other across sections, graph traversal helps. For general knowledge bases, the complexity might not be worth it. The prev/next chunk window you're doing is probably more reliable than entity links for most use cases.\n\nFor semantic chunking, look at Chonkie or the newer approaches in Docling. Basic sentence splitting loses too much context, especially with structured documents. Your YAML config approach is smart though - makes it easy to experiment with different strategies without touching code.\n\nThe single binary deployment is appealing for local setups. How are you handling memory usage with larger document sets? RAM-bound works for prototyping but becomes a constraint quickly. Your hybrid storage plan for v0.5 should help there.\n\nWhat's your chunking strategy looking like right now? Fixed size, sentence boundaries, or something else?",
          "score": 3,
          "created_utc": "2026-01-11 23:44:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "nz4wi37",
              "author": "sd_cips",
              "text": "Thanks for the great feedback and the tool suggestions! You are absolutely right, if the parsing/chunking is bad, the retrieval doesn't matter.\n\nTo answer your question on chunking: Right now, I'm primarily using a Recursive Character Splitter with configurable overlap via the YAML config that tries to respect document structure (splitting on paragraphs \\\\n\\\\n, then sentences \\\\n, etc.) to preserve context. I also have specialized splitters for Markdown and Code to handle those formats more intelligently. It's basic, but since the pipeline is modular I plan to improve it later.\n\nMemory: Currently, it relies on Int8 quantization (reducing footprint by \\~75%) and Float16 compression to fit larger datasets in RAM. It works well for local/mid-sized workloads, but the hybrid storage (planned for v0.5) will be the real fix for scaling beyond RAM limits.",
              "score": 1,
              "created_utc": "2026-01-12 09:39:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nz77jzg",
                  "author": "OnyxProyectoUno",
                  "text": "The recursive character splitter approach makes sense for getting started. The structure-aware splitting you're doing (paragraphs then sentences) is better than naive fixed-size chunks, but you'll still hit issues with things like lists, tables, or code blocks that span your chunk boundaries. When you do upgrade the chunking, test it against some messy real-world docs first. Clean markdown examples always work great, but PDFs with mixed formatting will show you where it breaks.\n\nInt8 quantization at 75% reduction is pretty good for the memory constraint. Are you quantizing just the embeddings or the whole pipeline? If you're running the embedding model in the same process, that's probably your bigger memory hog. The hybrid storage approach will help, but you might want to consider lazy loading embeddings from disk even before v0.5 if memory becomes a blocker for testing with larger datasets.",
                  "score": 2,
                  "created_utc": "2026-01-12 17:52:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qc5qua",
      "title": "Is RAG the right approach for exhaustive searches over a corpus of complex documents?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qc5qua/is_rag_the_right_approach_for_exhaustive_searches/",
      "author": "cleinias",
      "created_utc": "2026-01-13 22:35:27",
      "score": 11,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "Disclaimer: I am completely new to RAG systems and I am trying to determine whether they are the right approach to my use cases.  I just spent the last few hours reading various material and watching videos on the subject, but still can't  figure out the answer.\n\nConsider this use case (more of a toy problem than a real use case, but close enough in spirit):\n\nYou have a collection of cookbooks, each one being a PDF file several hundred pages long. Let's say you have a few hundreds of them. That is your knowledge base\n\nYou want to be able to query *exclusively* and *exhaustively* this knowledge base with question that may be as simple as:\n\n\"List *all* the recipes using kale in the knowledge base providing the source title, author, and page number.\"\n\nto more complex one such as, for instance, \n\n\"Provide a list of *all* recipes suitable as a main course that include a green vegetable similar to kale as one of the main ingredients, providing the source title, author, and page number.\"\n\n  \nIn short: I have a corpus of documents that are semantically fairly homogeneous and therefore all more or less relevant to the possible queries and I need to the answers to be exhaustive.\n\nThe resources I have read and watched, on the other hand, seem to focus on a different set of use cases, where they are confronted with a vast collection of potentially heterogeneous documents (e.g., all the internal policy documents of a large company) and are keen to extract the very few items relevant to the query at hand in order to integrate the LLM processing step.\n\nWelcoming all suggestions!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qc5qua/is_rag_the_right_approach_for_exhaustive_searches/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzge88w",
          "author": "OnyxProyectoUno",
          "text": "Your instinct that standard RAG might not fit here is worth taking seriously. The typical RAG setup optimizes for finding the most relevant handful of chunks, not exhaustive recall across a corpus. When you need every recipe with kale, similarity search with a top-k cutoff is fundamentally the wrong tool.\n\nWhat you're describing sounds more like a structured extraction problem. You'd want to parse each cookbook, extract recipe entities with their ingredients, metadata, and page references, then store that in something queryable like a database or search index. The LLM piece comes in for the fuzzy matching (\"green vegetable similar to kale\") but the exhaustive listing part needs deterministic retrieval.\n\nThe preprocessing step is where this gets tricky. PDF cookbooks are a nightmare because recipes span pages, ingredient lists get mangled, and section headers don't always parse cleanly. I work on document processing tooling at vectorflow.dev and this exact pattern comes up a lot. You need to see what your parser is actually extracting before you can trust any downstream queries.\n\nFor your use case, I'd explore a hybrid approach: structured extraction for the exhaustive search, semantic matching for the similarity queries. What format are these PDFs in? Scanned images or native text?",
          "score": 6,
          "created_utc": "2026-01-14 00:46:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzgo3yx",
              "author": "ai_hedge_fund",
              "text": "Agree 100%\n\nI see this as going into some structured database first, possibly NoSQL, and then maybe using an LLM-as-a-Judge or Classifier to decide how well the search results meet the goal of the query, filtering, etc. \n\nDepending on the real details of the use case the end user may or may not want further generation using those chunks. \n\nSo, yeah, vector search and generation may not be the ultimate configuration.",
              "score": 2,
              "created_utc": "2026-01-14 01:42:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzh0j52",
                  "author": "sqm_prout",
                  "text": "Totally agree, structured databases can really help here. Using something like a NoSQL setup to store the extracted data will make those complex queries way easier. Plus, integrating an LLM to refine the results can add that extra layer of intelligence without losing the exhaustiveness you’re after. Just make sure your extraction process is solid to handle those tricky PDF formats!",
                  "score": 2,
                  "created_utc": "2026-01-14 02:52:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzfs4k9",
          "author": "Far_Statistician1479",
          "text": "I’m not 100% sold that semantic search is really an improvement over tokenized search, but if you want ai refined search, really the only way to do that is to give an LLM a few search tools. And semantic search could be the underlying search mechanism if it works well for you.",
          "score": 2,
          "created_utc": "2026-01-13 22:48:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzg5tdl",
              "author": "Infamous_Ad5702",
              "text": "I love semantic search. Accurate. Low cost. No gpu for me. I can’t do hallucinations for my client…must be offline also.",
              "score": 2,
              "created_utc": "2026-01-14 00:00:43",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzg6ztr",
                  "author": "Far_Statistician1479",
                  "text": "All of these are features of traditional tokenized search as well?",
                  "score": 2,
                  "created_utc": "2026-01-14 00:07:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzglc55",
          "author": "jordan_yeo",
          "text": "I’ve taken an approach for exhaustive search where we first grab all unique document ids that contain a matching chunk, then for each of those docs, do the hybrid search, run through reranker. You can then take all the cross document results and rerank again. It’s expensive, and not instant, but has done really well ensuring we get comprehensive results across the corpus",
          "score": 2,
          "created_utc": "2026-01-14 01:26:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzgq470",
          "author": "BusinessMindedAI",
          "text": "RAG is built for top-K relevance, not exhaustive discovery, so it will miss recipes in your case.\nYou need to extract every recipe into a structured index (ingredients, pages, categories) using LLMs once.\nThen run database or graph queries for 100% recall, using LLMs only to interpret and explain results.",
          "score": 2,
          "created_utc": "2026-01-14 01:53:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzi92ry",
          "author": "GP_103",
          "text": "Hybrid Search - Graph + BM25 with lightweight intent classifier.\n\nMake no mistake though, you’ll have to pre-process those docs in a schema that adds some hierarchy to the content blocks. And there’s work to get BM25 weighted right.",
          "score": 1,
          "created_utc": "2026-01-14 08:26:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkxa1r",
          "author": "Curious-Sample6113",
          "text": "Only way to find out is to start",
          "score": 1,
          "created_utc": "2026-01-14 18:12:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o026vpr",
          "author": "blue-or-brown-keys",
          "text": "This problem requires aggregations across an entire corpus—potentially hundreds of pages. An LLM can only answer accurately if it effectively “sees” the whole corpus, which exceeds the context window.\n\nTo solve this, I’d precompute structured aggregations ahead of time and enrich documents with extracted tags or attributes. You need an extraction + aggregation layer on top of the raw documents; otherwise, the data simply won’t fit into context.\n\nBasic metadata like title or author is easy—retrieval already handles citations. But queries like “find all recipes that contain a given ingredient” are hard unless those ingredients have been explicitly extracted and aggregated in advance.",
          "score": 1,
          "created_utc": "2026-01-17 05:49:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qblw1k",
      "title": "Best knowledge graph graph view?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qblw1k/best_knowledge_graph_graph_view/",
      "author": "PutridPut7225",
      "created_utc": "2026-01-13 08:19:05",
      "score": 11,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "What is the most advanced graph view out there currently I do find them all pretty limited especially for very high node count. But I also don't know a lot of knowledge graph software. So maybe you guys know something I don't ",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qblw1k/best_knowledge_graph_graph_view/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzbmslo",
          "author": "Rokpiy",
          "text": "neo4j bloom handles large graphs better than most but starts choking around 50k+ nodes. for really high counts, check out graphistry or yworks yed - they use gpu rendering which makes a difference. \n\ngephi is free and handles 100k+ nodes but the ui is dated. if you're dealing with millions of nodes, you'll need something like tigergraph's graph studio or memgraph lab which are built for scale.\n\ndepends on your use case though. what node count are you working with?",
          "score": 2,
          "created_utc": "2026-01-13 08:58:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbw757",
              "author": "PutridPut7225",
              "text": "10 k is enough for me. But I want the visuality as big as possible. So my problem is less how well it can handle many nodes performance speaking wise, but how fast can I find a specific node. So it's more about space management and so on. Anyways thanks for your suggested tools. Appreciate it",
              "score": 1,
              "created_utc": "2026-01-13 10:28:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzbnp6d",
          "author": "Striking-Bluejay6155",
          "text": "Consider looking at [FalkorDB's browser](https://browser.falkordb.com/). You can also look at g.v() if you've got an underlying graph database. How many nodes are we talking?",
          "score": 2,
          "created_utc": "2026-01-13 09:07:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzbwci9",
              "author": "PutridPut7225",
              "text": "Thanks will look into it. Normally not more than 10k nodes. However I need to find the node I am looking for the fastest way possible in the graph",
              "score": 2,
              "created_utc": "2026-01-13 10:30:06",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzclk5d",
                  "author": "Striking-Bluejay6155",
                  "text": "You're welcome. Has the issue so far been writing the cypher query? What I shared comes with built-in filters so you could probably narrow down 10k to a few. Plus you can control the sizes of the nodes according to your needs, so the ones you frequently look at can be larger and different color",
                  "score": 2,
                  "created_utc": "2026-01-13 13:36:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdybh4",
          "author": "Whole-Assignment6240",
          "text": "all kg comes with a browser. comes down to which kg you pick . neo4j is pretty decent, i used in many projects",
          "score": 1,
          "created_utc": "2026-01-13 17:44:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzg10yt",
          "author": "TrustGraph",
          "text": "I've never found graph viewers useful from a data analysis perspective. I've had some folks from Neo4j tell me it's about 50/50, people that love graph viewers vs. people that never use them. I happen to be a never uses them.\n\nIf you want pretty, there's [Graphistry](https://www.graphistry.com/). I can't argue with it's aesthetics. Most all GraphDB systems have graph viewers. In fact, Neo4j's graph viewer is what really gained them their fame.\n\nTrustGraph uses [3D Force Graph](https://github.com/vasturiano/3d-force-graph) to show 3D graphs that could be stored in Cassandra, Neo4j, Memgraph, or FalkorDB. It has a ton of customization available in it.\n\nIf you'd like the ability to build context graphs and view them in 3D in a single platform with zero coding, TrustGraph is free and open source:\n\n[https://github.com/trustgraph-ai/trustgraph](https://github.com/trustgraph-ai/trustgraph)",
          "score": 1,
          "created_utc": "2026-01-13 23:35:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzkmavb",
          "author": "coderarun",
          "text": "Store it in r/DuckDB and query it via r/LadybugDB\n\n[https://adsharma.github.io/explainable-ai/](https://adsharma.github.io/explainable-ai/)\n\nVisualization: you can probably ask your favorite terminal based coding agent to spit out one. Just need a mcp-server to talk to the data source.\n\nI've tested wikidata (90 million nodes) and am currently testing something 3x its size.",
          "score": 1,
          "created_utc": "2026-01-14 17:23:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzkmrpo",
              "author": "coderarun",
              "text": "Also see: [https://adsharma.github.io/duckdb-wikidata-compression/](https://adsharma.github.io/duckdb-wikidata-compression/)",
              "score": 1,
              "created_utc": "2026-01-14 17:25:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzv6yjs",
          "author": "One_Milk_7025",
          "text": "i am using basic postgres and writing things from scratch.. and i am getting quite good result till now .. almost 30k nodes now.. its stable with react-force-graph-2d",
          "score": 1,
          "created_utc": "2026-01-16 04:50:56",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qav580",
      "title": "Project ideas!!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qav580/project_ideas/",
      "author": "rayanskrrr",
      "created_utc": "2026-01-12 13:40:06",
      "score": 11,
      "num_comments": 13,
      "upvote_ratio": 0.92,
      "text": "Can anyone recommend some begginer friendly rag project idea to someone who's new to generative ai something which is unique and not npc which would standout while being begginer friendly as well",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qav580/project_ideas/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nz8l391",
          "author": "Stock-Cucumber6406",
          "text": "I would start with the context engineers discord server. Infinite resources :)\n\nhttps://discord.gg/EDwCHfPn9",
          "score": 4,
          "created_utc": "2026-01-12 21:40:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz5vt1f",
          "author": "anashel",
          "text": "I think a quiz game is a good test, as it show your rag capacity to retrieve knwoeldge correctly for context to make the question and with precision for calidating the answer…\n\nLike, instead of building yet another “chat with PDF”, you build a little game that forces grounding:\n- You ingest a single book (public domain novel, short story collection, or even a cookbook).\n- Your system generates questions only from retrieved passages (characters, places, plot events, relationships, timelines).\n-Then when the player answers, the system validates using retrieval again and shows citations from the exact chunks that justify the correct answer.\n- Bonus: show a side panel that compares “LLM with RAG” vs “LLM without RAG” so people instantly see hallucinations disappear.\n\nConcrete book example:\n- Pick something fun like Sherlock Holmes, Dracula, or Alice in Wonderland\n-Mode 1: “Book review mode” where the LLM writes a review, but every claim must be backed by a cited snippet (themes, character arcs, key scenes).\n- Mode 2: “Quiz mode” where it asks stuff like “Who said this line?”, “Where were they when X happened?”, “What happened right before Y?”, “How does character A relate to B?”…\n\nFor the dev, i suggest cloudflare.\n-simple worker typescript, very simple and one line to deploy (wrangler)\n- easy to upload your doc (r2 bucket)\n- you can build your own rag indexing (supabase postgres with pgvector) and use cloudflare hyperdrive (basically one line to integrate in your worker)\n- or you can use their RAG indexer to index your r2 bucket and chunk it automatically\n- ai gateway gives you nice visibility (logs and traceability) of all llm interactions\n\nYou can spin it in a nice simple react app to play with it and have a side by side answer; with and without rag to see the actual quality",
          "score": 4,
          "created_utc": "2026-01-12 14:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzfxvoc",
              "author": "CompellingBytes",
              "text": "I was actually thinking of doing this in Jeopardy format.",
              "score": 2,
              "created_utc": "2026-01-13 23:18:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nzggech",
                  "author": "anashel",
                  "text": "Thats a great mvp! Let me know if you do it, i’ll love to see that!",
                  "score": 1,
                  "created_utc": "2026-01-14 00:58:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nz93a82",
              "author": "FormalAd7367",
              "text": "Notebooklm?",
              "score": 1,
              "created_utc": "2026-01-12 23:08:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nz65i0i",
          "author": "bsenftner",
          "text": "Add which nobody seems to add to RAG, which is business-wise really critically important: create a library for the tracking of the expense of RAG, of GraphRAG or really any RAG-like solution. Which is rather dynamic, because the expense of use of a RAG system depends upon the ingestion cost, the frequency of re-ingestion due to ingested document changes, and then the questions against them, any post-question re-indexing for long conversation support and all these expenses finally summed. In some environments with frequently changing large documents, RAG becomes questionable from an expense standpoint. This type of tracking and financial accounting is sorely lacking in software today, and that probably needs to change. Considering all the paid-use APIs there are now, such needs are inevitable.",
          "score": 1,
          "created_utc": "2026-01-12 14:54:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6hcv1",
          "author": "Valeria_Xenakis",
          "text": "Build a RAG system that compares two software versions and explains actually breaking changes on upgrade.\n\nInstead of summarizing changelogs, it retrieves real-world breaking changes from release notes, migration guides, and “this update broke my app” posts.\n\nIt filters changes based on how the user uses the software and classifies them by impact.\nThe focus is answering “do I care and what should I fix?” rather than just “what changed.”",
          "score": 1,
          "created_utc": "2026-01-12 15:52:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz6po7s",
          "author": "AsparagusKlutzy1817",
          "text": "Do you want an AI idea or an agent idea ? RAG is a tool - like a hammer - what is the nail you want to hit ?",
          "score": 1,
          "created_utc": "2026-01-12 16:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8o3l8",
          "author": "yogonflame",
          "text": "start with zeroentropy stack and their discord server.",
          "score": 1,
          "created_utc": "2026-01-12 21:53:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nz8yg4s",
          "author": "Strong_Worker4090",
          "text": "What are your top 3 hobbies? I'd like to give you some that you might actually find interesting.",
          "score": 1,
          "created_utc": "2026-01-12 22:43:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzjdg36",
              "author": "rayanskrrr",
              "text": "Arghh watching shows animes playing football and arghh doom scrolling",
              "score": 1,
              "created_utc": "2026-01-14 13:47:04",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzkt34i",
                  "author": "Strong_Worker4090",
                  "text": "Ok cool cool, those hobbies are perfect for a couple beginner RAG projects that don’t feel NPC:\n\n**1) Anime recs RAG (actually useful)**  \nBuild a small DB from episode summaries + reviews + Reddit threads. Ask stuff like:\n\n* “I liked AOT for politics + pacing, what’s similar?” Make it stand out by saving user prefs and citing sources in every answer.\n\n**2) Football injury + lineup tracker RAG**  \nScrape a few reliable injury/news sources on a schedule. Ask:\n\n* “Is \\_\\_\\_ expected to start this weekend?” Standout feature: time-based answers (“latest update as of…”) + citations.\n\n**3) Doomscroll-to-summary RAG**  \nYou drop links/posts, it ingests and answers:\n\n* “What are the 3 main takes across these threads?” Standout feature: consensus vs disagreement + citations.",
                  "score": 1,
                  "created_utc": "2026-01-14 17:53:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzdy3ot",
          "author": "Whole-Assignment6240",
          "text": "shared 20 example project here - [https://cocoindex.io/examples/](https://cocoindex.io/examples/) \\- lmk if it is helpful! (i'm the maintainer of the framework)",
          "score": 1,
          "created_utc": "2026-01-13 17:43:51",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbgeap",
      "title": "OSS Alternative to Glean",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbgeap/oss_alternative_to_glean/",
      "author": "Uiqueblhats",
      "created_utc": "2026-01-13 03:21:44",
      "score": 11,
      "num_comments": 3,
      "upvote_ratio": 0.92,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, Connect any LLM to your internal knowledge sources (Search Engines, Drive, Calendar, Notion and 15+ other connectors) and chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams)\n* Supports 100+ LLMs\n* Supports local Ollama or vLLM setups\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Multi Collaborative Chats\n* Multi Collaborative Documents\n* Real Time Features\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qbgeap/oss_alternative_to_glean/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzaftxf",
          "author": "Oshden",
          "text": "Hey OP, how would one get involved in testing out the system and giving you some feedback? This looks perfect for something I’m trying to implement",
          "score": 1,
          "created_utc": "2026-01-13 03:30:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzai3cy",
              "author": "Uiqueblhats",
              "text": "Hey, we are actively looking for feedback. The easiest way to test the product is to try our cloud version. If you find any bugs, please create an issue at [https://github.com/MODSetter/SurfSense/issues](https://github.com/MODSetter/SurfSense/issues).",
              "score": 1,
              "created_utc": "2026-01-13 03:42:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzd046k",
                  "author": "Oshden",
                  "text": "This looks awesome!! I’ll have to do a bit of research on how this could work but I’m excited to try it. Maybe even run it locally! Do you guys have a way to set up custom agent instructions to further refine the desired behavior for the chat agent?",
                  "score": 1,
                  "created_utc": "2026-01-13 14:53:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qcgoo8",
      "title": "need help embedding 250M vectors / chunks at 1024 dims, should I self host embedder (BGE-M3) and self host Qdrant OR use voyage-3.5 or 4?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "author": "zriyansh",
      "created_utc": "2026-01-14 07:06:24",
      "score": 10,
      "num_comments": 18,
      "upvote_ratio": 0.92,
      "text": "hey redditors, I am building a legal research RAG tool for law firms, just research and nothing else.\n\n  \nI have around 1.5TB of legal precedence data, parsed them all using 64 core Azure VM, using PyMuPDF + Layout + Pro. Using custom scripts and getting around 30 - 150 files / second parse speed. \n\nVoyage-3-large surpassed voyage-law-2 and now gemini 001 embedder is ranked #2 (MTEB ranking).  Domain specific models are now overthrown by general embedders. \n\nI have around 250 million vectors to embed, and even using voyage-3.5 (0.06$/mill token), the cost is around $3k dollars. \n\n  \nUsing Qdrant cloud will be another $500.\n\n  \nQuestion I need help with:\n\n1. Should I self host embedder and vectorDB? (for chunking as well retrival later on)  \n2. Bear one time cost of it and be hastle free? \n\n\n\nFeel free to DM me for the parsing and chunking and embedding scripts. Using BM25 + RRF + Hybrid search + Rerank using voyage-rank2.5, CRAG + Web Search. \n\n  \nCurrent latency woth 2048 dims on test dataset of 400k legal text vectors is 5 seconds. \n\nChunking by characters and not token.\n\n|Metric|Value|\n|:-|:-|\n|**Avg parsed file size**|68.5 KB|\n|**Sample text length**|2,521 chars (small doc)|\n|**Total PDFs**|16,428,832|\n|**Chunk size**|4,096 chars (\\~1,024 tokens)|\n|**Chunk overlap**|512 chars (\\~128 tokens)|\n|**Min chunk size**|256 chars|\n\n  \n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcgoo8/need_help_embedding_250m_vectors_chunks_at_1024/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzi2930",
          "author": "aiprod",
          "text": "Do you have an eval set that would allow you to test recall etc. on a subset of the corpus? Should help with selecting the right model. I’d also look into getting those 2048 dims down. Will save you a lot on vector db costs and reduces latency. Five seconds seems very slow. How did you test that? Was it pure embedding retrieval or your full retrieval pipeline?",
          "score": 5,
          "created_utc": "2026-01-14 07:22:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2p88",
              "author": "zriyansh",
              "text": "I dont have an eval set just yet, working on that. This is qdrant telling me about the latency. Okay it improved from yesterday lol. \n\nhttps://preview.redd.it/7ki00keho9dg1.png?width=2378&format=png&auto=webp&s=83bb39f165ed1da3c98c4b0af0d3d162ea4b5706",
              "score": 1,
              "created_utc": "2026-01-14 07:26:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzi4ay7",
                  "author": "aiprod",
                  "text": "That looks better although some of the queries are still a bit slow. How are you running the qdrant cluster? Is that through their cloud offering?",
                  "score": 1,
                  "created_utc": "2026-01-14 07:41:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzi2gda",
          "author": "bravelogitex",
          "text": "able to embed across a large number of gpu instances? havent heard of that done before",
          "score": 1,
          "created_utc": "2026-01-14 07:24:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi2r62",
              "author": "zriyansh",
              "text": "CPU\\*",
              "score": 1,
              "created_utc": "2026-01-14 07:26:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nzi3k45",
          "author": "ai_hedge_fund",
          "text": "This is interesting because you use all different units than I usually think in terms of\n\nWhen you say self host, what GPUs do you have or are you backing into a budget?\n\nThen it becomes a question of turnaround time\n\nIf the time isn’t an issue then I think you can do better than the $3K\n\nIf you want it done fast then $3K to $4K sounds about right if you rent GPUs / have quota in Azure\n\nTo me the cloud DB question depends on how users will access the data and when. I’d think you could defer that $500 now if it’s an issue and just store the vectors yourself.",
          "score": 1,
          "created_utc": "2026-01-14 07:34:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzi8rqo",
              "author": "zriyansh",
              "text": "expecting around 50 users in a month, and 10 queries per user each day.\n\nyeah not using token because character is what I understand well, so it works for me. \n\nI have a budget for $1K for now as we dont have any customers, using my savings for this.\n\nAs far as I understanding, embedding and hosting a vector DB is CPU intensive not GPU (can be wrong here), I have 1k$ credit from Azure as I registered my startup with them (and linked my LinkedIn with them as well).   \n  \nIf we break even, I will want to use cloud services and focus on what we do best.",
              "score": 1,
              "created_utc": "2026-01-14 08:23:18",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "nziamtn",
          "author": "mwon",
          "text": "I have also a side project in the field of legal AI, but with considerable lower size. My biggest index has about 11M vectors, size 1024, generated form a fined tuned BGE-M3.   \nI use Milvus with index in disk in  a dedicated server fom Hetzner, and my latency is bellow 0.5s.  \nI think is a bit odd your latency is 5s for only 400k vectors. You should check if everything is ok, because is too much. I also think chunks of 1024 is too much. You will l likely loose a lot of recall.",
          "score": 1,
          "created_utc": "2026-01-14 08:41:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzib92e",
              "author": "zriyansh",
              "text": "so its self hosted embedder I suppose, what kind of machine are you using? and anything I need to take care of here?",
              "score": 1,
              "created_utc": "2026-01-14 08:47:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "nzic6mi",
                  "author": "mwon",
                  "text": "Yes, self embedder. I never use embedding services. Very expensive for what they do and no better than many OS solutions that you can instantiate locally.   \n  \nI'm using one of theirs 64GB ram dedicated servers. They are very cheap, like 40-50 EUR/month. \n\nYou need to be careful with your benchmarks estimation. A sample of 400k vector is very small compared with your final production setup. Recall values will be very diferent with 250M vectors.",
                  "score": 1,
                  "created_utc": "2026-01-14 08:56:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "nziqy8d",
              "author": "explodedgiraffe",
              "text": "I am also working on a side project of that size. I was thinking of using qwen 4b embedding and rerank. How was your experience with BGE-M3? I like the architecture of it (sparse/dense/multi vector) but their benchmarks weren't that impressive compared to dense of the same size. Also curious how you fined tuned it for your usecase.\n\n  \n5s must be caused by network latency from external rerank and web search calls?",
              "score": 1,
              "created_utc": "2026-01-14 11:14:09",
              "is_submitter": false,
              "replies": [
                {
                  "id": "nziseix",
                  "author": "mwon",
                  "text": "qwen-4b vs BGE-M3 both out of the box, qwen likely wins. After all it is a 4B model against vs 0.5B. But you can give a good boost to BGE-M3 by finetuning it, which will give you a small model that does not need GPU for inference. The sparse part is also nice because it allows you to do hybrid with a sparse search in one go. Note however that from my experiences with BGE-M3, BM25 is still better than its sparse.",
                  "score": 1,
                  "created_utc": "2026-01-14 11:26:14",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "nzicnrv",
          "author": "UseMoreBandwith",
          "text": "how long does it take to process 1.5TB ?",
          "score": 1,
          "created_utc": "2026-01-14 09:01:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "nzicuih",
              "author": "zriyansh",
              "text": "around 3 days with 64 core CPU, but there exist faster parsers which can parse 4-5k documents per second with such beast machine but I wasn't able to run that properly, its a C implementation of pymupdf4llm-c",
              "score": 2,
              "created_utc": "2026-01-14 09:03:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qct663",
      "title": "Designing inverted indexes in a KV-store on object storage",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qct663/designing_inverted_indexes_in_a_kvstore_on_object/",
      "author": "itty-bitty-birdy-tb",
      "created_utc": "2026-01-14 17:17:49",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "my colleague morgan has been working on redesigning turbopuffer's inverted index structure for full-text search and attribute filtering, and he wrote about it: [https://turbopuffer.com/blog/fts-v2-postings](https://turbopuffer.com/blog/fts-v2-postings)   \n  \nThe main takeaways are that the index structure is designed using fixed-sized posting blocks (as opposed to our prior approach which set posting list partition boundaries at existing vector cluster boundaries) which minimizes KV overhead and improves compression to reduce the physical size of the index by up to 10x. combined with our [vectorized MAXSCORE algorithm](https://turbopuffer.com/blog/fts-v2-maxscore) this has sped up some full-text search queries by up to 20x.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qct663/designing_inverted_indexes_in_a_kvstore_on_object/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzkv3f2",
          "author": "Necessary-Dot-8101",
          "text": "compression-aware intelligence is a fundamentally different design layer than prompting or RAG and meta only just started using it over the past few days. \n\nsuper useful for this bc it treats hallucinations, identity drift, and reasoning collapse not as output errors but as structural consequences of compression strain within intermediate representations. it provides instrumentation to detect where representations are conflicting and routing strategies that stabilize reasoning rather than patch outputs",
          "score": 1,
          "created_utc": "2026-01-14 18:02:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qbn7fc",
      "title": "A good way to reduce cost of your RAG system",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qbn7fc/a_good_way_to_reduce_cost_of_your_rag_system/",
      "author": "ProfessionalLaugh354",
      "created_utc": "2026-01-13 09:43:42",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "I've been working on RAG systems and kept running into the same frustrating pattern: I'd retrieve 10 documents per query, each a few thousand tokens long, but only a handful of sentences actually answered the question. The LLM would get distracted by all the noise, and my token costs were spiraling.\n\nI tried a few existing context pruning models, but they either only had tiny context windows (512 tokens), or weren't commercially usable. Nothing fit what I needed.\n\nSo I trained my own model to do semantic highlighting - basically, it scans through your retrieved context and identifies which sentences are actually relevant to the query. It's a small encoder-only model (0.6B params) that's fast to run and supports both English and Chinese.\n\nHere's how it works in practice:\n\n    from transformers import AutoModel\n    \n    model = AutoModel.from_pretrained(\n        \"zilliz/semantic-highlight-bilingual-v1\",\n        trust_remote_code=True\n    )\n    \n    question = \"What are the symptoms of dehydration?\"\n    context = \"\"\"\n    Dehydration occurs when your body loses more fluid than you take in.\n    Common signs include feeling thirsty and having a dry mouth.\n    The human body is composed of about 60% water.\n    Dark yellow urine and infrequent urination are warning signs.\n    Water is essential for many bodily functions.\n    Dizziness, fatigue, and headaches can indicate severe dehydration.\n    Drinking 8 glasses of water daily is often recommended.\n    \"\"\"\n    \n    result = model.process(\n        question=question,\n        context=context,\n        threshold=0.5,\n        # language=\"en\",  # Language can be auto-detected, or explicitly specified\n        return_sentence_metrics=True,  # Enable sentence probabilities\n    )\n    \n    highlighted = result[\"highlighted_sentences\"]\n    print(f\"Highlighted {len(highlighted)} sentences:\")\n    for i, sent in enumerate(highlighted, 1):\n        print(f\"  {i}. {sent}\")\n    print(f\"\\nTotal sentences in context: {len(context.strip().split('.')) - 1}\")\n    \n    # Print sentence probabilities if available\n    if \"sentence_probabilities\" in result:\n        probs = result[\"sentence_probabilities\"]\n        print(f\"\\nSentence probabilities: {probs}\")\n\nOutput:\n\n    Highlighted 3 sentences:\n      1. Common signs include feeling thirsty and having a dry mouth.\n      2. Dark yellow urine and infrequent urination are warning signs.\n      3. Dizziness, fatigue, and headaches can indicate severe dehydration.\n    \n    Total sentences in context: 7\n    \n    Sentence probabilities: [0.017, 0.990, 0.002, 0.947, 0.001, 0.972, 0.001]\n\nOut of 7 sentences, it correctly picked the 3 that actually answer the question. The token reduction is huge - I'm seeing 70-80% savings in production use cases.\n\nThe model is based on the Provence architecture (encoder-only, token-level scoring) and trained on 5M+ bilingual samples. I used BGE-M3 Reranker v2 as the base model since it already handles long contexts (8192 tokens) and supports multiple languages well.\n\nReleased everything under MIT license if anyone wants to try it out.\n\nCurious if others have been tackling similar problems with RAG context management. What approaches have worked for you?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qbn7fc/a_good_way_to_reduce_cost_of_your_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzcqz6s",
          "author": "shitmoji",
          "text": "nice. I'll try it on an Arabic corpus.",
          "score": 2,
          "created_utc": "2026-01-13 14:05:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzd7sbh",
          "author": "mrnoirblack",
          "text": "Download where pls",
          "score": 1,
          "created_utc": "2026-01-13 15:30:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzdjx4g",
          "author": "IdeaAffectionate945",
          "text": "Interesting idea. I've built something completely different, that kind of solves the same problem, since it allows you much more control over your RAG data - Especially during scraping. You can check out the general idea here ==> [https://ainiro.io/natural-language-api](https://ainiro.io/natural-language-api)",
          "score": 1,
          "created_utc": "2026-01-13 16:26:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nze33jz",
          "author": "OnyxProyectoUno",
          "text": "Yeah, that's the usual story with RAG systems. You pull back way more context than you need and the LLM gets lost in the noise. Smart move training your own model for this.\n\nThe semantic highlighting approach is solid. I've seen similar patterns where people try to solve this with better retrieval (hybrid search, reranking) but miss that the real issue is context bloat after retrieval. Your model catches the stuff that made it through retrieval but shouldn't have.\n\nFew things to watch out for with this approach. First, the threshold tuning can be tricky. 0.5 might work great for some domains but be way off for others. I'd test across different question types because technical queries vs conversational ones can have very different score distributions.\n\nAlso, sentence-level splitting can miss context that spans multiple sentences. Like if you have \"The company reported losses. This was due to supply chain issues.\" Your model might only highlight the second sentence but lose the connection to what \"this\" refers to. Might be worth experimenting with paragraph-level or sliding window approaches for certain document types.\n\nThe bilingual support is nice though. Most context pruning solutions are English-only which is a pain if you're dealing with mixed-language documents.\n\nWhat kind of domains are you testing this on? Technical docs vs conversational content can behave pretty differently with semantic scoring. And are you seeing any cases where it's too aggressive with the pruning?",
          "score": 1,
          "created_utc": "2026-01-13 18:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzfr0f8",
          "author": "Technical-Will-2862",
          "text": "Is this not the same thing? https://www.reddit.com/r/Rag/comments/1qbjr5n/we_built_a_semantic_highlighting_model_for_rag/",
          "score": 1,
          "created_utc": "2026-01-13 22:42:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qcy7nw",
      "title": "Solo Building a Custom RAG Model for Financial Due Diligence - Need Help",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qcy7nw/solo_building_a_custom_rag_model_for_financial/",
      "author": "PositionBoring9826",
      "created_utc": "2026-01-14 20:19:51",
      "score": 9,
      "num_comments": 6,
      "upvote_ratio": 0.91,
      "text": "Hey everyone, \n\nI am new to this community and came here because I have been spinning my wheels for awhile. I am new to RAG and trying to build a RAG model for a private equity firm solo. I understand the concepts and have used LlamaIndex, openai-embeddings, and chromadb to build a \"working\" RAG system. \n\nThe problem I am running into is the type of documents we need to index are pitch deck pdfs (about 100 pages of marketing material, branding images, graphs and visuals, financial tables, and commentary with no whitespace). How do I chunk these documents? Is there any custom embedding model for financial purposes? What methods can I use to improve the accuracy and reduce hallucinations? Where should I even start? I also am curious to see how people metadata-tag these documents. Any advice would be appreciated. ",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qcy7nw/solo_building_a_custom_rag_model_for_financial/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nznf38f",
          "author": "Whole-Assignment6240",
          "text": "depends on what you do - if you have lots of images with text, i find it effective to split pages then do a colpali embedding for just search. and you can add metadata information on that page.\n\n[https://cocoindex.io/examples/multi\\_format\\_index](https://cocoindex.io/examples/multi_format_index)\n\n(i'm one of the maintainer for the framework, this is a tutorial captures the high level gist.)\n\ni see people building with OCR too, and extract  image and text elements with relations, depends on what you need to do with different metadata etc.",
          "score": 1,
          "created_utc": "2026-01-15 01:26:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzp0a9q",
          "author": "aiprod",
          "text": "I’ve seen success with slides by feeding them into a vision language model (smaller ones like haiku or gpt5-mini are good for this and don’t break the bank). Ask the VLM to extract the text and describe any tables, charts and images. This is what you index with a normal embedding model. For retrieval, you retrieve the text version of a full slide and then also fetch a screenshot of the slide in a second step. You feed both to the LLM and you will get good answers.",
          "score": 1,
          "created_utc": "2026-01-15 08:07:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nztno18",
          "author": "blue-or-brown-keys",
          "text": "u/PositionBoring9826 I added a chapter on chunking strategies in the 21 RAG strategies Book. check it out [https://www.twig.so/#DownloadEbookSection](https://www.twig.so/#DownloadEbookSection)",
          "score": 1,
          "created_utc": "2026-01-15 23:35:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzxeg12",
          "author": "Popular_Sand2773",
          "text": "I think one thing people struggle to realize when just starting out is what you search doesn’t have to be what you return. You are on the right track with metadata but with complex multimodal and long docs like this I think you would see a lot of success if you used a summarizer. \n\nIt keeps the search surface clean, does the heavy lifting off the hot path and lets you separate what you search from what you return. A cook doesn’t use raw eggs to make an omelette they whip them first. It’s no different than that.",
          "score": 1,
          "created_utc": "2026-01-16 14:42:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzm2tfs",
          "author": "OnyxProyectoUno",
          "text": "You've got tables that get mangled, images that carry meaning but no text, and dense financial commentary all mashed together. Standard chunking just flattens everything into text soup.\n\nThe issue is your parser, not your chunker. Most PDF parsers treat tables as lines of text, which destroys the structure. You need something that extracts tables as tables, like Docling or Unstructured with table extraction enabled. Once you have clean structured output, chunking becomes way easier.\n\nFor the financial tables specifically, consider keeping them as separate chunks with metadata indicating they're tabular data. Don't try to embed a P&L statement the same way you embed commentary paragraphs. Different content types need different treatment.\n\nOn embeddings, OpenAI's ada-002 is fine for financial text. The custom embedding model rabbit hole rarely pays off unless you're doing something very domain-specific like ticker symbol matching. Your retrieval problems are almost certainly upstream of the embedding step.\n\nMetadata tagging, attach document type, section headers, page numbers, and whether the chunk contains tables or narrative text. This lets you filter at query time instead of hoping the embedding similarity sorts it out.\n\nI've been building [VectorFlow](https://vectorflow.dev/?utm_source=redditCP_i) to let you preview what your docs look like after each transformation, which helps catch table mangling before it hits your vector store. What parser are you using right now?",
          "score": 1,
          "created_utc": "2026-01-14 21:19:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "nzltt44",
          "author": "Anth-Virtus",
          "text": "Try out superlinked embedding methods. For your use case it seems really well suited. Also, look up late-chunking, it's probably better for you.",
          "score": 0,
          "created_utc": "2026-01-14 20:38:34",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qep0ap",
      "title": "Web pages are best performing sources in RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qep0ap/web_pages_are_best_performing_sources_in_rag/",
      "author": "pskd73",
      "created_utc": "2026-01-16 19:04:32",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "I found that the web pages perform a lot better in RAG as a quality sources. The reason is, they are mostly already divided by topic, example, installation, api-fetch, api-update etc. In semantic search it is important for a chunk to be of a specific topic, if a chunk covers multiple topics, the chances that the chunk getting low scores is very high.\n\nBecause of the same reason, I have observed a very consistent pattern. The landing pages generally perform poor because they cover all the topics.\n\nSo chunking is a very an important process and web pages inherently have an advantage. Anybody has similar approach for files, pdfs etc?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qep0ap/web_pages_are_best_performing_sources_in_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "nzzk6d5",
          "author": "hrishikamath",
          "text": "Fortunately sec filings had hierarchical structure so I divided it into sections and did it for finance. Can check the details here: https://github.com/kamathhrishi/stratalens-ai/tree/main/agent",
          "score": 2,
          "created_utc": "2026-01-16 20:33:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o01y2mn",
              "author": "Much-Researcher6135",
              "text": "Wait, you're scraping SEC filings? That's a freaking GREAT idea! People pay good money for those data. But with the advent of LLMs, this process should become open. Well done!",
              "score": 1,
              "created_utc": "2026-01-17 04:44:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "nzzpou2",
          "author": "OnyxProyectoUno",
          "text": "The HTML structure does half the chunking work for you.\n\nFor PDFs and documents, the trick is preserving that same logical structure during parsing. Most people just extract raw text and lose all the semantic boundaries that authors built in. Headers, sections, subsections get flattened into one blob, then you're chunking blindly by token count.\n\nWhat works better is structure-aware parsing first. Keep the document hierarchy intact, then chunk within those logical boundaries. So instead of \"chunk every 500 tokens regardless,\" you're doing \"chunk within sections, respect paragraph breaks, don't split tables.\" \n\nThe other thing is metadata propagation. Web pages have URLs, titles, breadcrumbs. Documents have section headers, page numbers, document titles. That context needs to flow down to the chunks, not get lost during processing.\n\nI've been building document processing tooling around this exact problem at vectorflow.dev because you can't see these structural issues until retrieval fails. By then you're debugging symptoms, not causes.\n\nAre you doing any preprocessing on your web scraping to preserve that HTML structure, or just extracting clean text? The approach there usually translates well to document processing.",
          "score": 2,
          "created_utc": "2026-01-16 20:59:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o02ry1t",
          "author": "Rokpiy",
          "text": "web pages win on single-topic constraint - most docs are multi-topic because they're reference material, not explanatory content. the closer your source chunks to 'one concept per document' the less you fight retrieval.",
          "score": 1,
          "created_utc": "2026-01-17 08:57:38",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}