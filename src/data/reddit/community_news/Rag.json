{
  "metadata": {
    "last_updated": "2026-02-05 02:54:50",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 77,
    "file_size_bytes": 111474
  },
  "items": [
    {
      "id": "1qvjhp4",
      "title": "What are the best resources for RAG in 2026?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "author": "willjacko1",
      "created_utc": "2026-02-04 08:52:16",
      "score": 63,
      "num_comments": 17,
      "upvote_ratio": 0.97,
      "text": "I've been diving deep into RAG architectures lately and wanted to compile/crowdsource the best resources out there. Here's what I've found so far:\n\n**GitHub Repos to Star:**\n- LangChain / LlamaIndex (obviously, but they've evolved a lot)\n- Ragas - for RAG evaluation metrics\n- Chroma / Weaviate / Qdrant - vector DB options with great docs\n- RAGFlow - end-to-end RAG framework\n- Haystack by deepset\n\n**AI Startups to Watch:**\n- Pinecone (vector search infrastructure)\n- Cohere (embeddings + reranking)\n- ZeroEntropy (SoTA Rerankers & Embeddings)\n- Vectara (RAG-as-a-service)\n- LlamaIndex (now a company, not just OSS)\n\n**Communities:**\n- r/RAG (obviously lol)\n- r/LocalLLaMA (great for self-hosted RAG setups)\n- LangChain Discord\n- Context Engineers Discord\n- MLOps Community\n\n**Learning Resources:**\n- LlamaIndex docs (actually really good tutorials)\n- Pinecone learning center\n- \"Building RAG Applications\" courses popping up everywhere\n\nWhat am I missing? Especially interested in:\n1. Any lesser-known GitHub repos that are actually good?\n2. New startups doing interesting RAG work?\n3. YouTube channels or podcasts focused on RAG?\n\nDrop your favorites below üëá",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3imltg",
          "author": "Informal_Tangerine51",
          "text": "Good resource list, but missing the production angle: when RAG fails, can you debug it?\n\nAll these tools help build RAG systems. The gap: when retrieval is wrong, proving what was retrieved requires more than better embeddings or reranking. You need capture of actual chunks, doc versions, timestamps.\n\nWe use similar stack (LlamaIndex, Chroma, rerankers). When Legal asks \"what docs informed this decision,\" the tools retrieved successfully but we can't verify: were chunks stale, which doc version, why these over others.\n\nResources for building RAG are plentiful. Resources for making RAG auditable are scarce. That's the actual production blocker - not accuracy, but provability.\n\nWhat resources exist for RAG evidence capture and incident replay?",
          "score": 15,
          "created_utc": "2026-02-04 12:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iuapn",
              "author": "aiwithphil",
              "text": "Well said.",
              "score": 3,
              "created_utc": "2026-02-04 13:03:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i152i",
          "author": "bravelogitex",
          "text": "no langchain, it is notorious for bad design",
          "score": 8,
          "created_utc": "2026-02-04 09:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i0h32",
          "author": "I_AM_HYLIAN",
          "text": "Context Engineers Discord: [https://discord.gg/F9VNyJzb](https://discord.gg/F9VNyJzb)",
          "score": 3,
          "created_utc": "2026-02-04 08:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i2mlk",
          "author": "ZwombleZ",
          "text": "RAG is so 2024....\n\n\nContext engineering. Agentic rag.",
          "score": 2,
          "created_utc": "2026-02-04 09:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3i31w9",
              "author": "Ok_Pomelo_5761",
              "text": "Agree! That discord server is gold actually (Context Engineers)",
              "score": 2,
              "created_utc": "2026-02-04 09:22:38",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3k74hy",
              "author": "visarga",
              "text": "I prefer [text files with navigable links](https://pastebin.com/VLq4CpCT) between paragraphs. Works with grep, works with coding agents, but unlike RAG it does not have myopic context, links follow where logically needed, not where embedding similarity leads. And this is r/w memory, not r/o like RAG. I don't have any chunking issues either, I don't even need to get very good retrieval from the first move because the agent can explore and follow links along the graph. You know who does the same thing? any coding agent navigating a repo.",
              "score": 2,
              "created_utc": "2026-02-04 17:06:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i9mj1",
          "author": "galdahan9",
          "text": "What about aws bedrock knowledge base?",
          "score": 2,
          "created_utc": "2026-02-04 10:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icamx",
          "author": "Infamous_Ad5702",
          "text": "I skipped rag. KG and indexes for me. No vector.",
          "score": 2,
          "created_utc": "2026-02-04 10:48:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mzx9x",
              "author": "cockerspanielhere",
              "text": "What is KG?",
              "score": 1,
              "created_utc": "2026-02-05 01:24:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n20rm",
                  "author": "Infamous_Ad5702",
                  "text": "Apologies. I normally remember not to abbreviate. It is knowledge graph. It helps to make rag much easier. No embedding and chunking when I have my index which builds the knowledge graph.",
                  "score": 1,
                  "created_utc": "2026-02-05 01:36:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i51gj",
          "author": "pgEdge_Postgres",
          "text": "Shameless self promotion:  \n  \nFor a RAG server that works out-of-the-box with PostgreSQL - [https://github.com/pgEdge/pgedge-rag-server](https://github.com/pgEdge/pgedge-rag-server) \n\nCheck out our repos, there's plenty of open-source tools that you might find helpful, like the [docloader](https://github.com/pgEdge/pgedge-docloader) or [vectorizer](https://github.com/pgEdge/pgedge-vectorizer).\n\nThe creator of the RAG server (and MCP server) wrote up a blog series on building a RAG server with PostgreSQL. It's three parts, here's part 1: [https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content](https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content)",
          "score": 4,
          "created_utc": "2026-02-04 09:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i3kme",
          "author": "Morphos91",
          "text": "Use postgresql for easy and free vector storage. \nYou could use ollama for local embedding models",
          "score": 1,
          "created_utc": "2026-02-04 09:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iw162",
          "author": "Striking-Bluejay6155",
          "text": "If you're looking for materials on Graph-based RAG or building knowledge/context graphs, [check this out](https://www.falkordb.com/blog/implementing-agentic-memory-graphiti/)",
          "score": 1,
          "created_utc": "2026-02-04 13:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmvmd",
          "author": "lizziejaeger",
          "text": "Ragie.ai! https://www.ragie.ai/",
          "score": 1,
          "created_utc": "2026-02-04 15:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jqexs",
          "author": "ZepSweden_88",
          "text": "Check out MIT RLM paper, RAG is dead üíÄ. Beyond 16k tokens the models starts to get context rot == build the RLM paper together with any chunking RAG.",
          "score": 1,
          "created_utc": "2026-02-04 15:49:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3km1n3",
          "author": "Hansehart",
          "text": "Check out Langfuse. Its open source and help you to monitor requests and e.g tool usage. Its mandatory in production to understand what happens under the hood and to debug. You can use it on cloud or self hosted. And as other mentioned GraphRAG is superior to VectorRAG. I personally have great experience with Haystack+Langfuse+Neo4J",
          "score": 1,
          "created_utc": "2026-02-04 18:14:49",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5zua",
      "title": "OpenClaw enterprise setup: MCP isn't enough, you need reranking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "author": "Queasy-Tomatillo8028",
      "created_utc": "2026-02-02 20:05:44",
      "score": 47,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "OpenClaw, 145k stars in 10 weeks. Everyone's talking about MCP - how agents dynamically discover tools, decide when to use them, etc.\n\nI connected a local RAG to OpenClaw via MCP. My agent now knows when to search my docs vs use its memory.\n\n**The problem:** it was searching at the right time, but bringing back garbage.\n\n**MCP solves the WHEN, not the WHAT**\n\nMCP is powerful for orchestration:\n\n* Agent discovers tools at runtime\n* Decides on its own when to invoke `query_documents` vs answer directly\n* Stateful session, shared context\n\nBut MCP doesn't care about the quality of what your tool returns. If your RAG brings back 10 chunks and 7 are noise, the agent will still use them.\n\n**MCP = intelligence on WHEN to search** **Context Engineering = intelligence on WHAT goes into the prompt**\n\nBoth need to work together.\n\n**The WHAT: reranking**\n\nMy initial setup: hybrid search (vector + BM25), top 10 chunks, straight into context.\n\nResult: agent found the right docs but cited wrong passages. Context was polluted.\n\nThe fix: **reranking**.\n\nAfter search, a model re-scores chunks by actual relevance. You keep only top 3-5.\n\nI use **ZeroEntropy**. On enterprise content (contracts, specs), it goes from \\~40% precision to \\~85%. Classic cross-encoders (ms-marco, BGE) work for generic stuff, but on technical jargon ZeroEntropy performs better.\n\n**The full flow**\n\n    User query via WhatsApp\n        ‚Üì\n    OpenClaw decides: \"I need to search the docs\" (MCP)\n        ‚Üì\n    My RAG tool receives the query\n        ‚Üì\n    Hybrid search ‚Üí 30 candidates\n        ‚Üì\n    ZeroEntropy reranking ‚Üí top 3\n        ‚Üì\n    Only these 3 chunks enter the context\n        ‚Üì\n    Precise answer with correct citations\n\nAgent is smart about WHEN to search (MCP). Reranking ensures what it brings back is relevant (Context Engineering).\n\n**Stack**\n\n* **MCP server:** custom, exposes `query_documents`\n* **Search:** hybrid vector + BM25, RRF fusion\n* **Reranking:** ZeroEntropy\n* **Vector store:** ChromaDB\n\n**Result**\n\nBefore: agent searched at the right time but answers were approximate.\n\nAfter: WhatsApp query \"gardening obligations in my lease\" ‚Üí 3 sec ‚Üí exact paragraph, page, quote. Accurate.\n\n**The point**\n\nMCP is one building block. Reranking is another.\n\nMost MCP + RAG setups forget reranking. The agent orchestrates well but brings back noise.\n\nContext Engineering = making sure every token entering the prompt deserves its place. Reranking is how you do that on the retrieval side.\n\nShootout to some smart folks i met on this discord server who helped me figuring out a lot of things: [Context Engineering](https://discord.gg/F9VNyJzb)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o38a4qg",
          "author": "-Cubie-",
          "text": "I like rerankers, but is this an ad?",
          "score": 6,
          "created_utc": "2026-02-02 21:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39d8if",
          "author": "Informal_Tangerine51",
          "text": "You solved retrieval quality but not retrieval proof. When the agent cites wrong passage despite reranking, can you replay what was actually in those top 3 chunks?\n\nWe use reranking too. Helps accuracy but doesn't solve the debugging problem. Agent extracts wrong data, we know reranking happened, but can't verify: were those top 3 chunks stale? Did reranking score change between dev and prod? What version of the docs were retrieved?\n\nMCP orchestration plus reranking gives better answers. Still can't answer \"prove what the agent saw at 2:47am on case #4521\" because logs show reranking executed, not what content passed through.\n\nFor WhatsApp queries this works great. For production agents where Legal asks for evidence, the gap is: can you capture and verify the actual retrieved content, not just that retrieval happened?\n\nDoes your setup store the reranked chunks with timestamps for replay, or just return them to the agent?",
          "score": 3,
          "created_utc": "2026-02-03 00:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3afsec",
              "author": "hncvj",
              "text": "I'm 100% with this person. We also do re-ranking in our projects having HIPAA compliance. We have to keep logs of every single thing, even the data that was sent to LLMs, PHI, De-id, Returned from LLMs, re-ranked, pulled from KG or Vector Database. Everything must be logged with timestamps.\n\nHowever, this depends on project to project basis. In other projects where there is no compliance and the final output is workable and is not required to be error free, it's ok to not have logs that deeper.",
              "score": 2,
              "created_utc": "2026-02-03 04:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37terf",
          "author": "Edcoopersound",
          "text": "What's your latency like end-to-end? From WhatsApp message to response.",
          "score": 2,
          "created_utc": "2026-02-02 20:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37tqnf",
              "author": "Queasy-Tomatillo8028",
              "text": "2-3 sec total",
              "score": 1,
              "created_utc": "2026-02-02 20:15:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3828ew",
          "author": "apirateiwasmeanttobe",
          "text": "I think what people often forget is that you can put anything behind an mcp tool definition. The good mcp tools behave like a person, with some sort of agency or reactivity, answering not with a wall of text but with curated and well trimmed context. You want to minimize the amount of output so that you don't deplete the context of the calling agent.",
          "score": 1,
          "created_utc": "2026-02-02 20:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39c8cy",
          "author": "blue-or-brown-keys",
          "text": "At Twig MCP handles  RAG noise via strategies, the Redwood(basic RAG strategy) does not do reranking but Cedar and Cypress do.",
          "score": 1,
          "created_utc": "2026-02-03 00:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b40my",
          "author": "primoco",
          "text": "I‚Äôve been banging my head against the same wall with enterprise RAG for months, and you're spot on. The \"toy\" setups like basic MCP or vanilla LangChain wrappers just fall apart the second you feed them high-density documents.\n\nIn my experience, if you aren't obsessing over the retrieval pipeline before the query even hits the LLM, you're just building a very expensive hallucination machine. A few things I‚Äôve learned the hard way:\n\n1. **Hybrid search is the only way out.** If you rely only on vector embeddings for factual stuff (like specific dates or IDs in a 500-page report), you‚Äôre going to get \"semantic blurring.\" You need BM25 keyword matching running alongside your vectors with a tunable alpha. It‚Äôs the only way to catch those \"needle in a haystack\" moments.\n2. **Rerankers are double-edged swords.** I‚Äôve seen Rerankers actually kill the correct result because the threshold was a hair too tight. Now I just pull a wider window (Top-K 20) and let the reranker sort the Top-5 without hard-filtering. It‚Äôs safer and much more consistent.\n3. **Small chunks > Big chunks.** We moved to 600-char chunks with a decent overlap and the \"contextual precision\" shot up. Big chunks just add too much noise and confuse the model.\n4. **Stop the \"vibe-checks.\"** You can‚Äôt tell if a RAG is good just because the answer \"sounds professional.\" I had to build a full eval pipeline to realize my \"best sounding\" model was actually making up half the citations.\n\nEnterprise RAG isn't about which LLM is smarter, it's about how much you can control the data flow.",
          "score": 1,
          "created_utc": "2026-02-03 08:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4673",
          "author": "Shekher_05",
          "text": "Ad Detected",
          "score": 1,
          "created_utc": "2026-02-03 08:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37utrg",
          "author": "Anth-Virtus",
          "text": "Hey, yeah, MCP alone isn't enough for a good RAG.\nThanks for sharing the discord link, I appreciate it",
          "score": 1,
          "created_utc": "2026-02-02 20:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ecml",
          "author": "LeadingFun1849",
          "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nHelp me improve it; you can find the link here to try it out:\n\nWebsite¬†https://dlovable.daveplanet.com\nCODE :¬†https://github.com/davidmonterocrespo24/DaveLovable",
          "score": 1,
          "created_utc": "2026-02-02 21:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qul1mq",
      "title": "NotebookLM For Teams",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:37",
      "score": 41,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3i198c",
          "author": "bravelogitex",
          "text": "I'll start taking a look tomorrow, thx for sharing",
          "score": 1,
          "created_utc": "2026-02-04 09:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmssq",
          "author": "tsquig",
          "text": "Something similar here. [NotebookLM...but more](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ax6mb",
          "author": "Otherwise_Wave9374",
          "text": "Cool project. The combination of \"team chat\" + internal sources + an agent that can actually take actions is the sweet spot.\n\nIf you have not already, you might want to think about a permissions model for agent actions (read vs write, connector scopes) plus a way to show citations for every claim to keep trust high.\n\nMore agent design notes here if helpful: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 07:05:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs87ld",
      "title": "Best chunking + embedding strategy for mixed documents converted to Markdown (Docling, FAQs, web data)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "author": "Particular-Gur-1339",
      "created_utc": "2026-01-31 17:19:34",
      "score": 29,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hey folks üëã\nI‚Äôm building a RAG pipeline and could use some advice on chunking and embedding strategies when everything is eventually normalized into Markdown.\n\nCurrent setup\n\nConverting different file types (PDFs, docs, etc.) into Markdown using Docling\nScraping website FAQ pages and storing those as Markdown as well\nEmbedding everything into a vector store for retrieval\n\nStructure of the data\nEach document/page usually has:\nA main heading\nSub-sections under that heading\nMultiple FAQs under each section\nWeb FAQs are often short Q&A pairs\n\nWhat I‚Äôm confused about\nChunking strategy\nShould I chunk by:\nPage\nHeading / sub-heading\nIndividual FAQ (Q + A as one chunk)\n\nHybrid approach (heading context + FAQ chunk)?\n\nChunk size\nFixed token size (for example 300 to 500 tokens)\nOr semantic chunks that vary in size?\nMetadata\n\n\nGoal\nHigh answer accuracy\nAvoid partial or out-of-context answers",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2xli7k",
          "author": "Dapper-Turn-3021",
          "text": "Yeah, you‚Äôre basically on the right track. There‚Äôs no single perfect chunking or embedding strategy, it really depends on your data and what you‚Äôre trying to achieve. Keeping metadata separate is correct you should embed only the actual text content and store the metadata alongside it so you can filter or rank with it later. Embedding metadata usually just adds noise.\nChunking isn‚Äôt fixed either. Sometimes smaller chunks work better, sometimes larger ones, and semantic chunking is often the best option. It‚Äôs normal to tweak your chunking strategy as you see how your retrieval performs.\n\n\nAnd you‚Äôre absolutely right about retrieval. Don‚Äôt rely only on embeddings. Combining embeddings with metadata filtering, keyword or BM25 search, and then adding a reranking step gives much better results in most cases.\n\n\nSo yes, what you described is basically how strong RAG systems are built. we are following same for Zynfo AI to build out chatbot",
          "score": 4,
          "created_utc": "2026-02-01 07:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tpw4u",
          "author": "Curious-Sample6113",
          "text": "Everything depends on your original source. You are on the right path and just have to do a lot of testing.",
          "score": 2,
          "created_utc": "2026-01-31 17:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfmn5",
              "author": "Particular-Gur-1339",
              "text": "What should be my chunking strategy?",
              "score": 1,
              "created_utc": "2026-02-01 02:18:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xc1r6",
                  "author": "Curious-Sample6113",
                  "text": "You optimize your strategy by testing. I don't know what your source looks like. For example: if you are building a legal agent then you will have a series of questions. The ones that fail will reveal the issues with your ingestion. \n\nThere isn't a universal solution. Everything is tailored to the source.",
                  "score": 1,
                  "created_utc": "2026-02-01 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ttf5n",
          "author": "jannemansonh",
          "text": "hit the same wall building doc search workflows... ended up using needle app since it handles the chunking/embedding/rag stuff automatically (has hybrid search built in). you just describe what you need and it builds it vs configuring all the pieces manually",
          "score": 1,
          "created_utc": "2026-01-31 18:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2utz79",
              "author": "Particular-Gur-1339",
              "text": "Didn't get this what is a needle app?",
              "score": 1,
              "created_utc": "2026-01-31 21:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2v26bj",
                  "author": "bwhitts66",
                  "text": "Needle is a tool that streamlines the process of building document search workflows. It automates chunking and embedding so you don‚Äôt have to set everything up manually. It‚Äôs pretty handy if you‚Äôre looking to simplify the RAG pipeline!",
                  "score": 3,
                  "created_utc": "2026-01-31 21:44:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30re8z",
          "author": "Higgs_AI",
          "text": "You‚Äôre asking the wrong question and I mean that in the most helpful way possible.\nThe chunking debate (semantic vs fixed size vs hierarchical) assumes your RAG architecture is correct. It‚Äôs not. You‚Äôre optimizing for retrieval when you should be optimizing for knowledge structure.\nHere‚Äôs the problem with your current setup. Docling to Markdown to Vectors to Retrieval. This pipeline loses the thing that makes FAQs useful, which is the relationships between questions. When you chunk Q+A as atomic units, you lose which questions are siblings under the same topic, which answers reference concepts explained elsewhere, and the hierarchy you already identified (heading to subheading to FAQ).\nWhat you actually want is to stop chunking for embedding and start structuring for reasoning.\nInstead of treating each FAQ as an isolated chunk, build a structure where each FAQ has an ID, knows what topic it belongs to, knows what other FAQs it relates to, and carries its prerequisites. Now your retrieval can find the relevant FAQ by semantic match, pull in related FAQs automatically so you don‚Äôt get out of context answers, and include parent topic context without re-embedding it every time.\nIf you‚Äôre committed to vector RAG, here‚Äôs the practical move. Chunk at FAQ level with Q and A together, you had this right. But prepend the heading hierarchy as metadata, not as embedded text. Store the path like ‚ÄúAccount Settings > Security > Password Recovery‚Äù and at retrieval time inject that context before the FAQ content. This gives you semantic search on the answer content while preserving structural context for the LLM.\nThe hybrid approach you‚Äôre circling around looks like this. Your chunk is the individual FAQ as a Q+A pair. Your metadata is the full heading path, related FAQ IDs, and section summary. Your embedding should be the question plus the first sentence of the answer, not the full text since answers tend to be verbose. At retrieval you grab your top-k FAQs plus their metadata plus the parent section summary.\nToken budget roughly 50 to 100 tokens per FAQ chunk, 20 to 30 tokens for heading context, pull 3 to 5 related FAQs and you‚Äôre at maybe 500 tokens total. That‚Äôs enough for high answer accuracy without blowing up your context window.\nTo hit your specific questions directly. Chunk by FAQ with Q and A together, yes this is correct. Use semantic size not fixed tokens since FAQs vary so let them. Metadata is your secret weapon here, the heading path, section ID, related IDs. And don‚Äôt embed the hierarchy, reference it. Embed the answer, retrieve the structure.\nIf you want to go deeper on this, look into knowledge graphs as a retrieval layer instead of pure vector search. Structure beats embedding for FAQ style content every time. And I do mean EVERY TIME! Just my opinion ü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-01 19:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30z7x9",
          "author": "Live-Guitar-8661",
          "text": "I have a thing and I‚Äôm looking for testers and willing to build a POC for free alongside. DM if you are interested",
          "score": 1,
          "created_utc": "2026-02-01 19:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o313cnn",
          "author": "ampancha",
          "text": "For FAQ-heavy Markdown, heading-anchored chunks with parent context (section title + sub-heading prepended to each chunk) consistently outperform fixed-token splits because the LLM gets retrieval results that carry their own scope. The deeper problem is that without retrieval evaluation and input validation on ingested content, any chunking strategy is an untestable guess, and scraped web pages become an injection surface the moment they land in your vector store. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 20:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xyuw",
          "author": "voycey",
          "text": "There is no single strategy that works, the reason most RAG systems fail is due to poor chunking methodologies.\n\nI will say that there is no real reason not to mix multiple chunking strategies into a single pipeline to ensure you are getting the best retrieval, especially if you are using re-ranking!",
          "score": 1,
          "created_utc": "2026-02-01 22:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32aak9",
          "author": "Odd-Affect236",
          "text": "What benefits does markdown provide when compared to simple plain text?",
          "score": 1,
          "created_utc": "2026-02-01 23:43:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq8y1q",
      "title": "TextTools ‚Äì High-Level NLP Toolkit Built on LLMs (Translation, NER, Categorization & More)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "author": "Due_Place_6635",
      "created_utc": "2026-01-29 13:45:08",
      "score": 25,
      "num_comments": 3,
      "upvote_ratio": 0.97,
      "text": "Hey everyone! üëã\n\nI've been working on TextTools, an open-source NLP toolkit that wraps LLMs with ready-to-use utilities for common text processing tasks. Think of it as a high-level API that gives you structured outputs without the prompt engineering hassle.\n\nWhat it does:\n\nTranslation, summarization, and text augmentation\n\nQuestion detection and generation\n\nCategorization and keyword extraction\n\nNamed Entity Recognition (NER)\n\nCustom tools for almost anything\n\nWhat makes it different:\n\nBoth sync and async APIs (TheTool & AsyncTheTool)\n\nStructured outputs with validation\n\nProduction-ready tools (tested) + experimental features\n\nWorks with any OpenAI-compatible endpoint\n\nQuick example:\n\n```python\nfrom texttools import TheTool\n\nthe_tool = TheTool(client=openai_client, model=\"your_model\")\nresult = the_tool.is_question(\"Is this a question?\")\nprint(result.to_json())\n```\nCheck it out: https://github.com/mohamad-tohidi/texttools\n\nI'd love to hear your thoughts! If you find it useful, contributions and feedback are super welcome. What other NLP utilities would you like to see added?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq8y1q/texttools_highlevel_nlp_toolkit_built_on_llms/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2ewxt9",
          "author": "Popular_Sand2773",
          "text": "Hey like the idea of a one stop shop but a lot of these are tasks I would never give to an llm. Have you thought about unifying the BIC options for these different tasks? BERTopic is a good example of a unifying framework that takes the best from everyone.",
          "score": 3,
          "created_utc": "2026-01-29 14:17:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qos79",
              "author": "Due_Place_6635",
              "text": "Hello, thank you for the reply  \ni completly agree, there are a lot of tasks that an small finetuned BERT model would be far better at them, rather than a giant LLM!\n\nyes, as a matter of fact, it used to be part of the library, but we decided to remove that\n\nbecause: the llm's are open domain, they are kind of like a quick Demo of what is possible...",
              "score": 1,
              "created_utc": "2026-01-31 05:10:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o31qx2m",
                  "author": "leppardfan",
                  "text": "Nice work! I think you should put the BERT models back in as another option for the user, depending on the problem.",
                  "score": 2,
                  "created_utc": "2026-02-01 22:01:19",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv17hv",
      "title": "We open-sourced our code that outperforms RAPTOR on multi-hop retrieval",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "author": "captainPigggy",
      "created_utc": "2026-02-03 19:14:03",
      "score": 23,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "We recently open-sourced a RAG system we built for internal use and figured it might be useful to others working on retrieval-heavy applications.\n\nThere‚Äôs no novel algorithm or research contribution here. The system is built by carefully combining existing techniques:\n\n* RAPTOR-style hierarchical trees\n* Knowledge graphs\n* HyDE query expansion\n* BM25 + dense hybrid search\n* Cohere reranker (this alone gave \\~+9%)\n\nOn benchmarks, it slightly outperforms RAPTOR on multi-hop retrieval (72.89% on MultiHop-RAG) and gets \\~99% retrieval accuracy on SQuAD.\n\nWe focused on making this something you can actually install, run, and modify without stitching together a dozen repos.\n\nWe built this for IncidentFox, where we use it to store and retrieve company and team knowledge. Since retrieval isn‚Äôt our product differentiator, we decided to open-source the RAG layer.\n\nRepo: [https://github.com/incidentfox/OpenRag](https://github.com/incidentfox/OpenRag)  \nWrite-up with details and benchmarks: [https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html](https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html)\n\nHappy to answer questions or hear feedback from folks building RAG systems.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ecql6",
          "author": "DashboardNight",
          "text": "Yeah, the Cohere reranker is really good. Unfortunately it remains a catastrophe with their privacy policy, where they can use anything that you provide. A local reranker may be preferable, or even a LLM-reranker using a local model or a GDPR-compliant provider. Other than that, good stuff!",
          "score": 6,
          "created_utc": "2026-02-03 19:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3elfp7",
              "author": "captainPigggy",
              "text": "good point, let it make this clear in readme",
              "score": 2,
              "created_utc": "2026-02-03 20:24:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ee7ua",
          "author": "Oshden",
          "text": "Amazing OP! Thank you for sharing this with the world at large. I‚Äôm definitely gonna star this repo!",
          "score": 3,
          "created_utc": "2026-02-03 19:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3el9hx",
              "author": "captainPigggy",
              "text": "of course thanks!",
              "score": 2,
              "created_utc": "2026-02-03 20:23:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3exmnn",
          "author": "iLoveSeiko",
          "text": "This is really cool compilation of techniques. Thanks for sharing pal",
          "score": 1,
          "created_utc": "2026-02-03 21:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1hgp",
          "author": "Regular-Forever5876",
          "text": "Thank you sir, will have a look into your implementation üôè",
          "score": 1,
          "created_utc": "2026-02-04 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gz6ty",
          "author": "WorkingOccasion902",
          "text": "Can this implement multi-tenant ?",
          "score": 1,
          "created_utc": "2026-02-04 03:59:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs2uzw",
      "title": "Building a RAG-Based Chat Assistant using Elasticsearch as a Vector Database",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "author": "Leading-Grape-6659",
      "created_utc": "2026-01-31 13:50:21",
      "score": 20,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "Hi everyone üëã\n\n\n\nI recently built a simple RAG (Retrieval-Augmented Generation) chat assistant using Elasticsearch as a vector database.\n\n\n\nThe blog covers:\n\n‚Ä¢ How vector embeddings are stored in Elasticsearch\n\n‚Ä¢ Semantic retrieval using vector search\n\n‚Ä¢ How retrieved context improves LLM responses\n\n‚Ä¢ Real-world use cases like internal knowledge bots\n\n\n\nFull technical walkthrough with code and architecture here:\n\nüëâ [https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends\\_link&sk=d2006b31e40e3c3ed714c18eabf8f271](https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends_link&sk=d2006b31e40e3c3ed714c18eabf8f271)\n\n\n\nHappy to hear feedback or suggestions from folks working with RAG and vector databases!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2sf6zr",
          "author": "cat47b",
          "text": "I did wonder about elastic but the hassle and cost of using that as a store has put me off a bit. Have you looked at Clickhouse?",
          "score": 2,
          "created_utc": "2026-01-31 13:55:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq1hly",
      "title": "PDFstract now supports chunking inspection & evaluation for RAG document pipelines",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "author": "GritSar",
      "created_utc": "2026-01-29 06:55:11",
      "score": 18,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I‚Äôve been experimenting with different chunking strategies for RAG pipelines, and one pain point I kept hitting was **not knowing whether a chosen strategy actually makes sense for a given document** before moving on to embeddings and indexing.\n\n\n\nSo I added a **chunking inspection & evaluation feature** to an open-source tool I‚Äôm building called **PDFstract**.\n\n\n\nHow it works:\n\n* You **choose a chunking strategy**\n* PDFstract applies it to your document\n* You can **inspect chunk boundaries, sizes, overlap, and structure**\n* Decide if it fits your use case *before* you spend time and tokens on embeddings\n\n\n\nIt sits as the **first layer in the pipeline**:\n\nExtract ‚Üí Chunk ‚Üí (Embedding coming next)\n\n\n\nI‚Äôm curious how others here validate chunking today:\n\n* Do you tune based on document structure?\n* Or rely on downstream retrieval metrics?\n\nWould love to hear what‚Äôs actually worked in production.\n\nRepo if anyone wants to try it:\n\n[https://github.com/AKSarav/pdfstract](https://github.com/AKSarav/pdfstract)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qq1hly/pdfstract_now_supports_chunking_inspection/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qv2ks2",
      "title": "Architecture breakdown: Processing 2GB+ of docs for RAG without OOM errors (Python + Generators)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "author": "jokiruiz",
      "created_utc": "2026-02-03 20:04:05",
      "score": 16,
      "num_comments": 2,
      "upvote_ratio": 0.95,
      "text": "Most RAG tutorials teach you to load a PDF into a list. That works for 5MB, but it crashes when you have 2GB of manuals or logs.\n\nI built a pipeline to handle large-scale ingestion efficiently on a consumer laptop. Here is the architecture I used to solve RAM bottlenecks and API rate limits:\n\n1. **Lazy Loading with Generators:** Instead of `docs = loader.load()`, I implemented a Python Generator (`yield`). This processes one file at a time, keeping RAM usage flat regardless of total dataset size.\n2. **Persistent Storage:** Using ChromaDB in persistent mode (on disk), not in-memory. Index once, query forever.\n3. **Smart Batching:** Sending embeddings in batches of 100 to the API with `tqdm` for monitoring, handling rate limits gracefully.\n4. **Recursive Chunking with Overlap:** Critical for maintaining semantic context across cuts.\n\nI made a full code-along video explaining the implementation line-by-line using Python and LangChain concepts.\n\n[https://youtu.be/QR-jTaHik8k?si=a\\_tfyuvG\\_mam4TEg](https://youtu.be/QR-jTaHik8k?si=a_tfyuvG_mam4TEg)\n\nIf you have questions about the `yield` implementation or the batching logic, ask away!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3et0i0",
          "author": "Oshden",
          "text": "Whoa this is awesome! Thanks for sharing!!!",
          "score": 3,
          "created_utc": "2026-02-03 21:00:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hwbel",
              "author": "jokiruiz",
              "text": "thanks! glad you like it!",
              "score": 1,
              "created_utc": "2026-02-04 08:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qur17q",
      "title": "Best open-source embedding model for a RAG system?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "author": "Public-Air3181",
      "created_utc": "2026-02-03 12:48:20",
      "score": 14,
      "num_comments": 11,
      "upvote_ratio": 0.9,
      "text": "I‚Äôm an **entry-level AI engineer**, currently in the training phase of a project, and I could really use some guidance from people who‚Äôve done this in the real world.\n\nRight now, I‚Äôm building a **RAG-based system** focused on **manufacturing units‚Äô rules, acts, and standards** (think compliance documents, safety regulations, SOPs, policy manuals, etc.). The data is mostly **text-heavy, formal, and domain-specific**, not casual conversational data.  \nI‚Äôm at the stage where I need to finalize an **embedding model**, and I‚Äôm specifically looking for:\n\n* **Open-source embedding models**\n* Good performance for **semantic search/retrieval**\n* Works well with **long, structured regulatory text**\n* Practical for real projects (not just benchmarks)\n\nI‚Äôve come across a few options like Sentence Transformers, BGE models, and E5-based embeddings, but I‚Äôm unsure which ones actually perform best in a **RAG setup for industrial or regulatory documents**.\n\nIf you‚Äôve:\n\n* Built a RAG system in production\n* Worked with manufacturing / legal / compliance-heavy data\n* Compared embedding models beyond toy datasets\n\nI‚Äôd love to hear:\n\n* Which embedding model worked best for you and **why**\n* Any pitfalls to avoid (chunking size, dimensionality, multilingual issues, etc.)\n\nAny advice, resources, or real-world experience would be super helpful.  \nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3dh4z3",
          "author": "hrishikamath",
          "text": "Probably just start off with: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2? Then iterate over. The big con of using a bad rag model is that you need to retrieve more chunks and use a cross encoder. I used this and got a like 91% on finance bench. So it‚Äôs still a good starting point, you can optimize later. Repo: https://github.com/kamathhrishi/stratalens-ai (going to update blogpost with latest accuracy)",
          "score": 6,
          "created_utc": "2026-02-03 17:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e608x",
          "author": "thecontentengineer",
          "text": "I have tried ZeroEntropy embeddings they are the best I‚Äôve tried.\n\nYou can find them at https://zeroEntropy.dev \n\nThey were in beta when I first tried them, not sure now.",
          "score": 5,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3evjbx",
              "author": "ghita__",
              "text": "oh! hello, im the founder, thank you for mentioning us! we're indeed planning GA release soon! stay tuned for sota open-weight embeddings :)",
              "score": 7,
              "created_utc": "2026-02-03 21:11:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3f0vwh",
          "author": "Informal_Tangerine51",
          "text": "For compliance docs, embedding quality matters but so does proving what was retrieved. You'll hit this when auditors ask \"what regulation informed this decision?\"\n\nWe use BGE-large for similar formal documents. Works well for semantic search. But when extraction is wrong, embeddings don't help you debug. Vector DB logs show query embedding, not what chunks were returned or if they were current versions.\n\nFor your use case, beyond embedding choice: how will you verify retrieved content later? Manufacturing compliance means \"prove this safety decision used regulation version X dated Y.\" Embeddings find relevant chunks, but you need to capture which chunks, from which doc version, retrieved when.\n\nPractical advice: test BGE-large vs E5-large on your actual compliance docs, not benchmarks. More important: design your RAG to store retrieval decisions (chunk IDs, doc versions, timestamps) not just return results. You'll need that evidence trail.\n\nWhat's your plan for handling doc version control when regulations update?",
          "score": 2,
          "created_utc": "2026-02-03 21:36:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gjqh9",
          "author": "sirebral",
          "text": "I really enjoy the Qwen 3 embedding models.  Even the . 6b is quite nice.",
          "score": 2,
          "created_utc": "2026-02-04 02:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ncji1",
              "author": "Disastrous-Nature269",
              "text": "Can confirm",
              "score": 1,
              "created_utc": "2026-02-05 02:36:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jfqgo",
          "author": "Academic_Track_2765",
          "text": "Hello. You can start with the MiniLM embeddings from sentence transformers, but I would encourage you to use the BGE embeddings (also available via sentence transformers). There is a evaluation dashboard that shows you which embedding models perform best for certain tasks, and you should definitely use it. \n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\nlately for my personal projects I have been using the Qwen .6b with their Qwen .6b reranker for the cross encoder stage and lastly the Qwen 3 Next 80B for synthesis. So far its been great!. I would also suggest you experiment with different embedding models. In my earlier years I did a lot of experimentation with embedding models, and BGE models performed well given they produced better results than the E5/GTR models, but the speed didn't come close to the MiniLM models. I have built many RAG systems in production for very long Health Care related documents, with varying complexity. Your biggest challenges will be handling the PII/PHI data, How to embed different document types, handling complex document structures, like nested tables in PDF files, image references etc, but you can use a vision model / ocr model to handle some of that. If you have questions just IM me directly, and I would be happy to help. ",
          "score": 2,
          "created_utc": "2026-02-04 14:58:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k5heu",
          "author": "lfnovo",
          "text": "Qwen3-embedding works wonders for me.",
          "score": 2,
          "created_utc": "2026-02-04 16:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cgc6m",
          "author": "polandtown",
          "text": "Those are the embedding models I'd use. IMO now you need to make a set of test questions - IMO ask your user group for a curated list of such that fits your use case. Then use such to test against each of the embedding models. Done. \n\nThe challenge here is to get a set of stratified example questions.",
          "score": 1,
          "created_utc": "2026-02-03 14:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g85bm",
          "author": "laurentbourrelly",
          "text": "Use the filters on the leaderboard to find precisely what you are looking for.\n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)",
          "score": 1,
          "created_utc": "2026-02-04 01:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gisgr",
          "author": "No_Wrongdoer41",
          "text": "embedding models can really struggle when complex reasoning is required. i have a graphrag approach built into a platform where you can upload the docs and we take care of everything else. id love for you to try it (for free) if you are willing! you can drag and drop the docs and then try out the resulting agent in our web app or via api.",
          "score": 1,
          "created_utc": "2026-02-04 02:23:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvy3hv",
      "title": "Context Blindness: A Fundamental Limitation of Vector-Based RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "author": "Diligent-Fly3756",
      "created_utc": "2026-02-04 19:22:45",
      "score": 14,
      "num_comments": 6,
      "upvote_ratio": 0.95,
      "text": "**Retrieval-Augmented Generation (RAG)** has become the dominant paradigm for grounding large language models (LLMs) in external knowledge. Among RAG approaches, **vector-based retrieval**‚Äîwhich embeds documents and queries into a shared semantic space and retrieves the most semantically similar chunks‚Äîhas emerged as the de facto standard.\n\nThis dominance is understandable: vector RAG is simple, scalable, and fits naturally into existing information-retrieval pipelines. However, as LLM systems evolve from single-turn question answering toward multi-turn, agentic, and reasoning-driven applications, the limitations of vector-based RAG are becoming increasingly apparent.\n\nMany of these limitations are well known. Others are less discussed, yet far more fundamental. This article argues that **context blindness**, the inability of vector-based retrieval to condition on full conversational and reasoning context, is the most critical limitation of vector-based RAG, and one that fundamentally constrains its role in modern LLM systems.\n\n# Commonly Discussed Limitations of Vector-Based RAG\n\n**The Limitations of Semantic Similarity**\n\nVector-based retrieval assumes that semantic similarity between a query and a passage is a reliable proxy for relevance. This assumption breaks down in two fundamental ways.\n\nFirst, similarity-based retrieval often misses what should be retrieved (false negatives). User queries typically express intent rather than the literal surface form of the supporting evidence, and the information that satisfies the intent is often implicit, procedural, or distributed across multiple parts of a document. As a result, truly relevant evidence may share little semantic overlap with the query and therefore fails to be retrieved by similarity search, creating a **context gap** between what the user is trying to retrieve and what similarity search can represent.\n\nSecond, similarity-based retrieval often returns what should not be retrieved (false positives). Even when retrieved passages appear highly similar to the query, similarity does not guarantee relevance, especially in domain-specific documents such as financial reports, legal contracts, and technical manuals, where many sections share near-identical language but differ in critical details such as numerical thresholds, applicability conditions, definitions, or exceptions. Vector embeddings tend to blur these distinctions, creating **context confusion**: passages that appear relevant in isolation are retrieved despite being incorrect given the actual scope, constraints, or exceptions. In professional and enterprise settings, this failure mode is particularly dangerous because it grounds confident answers in plausible but incorrect evidence.\n\n**The Limitations of Embedding Models**\n\nEmbedding models transform passages into vector representations. However, the input length limits of the embedding model force documents to be split into chunks, disrupting their structure and introducing information discontinuities. Definitions become separated from constraints, tables from explanations, and exceptions from governing rules. Although often cited as the main limitation of vector-based RAG, chunking is better viewed as a secondary consequence of deeper architectural constraints.\n\n# The Under-Discussed Core Problem: Context Blindness\n\nA core limitation of vector-based RAG that is rarely discussed is its **context blindness**: the retrieval query cannot carry the full context that led to the question. In modern LLM applications, queries are rarely standalone. They depend on prior dialogue, intermediate conclusions, implicit assumptions, operational context, and evolving user intent. Yet vector-based retrieval operates on a short, decontextualized query that must be compressed into one or more fixed-length vectors.\n\nThis compression is not incidental ‚Äî it is fundamental. A vector embedding has limited representational capacity: it must collapse rich, structured reasoning context into a dense numerical representation that cannot faithfully preserve dependencies, conditionals, negations, or conversational state. As a result, vector-based retrieval is inherently **context-independent**. Documents are matched against a static semantic representation rather than the full reasoning state of the system. This creates a structural disconnect: the LLM reasons over a long, evolving context, while the vector retriever operates on a minimal, compressed, and flattened signal. In other words, **the LLM reasoner is stateful, while the vector retriever is not.** Even with prompt engineering, query expansion, multi-vector retrieval, or reranking, this mismatch persists, because the limitation lies in the representational bottleneck of vectors themselves. The vector retriever remains blind to the very context that determines what ‚Äúrelevant‚Äù means.\n\n# Paradigm Shift: From Context-Independent Semantic Similarity to Context-Dependent Relevance Classification\n\nThe solution to context blindness is not a better embedding model or a larger vector database, but a change in how retrieval itself is formulated. Instead of treating retrieval as a semantic similarity search performed by an external embedding model, retrieval should be framed as a **relevance classification problem** executed by an LLM that has access to the **full reasoning context**.\n\nIn this formulation, the question is no longer ‚ÄúWhich passages are closest to this query in embedding space?‚Äù, but rather ‚ÄúGiven everything the system knows so far‚Äîuser intent, prior dialogue, assumptions, and constraints‚Äîis this piece of content relevant or not?‚Äù Relevance becomes an explicit decision conditioned on context, rather than an implicit signal derived from vector proximity.\n\nBecause modern LLMs are designed to reason over long, structured context, they are naturally well-suited to this role. Unlike embedding models, which must compress inputs into fixed-length vectors and inevitably discard structure and dependencies, LLM-based relevance classification can directly condition on the entire conversation history and intermediate reasoning steps. As a result, retrieval becomes context-aware and adapts dynamically as the user‚Äôs intent evolves.\n\nThis shift transforms retrieval from a standalone preprocessing step into part of the reasoning loop itself. Instead of operating outside the LLM stack as a static similarity lookup, retrieval becomes tightly coupled with decision-making, enabling RAG systems that scale naturally to multi-turn, agentic, and long-context settings.\n\n# Scaling Relevance Classification via Tree Search\n\nA common concern with context-dependent, relevance-classification-based retrieval is token efficiency. **Naively classifying relevance over the entire knowledge base via brute-force evaluation is token-inefficient and does not scale.** However, token inefficiency is not inherent to relevance-classification-based retrieval; it arises from flat, brute-force evaluation rather than **hierarchical classification**.\n\nIn **PageIndex**, retrieval is implemented as a **hierarchical relevance classification** over document structure (sections ‚Üí pages ‚Üí blocks), where relevance is evaluated top-down and entire subtrees are pruned once a high-level unit is deemed irrelevant. This transforms retrieval from exhaustive enumeration into selective exploration, focusing computation only on promising regions. The intuition resembles systems such as **AlphaGo**, which achieved efficiency not by enumerating all possible moves, but by navigating a large decision tree through learned evaluation and selective expansion. Similarly, PageIndex avoids wasting tokens on irrelevant content, enabling context-conditioned retrieval that is both more accurate and more efficient than flat vector-based RAG pipelines that depend on large candidate sets, reranking, and repeated retrieval calls.\n\n# The Future of RAG\n\nThe rise of frameworks such as **PageIndex** signals a broader shift in the AI stack. As language models become increasingly capable of planning, reasoning, and maintaining long-horizon context, the responsibility for finding relevant information is gradually **moving** **from the database layer to the model layer**.\n\nThis transition is already evident in the coding domain. Agentic tools such as **Claude Code** are moving beyond simple vector lookups toward active codebase exploration: navigating file hierarchies, inspecting symbols, following dependencies, and iteratively refining their search based on intermediate findings. Generic document retrieval is likely to follow the same trajectory. As tasks become more multi-step and context-dependent, passive similarity search increasingly gives way to structured exploration driven by reasoning.\n\nVector databases will continue to have important, well-defined use cases, such as recommendation systems and other settings, where semantic similarity **is the objective**. However, their historical role as the default retrieval layer for LLM-based systems is becoming less clear. As retrieval shifts from similarity matching to context-dependent decision-making, agentic systems increasingly demand mechanisms that can reason, adapt, and operate over structure, rather than relying solely on embedding proximity.\n\nIn this emerging paradigm, retrieval is no longer a passive lookup operation. It becomes an integral part of the model‚Äôs reasoning process: executed by the model, guided by intent, and grounded in context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3l1d0c",
          "author": "Diligent-Fly3756",
          "text": "PageIndex's GitHub Repo: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-02-04 19:23:59",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3l468x",
              "author": "CathyCCCAAAI",
              "text": "Thanks for sharing! ",
              "score": 2,
              "created_utc": "2026-02-04 19:37:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3l3sz7",
          "author": "Pure_Squirrel175",
          "text": "Thx for sharing this, very insightful",
          "score": 2,
          "created_utc": "2026-02-04 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lme7z",
          "author": "trollsmurf",
          "text": "\"the dominant paradigm for grounding large language models (LLMs) in external knowledge\"\n\nRAG is semantic search with LLM summary and is a kludge that's used way too much for things it's not at all suited for.\n\nI propose:\n\nFuture content solutions need to generate its own code for querying / modifying whatever the user requests. No RAG/CAG, no embedding, always working on the whole corpus (including formatting) via generated code and an LLM being used for understanding intent and generating cohesive and human-understandable output. The generated code can in turn use embedding or whatever is needed to get the results requested.",
          "score": 1,
          "created_utc": "2026-02-04 21:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m25mm",
          "author": "Informal_Tangerine51",
          "text": "Context-aware retrieval is interesting but doesn't solve the accountability gap. When relevance classification retrieves wrong documents, can you prove which ones were evaluated and why they scored as relevant?\n\nYour hierarchical approach prunes irrelevant subtrees efficiently. But when an agent makes bad decision based on retrieved context, debugging needs more than \"it classified these as relevant\" - needs the actual classification scores, which documents were considered, what caused pruning at each level.\n\nWe hit this with vector RAG: retrieval happens, model decides, incident occurs, and we can't replay what was actually retrieved or how fresh it was. Context-aware retrieval improves accuracy but doesn't automatically capture decision evidence.\n\nFor production agents where compliance asks \"prove what documents informed this,\" does your system capture classification decisions as verifiable artifacts? Or focus on improving retrieval accuracy without evidence trails?",
          "score": 1,
          "created_utc": "2026-02-04 22:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ncmo8",
              "author": "iamaiimpala",
              "text": "These are such good points. Audit trails and governance are seriously lacking in a lot of solutions, and are non-negotiable for real enterprise level adoption.",
              "score": 1,
              "created_utc": "2026-02-05 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsenyn",
      "title": "MiRAGE: A Multi-Agent Framework for Generating Multimodal, Multihop Evaluation Datasets (Paper + Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "author": "Socaplaya21",
      "created_utc": "2026-01-31 21:21:26",
      "score": 13,
      "num_comments": 0,
      "upvote_ratio": 0.89,
      "text": "TL;DR**:** We developed a multi-agent framework that generates multimodal, multihop QA pairs from technical documents (PDFs containing text, tables, charts). Unlike existing pipelines that often generate shallow questions, MiRAGE uses an adversarial verifier and expert persona injection to create complex reasoning chains (avg 2.3+ hops).\n\n**Paper:** [https://arxiv.org/abs/2601.15487](https://arxiv.org/abs/2601.15487)\n\n**Code:** [https://github.com/ChandanKSahu/MiRAGE](https://github.com/ChandanKSahu/MiRAGE)\n\nHi everyone,\n\nWe've been working on evaluating RAG systems for industrial/enterprise use cases (technical manuals, financial reports, regulations), and (as many have) we hit a recurring problem: standard benchmarks like Natural Questions or MS MARCO don't reflect the complexity of our data.\n\nMost existing eval datasets are single-hop and purely textual. In the real world, our documents are multimodal (*especially* heavy on tables/charts in our use cases) and require reasoning across disjoint sections (multi-hop).\n\nWe built and open-sourced MiRAGE, a multi-agent framework designed to automate the creation of \"Gold Standard\" evaluation datasets from your arbitrary corpora.\n\nInstead of a linear generation pipeline (which often leads to hallucinations or shallow questions), we use a swarm of specialized agents.\n\nInstead of immediate generation, we use a retrieval agent that recursively builds a semantic context window. This agent gathers scattered evidence to support complex inquiries *before* a question-answer pair is formulated, allowing the system to generate multi-hop queries (averaging >2.3 hops) rather than simple keyword lookups.\n\nWe address the reliability of synthetic data through an adversarial verification phase. A dedicated verifier agent fact-checks the generated answer against the source context to ensure factual grounding and verifies that the question does not rely on implicit context (e.g., rejecting questions like \"In the table below...\").\n\nWhile the system handles text and tables well, visual grounding remains a frontier. Our ablation studies revealed that current VLMs still rely significantly on dense textual descriptions to bridge the visual reasoning gap, when descriptions were removed, faithfulness dropped significantly.\n\nThe repo supports local and cloud API model calls. We're hoping this helps others stress test their pipelines.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvttdv",
      "title": "Bayesian BM25 blends more smoothly with vector scores (less scale mismatch than simple weighted sum)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-04 16:51:46",
      "score": 13,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "when it comes to retrieval, aggregation methods really matter and yet many people use heuristics which are not always very rigorous.\n\nbm25 scores and dense similarity scores live on very different scales and distributions. Even with normalization, the balance is usually heuristic and dataset‚Äëdependent, so you often end up tuning weights per domain.\n\nrrf ignores score magnitudes and uses only rank positions. That‚Äôs robust to scale mismatch, but it can discard useful confidence information and flatten large gaps between documents, which matters when one signal is clearly stronger.\n\n\\## Experiments\n\n    Setup\n    - Dataset: SQuAD\n    - Metrics: NDCG@10, MRR@10\n    - Dense model: BGE-M3\n    - Compared: weighted-sum (WS) hybrid vs RRF\n    \n    Results\n    - WS (bb25 + Dense): NDCG@10 0.9149, MRR@10 0.8850\n    - WS (BM25 + Dense): NDCG@10 0.9051, MRR@10 0.8717\n    - RRF (BM25 + Dense): NDCG@10 0.8874, MRR@10 0.8483\n\nBayesian BM25 maps BM25 scores into calibrated probabilities using a likelihood and prior model. Once lexical scores are on a probabilistic scale, they combine more naturally with vector scores (also treated as probabilities). In practice this reduces scale mismatch and stabilizes hybrid fusion without heavy tuning.\n\nuse with \\`pip install bb25\\`. happy to share code and details if anyone‚Äôs interested. feedback welcome!\n\n\n\nRepo:¬†[https://github.com/sigridjineth/bb25](https://github.com/sigridjineth/bb25)\n\nLibrary:¬†[http://pypi.org/project/bb25/](http://pypi.org/project/bb25/)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3kchwd",
          "author": "Routine_Paramedic_82",
          "text": "Will test this, looks interesting",
          "score": 3,
          "created_utc": "2026-02-04 17:31:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qrdj5s",
      "title": "Reranker Strategy: Switching from MiniLM to Jina v2 or BGE m3 for larger chunks?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-01-30 18:24:03",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\nI'm upgrading the reranker in my RAG setup. I'm moving off ms-marco-MiniLM-L12-v2 because its 512-token limit is truncating my 500-word chunks.\n\nI need something with at least a 1k token context window that offers a good balance of modern accuracy and decent latency on a GPU.\n\nI'm currently torn between:\n\n1. jinaai/jina-reranker-v2-base-multilingual\n\n2. BAAI/bge-reranker-v2-m3\n\nIs the Jina model actually faster in practice? Is BGE's accuracy worth the extra compute? If anyone is using these for chunks of similar size, I'd love to hear your experience.\n\nOpen to other suggestions as well!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o31hlfn",
          "author": "ampancha",
          "text": "Switching to a longer-context reranker solves truncation, but it also means more tokens per retrieval hit flowing into your LLM. If you don't have per-query token caps and cost attribution already in place, the accuracy upgrade can quietly double your inference spend. Worth locking that down before you benchmark the new model. Sent you a DM.",
          "score": 2,
          "created_utc": "2026-02-01 21:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qlkx0",
          "author": "Horror-Turnover6198",
          "text": "Theoretically you could chunk again post-retrieval to sub 512, then rerank, and if one of the sub-chunks makes the cut you splice it back together. Or discard the other sub-chunk. Just depends on your chunking strategy I guess. Not sure if that‚Äôs appealing but it may be an option if bge or jina performance isn‚Äôt great.",
          "score": 1,
          "created_utc": "2026-01-31 04:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlyhg",
              "author": "CourtAdventurous_1",
              "text": "Bit if i truncate and then rerank it, wouldn‚Äôt it decrease the accuracy or the actual meaning of the chunk pr something like that?",
              "score": 1,
              "created_utc": "2026-01-31 04:49:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qmz8g",
                  "author": "Horror-Turnover6198",
                  "text": "So let‚Äôs say you retrieved 10 candidates of 500 words each. If you took each candidate and split it into 3 or 4 chunks, you‚Äôd end up with 30 or 40 sub-512 token chunks, that would then get reranked individually. After rerank, let‚Äôs say chunks 7, 14, and 16 ended up in your top_k. You would then lookup which chunk 7, 14 and 16 came from, and just use those. So the meaning of each chunk would make it into your reranker, just not all at once. Some semantic meaning might be lost by breaking up the chunks for the reranker, but as I understand it, it‚Äôs hard to keep tight meaning across much more than 512 tokens in the first place. I‚Äôm totally open to being called out as wrong here though.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpl86",
          "author": "jannemansonh",
          "text": "spent way too much time optimizing rerankers and chunk sizes for my doc workflows... ended up moving those to needle app since the rag stack is built in. still run custom rag for specific use cases though. for your question though - bge m3 accuracy is solid if you've got the compute, jina v2 is faster but check the multilingual overhead if you're english-only",
          "score": 1,
          "created_utc": "2026-01-31 10:39:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qun1d9",
      "title": "Chunking strategy",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qun1d9/chunking_strategy/",
      "author": "Ordinary_Pineapple27",
      "created_utc": "2026-02-03 08:58:54",
      "score": 10,
      "num_comments": 11,
      "upvote_ratio": 0.86,
      "text": "Hi guys,\n\nNowadays I am working on Text Retrieval project where I have thousands of pdf files and the task is given a query the system should return related passage (highlighted as Google does) within documents.   \nFor text extraction, I am using paddleocr vl which is doing well so far. As most of you know, given a single pdf file, paddleocr vl returns a folder with md and json files (as set to save both md and json files) for each page. If the pdf file has 50 pages, there are 50 md and json files.   \n  \nI am having difficulty in how to do the chunking. I know that given a query, I need the page information as a metadata to show the related page and passage within documents.   \nIf I just concatenate all the md files and do one of the chunking strategies, I will lose the page information. But If I do not concatenate them, I will lose context of some passages where one half is on the first page and the other is one the next page. \n\nBesides that I am well-aware of embedding models, the RAG architecture, rerankers, etc. But no matter how good your overall architecture is, if your chunks are garbage, the retrieval results will also be garbage.\n\nThose, who have come accross with such issue, please, advice me.  \nThank you beforehand.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qun1d9/chunking_strategy/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3bargd",
          "author": "Better_Ad_3004",
          "text": "Commenting to come back later",
          "score": 2,
          "created_utc": "2026-02-03 09:14:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bep68",
              "author": "stingraycharles",
              "text": "You know that Reddit has a save function, right?",
              "score": 6,
              "created_utc": "2026-02-03 09:53:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3cdo4l",
              "author": "welcome-overlords",
              "text": "Same here. \n\nMy specific case is that there are hundreds of pages of large blueprints in pdf files.\n\nSo far ive been using aws bedrock knowledge bases and their default chunkings etc. Seems to work ok but there's a lot of improvements to be made",
              "score": 1,
              "created_utc": "2026-02-03 14:07:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlgbh",
                  "author": "_harryj",
                  "text": "You might want to look into overlapping chunking strategies where you take a few lines from the end of one page and a few from the beginning of the next. It helps maintain context while still giving you the page metadata you need. Also, consider tagging your chunks with page numbers to keep track of where each piece originated.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dryce",
          "author": "Smart_MoneyTor",
          "text": "I am working on something similar but at a much larger scale. I am still in ideation phase, but I can already tell you that there is no one-size-fits-all solution. Chunking at fixed window sizes will for sure rotten your context, so the chunking strategy is of extreme importance. \n\nMy approach, which is theoretical at this stage, is to adopt a multi-stage conditional chunking approach as follows: \n- Set a max chunk size (C_max)\n- recursively traverse the document (e.g., md) and chunk if section/subsection size <= C_max, else (i.e., section size > C_max) initiate a chunk set [c_0, .. c_i] where you chunk at max size up until the last chunk will is treated similarly to first if branch. Your metadata should maintain the link between chunks within a set, so that if one element is hit with your vector search, the whole set is retrieved. \nThe metadata may also include section header numbering/titles, page numbers etc. when you rank chunks that you‚Äôll populate the context with, you can use the page numbers associated with those in the metadata to get the snippets highlighting used chunks. \n\nYou may further try to ‚Äúcompress‚Äù the set of chunks to fit it into your context. \n\nThis is the approach I intend to work with. If anyone has any ideas on how to improve this, or thinks there are problems I‚Äôm overlooking, please feel free to chime in.",
          "score": 1,
          "created_utc": "2026-02-03 18:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e7lf7",
          "author": "thecontentengineer",
          "text": "You can join context engineering discord. Found much better help than here. Lots of smart folks. \n\nhttps://discord.gg/FC7Mw66GY",
          "score": 1,
          "created_utc": "2026-02-03 19:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6i50",
          "author": "Informal_Tangerine51",
          "text": "Page metadata versus context continuity is a real tradeoff. But beyond chunking strategy, can you verify which chunks were actually retrieved when results are wrong?\n\nFor your use case: chunk with overlap across page boundaries, store page number as metadata for each chunk. When chunk spans pages, metadata shows \"pages 5-6.\" Retrieval shows correct pages, overlap preserves context.\n\nThe gap most people miss: good chunking gets better results, but when retrieval is still wrong, can you prove which chunks were returned and whether they were from current doc version? Legal asks \"what regulation text did the system retrieve,\" you need chunk provenance not just accuracy.\n\nYour architecture focus is right - chunking quality matters first. Also plan for: how will you verify what was retrieved six months later when someone questions a result?\n\nWhat's your plan for tracking chunk lineage back to source docs?",
          "score": 1,
          "created_utc": "2026-02-03 22:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fznxa",
          "author": "Live-Guitar-8661",
          "text": "We are about to release tree based RAG and I think it‚Äôs going to be a game changer.\n\nTechnically PageIndex got there first, we were doing a hybrid strategy between splitting the doc up and turning it into chunks by section, but tree based is way more effective.\n\nShould be out next week with an OSS version to follow (hopefully next week as well)\n\nhttps://orchata.ai\n\nHMU if you wanna chat",
          "score": 1,
          "created_utc": "2026-02-04 00:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bdrs3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -1,
          "created_utc": "2026-02-03 09:44:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3chy0g",
              "author": "Trotskyist",
              "text": "pretty sure this is either a bot or the developer of needle fwiw given how they casually seem to name drop it in nearly every thread in this subreddit",
              "score": 5,
              "created_utc": "2026-02-03 14:30:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gc3jk",
                  "author": "Wimiam1",
                  "text": "If you look at their profile, you‚Äôll see they‚Äôre the dev. It‚Äôs really lame behaviour. I almost used their product, but their underhanded self promotion completely turned me off",
                  "score": 1,
                  "created_utc": "2026-02-04 01:45:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qr1suf",
      "title": "Build n8n Automation with RAG and AI Agents ‚Äì Real Story from the Trenches",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "author": "According-Site9848",
      "created_utc": "2026-01-30 10:24:49",
      "score": 9,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "One of the hardest lessons I learned while building n8n automations with RAG (Retrieval-Augmented Generation) and AI agents is that the problem isn‚Äôt writing workflows its handling real-world chaos. I was helping a mid-sized e-commerce client who sold across Shopify, eBay, and YouTube and the volume of incoming customer questions, order updates and content requests was overwhelming their small team. The breakthrough came when we layered RAG on top of n8n: every new message or order triggers a workflow that first retrieves relevant historical context (past orders, previous customer messages, product FAQs) and then passes it to an AI agent that drafts a response or generates a content snippet. This reduced manual errors drastically and allowed staff to focus on exceptions instead of repetitive tasks. For example, a new Shopify order automatically pulled product specs, checked inventory, created a draft invoice in QuickBooks and even generated a YouTube short highlighting the new product without human intervention. The key insight: start with the simplest reliable automation backbone (parsing inputs ‚Üí enriching via RAG ‚Üí action via AI agents), then expand iteratively. If anyone wants to map their messy multi-platform workflows into a clean, intelligent n8n + RAG setup, I‚Äôm happy to guide and  to help get it running efficiently in real operations.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2nhhpu",
          "author": "Whole-Board4430",
          "text": "Hi i read your post, i am new to RAG. for one of my employers where i'm helping with marketing and implementing basic AI, i am building a RAG agent. For now i just built a simple agent on n8n, 1 workflow to vector the documents into a database, the agent to retrieve it.\n\nWhat i want this agent to be able to do (if possible) is using our business documents + certain books, methods and other data we find important, and transform this working together with a seperate LLM to generate content, help our customers with questions, help our team with their work, onboard new people into the team when nessecarry. Do you mind if i ask you some questions, you now have a basic view of what i am trying to get working",
          "score": 1,
          "created_utc": "2026-01-30 18:46:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qth7ek",
      "title": "RAG, Medical Models <20B, guardrails, and sVLMs for medical scans ?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "author": "jiii95",
      "created_utc": "2026-02-02 01:26:49",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.85,
      "text": "[](https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22Resources%22)\n\nSo, I am in the cardiovascular area, and I am looking for small models < 20B params, that can work for my rag that is dealing with structured JSON data. Do you have any suggestions ? I also suffer from some hallucinations, and I want also to imlement guardrails for my application to answer only medical questions about cardiovascular & data that is present and cited in the docs, will LLM be efficient with some prompts for guardrails or do you have something specific to offer. I am open only for open-source solutions, not enterprise paid software.  \nI am also looking for any sVLMs (Small Vision Language Models) that can take scans of the chest region or aorta and interpret them, or at least do segmentation or classification, any suggestions? If not a complete answer you have, any resources to look into?\n\nThank you very much (If you think I can cross-post in some other subreddit, please, any answer you can give and be beneficial, please)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o32vlim",
          "author": "Yablan",
          "text": "Literally guardrails then?\n\nhttps://github.com/guardrails-ai/guardrails",
          "score": 2,
          "created_utc": "2026-02-02 01:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32wh3m",
              "author": "jiii95",
              "text": "Well I want the output to be two things: output aigh cited data or only medical content about cardiovascular. Anything else such as medical advice or any otherput must marked as Off-topic and nothing as ouput",
              "score": 1,
              "created_utc": "2026-02-02 01:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35jeqi",
          "author": "sp3d2orbit",
          "text": "Have you considered a different approach? When I try to apply RAG directly to medical Data, the problem is always in the vector embeddings. No matter how I create them (frontier models or specialized models).\n\n  \nSince you already have everything in json format that means you're dealing with structured data. I would build an ontology on top of it and do ontology guiding search‚Äã. That means you're not dealing with hallucinations. And still use the llm in the parts of the pipeline where it makes sense, just not for the parts that require no hallucination.",
          "score": 1,
          "created_utc": "2026-02-02 13:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35jkrr",
              "author": "jiii95",
              "text": "Can you please elaborate more ? Very interesting",
              "score": 1,
              "created_utc": "2026-02-02 13:43:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35qm22",
                  "author": "sp3d2orbit",
                  "text": "So I don't know your exact use case. But you mentioned cardiology. If look at the ICD-10 ontology there's something like 1200 codes that are applicable to cardiology. If we look at SNOMED-CT there might be 10 times that number if you consider all the different structures disorders etc. \n\n  \nThose codes exist because there's some sort of medical workflow or billing logic or treatment that depends on them being distinct. When you try to apply RAG to this problem, your in essence hoping that you can fragment the documents into those 1200 codes or 12,000 codes depending on your scenario. \n\n  \nEven the very best models are going to have a hard time doing that with Fidelity. I find it's better to invert the problem, use the ontology first, and then then we use the llm to do generative tasks.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:22:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qqfxko",
      "title": "Tried to Build a Personal AI Memory that Actually Remembers - Need Your Help!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "author": "Tough-Percentage-864",
      "created_utc": "2026-01-29 18:01:20",
      "score": 8,
      "num_comments": 8,
      "upvote_ratio": 0.9,
      "text": "Hey everyone, I was inspired by the **Shark Tank NeoSapien concept**, so I built my own **Eternal Memory system** that doesn‚Äôt just store data - it *evolves with time*.([LinkedIn](https://www.linkedin.com/posts/abhaygupta53_ai-python-eternal-activity-7422690736359415808-PjWd?rcm=ACoAADg6WLoBZEyh7uuClgIZx3hLSD1IzZb81Nc&utm_medium=member_desktop&utm_source=share))\n\nRight now it can:  \n\\-Transcribe audio + remember context  \n\\-  Create **Daily / Weekly / Monthly summaries**  \n\\- Maintain short-term memory that fades into long-term  \n\\- Run **semantic + keyword search** over your entire history\n\nI‚Äôm also working on **GraphRAG for relationship mapping** and **speaker identification** so it knows *who said what*.\n\nI‚Äôm looking for **high-quality conversational / life-log / audio datasets** to stress-test the memory evolution logic.  \n**Does anyone have suggestions? Or example datasets (even just in DataFrame form) I could try?**\n\nExamples of questions I want to answer with a dataset:\n\n* ‚ÄúWhat did I do in **Feb 2024?**‚Äù\n* ‚ÄúWhy was I sad in **March 2024?**‚Äù\n* Anything where a system can actually recall patterns or context over time.\n\nDrop links, dataset names, or even Pandas DataFrame ideas anything helps! üôå\n\n  \n\n\n",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qqfxko/tried_to_build_a_personal_ai_memory_that_actually/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2g9zjj",
          "author": "Tough-Percentage-864",
          "text": "Repo link --> [https://github.com/Abhay-404/Eternal-Memory](https://github.com/Abhay-404/Eternal-Memory)",
          "score": 2,
          "created_utc": "2026-01-29 18:01:54",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o2rt5dr",
          "author": "vikasprogrammer",
          "text": "I am also interested in this, I have started working on the hardware part using this chip Seeed Studio XIAO Nordic nRF52840 Sense Module (chip -> BLE audio -> app -> AI/memory -> output). I want to combine efforts.",
          "score": 2,
          "created_utc": "2026-01-31 11:12:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2sgx12",
              "author": "Tough-Percentage-864",
              "text": "Cool Keep buliding üöÄ",
              "score": 1,
              "created_utc": "2026-01-31 14:05:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2iiim8",
          "author": "DetectiveMindless652",
          "text": "Dm me",
          "score": 1,
          "created_utc": "2026-01-30 00:35:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kqo5a",
              "author": "Tough-Percentage-864",
              "text": "ü§î",
              "score": 1,
              "created_utc": "2026-01-30 09:43:37",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2ks73z",
                  "author": "DetectiveMindless652",
                  "text": "We‚Äôre working on something adjacent mate",
                  "score": 2,
                  "created_utc": "2026-01-30 09:57:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qtnjhr",
      "title": "Is this \"Probe + NLI Verification\" logic overkill for accurate GraphRAG? (Replacing standard rerankers)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qtnjhr/is_this_probe_nli_verification_logic_overkill_for/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-02-02 06:30:49",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI'm building a RAG pipeline that relies on graph-based connections between large chunks (\\~500 words). I previously used a standard reranker (BGE-M3) to establish edges like \"Supports\" or \"Contradicts,\" but I ran into a major semantic collision problem:\n\nThe Problem:\n\nRelevance models don't understand logic. To BGE-M3, Chunk A (\"AI is safe\") and Chunk B (\"AI is NOT safe\") are 95% similar. My graph ended up with edges saying Chunk A both SUPPORTS and CONTRADICTS Chunk B.\n\nThe Proposed Fix (My \"Probe Graph\" Logic):\n\nI'm shifting to a new architecture and want to know if this is a solid approach or if I'm over-engineering it.\n\n1. Intent Probing (Vector Search): Instead of one generic search, I run 5 parallel searches with specific query templates (e.g., Query for Contradicts: \"Criticism and counter-arguments to {Chunk\\_Summary}\").\n\n2. Logic Gating (Zero-Shot): I pass the candidates to ModernBERT-large-zeroshot with specific labels (supports, contradicts, example of).\n\n3. Strict Filtering: I only create the edge if the NLI model predicts the specific relationship and rejects the others (e.g., if I'm probing for \"Supports,\" I reject the edge if the model detects \"Contradiction\").\n\nMy Question:\n\nHas anyone successfully used Zero-Shot classifiers (like ModernBERT) as a \"Logic Gate\" for graph edges in production?\n\n‚Ä¢ Does the latency hit (running NLI on top-k pairs) justify the accuracy gain?\n\n‚Ä¢ Are there lighter-weight ways to stop \"Supports/Contradicts\" collisions without running a full cross-encoder?\n\nStack: Infinity (Rust) for Embeddings + ModernBERT (Bfloat16) for Logic.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qtnjhr/is_this_probe_nli_verification_logic_overkill_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o35fbzw",
          "author": "saiprasad04",
          "text": "The semantic collision problem you're describing is a real pain point with embedding models - they capture topic similarity but not logical relationships. Your approach is solid.\n\nA few thoughts on your questions:\n\nFor lighter alternatives to full NLI, you might consider using negation-aware embeddings (like InstructorXL with explicit instructions about stance) or adding a simple negation detection layer before your classifier. Some teams also use contrastive fine-tuning on their specific domain to make the embedding space more logic-aware.\n\nOn latency - if you're running NLI on top-k pairs (say k=20), that's manageable in most cases. The bigger question is whether you batch these calls or run them sequentially. Batching on GPU makes a huge difference.\n\nYour strict filtering approach (rejecting edges when NLI detects conflicting signals) is a good safeguard. One refinement: consider adding a confidence threshold rather than binary accept/reject - edges with ambiguous NLI scores might warrant a different edge type like \"related\" rather than being dropped entirely.",
          "score": 1,
          "created_utc": "2026-02-02 13:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35n2f3",
              "author": "CourtAdventurous_1",
              "text": "Thanks for the suggestion but i am using multiple probes like similar,contradict,elaborate,depends on and example of,\nSo will a nli model handle all this probes or is there any alternative (and for my pipeline if the probe graph creation takes some time its not that much of a problem as it is not directly related to output of the user‚Äôs query)",
              "score": 1,
              "created_utc": "2026-02-02 14:03:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35iowx",
          "author": "jannemansonh",
          "text": "the semantic collision problem you're hitting is brutal... spent way too long wiring custom reranker + nli logic for graph-based rag before. ended up using needle app for those workflows since it handles the embedding/relationship logic internally.... way simpler than managing the bge-m3 + modernbert pipeline yourself, especially for the supports/contradicts edge cases",
          "score": 1,
          "created_utc": "2026-02-02 13:38:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35zynq",
          "author": "coderarun",
          "text": "How does this approach compare to extracting a KG and have concepts/arguments as nodes and \"SUPPORTS/CONTRADICTS\" as edges?",
          "score": 1,
          "created_utc": "2026-02-02 15:10:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qq847v",
      "title": "RAG SDK: would this benefit anyone?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qq847v/rag_sdk_would_this_benefit_anyone/",
      "author": "DetectiveMindless652",
      "created_utc": "2026-01-29 13:09:32",
      "score": 7,
      "num_comments": 11,
      "upvote_ratio": 0.82,
      "text": "Hey everyone,\n\n\n\nI've been working on a local RAG SDK that runs entirely on your machine - no cloud, no API keys needed. It's built on top of a persistent knowledge graph engine and I'm looking for developers to test it and give honest feedback.\n\n\n\nWe'd really love people's feedback on this. We've had about 10 testers so far and they love it - but we want to make sure it works well for more use cases before we call it production-ready. If you're building RAG applications or working with LLMs, we'd appreciate you giving it a try.\n\n\n\nWhat it does:\n\n\\- Local embeddings using sentence-transformers (works offline)\n\n\\- Semantic search with 10-20ms latency (vs 50-150ms for cloud solutions)\n\n\\- Document storage with automatic chunking\n\n\\- Context retrieval ready for LLMs\n\n\\- ACID guarantees (data never lost)\n\n\n\nBenefits:\n\n\\- 2-5x faster than cloud alternatives (no network latency)\n\n\\- Complete privacy (data never leaves your machine)\n\n\\- Works offline (no internet required after setup)\n\n\\- One-click installer (5 minutes to get started)\n\n\\- Free to test (beer money - just looking for feedback)\n\n\n\nWhy I'm posting:\n\nI want to know if this actually works well in real use cases. It's completely free to test - I just need honest feedback:\n\n\\- Does it work as advertised?\n\n\\- Is the performance better than what you're using?\n\n\\- What features are missing?\n\n\\- Would you actually use this?\n\n\n\nIf you're interested, DM me and I'll send you the full package with examples and documentation. Happy to answer questions here too!\n\n\n\nThanks for reading - really appreciate any feedback you can give.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qq847v/rag_sdk_would_this_benefit_anyone/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2kt1t8",
          "author": "Orihara-Izaya",
          "text": "Github????",
          "score": 1,
          "created_utc": "2026-01-30 10:05:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2kwf7d",
              "author": "DetectiveMindless652",
              "text": "24 hours bro",
              "score": 1,
              "created_utc": "2026-01-30 10:35:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2o703k",
          "author": "Diablo_Rigs",
          "text": "I‚Äôd love to give a try, was planning on building my own but this is wonderful.",
          "score": 1,
          "created_utc": "2026-01-30 20:43:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2s3uep",
              "author": "DetectiveMindless652",
              "text": "Dm me",
              "score": 1,
              "created_utc": "2026-01-31 12:41:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2z7z5g",
          "author": "Expensive_Culture_46",
          "text": "If you had an interface that was user friendly this would ruin a lot of people‚Äôs scams. \n\nGo for it.",
          "score": 1,
          "created_utc": "2026-02-01 14:53:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o309p3x",
              "author": "DetectiveMindless652",
              "text": "Please dm me, this is a brilliant idea",
              "score": 1,
              "created_utc": "2026-02-01 17:50:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o30lfg7",
                  "author": "Expensive_Culture_46",
                  "text": "Streamlit and then flask for file uploads",
                  "score": 1,
                  "created_utc": "2026-02-01 18:42:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o34a0rj",
          "author": "relcyoj",
          "text": "‚úãüôÇ‚Äç‚ÜîÔ∏èüëç",
          "score": 1,
          "created_utc": "2026-02-02 07:25:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o355nhj",
              "author": "DetectiveMindless652",
              "text": "dm me",
              "score": 1,
              "created_utc": "2026-02-02 12:13:39",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o34wfpn",
          "author": "FancyAd4519",
          "text": "as long as its not dependant on torch or numpy",
          "score": 1,
          "created_utc": "2026-02-02 10:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o356222",
              "author": "DetectiveMindless652",
              "text": "sure isnt!",
              "score": 1,
              "created_utc": "2026-02-02 12:16:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}