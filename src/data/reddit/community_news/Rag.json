{
  "metadata": {
    "last_updated": "2026-02-07 16:47:15",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 128,
    "file_size_bytes": 155833
  },
  "items": [
    {
      "id": "1qvjhp4",
      "title": "What are the best resources for RAG in 2026?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "author": "willjacko1",
      "created_utc": "2026-02-04 08:52:16",
      "score": 92,
      "num_comments": 27,
      "upvote_ratio": 0.98,
      "text": "I've been diving deep into RAG architectures lately and wanted to compile/crowdsource the best resources out there. Here's what I've found so far:\n\n**GitHub Repos to Star:**\n- LangChain / LlamaIndex (obviously, but they've evolved a lot)\n- Ragas - for RAG evaluation metrics\n- Chroma / Weaviate / Qdrant - vector DB options with great docs\n- RAGFlow - end-to-end RAG framework\n- Haystack by deepset\n\n**AI Startups to Watch:**\n- Pinecone (vector search infrastructure)\n- Cohere (embeddings + reranking)\n- ZeroEntropy (SoTA Rerankers & Embeddings)\n- Vectara (RAG-as-a-service)\n- LlamaIndex (now a company, not just OSS)\n\n**Communities:**\n- r/RAG (obviously lol)\n- r/LocalLLaMA (great for self-hosted RAG setups)\n- LangChain Discord\n- Context Engineers Discord\n- MLOps Community\n\n**Learning Resources:**\n- LlamaIndex docs (actually really good tutorials)\n- Pinecone learning center\n- \"Building RAG Applications\" courses popping up everywhere\n\nWhat am I missing? Especially interested in:\n1. Any lesser-known GitHub repos that are actually good?\n2. New startups doing interesting RAG work?\n3. YouTube channels or podcasts focused on RAG?\n\nDrop your favorites below üëá",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3imltg",
          "author": "Informal_Tangerine51",
          "text": "Good resource list, but missing the production angle: when RAG fails, can you debug it?\n\nAll these tools help build RAG systems. The gap: when retrieval is wrong, proving what was retrieved requires more than better embeddings or reranking. You need capture of actual chunks, doc versions, timestamps.\n\nWe use similar stack (LlamaIndex, Chroma, rerankers). When Legal asks \"what docs informed this decision,\" the tools retrieved successfully but we can't verify: were chunks stale, which doc version, why these over others.\n\nResources for building RAG are plentiful. Resources for making RAG auditable are scarce. That's the actual production blocker - not accuracy, but provability.\n\nWhat resources exist for RAG evidence capture and incident replay?",
          "score": 17,
          "created_utc": "2026-02-04 12:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iuapn",
              "author": "aiwithphil",
              "text": "Well said.",
              "score": 3,
              "created_utc": "2026-02-04 13:03:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3plcte",
              "author": "ChlorrOfTheMask",
              "text": "Do you have recommendations on resources or tools for making RAG auditable?",
              "score": 1,
              "created_utc": "2026-02-05 13:09:21",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o40nauc",
              "author": "__init__i",
              "text": "Refer to graphRAG, lightRAG",
              "score": 1,
              "created_utc": "2026-02-07 02:54:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i152i",
          "author": "bravelogitex",
          "text": "no langchain, it is notorious for bad design",
          "score": 9,
          "created_utc": "2026-02-04 09:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i0h32",
          "author": "I_AM_HYLIAN",
          "text": "Context Engineers Discord: [https://discord.gg/F9VNyJzb](https://discord.gg/F9VNyJzb)",
          "score": 5,
          "created_utc": "2026-02-04 08:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i51gj",
          "author": "pgEdge_Postgres",
          "text": "Shameless self promotion:  \n  \nFor a RAG server that works out-of-the-box with PostgreSQL - [https://github.com/pgEdge/pgedge-rag-server](https://github.com/pgEdge/pgedge-rag-server) \n\nCheck out our repos, there's plenty of open-source tools that you might find helpful, like the [docloader](https://github.com/pgEdge/pgedge-docloader) or [vectorizer](https://github.com/pgEdge/pgedge-vectorizer).\n\nThe creator of the RAG server (and MCP server) wrote up a blog series on building a RAG server with PostgreSQL. It's three parts, here's part 1: [https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content](https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content)",
          "score": 4,
          "created_utc": "2026-02-04 09:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i2mlk",
          "author": "ZwombleZ",
          "text": "RAG is so 2024....\n\n\nContext engineering. Agentic rag.",
          "score": 2,
          "created_utc": "2026-02-04 09:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k74hy",
              "author": "visarga",
              "text": "I prefer [text files with navigable links](https://pastebin.com/VLq4CpCT) between paragraphs. Works with grep, works with coding agents, but unlike RAG it does not have myopic context, links follow where logically needed, not where embedding similarity leads. And this is r/w memory, not r/o like RAG. I don't have any chunking issues either, I don't even need to get very good retrieval from the first move because the agent can explore and follow links along the graph. You know who does the same thing? any coding agent navigating a repo.",
              "score": 4,
              "created_utc": "2026-02-04 17:06:24",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3yl91z",
                  "author": "Single-Constant9518",
                  "text": "Navigating with links sounds interesting! It does seem like a more flexible approach, especially for complex tasks. Have you found any specific use cases where this method really outperformed traditional RAG setups?",
                  "score": 1,
                  "created_utc": "2026-02-06 20:04:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i9mj1",
          "author": "galdahan9",
          "text": "What about aws bedrock knowledge base?",
          "score": 2,
          "created_utc": "2026-02-04 10:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icamx",
          "author": "Infamous_Ad5702",
          "text": "I skipped rag. KG and indexes for me. No vector.",
          "score": 2,
          "created_utc": "2026-02-04 10:48:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mzx9x",
              "author": "cockerspanielhere",
              "text": "What is KG?",
              "score": 1,
              "created_utc": "2026-02-05 01:24:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n20rm",
                  "author": "Infamous_Ad5702",
                  "text": "Apologies. I normally remember not to abbreviate. It is knowledge graph. It helps to make rag much easier. No embedding and chunking when I have my index which builds the knowledge graph.",
                  "score": 1,
                  "created_utc": "2026-02-05 01:36:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i3kme",
          "author": "Morphos91",
          "text": "Use postgresql for easy and free vector storage. \nYou could use ollama for local embedding models",
          "score": 1,
          "created_utc": "2026-02-04 09:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iw162",
          "author": "Striking-Bluejay6155",
          "text": "If you're looking for materials on Graph-based RAG or building knowledge/context graphs, [check this out](https://www.falkordb.com/blog/implementing-agentic-memory-graphiti/)",
          "score": 1,
          "created_utc": "2026-02-04 13:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmvmd",
          "author": "lizziejaeger",
          "text": "Ragie.ai! https://www.ragie.ai/",
          "score": 1,
          "created_utc": "2026-02-04 15:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jqexs",
          "author": "ZepSweden_88",
          "text": "Check out MIT RLM paper, RAG is dead üíÄ. Beyond 16k tokens the models starts to get context rot == build the RLM paper together with any chunking RAG.",
          "score": 1,
          "created_utc": "2026-02-04 15:49:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3km1n3",
          "author": "Hansehart",
          "text": "Check out Langfuse. Its open source and help you to monitor requests and e.g tool usage. Its mandatory in production to understand what happens under the hood and to debug. You can use it on cloud or self hosted. And as other mentioned GraphRAG is superior to VectorRAG. I personally have great experience with Haystack+Langfuse+Neo4J",
          "score": 1,
          "created_utc": "2026-02-04 18:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o2bm1",
          "author": "ReverseBlade",
          "text": "[https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap](https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap)",
          "score": 1,
          "created_utc": "2026-02-05 05:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qyfjb",
          "author": "DeadPukka",
          "text": "Check out [Graphlit](https://www.graphlit.com), build your context layer for AI agents.",
          "score": 1,
          "created_utc": "2026-02-05 17:16:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3x04bu",
              "author": "Shot_Platypus4420",
              "text": "Hi! You have a really interesting project. I'd like to test it with my bot and see how it performs. But I'm confused about how to estimate my future costs. :) I can't say I've thoroughly studied the graphlit documentation. :) Perhaps my questions weren't well formulated, but the graphlit bot told me there are no data extraction costs, only indexing costs. I've tried searching the documentation to get a clear picture of the pricing and my future costs, but so far without success. :) Could you please provide links to the information I'm looking for?",
              "score": 2,
              "created_utc": "2026-02-06 15:31:39",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3xnhar",
                  "author": "DeadPukka",
                  "text": "Thanks for trying it out! We are going to add more cost info to the context of the chatbot - good point. \n\nBut you can think of credits as based on compute, storage, LLM tokens and third party API call (transcription, OCR, etc). \n\nYou‚Äôll pay for effort to ingest into the Graphlit project.  And then pay at retrieval time (much smaller cost). \n\nFeel free to join our Discord (linked in docs) and I‚Äôd be happy to help model out your use case with you. (I‚Äôm kirkm* on Discord)",
                  "score": 2,
                  "created_utc": "2026-02-06 17:21:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3rl011",
          "author": "pantoniades",
          "text": "Curious- why is pinecone different from the other vector databases you list?",
          "score": 1,
          "created_utc": "2026-02-05 18:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s87jj",
          "author": "Ok_Constant_9886",
          "text": "Personally recommend deepeval over ragas, everyone from my company switched over",
          "score": 1,
          "created_utc": "2026-02-05 20:49:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5zua",
      "title": "OpenClaw enterprise setup: MCP isn't enough, you need reranking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "author": "Queasy-Tomatillo8028",
      "created_utc": "2026-02-02 20:05:44",
      "score": 47,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "OpenClaw, 145k stars in 10 weeks. Everyone's talking about MCP - how agents dynamically discover tools, decide when to use them, etc.\n\nI connected a local RAG to OpenClaw via MCP. My agent now knows when to search my docs vs use its memory.\n\n**The problem:** it was searching at the right time, but bringing back garbage.\n\n**MCP solves the WHEN, not the WHAT**\n\nMCP is powerful for orchestration:\n\n* Agent discovers tools at runtime\n* Decides on its own when to invoke `query_documents` vs answer directly\n* Stateful session, shared context\n\nBut MCP doesn't care about the quality of what your tool returns. If your RAG brings back 10 chunks and 7 are noise, the agent will still use them.\n\n**MCP = intelligence on WHEN to search** **Context Engineering = intelligence on WHAT goes into the prompt**\n\nBoth need to work together.\n\n**The WHAT: reranking**\n\nMy initial setup: hybrid search (vector + BM25), top 10 chunks, straight into context.\n\nResult: agent found the right docs but cited wrong passages. Context was polluted.\n\nThe fix: **reranking**.\n\nAfter search, a model re-scores chunks by actual relevance. You keep only top 3-5.\n\nI use **ZeroEntropy**. On enterprise content (contracts, specs), it goes from \\~40% precision to \\~85%. Classic cross-encoders (ms-marco, BGE) work for generic stuff, but on technical jargon ZeroEntropy performs better.\n\n**The full flow**\n\n    User query via WhatsApp\n        ‚Üì\n    OpenClaw decides: \"I need to search the docs\" (MCP)\n        ‚Üì\n    My RAG tool receives the query\n        ‚Üì\n    Hybrid search ‚Üí 30 candidates\n        ‚Üì\n    ZeroEntropy reranking ‚Üí top 3\n        ‚Üì\n    Only these 3 chunks enter the context\n        ‚Üì\n    Precise answer with correct citations\n\nAgent is smart about WHEN to search (MCP). Reranking ensures what it brings back is relevant (Context Engineering).\n\n**Stack**\n\n* **MCP server:** custom, exposes `query_documents`\n* **Search:** hybrid vector + BM25, RRF fusion\n* **Reranking:** ZeroEntropy\n* **Vector store:** ChromaDB\n\n**Result**\n\nBefore: agent searched at the right time but answers were approximate.\n\nAfter: WhatsApp query \"gardening obligations in my lease\" ‚Üí 3 sec ‚Üí exact paragraph, page, quote. Accurate.\n\n**The point**\n\nMCP is one building block. Reranking is another.\n\nMost MCP + RAG setups forget reranking. The agent orchestrates well but brings back noise.\n\nContext Engineering = making sure every token entering the prompt deserves its place. Reranking is how you do that on the retrieval side.\n\nShootout to some smart folks i met on this discord server who helped me figuring out a lot of things: [Context Engineering](https://discord.gg/F9VNyJzb)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o38a4qg",
          "author": "-Cubie-",
          "text": "I like rerankers, but is this an ad?",
          "score": 6,
          "created_utc": "2026-02-02 21:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39d8if",
          "author": "Informal_Tangerine51",
          "text": "You solved retrieval quality but not retrieval proof. When the agent cites wrong passage despite reranking, can you replay what was actually in those top 3 chunks?\n\nWe use reranking too. Helps accuracy but doesn't solve the debugging problem. Agent extracts wrong data, we know reranking happened, but can't verify: were those top 3 chunks stale? Did reranking score change between dev and prod? What version of the docs were retrieved?\n\nMCP orchestration plus reranking gives better answers. Still can't answer \"prove what the agent saw at 2:47am on case #4521\" because logs show reranking executed, not what content passed through.\n\nFor WhatsApp queries this works great. For production agents where Legal asks for evidence, the gap is: can you capture and verify the actual retrieved content, not just that retrieval happened?\n\nDoes your setup store the reranked chunks with timestamps for replay, or just return them to the agent?",
          "score": 3,
          "created_utc": "2026-02-03 00:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3afsec",
              "author": "hncvj",
              "text": "I'm 100% with this person. We also do re-ranking in our projects having HIPAA compliance. We have to keep logs of every single thing, even the data that was sent to LLMs, PHI, De-id, Returned from LLMs, re-ranked, pulled from KG or Vector Database. Everything must be logged with timestamps.\n\nHowever, this depends on project to project basis. In other projects where there is no compliance and the final output is workable and is not required to be error free, it's ok to not have logs that deeper.",
              "score": 2,
              "created_utc": "2026-02-03 04:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37terf",
          "author": "Edcoopersound",
          "text": "What's your latency like end-to-end? From WhatsApp message to response.",
          "score": 2,
          "created_utc": "2026-02-02 20:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37tqnf",
              "author": "Queasy-Tomatillo8028",
              "text": "2-3 sec total",
              "score": 1,
              "created_utc": "2026-02-02 20:15:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3828ew",
          "author": "apirateiwasmeanttobe",
          "text": "I think what people often forget is that you can put anything behind an mcp tool definition. The good mcp tools behave like a person, with some sort of agency or reactivity, answering not with a wall of text but with curated and well trimmed context. You want to minimize the amount of output so that you don't deplete the context of the calling agent.",
          "score": 1,
          "created_utc": "2026-02-02 20:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39c8cy",
          "author": "blue-or-brown-keys",
          "text": "At Twig MCP handles  RAG noise via strategies, the Redwood(basic RAG strategy) does not do reranking but Cedar and Cypress do.",
          "score": 1,
          "created_utc": "2026-02-03 00:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b40my",
          "author": "primoco",
          "text": "I‚Äôve been banging my head against the same wall with enterprise RAG for months, and you're spot on. The \"toy\" setups like basic MCP or vanilla LangChain wrappers just fall apart the second you feed them high-density documents.\n\nIn my experience, if you aren't obsessing over the retrieval pipeline before the query even hits the LLM, you're just building a very expensive hallucination machine. A few things I‚Äôve learned the hard way:\n\n1. **Hybrid search is the only way out.** If you rely only on vector embeddings for factual stuff (like specific dates or IDs in a 500-page report), you‚Äôre going to get \"semantic blurring.\" You need BM25 keyword matching running alongside your vectors with a tunable alpha. It‚Äôs the only way to catch those \"needle in a haystack\" moments.\n2. **Rerankers are double-edged swords.** I‚Äôve seen Rerankers actually kill the correct result because the threshold was a hair too tight. Now I just pull a wider window (Top-K 20) and let the reranker sort the Top-5 without hard-filtering. It‚Äôs safer and much more consistent.\n3. **Small chunks > Big chunks.** We moved to 600-char chunks with a decent overlap and the \"contextual precision\" shot up. Big chunks just add too much noise and confuse the model.\n4. **Stop the \"vibe-checks.\"** You can‚Äôt tell if a RAG is good just because the answer \"sounds professional.\" I had to build a full eval pipeline to realize my \"best sounding\" model was actually making up half the citations.\n\nEnterprise RAG isn't about which LLM is smarter, it's about how much you can control the data flow.",
          "score": 1,
          "created_utc": "2026-02-03 08:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4673",
          "author": "Shekher_05",
          "text": "Ad Detected",
          "score": 1,
          "created_utc": "2026-02-03 08:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37utrg",
          "author": "Anth-Virtus",
          "text": "Hey, yeah, MCP alone isn't enough for a good RAG.\nThanks for sharing the discord link, I appreciate it",
          "score": 1,
          "created_utc": "2026-02-02 20:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ecml",
          "author": "LeadingFun1849",
          "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nHelp me improve it; you can find the link here to try it out:\n\nWebsite¬†https://dlovable.daveplanet.com\nCODE :¬†https://github.com/davidmonterocrespo24/DaveLovable",
          "score": 1,
          "created_utc": "2026-02-02 21:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdkek",
      "title": "So is RAG dead now that Claude Cowork exists, or did we just fall for another hype cycle?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "author": "ethanchen20250322",
      "created_utc": "2026-02-05 06:12:54",
      "score": 47,
      "num_comments": 23,
      "upvote_ratio": 0.76,
      "text": "Every few months someone declares RAG is dead and I have to update my resume again.\n\nThis time it's because¬†**Claude Cowork**¬†(and similar long-running agents) can \"remember\" stuff across sessions. No more context window panic. No more \"as I mentioned earlier\" when you definitely did not mention it earlier.\n\nSo naturally: \"Why do we even need RAG anymore??\"\n\nI actually dug into this and... It's not that simple (shocking, I know).\n\nBasically:\n\n* **Agent memory**¬†= remembers what IT was doing (task state)\n* **RAG**¬†= retrieves what THE WORLD knows (external facts)\n\nOne is your agent's personal journal. The other is the company wiki it keeps forgetting exists.\n\n**An agent**¬†with perfect memory but no retrieval is like a coworker who remembers every meeting but never reads the docs. We've all worked with that guy.\n\n**A RAG system**¬†with no memory is like that other coworker who reads everything but forgets what you talked about 5 minutes ago. Also that guy.\n\nTurns out the answer is: stack both. Memory for state, retrieval for facts, vector DB(Like Milvus) underneath.\n\nRAG isn't dead. It just got a roommate who leaves dishes in the sink.\n\nüëâ Full breakdown here if you want the deep dive [https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md](https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md)\n\n**TL;DR:**¬†Claude Cowork's memory is for tracking task state. RAG is for grounding the model in external knowledge. They're complementary, not competitive. We can all calm down (for now).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3oeh9p",
          "author": "stingraycharles",
          "text": "Eh, Claude Projects has been able to do that since forever, they just made that same functionality available in Claude Cowork. \n\nIf anything, this is a validation of RAGs, as this functionality is an implementation of it.",
          "score": 13,
          "created_utc": "2026-02-05 07:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3plwdr",
          "author": "TechnicalGeologist99",
          "text": "This is just a misunderstanding by people on what RAG is. \n\nIt is often conflated with: semantic search, on prem/hybrid/in-house solutions to document search. \n\nNote that RAG systems are a high level category of systems. Namely those that have the concerns:\n\n- Automatic retrieval of information \n- Injection of relevant information into a prompt for Autoregressive Causal Inference \n\n RAG does not mean \"embed and retrieve documents to guide an LLM at query time\"\n\nRather RAG is a broad topic of which \"embed and retrieve documents....\" is a member. \n\nKey word search is also RAG if you put the results into an LLM.\n\nRandom search is also RAG if you put the results into an LLM.\n\nHaving the system send an email to an office worker to ask for information at query time is still RAG (it's just not useful) \n\nRAG is not dead. It is a battle tested framework for designing systems. \n\nWhen people say \"RAG is dead\" to me that is a smell that tells me they've never written a line of code in their life and they are likely one of those jumped up sales people earning 250k per year for selling copilot licenses.",
          "score": 13,
          "created_utc": "2026-02-05 13:12:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sem33",
              "author": "ThirDiamondEye",
              "text": "I came to see people correcting the question, and am surprised I only found you.",
              "score": 2,
              "created_utc": "2026-02-05 21:20:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sljf2",
                  "author": "TechnicalGeologist99",
                  "text": "Tis a pet peeve of mine haha",
                  "score": 1,
                  "created_utc": "2026-02-05 21:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oo7f9",
          "author": "eurydice1727",
          "text": "Rag protects security. On premise solutions where companies do not want data exposed to any cloud models especially.",
          "score": 11,
          "created_utc": "2026-02-05 08:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oc06f",
          "author": "scoby_cat",
          "text": "RAG is not ‚Äúthe world‚Äù though",
          "score": 3,
          "created_utc": "2026-02-05 06:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3olrnb",
          "author": "Diligent-Builder7762",
          "text": "two ways of doing same thing, rag and custom pipelines to understand the codebase / using llm driven agents to do it... not much different, one seem to be more future leaning agentic way, the other efficiency.",
          "score": 2,
          "created_utc": "2026-02-05 08:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pb0ku",
          "author": "HealthOk5149",
          "text": "I don't think RAG will be dead anytime soon. Many companies still trying to figure out how to incorporate RAG solutions in their processes. In medical/law/financial sectors they have tons od data in air-gapped environment (due to security reasons/policies ofc especially in EU), so proper on-premise RAG would be a blessing for them imo.",
          "score": 2,
          "created_utc": "2026-02-05 11:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pwd2x",
          "author": "Diligent-Fly3756",
          "text": "https://preview.redd.it/96zneunsoohg1.jpeg?width=1107&format=pjpg&auto=webp&s=5e01383bb79ca9f81fc09569fe84dda2fefe7e9a\n\nHere‚Äôs what Boris Cherny (who invented Claude Code) said.",
          "score": 2,
          "created_utc": "2026-02-05 14:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t260c",
          "author": "zulrang",
          "text": "It doesn't matter what you label it -- it's all context retrieval.  ",
          "score": 2,
          "created_utc": "2026-02-05 23:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3urpnw",
          "author": "voycey",
          "text": "I swear each time I see something like this I am convinced that no one actually knows what RAG is",
          "score": 2,
          "created_utc": "2026-02-06 05:39:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3obawp",
          "author": "One_Milk_7025",
          "text": "Good explanation.. rag is not dead and when there is no bm25 similarity or regex doesn't work then when the rag shines.. yes claude regex find will work after 3-4 try because it will guess what can be the word for that particular query..\nA agent need both a rag and a memory.. single memory or rag won't survive solo",
          "score": 2,
          "created_utc": "2026-02-05 06:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pmoox",
              "author": "TechnicalGeologist99",
              "text": "Bm25 is not the opposite of RAG. It is a method of retrieval. Any method of retrieval still fits the RAG framework.\n\nSemantic search is an example of retrieval...it is not the definition of RAG",
              "score": 3,
              "created_utc": "2026-02-05 13:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pneuh",
          "author": "Rock--Lee",
          "text": "Ofcourse it's not dead. I'm building a custom GraphRAG with neo4j which I use as a knowledge base and memory service for my app and agents. I'd like Claude Coworker try to use its memory for multiple books and documents across domains, with user notes in there too.\n\nRAG on itself isn't something you replace. It's a technique and Claude uses RAG, but that doesn't make other RAG systems obsolete. That's like saying should I uninstall Outlook now that Claude can also fetch my mails.",
          "score": 1,
          "created_utc": "2026-02-05 13:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ptv70",
          "author": "Main-Space-3543",
          "text": "Not sure this is the right way to think about RAG.  \n\nThe alternative to RAG is not Cowork or Code.  In fact - Cursor / Anthropic used RAG to power their coding agents at one point.   \n\nAnthropic switched from RAG to agentic search because it‚Äôs allowed them to scale past the limits of RAG.  \n\nAll 3 of these things are worth paying attention and to leverage in the right context:\n\n- increasing context windows \n- RAG\n- agentic search \n\nAll 3 make it possible to improve the context of the query handed to the LLM. \n\nCoding agents / cowork - very different patterns.  \n\nBy the way - the LLM is the representation of the worlds data - not RAG.",
          "score": 1,
          "created_utc": "2026-02-05 13:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk3m0",
          "author": "Wooden_Leek_7258",
          "text": "Its a component not the system. Anyone try blending graph, vector and SQL systems in a layerd approach? Working for me.",
          "score": 1,
          "created_utc": "2026-02-05 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rlwjt",
          "author": "infinitejennifer",
          "text": "lol.",
          "score": 1,
          "created_utc": "2026-02-05 19:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sdpfo",
          "author": "satechguy",
          "text": "RAG is dead, again?",
          "score": 1,
          "created_utc": "2026-02-05 21:15:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t3eb7",
          "author": "CEBarnes",
          "text": "RAG makes using a database more flexible. Like talking to ‚ÄòMother‚Äô in the Alien series. Using AI+skills+api seems better than a fixed UI with functions mapped to buttons.",
          "score": 1,
          "created_utc": "2026-02-05 23:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tg48g",
          "author": "chungyeung",
          "text": "Rag, rag never changes.",
          "score": 1,
          "created_utc": "2026-02-06 00:37:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3x0h49",
          "author": "Challseus",
          "text": "I must be doing rag wrong because every time it‚Äôs dead, it still works for me ü§∑üèæ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-06 15:33:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xmgv6",
          "author": "jeffreyhuber",
          "text": "Cowork does retrieval augmented generation and search. ",
          "score": 1,
          "created_utc": "2026-02-06 17:17:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o41mf7m",
          "author": "Fantastic_suit143",
          "text": "Woah it didn't know that damn I will try to implement it in my own project now this is revolutionary yeah rag really is that person doesn't remember anything even after reading the context earlier on .",
          "score": 1,
          "created_utc": "2026-02-07 07:22:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qul1mq",
      "title": "NotebookLM For Teams",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:37",
      "score": 47,
      "num_comments": 5,
      "upvote_ratio": 0.96,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3i198c",
          "author": "bravelogitex",
          "text": "I'll start taking a look tomorrow, thx for sharing",
          "score": 1,
          "created_utc": "2026-02-04 09:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmssq",
          "author": "tsquig",
          "text": "Something similar here. [NotebookLM...but more](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3w4jxy",
          "author": "gg223422",
          "text": "Interesting concept. I‚Äôll skim the repo and see how the RAG and connectors are implemented",
          "score": 1,
          "created_utc": "2026-02-06 12:41:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ax6mb",
          "author": "Otherwise_Wave9374",
          "text": "Cool project. The combination of \"team chat\" + internal sources + an agent that can actually take actions is the sweet spot.\n\nIf you have not already, you might want to think about a permissions model for agent actions (read vs write, connector scopes) plus a way to show citations for every claim to keep trust high.\n\nMore agent design notes here if helpful: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 07:05:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxct95",
      "title": "My weekend project just got a $1,500 buyout offer.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "author": "Physical_Badger1281",
      "created_utc": "2026-02-06 09:08:40",
      "score": 35,
      "num_comments": 32,
      "upvote_ratio": 0.82,
      "text": "I built a simple RAG (AI) starter kit 2 months ago.\n\nThe goal was just to help devs scrape websites and PDFs for their AI chatbots without hitting anti-bot walls.\n\nProgress:\n- 10+ Sales (Organic)\n- $0 Ad Spend\n- $1,500 Acquisition Offer received yesterday.\n\nI see a lot of people overthinking their startup ideas. This is just a reminder that \"boring\" developer tools still work. I solved a scraping problem, put up a landing page, and the market responded.\n\nI'm likely going to reject the offer and keep building, but it feels good to know the asset has value.",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxct95/my_weekend_project_just_got_a_1500_buyout_offer/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3vl7fc",
          "author": "Available-Appeal-173",
          "text": "Did you open source it?",
          "score": 3,
          "created_utc": "2026-02-06 10:05:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vri0e",
              "author": "Physical_Badger1281",
              "text": "It‚Äôs built on open-source tech (Next.js, LangChain, Pinecone), but the repo itself is a paid boilerplate.\n\n\nI decided to sell it as a 'Source-Available' product because the real value isn't the stack, but the 100+ hours of glue-code (specifically the Puppeteer scraping config and Auth setup) that saves you from debugging for weeks.\nI kept the price super low ($5 range) so it‚Äôs accessible to almost anyone who wants to skip the setup.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 2,
              "created_utc": "2026-02-06 11:02:17",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3xrwyp",
                  "author": "Playwithme408",
                  "text": "Where did you list it.",
                  "score": 1,
                  "created_utc": "2026-02-06 17:43:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vsick",
          "author": "Joy_Boy_12",
          "text": "I didn't understand the use case.\nI have a chatbot but I scrape the website with docling and insert the data to vector db so it will be available for my chatbot.\n\n\nWhere's the need for your product?",
          "score": 3,
          "created_utc": "2026-02-06 11:10:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vuh35",
              "author": "Physical_Badger1281",
              "text": "Great question. If you're scraping static documentation or PDFs, tools like Docling or standard parsers work perfectly.\n\nThe specific use case FastRAG solves is Client-Side Rendered (SPA) websites. A lot of modern React/Next.js sites return empty HTML until the JavaScript runs. Standard scrapers often fail there.\n\nFastRAG uses a headless browser instance (Puppeteer) to fully hydrate the DOM before scraping, so you catch the content that only appears after JS execution. It‚Äôs overkill for simple docs, but necessary for a robust 'Chat with Any Website' SaaS.\n\n[fastrag.live](https://www.fastrag.live)",
              "score": 6,
              "created_utc": "2026-02-06 11:27:31",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o3vy3ns",
              "author": "motorsportlife",
              "text": "Curious if you are you hashing files to ensure you don't scrape the same document twice and duplicate entries in the db",
              "score": 2,
              "created_utc": "2026-02-06 11:55:45",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3w1kej",
                  "author": "Physical_Badger1281",
                  "text": "Currently, we rely on URL-based upserts to prevent database duplication (same URL = overwrite existing vectors)\n\nHowever, actual Content Hashing is on the roadmap for v1.5. It would definitely save on embedding costs for pages that haven't been updated. \n\nIs there a specific library you prefer for that flow?",
                  "score": 1,
                  "created_utc": "2026-02-06 12:20:54",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3w8289",
          "author": "StatusFoundation5472",
          "text": "True story",
          "score": 1,
          "created_utc": "2026-02-06 13:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3w8fdz",
              "author": "Physical_Badger1281",
              "text": "Facts. Just gotta keep building! üöÄ",
              "score": 1,
              "created_utc": "2026-02-06 13:05:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wcqtf",
          "author": "StatusFoundation5472",
          "text": "I checked out your landing page. A question please. What's your experience with gumroad?",
          "score": 1,
          "created_utc": "2026-02-06 13:30:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z0uxk",
              "author": "AnxietyPrudent1425",
              "text": "Lemon Squeezy is the way to go. Especially if you sell licenses.",
              "score": 1,
              "created_utc": "2026-02-06 21:22:08",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o40tefk",
                  "author": "Physical_Badger1281",
                  "text": "Lemon Squeezy, okay I'll surely give it a try.\nThanks!",
                  "score": 1,
                  "created_utc": "2026-02-07 03:34:00",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3wdpah",
          "author": "Interesting-Town-433",
          "text": "How are you able to run an llm on your site without getting crushed by gpu cost?",
          "score": 1,
          "created_utc": "2026-02-06 13:35:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3weh0l",
              "author": "Physical_Badger1281",
              "text": "Great question. Since this is utilizing OpenAI's API, there are no fixed GPU costs - it's purely pay-as-you-go.\n\nFor the live demo, I default to gpt-4o-mini, which is incredibly cheap. I also have rate limiting set up on Vercel to ensure no one user drains the API credits.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 3,
              "created_utc": "2026-02-06 13:40:05",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wg082",
          "author": "jennylane29",
          "text": "Great tool, wish you good luck!\nHow did you market your solution/landing page? I find getting it out there is particularly tricky.",
          "score": 1,
          "created_utc": "2026-02-06 13:48:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3wj8l7",
              "author": "Physical_Badger1281",
              "text": "It really is the hardest part! For this project, I stuck to a 100% organic 'Build in Public' approach:\n- Reddit: Posting deep-dives on the tech stack (like why I chose Puppeteer over Cheerio) in subreddits like r/Nextjs, r/rag and r/SaaS.\n- Twitter/X: Sharing revenue milestones and 'behind the scenes' screenshots.\nI haven't spent a dime on ads. I think developers just appreciate seeing the code/process rather than a flashy marketing video",
              "score": 3,
              "created_utc": "2026-02-06 14:05:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xj3wf",
          "author": "TalosStalioux",
          "text": "Congrats dude",
          "score": 1,
          "created_utc": "2026-02-06 17:00:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xjoq4",
              "author": "Physical_Badger1281",
              "text": "Thanks buddy!",
              "score": 1,
              "created_utc": "2026-02-06 17:03:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o407acm",
          "author": "finnomo",
          "text": "Launched projects cost at least 10 times more than that. Don't sell.",
          "score": 1,
          "created_utc": "2026-02-07 01:15:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40t1bl",
              "author": "Physical_Badger1281",
              "text": "Thanks for the sanity check! üôå\n\nSometimes when you're deep in the code, you forget how much effort it actually took to get from 'localhost' to 'live URL with Gumroad attached.'\n\nI definitely decided to hold. The validation of the offer was nice, but the asset is worth way more to me as a business.\n\n[Fastrag](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 03:31:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o434hzl",
          "author": "Top_Yogurtcloset_258",
          "text": "I might actually buy this, I've been trying to scrape client loaded data for a while using Firecrawl, and it's annoying. Have you used Firecrawl before?",
          "score": 1,
          "created_utc": "2026-02-07 14:43:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o434rek",
              "author": "Physical_Badger1281",
              "text": "No, but you can try the demo\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 14:45:28",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o436526",
                  "author": "Top_Yogurtcloset_258",
                  "text": "It gave an error when I put the link in the demo\n\nWebsite link I put: [https://partners.naeem.cg.sa/book/eleven-wishes-salon](https://partners.naeem.cg.sa/book/eleven-wishes-salon)  \n\n\nError scraping URL.\n\nError message:  \nAn error occurred with your deployment\n\nFUNCTION\\_INVOCATION\\_TIMEOUT\n\ndxb1::7fw5m-1770475864803-9e9fcb110063",
                  "score": 1,
                  "created_utc": "2026-02-07 14:53:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3z0dlp",
          "author": "AnxietyPrudent1425",
          "text": "You can go bigger. $1500 is barely worth the AI credits and food",
          "score": 1,
          "created_utc": "2026-02-06 21:19:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o40sn10",
              "author": "Physical_Badger1281",
              "text": "100%. That‚Äôs exactly why I turned it down.\n\nIf I just sell a couple of licenses a week, I beat that offer in a few months. I‚Äôd rather own a cash-flowing asset than take a quick $1.5k exit. I'm betting on the long game here!\n\n[Fastrag ](https://www.fastrag.live)",
              "score": 1,
              "created_utc": "2026-02-07 03:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvy3hv",
      "title": "Context Blindness: A Fundamental Limitation of Vector-Based RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "author": "Diligent-Fly3756",
      "created_utc": "2026-02-04 19:22:45",
      "score": 34,
      "num_comments": 12,
      "upvote_ratio": 0.98,
      "text": "**Retrieval-Augmented Generation (RAG)** has become the dominant paradigm for grounding large language models (LLMs) in external knowledge. Among RAG approaches, **vector-based retrieval**‚Äîwhich embeds documents and queries into a shared semantic space and retrieves the most semantically similar chunks‚Äîhas emerged as the de facto standard.\n\nThis dominance is understandable: vector RAG is simple, scalable, and fits naturally into existing information-retrieval pipelines. However, as LLM systems evolve from single-turn question answering toward multi-turn, agentic, and reasoning-driven applications, the limitations of vector-based RAG are becoming increasingly apparent.\n\nMany of these limitations are well known. Others are less discussed, yet far more fundamental. This article argues that **context blindness**, the inability of vector-based retrieval to condition on full conversational and reasoning context, is the most critical limitation of vector-based RAG, and one that fundamentally constrains its role in modern LLM systems.\n\n# Commonly Discussed Limitations of Vector-Based RAG\n\n**The Limitations of Semantic Similarity**\n\nVector-based retrieval assumes that semantic similarity between a query and a passage is a reliable proxy for relevance. This assumption breaks down in two fundamental ways.\n\nFirst, similarity-based retrieval often misses what should be retrieved (false negatives). User queries typically express intent rather than the literal surface form of the supporting evidence, and the information that satisfies the intent is often implicit, procedural, or distributed across multiple parts of a document. As a result, truly relevant evidence may share little semantic overlap with the query and therefore fails to be retrieved by similarity search, creating a **context gap** between what the user is trying to retrieve and what similarity search can represent.\n\nSecond, similarity-based retrieval often returns what should not be retrieved (false positives). Even when retrieved passages appear highly similar to the query, similarity does not guarantee relevance, especially in domain-specific documents such as financial reports, legal contracts, and technical manuals, where many sections share near-identical language but differ in critical details such as numerical thresholds, applicability conditions, definitions, or exceptions. Vector embeddings tend to blur these distinctions, creating **context confusion**: passages that appear relevant in isolation are retrieved despite being incorrect given the actual scope, constraints, or exceptions. In professional and enterprise settings, this failure mode is particularly dangerous because it grounds confident answers in plausible but incorrect evidence.\n\n**The Limitations of Embedding Models**\n\nEmbedding models transform passages into vector representations. However, the input length limits of the embedding model force documents to be split into chunks, disrupting their structure and introducing information discontinuities. Definitions become separated from constraints, tables from explanations, and exceptions from governing rules. Although often cited as the main limitation of vector-based RAG, chunking is better viewed as a secondary consequence of deeper architectural constraints.\n\n# The Under-Discussed Core Problem: Context Blindness\n\nA core limitation of vector-based RAG that is rarely discussed is its **context blindness**: the retrieval query cannot carry the full context that led to the question. In modern LLM applications, queries are rarely standalone. They depend on prior dialogue, intermediate conclusions, implicit assumptions, operational context, and evolving user intent. Yet vector-based retrieval operates on a short, decontextualized query that must be compressed into one or more fixed-length vectors.\n\nThis compression is not incidental ‚Äî it is fundamental. A vector embedding has limited representational capacity: it must collapse rich, structured reasoning context into a dense numerical representation that cannot faithfully preserve dependencies, conditionals, negations, or conversational state. As a result, vector-based retrieval is inherently **context-independent**. Documents are matched against a static semantic representation rather than the full reasoning state of the system. This creates a structural disconnect: the LLM reasons over a long, evolving context, while the vector retriever operates on a minimal, compressed, and flattened signal. In other words, **the LLM reasoner is stateful, while the vector retriever is not.** Even with prompt engineering, query expansion, multi-vector retrieval, or reranking, this mismatch persists, because the limitation lies in the representational bottleneck of vectors themselves. The vector retriever remains blind to the very context that determines what ‚Äúrelevant‚Äù means.\n\n# Paradigm Shift: From Context-Independent Semantic Similarity to Context-Dependent Relevance Classification\n\nThe solution to context blindness is not a better embedding model or a larger vector database, but a change in how retrieval itself is formulated. Instead of treating retrieval as a semantic similarity search performed by an external embedding model, retrieval should be framed as a **relevance classification problem** executed by an LLM that has access to the **full reasoning context**.\n\nIn this formulation, the question is no longer ‚ÄúWhich passages are closest to this query in embedding space?‚Äù, but rather ‚ÄúGiven everything the system knows so far‚Äîuser intent, prior dialogue, assumptions, and constraints‚Äîis this piece of content relevant or not?‚Äù Relevance becomes an explicit decision conditioned on context, rather than an implicit signal derived from vector proximity.\n\nBecause modern LLMs are designed to reason over long, structured context, they are naturally well-suited to this role. Unlike embedding models, which must compress inputs into fixed-length vectors and inevitably discard structure and dependencies, LLM-based relevance classification can directly condition on the entire conversation history and intermediate reasoning steps. As a result, retrieval becomes context-aware and adapts dynamically as the user‚Äôs intent evolves.\n\nThis shift transforms retrieval from a standalone preprocessing step into part of the reasoning loop itself. Instead of operating outside the LLM stack as a static similarity lookup, retrieval becomes tightly coupled with decision-making, enabling RAG systems that scale naturally to multi-turn, agentic, and long-context settings.\n\n# Scaling Relevance Classification via Tree Search\n\nA common concern with context-dependent, relevance-classification-based retrieval is token efficiency. **Naively classifying relevance over the entire knowledge base via brute-force evaluation is token-inefficient and does not scale.** However, token inefficiency is not inherent to relevance-classification-based retrieval; it arises from flat, brute-force evaluation rather than **hierarchical classification**.\n\nIn **PageIndex**, retrieval is implemented as a **hierarchical relevance classification** over document structure (sections ‚Üí pages ‚Üí blocks), where relevance is evaluated top-down and entire subtrees are pruned once a high-level unit is deemed irrelevant. This transforms retrieval from exhaustive enumeration into selective exploration, focusing computation only on promising regions. The intuition resembles systems such as **AlphaGo**, which achieved efficiency not by enumerating all possible moves, but by navigating a large decision tree through learned evaluation and selective expansion. Similarly, PageIndex avoids wasting tokens on irrelevant content, enabling context-conditioned retrieval that is both more accurate and more efficient than flat vector-based RAG pipelines that depend on large candidate sets, reranking, and repeated retrieval calls.\n\n# The Future of RAG\n\nThe rise of frameworks such as **PageIndex** signals a broader shift in the AI stack. As language models become increasingly capable of planning, reasoning, and maintaining long-horizon context, the responsibility for finding relevant information is gradually **moving** **from the database layer to the model layer**.\n\nThis transition is already evident in the coding domain. Agentic tools such as **Claude Code** are moving beyond simple vector lookups toward active codebase exploration: navigating file hierarchies, inspecting symbols, following dependencies, and iteratively refining their search based on intermediate findings. Generic document retrieval is likely to follow the same trajectory. As tasks become more multi-step and context-dependent, passive similarity search increasingly gives way to structured exploration driven by reasoning.\n\nVector databases will continue to have important, well-defined use cases, such as recommendation systems and other settings, where semantic similarity **is the objective**. However, their historical role as the default retrieval layer for LLM-based systems is becoming less clear. As retrieval shifts from similarity matching to context-dependent decision-making, agentic systems increasingly demand mechanisms that can reason, adapt, and operate over structure, rather than relying solely on embedding proximity.\n\nIn this emerging paradigm, retrieval is no longer a passive lookup operation. It becomes an integral part of the model‚Äôs reasoning process: executed by the model, guided by intent, and grounded in context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3l1d0c",
          "author": "Diligent-Fly3756",
          "text": "PageIndex's GitHub Repo: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-02-04 19:23:59",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3l468x",
              "author": "CathyCCCAAAI",
              "text": "Thanks for sharing! ",
              "score": 2,
              "created_utc": "2026-02-04 19:37:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3l3sz7",
          "author": "Pure_Squirrel175",
          "text": "Thx for sharing this, very insightful",
          "score": 2,
          "created_utc": "2026-02-04 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lme7z",
          "author": "trollsmurf",
          "text": "\"the dominant paradigm for grounding large language models (LLMs) in external knowledge\"\n\nRAG is semantic search with LLM summary and is a kludge that's used way too much for things it's not at all suited for.\n\nI propose:\n\nFuture content solutions need to generate its own code for querying / modifying whatever the user requests. No RAG/CAG, no embedding, always working on the whole corpus (including formatting) via generated code and an LLM being used for understanding intent and generating cohesive and human-understandable output. The generated code can in turn use embedding or whatever is needed to get the results requested.",
          "score": 2,
          "created_utc": "2026-02-04 21:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m25mm",
          "author": "Informal_Tangerine51",
          "text": "Context-aware retrieval is interesting but doesn't solve the accountability gap. When relevance classification retrieves wrong documents, can you prove which ones were evaluated and why they scored as relevant?\n\nYour hierarchical approach prunes irrelevant subtrees efficiently. But when an agent makes bad decision based on retrieved context, debugging needs more than \"it classified these as relevant\" - needs the actual classification scores, which documents were considered, what caused pruning at each level.\n\nWe hit this with vector RAG: retrieval happens, model decides, incident occurs, and we can't replay what was actually retrieved or how fresh it was. Context-aware retrieval improves accuracy but doesn't automatically capture decision evidence.\n\nFor production agents where compliance asks \"prove what documents informed this,\" does your system capture classification decisions as verifiable artifacts? Or focus on improving retrieval accuracy without evidence trails?",
          "score": 2,
          "created_utc": "2026-02-04 22:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o7vfs",
              "author": "Wooden_Leek_7258",
              "text": "SQL is the anwser... you just log the sql queries and the resulting document provided to the LLM. I built my system with a 'glass box' audit capacity. Vector RAGs are not meant for retrieval trying to make it work is just going to leave people dissapointed. This is what that 'Agentic Mirror' article on Medium this week is groping around.",
              "score": 2,
              "created_utc": "2026-02-05 06:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ncmo8",
              "author": "iamaiimpala",
              "text": "These are such good points. Audit trails and governance are seriously lacking in a lot of solutions, and are non-negotiable for real enterprise level adoption.",
              "score": 1,
              "created_utc": "2026-02-05 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nyles",
          "author": "Wooden_Leek_7258",
          "text": "FFS people.\n\nTL;DR Install an SQL Index. Make it query the SQL instead of probability matching.",
          "score": 2,
          "created_utc": "2026-02-05 04:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3on088",
          "author": "New_Animator_7710",
          "text": "This really nails the real failure mode of vector RAG: **relevance is context-dependent, similarity is not**. Once you move beyond single-turn QA, embeddings become a lossy compression of intent and state. Treating retrieval as LLM-conditioned relevance classification‚Äînot proximity in vector space‚Äîfeels like the inevitable shift for agentic and multi-turn systems. Vector RAG still has its place, but as a heuristic, not the reasoning layer.",
          "score": 2,
          "created_utc": "2026-02-05 08:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5ilw",
          "author": "reddefcode",
          "text": "A hybrid approach often works better; use vector search for initial broad recall, then apply LLM-based reranking or classification on a filtered candidate set.",
          "score": 1,
          "created_utc": "2026-02-05 05:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3paz4h",
          "author": "relcyoj",
          "text": "If we‚Äôre talking about enterprise corpora with many small-to-medium documents (TB scale), PageIndex is not a drop-in replacement for vector-based retrieval.\n\nVector/BM25 retrieval still wins at the **global routing problem**: quickly narrowing millions of documents down to a manageable candidate set with low latency and cost. Using PageIndex alone would require LLM-driven relevance decisions across too many documents, which doesn‚Äôt scale well in tokens or latency.",
          "score": 1,
          "created_utc": "2026-02-05 11:58:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vg554",
          "author": "nitinmms1",
          "text": "Well, another important aspect is the data on which embedding models are trained. \n\nIf that data does not contain the domain specific jargon, symbolic equivalence or taxonomy the quality of embeddings will not be good.",
          "score": 1,
          "created_utc": "2026-02-06 09:17:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxm3z3",
      "title": "I tested Opus 4.6 for RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "author": "midamurat",
      "created_utc": "2026-02-06 16:21:47",
      "score": 31,
      "num_comments": 9,
      "upvote_ratio": 0.94,
      "text": "I just finished comparing the new Opus 4.6 in a RAG setup against 11 other models.\n\n  \nThe TL;DR results I saw:\n\n* **Factual QA** king: It hit an 81.2% win rate on factual queries\n* **vs. Opus 4.5:** Massive jump in synthesis capabilities (+387 ELO), it no longer degrades as badly on multi-doc queries\n* **vs. GPT-5.1:** 4.6 is more consistent across the board, but GPT-5.1 still wins on deep, long-form synthesis.\n\nVerdict: I'm making this my default for source-critical RAG where accuracy is more imprtant than verbosity.\n\nHappy to answer questions on the data or methodology!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qxm3z3/i_tested_opus_46_for_rag/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3xv27r",
          "author": "chearmstrong",
          "text": "Using a top-tier model for answer generation in RAG is often unnecessary.\n\nOnce retrieval quality is high and you have post-retrieval steps (relevance filtering / re-ranking, dedupe, metadata filters, chunk stitching), the generator‚Äôs job is mostly summarise, structure, and stay grounded. A cheaper/faster model is usually sufficient.\n\nCommon best practice we‚Äôve seen work well:\n\n- Spend compute on retrieval quality (chunking, filters, re-ranking), not the final generator.\n- Use a fast default generator, and only escalate to a stronger model when signals suggest it‚Äôs needed (low relevance scores, sparse matches, high ambiguity, multi-doc synthesis).\n- Treat generation as a formatting + synthesis step, not the place to ‚Äúfix‚Äù weak retrieval.\n\nIn other words: if you need a very powerful model to get good answers, that‚Äôs often a retrieval problem, not a generation one.",
          "score": 31,
          "created_utc": "2026-02-06 17:58:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3yswzx",
              "author": "midamurat",
              "text": "that's right, i agree. and in this comparison , models were under fixed retrieval + reranking to keep it fair",
              "score": 0,
              "created_utc": "2026-02-06 20:42:30",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xrmzb",
          "author": "One_Milk_7025",
          "text": "What is the factual rag means? If the retrieval is perfect any llm can answer that right? How opus is better ? Multi loop or making hyde method to create hypothetical question? For factual rag llm is not needed if the fact checking and rag pipeline is good enough..",
          "score": 2,
          "created_utc": "2026-02-06 17:41:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ytrk6",
              "author": "midamurat",
              "text": "for factual rag, scifact dataset was used. in theory, what you say might work but in practice, even with the same docs, models differ (like, some over generalize or hide uncertainty). Opus 4.6 was more conservative meaning it actually sticked closer to the source than others",
              "score": -2,
              "created_utc": "2026-02-06 20:46:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xwfue",
          "author": "Informal-Resolve-831",
          "text": "So how much better it is really?",
          "score": 2,
          "created_utc": "2026-02-06 18:04:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ysg1n",
              "author": "midamurat",
              "text": "main gain is multi-doc synthesis which is about +387 ELO vs 4.5, much less degradation when sources overlap. or disagree. \n\n(and elo is score from pairwise model-vs-model comparisons using an LLM judge)",
              "score": 1,
              "created_utc": "2026-02-06 20:40:06",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xg60a",
          "author": "Legitimate-Leek4235",
          "text": "How about Gemini 3.0 flash ? Want to keep costs low and sacrifice bit on quality. How did you measure this ?",
          "score": 1,
          "created_utc": "2026-02-06 16:47:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xoyhe",
              "author": "midamurat",
              "text": "gemini 3 flash was also very good when I tested especially in terms of being strong in factual RAG. wrote about that too a while ago: [https://agentset.ai/blog/gemini-3-flash](https://agentset.ai/blog/gemini-3-flash) \n\n  \n",
              "score": 0,
              "created_utc": "2026-02-06 17:28:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3xau2g",
          "author": "midamurat",
          "text": "if interested in detail writeup: [https://agentset.ai/blog/opus-4.6-in-rag](https://agentset.ai/blog/opus-4.6-in-rag)",
          "score": 0,
          "created_utc": "2026-02-06 16:22:29",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs87ld",
      "title": "Best chunking + embedding strategy for mixed documents converted to Markdown (Docling, FAQs, web data)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "author": "Particular-Gur-1339",
      "created_utc": "2026-01-31 17:19:34",
      "score": 29,
      "num_comments": 12,
      "upvote_ratio": 0.96,
      "text": "Hey folks üëã\nI‚Äôm building a RAG pipeline and could use some advice on chunking and embedding strategies when everything is eventually normalized into Markdown.\n\nCurrent setup\n\nConverting different file types (PDFs, docs, etc.) into Markdown using Docling\nScraping website FAQ pages and storing those as Markdown as well\nEmbedding everything into a vector store for retrieval\n\nStructure of the data\nEach document/page usually has:\nA main heading\nSub-sections under that heading\nMultiple FAQs under each section\nWeb FAQs are often short Q&A pairs\n\nWhat I‚Äôm confused about\nChunking strategy\nShould I chunk by:\nPage\nHeading / sub-heading\nIndividual FAQ (Q + A as one chunk)\n\nHybrid approach (heading context + FAQ chunk)?\n\nChunk size\nFixed token size (for example 300 to 500 tokens)\nOr semantic chunks that vary in size?\nMetadata\n\n\nGoal\nHigh answer accuracy\nAvoid partial or out-of-context answers",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2xli7k",
          "author": "Dapper-Turn-3021",
          "text": "Yeah, you‚Äôre basically on the right track. There‚Äôs no single perfect chunking or embedding strategy, it really depends on your data and what you‚Äôre trying to achieve. Keeping metadata separate is correct you should embed only the actual text content and store the metadata alongside it so you can filter or rank with it later. Embedding metadata usually just adds noise.\nChunking isn‚Äôt fixed either. Sometimes smaller chunks work better, sometimes larger ones, and semantic chunking is often the best option. It‚Äôs normal to tweak your chunking strategy as you see how your retrieval performs.\n\n\nAnd you‚Äôre absolutely right about retrieval. Don‚Äôt rely only on embeddings. Combining embeddings with metadata filtering, keyword or BM25 search, and then adding a reranking step gives much better results in most cases.\n\n\nSo yes, what you described is basically how strong RAG systems are built. we are following same for Zynfo AI to build out chatbot",
          "score": 4,
          "created_utc": "2026-02-01 07:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tpw4u",
          "author": "Curious-Sample6113",
          "text": "Everything depends on your original source. You are on the right path and just have to do a lot of testing.",
          "score": 2,
          "created_utc": "2026-01-31 17:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfmn5",
              "author": "Particular-Gur-1339",
              "text": "What should be my chunking strategy?",
              "score": 1,
              "created_utc": "2026-02-01 02:18:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xc1r6",
                  "author": "Curious-Sample6113",
                  "text": "You optimize your strategy by testing. I don't know what your source looks like. For example: if you are building a legal agent then you will have a series of questions. The ones that fail will reveal the issues with your ingestion. \n\nThere isn't a universal solution. Everything is tailored to the source.",
                  "score": 1,
                  "created_utc": "2026-02-01 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ttf5n",
          "author": "jannemansonh",
          "text": "hit the same wall building doc search workflows... ended up using needle app since it handles the chunking/embedding/rag stuff automatically (has hybrid search built in). you just describe what you need and it builds it vs configuring all the pieces manually",
          "score": 1,
          "created_utc": "2026-01-31 18:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2utz79",
              "author": "Particular-Gur-1339",
              "text": "Didn't get this what is a needle app?",
              "score": 1,
              "created_utc": "2026-01-31 21:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2v26bj",
                  "author": "bwhitts66",
                  "text": "Needle is a tool that streamlines the process of building document search workflows. It automates chunking and embedding so you don‚Äôt have to set everything up manually. It‚Äôs pretty handy if you‚Äôre looking to simplify the RAG pipeline!",
                  "score": 3,
                  "created_utc": "2026-01-31 21:44:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30re8z",
          "author": "Higgs_AI",
          "text": "You‚Äôre asking the wrong question and I mean that in the most helpful way possible.\nThe chunking debate (semantic vs fixed size vs hierarchical) assumes your RAG architecture is correct. It‚Äôs not. You‚Äôre optimizing for retrieval when you should be optimizing for knowledge structure.\nHere‚Äôs the problem with your current setup. Docling to Markdown to Vectors to Retrieval. This pipeline loses the thing that makes FAQs useful, which is the relationships between questions. When you chunk Q+A as atomic units, you lose which questions are siblings under the same topic, which answers reference concepts explained elsewhere, and the hierarchy you already identified (heading to subheading to FAQ).\nWhat you actually want is to stop chunking for embedding and start structuring for reasoning.\nInstead of treating each FAQ as an isolated chunk, build a structure where each FAQ has an ID, knows what topic it belongs to, knows what other FAQs it relates to, and carries its prerequisites. Now your retrieval can find the relevant FAQ by semantic match, pull in related FAQs automatically so you don‚Äôt get out of context answers, and include parent topic context without re-embedding it every time.\nIf you‚Äôre committed to vector RAG, here‚Äôs the practical move. Chunk at FAQ level with Q and A together, you had this right. But prepend the heading hierarchy as metadata, not as embedded text. Store the path like ‚ÄúAccount Settings > Security > Password Recovery‚Äù and at retrieval time inject that context before the FAQ content. This gives you semantic search on the answer content while preserving structural context for the LLM.\nThe hybrid approach you‚Äôre circling around looks like this. Your chunk is the individual FAQ as a Q+A pair. Your metadata is the full heading path, related FAQ IDs, and section summary. Your embedding should be the question plus the first sentence of the answer, not the full text since answers tend to be verbose. At retrieval you grab your top-k FAQs plus their metadata plus the parent section summary.\nToken budget roughly 50 to 100 tokens per FAQ chunk, 20 to 30 tokens for heading context, pull 3 to 5 related FAQs and you‚Äôre at maybe 500 tokens total. That‚Äôs enough for high answer accuracy without blowing up your context window.\nTo hit your specific questions directly. Chunk by FAQ with Q and A together, yes this is correct. Use semantic size not fixed tokens since FAQs vary so let them. Metadata is your secret weapon here, the heading path, section ID, related IDs. And don‚Äôt embed the hierarchy, reference it. Embed the answer, retrieve the structure.\nIf you want to go deeper on this, look into knowledge graphs as a retrieval layer instead of pure vector search. Structure beats embedding for FAQ style content every time. And I do mean EVERY TIME! Just my opinion ü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-01 19:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30z7x9",
          "author": "Live-Guitar-8661",
          "text": "I have a thing and I‚Äôm looking for testers and willing to build a POC for free alongside. DM if you are interested",
          "score": 1,
          "created_utc": "2026-02-01 19:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o313cnn",
          "author": "ampancha",
          "text": "For FAQ-heavy Markdown, heading-anchored chunks with parent context (section title + sub-heading prepended to each chunk) consistently outperform fixed-token splits because the LLM gets retrieval results that carry their own scope. The deeper problem is that without retrieval evaluation and input validation on ingested content, any chunking strategy is an untestable guess, and scraped web pages become an injection surface the moment they land in your vector store. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 20:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xyuw",
          "author": "voycey",
          "text": "There is no single strategy that works, the reason most RAG systems fail is due to poor chunking methodologies.\n\nI will say that there is no real reason not to mix multiple chunking strategies into a single pipeline to ensure you are getting the best retrieval, especially if you are using re-ranking!",
          "score": 1,
          "created_utc": "2026-02-01 22:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32aak9",
          "author": "Odd-Affect236",
          "text": "What benefits does markdown provide when compared to simple plain text?",
          "score": 1,
          "created_utc": "2026-02-01 23:43:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv17hv",
      "title": "We open-sourced our code that outperforms RAPTOR on multi-hop retrieval",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "author": "captainPigggy",
      "created_utc": "2026-02-03 19:14:03",
      "score": 25,
      "num_comments": 7,
      "upvote_ratio": 0.87,
      "text": "We recently open-sourced a RAG system we built for internal use and figured it might be useful to others working on retrieval-heavy applications.\n\nThere‚Äôs no novel algorithm or research contribution here. The system is built by carefully combining existing techniques:\n\n* RAPTOR-style hierarchical trees\n* Knowledge graphs\n* HyDE query expansion\n* BM25 + dense hybrid search\n* Cohere reranker (this alone gave \\~+9%)\n\nOn benchmarks, it slightly outperforms RAPTOR on multi-hop retrieval (72.89% on MultiHop-RAG) and gets \\~99% retrieval accuracy on SQuAD.\n\nWe focused on making this something you can actually install, run, and modify without stitching together a dozen repos.\n\nWe built this for IncidentFox, where we use it to store and retrieve company and team knowledge. Since retrieval isn‚Äôt our product differentiator, we decided to open-source the RAG layer.\n\nRepo: [https://github.com/incidentfox/OpenRag](https://github.com/incidentfox/OpenRag)  \nWrite-up with details and benchmarks: [https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html](https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html)\n\nHappy to answer questions or hear feedback from folks building RAG systems.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ecql6",
          "author": "DashboardNight",
          "text": "Yeah, the Cohere reranker is really good. Unfortunately it remains a catastrophe with their privacy policy, where they can use anything that you provide. A local reranker may be preferable, or even a LLM-reranker using a local model or a GDPR-compliant provider. Other than that, good stuff!",
          "score": 6,
          "created_utc": "2026-02-03 19:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3elfp7",
              "author": "captainPigggy",
              "text": "good point, let it make this clear in readme",
              "score": 2,
              "created_utc": "2026-02-03 20:24:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ee7ua",
          "author": "Oshden",
          "text": "Amazing OP! Thank you for sharing this with the world at large. I‚Äôm definitely gonna star this repo!",
          "score": 3,
          "created_utc": "2026-02-03 19:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3el9hx",
              "author": "captainPigggy",
              "text": "of course thanks!",
              "score": 2,
              "created_utc": "2026-02-03 20:23:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3exmnn",
          "author": "iLoveSeiko",
          "text": "This is really cool compilation of techniques. Thanks for sharing pal",
          "score": 1,
          "created_utc": "2026-02-03 21:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1hgp",
          "author": "Regular-Forever5876",
          "text": "Thank you sir, will have a look into your implementation üôè",
          "score": 1,
          "created_utc": "2026-02-04 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gz6ty",
          "author": "WorkingOccasion902",
          "text": "Can this implement multi-tenant ?",
          "score": 1,
          "created_utc": "2026-02-04 03:59:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qxa4fq",
      "title": "Built a Website Crawler + RAG (fixed it last night üòÖ)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "author": "Cod3Conjurer",
      "created_utc": "2026-02-06 06:25:37",
      "score": 17,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "I‚Äôm **new to RAG** and learning by building projects.  \nAlmost **2 months ago** I made a very simple RAG, but the **crawler & ingestion were hallucinating**, so the answers were bad.\n\nYesterday night (after office stuff üíª), I thought:  \nEveryone is feeding PDFs‚Ä¶ **why not try something that‚Äôs not PDF ingestion?**\n\nSo I focused on fixing the **real problem ‚Äî crawling quality**.\n\nüîó GitHub: [https://github.com/AnkitNayak-eth/CrawlAI-RAG](https://github.com/AnkitNayak-eth/CrawlAI-RAG)\n\n**What‚Äôs better now:**\n\n* Playwright-based crawler (handles JS websites)\n* Clean content extraction (no navbar/footer noise)\n* Smarter chunking + deduplication\n* RAG over **entire websites**, not just PDFs\n\nBad crawling = bad RAG.\n\nIf you all want, **I can make this live / online** as well üëÄ  \nFeedback, suggestions, and ‚≠ês are welcome!",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qxa4fq/built_a_website_crawler_rag_fixed_it_last_night/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3uy709",
          "author": "Legitimate-Fun7608",
          "text": "the crawler quality insight is spot on. most people underestimate how much bad extraction ruins everything downstream.\n\n  \ncurious - how are you handling duplicate content across pages? (like shared headers/footers that make it through, or pages with similar structure). playwright helps but usually need some fuzzy matching too.",
          "score": 2,
          "created_utc": "2026-02-06 06:32:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v3m2h",
              "author": "Cod3Conjurer",
              "text": "    # Deduplication check\n\n                    import hashlib\n\n                    content_hash = hashlib.md5(copied_text.encode('utf-8')).hexdigest()\n\n                    if content_hash in content_hashes:\n\n                        print(f\"Skipping duplicate content: {clean_url}\")\n\n                        continue\n\n                    content_hashes.add(content_hash)\n\nRight now, I just strict content hashing (MD5) to catch identical pages and URL normalization. I haven't implemented footer-stripping or MinHash yet because the vector store is surprisingly creating good separation between the 'content' chunks and the 'boilerplate' chunks on its own üòÇ  \n",
              "score": 1,
              "created_utc": "2026-02-06 07:18:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv2ks2",
      "title": "Architecture breakdown: Processing 2GB+ of docs for RAG without OOM errors (Python + Generators)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "author": "jokiruiz",
      "created_utc": "2026-02-03 20:04:05",
      "score": 16,
      "num_comments": 2,
      "upvote_ratio": 0.92,
      "text": "Most RAG tutorials teach you to load a PDF into a list. That works for 5MB, but it crashes when you have 2GB of manuals or logs.\n\nI built a pipeline to handle large-scale ingestion efficiently on a consumer laptop. Here is the architecture I used to solve RAM bottlenecks and API rate limits:\n\n1. **Lazy Loading with Generators:** Instead of `docs = loader.load()`, I implemented a Python Generator (`yield`). This processes one file at a time, keeping RAM usage flat regardless of total dataset size.\n2. **Persistent Storage:** Using ChromaDB in persistent mode (on disk), not in-memory. Index once, query forever.\n3. **Smart Batching:** Sending embeddings in batches of 100 to the API with `tqdm` for monitoring, handling rate limits gracefully.\n4. **Recursive Chunking with Overlap:** Critical for maintaining semantic context across cuts.\n\nI made a full code-along video explaining the implementation line-by-line using Python and LangChain concepts.\n\n[https://youtu.be/QR-jTaHik8k?si=a\\_tfyuvG\\_mam4TEg](https://youtu.be/QR-jTaHik8k?si=a_tfyuvG_mam4TEg)\n\nIf you have questions about the `yield` implementation or the batching logic, ask away!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3et0i0",
          "author": "Oshden",
          "text": "Whoa this is awesome! Thanks for sharing!!!",
          "score": 3,
          "created_utc": "2026-02-03 21:00:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hwbel",
              "author": "jokiruiz",
              "text": "thanks! glad you like it!",
              "score": 1,
              "created_utc": "2026-02-04 08:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qvttdv",
      "title": "Bayesian BM25 blends more smoothly with vector scores (less scale mismatch than simple weighted sum)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-04 16:51:46",
      "score": 15,
      "num_comments": 3,
      "upvote_ratio": 0.95,
      "text": "when it comes to retrieval, aggregation methods really matter and yet many people use heuristics which are not always very rigorous.\n\nbm25 scores and dense similarity scores live on very different scales and distributions. Even with normalization, the balance is usually heuristic and dataset‚Äëdependent, so you often end up tuning weights per domain.\n\nrrf ignores score magnitudes and uses only rank positions. That‚Äôs robust to scale mismatch, but it can discard useful confidence information and flatten large gaps between documents, which matters when one signal is clearly stronger.\n\n\\## Experiments\n\n    Setup\n    - Dataset: SQuAD\n    - Metrics: NDCG@10, MRR@10\n    - Dense model: BGE-M3\n    - Compared: weighted-sum (WS) hybrid vs RRF\n    \n    Results\n    - WS (bb25 + Dense): NDCG@10 0.9149, MRR@10 0.8850\n    - WS (BM25 + Dense): NDCG@10 0.9051, MRR@10 0.8717\n    - RRF (BM25 + Dense): NDCG@10 0.8874, MRR@10 0.8483\n\nBayesian BM25 maps BM25 scores into calibrated probabilities using a likelihood and prior model. Once lexical scores are on a probabilistic scale, they combine more naturally with vector scores (also treated as probabilities). In practice this reduces scale mismatch and stabilizes hybrid fusion without heavy tuning.\n\nuse with \\`pip install bb25\\`. happy to share code and details if anyone‚Äôs interested. feedback welcome!\n\n\n\nRepo:¬†[https://github.com/sigridjineth/bb25](https://github.com/sigridjineth/bb25)\n\nLibrary:¬†[http://pypi.org/project/bb25/](http://pypi.org/project/bb25/)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3kchwd",
          "author": "Routine_Paramedic_82",
          "text": "Will test this, looks interesting",
          "score": 3,
          "created_utc": "2026-02-04 17:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r8qvj",
          "author": "Informal_Tangerine51",
          "text": "Interesting approach to scale mismatch but misses the production reliability question: when retrieval quality degrades, can you debug it?\n\nYour weighted-sum beats RRF on SQuAD, but SQuAD is clean evaluation data. In production, retrieval fails for reasons metrics don't capture: stale index, query typos, edge case queries, domain drift after model updates.\n\nWhen retrieval returns wrong documents, you need to know: which fusion method chose this, what were the raw scores, why did BM25 rank differently than dense, what would different weights have returned? Bayesian probabilities help fusion accuracy but don't help incident debugging.\n\nWe hit this with hybrid search on document intake. Metrics looked good, but specific customer queries failed mysteriously. Couldn't reconstruct why fusion chose document A over B without logging raw scores, fusion weights, and retrieval timestamps. Calibrated probabilities make fusion more principled but debugging still requires evidence capture.\n\nFor production systems: are you logging fusion decisions (raw scores, calibrated probs, final ranking) in a way that's replayable later? Or just optimizing for eval metrics?",
          "score": 0,
          "created_utc": "2026-02-05 18:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3raojv",
              "author": "Ok_Rub1689",
              "text": "I smell ai sloppish writing style on you. Scam",
              "score": 1,
              "created_utc": "2026-02-05 18:12:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qur17q",
      "title": "Best open-source embedding model for a RAG system?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "author": "Public-Air3181",
      "created_utc": "2026-02-03 12:48:20",
      "score": 15,
      "num_comments": 11,
      "upvote_ratio": 0.95,
      "text": "I‚Äôm an **entry-level AI engineer**, currently in the training phase of a project, and I could really use some guidance from people who‚Äôve done this in the real world.\n\nRight now, I‚Äôm building a **RAG-based system** focused on **manufacturing units‚Äô rules, acts, and standards** (think compliance documents, safety regulations, SOPs, policy manuals, etc.). The data is mostly **text-heavy, formal, and domain-specific**, not casual conversational data.  \nI‚Äôm at the stage where I need to finalize an **embedding model**, and I‚Äôm specifically looking for:\n\n* **Open-source embedding models**\n* Good performance for **semantic search/retrieval**\n* Works well with **long, structured regulatory text**\n* Practical for real projects (not just benchmarks)\n\nI‚Äôve come across a few options like Sentence Transformers, BGE models, and E5-based embeddings, but I‚Äôm unsure which ones actually perform best in a **RAG setup for industrial or regulatory documents**.\n\nIf you‚Äôve:\n\n* Built a RAG system in production\n* Worked with manufacturing / legal / compliance-heavy data\n* Compared embedding models beyond toy datasets\n\nI‚Äôd love to hear:\n\n* Which embedding model worked best for you and **why**\n* Any pitfalls to avoid (chunking size, dimensionality, multilingual issues, etc.)\n\nAny advice, resources, or real-world experience would be super helpful.  \nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3dh4z3",
          "author": "hrishikamath",
          "text": "Probably just start off with: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2? Then iterate over. The big con of using a bad rag model is that you need to retrieve more chunks and use a cross encoder. I used this and got a like 91% on finance bench. So it‚Äôs still a good starting point, you can optimize later. Repo: https://github.com/kamathhrishi/stratalens-ai (going to update blogpost with latest accuracy)",
          "score": 6,
          "created_utc": "2026-02-03 17:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e608x",
          "author": "thecontentengineer",
          "text": "I have tried ZeroEntropy embeddings they are the best I‚Äôve tried.\n\nYou can find them at https://zeroEntropy.dev \n\nThey were in beta when I first tried them, not sure now.",
          "score": 5,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3evjbx",
              "author": "ghita__",
              "text": "oh! hello, im the founder, thank you for mentioning us! we're indeed planning GA release soon! stay tuned for sota open-weight embeddings :)",
              "score": 8,
              "created_utc": "2026-02-03 21:11:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g85bm",
          "author": "laurentbourrelly",
          "text": "Use the filters on the leaderboard to find precisely what you are looking for.\n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)",
          "score": 3,
          "created_utc": "2026-02-04 01:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f0vwh",
          "author": "Informal_Tangerine51",
          "text": "For compliance docs, embedding quality matters but so does proving what was retrieved. You'll hit this when auditors ask \"what regulation informed this decision?\"\n\nWe use BGE-large for similar formal documents. Works well for semantic search. But when extraction is wrong, embeddings don't help you debug. Vector DB logs show query embedding, not what chunks were returned or if they were current versions.\n\nFor your use case, beyond embedding choice: how will you verify retrieved content later? Manufacturing compliance means \"prove this safety decision used regulation version X dated Y.\" Embeddings find relevant chunks, but you need to capture which chunks, from which doc version, retrieved when.\n\nPractical advice: test BGE-large vs E5-large on your actual compliance docs, not benchmarks. More important: design your RAG to store retrieval decisions (chunk IDs, doc versions, timestamps) not just return results. You'll need that evidence trail.\n\nWhat's your plan for handling doc version control when regulations update?",
          "score": 2,
          "created_utc": "2026-02-03 21:36:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gjqh9",
          "author": "sirebral",
          "text": "I really enjoy the Qwen 3 embedding models.  Even the . 6b is quite nice.",
          "score": 2,
          "created_utc": "2026-02-04 02:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ncji1",
              "author": "Disastrous-Nature269",
              "text": "Can confirm",
              "score": 1,
              "created_utc": "2026-02-05 02:36:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jfqgo",
          "author": "Academic_Track_2765",
          "text": "Hello. You can start with the MiniLM embeddings from sentence transformers, but I would encourage you to use the BGE embeddings (also available via sentence transformers). There is a evaluation dashboard that shows you which embedding models perform best for certain tasks, and you should definitely use it. \n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\nlately for my personal projects I have been using the Qwen .6b with their Qwen .6b reranker for the cross encoder stage and lastly the Qwen 3 Next 80B for synthesis. So far its been great!. I would also suggest you experiment with different embedding models. In my earlier years I did a lot of experimentation with embedding models, and BGE models performed well given they produced better results than the E5/GTR models, but the speed didn't come close to the MiniLM models. I have built many RAG systems in production for very long Health Care related documents, with varying complexity. Your biggest challenges will be handling the PII/PHI data, How to embed different document types, handling complex document structures, like nested tables in PDF files, image references etc, but you can use a vision model / ocr model to handle some of that. If you have questions just IM me directly, and I would be happy to help. ",
          "score": 2,
          "created_utc": "2026-02-04 14:58:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k5heu",
          "author": "lfnovo",
          "text": "Qwen3-embedding works wonders for me.",
          "score": 2,
          "created_utc": "2026-02-04 16:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cgc6m",
          "author": "polandtown",
          "text": "Those are the embedding models I'd use. IMO now you need to make a set of test questions - IMO ask your user group for a curated list of such that fits your use case. Then use such to test against each of the embedding models. Done. \n\nThe challenge here is to get a set of stratified example questions.",
          "score": 1,
          "created_utc": "2026-02-03 14:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gisgr",
          "author": "No_Wrongdoer41",
          "text": "embedding models can really struggle when complex reasoning is required. i have a graphrag approach built into a platform where you can upload the docs and we take care of everything else. id love for you to try it (for free) if you are willing! you can drag and drop the docs and then try out the resulting agent in our web app or via api.",
          "score": 1,
          "created_utc": "2026-02-04 02:23:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwsa95",
      "title": "Best Local RAG Setup for Internal PDFs? (RTX 6000 24GB | 256GB RAM | i9-10980XE)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwsa95/best_local_rag_setup_for_internal_pdfs_rtx_6000/",
      "author": "Stock_Ingenuity8105",
      "created_utc": "2026-02-05 17:54:06",
      "score": 13,
      "num_comments": 15,
      "upvote_ratio": 1.0,
      "text": "Hey everyone,\n\nI‚Äôm looking to build a local RAG (Retrieval-Augmented Generation) system to query our internal company documents (PDFs, guidelines, SOPs). Privacy is a priority, so I want to keep everything running locally and iam doing it on openwebui\n\nMy Hardware:\n\n‚Ä¢ GPU: NVIDIA RTX 6000 (24GB VRAM)\n\n‚Ä¢ RAM: 256GB DDR4\n\n‚Ä¢ CPU: Intel Core i9-10980XE (18 Cores)\n\nSince I have a massive amount of system RAM but am limited to 24GB of VRAM, I‚Äôm looking for the \"sweet spot\" for performance and accuracy.\n\nMy questions:\n\n1. RAG Configuration: \\* Chunking: What strategy works best for dense PDFs (tables, nested headers)? Recursive character splitting or something more semantic?\n\n‚Ä¢ Vector DB: Thinking about ChromaDB or Qdrant. Any preferences for this hardware?\n\n‚Ä¢ Search: Is simple similarity search enough, or should I implement Hybrid Search (BM25 + Vector) and a Re-ranker (like bge-reranker-v2-m3)?\n\nI'd love to hear from anyone running a similar \"high RAM / mid-VRAM\" setup. How are your inference speeds and retrieval accuracy?\n\nThanks in advance!",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwsa95/best_local_rag_setup_for_internal_pdfs_rtx_6000/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ri5jy",
          "author": "bravelogitex",
          "text": "Start backwards with evals. Docling is a solid choice",
          "score": 3,
          "created_utc": "2026-02-05 18:47:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sogas",
          "author": "exaknight21",
          "text": "Use CPU based OCR library like OCRMyPDF (with force_ocr option). Run a small, smart model with high context like 32K (enough for RAG). Use Hugging Face Transformers or vLLM or anything else you like (i prefer vLLM + qwen3:4b-instruct for RAG). \n\nUse parallel processing for your PDF + sentence transformers for reranker, dgraph for knowledge graphs, qdrant/lancedb or your favorite vector db for vector data storage.\n\nUse 500 chunks for embeddings with 10% overlap. Use Hybrid (BM25 + Lexical) for searching, the hybrid approach is very effective, especially for technical documents.\n\nAn effective RAG set up smartly searches the vector database and doesn‚Äôt necessarily require a high RAM usage. You‚Äôre essentially performing fanciest of the searches from within your vector database.\n\nFor local set up, I‚Äôd recommend dockerized pgvector + postgre, you can save metadata into postgre and maintain course.\n\nAdditionally, for embeddings, I haven‚Äôt played with CPU based embedding generation, but, you can play with it. Or throw like $5 or $10 into OpenAI and get insanely cost effective embeddings model text-embedding-3-small. I‚Äôd hard-truncate to 1024 dims, and maintain that, because if you go local, say a secondary 3060 12 GB for qwen3-0.6B for embeddings, it‚Äôs max capability is 1024 dims, and it‚Äôs also a sweet spot imho, so you‚Äôd be able to essentially scale as much as possible.",
          "score": 3,
          "created_utc": "2026-02-05 22:07:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3u5rf5",
              "author": "webmonger",
              "text": "is vector db selection matter much ? or its like selecting between postgres/sql server/mysql",
              "score": 1,
              "created_utc": "2026-02-06 03:10:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3uda3n",
                  "author": "exaknight21",
                  "text": "If you‚Äôre going production, I would personally recommend a database that allows direct object storage instead of hd/ssd. The data can grow quite a bit, depending on the type of data. I use lancedb 0.17.0 with backblaze b2 as backend. Cheap and effective for MVPs. Later, I can simply use S3 and latest LanceDB. Migration isn‚Äôt a big deal.",
                  "score": 2,
                  "created_utc": "2026-02-06 03:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o3uhdo7",
              "author": "Prothagarus",
              "text": "You say funny words magic man. I am finally starting to understand some of this but have no idea when to change my embedding model dimension size. Any pointers there? Like how to determine what is needed for what model? Or better yet any place you can point me so I can rtfm?",
              "score": 1,
              "created_utc": "2026-02-06 04:24:37",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3upl2m",
                  "author": "exaknight21",
                  "text": "Well. Vectors are numbers. Dimensions are essentially blocks of said numbers. Blocks of 768 numbers, 1024 numbers, 1536 numbers, etc. when you create a vector database, you must define how many dimensions that database will essentially be ‚Äúconfigured‚Äù for. You can have multiple databases with different dimensions, each serving strategic purpose. \n\nThe more dimensions, the more context it has, however, more does not equate to better understanding in this context. \n\nBecause we would essentially strategize to give more meaning in a set number of blocks, 1024 becomes a sweet spot. 1024 dimensions compact enough context, and via context chaining (done by your LLM), you‚Äôre able to provide ‚Äúcomprehension‚Äù of the context to your query. That query is to LLM, comprehension is for the LLM. \n\nSo why wouldn‚Äôt someone use smaller dims, or higher dims. Because compute. The smaller it is, the more it loses context (at least per my experiment on certain documents), the higher it is, the more compute it does per dim, yielding higher latency and context lose (bloating if you will). 1024 seems to be fast, efficient, and a sweet spot. \n\nI use qwen3:0.6b embedding model as well as openai‚Äôs text embedding 3 small - both at 1024 dims. OpenAI‚Äôs is hard truncated (built in logic), while the qwen3:0.6b naturally can produce 1024 but no higher than that.\n\nIn today‚Äôs world, you can ask chatgpt, or deepseek this question and get an understanding further. This is the gist of it.",
                  "score": 1,
                  "created_utc": "2026-02-06 05:23:07",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3tepch",
          "author": "digital_legacy",
          "text": "We have a nice UI for working with documents and RAG locally, no licence fees or anything. Check it out:\n\n[https://www.reddit.com/r/eMediaLibrary/](https://www.reddit.com/r/eMediaLibrary/) and  [https://emedialibrary.org/](https://emedialibrary.org/)",
          "score": 1,
          "created_utc": "2026-02-06 00:29:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3twgaa",
          "author": "proxima_centauri05",
          "text": "Hey checkout https://talkingdocuments.com, was built exactly for this. I'm looking for users to run case studies(One year free licence). If you're interested let me know.",
          "score": 1,
          "created_utc": "2026-02-06 02:14:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3u62w4",
          "author": "Glittering_Ice3647",
          "text": "just use pdf2text and chonkie for chunking, potion-32M embeddings, faiss index for search. no need vector db",
          "score": 1,
          "created_utc": "2026-02-06 03:12:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ur11l",
          "author": "voycey",
          "text": "24GB won't give you too many options, for naive RAG you should focus on a good OCR model and a good chunking strategy over most other things!",
          "score": 1,
          "created_utc": "2026-02-06 05:34:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3uzmqm",
          "author": "aieatstheworld",
          "text": "Hi, we are working on exactly this for almost a year now and our our pilot program is about to start. We can provide you the ready to go plug-and-play solution give at a good discount. Deployments are privacy focused. You retain data control. Can I share more info over DM ?",
          "score": 1,
          "created_utc": "2026-02-06 06:44:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v6ahv",
          "author": "oriol_9",
          "text": "look\n\n  \n[https://hyperlink.nexa.ai/](https://hyperlink.nexa.ai/)\n\n  \noriol from barcelona",
          "score": 1,
          "created_utc": "2026-02-06 07:43:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vc50i",
          "author": "jannemansonh",
          "text": "may want to use RAG API e.g. [https://docs.needle.app/docs/guides/hello-needle/getting-started/](https://docs.needle.app/docs/guides/hello-needle/getting-started/)",
          "score": 1,
          "created_utc": "2026-02-06 08:38:29",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qun1d9",
      "title": "Chunking strategy",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qun1d9/chunking_strategy/",
      "author": "Ordinary_Pineapple27",
      "created_utc": "2026-02-03 08:58:54",
      "score": 12,
      "num_comments": 13,
      "upvote_ratio": 0.93,
      "text": "Hi guys,\n\nNowadays I am working on Text Retrieval project where I have thousands of pdf files and the task is given a query the system should return related passage (highlighted as Google does) within documents.   \nFor text extraction, I am using paddleocr vl which is doing well so far. As most of you know, given a single pdf file, paddleocr vl returns a folder with md and json files (as set to save both md and json files) for each page. If the pdf file has 50 pages, there are 50 md and json files.   \n  \nI am having difficulty in how to do the chunking. I know that given a query, I need the page information as a metadata to show the related page and passage within documents.   \nIf I just concatenate all the md files and do one of the chunking strategies, I will lose the page information. But If I do not concatenate them, I will lose context of some passages where one half is on the first page and the other is one the next page. \n\nBesides that I am well-aware of embedding models, the RAG architecture, rerankers, etc. But no matter how good your overall architecture is, if your chunks are garbage, the retrieval results will also be garbage.\n\nThose, who have come accross with such issue, please, advice me.  \nThank you beforehand.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qun1d9/chunking_strategy/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3bargd",
          "author": "Better_Ad_3004",
          "text": "Commenting to come back later",
          "score": 2,
          "created_utc": "2026-02-03 09:14:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bep68",
              "author": "stingraycharles",
              "text": "You know that Reddit has a save function, right?",
              "score": 6,
              "created_utc": "2026-02-03 09:53:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3cdo4l",
              "author": "welcome-overlords",
              "text": "Same here. \n\nMy specific case is that there are hundreds of pages of large blueprints in pdf files.\n\nSo far ive been using aws bedrock knowledge bases and their default chunkings etc. Seems to work ok but there's a lot of improvements to be made",
              "score": 1,
              "created_utc": "2026-02-03 14:07:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlgbh",
                  "author": "_harryj",
                  "text": "You might want to look into overlapping chunking strategies where you take a few lines from the end of one page and a few from the beginning of the next. It helps maintain context while still giving you the page metadata you need. Also, consider tagging your chunks with page numbers to keep track of where each piece originated.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dryce",
          "author": "Smart_MoneyTor",
          "text": "I am working on something similar but at a much larger scale. I am still in ideation phase, but I can already tell you that there is no one-size-fits-all solution. Chunking at fixed window sizes will for sure rotten your context, so the chunking strategy is of extreme importance. \n\nMy approach, which is theoretical at this stage, is to adopt a multi-stage conditional chunking approach as follows: \n- Set a max chunk size (C_max)\n- recursively traverse the document (e.g., md) and chunk if section/subsection size <= C_max, else (i.e., section size > C_max) initiate a chunk set [c_0, .. c_i] where you chunk at max size up until the last chunk will is treated similarly to first if branch. Your metadata should maintain the link between chunks within a set, so that if one element is hit with your vector search, the whole set is retrieved. \nThe metadata may also include section header numbering/titles, page numbers etc. when you rank chunks that you‚Äôll populate the context with, you can use the page numbers associated with those in the metadata to get the snippets highlighting used chunks. \n\nYou may further try to ‚Äúcompress‚Äù the set of chunks to fit it into your context. \n\nThis is the approach I intend to work with. If anyone has any ideas on how to improve this, or thinks there are problems I‚Äôm overlooking, please feel free to chime in.",
          "score": 1,
          "created_utc": "2026-02-03 18:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e7lf7",
          "author": "thecontentengineer",
          "text": "You can join context engineering discord. Found much better help than here. Lots of smart folks. \n\nhttps://discord.gg/FC7Mw66GY",
          "score": 1,
          "created_utc": "2026-02-03 19:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6i50",
          "author": "Informal_Tangerine51",
          "text": "Page metadata versus context continuity is a real tradeoff. But beyond chunking strategy, can you verify which chunks were actually retrieved when results are wrong?\n\nFor your use case: chunk with overlap across page boundaries, store page number as metadata for each chunk. When chunk spans pages, metadata shows \"pages 5-6.\" Retrieval shows correct pages, overlap preserves context.\n\nThe gap most people miss: good chunking gets better results, but when retrieval is still wrong, can you prove which chunks were returned and whether they were from current doc version? Legal asks \"what regulation text did the system retrieve,\" you need chunk provenance not just accuracy.\n\nYour architecture focus is right - chunking quality matters first. Also plan for: how will you verify what was retrieved six months later when someone questions a result?\n\nWhat's your plan for tracking chunk lineage back to source docs?",
          "score": 1,
          "created_utc": "2026-02-03 22:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fznxa",
          "author": "Live-Guitar-8661",
          "text": "We are about to release tree based RAG and I think it‚Äôs going to be a game changer.\n\nTechnically PageIndex got there first, we were doing a hybrid strategy between splitting the doc up and turning it into chunks by section, but tree based is way more effective.\n\nShould be out next week with an OSS version to follow (hopefully next week as well)\n\nhttps://orchata.ai\n\nHMU if you wanna chat",
          "score": 1,
          "created_utc": "2026-02-04 00:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vb04g",
          "author": "ampancha",
          "text": "The page boundary problem is an architecture issue, not a chunking issue. Use overlapping chunks that cross page boundaries, but embed page numbers as metadata on each chunk (a chunk spanning pages 12-13 carries both). At query time, retrieve passages first, then resolve to pages for highlighting. The real failure mode comes later: when this hits production with thousands of PDFs, you'll need cost attribution per query and latency caps on reranking, or bills and response times drift without explanation. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-06 08:27:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o42cab0",
          "author": "stefan2017ch",
          "text": "Hi everyone, I hope this is okay to ask here. How would you respond to someone who says that ‚ÄúGoogle Drive + Gemini already does this perfectly‚Äù? I‚Äôm asking because I recently pitched a similar idea to a corporate partner and was politely dismissed with that argument, and I‚Äôm trying to understand what I might be missing.\n\nI can see data privacy / data control as one reason someone might prefer a RAG-based approach, but beyond that, what are the strongest practical reasons to build (or buy) a separate RAG solution instead of relying on Drive + Gemini? For example, are there meaningful differences in retrieval quality, governance/compliance, source attribution, admin controls, integration flexibility, cost predictability, or anything else that tends to matter in real deployments?\n\nApologies if this is a basic question. I‚Äôm genuinely trying to learn how others think about the tradeoffs, especially in a corporate setting.",
          "score": 1,
          "created_utc": "2026-02-07 11:32:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bdrs3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -1,
          "created_utc": "2026-02-03 09:44:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3chy0g",
              "author": "Trotskyist",
              "text": "pretty sure this is either a bot or the developer of needle fwiw given how they casually seem to name drop it in nearly every thread in this subreddit",
              "score": 5,
              "created_utc": "2026-02-03 14:30:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gc3jk",
                  "author": "Wimiam1",
                  "text": "If you look at their profile, you‚Äôll see they‚Äôre the dev. It‚Äôs really lame behaviour. I almost used their product, but their underhanded self promotion completely turned me off",
                  "score": 1,
                  "created_utc": "2026-02-04 01:45:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qsenyn",
      "title": "MiRAGE: A Multi-Agent Framework for Generating Multimodal, Multihop Evaluation Datasets (Paper + Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "author": "Socaplaya21",
      "created_utc": "2026-01-31 21:21:26",
      "score": 12,
      "num_comments": 0,
      "upvote_ratio": 0.88,
      "text": "TL;DR**:** We developed a multi-agent framework that generates multimodal, multihop QA pairs from technical documents (PDFs containing text, tables, charts). Unlike existing pipelines that often generate shallow questions, MiRAGE uses an adversarial verifier and expert persona injection to create complex reasoning chains (avg 2.3+ hops).\n\n**Paper:** [https://arxiv.org/abs/2601.15487](https://arxiv.org/abs/2601.15487)\n\n**Code:** [https://github.com/ChandanKSahu/MiRAGE](https://github.com/ChandanKSahu/MiRAGE)\n\nHi everyone,\n\nWe've been working on evaluating RAG systems for industrial/enterprise use cases (technical manuals, financial reports, regulations), and (as many have) we hit a recurring problem: standard benchmarks like Natural Questions or MS MARCO don't reflect the complexity of our data.\n\nMost existing eval datasets are single-hop and purely textual. In the real world, our documents are multimodal (*especially* heavy on tables/charts in our use cases) and require reasoning across disjoint sections (multi-hop).\n\nWe built and open-sourced MiRAGE, a multi-agent framework designed to automate the creation of \"Gold Standard\" evaluation datasets from your arbitrary corpora.\n\nInstead of a linear generation pipeline (which often leads to hallucinations or shallow questions), we use a swarm of specialized agents.\n\nInstead of immediate generation, we use a retrieval agent that recursively builds a semantic context window. This agent gathers scattered evidence to support complex inquiries *before* a question-answer pair is formulated, allowing the system to generate multi-hop queries (averaging >2.3 hops) rather than simple keyword lookups.\n\nWe address the reliability of synthetic data through an adversarial verification phase. A dedicated verifier agent fact-checks the generated answer against the source context to ensure factual grounding and verifies that the question does not rely on implicit context (e.g., rejecting questions like \"In the table below...\").\n\nWhile the system handles text and tables well, visual grounding remains a frontier. Our ablation studies revealed that current VLMs still rely significantly on dense textual descriptions to bridge the visual reasoning gap, when descriptions were removed, faithfulness dropped significantly.\n\nThe repo supports local and cloud API model calls. We're hoping this helps others stress test their pipelines.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvz6ps",
      "title": "POV: RAG is a triangle: Accuracy vs Latency vs Cost (you‚Äôre locked inside it)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvz6ps/pov_rag_is_a_triangle_accuracy_vs_latency_vs_cost/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-04 20:01:54",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 0.86,
      "text": "I see people treating RAG like there‚Äôs a ‚Äúbest stack‚Äù ‚Äî if you just tweak chunk size / vector DB / prompt enough.\n\nNot exactly like it. IMHO RAG is physics.\n\nYou‚Äôre always trading off accuracy, latency, and cost. Whatever you ship is locked inside that triangle.\n\n# The triangle\n\n* Accuracy: grounded answers + correct retrieval\n* Latency: time-to-answer (P95/P99)\n* Cost: $ per *successful outcome* (not per request)\n\nYou can move within the triangle. You can‚Äôt escape it.\n\n# The trade-offs (pick 2, the 3rd fights back)\n\n**Accuracy + Latency ‚Üí Cost goes up**  \nYou add: rerankers, query rewriting, multi-stage retrieval, more evals, more calls.\n\n**Accuracy + Cost ‚Üí Latency goes up**  \nYou accept slower flows: stricter gating, extraction-first, better versioning, more checks.\n\n**Latency + Cost ‚Üí Accuracy is constrained**  \nYou go cache-heavy, link-first answers, templates, smaller models, fewer steps.\n\n# Why agentic chains make it worse\n\nIf each step is \\~80% reliable, 3 steps is:\n\n0.8 √ó 0.8 √ó 0.8 = **51%**\n\nThat‚Äôs not ‚Äúpretty good.‚Äù That‚Äôs coin-flip automation with nicer UX.\n\n# My practical rule: define floors, don‚Äôt chase ‚Äúbest RAG‚Äù\n\nThere's no best RAG architecture.  \nFor every use-case, there's the minimum acceptable floor for each constraint.\n\nExamples:\n\n* Support bot: optimize accuracy+latency, pay cost, add confidence gates\n* Legal/Finance: optimize accuracy+cost, allow latency, quote-first + abstain\n* Sales/Marketing: optimize latency+cost, constrain accuracy, claim-gates + sources\n\n# Levers that actually move the needle\n\n# Accuracy levers (usually cost/latency ‚Üë)\n\n* hybrid search + metadata filters\n* reranking (cross-encoder)\n* query rewrite / decomposition\n* structure-based chunking (clauses/sections)\n* confidence gates + abstention\n* evals + regression tests\n\n# Latency levers\n\n* routing (fast path vs safe path)\n* caching (answers, retrieval results, embeddings)\n* precompute (summaries, intent bundles)\n* strict token budgets\n* fewer model calls\n\n# Cost levers\n\n* small model for routing/classification\n* extraction over generation (numbers/thresholds)\n* offline summarization (don‚Äôt pay per query)\n* reuse approved snippets/templates\n* measure cost per *resolved task*\n\n# Metrics (the bare minimum)\n\n* grounded answer rate / citation rate\n* P95/P99 latency end-to-end\n* cost per resolved conversation/task\n* abstention quality (abstain when you should)\n* unsupported-claim rate (should trend to 0)\n\nWDYT? Agree?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvz6ps/pov_rag_is_a_triangle_accuracy_vs_latency_vs_cost/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3m1jum",
          "author": "emudojo",
          "text": "I'm often within the accurate path, I hate screen rag based replied are wacky and so does our users. \n\nI'm heavy into evals and regression testing\n\nNeed to look into reranking and cache +system is slow to reply as I do have a judge in the middle to ground replies even after coming from the reasoning+rag agent",
          "score": 1,
          "created_utc": "2026-02-04 22:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3phvws",
              "author": "Donkit_AI",
              "text": "Reranking - 100%, especially if your retrieved set is noisy. And it bumps your accuracy and speed altogether.\n\nAlso a two-tier judge can do some good: chap gate -> expensive judge:\n\n* Cheap gate: ‚Äúis there adequate evidence coverage?‚Äù (retrieval score / reranker score / simple classifier).\n* Only if it passes -> run the full judge.\n\nOn top of that, measure where the time goes. There might be some optimization quick wins.",
              "score": 1,
              "created_utc": "2026-02-05 12:47:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pvypl",
                  "author": "emudojo",
                  "text": "Yep, for the most I work with structured data and I'm my case the first judge just compares tool output with the reasoning model responde (that must incorporate tool output if valid), but still even with that sometimes it just plain makes stuff, out of thin air üòÇ",
                  "score": 1,
                  "created_utc": "2026-02-05 14:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxd9xv",
      "title": "A-RAG: A new approach to Agentic RAG for efficient AI applications!",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qxd9xv/arag_a_new_approach_to_agentic_rag_for_efficient/",
      "author": "PavanBelagatti",
      "created_utc": "2026-02-06 09:39:00",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Agentic RAG sounds powerful, but it will burn your tokens like crazy.  \n  \nI was just going through this new paper that introduces a new Agentic RAG framework 'A-RAG' - A framework designed to unlock the reasoning capabilities of frontier AI models that traditional RAG systems underutilise.   \n  \nWhile Naive Agentic RAG grants models the autonomy to explore, it is limited by using only a single embedding-based retrieval tool. This makes it inefficient and less useful, as it consumes a massive amount of tokens while delivering lower accuracy than the full framework.  \n  \nTo address this, the authors created the A-RAG (Full) framework featuring hierarchical retrieval interfaces. It provides specific tools for keyword search, semantic search, and chunk reading.   \n  \nThis allows for progressive information disclosure, where the agent views brief snippets before deciding which full chunks are relevant enough to read.   \n  \nThis approach solves the \"noise\" problem of traditional systems by drastically improving context efficiency - retrieving far fewer tokens - while reaching higher accuracy.   \n  \nUltimately, A-RAG shifts the primary failure bottleneck: while traditional RAG often fails because it cannot find documents, A-RAG finds them so reliably that the only remaining challenge is the model‚Äôs reasoning quality.   \n  \nThis positions A-RAG as a truly agentic system that scales alongside advances in model intelligence.  \n  \nRead more about this new Agentic RAG framework A-RAG in the [research paper](https://arxiv.org/pdf/2602.03442).",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qxd9xv/arag_a_new_approach_to_agentic_rag_for_efficient/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3wqfqj",
          "author": "Academic_Track_2765",
          "text": "Yes! I use this approach. You can build this yourself too. I wont name names but there are certain tools that work exactly like this and you can replicate their behavior. It works very well! I am not sure if that team came up with this approach as one of the large LLM providers has been doing this for the last 4-5 months.",
          "score": 2,
          "created_utc": "2026-02-06 14:43:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3whhp3",
          "author": "digital_legacy",
          "text": "Looks cool! But how do I participate? Sounds like it does a semantic pre-pass to select the embeded records?",
          "score": 1,
          "created_utc": "2026-02-06 13:56:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vo5az",
          "author": "cat47b",
          "text": "Do you have a link to any code?",
          "score": 0,
          "created_utc": "2026-02-06 10:32:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o415bg3",
              "author": "PavanBelagatti",
              "text": "Don't know about the code. You can check the link to the paper in my post above. ",
              "score": 1,
              "created_utc": "2026-02-07 04:58:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o42q1o1",
                  "author": "cuervodelsur17",
                  "text": "A preprint is kind of useless without a github repository",
                  "score": 1,
                  "created_utc": "2026-02-07 13:19:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qwatbi",
      "title": "How does one go about validating and verify the correctness of a RAG's 'knowledge source'?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwatbi/how_does_one_go_about_validating_and_verify_the/",
      "author": "boombox_8",
      "created_utc": "2026-02-05 03:55:40",
      "score": 10,
      "num_comments": 9,
      "upvote_ratio": 0.82,
      "text": "Hey guys! I am new to the world of knowledge graphs and RAGs, and am very interested in exploring it!\n\nI am currently looking at using property graphs (neo4j to be specific) as the 'knowledge base' for RAG implementations since I've read that they're more powerful than the alternative of RDFs. In other words, I am building my RAG's 'knowledge source' using a knowledge graph\n\n\n\nThere is just one problem here I can't quite seem to crack, and that's the validation of the knowledge source (be it a vector DB, a knowledge graph, or otherwise). A RAG builds itself on the assurance that its underlying data-source is correct. But if you can't validate and verify the data-source, how do you 'trust' the RAG's output?\n\n\n\nI am seeing two schools of thought when it comes to building the data-source (assuming I am working with Knowledge Graphs here) :\n\n1) Give an LLM your documents, and ask it to output the data in the format you want (exp, 3-tuples for KGs, JSON, if you're building your data-source on JSON and so on)\n\n  \n2) Use traditional NER+NLP techniques to more deterministically extract data, and output it into the data-source you want\n\n\n\n  \nTo BUILD a decent knowledge graph however, you need a relatively large corpus of your data 'documents', potentially from various different sources, making the problem of verifying how correct the data is, hard\n\n  \nI've gone through a commonly-cited paper here on Reddit that delves into verifying the correctness *(KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction)*\n\n  \nThe paper's methodology essentially boils down to (\"Use an LLM to verify if your data source is correct, and THEN, use ANOTHER RAG as reference to verify the correctness, and THEN, use another knowledge graph as reference to verify the correctness\")\n\n  \nFor one, it feels like a chicken-egg problem. I am creating a KG-based RAG in my domain (which in and of itself is a bit on the niche side and occasionally involves transliterated language from a non-English language at times) for the first time. So there IS no pre-existing RAG or KG I can depend on for cross-referencing and verifying  \n  \n  \nSecond, I find it hard to trust an LLM with completely and accurately validating a knowledge graph if LLMs are inherently prone to hallucination (and is the reason I am shifting to a RAG-based methodology in the first place; to avoid hallucinations over a very specific domain/problem-space), because I am worried about running into the ***garbage in = garbage out*** problem \n\n\n\nI can't seem to think of any deterministic and 'scientifically rigorous' way to validate the correctness of a RAG's data-source (Especially when it comes to assigning metrics to the validation process). Web-scraping has the same problem, though I did have an idea of web-scraping from trusted sites and feeding it as context to an LLM for validation (Though again, it's non-deterministic by design)  \n  \n  \nIs there any better way to solve it, or are the above mentioned techniques the only options?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwatbi/how_does_one_go_about_validating_and_verify_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3offki",
          "author": "Esseratecades",
          "text": "I came across this problem in the early days of RAG. Essentially you can't know if your data is accurate without comparing it to data that you know is accurate, which means you already have a place where the data is accurate.\n\n\nI did write a proprietary algorithm to guarantee that the data you have is logically consistent, but accuracy can't be solved this way. Even with the algorithm that I wrote, you have to assume that the true story actually exists somewhere within the data to begin with.\n\n\nThe only way to actually do this is to only pull in data from sources that you trust.",
          "score": 3,
          "created_utc": "2026-02-05 07:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o6ykj",
          "author": "Academic_Track_2765",
          "text": "There is not a good solution to this problem besides making sure that your ground truth is the ground truth. Using a KG or a standard RAG won‚Äôt change that one bit. Let‚Äôs assume you create a KG / RAG for this sentence. ‚ÄúPresident Trump was the first US president‚Äù plus the same type of data for some other presidents. Now you build a KG/ RAG and ask the question who was the first president ? Your rag will perform a semantic search / KG path traversal and bring back Trump, but your LLM knows that‚Äôs not true, so after the synthesis stage your LLM might say that your retrieved result is not true. While it works here because the LLM was trained on this data, it will lack details about domain specific things e.g., different acronyms used internally or anything the LLM was not trained on. As you said you can do a web search to correct the result but you are back to domain issue. So make sure the ground truth is the actual truth and things will work out well!",
          "score": 2,
          "created_utc": "2026-02-05 05:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o3b5j",
          "author": "scoby_cat",
          "text": "Maybe I don‚Äôt understand. If you don‚Äôt know if the data is good why are you including it in your RAG?",
          "score": 2,
          "created_utc": "2026-02-05 05:28:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ooivs",
          "author": "eurydice1727",
          "text": "Citations. My system cites its source, and I have a full citation feature that renders the citation in a panel when clicked. In MD format or full PDF.",
          "score": 1,
          "created_utc": "2026-02-05 08:33:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3opoqx",
              "author": "GP_103",
              "text": "This. Several approaches to ‚Äúanchoring‚Äù the content block, forwarding those along through your pipeline and together, paired for retrieval.",
              "score": 1,
              "created_utc": "2026-02-05 08:44:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3prlpc",
              "author": "Academic_Track_2765",
              "text": "That‚Äôs just citation brother, if the ground truth is wrong it‚Äôs wrong, it doesn‚Äôt fix the core issue of the data being wrong while citing where it got the wrong data from.",
              "score": 1,
              "created_utc": "2026-02-05 13:45:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pt6pd",
          "author": "Informal_Tangerine51",
          "text": "The validation problem compounds at scale. We hit this on document intake where extraction errors accumulate.\n\nFor initial construction, NER+NLP (option 2) is more controllable but needs human validation on samples. We ran spaCy + custom rules on 500 docs, manually checked 10% random sample. Found 23% entity errors, 31% relationship errors. Three iterations to get under 5% error rate.\n\nTried LLM-based validation (GPT-4 checking triples) but it hallucinates plausible corrections that are wrong. Your exact concern.\n\nWhat worked: golden test sets. Manually verify 50-100 documents, extract to triples, freeze as regression fixtures. Every pipeline change re-runs against golden set. Catches degradation immediately.\n\nFor new data without external reference: consistency checks over correctness. Entity with conflicting attributes? Flag it. Relationships violating domain constraints (person born after death)? Flag it. Catches catastrophic errors, not subtle ones.\n\nFor your niche domain with transliteration - can domain experts spot-check 5% stratified samples (by source, date, entity type)? Would give you confidence bounds.\n\nWhat corpus size are you working with? Wondering if seed set manual validation is feasible before scaling.",
          "score": 1,
          "created_utc": "2026-02-05 13:54:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o41hye1",
              "author": "boombox_8",
              "text": "So for your initial construction, you hand validated them?",
              "score": 1,
              "created_utc": "2026-02-07 06:41:50",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3zj2k1",
          "author": "licjon",
          "text": "Shouldn't the user determine if the data itself is true or not? A RAG should retrieve the most likely text and use it to answer a query. If you use citations, then the user can verify for themselves. There could be situations where false information is desirable. The method of data curation depends greatly on the domain too. ",
          "score": 1,
          "created_utc": "2026-02-06 22:55:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qth7ek",
      "title": "RAG, Medical Models <20B, guardrails, and sVLMs for medical scans ?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "author": "jiii95",
      "created_utc": "2026-02-02 01:26:49",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.85,
      "text": "[](https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22Resources%22)\n\nSo, I am in the cardiovascular area, and I am looking for small models < 20B params, that can work for my rag that is dealing with structured JSON data. Do you have any suggestions ? I also suffer from some hallucinations, and I want also to imlement guardrails for my application to answer only medical questions about cardiovascular & data that is present and cited in the docs, will LLM be efficient with some prompts for guardrails or do you have something specific to offer. I am open only for open-source solutions, not enterprise paid software.  \nI am also looking for any sVLMs (Small Vision Language Models) that can take scans of the chest region or aorta and interpret them, or at least do segmentation or classification, any suggestions? If not a complete answer you have, any resources to look into?\n\nThank you very much (If you think I can cross-post in some other subreddit, please, any answer you can give and be beneficial, please)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o32vlim",
          "author": "Yablan",
          "text": "Literally guardrails then?\n\nhttps://github.com/guardrails-ai/guardrails",
          "score": 2,
          "created_utc": "2026-02-02 01:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32wh3m",
              "author": "jiii95",
              "text": "Well I want the output to be two things: output aigh cited data or only medical content about cardiovascular. Anything else such as medical advice or any otherput must marked as Off-topic and nothing as ouput",
              "score": 1,
              "created_utc": "2026-02-02 01:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35jeqi",
          "author": "sp3d2orbit",
          "text": "Have you considered a different approach? When I try to apply RAG directly to medical Data, the problem is always in the vector embeddings. No matter how I create them (frontier models or specialized models).\n\n  \nSince you already have everything in json format that means you're dealing with structured data. I would build an ontology on top of it and do ontology guiding search‚Äã. That means you're not dealing with hallucinations. And still use the llm in the parts of the pipeline where it makes sense, just not for the parts that require no hallucination.",
          "score": 1,
          "created_utc": "2026-02-02 13:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35jkrr",
              "author": "jiii95",
              "text": "Can you please elaborate more ? Very interesting",
              "score": 1,
              "created_utc": "2026-02-02 13:43:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35qm22",
                  "author": "sp3d2orbit",
                  "text": "So I don't know your exact use case. But you mentioned cardiology. If look at the ICD-10 ontology there's something like 1200 codes that are applicable to cardiology. If we look at SNOMED-CT there might be 10 times that number if you consider all the different structures disorders etc. \n\n  \nThose codes exist because there's some sort of medical workflow or billing logic or treatment that depends on them being distinct. When you try to apply RAG to this problem, your in essence hoping that you can fragment the documents into those 1200 codes or 12,000 codes depending on your scenario. \n\n  \nEven the very best models are going to have a hard time doing that with Fidelity. I find it's better to invert the problem, use the ontology first, and then then we use the llm to do generative tasks.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:22:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}