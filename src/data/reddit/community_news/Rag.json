{
  "metadata": {
    "last_updated": "2026-02-06 02:55:57",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 101,
    "file_size_bytes": 130311
  },
  "items": [
    {
      "id": "1qvjhp4",
      "title": "What are the best resources for RAG in 2026?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "author": "willjacko1",
      "created_utc": "2026-02-04 08:52:16",
      "score": 77,
      "num_comments": 22,
      "upvote_ratio": 0.98,
      "text": "I've been diving deep into RAG architectures lately and wanted to compile/crowdsource the best resources out there. Here's what I've found so far:\n\n**GitHub Repos to Star:**\n- LangChain / LlamaIndex (obviously, but they've evolved a lot)\n- Ragas - for RAG evaluation metrics\n- Chroma / Weaviate / Qdrant - vector DB options with great docs\n- RAGFlow - end-to-end RAG framework\n- Haystack by deepset\n\n**AI Startups to Watch:**\n- Pinecone (vector search infrastructure)\n- Cohere (embeddings + reranking)\n- ZeroEntropy (SoTA Rerankers & Embeddings)\n- Vectara (RAG-as-a-service)\n- LlamaIndex (now a company, not just OSS)\n\n**Communities:**\n- r/RAG (obviously lol)\n- r/LocalLLaMA (great for self-hosted RAG setups)\n- LangChain Discord\n- Context Engineers Discord\n- MLOps Community\n\n**Learning Resources:**\n- LlamaIndex docs (actually really good tutorials)\n- Pinecone learning center\n- \"Building RAG Applications\" courses popping up everywhere\n\nWhat am I missing? Especially interested in:\n1. Any lesser-known GitHub repos that are actually good?\n2. New startups doing interesting RAG work?\n3. YouTube channels or podcasts focused on RAG?\n\nDrop your favorites below üëá",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvjhp4/what_are_the_best_resources_for_rag_in_2026/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3imltg",
          "author": "Informal_Tangerine51",
          "text": "Good resource list, but missing the production angle: when RAG fails, can you debug it?\n\nAll these tools help build RAG systems. The gap: when retrieval is wrong, proving what was retrieved requires more than better embeddings or reranking. You need capture of actual chunks, doc versions, timestamps.\n\nWe use similar stack (LlamaIndex, Chroma, rerankers). When Legal asks \"what docs informed this decision,\" the tools retrieved successfully but we can't verify: were chunks stale, which doc version, why these over others.\n\nResources for building RAG are plentiful. Resources for making RAG auditable are scarce. That's the actual production blocker - not accuracy, but provability.\n\nWhat resources exist for RAG evidence capture and incident replay?",
          "score": 15,
          "created_utc": "2026-02-04 12:11:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3iuapn",
              "author": "aiwithphil",
              "text": "Well said.",
              "score": 3,
              "created_utc": "2026-02-04 13:03:03",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3plcte",
              "author": "ChlorrOfTheMask",
              "text": "Do you have recommendations on resources or tools for making RAG auditable?",
              "score": 1,
              "created_utc": "2026-02-05 13:09:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i152i",
          "author": "bravelogitex",
          "text": "no langchain, it is notorious for bad design",
          "score": 9,
          "created_utc": "2026-02-04 09:04:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i0h32",
          "author": "I_AM_HYLIAN",
          "text": "Context Engineers Discord: [https://discord.gg/F9VNyJzb](https://discord.gg/F9VNyJzb)",
          "score": 5,
          "created_utc": "2026-02-04 08:58:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i51gj",
          "author": "pgEdge_Postgres",
          "text": "Shameless self promotion:  \n  \nFor a RAG server that works out-of-the-box with PostgreSQL - [https://github.com/pgEdge/pgedge-rag-server](https://github.com/pgEdge/pgedge-rag-server) \n\nCheck out our repos, there's plenty of open-source tools that you might find helpful, like the [docloader](https://github.com/pgEdge/pgedge-docloader) or [vectorizer](https://github.com/pgEdge/pgedge-vectorizer).\n\nThe creator of the RAG server (and MCP server) wrote up a blog series on building a RAG server with PostgreSQL. It's three parts, here's part 1: [https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content](https://www.pgedge.com/blog/building-a-rag-server-with-postgresql-part-1-loading-your-content)",
          "score": 4,
          "created_utc": "2026-02-04 09:41:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i2mlk",
          "author": "ZwombleZ",
          "text": "RAG is so 2024....\n\n\nContext engineering. Agentic rag.",
          "score": 2,
          "created_utc": "2026-02-04 09:18:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k74hy",
              "author": "visarga",
              "text": "I prefer [text files with navigable links](https://pastebin.com/VLq4CpCT) between paragraphs. Works with grep, works with coding agents, but unlike RAG it does not have myopic context, links follow where logically needed, not where embedding similarity leads. And this is r/w memory, not r/o like RAG. I don't have any chunking issues either, I don't even need to get very good retrieval from the first move because the agent can explore and follow links along the graph. You know who does the same thing? any coding agent navigating a repo.",
              "score": 3,
              "created_utc": "2026-02-04 17:06:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3i31w9",
              "author": "Ok_Pomelo_5761",
              "text": "Agree! That discord server is gold actually (Context Engineers)",
              "score": 2,
              "created_utc": "2026-02-04 09:22:38",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i9mj1",
          "author": "galdahan9",
          "text": "What about aws bedrock knowledge base?",
          "score": 2,
          "created_utc": "2026-02-04 10:24:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3icamx",
          "author": "Infamous_Ad5702",
          "text": "I skipped rag. KG and indexes for me. No vector.",
          "score": 2,
          "created_utc": "2026-02-04 10:48:30",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3mzx9x",
              "author": "cockerspanielhere",
              "text": "What is KG?",
              "score": 1,
              "created_utc": "2026-02-05 01:24:12",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3n20rm",
                  "author": "Infamous_Ad5702",
                  "text": "Apologies. I normally remember not to abbreviate. It is knowledge graph. It helps to make rag much easier. No embedding and chunking when I have my index which builds the knowledge graph.",
                  "score": 1,
                  "created_utc": "2026-02-05 01:36:27",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3i3kme",
          "author": "Morphos91",
          "text": "Use postgresql for easy and free vector storage. \nYou could use ollama for local embedding models",
          "score": 1,
          "created_utc": "2026-02-04 09:27:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3iw162",
          "author": "Striking-Bluejay6155",
          "text": "If you're looking for materials on Graph-based RAG or building knowledge/context graphs, [check this out](https://www.falkordb.com/blog/implementing-agentic-memory-graphiti/)",
          "score": 1,
          "created_utc": "2026-02-04 13:13:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmvmd",
          "author": "lizziejaeger",
          "text": "Ragie.ai! https://www.ragie.ai/",
          "score": 1,
          "created_utc": "2026-02-04 15:33:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jqexs",
          "author": "ZepSweden_88",
          "text": "Check out MIT RLM paper, RAG is dead üíÄ. Beyond 16k tokens the models starts to get context rot == build the RLM paper together with any chunking RAG.",
          "score": 1,
          "created_utc": "2026-02-04 15:49:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3km1n3",
          "author": "Hansehart",
          "text": "Check out Langfuse. Its open source and help you to monitor requests and e.g tool usage. Its mandatory in production to understand what happens under the hood and to debug. You can use it on cloud or self hosted. And as other mentioned GraphRAG is superior to VectorRAG. I personally have great experience with Haystack+Langfuse+Neo4J",
          "score": 1,
          "created_utc": "2026-02-04 18:14:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o2bm1",
          "author": "ReverseBlade",
          "text": "[https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap](https://nemorize.com/roadmaps/2026-modern-ai-search-rag-roadmap)",
          "score": 1,
          "created_utc": "2026-02-05 05:20:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3qyfjb",
          "author": "DeadPukka",
          "text": "Check out [Graphlit](https://www.graphlit.com), build your context layer for AI agents.",
          "score": 1,
          "created_utc": "2026-02-05 17:16:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rl011",
          "author": "pantoniades",
          "text": "Curious- why is pinecone different from the other vector databases you list?",
          "score": 1,
          "created_utc": "2026-02-05 18:59:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3s87jj",
          "author": "Ok_Constant_9886",
          "text": "Personally recommend deepeval over ragas, everyone from my company switched over",
          "score": 1,
          "created_utc": "2026-02-05 20:49:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu5zua",
      "title": "OpenClaw enterprise setup: MCP isn't enough, you need reranking",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "author": "Queasy-Tomatillo8028",
      "created_utc": "2026-02-02 20:05:44",
      "score": 47,
      "num_comments": 11,
      "upvote_ratio": 0.89,
      "text": "OpenClaw, 145k stars in 10 weeks. Everyone's talking about MCP - how agents dynamically discover tools, decide when to use them, etc.\n\nI connected a local RAG to OpenClaw via MCP. My agent now knows when to search my docs vs use its memory.\n\n**The problem:** it was searching at the right time, but bringing back garbage.\n\n**MCP solves the WHEN, not the WHAT**\n\nMCP is powerful for orchestration:\n\n* Agent discovers tools at runtime\n* Decides on its own when to invoke `query_documents` vs answer directly\n* Stateful session, shared context\n\nBut MCP doesn't care about the quality of what your tool returns. If your RAG brings back 10 chunks and 7 are noise, the agent will still use them.\n\n**MCP = intelligence on WHEN to search** **Context Engineering = intelligence on WHAT goes into the prompt**\n\nBoth need to work together.\n\n**The WHAT: reranking**\n\nMy initial setup: hybrid search (vector + BM25), top 10 chunks, straight into context.\n\nResult: agent found the right docs but cited wrong passages. Context was polluted.\n\nThe fix: **reranking**.\n\nAfter search, a model re-scores chunks by actual relevance. You keep only top 3-5.\n\nI use **ZeroEntropy**. On enterprise content (contracts, specs), it goes from \\~40% precision to \\~85%. Classic cross-encoders (ms-marco, BGE) work for generic stuff, but on technical jargon ZeroEntropy performs better.\n\n**The full flow**\n\n    User query via WhatsApp\n        ‚Üì\n    OpenClaw decides: \"I need to search the docs\" (MCP)\n        ‚Üì\n    My RAG tool receives the query\n        ‚Üì\n    Hybrid search ‚Üí 30 candidates\n        ‚Üì\n    ZeroEntropy reranking ‚Üí top 3\n        ‚Üì\n    Only these 3 chunks enter the context\n        ‚Üì\n    Precise answer with correct citations\n\nAgent is smart about WHEN to search (MCP). Reranking ensures what it brings back is relevant (Context Engineering).\n\n**Stack**\n\n* **MCP server:** custom, exposes `query_documents`\n* **Search:** hybrid vector + BM25, RRF fusion\n* **Reranking:** ZeroEntropy\n* **Vector store:** ChromaDB\n\n**Result**\n\nBefore: agent searched at the right time but answers were approximate.\n\nAfter: WhatsApp query \"gardening obligations in my lease\" ‚Üí 3 sec ‚Üí exact paragraph, page, quote. Accurate.\n\n**The point**\n\nMCP is one building block. Reranking is another.\n\nMost MCP + RAG setups forget reranking. The agent orchestrates well but brings back noise.\n\nContext Engineering = making sure every token entering the prompt deserves its place. Reranking is how you do that on the retrieval side.\n\nShootout to some smart folks i met on this discord server who helped me figuring out a lot of things: [Context Engineering](https://discord.gg/F9VNyJzb)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qu5zua/openclaw_enterprise_setup_mcp_isnt_enough_you/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o38a4qg",
          "author": "-Cubie-",
          "text": "I like rerankers, but is this an ad?",
          "score": 6,
          "created_utc": "2026-02-02 21:32:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39d8if",
          "author": "Informal_Tangerine51",
          "text": "You solved retrieval quality but not retrieval proof. When the agent cites wrong passage despite reranking, can you replay what was actually in those top 3 chunks?\n\nWe use reranking too. Helps accuracy but doesn't solve the debugging problem. Agent extracts wrong data, we know reranking happened, but can't verify: were those top 3 chunks stale? Did reranking score change between dev and prod? What version of the docs were retrieved?\n\nMCP orchestration plus reranking gives better answers. Still can't answer \"prove what the agent saw at 2:47am on case #4521\" because logs show reranking executed, not what content passed through.\n\nFor WhatsApp queries this works great. For production agents where Legal asks for evidence, the gap is: can you capture and verify the actual retrieved content, not just that retrieval happened?\n\nDoes your setup store the reranked chunks with timestamps for replay, or just return them to the agent?",
          "score": 3,
          "created_utc": "2026-02-03 00:56:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3afsec",
              "author": "hncvj",
              "text": "I'm 100% with this person. We also do re-ranking in our projects having HIPAA compliance. We have to keep logs of every single thing, even the data that was sent to LLMs, PHI, De-id, Returned from LLMs, re-ranked, pulled from KG or Vector Database. Everything must be logged with timestamps.\n\nHowever, this depends on project to project basis. In other projects where there is no compliance and the final output is workable and is not required to be error free, it's ok to not have logs that deeper.",
              "score": 2,
              "created_utc": "2026-02-03 04:47:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37terf",
          "author": "Edcoopersound",
          "text": "What's your latency like end-to-end? From WhatsApp message to response.",
          "score": 2,
          "created_utc": "2026-02-02 20:13:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o37tqnf",
              "author": "Queasy-Tomatillo8028",
              "text": "2-3 sec total",
              "score": 1,
              "created_utc": "2026-02-02 20:15:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3828ew",
          "author": "apirateiwasmeanttobe",
          "text": "I think what people often forget is that you can put anything behind an mcp tool definition. The good mcp tools behave like a person, with some sort of agency or reactivity, answering not with a wall of text but with curated and well trimmed context. You want to minimize the amount of output so that you don't deplete the context of the calling agent.",
          "score": 1,
          "created_utc": "2026-02-02 20:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o39c8cy",
          "author": "blue-or-brown-keys",
          "text": "At Twig MCP handles  RAG noise via strategies, the Redwood(basic RAG strategy) does not do reranking but Cedar and Cypress do.",
          "score": 1,
          "created_utc": "2026-02-03 00:50:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b40my",
          "author": "primoco",
          "text": "I‚Äôve been banging my head against the same wall with enterprise RAG for months, and you're spot on. The \"toy\" setups like basic MCP or vanilla LangChain wrappers just fall apart the second you feed them high-density documents.\n\nIn my experience, if you aren't obsessing over the retrieval pipeline before the query even hits the LLM, you're just building a very expensive hallucination machine. A few things I‚Äôve learned the hard way:\n\n1. **Hybrid search is the only way out.** If you rely only on vector embeddings for factual stuff (like specific dates or IDs in a 500-page report), you‚Äôre going to get \"semantic blurring.\" You need BM25 keyword matching running alongside your vectors with a tunable alpha. It‚Äôs the only way to catch those \"needle in a haystack\" moments.\n2. **Rerankers are double-edged swords.** I‚Äôve seen Rerankers actually kill the correct result because the threshold was a hair too tight. Now I just pull a wider window (Top-K 20) and let the reranker sort the Top-5 without hard-filtering. It‚Äôs safer and much more consistent.\n3. **Small chunks > Big chunks.** We moved to 600-char chunks with a decent overlap and the \"contextual precision\" shot up. Big chunks just add too much noise and confuse the model.\n4. **Stop the \"vibe-checks.\"** You can‚Äôt tell if a RAG is good just because the answer \"sounds professional.\" I had to build a full eval pipeline to realize my \"best sounding\" model was actually making up half the citations.\n\nEnterprise RAG isn't about which LLM is smarter, it's about how much you can control the data flow.",
          "score": 1,
          "created_utc": "2026-02-03 08:08:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3b4673",
          "author": "Shekher_05",
          "text": "Ad Detected",
          "score": 1,
          "created_utc": "2026-02-03 08:10:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o37utrg",
          "author": "Anth-Virtus",
          "text": "Hey, yeah, MCP alone isn't enough for a good RAG.\nThanks for sharing the discord link, I appreciate it",
          "score": 1,
          "created_utc": "2026-02-02 20:20:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o38ecml",
          "author": "LeadingFun1849",
          "text": "I've been working on this project for a while.\n\nDaveLovable is an open-source, AI-powered web UI/UX development platform, inspired by Lovable, Vercel v0, and Google's Stitch. It combines cutting-edge AI orchestration with browser-based execution to offer the most advanced open-source alternative for rapid frontend prototyping.\n\nHelp me improve it; you can find the link here to try it out:\n\nWebsite¬†https://dlovable.daveplanet.com\nCODE :¬†https://github.com/davidmonterocrespo24/DaveLovable",
          "score": 1,
          "created_utc": "2026-02-02 21:52:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qul1mq",
      "title": "NotebookLM For Teams",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "author": "Uiqueblhats",
      "created_utc": "2026-02-03 06:56:37",
      "score": 43,
      "num_comments": 4,
      "upvote_ratio": 0.96,
      "text": "For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.\n\nIn short, it is NotebookLM for teams, as it connects any LLM to your internal knowledge sources (search engines, Drive, Calendar, Notion, Obsidian, and 15+ other connectors) and lets you chat with it in real time alongside your team.\n\nI'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.\n\nHere's a quick look at what SurfSense offers right now:\n\n**Features**\n\n* Self-Hostable (with docker support)\n* Real Time Collaborative Chats\n* Real Time Commenting\n* Deep Agentic Agent\n* RBAC (Role Based Access for Teams Members)\n* Supports Any LLM (OpenAI spec with LiteLLM)\n* 6000+ Embedding Models\n* 50+ File extensions supported (Added Docling recently)\n* Local TTS/STT support.\n* Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc\n* Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.\n\n**Upcoming Planned Features**\n\n* Slide Creation Support\n* Multilingual Podcast Support\n* Video Creation Agent\n\nGitHub: [https://github.com/MODSetter/SurfSense](https://github.com/MODSetter/SurfSense)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qul1mq/notebooklm_for_teams/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3i198c",
          "author": "bravelogitex",
          "text": "I'll start taking a look tomorrow, thx for sharing",
          "score": 1,
          "created_utc": "2026-02-04 09:05:25",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3jmssq",
          "author": "tsquig",
          "text": "Something similar here. [NotebookLM...but more](https://implicit.cloud).",
          "score": 1,
          "created_utc": "2026-02-04 15:32:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ax6mb",
          "author": "Otherwise_Wave9374",
          "text": "Cool project. The combination of \"team chat\" + internal sources + an agent that can actually take actions is the sweet spot.\n\nIf you have not already, you might want to think about a permissions model for agent actions (read vs write, connector scopes) plus a way to show citations for every claim to keep trust high.\n\nMore agent design notes here if helpful: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-03 07:05:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwdkek",
      "title": "So is RAG dead now that Claude Cowork exists, or did we just fall for another hype cycle?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "author": "ethanchen20250322",
      "created_utc": "2026-02-05 06:12:54",
      "score": 40,
      "num_comments": 19,
      "upvote_ratio": 0.77,
      "text": "Every few months someone declares RAG is dead and I have to update my resume again.\n\nThis time it's because¬†**Claude Cowork**¬†(and similar long-running agents) can \"remember\" stuff across sessions. No more context window panic. No more \"as I mentioned earlier\" when you definitely did not mention it earlier.\n\nSo naturally: \"Why do we even need RAG anymore??\"\n\nI actually dug into this and... It's not that simple (shocking, I know).\n\nBasically:\n\n* **Agent memory**¬†= remembers what IT was doing (task state)\n* **RAG**¬†= retrieves what THE WORLD knows (external facts)\n\nOne is your agent's personal journal. The other is the company wiki it keeps forgetting exists.\n\n**An agent**¬†with perfect memory but no retrieval is like a coworker who remembers every meeting but never reads the docs. We've all worked with that guy.\n\n**A RAG system**¬†with no memory is like that other coworker who reads everything but forgets what you talked about 5 minutes ago. Also that guy.\n\nTurns out the answer is: stack both. Memory for state, retrieval for facts, vector DB(Like Milvus) underneath.\n\nRAG isn't dead. It just got a roommate who leaves dishes in the sink.\n\nüëâ Full breakdown here if you want the deep dive [https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md](https://milvus.io/blog/is-rag-become-outdated-now-long-running-agents-like-claude-cowork-are-emerging.md)\n\n**TL;DR:**¬†Claude Cowork's memory is for tracking task state. RAG is for grounding the model in external knowledge. They're complementary, not competitive. We can all calm down (for now).",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwdkek/so_is_rag_dead_now_that_claude_cowork_exists_or/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3oeh9p",
          "author": "stingraycharles",
          "text": "Eh, Claude Projects has been able to do that since forever, they just made that same functionality available in Claude Cowork. \n\nIf anything, this is a validation of RAGs, as this functionality is an implementation of it.",
          "score": 14,
          "created_utc": "2026-02-05 07:00:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3oo7f9",
          "author": "eurydice1727",
          "text": "Rag protects security. On premise solutions where companies do not want data exposed to any cloud models especially.",
          "score": 10,
          "created_utc": "2026-02-05 08:30:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3plwdr",
          "author": "TechnicalGeologist99",
          "text": "This is just a misunderstanding by people on what RAG is. \n\nIt is often conflated with: semantic search, on prem/hybrid/in-house solutions to document search. \n\nNote that RAG systems are a high level category of systems. Namely those that have the concerns:\n\n- Automatic retrieval of information \n- Injection of relevant information into a prompt for Autoregressive Causal Inference \n\n RAG does not mean \"embed and retrieve documents to guide an LLM at query time\"\n\nRather RAG is a broad topic of which \"embed and retrieve documents....\" is a member. \n\nKey word search is also RAG if you put the results into an LLM.\n\nRandom search is also RAG if you put the results into an LLM.\n\nHaving the system send an email to an office worker to ask for information at query time is still RAG (it's just not useful) \n\nRAG is not dead. It is a battle tested framework for designing systems. \n\nWhen people say \"RAG is dead\" to me that is a smell that tells me they've never written a line of code in their life and they are likely one of those jumped up sales people earning 250k per year for selling copilot licenses.",
          "score": 9,
          "created_utc": "2026-02-05 13:12:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3sem33",
              "author": "ThirDiamondEye",
              "text": "I came to see people correcting the question, and am surprised I only found you.",
              "score": 2,
              "created_utc": "2026-02-05 21:20:25",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3sljf2",
                  "author": "TechnicalGeologist99",
                  "text": "Tis a pet peeve of mine haha",
                  "score": 1,
                  "created_utc": "2026-02-05 21:53:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3oc06f",
          "author": "scoby_cat",
          "text": "RAG is not ‚Äúthe world‚Äù though",
          "score": 3,
          "created_utc": "2026-02-05 06:38:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3olrnb",
          "author": "Diligent-Builder7762",
          "text": "two ways of doing same thing, rag and custom pipelines to understand the codebase / using llm driven agents to do it... not much different, one seem to be more future leaning agentic way, the other efficiency.",
          "score": 2,
          "created_utc": "2026-02-05 08:07:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pb0ku",
          "author": "HealthOk5149",
          "text": "I don't think RAG will be dead anytime soon. Many companies still trying to figure out how to incorporate RAG solutions in their processes. In medical/law/financial sectors they have tons od data in air-gapped environment (due to security reasons/policies ofc especially in EU), so proper on-premise RAG would be a blessing for them imo.",
          "score": 2,
          "created_utc": "2026-02-05 11:58:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3pwd2x",
          "author": "Diligent-Fly3756",
          "text": "https://preview.redd.it/96zneunsoohg1.jpeg?width=1107&format=pjpg&auto=webp&s=5e01383bb79ca9f81fc09569fe84dda2fefe7e9a\n\nHere‚Äôs what Boris Cherny (who invented Claude Code) said.",
          "score": 2,
          "created_utc": "2026-02-05 14:12:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t260c",
          "author": "zulrang",
          "text": "It doesn't matter what you label it -- it's all context retrieval.  ",
          "score": 2,
          "created_utc": "2026-02-05 23:18:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3obawp",
          "author": "One_Milk_7025",
          "text": "Good explanation.. rag is not dead and when there is no bm25 similarity or regex doesn't work then when the rag shines.. yes claude regex find will work after 3-4 try because it will guess what can be the word for that particular query..\nA agent need both a rag and a memory.. single memory or rag won't survive solo",
          "score": 3,
          "created_utc": "2026-02-05 06:32:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3pmoox",
              "author": "TechnicalGeologist99",
              "text": "Bm25 is not the opposite of RAG. It is a method of retrieval. Any method of retrieval still fits the RAG framework.\n\nSemantic search is an example of retrieval...it is not the definition of RAG",
              "score": 3,
              "created_utc": "2026-02-05 13:17:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pneuh",
          "author": "Rock--Lee",
          "text": "Ofcourse it's not dead. I'm building a custom GraphRAG with neo4j which I use as a knowledge base and memory service for my app and agents. I'd like Claude Coworker try to use its memory for multiple books and documents across domains, with user notes in there too.\n\nRAG on itself isn't something you replace. It's a technique and Claude uses RAG, but that doesn't make other RAG systems obsolete. That's like saying should I uninstall Outlook now that Claude can also fetch my mails.",
          "score": 1,
          "created_utc": "2026-02-05 13:21:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ptv70",
          "author": "Main-Space-3543",
          "text": "Not sure this is the right way to think about RAG.  \n\nThe alternative to RAG is not Cowork or Code.  In fact - Cursor / Anthropic used RAG to power their coding agents at one point.   \n\nAnthropic switched from RAG to agentic search because it‚Äôs allowed them to scale past the limits of RAG.  \n\nAll 3 of these things are worth paying attention and to leverage in the right context:\n\n- increasing context windows \n- RAG\n- agentic search \n\nAll 3 make it possible to improve the context of the query handed to the LLM. \n\nCoding agents / cowork - very different patterns.  \n\nBy the way - the LLM is the representation of the worlds data - not RAG.",
          "score": 1,
          "created_utc": "2026-02-05 13:58:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rk3m0",
          "author": "Wooden_Leek_7258",
          "text": "Its a component not the system. Anyone try blending graph, vector and SQL systems in a layerd approach? Working for me.",
          "score": 1,
          "created_utc": "2026-02-05 18:55:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3rlwjt",
          "author": "infinitejennifer",
          "text": "lol.",
          "score": 1,
          "created_utc": "2026-02-05 19:03:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3sdpfo",
          "author": "satechguy",
          "text": "RAG is dead, again?",
          "score": 1,
          "created_utc": "2026-02-05 21:15:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3t3eb7",
          "author": "CEBarnes",
          "text": "RAG makes using a database more flexible. Like talking to ‚ÄòMother‚Äô in the Alien series. Using AI+skills+api seems better than a fixed UI with functions mapped to buttons.",
          "score": 1,
          "created_utc": "2026-02-05 23:25:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3tg48g",
          "author": "chungyeung",
          "text": "Rag, rag never changes.",
          "score": 1,
          "created_utc": "2026-02-06 00:37:59",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs87ld",
      "title": "Best chunking + embedding strategy for mixed documents converted to Markdown (Docling, FAQs, web data)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "author": "Particular-Gur-1339",
      "created_utc": "2026-01-31 17:19:34",
      "score": 31,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "Hey folks üëã\nI‚Äôm building a RAG pipeline and could use some advice on chunking and embedding strategies when everything is eventually normalized into Markdown.\n\nCurrent setup\n\nConverting different file types (PDFs, docs, etc.) into Markdown using Docling\nScraping website FAQ pages and storing those as Markdown as well\nEmbedding everything into a vector store for retrieval\n\nStructure of the data\nEach document/page usually has:\nA main heading\nSub-sections under that heading\nMultiple FAQs under each section\nWeb FAQs are often short Q&A pairs\n\nWhat I‚Äôm confused about\nChunking strategy\nShould I chunk by:\nPage\nHeading / sub-heading\nIndividual FAQ (Q + A as one chunk)\n\nHybrid approach (heading context + FAQ chunk)?\n\nChunk size\nFixed token size (for example 300 to 500 tokens)\nOr semantic chunks that vary in size?\nMetadata\n\n\nGoal\nHigh answer accuracy\nAvoid partial or out-of-context answers",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qs87ld/best_chunking_embedding_strategy_for_mixed/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2xli7k",
          "author": "Dapper-Turn-3021",
          "text": "Yeah, you‚Äôre basically on the right track. There‚Äôs no single perfect chunking or embedding strategy, it really depends on your data and what you‚Äôre trying to achieve. Keeping metadata separate is correct you should embed only the actual text content and store the metadata alongside it so you can filter or rank with it later. Embedding metadata usually just adds noise.\nChunking isn‚Äôt fixed either. Sometimes smaller chunks work better, sometimes larger ones, and semantic chunking is often the best option. It‚Äôs normal to tweak your chunking strategy as you see how your retrieval performs.\n\n\nAnd you‚Äôre absolutely right about retrieval. Don‚Äôt rely only on embeddings. Combining embeddings with metadata filtering, keyword or BM25 search, and then adding a reranking step gives much better results in most cases.\n\n\nSo yes, what you described is basically how strong RAG systems are built. we are following same for Zynfo AI to build out chatbot",
          "score": 5,
          "created_utc": "2026-02-01 07:13:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2tpw4u",
          "author": "Curious-Sample6113",
          "text": "Everything depends on your original source. You are on the right path and just have to do a lot of testing.",
          "score": 2,
          "created_utc": "2026-01-31 17:51:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2wfmn5",
              "author": "Particular-Gur-1339",
              "text": "What should be my chunking strategy?",
              "score": 1,
              "created_utc": "2026-02-01 02:18:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2xc1r6",
                  "author": "Curious-Sample6113",
                  "text": "You optimize your strategy by testing. I don't know what your source looks like. For example: if you are building a legal agent then you will have a series of questions. The ones that fail will reveal the issues with your ingestion. \n\nThere isn't a universal solution. Everything is tailored to the source.",
                  "score": 1,
                  "created_utc": "2026-02-01 05:54:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2ttf5n",
          "author": "jannemansonh",
          "text": "hit the same wall building doc search workflows... ended up using needle app since it handles the chunking/embedding/rag stuff automatically (has hybrid search built in). you just describe what you need and it builds it vs configuring all the pieces manually",
          "score": 1,
          "created_utc": "2026-01-31 18:08:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2utz79",
              "author": "Particular-Gur-1339",
              "text": "Didn't get this what is a needle app?",
              "score": 1,
              "created_utc": "2026-01-31 21:04:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2v26bj",
                  "author": "bwhitts66",
                  "text": "Needle is a tool that streamlines the process of building document search workflows. It automates chunking and embedding so you don‚Äôt have to set everything up manually. It‚Äôs pretty handy if you‚Äôre looking to simplify the RAG pipeline!",
                  "score": 3,
                  "created_utc": "2026-01-31 21:44:46",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o30re8z",
          "author": "Higgs_AI",
          "text": "You‚Äôre asking the wrong question and I mean that in the most helpful way possible.\nThe chunking debate (semantic vs fixed size vs hierarchical) assumes your RAG architecture is correct. It‚Äôs not. You‚Äôre optimizing for retrieval when you should be optimizing for knowledge structure.\nHere‚Äôs the problem with your current setup. Docling to Markdown to Vectors to Retrieval. This pipeline loses the thing that makes FAQs useful, which is the relationships between questions. When you chunk Q+A as atomic units, you lose which questions are siblings under the same topic, which answers reference concepts explained elsewhere, and the hierarchy you already identified (heading to subheading to FAQ).\nWhat you actually want is to stop chunking for embedding and start structuring for reasoning.\nInstead of treating each FAQ as an isolated chunk, build a structure where each FAQ has an ID, knows what topic it belongs to, knows what other FAQs it relates to, and carries its prerequisites. Now your retrieval can find the relevant FAQ by semantic match, pull in related FAQs automatically so you don‚Äôt get out of context answers, and include parent topic context without re-embedding it every time.\nIf you‚Äôre committed to vector RAG, here‚Äôs the practical move. Chunk at FAQ level with Q and A together, you had this right. But prepend the heading hierarchy as metadata, not as embedded text. Store the path like ‚ÄúAccount Settings > Security > Password Recovery‚Äù and at retrieval time inject that context before the FAQ content. This gives you semantic search on the answer content while preserving structural context for the LLM.\nThe hybrid approach you‚Äôre circling around looks like this. Your chunk is the individual FAQ as a Q+A pair. Your metadata is the full heading path, related FAQ IDs, and section summary. Your embedding should be the question plus the first sentence of the answer, not the full text since answers tend to be verbose. At retrieval you grab your top-k FAQs plus their metadata plus the parent section summary.\nToken budget roughly 50 to 100 tokens per FAQ chunk, 20 to 30 tokens for heading context, pull 3 to 5 related FAQs and you‚Äôre at maybe 500 tokens total. That‚Äôs enough for high answer accuracy without blowing up your context window.\nTo hit your specific questions directly. Chunk by FAQ with Q and A together, yes this is correct. Use semantic size not fixed tokens since FAQs vary so let them. Metadata is your secret weapon here, the heading path, section ID, related IDs. And don‚Äôt embed the hierarchy, reference it. Embed the answer, retrieve the structure.\nIf you want to go deeper on this, look into knowledge graphs as a retrieval layer instead of pure vector search. Structure beats embedding for FAQ style content every time. And I do mean EVERY TIME! Just my opinion ü§∑üèΩ‚Äç‚ôÇÔ∏è",
          "score": 1,
          "created_utc": "2026-02-01 19:09:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o30z7x9",
          "author": "Live-Guitar-8661",
          "text": "I have a thing and I‚Äôm looking for testers and willing to build a POC for free alongside. DM if you are interested",
          "score": 1,
          "created_utc": "2026-02-01 19:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o313cnn",
          "author": "ampancha",
          "text": "For FAQ-heavy Markdown, heading-anchored chunks with parent context (section title + sub-heading prepended to each chunk) consistently outperform fixed-token splits because the LLM gets retrieval results that carry their own scope. The deeper problem is that without retrieval evaluation and input validation on ingested content, any chunking strategy is an untestable guess, and scraped web pages become an injection surface the moment they land in your vector store. Sent you a DM.",
          "score": 1,
          "created_utc": "2026-02-01 20:06:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o31xyuw",
          "author": "voycey",
          "text": "There is no single strategy that works, the reason most RAG systems fail is due to poor chunking methodologies.\n\nI will say that there is no real reason not to mix multiple chunking strategies into a single pipeline to ensure you are getting the best retrieval, especially if you are using re-ranking!",
          "score": 1,
          "created_utc": "2026-02-01 22:36:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o32aak9",
          "author": "Odd-Affect236",
          "text": "What benefits does markdown provide when compared to simple plain text?",
          "score": 1,
          "created_utc": "2026-02-01 23:43:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvy3hv",
      "title": "Context Blindness: A Fundamental Limitation of Vector-Based RAG",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "author": "Diligent-Fly3756",
      "created_utc": "2026-02-04 19:22:45",
      "score": 29,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "**Retrieval-Augmented Generation (RAG)** has become the dominant paradigm for grounding large language models (LLMs) in external knowledge. Among RAG approaches, **vector-based retrieval**‚Äîwhich embeds documents and queries into a shared semantic space and retrieves the most semantically similar chunks‚Äîhas emerged as the de facto standard.\n\nThis dominance is understandable: vector RAG is simple, scalable, and fits naturally into existing information-retrieval pipelines. However, as LLM systems evolve from single-turn question answering toward multi-turn, agentic, and reasoning-driven applications, the limitations of vector-based RAG are becoming increasingly apparent.\n\nMany of these limitations are well known. Others are less discussed, yet far more fundamental. This article argues that **context blindness**, the inability of vector-based retrieval to condition on full conversational and reasoning context, is the most critical limitation of vector-based RAG, and one that fundamentally constrains its role in modern LLM systems.\n\n# Commonly Discussed Limitations of Vector-Based RAG\n\n**The Limitations of Semantic Similarity**\n\nVector-based retrieval assumes that semantic similarity between a query and a passage is a reliable proxy for relevance. This assumption breaks down in two fundamental ways.\n\nFirst, similarity-based retrieval often misses what should be retrieved (false negatives). User queries typically express intent rather than the literal surface form of the supporting evidence, and the information that satisfies the intent is often implicit, procedural, or distributed across multiple parts of a document. As a result, truly relevant evidence may share little semantic overlap with the query and therefore fails to be retrieved by similarity search, creating a **context gap** between what the user is trying to retrieve and what similarity search can represent.\n\nSecond, similarity-based retrieval often returns what should not be retrieved (false positives). Even when retrieved passages appear highly similar to the query, similarity does not guarantee relevance, especially in domain-specific documents such as financial reports, legal contracts, and technical manuals, where many sections share near-identical language but differ in critical details such as numerical thresholds, applicability conditions, definitions, or exceptions. Vector embeddings tend to blur these distinctions, creating **context confusion**: passages that appear relevant in isolation are retrieved despite being incorrect given the actual scope, constraints, or exceptions. In professional and enterprise settings, this failure mode is particularly dangerous because it grounds confident answers in plausible but incorrect evidence.\n\n**The Limitations of Embedding Models**\n\nEmbedding models transform passages into vector representations. However, the input length limits of the embedding model force documents to be split into chunks, disrupting their structure and introducing information discontinuities. Definitions become separated from constraints, tables from explanations, and exceptions from governing rules. Although often cited as the main limitation of vector-based RAG, chunking is better viewed as a secondary consequence of deeper architectural constraints.\n\n# The Under-Discussed Core Problem: Context Blindness\n\nA core limitation of vector-based RAG that is rarely discussed is its **context blindness**: the retrieval query cannot carry the full context that led to the question. In modern LLM applications, queries are rarely standalone. They depend on prior dialogue, intermediate conclusions, implicit assumptions, operational context, and evolving user intent. Yet vector-based retrieval operates on a short, decontextualized query that must be compressed into one or more fixed-length vectors.\n\nThis compression is not incidental ‚Äî it is fundamental. A vector embedding has limited representational capacity: it must collapse rich, structured reasoning context into a dense numerical representation that cannot faithfully preserve dependencies, conditionals, negations, or conversational state. As a result, vector-based retrieval is inherently **context-independent**. Documents are matched against a static semantic representation rather than the full reasoning state of the system. This creates a structural disconnect: the LLM reasons over a long, evolving context, while the vector retriever operates on a minimal, compressed, and flattened signal. In other words, **the LLM reasoner is stateful, while the vector retriever is not.** Even with prompt engineering, query expansion, multi-vector retrieval, or reranking, this mismatch persists, because the limitation lies in the representational bottleneck of vectors themselves. The vector retriever remains blind to the very context that determines what ‚Äúrelevant‚Äù means.\n\n# Paradigm Shift: From Context-Independent Semantic Similarity to Context-Dependent Relevance Classification\n\nThe solution to context blindness is not a better embedding model or a larger vector database, but a change in how retrieval itself is formulated. Instead of treating retrieval as a semantic similarity search performed by an external embedding model, retrieval should be framed as a **relevance classification problem** executed by an LLM that has access to the **full reasoning context**.\n\nIn this formulation, the question is no longer ‚ÄúWhich passages are closest to this query in embedding space?‚Äù, but rather ‚ÄúGiven everything the system knows so far‚Äîuser intent, prior dialogue, assumptions, and constraints‚Äîis this piece of content relevant or not?‚Äù Relevance becomes an explicit decision conditioned on context, rather than an implicit signal derived from vector proximity.\n\nBecause modern LLMs are designed to reason over long, structured context, they are naturally well-suited to this role. Unlike embedding models, which must compress inputs into fixed-length vectors and inevitably discard structure and dependencies, LLM-based relevance classification can directly condition on the entire conversation history and intermediate reasoning steps. As a result, retrieval becomes context-aware and adapts dynamically as the user‚Äôs intent evolves.\n\nThis shift transforms retrieval from a standalone preprocessing step into part of the reasoning loop itself. Instead of operating outside the LLM stack as a static similarity lookup, retrieval becomes tightly coupled with decision-making, enabling RAG systems that scale naturally to multi-turn, agentic, and long-context settings.\n\n# Scaling Relevance Classification via Tree Search\n\nA common concern with context-dependent, relevance-classification-based retrieval is token efficiency. **Naively classifying relevance over the entire knowledge base via brute-force evaluation is token-inefficient and does not scale.** However, token inefficiency is not inherent to relevance-classification-based retrieval; it arises from flat, brute-force evaluation rather than **hierarchical classification**.\n\nIn **PageIndex**, retrieval is implemented as a **hierarchical relevance classification** over document structure (sections ‚Üí pages ‚Üí blocks), where relevance is evaluated top-down and entire subtrees are pruned once a high-level unit is deemed irrelevant. This transforms retrieval from exhaustive enumeration into selective exploration, focusing computation only on promising regions. The intuition resembles systems such as **AlphaGo**, which achieved efficiency not by enumerating all possible moves, but by navigating a large decision tree through learned evaluation and selective expansion. Similarly, PageIndex avoids wasting tokens on irrelevant content, enabling context-conditioned retrieval that is both more accurate and more efficient than flat vector-based RAG pipelines that depend on large candidate sets, reranking, and repeated retrieval calls.\n\n# The Future of RAG\n\nThe rise of frameworks such as **PageIndex** signals a broader shift in the AI stack. As language models become increasingly capable of planning, reasoning, and maintaining long-horizon context, the responsibility for finding relevant information is gradually **moving** **from the database layer to the model layer**.\n\nThis transition is already evident in the coding domain. Agentic tools such as **Claude Code** are moving beyond simple vector lookups toward active codebase exploration: navigating file hierarchies, inspecting symbols, following dependencies, and iteratively refining their search based on intermediate findings. Generic document retrieval is likely to follow the same trajectory. As tasks become more multi-step and context-dependent, passive similarity search increasingly gives way to structured exploration driven by reasoning.\n\nVector databases will continue to have important, well-defined use cases, such as recommendation systems and other settings, where semantic similarity **is the objective**. However, their historical role as the default retrieval layer for LLM-based systems is becoming less clear. As retrieval shifts from similarity matching to context-dependent decision-making, agentic systems increasingly demand mechanisms that can reason, adapt, and operate over structure, rather than relying solely on embedding proximity.\n\nIn this emerging paradigm, retrieval is no longer a passive lookup operation. It becomes an integral part of the model‚Äôs reasoning process: executed by the model, guided by intent, and grounded in context.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvy3hv/context_blindness_a_fundamental_limitation_of/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3l1d0c",
          "author": "Diligent-Fly3756",
          "text": "PageIndex's GitHub Repo: [https://github.com/VectifyAI/PageIndex](https://github.com/VectifyAI/PageIndex)",
          "score": 2,
          "created_utc": "2026-02-04 19:23:59",
          "is_submitter": true,
          "replies": [
            {
              "id": "o3l468x",
              "author": "CathyCCCAAAI",
              "text": "Thanks for sharing! ",
              "score": 2,
              "created_utc": "2026-02-04 19:37:14",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3l3sz7",
          "author": "Pure_Squirrel175",
          "text": "Thx for sharing this, very insightful",
          "score": 2,
          "created_utc": "2026-02-04 19:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3lme7z",
          "author": "trollsmurf",
          "text": "\"the dominant paradigm for grounding large language models (LLMs) in external knowledge\"\n\nRAG is semantic search with LLM summary and is a kludge that's used way too much for things it's not at all suited for.\n\nI propose:\n\nFuture content solutions need to generate its own code for querying / modifying whatever the user requests. No RAG/CAG, no embedding, always working on the whole corpus (including formatting) via generated code and an LLM being used for understanding intent and generating cohesive and human-understandable output. The generated code can in turn use embedding or whatever is needed to get the results requested.",
          "score": 2,
          "created_utc": "2026-02-04 21:04:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3m25mm",
          "author": "Informal_Tangerine51",
          "text": "Context-aware retrieval is interesting but doesn't solve the accountability gap. When relevance classification retrieves wrong documents, can you prove which ones were evaluated and why they scored as relevant?\n\nYour hierarchical approach prunes irrelevant subtrees efficiently. But when an agent makes bad decision based on retrieved context, debugging needs more than \"it classified these as relevant\" - needs the actual classification scores, which documents were considered, what caused pruning at each level.\n\nWe hit this with vector RAG: retrieval happens, model decides, incident occurs, and we can't replay what was actually retrieved or how fresh it was. Context-aware retrieval improves accuracy but doesn't automatically capture decision evidence.\n\nFor production agents where compliance asks \"prove what documents informed this,\" does your system capture classification decisions as verifiable artifacts? Or focus on improving retrieval accuracy without evidence trails?",
          "score": 2,
          "created_utc": "2026-02-04 22:19:54",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3o7vfs",
              "author": "Wooden_Leek_7258",
              "text": "SQL is the anwser... you just log the sql queries and the resulting document provided to the LLM. I built my system with a 'glass box' audit capacity. Vector RAGs are not meant for retrieval trying to make it work is just going to leave people dissapointed. This is what that 'Agentic Mirror' article on Medium this week is groping around.",
              "score": 2,
              "created_utc": "2026-02-05 06:03:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3ncmo8",
              "author": "iamaiimpala",
              "text": "These are such good points. Audit trails and governance are seriously lacking in a lot of solutions, and are non-negotiable for real enterprise level adoption.",
              "score": 1,
              "created_utc": "2026-02-05 02:37:07",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3nyles",
          "author": "Wooden_Leek_7258",
          "text": "FFS people.\n\nTL;DR Install an SQL Index. Make it query the SQL instead of probability matching.",
          "score": 2,
          "created_utc": "2026-02-05 04:54:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3on088",
          "author": "New_Animator_7710",
          "text": "This really nails the real failure mode of vector RAG: **relevance is context-dependent, similarity is not**. Once you move beyond single-turn QA, embeddings become a lossy compression of intent and state. Treating retrieval as LLM-conditioned relevance classification‚Äînot proximity in vector space‚Äîfeels like the inevitable shift for agentic and multi-turn systems. Vector RAG still has its place, but as a heuristic, not the reasoning layer.",
          "score": 2,
          "created_utc": "2026-02-05 08:19:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5ilw",
          "author": "reddefcode",
          "text": "A hybrid approach often works better; use vector search for initial broad recall, then apply LLM-based reranking or classification on a filtered candidate set.",
          "score": 1,
          "created_utc": "2026-02-05 05:44:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3paz4h",
          "author": "relcyoj",
          "text": "If we‚Äôre talking about enterprise corpora with many small-to-medium documents (TB scale), PageIndex is not a drop-in replacement for vector-based retrieval.\n\nVector/BM25 retrieval still wins at the **global routing problem**: quickly narrowing millions of documents down to a manageable candidate set with low latency and cost. Using PageIndex alone would require LLM-driven relevance decisions across too many documents, which doesn‚Äôt scale well in tokens or latency.",
          "score": 1,
          "created_utc": "2026-02-05 11:58:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qv17hv",
      "title": "We open-sourced our code that outperforms RAPTOR on multi-hop retrieval",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "author": "captainPigggy",
      "created_utc": "2026-02-03 19:14:03",
      "score": 26,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "We recently open-sourced a RAG system we built for internal use and figured it might be useful to others working on retrieval-heavy applications.\n\nThere‚Äôs no novel algorithm or research contribution here. The system is built by carefully combining existing techniques:\n\n* RAPTOR-style hierarchical trees\n* Knowledge graphs\n* HyDE query expansion\n* BM25 + dense hybrid search\n* Cohere reranker (this alone gave \\~+9%)\n\nOn benchmarks, it slightly outperforms RAPTOR on multi-hop retrieval (72.89% on MultiHop-RAG) and gets \\~99% retrieval accuracy on SQuAD.\n\nWe focused on making this something you can actually install, run, and modify without stitching together a dozen repos.\n\nWe built this for IncidentFox, where we use it to store and retrieve company and team knowledge. Since retrieval isn‚Äôt our product differentiator, we decided to open-source the RAG layer.\n\nRepo: [https://github.com/incidentfox/OpenRag](https://github.com/incidentfox/OpenRag)  \nWrite-up with details and benchmarks: [https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html](https://www.incidentfox.ai/blog/how-we-beat-raptor-rag.html)\n\nHappy to answer questions or hear feedback from folks building RAG systems.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qv17hv/we_opensourced_our_code_that_outperforms_raptor/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3ecql6",
          "author": "DashboardNight",
          "text": "Yeah, the Cohere reranker is really good. Unfortunately it remains a catastrophe with their privacy policy, where they can use anything that you provide. A local reranker may be preferable, or even a LLM-reranker using a local model or a GDPR-compliant provider. Other than that, good stuff!",
          "score": 7,
          "created_utc": "2026-02-03 19:43:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3elfp7",
              "author": "captainPigggy",
              "text": "good point, let it make this clear in readme",
              "score": 2,
              "created_utc": "2026-02-03 20:24:47",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ee7ua",
          "author": "Oshden",
          "text": "Amazing OP! Thank you for sharing this with the world at large. I‚Äôm definitely gonna star this repo!",
          "score": 3,
          "created_utc": "2026-02-03 19:50:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3el9hx",
              "author": "captainPigggy",
              "text": "of course thanks!",
              "score": 2,
              "created_utc": "2026-02-03 20:23:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o3exmnn",
          "author": "iLoveSeiko",
          "text": "This is really cool compilation of techniques. Thanks for sharing pal",
          "score": 1,
          "created_utc": "2026-02-03 21:21:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3g1hgp",
          "author": "Regular-Forever5876",
          "text": "Thank you sir, will have a look into your implementation üôè",
          "score": 1,
          "created_utc": "2026-02-04 00:45:51",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gz6ty",
          "author": "WorkingOccasion902",
          "text": "Can this implement multi-tenant ?",
          "score": 1,
          "created_utc": "2026-02-04 03:59:14",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qs2uzw",
      "title": "Building a RAG-Based Chat Assistant using Elasticsearch as a Vector Database",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "author": "Leading-Grape-6659",
      "created_utc": "2026-01-31 13:50:21",
      "score": 19,
      "num_comments": 1,
      "upvote_ratio": 0.89,
      "text": "Hi everyone üëã\n\n\n\nI recently built a simple RAG (Retrieval-Augmented Generation) chat assistant using Elasticsearch as a vector database.\n\n\n\nThe blog covers:\n\n‚Ä¢ How vector embeddings are stored in Elasticsearch\n\n‚Ä¢ Semantic retrieval using vector search\n\n‚Ä¢ How retrieved context improves LLM responses\n\n‚Ä¢ Real-world use cases like internal knowledge bots\n\n\n\nFull technical walkthrough with code and architecture here:\n\nüëâ [https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends\\_link&sk=d2006b31e40e3c3ed714c18eabf8f271](https://medium.com/@durgeshbhardwaj5100/building-a-rag-based-chat-assistant-using-elasticsearch-as-a-vector-database-2f892f6f4c94?source=friends_link&sk=d2006b31e40e3c3ed714c18eabf8f271)\n\n\n\nHappy to hear feedback or suggestions from folks working with RAG and vector databases!\n\n",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qs2uzw/building_a_ragbased_chat_assistant_using/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2sf6zr",
          "author": "cat47b",
          "text": "I did wonder about elastic but the hassle and cost of using that as a store has put me off a bit. Have you looked at Clickhouse?",
          "score": 2,
          "created_utc": "2026-01-31 13:55:08",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qur17q",
      "title": "Best open-source embedding model for a RAG system?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "author": "Public-Air3181",
      "created_utc": "2026-02-03 12:48:20",
      "score": 18,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm an **entry-level AI engineer**, currently in the training phase of a project, and I could really use some guidance from people who‚Äôve done this in the real world.\n\nRight now, I‚Äôm building a **RAG-based system** focused on **manufacturing units‚Äô rules, acts, and standards** (think compliance documents, safety regulations, SOPs, policy manuals, etc.). The data is mostly **text-heavy, formal, and domain-specific**, not casual conversational data.  \nI‚Äôm at the stage where I need to finalize an **embedding model**, and I‚Äôm specifically looking for:\n\n* **Open-source embedding models**\n* Good performance for **semantic search/retrieval**\n* Works well with **long, structured regulatory text**\n* Practical for real projects (not just benchmarks)\n\nI‚Äôve come across a few options like Sentence Transformers, BGE models, and E5-based embeddings, but I‚Äôm unsure which ones actually perform best in a **RAG setup for industrial or regulatory documents**.\n\nIf you‚Äôve:\n\n* Built a RAG system in production\n* Worked with manufacturing / legal / compliance-heavy data\n* Compared embedding models beyond toy datasets\n\nI‚Äôd love to hear:\n\n* Which embedding model worked best for you and **why**\n* Any pitfalls to avoid (chunking size, dimensionality, multilingual issues, etc.)\n\nAny advice, resources, or real-world experience would be super helpful.  \nThanks in advance üôè",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qur17q/best_opensource_embedding_model_for_a_rag_system/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3dh4z3",
          "author": "hrishikamath",
          "text": "Probably just start off with: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2? Then iterate over. The big con of using a bad rag model is that you need to retrieve more chunks and use a cross encoder. I used this and got a like 91% on finance bench. So it‚Äôs still a good starting point, you can optimize later. Repo: https://github.com/kamathhrishi/stratalens-ai (going to update blogpost with latest accuracy)",
          "score": 6,
          "created_utc": "2026-02-03 17:19:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e608x",
          "author": "thecontentengineer",
          "text": "I have tried ZeroEntropy embeddings they are the best I‚Äôve tried.\n\nYou can find them at https://zeroEntropy.dev \n\nThey were in beta when I first tried them, not sure now.",
          "score": 6,
          "created_utc": "2026-02-03 19:12:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3evjbx",
              "author": "ghita__",
              "text": "oh! hello, im the founder, thank you for mentioning us! we're indeed planning GA release soon! stay tuned for sota open-weight embeddings :)",
              "score": 9,
              "created_utc": "2026-02-03 21:11:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3g85bm",
          "author": "laurentbourrelly",
          "text": "Use the filters on the leaderboard to find precisely what you are looking for.\n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)",
          "score": 3,
          "created_utc": "2026-02-04 01:23:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f0vwh",
          "author": "Informal_Tangerine51",
          "text": "For compliance docs, embedding quality matters but so does proving what was retrieved. You'll hit this when auditors ask \"what regulation informed this decision?\"\n\nWe use BGE-large for similar formal documents. Works well for semantic search. But when extraction is wrong, embeddings don't help you debug. Vector DB logs show query embedding, not what chunks were returned or if they were current versions.\n\nFor your use case, beyond embedding choice: how will you verify retrieved content later? Manufacturing compliance means \"prove this safety decision used regulation version X dated Y.\" Embeddings find relevant chunks, but you need to capture which chunks, from which doc version, retrieved when.\n\nPractical advice: test BGE-large vs E5-large on your actual compliance docs, not benchmarks. More important: design your RAG to store retrieval decisions (chunk IDs, doc versions, timestamps) not just return results. You'll need that evidence trail.\n\nWhat's your plan for handling doc version control when regulations update?",
          "score": 2,
          "created_utc": "2026-02-03 21:36:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gjqh9",
          "author": "sirebral",
          "text": "I really enjoy the Qwen 3 embedding models.  Even the . 6b is quite nice.",
          "score": 2,
          "created_utc": "2026-02-04 02:28:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ncji1",
              "author": "Disastrous-Nature269",
              "text": "Can confirm",
              "score": 1,
              "created_utc": "2026-02-05 02:36:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3jfqgo",
          "author": "Academic_Track_2765",
          "text": "Hello. You can start with the MiniLM embeddings from sentence transformers, but I would encourage you to use the BGE embeddings (also available via sentence transformers). There is a evaluation dashboard that shows you which embedding models perform best for certain tasks, and you should definitely use it. \n\n[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n\nlately for my personal projects I have been using the Qwen .6b with their Qwen .6b reranker for the cross encoder stage and lastly the Qwen 3 Next 80B for synthesis. So far its been great!. I would also suggest you experiment with different embedding models. In my earlier years I did a lot of experimentation with embedding models, and BGE models performed well given they produced better results than the E5/GTR models, but the speed didn't come close to the MiniLM models. I have built many RAG systems in production for very long Health Care related documents, with varying complexity. Your biggest challenges will be handling the PII/PHI data, How to embed different document types, handling complex document structures, like nested tables in PDF files, image references etc, but you can use a vision model / ocr model to handle some of that. If you have questions just IM me directly, and I would be happy to help. ",
          "score": 2,
          "created_utc": "2026-02-04 14:58:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k5heu",
          "author": "lfnovo",
          "text": "Qwen3-embedding works wonders for me.",
          "score": 2,
          "created_utc": "2026-02-04 16:58:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3cgc6m",
          "author": "polandtown",
          "text": "Those are the embedding models I'd use. IMO now you need to make a set of test questions - IMO ask your user group for a curated list of such that fits your use case. Then use such to test against each of the embedding models. Done. \n\nThe challenge here is to get a set of stratified example questions.",
          "score": 1,
          "created_utc": "2026-02-03 14:22:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3gisgr",
          "author": "No_Wrongdoer41",
          "text": "embedding models can really struggle when complex reasoning is required. i have a graphrag approach built into a platform where you can upload the docs and we take care of everything else. id love for you to try it (for free) if you are willing! you can drag and drop the docs and then try out the resulting agent in our web app or via api.",
          "score": 1,
          "created_utc": "2026-02-04 02:23:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvttdv",
      "title": "Bayesian BM25 blends more smoothly with vector scores (less scale mismatch than simple weighted sum)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "author": "Ok_Rub1689",
      "created_utc": "2026-02-04 16:51:46",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "when it comes to retrieval, aggregation methods really matter and yet many people use heuristics which are not always very rigorous.\n\nbm25 scores and dense similarity scores live on very different scales and distributions. Even with normalization, the balance is usually heuristic and dataset‚Äëdependent, so you often end up tuning weights per domain.\n\nrrf ignores score magnitudes and uses only rank positions. That‚Äôs robust to scale mismatch, but it can discard useful confidence information and flatten large gaps between documents, which matters when one signal is clearly stronger.\n\n\\## Experiments\n\n    Setup\n    - Dataset: SQuAD\n    - Metrics: NDCG@10, MRR@10\n    - Dense model: BGE-M3\n    - Compared: weighted-sum (WS) hybrid vs RRF\n    \n    Results\n    - WS (bb25 + Dense): NDCG@10 0.9149, MRR@10 0.8850\n    - WS (BM25 + Dense): NDCG@10 0.9051, MRR@10 0.8717\n    - RRF (BM25 + Dense): NDCG@10 0.8874, MRR@10 0.8483\n\nBayesian BM25 maps BM25 scores into calibrated probabilities using a likelihood and prior model. Once lexical scores are on a probabilistic scale, they combine more naturally with vector scores (also treated as probabilities). In practice this reduces scale mismatch and stabilizes hybrid fusion without heavy tuning.\n\nuse with \\`pip install bb25\\`. happy to share code and details if anyone‚Äôs interested. feedback welcome!\n\n\n\nRepo:¬†[https://github.com/sigridjineth/bb25](https://github.com/sigridjineth/bb25)\n\nLibrary:¬†[http://pypi.org/project/bb25/](http://pypi.org/project/bb25/)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1qvttdv/bayesian_bm25_blends_more_smoothly_with_vector/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3kchwd",
          "author": "Routine_Paramedic_82",
          "text": "Will test this, looks interesting",
          "score": 3,
          "created_utc": "2026-02-04 17:31:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3r8qvj",
          "author": "Informal_Tangerine51",
          "text": "Interesting approach to scale mismatch but misses the production reliability question: when retrieval quality degrades, can you debug it?\n\nYour weighted-sum beats RRF on SQuAD, but SQuAD is clean evaluation data. In production, retrieval fails for reasons metrics don't capture: stale index, query typos, edge case queries, domain drift after model updates.\n\nWhen retrieval returns wrong documents, you need to know: which fusion method chose this, what were the raw scores, why did BM25 rank differently than dense, what would different weights have returned? Bayesian probabilities help fusion accuracy but don't help incident debugging.\n\nWe hit this with hybrid search on document intake. Metrics looked good, but specific customer queries failed mysteriously. Couldn't reconstruct why fusion chose document A over B without logging raw scores, fusion weights, and retrieval timestamps. Calibrated probabilities make fusion more principled but debugging still requires evidence capture.\n\nFor production systems: are you logging fusion decisions (raw scores, calibrated probs, final ranking) in a way that's replayable later? Or just optimizing for eval metrics?",
          "score": 0,
          "created_utc": "2026-02-05 18:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3raojv",
              "author": "Ok_Rub1689",
              "text": "I smell ai sloppish writing style on you. Scam",
              "score": 1,
              "created_utc": "2026-02-05 18:12:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qv2ks2",
      "title": "Architecture breakdown: Processing 2GB+ of docs for RAG without OOM errors (Python + Generators)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "author": "jokiruiz",
      "created_utc": "2026-02-03 20:04:05",
      "score": 15,
      "num_comments": 2,
      "upvote_ratio": 0.91,
      "text": "Most RAG tutorials teach you to load a PDF into a list. That works for 5MB, but it crashes when you have 2GB of manuals or logs.\n\nI built a pipeline to handle large-scale ingestion efficiently on a consumer laptop. Here is the architecture I used to solve RAM bottlenecks and API rate limits:\n\n1. **Lazy Loading with Generators:** Instead of `docs = loader.load()`, I implemented a Python Generator (`yield`). This processes one file at a time, keeping RAM usage flat regardless of total dataset size.\n2. **Persistent Storage:** Using ChromaDB in persistent mode (on disk), not in-memory. Index once, query forever.\n3. **Smart Batching:** Sending embeddings in batches of 100 to the API with `tqdm` for monitoring, handling rate limits gracefully.\n4. **Recursive Chunking with Overlap:** Critical for maintaining semantic context across cuts.\n\nI made a full code-along video explaining the implementation line-by-line using Python and LangChain concepts.\n\n[https://youtu.be/QR-jTaHik8k?si=a\\_tfyuvG\\_mam4TEg](https://youtu.be/QR-jTaHik8k?si=a_tfyuvG_mam4TEg)\n\nIf you have questions about the `yield` implementation or the batching logic, ask away!",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1qv2ks2/architecture_breakdown_processing_2gb_of_docs_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3et0i0",
          "author": "Oshden",
          "text": "Whoa this is awesome! Thanks for sharing!!!",
          "score": 3,
          "created_utc": "2026-02-03 21:00:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3hwbel",
              "author": "jokiruiz",
              "text": "thanks! glad you like it!",
              "score": 1,
              "created_utc": "2026-02-04 08:18:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qsenyn",
      "title": "MiRAGE: A Multi-Agent Framework for Generating Multimodal, Multihop Evaluation Datasets (Paper + Code)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "author": "Socaplaya21",
      "created_utc": "2026-01-31 21:21:26",
      "score": 11,
      "num_comments": 0,
      "upvote_ratio": 0.84,
      "text": "TL;DR**:** We developed a multi-agent framework that generates multimodal, multihop QA pairs from technical documents (PDFs containing text, tables, charts). Unlike existing pipelines that often generate shallow questions, MiRAGE uses an adversarial verifier and expert persona injection to create complex reasoning chains (avg 2.3+ hops).\n\n**Paper:** [https://arxiv.org/abs/2601.15487](https://arxiv.org/abs/2601.15487)\n\n**Code:** [https://github.com/ChandanKSahu/MiRAGE](https://github.com/ChandanKSahu/MiRAGE)\n\nHi everyone,\n\nWe've been working on evaluating RAG systems for industrial/enterprise use cases (technical manuals, financial reports, regulations), and (as many have) we hit a recurring problem: standard benchmarks like Natural Questions or MS MARCO don't reflect the complexity of our data.\n\nMost existing eval datasets are single-hop and purely textual. In the real world, our documents are multimodal (*especially* heavy on tables/charts in our use cases) and require reasoning across disjoint sections (multi-hop).\n\nWe built and open-sourced MiRAGE, a multi-agent framework designed to automate the creation of \"Gold Standard\" evaluation datasets from your arbitrary corpora.\n\nInstead of a linear generation pipeline (which often leads to hallucinations or shallow questions), we use a swarm of specialized agents.\n\nInstead of immediate generation, we use a retrieval agent that recursively builds a semantic context window. This agent gathers scattered evidence to support complex inquiries *before* a question-answer pair is formulated, allowing the system to generate multi-hop queries (averaging >2.3 hops) rather than simple keyword lookups.\n\nWe address the reliability of synthetic data through an adversarial verification phase. A dedicated verifier agent fact-checks the generated answer against the source context to ensure factual grounding and verifies that the question does not rely on implicit context (e.g., rejecting questions like \"In the table below...\").\n\nWhile the system handles text and tables well, visual grounding remains a frontier. Our ablation studies revealed that current VLMs still rely significantly on dense textual descriptions to bridge the visual reasoning gap, when descriptions were removed, faithfulness dropped significantly.\n\nThe repo supports local and cloud API model calls. We're hoping this helps others stress test their pipelines.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qsenyn/mirage_a_multiagent_framework_for_generating/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qun1d9",
      "title": "Chunking strategy",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qun1d9/chunking_strategy/",
      "author": "Ordinary_Pineapple27",
      "created_utc": "2026-02-03 08:58:54",
      "score": 11,
      "num_comments": 11,
      "upvote_ratio": 0.92,
      "text": "Hi guys,\n\nNowadays I am working on Text Retrieval project where I have thousands of pdf files and the task is given a query the system should return related passage (highlighted as Google does) within documents.   \nFor text extraction, I am using paddleocr vl which is doing well so far. As most of you know, given a single pdf file, paddleocr vl returns a folder with md and json files (as set to save both md and json files) for each page. If the pdf file has 50 pages, there are 50 md and json files.   \n  \nI am having difficulty in how to do the chunking. I know that given a query, I need the page information as a metadata to show the related page and passage within documents.   \nIf I just concatenate all the md files and do one of the chunking strategies, I will lose the page information. But If I do not concatenate them, I will lose context of some passages where one half is on the first page and the other is one the next page. \n\nBesides that I am well-aware of embedding models, the RAG architecture, rerankers, etc. But no matter how good your overall architecture is, if your chunks are garbage, the retrieval results will also be garbage.\n\nThose, who have come accross with such issue, please, advice me.  \nThank you beforehand.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qun1d9/chunking_strategy/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3bargd",
          "author": "Better_Ad_3004",
          "text": "Commenting to come back later",
          "score": 2,
          "created_utc": "2026-02-03 09:14:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bep68",
              "author": "stingraycharles",
              "text": "You know that Reddit has a save function, right?",
              "score": 4,
              "created_utc": "2026-02-03 09:53:05",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3cdo4l",
              "author": "welcome-overlords",
              "text": "Same here. \n\nMy specific case is that there are hundreds of pages of large blueprints in pdf files.\n\nSo far ive been using aws bedrock knowledge bases and their default chunkings etc. Seems to work ok but there's a lot of improvements to be made",
              "score": 1,
              "created_utc": "2026-02-03 14:07:52",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3mlgbh",
                  "author": "_harryj",
                  "text": "You might want to look into overlapping chunking strategies where you take a few lines from the end of one page and a few from the beginning of the next. It helps maintain context while still giving you the page metadata you need. Also, consider tagging your chunks with page numbers to keep track of where each piece originated.",
                  "score": 1,
                  "created_utc": "2026-02-05 00:03:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3dryce",
          "author": "Smart_MoneyTor",
          "text": "I am working on something similar but at a much larger scale. I am still in ideation phase, but I can already tell you that there is no one-size-fits-all solution. Chunking at fixed window sizes will for sure rotten your context, so the chunking strategy is of extreme importance. \n\nMy approach, which is theoretical at this stage, is to adopt a multi-stage conditional chunking approach as follows: \n- Set a max chunk size (C_max)\n- recursively traverse the document (e.g., md) and chunk if section/subsection size <= C_max, else (i.e., section size > C_max) initiate a chunk set [c_0, .. c_i] where you chunk at max size up until the last chunk will is treated similarly to first if branch. Your metadata should maintain the link between chunks within a set, so that if one element is hit with your vector search, the whole set is retrieved. \nThe metadata may also include section header numbering/titles, page numbers etc. when you rank chunks that you‚Äôll populate the context with, you can use the page numbers associated with those in the metadata to get the snippets highlighting used chunks. \n\nYou may further try to ‚Äúcompress‚Äù the set of chunks to fit it into your context. \n\nThis is the approach I intend to work with. If anyone has any ideas on how to improve this, or thinks there are problems I‚Äôm overlooking, please feel free to chime in.",
          "score": 1,
          "created_utc": "2026-02-03 18:08:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3e7lf7",
          "author": "thecontentengineer",
          "text": "You can join context engineering discord. Found much better help than here. Lots of smart folks. \n\nhttps://discord.gg/FC7Mw66GY",
          "score": 1,
          "created_utc": "2026-02-03 19:19:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3f6i50",
          "author": "Informal_Tangerine51",
          "text": "Page metadata versus context continuity is a real tradeoff. But beyond chunking strategy, can you verify which chunks were actually retrieved when results are wrong?\n\nFor your use case: chunk with overlap across page boundaries, store page number as metadata for each chunk. When chunk spans pages, metadata shows \"pages 5-6.\" Retrieval shows correct pages, overlap preserves context.\n\nThe gap most people miss: good chunking gets better results, but when retrieval is still wrong, can you prove which chunks were returned and whether they were from current doc version? Legal asks \"what regulation text did the system retrieve,\" you need chunk provenance not just accuracy.\n\nYour architecture focus is right - chunking quality matters first. Also plan for: how will you verify what was retrieved six months later when someone questions a result?\n\nWhat's your plan for tracking chunk lineage back to source docs?",
          "score": 1,
          "created_utc": "2026-02-03 22:02:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3fznxa",
          "author": "Live-Guitar-8661",
          "text": "We are about to release tree based RAG and I think it‚Äôs going to be a game changer.\n\nTechnically PageIndex got there first, we were doing a hybrid strategy between splitting the doc up and turning it into chunks by section, but tree based is way more effective.\n\nShould be out next week with an OSS version to follow (hopefully next week as well)\n\nhttps://orchata.ai\n\nHMU if you wanna chat",
          "score": 1,
          "created_utc": "2026-02-04 00:35:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3bdrs3",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": -1,
          "created_utc": "2026-02-03 09:44:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3chy0g",
              "author": "Trotskyist",
              "text": "pretty sure this is either a bot or the developer of needle fwiw given how they casually seem to name drop it in nearly every thread in this subreddit",
              "score": 5,
              "created_utc": "2026-02-03 14:30:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3gc3jk",
                  "author": "Wimiam1",
                  "text": "If you look at their profile, you‚Äôll see they‚Äôre the dev. It‚Äôs really lame behaviour. I almost used their product, but their underhanded self promotion completely turned me off",
                  "score": 1,
                  "created_utc": "2026-02-04 01:45:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qrdj5s",
      "title": "Reranker Strategy: Switching from MiniLM to Jina v2 or BGE m3 for larger chunks?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-01-30 18:24:03",
      "score": 10,
      "num_comments": 10,
      "upvote_ratio": 1.0,
      "text": "Hi all,\n\nI'm upgrading the reranker in my RAG setup. I'm moving off ms-marco-MiniLM-L12-v2 because its 512-token limit is truncating my 500-word chunks.\n\nI need something with at least a 1k token context window that offers a good balance of modern accuracy and decent latency on a GPU.\n\nI'm currently torn between:\n\n1. jinaai/jina-reranker-v2-base-multilingual\n\n2. BAAI/bge-reranker-v2-m3\n\nIs the Jina model actually faster in practice? Is BGE's accuracy worth the extra compute? If anyone is using these for chunks of similar size, I'd love to hear your experience.\n\nOpen to other suggestions as well!",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qrdj5s/reranker_strategy_switching_from_minilm_to_jina/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o31hlfn",
          "author": "ampancha",
          "text": "Switching to a longer-context reranker solves truncation, but it also means more tokens per retrieval hit flowing into your LLM. If you don't have per-query token caps and cost attribution already in place, the accuracy upgrade can quietly double your inference spend. Worth locking that down before you benchmark the new model. Sent you a DM.",
          "score": 2,
          "created_utc": "2026-02-01 21:16:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o2qlkx0",
          "author": "Horror-Turnover6198",
          "text": "Theoretically you could chunk again post-retrieval to sub 512, then rerank, and if one of the sub-chunks makes the cut you splice it back together. Or discard the other sub-chunk. Just depends on your chunking strategy I guess. Not sure if that‚Äôs appealing but it may be an option if bge or jina performance isn‚Äôt great.",
          "score": 1,
          "created_utc": "2026-01-31 04:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2qlyhg",
              "author": "CourtAdventurous_1",
              "text": "Bit if i truncate and then rerank it, wouldn‚Äôt it decrease the accuracy or the actual meaning of the chunk pr something like that?",
              "score": 1,
              "created_utc": "2026-01-31 04:49:54",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2qmz8g",
                  "author": "Horror-Turnover6198",
                  "text": "So let‚Äôs say you retrieved 10 candidates of 500 words each. If you took each candidate and split it into 3 or 4 chunks, you‚Äôd end up with 30 or 40 sub-512 token chunks, that would then get reranked individually. After rerank, let‚Äôs say chunks 7, 14, and 16 ended up in your top_k. You would then lookup which chunk 7, 14 and 16 came from, and just use those. So the meaning of each chunk would make it into your reranker, just not all at once. Some semantic meaning might be lost by breaking up the chunks for the reranker, but as I understand it, it‚Äôs hard to keep tight meaning across much more than 512 tokens in the first place. I‚Äôm totally open to being called out as wrong here though.",
                  "score": 1,
                  "created_utc": "2026-01-31 04:57:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o2rpl86",
          "author": "jannemansonh",
          "text": "spent way too much time optimizing rerankers and chunk sizes for my doc workflows... ended up moving those to needle app since the rag stack is built in. still run custom rag for specific use cases though. for your question though - bge m3 accuracy is solid if you've got the compute, jina v2 is faster but check the multilingual overhead if you're english-only",
          "score": 1,
          "created_utc": "2026-01-31 10:39:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qr1suf",
      "title": "Build n8n Automation with RAG and AI Agents ‚Äì Real Story from the Trenches",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "author": "According-Site9848",
      "created_utc": "2026-01-30 10:24:49",
      "score": 10,
      "num_comments": 1,
      "upvote_ratio": 0.86,
      "text": "One of the hardest lessons I learned while building n8n automations with RAG (Retrieval-Augmented Generation) and AI agents is that the problem isn‚Äôt writing workflows its handling real-world chaos. I was helping a mid-sized e-commerce client who sold across Shopify, eBay, and YouTube and the volume of incoming customer questions, order updates and content requests was overwhelming their small team. The breakthrough came when we layered RAG on top of n8n: every new message or order triggers a workflow that first retrieves relevant historical context (past orders, previous customer messages, product FAQs) and then passes it to an AI agent that drafts a response or generates a content snippet. This reduced manual errors drastically and allowed staff to focus on exceptions instead of repetitive tasks. For example, a new Shopify order automatically pulled product specs, checked inventory, created a draft invoice in QuickBooks and even generated a YouTube short highlighting the new product without human intervention. The key insight: start with the simplest reliable automation backbone (parsing inputs ‚Üí enriching via RAG ‚Üí action via AI agents), then expand iteratively. If anyone wants to map their messy multi-platform workflows into a clean, intelligent n8n + RAG setup, I‚Äôm happy to guide and  to help get it running efficiently in real operations.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qr1suf/build_n8n_automation_with_rag_and_ai_agents_real/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o2nhhpu",
          "author": "Whole-Board4430",
          "text": "Hi i read your post, i am new to RAG. for one of my employers where i'm helping with marketing and implementing basic AI, i am building a RAG agent. For now i just built a simple agent on n8n, 1 workflow to vector the documents into a database, the agent to retrieve it.\n\nWhat i want this agent to be able to do (if possible) is using our business documents + certain books, methods and other data we find important, and transform this working together with a seperate LLM to generate content, help our customers with questions, help our team with their work, onboard new people into the team when nessecarry. Do you mind if i ask you some questions, you now have a basic view of what i am trying to get working",
          "score": 1,
          "created_utc": "2026-01-30 18:46:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qth7ek",
      "title": "RAG, Medical Models <20B, guardrails, and sVLMs for medical scans ?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "author": "jiii95",
      "created_utc": "2026-02-02 01:26:49",
      "score": 9,
      "num_comments": 5,
      "upvote_ratio": 0.85,
      "text": "[](https://www.reddit.com/r/LocalLLaMA/?f=flair_name%3A%22Resources%22)\n\nSo, I am in the cardiovascular area, and I am looking for small models < 20B params, that can work for my rag that is dealing with structured JSON data. Do you have any suggestions ? I also suffer from some hallucinations, and I want also to imlement guardrails for my application to answer only medical questions about cardiovascular & data that is present and cited in the docs, will LLM be efficient with some prompts for guardrails or do you have something specific to offer. I am open only for open-source solutions, not enterprise paid software.  \nI am also looking for any sVLMs (Small Vision Language Models) that can take scans of the chest region or aorta and interpret them, or at least do segmentation or classification, any suggestions? If not a complete answer you have, any resources to look into?\n\nThank you very much (If you think I can cross-post in some other subreddit, please, any answer you can give and be beneficial, please)",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1qth7ek/rag_medical_models_20b_guardrails_and_svlms_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o32vlim",
          "author": "Yablan",
          "text": "Literally guardrails then?\n\nhttps://github.com/guardrails-ai/guardrails",
          "score": 2,
          "created_utc": "2026-02-02 01:42:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o32wh3m",
              "author": "jiii95",
              "text": "Well I want the output to be two things: output aigh cited data or only medical content about cardiovascular. Anything else such as medical advice or any otherput must marked as Off-topic and nothing as ouput",
              "score": 1,
              "created_utc": "2026-02-02 01:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35jeqi",
          "author": "sp3d2orbit",
          "text": "Have you considered a different approach? When I try to apply RAG directly to medical Data, the problem is always in the vector embeddings. No matter how I create them (frontier models or specialized models).\n\n  \nSince you already have everything in json format that means you're dealing with structured data. I would build an ontology on top of it and do ontology guiding search‚Äã. That means you're not dealing with hallucinations. And still use the llm in the parts of the pipeline where it makes sense, just not for the parts that require no hallucination.",
          "score": 1,
          "created_utc": "2026-02-02 13:42:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35jkrr",
              "author": "jiii95",
              "text": "Can you please elaborate more ? Very interesting",
              "score": 1,
              "created_utc": "2026-02-02 13:43:49",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o35qm22",
                  "author": "sp3d2orbit",
                  "text": "So I don't know your exact use case. But you mentioned cardiology. If look at the ICD-10 ontology there's something like 1200 codes that are applicable to cardiology. If we look at SNOMED-CT there might be 10 times that number if you consider all the different structures disorders etc. \n\n  \nThose codes exist because there's some sort of medical workflow or billing logic or treatment that depends on them being distinct. When you try to apply RAG to this problem, your in essence hoping that you can fragment the documents into those 1200 codes or 12,000 codes depending on your scenario. \n\n  \nEven the very best models are going to have a hard time doing that with Fidelity. I find it's better to invert the problem, use the ontology first, and then then we use the llm to do generative tasks.",
                  "score": 1,
                  "created_utc": "2026-02-02 14:22:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qti5up",
      "title": "FAQ content formatting advice for RAG chatbot",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qti5up/faq_content_formatting_advice_for_rag_chatbot/",
      "author": "Odd-Affect236",
      "created_utc": "2026-02-02 02:09:25",
      "score": 8,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "I‚Äôm building a RAG‚Äëbased chatbot for FAQ content. Currently, the FAQ data is stored in HTML tags as JSON within our CMS, but it contains many extra fields that aren‚Äôt needed for this use case. I‚Äôm trying to decide on the best format for storing the content. Should I use plain text (`.txt`), Markdown (`.md`), or something else?\n\nAdditionally, should all FAQs be placed in a single file or grouped logically into multiple files?\n\nI‚Äôm considering using a structure like this:\n\n    Q1\n    A1\n    \n    Q2\n    A2\n    \n    ...\n    ...\n\nDoes this approach make sense?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qti5up/faq_content_formatting_advice_for_rag_chatbot/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o37f38b",
          "author": "blue-or-brown-keys",
          "text": "Depending on how many FAQ you have, if there are too few all of it in a single context window will be useful if you have too many then you need to invest each one of them as a separate input into your vector database",
          "score": 1,
          "created_utc": "2026-02-02 19:06:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ghvt9",
          "author": "Suspicious-Juice3897",
          "text": "maybe, you should store them in a vector database and do a similarity search on that or bm25 and similarity search",
          "score": 1,
          "created_utc": "2026-02-04 02:18:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qtnjhr",
      "title": "Is this \"Probe + NLI Verification\" logic overkill for accurate GraphRAG? (Replacing standard rerankers)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qtnjhr/is_this_probe_nli_verification_logic_overkill_for/",
      "author": "CourtAdventurous_1",
      "created_utc": "2026-02-02 06:30:49",
      "score": 7,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "Hi everyone,\n\nI'm building a RAG pipeline that relies on graph-based connections between large chunks (\\~500 words). I previously used a standard reranker (BGE-M3) to establish edges like \"Supports\" or \"Contradicts,\" but I ran into a major semantic collision problem:\n\nThe Problem:\n\nRelevance models don't understand logic. To BGE-M3, Chunk A (\"AI is safe\") and Chunk B (\"AI is NOT safe\") are 95% similar. My graph ended up with edges saying Chunk A both SUPPORTS and CONTRADICTS Chunk B.\n\nThe Proposed Fix (My \"Probe Graph\" Logic):\n\nI'm shifting to a new architecture and want to know if this is a solid approach or if I'm over-engineering it.\n\n1. Intent Probing (Vector Search): Instead of one generic search, I run 5 parallel searches with specific query templates (e.g., Query for Contradicts: \"Criticism and counter-arguments to {Chunk\\_Summary}\").\n\n2. Logic Gating (Zero-Shot): I pass the candidates to ModernBERT-large-zeroshot with specific labels (supports, contradicts, example of).\n\n3. Strict Filtering: I only create the edge if the NLI model predicts the specific relationship and rejects the others (e.g., if I'm probing for \"Supports,\" I reject the edge if the model detects \"Contradiction\").\n\nMy Question:\n\nHas anyone successfully used Zero-Shot classifiers (like ModernBERT) as a \"Logic Gate\" for graph edges in production?\n\n‚Ä¢ Does the latency hit (running NLI on top-k pairs) justify the accuracy gain?\n\n‚Ä¢ Are there lighter-weight ways to stop \"Supports/Contradicts\" collisions without running a full cross-encoder?\n\nStack: Infinity (Rust) for Embeddings + ModernBERT (Bfloat16) for Logic.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qtnjhr/is_this_probe_nli_verification_logic_overkill_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o35fbzw",
          "author": "saiprasad04",
          "text": "The semantic collision problem you're describing is a real pain point with embedding models - they capture topic similarity but not logical relationships. Your approach is solid.\n\nA few thoughts on your questions:\n\nFor lighter alternatives to full NLI, you might consider using negation-aware embeddings (like InstructorXL with explicit instructions about stance) or adding a simple negation detection layer before your classifier. Some teams also use contrastive fine-tuning on their specific domain to make the embedding space more logic-aware.\n\nOn latency - if you're running NLI on top-k pairs (say k=20), that's manageable in most cases. The bigger question is whether you batch these calls or run them sequentially. Batching on GPU makes a huge difference.\n\nYour strict filtering approach (rejecting edges when NLI detects conflicting signals) is a good safeguard. One refinement: consider adding a confidence threshold rather than binary accept/reject - edges with ambiguous NLI scores might warrant a different edge type like \"related\" rather than being dropped entirely.",
          "score": 1,
          "created_utc": "2026-02-02 13:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o35n2f3",
              "author": "CourtAdventurous_1",
              "text": "Thanks for the suggestion but i am using multiple probes like similar,contradict,elaborate,depends on and example of,\nSo will a nli model handle all this probes or is there any alternative (and for my pipeline if the probe graph creation takes some time its not that much of a problem as it is not directly related to output of the user‚Äôs query)",
              "score": 1,
              "created_utc": "2026-02-02 14:03:11",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o35iowx",
          "author": "jannemansonh",
          "text": "the semantic collision problem you're hitting is brutal... spent way too long wiring custom reranker + nli logic for graph-based rag before. ended up using needle app for those workflows since it handles the embedding/relationship logic internally.... way simpler than managing the bge-m3 + modernbert pipeline yourself, especially for the supports/contradicts edge cases",
          "score": 1,
          "created_utc": "2026-02-02 13:38:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o35zynq",
          "author": "coderarun",
          "text": "How does this approach compare to extracting a KG and have concepts/arguments as nodes and \"SUPPORTS/CONTRADICTS\" as edges?",
          "score": 1,
          "created_utc": "2026-02-02 15:10:50",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwatbi",
      "title": "How does one go about validating and verify the correctness of a RAG's 'knowledge source'?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qwatbi/how_does_one_go_about_validating_and_verify_the/",
      "author": "boombox_8",
      "created_utc": "2026-02-05 03:55:40",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.78,
      "text": "Hey guys! I am new to the world of knowledge graphs and RAGs, and am very interested in exploring it!\n\nI am currently looking at using property graphs (neo4j to be specific) as the 'knowledge base' for RAG implementations since I've read that they're more powerful than the alternative of RDFs. In other words, I am building my RAG's 'knowledge source' using a knowledge graph\n\n\n\nThere is just one problem here I can't quite seem to crack, and that's the validation of the knowledge source (be it a vector DB, a knowledge graph, or otherwise). A RAG builds itself on the assurance that its underlying data-source is correct. But if you can't validate and verify the data-source, how do you 'trust' the RAG's output?\n\n\n\nI am seeing two schools of thought when it comes to building the data-source (assuming I am working with Knowledge Graphs here) :\n\n1) Give an LLM your documents, and ask it to output the data in the format you want (exp, 3-tuples for KGs, JSON, if you're building your data-source on JSON and so on)\n\n  \n2) Use traditional NER+NLP techniques to more deterministically extract data, and output it into the data-source you want\n\n\n\n  \nTo BUILD a decent knowledge graph however, you need a relatively large corpus of your data 'documents', potentially from various different sources, making the problem of verifying how correct the data is, hard\n\n  \nI've gone through a commonly-cited paper here on Reddit that delves into verifying the correctness *(KGValidator: A Framework for Automatic Validation of Knowledge Graph Construction)*\n\n  \nThe paper's methodology essentially boils down to (\"Use an LLM to verify if your data source is correct, and THEN, use ANOTHER RAG as reference to verify the correctness, and THEN, use another knowledge graph as reference to verify the correctness\")\n\n  \nFor one, it feels like a chicken-egg problem. I am creating a KG-based RAG in my domain (which in and of itself is a bit on the niche side and occasionally involves transliterated language from a non-English language at times) for the first time. So there IS no pre-existing RAG or KG I can depend on for cross-referencing and verifying  \n  \n  \nSecond, I find it hard to trust an LLM with completely and accurately validating a knowledge graph if LLMs are inherently prone to hallucination (and is the reason I am shifting to a RAG-based methodology in the first place; to avoid hallucinations over a very specific domain/problem-space), because I am worried about running into the ***garbage in = garbage out*** problem \n\n\n\nI can't seem to think of any deterministic and 'scientifically rigorous' way to validate the correctness of a RAG's data-source (Especially when it comes to assigning metrics to the validation process). Web-scraping has the same problem, though I did have an idea of web-scraping from trusted sites and feeding it as context to an LLM for validation (Though again, it's non-deterministic by design)  \n  \n  \nIs there any better way to solve it, or are the above mentioned techniques the only options?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qwatbi/how_does_one_go_about_validating_and_verify_the/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3offki",
          "author": "Esseratecades",
          "text": "I came across this problem in the early days of RAG. Essentially you can't know if your data is accurate without comparing it to data that you know is accurate, which means you already have a place where the data is accurate.\n\n\nI did write a proprietary algorithm to guarantee that the data you have is logically consistent, but accuracy can't be solved this way. Even with the algorithm that I wrote, you have to assume that the true story actually exists somewhere within the data to begin with.\n\n\nThe only way to actually do this is to only pull in data from sources that you trust.",
          "score": 3,
          "created_utc": "2026-02-05 07:08:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o6ykj",
          "author": "Academic_Track_2765",
          "text": "There is not a good solution to this problem besides making sure that your ground truth is the ground truth. Using a KG or a standard RAG won‚Äôt change that one bit. Let‚Äôs assume you create a KG / RAG for this sentence. ‚ÄúPresident Trump was the first US president‚Äù plus the same type of data for some other presidents. Now you build a KG/ RAG and ask the question who was the first president ? Your rag will perform a semantic search / KG path traversal and bring back Trump, but your LLM knows that‚Äôs not true, so after the synthesis stage your LLM might say that your retrieved result is not true. While it works here because the LLM was trained on this data, it will lack details about domain specific things e.g., different acronyms used internally or anything the LLM was not trained on. As you said you can do a web search to correct the result but you are back to domain issue. So make sure the ground truth is the actual truth and things will work out well!",
          "score": 2,
          "created_utc": "2026-02-05 05:56:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o3b5j",
          "author": "scoby_cat",
          "text": "Maybe I don‚Äôt understand. If you don‚Äôt know if the data is good why are you including it in your RAG?",
          "score": 2,
          "created_utc": "2026-02-05 05:28:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ooivs",
          "author": "eurydice1727",
          "text": "Citations. My system cites its source, and I have a full citation feature that renders the citation in a panel when clicked. In MD format or full PDF.",
          "score": 1,
          "created_utc": "2026-02-05 08:33:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3opoqx",
              "author": "GP_103",
              "text": "This. Several approaches to ‚Äúanchoring‚Äù the content block, forwarding those along through your pipeline and together, paired for retrieval.",
              "score": 1,
              "created_utc": "2026-02-05 08:44:46",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o3prlpc",
              "author": "Academic_Track_2765",
              "text": "That‚Äôs just citation brother, if the ground truth is wrong it‚Äôs wrong, it doesn‚Äôt fix the core issue of the data being wrong while citing where it got the wrong data from.",
              "score": 1,
              "created_utc": "2026-02-05 13:45:41",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3pt6pd",
          "author": "Informal_Tangerine51",
          "text": "The validation problem compounds at scale. We hit this on document intake where extraction errors accumulate.\n\nFor initial construction, NER+NLP (option 2) is more controllable but needs human validation on samples. We ran spaCy + custom rules on 500 docs, manually checked 10% random sample. Found 23% entity errors, 31% relationship errors. Three iterations to get under 5% error rate.\n\nTried LLM-based validation (GPT-4 checking triples) but it hallucinates plausible corrections that are wrong. Your exact concern.\n\nWhat worked: golden test sets. Manually verify 50-100 documents, extract to triples, freeze as regression fixtures. Every pipeline change re-runs against golden set. Catches degradation immediately.\n\nFor new data without external reference: consistency checks over correctness. Entity with conflicting attributes? Flag it. Relationships violating domain constraints (person born after death)? Flag it. Catches catastrophic errors, not subtle ones.\n\nFor your niche domain with transliteration - can domain experts spot-check 5% stratified samples (by source, date, entity type)? Would give you confidence bounds.\n\nWhat corpus size are you working with? Wondering if seed set manual validation is feasible before scaling.",
          "score": 1,
          "created_utc": "2026-02-05 13:54:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvz6ps",
      "title": "POV: RAG is a triangle: Accuracy vs Latency vs Cost (you‚Äôre locked inside it)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1qvz6ps/pov_rag_is_a_triangle_accuracy_vs_latency_vs_cost/",
      "author": "Donkit_AI",
      "created_utc": "2026-02-04 20:01:54",
      "score": 6,
      "num_comments": 3,
      "upvote_ratio": 0.72,
      "text": "I see people treating RAG like there‚Äôs a ‚Äúbest stack‚Äù ‚Äî if you just tweak chunk size / vector DB / prompt enough.\n\nNot exactly like it. IMHO RAG is physics.\n\nYou‚Äôre always trading off accuracy, latency, and cost. Whatever you ship is locked inside that triangle.\n\n# The triangle\n\n* Accuracy: grounded answers + correct retrieval\n* Latency: time-to-answer (P95/P99)\n* Cost: $ per *successful outcome* (not per request)\n\nYou can move within the triangle. You can‚Äôt escape it.\n\n# The trade-offs (pick 2, the 3rd fights back)\n\n**Accuracy + Latency ‚Üí Cost goes up**  \nYou add: rerankers, query rewriting, multi-stage retrieval, more evals, more calls.\n\n**Accuracy + Cost ‚Üí Latency goes up**  \nYou accept slower flows: stricter gating, extraction-first, better versioning, more checks.\n\n**Latency + Cost ‚Üí Accuracy is constrained**  \nYou go cache-heavy, link-first answers, templates, smaller models, fewer steps.\n\n# Why agentic chains make it worse\n\nIf each step is \\~80% reliable, 3 steps is:\n\n0.8 √ó 0.8 √ó 0.8 = **51%**\n\nThat‚Äôs not ‚Äúpretty good.‚Äù That‚Äôs coin-flip automation with nicer UX.\n\n# My practical rule: define floors, don‚Äôt chase ‚Äúbest RAG‚Äù\n\nThere's no best RAG architecture.  \nFor every use-case, there's the minimum acceptable floor for each constraint.\n\nExamples:\n\n* Support bot: optimize accuracy+latency, pay cost, add confidence gates\n* Legal/Finance: optimize accuracy+cost, allow latency, quote-first + abstain\n* Sales/Marketing: optimize latency+cost, constrain accuracy, claim-gates + sources\n\n# Levers that actually move the needle\n\n# Accuracy levers (usually cost/latency ‚Üë)\n\n* hybrid search + metadata filters\n* reranking (cross-encoder)\n* query rewrite / decomposition\n* structure-based chunking (clauses/sections)\n* confidence gates + abstention\n* evals + regression tests\n\n# Latency levers\n\n* routing (fast path vs safe path)\n* caching (answers, retrieval results, embeddings)\n* precompute (summaries, intent bundles)\n* strict token budgets\n* fewer model calls\n\n# Cost levers\n\n* small model for routing/classification\n* extraction over generation (numbers/thresholds)\n* offline summarization (don‚Äôt pay per query)\n* reuse approved snippets/templates\n* measure cost per *resolved task*\n\n# Metrics (the bare minimum)\n\n* grounded answer rate / citation rate\n* P95/P99 latency end-to-end\n* cost per resolved conversation/task\n* abstention quality (abstain when you should)\n* unsupported-claim rate (should trend to 0)\n\nWDYT? Agree?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1qvz6ps/pov_rag_is_a_triangle_accuracy_vs_latency_vs_cost/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o3m1jum",
          "author": "emudojo",
          "text": "I'm often within the accurate path, I hate screen rag based replied are wacky and so does our users. \n\nI'm heavy into evals and regression testing\n\nNeed to look into reranking and cache +system is slow to reply as I do have a judge in the middle to ground replies even after coming from the reasoning+rag agent",
          "score": 1,
          "created_utc": "2026-02-04 22:16:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3phvws",
              "author": "Donkit_AI",
              "text": "Reranking - 100%, especially if your retrieved set is noisy. And it bumps your accuracy and speed altogether.\n\nAlso a two-tier judge can do some good: chap gate -> expensive judge:\n\n* Cheap gate: ‚Äúis there adequate evidence coverage?‚Äù (retrieval score / reranker score / simple classifier).\n* Only if it passes -> run the full judge.\n\nOn top of that, measure where the time goes. There might be some optimization quick wins.",
              "score": 1,
              "created_utc": "2026-02-05 12:47:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3pvypl",
                  "author": "emudojo",
                  "text": "Yep, for the most I work with structured data and I'm my case the first judge just compares tool output with the reasoning model responde (that must incorporate tool output if valid), but still even with that sometimes it just plain makes stuff, out of thin air üòÇ",
                  "score": 1,
                  "created_utc": "2026-02-05 14:09:54",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}