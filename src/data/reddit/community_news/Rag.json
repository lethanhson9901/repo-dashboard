{
  "metadata": {
    "last_updated": "2026-03-02 02:56:22",
    "time_filter": "week",
    "subreddit": "Rag",
    "total_items": 20,
    "total_comments": 156,
    "file_size_bytes": 185728
  },
  "items": [
    {
      "id": "1rcba6y",
      "title": "What's the best embedding model for RAG in 2026? My retrieval quality is all over the place",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "author": "DarfleChorf",
      "created_utc": "2026-02-23 07:44:08",
      "score": 62,
      "num_comments": 39,
      "upvote_ratio": 0.94,
      "text": "I've been running a RAG pipeline for a legal document search tool.\n\nCurrently using OpenAI text-embedding-3-large but my retrieval precision is around 78% and I keep getting irrelevant chunks mixed in with good results.\n\nI've seen people mention Cohere embed-v4, Voyage AI, and Jina v3. Has anyone done real benchmarks on production data, not just MTEB synthetic stuff?\n\nSpecifically interested in retrieval accuracy on domain-specific text, latency at scale (10M+ docs), and cost per 1M tokens.\n\nWhat's working for you in production?\n\njust got access to zeroentropy's embeddings. amazing stuff! [zeroentropy.dev](http://zeroentropy.dev)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcba6y/whats_the_best_embedding_model_for_rag_in_2026_my/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1lo2",
          "author": "ChapterEquivalent188",
          "text": "garbage in, garbage out.... semantic chunking and a lot of stuff on ingest AND retrieval side. its all here https://github.com/2dogsandanerd/Knowledge-Base-Self-Hosting-Kit as a basis system to build on it or the long run you may read here https://github.com/2dogsandanerd/RAG_enterprise_core  have fun. im tired of reading every day bout wrappers which you cant trust..... for legal docs you may need a auditrail as well also for ingest AND retrieval-----\nnever use the happy path as it will never make you happy",
          "score": 27,
          "created_utc": "2026-02-23 07:50:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z0acu",
              "author": "revovivo",
              "text": "but how does it solve the problem of chunk sizing and retrieval ",
              "score": 1,
              "created_utc": "2026-02-23 16:11:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o74vkvz",
                  "author": "ChapterEquivalent188",
                  "text": "semantic chunking for a start ? ",
                  "score": 1,
                  "created_utc": "2026-02-24 13:48:08",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7nz2nh",
              "author": "Transcontinenta1",
              "text": "Thanks for your work. This sub has made me wait, but I canâ€™t wait much longer for improvement. I think thereâ€™s an answer: which one fits my use case. We build filling machinery to spec for major companies. We have unique models, so itâ€™s hard to have service techs who know all lines. This knowledge gap is challenging when only one person knows a machine and its models. A group usually has to figure out whatâ€™s wrong when one person would know. I want a comprehensive knowledge base of our manuals, service reports, BOMs, and recorded calls that weâ€™ll summarize. Iâ€™d love to get our cad drawings in there. \n\nWhat trusted sources did you use during your learning journey? What sources would you avoid if you knew then? Canâ€™t wait to look your gits over. Thanks again\n\nEdit: itâ€™s a mix of PDFs, solidworks slddrw > PDF, scanned pdf, and a lot of manuals are paper Belden the year 2008? lol we have been around a lot longer but there is a cutoff bc they went digital at some point. \n\nWe also ingested 4-5 companies in the last decade.",
              "score": 1,
              "created_utc": "2026-02-27 07:49:36",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7pbe6o",
                  "author": "ChapterEquivalent188",
                  "text": "This is a 'Hell Level' RAG case which i love ;) \n\nIndustrial machinery + 2008 Scans + BOMs is where 99% of tutorials fail \nbecause standard parsers destroy the table structure.\n\nEven with Docling (which ClawRAG uses), scanned BOMs are the final boss. If the OCR misreads a part number in a table, your technician orders the wrong part\n\nIf you have a nasty page (anonymized scan with a table), DM me a link. Iâ€™ll run it through my PantheonRAG and send you the raw parsed database (SQLite/Chroma) back\n\nThat way you see exactly if the data survives the ingestion process before you invest time setting it up. If my engine fails, even after the HITL at least you know the limit\n\nQuick tip on CAD: RAG can't read .slddrw natively yet. Batch-export them to 2D PDFs with text labels first ",
                  "score": 1,
                  "created_utc": "2026-02-27 14:08:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6x57m7",
              "author": "krimpenrik",
              "text": "Great resources!! \n\nHave you tested both?",
              "score": 0,
              "created_utc": "2026-02-23 08:25:54",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x6td3",
                  "author": "ChapterEquivalent188",
                  "text": "sort of ;) working everyday on it to make it perfect..will release some plugins and extensions to the opensourced ones soon\n\n\nedit: if you r intrested of the outcome just send me a set of docs and ill provide you with the rag for retrieval testing",
                  "score": 2,
                  "created_utc": "2026-02-23 08:41:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2u1w",
          "author": "lucasbennett_1",
          "text": "before switching models its worth figuring out whether the irrelevant chunks are a retrieval problm or a chunking problem.. 78% precision on legal docs can come from either, and they need different fixes,, embedding model swaps sometimes help but chunking strategy for legal text matters a lot because clauses and definitions often span weird boundaries that standard splitters handle badly.. that said BGE M3 does tend to outperform text embedding 3- arge on domain-specific retrieval in most comparison.. its available through several providers like deepinfra or huggingface at lower cost than openai embeddings which helps if you are iterating on a 10M doc corpus and running a lot of test queries",
          "score": 6,
          "created_utc": "2026-02-23 08:02:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x74f5",
              "author": "ChapterEquivalent188",
              "text": "this.  domain spec retrieval is one importend part but never can solve the garbage in and wordsalad problem ",
              "score": 1,
              "created_utc": "2026-02-23 08:44:48",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o78n3xq",
              "author": "liannehynes",
              "text": "I've tried Zeroentropy Embedddings (still in beta) and it outperforms ALL of the above ;) ",
              "score": 0,
              "created_utc": "2026-02-25 00:34:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78q93v",
                  "author": "No_Injury_7940",
                  "text": "+1",
                  "score": 0,
                  "created_utc": "2026-02-25 00:51:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2nwf",
          "author": "crewone",
          "text": "We have done extensive tests for book data. Voyage is always the best, but also has a high latency. A local Qwen3 embedder delivers 90% of that performance at a fraction of the cost and latency. The rest comes down to chunking strategies.",
          "score": 11,
          "created_utc": "2026-02-23 08:00:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x7dyb",
              "author": "picturpoet",
              "text": "How do you decide on one chunking strategy versus the other? Are there any best practises for different types of documents/document structures ",
              "score": 1,
              "created_utc": "2026-02-23 08:47:23",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6xlr9w",
                  "author": "crewone",
                  "text": "Read up on strategies. Then test and compare. (For large book context we use context-aware chunking)",
                  "score": 2,
                  "created_utc": "2026-02-23 11:06:13",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78njr2",
              "author": "liannehynes",
              "text": "I've tried zeroentropy embeddings and it outperforms voyage :) ",
              "score": 1,
              "created_utc": "2026-02-25 00:37:03",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7c0mrs",
                  "author": "crewone",
                  "text": "Then use that.",
                  "score": 1,
                  "created_utc": "2026-02-25 14:45:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x2cp5",
          "author": "thecontentengineer",
          "text": "Have you tried ZeroEntropy Embeddings? Theyâ€™re as good as their rerankers. You should always use ZeroEntropy, even for embeddings!!",
          "score": 14,
          "created_utc": "2026-02-23 07:57:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xa7he",
              "author": "DarfleChorf",
              "text": "Wait they have embeddings too? I only tried their reranker, didn't realize they had an embedding model. Might give it a shot.",
              "score": 2,
              "created_utc": "2026-02-23 09:15:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o78hcow",
                  "author": "thecontentengineer",
                  "text": "Yes they have embeddings and theyâ€™re the best we have tried. Way better than Cohere and Voyage.",
                  "score": 0,
                  "created_utc": "2026-02-25 00:03:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o78hoxb",
              "author": "liannehynes",
              "text": "yes we did!! ZeroEntropy has the best embeddings compared to all other providers.",
              "score": 1,
              "created_utc": "2026-02-25 00:05:33",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o78hymx",
                  "author": "DarfleChorf",
                  "text": "How do you get access?",
                  "score": 2,
                  "created_utc": "2026-02-25 00:07:03",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6xaud9",
          "author": "thefishflaps",
          "text": "Yeah ZeroEntropy reranker is solid, and they just dropped embeddings too. Worth checking out.",
          "score": 5,
          "created_utc": "2026-02-23 09:21:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x59v8",
          "author": "xeraa-net",
          "text": "Jina v5 just came out; especially for the model size pushing the state of the art: https://jina.ai/news/jina-embeddings-v5-text-distilling-4b-quality-into-sub-1b-multilingual-embeddings/\nBut it will of course always depend on the domain, language, cleanliness of data, quantization,â€¦\n\n\nDisclaimer: I work for Elastic.",
          "score": 2,
          "created_utc": "2026-02-23 08:26:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yrokw",
          "author": "tom_at_zedly",
          "text": "On legal docs, the model usually isn't the failure point, it's almost always the chunking.\n\nLegal text has too many nested clauses and cross-references that get cut off by standard recursive splitters just butcher. Weâ€™ve found that even basic embedding models work fine if you fix the chunking to keep definitions with their clauses (or use semantic chunking). That usually moves the needle way more than switching from OpenAI to Voyage or BGE.",
          "score": 2,
          "created_utc": "2026-02-23 15:31:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6z9oxo",
          "author": "remoteinspace",
          "text": "we've tried a bunch at papr and qwen 4b is best based on our evals",
          "score": 2,
          "created_utc": "2026-02-23 16:55:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70qww8",
          "author": "liannehynes",
          "text": "ZeroEntropy have SOTA embeddings but i think they're still on beta",
          "score": 4,
          "created_utc": "2026-02-23 21:04:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y5ax8",
          "author": "CEBarnes",
          "text": "Iâ€™ve had good reliable results with SPL files. (https://dailymed.nlm.nih.gov/dailymed/index.cfm). Step one was to spend 5-6 months building a parser that normalized and populated a database.\n\nIn your case, given you have doc files, would be to structure the data. Headings, line numbers, citations, parties, proposed order, etc. Stack on a categorizer and add its results to the structure. Build a specific schema that has enough flexibility that you donâ€™t end up with edge cases everywhere. Once you have semi structured data, then populate an old school database and build an API. Lastly, create the skills the AI needs to â€œintelligently,â€ use the API. \n\nGranted, even if vibed all the way to the end, this is likely a 9 to 12 month endeavor. But, your results will be magical.",
          "score": 1,
          "created_utc": "2026-02-23 13:30:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74rdak",
          "author": "Ascending_Valley",
          "text": "I use sentence transformer (and others) and then reduce and transform with a method that weights toward known similar samples. I end up in R50 with simple weighted distance being very effective.",
          "score": 1,
          "created_utc": "2026-02-24 13:24:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vyq85",
          "author": "miff_miffy923",
          "text": "if the retrieval is not stable maybe it's not because of the embedding. by chunking and retrieval runtime creates better result than just switching embedding models. for 10M+ docs, network latency to vector DB may introduce variance in top-k results. moving to a local semantic runtime (e.g., moss local or runtime level semantic search) may create more stable precision, maybe can also consider hybrid search + reranking.",
          "score": 1,
          "created_utc": "2026-02-28 14:56:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcb47i",
      "title": "My RAG retrieval accuracy is stuck at 75% no matter what I try. What am I missing?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "author": "Equivalent-Bell9414",
      "created_utc": "2026-02-23 07:34:01",
      "score": 54,
      "num_comments": 47,
      "upvote_ratio": 0.92,
      "text": "I've been building a RAG pipeline for an internal knowledge base, around 20K docs, mix of PDFs and markdown. Using LangChain with ChromaDB and OpenAI embeddings.\n\nI've tried different chunk sizes (256, 512, 1024), overlap tuning, hybrid search with BM25 plus vector, and switching between OpenAI and Cohere embeddings.\n\nStill hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\n\nIs this a chunking problem or an embedding problem? What else should I be trying? Starting to wonder if I need to add a reranking step after retrieval but not sure where to start with that.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rcb47i/my_rag_retrieval_accuracy_is_stuck_at_75_no/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6x1e94",
          "author": "xpatmatt",
          "text": "I had an issue where a lot of documents and data included very similar terms used in very different contexts which made retrieval for any particular query difficult due to irrelevant retrievals. \n\nI had to segment the docs/data into six different vector DBS based on user intent and route queries to the appropriate DB based on the user's intent. Works great now.",
          "score": 17,
          "created_utc": "2026-02-23 07:48:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6x3r92",
              "author": "redditorialy_retard",
              "text": "could you tell me more about this? very intriguedÂ ",
              "score": 2,
              "created_utc": "2026-02-23 08:11:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o6x41ly",
                  "author": "xpatmatt",
                  "text": "Yes if you have questions I can answer them",
                  "score": 2,
                  "created_utc": "2026-02-23 08:14:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6ykvj4",
              "author": "Cool_Injury4075",
              "text": "Did you try using a reranker? Unlike embeddings, rerankers can understand the intent of the question and filter out all the junk (unless, of course, the documents need context).",
              "score": 2,
              "created_utc": "2026-02-23 14:57:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xazxb",
          "author": "adukhet",
          "text": "Your problem is not embeddings, try below \n-if you chunk purely by token length, try markdown aware or/and semantic chunking\n-use rerankers but consider latency. Cross-encoders likely fixes semantically similar but irrelevant issues- but if not try late-interaction\n-try query rewriting/query expansion (e.g. HyDE)\n\nBut most importantly you must diagnose where failure arise before changing architecture",
          "score": 5,
          "created_utc": "2026-02-23 09:23:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x5f9u",
          "author": "ampancha",
          "text": "Reranking with a cross-encoder will likely push you past 80%, but persistent semantic pollution usually means chunking isn't preserving document boundaries or metadata context. The harder problem: your eval set won't cover the queries that actually break in production. You need per-query observability to see which retrievals are failing live, not just aggregate precision. Sent you a DM",
          "score": 4,
          "created_utc": "2026-02-23 08:28:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xi3nk",
              "author": "welcome-overlords",
              "text": "Im probably having similar issues. Would be interested in hearing more in DM",
              "score": 1,
              "created_utc": "2026-02-23 10:32:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6xdn8f",
          "author": "StuckInREM",
          "text": "I think sharing a complete pipeline of what you are doing would be useful, what do your metadata look like for the documents to enanche the retrieval phase? recursive split chunking is for sure not optimal, what do your document structure look like in terms of paragraphs? have you tried with a reranker?",
          "score": 3,
          "created_utc": "2026-02-23 09:49:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x0fc9",
          "author": "grabGPT",
          "text": "Are you using OCR on PDFs? Have you checked the accuracy?",
          "score": 2,
          "created_utc": "2026-02-23 07:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xmckb",
              "author": "Transcontinenta1",
              "text": "I am trying to use deepseeks ocr. Is there a better free one?",
              "score": 1,
              "created_utc": "2026-02-23 11:11:41",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ue1h0",
                  "author": "Agitated_Heat_1719",
                  "text": "There are tons of libraries for python only. Not all work the same way. \n\nPDF (structure) parsing libraries are fast, but have issues with some encodings or PDF text representations.\n\nOCR based implementations are waay slower (`marker`, `pytesseract`, `docTR`...)\n\nThis is how my extraction folder[s] look like:\n\n```\n.\nâ”œâ”€â”€ images\nâ”‚Â Â  â””â”€â”€ py\nâ”‚Â Â      â”œâ”€â”€ minecart\nâ”‚Â Â      â”œâ”€â”€ pikepdf\nâ”‚Â Â      â”œâ”€â”€ PyMuPDF-fitz\nâ”‚Â Â      â””â”€â”€ PyPDF2-pypdf\nâ”œâ”€â”€ tables\nâ”‚Â Â  â””â”€â”€ py\nâ”‚Â Â      â”œâ”€â”€ camelot\nâ”‚Â Â      â”œâ”€â”€ docling\nâ”‚Â Â      â”œâ”€â”€ gmft\nâ”‚Â Â      â”œâ”€â”€ marker\nâ”‚Â Â      â”œâ”€â”€ pdfplumber\nâ”‚Â Â      â””â”€â”€ tabula-py\nâ””â”€â”€ text\n    â””â”€â”€ py\n        â”œâ”€â”€ docling\n        â”œâ”€â”€ docTR\n        â”œâ”€â”€ kreuzberg\n        â”œâ”€â”€ marker\n        â”œâ”€â”€ markitdown\n        â”œâ”€â”€ MarkItDown\n        â”œâ”€â”€ pdfminer_six\n        â”œâ”€â”€ pdfplumber\n        â”œâ”€â”€ PyMuPDF_fitz\n        â”œâ”€â”€ pymupdf4llm\n        â”œâ”€â”€ PyPDF2\n        â”œâ”€â”€ pypdfium2\n        â”œâ”€â”€ pytesseract\n        â””â”€â”€ unstructured\n```\n\nUsers need to play with those and see what works for them and their corpus.\n\nHope this helps.",
                  "score": 2,
                  "created_utc": "2026-02-28 07:26:30",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o701n3s",
                  "author": "grabGPT",
                  "text": "It depends on whether the documents you're using are handwritten notes or machine printed. In both cases, accuracy will vary",
                  "score": 1,
                  "created_utc": "2026-02-23 19:04:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6x1sk8",
          "author": "ggone20",
          "text": "Not enough information to answer your question. What does your corpus look like?",
          "score": 2,
          "created_utc": "2026-02-23 07:52:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x23jw",
          "author": "AmbitionCrazy7039",
          "text": "You need structural filtering. Try to classify your documents as precise as possible. Maybe you want to build some relational database around it.Â \n\nFor example, if you query the Knowledge Base for some â€žManual Xâ€œ question, you only want to search similiar manuals. BM25 is only keyword search, most likely not sufficient. In this example keyword filtering might suggests non-manuals because other docs may relate more often to manuals.",
          "score": 2,
          "created_utc": "2026-02-23 07:55:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mrh0",
          "author": "Tough-Survey-2155",
          "text": "You need Agentic router: https://github.com/hamzafarooq/multi-agent-course/tree/main/Module_3_Agentic_RAG",
          "score": 2,
          "created_utc": "2026-02-24 07:50:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6x9817",
          "author": "Glass-Combination-69",
          "text": "Throw it into cognee and see if you get 100%. Graph might be whatâ€™s missing",
          "score": 1,
          "created_utc": "2026-02-23 09:05:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ya4xo",
          "author": "jrochkind",
          "text": "I am not an expert, but have you tried cross-encoder re-ranking?  (Over-fetching, then re-ranking to get your K). \n\nI have not yet myself, but have been considering it.  Oh from your last line it sounds like you too have been considering it but have not tried it. I think that's what would make sense to try?  I would be curious to your results. \n\nI haven't done it, but it seems pretty straightforward, you just feed your over-fetched results to the re-ranker, with your query, and it reorders them, hopefully putting the less relevant ones at the bottom and out of your final selection slice.",
          "score": 1,
          "created_utc": "2026-02-23 13:58:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yatxf",
          "author": "code_vlogger2003",
          "text": "Hey have you stored any metadata for every chunk such that in the first hand you can verify that my retrieval step is actually returning the exact relevant ground truth answer page numbers or not etc. In this step you can identify whether it's the chunking issue or embedding drift etc.",
          "score": 1,
          "created_utc": "2026-02-23 14:02:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yk2by",
          "author": "Dense_Gate_5193",
          "text": "Have you tried using RRF with reranking instead?\n\nNornicDB uses BM25+vector search and uses a reranking model (BYOM) https://github.com/orneryd/NornicDB",
          "score": 1,
          "created_utc": "2026-02-23 14:52:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ytm77",
          "author": "namognamrm",
          "text": "You did rerank?",
          "score": 1,
          "created_utc": "2026-02-23 15:40:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6za0tz",
          "author": "remoteinspace",
          "text": "have you tried using a knowledge graph? that worked well for us at papr.. got us 92% retrieval accuracy (top 5 results) on stanford's stark benchmark which has arxiv like docs in their data set. dm me and i can help",
          "score": 1,
          "created_utc": "2026-02-23 16:56:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o70caqp",
          "author": "blue-or-brown-keys",
          "text": "\"Still hovering around 75% precision on my eval set. The main issue is that semantically similar but irrelevant chunks keep polluting the results.\"\n\nTry synthetic data? Summarize the document , store the summary and drop the document. ",
          "score": 1,
          "created_utc": "2026-02-23 19:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z56f",
          "author": "Much-Researcher6135",
          "text": "yes pull back 3x and use a reranker",
          "score": 1,
          "created_utc": "2026-02-24 04:36:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72z5te",
          "author": "WorkingOccasion902",
          "text": "Have you considered Knowledge Graphs?",
          "score": 1,
          "created_utc": "2026-02-24 04:36:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73mov3",
          "author": "Informal-Victory8655",
          "text": "Change embeddings model",
          "score": 1,
          "created_utc": "2026-02-24 07:49:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75u4y3",
          "author": "TransportationFit331",
          "text": "I recommend Mastra.ai",
          "score": 1,
          "created_utc": "2026-02-24 16:35:08",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hij9h",
          "author": "cointegration",
          "text": "what you need is a cross encoder, it ties the query back to the chunks retrieved to maximise relevance",
          "score": 1,
          "created_utc": "2026-02-26 09:18:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hk0by",
          "author": "cointegration",
          "text": "1) different types of docs do better with different chunk sizes and different retrieval methods  \n2) long text essays (manuals, legal docs, fiction etc) benefit from larger chunks, vector search and knowledge graphs  \n3) tabular, itemised, charts or docs with many tables (invoices, receipts, performance reports etc) benefit more from BM25 against extracted metadata and shorter chunks  \n4) its a balance between precision, recall and semantic relevance, but strategies must exist for all 3. BM25 for precision, vector search for recall, knowledge graphs for semantic relevance.  \n5) use a cross encoder and rerank above 3, apply your own tweekable weights",
          "score": 1,
          "created_utc": "2026-02-26 09:32:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7mbg9e",
          "author": "RecommendationFit374",
          "text": "We do semantic and graph aware hierarchal chunking, re ranking and query expansion. The problem you have is embeddings only capture semantic meanings once you have large document corpus your hitting physical limits on vector dimensionality. \n\nYou end up having so much noise where itâ€™s hard to make the right signal sharp enough. \n\nFor example if you have â€œI am very happyâ€ or â€œI am not very happyâ€ both are close in cosine similarity but carry different meanings. Actually, semantic meanings miss graph relationships, temporal sequences, causalâ€¦ â€œvitamin E causes cancerâ€ and â€œvitamin E prevents cancerâ€ also close cosine sim but are very different meanings.\n\nWe mainly use papr.ai - predictive memory architecture that uses vector db, graph (using custom schema) and prediction models which helped us achieve 92% hit@5 in Stanford STARK benchmark MAG dataset.\n\nHappy to help and share our learnings on a call. Free fee to dm me - below is a doc on our chunking technique\n\nhttps://github.com/Papr-ai/memory-opensource/blob/main/docs/features/documents/CONTEXT_AWARE_CHUNKING_ARCHITECTURE.md",
          "score": 1,
          "created_utc": "2026-02-27 01:03:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nb9rb",
          "author": "Pasha_Hu",
          "text": "I have one suggestion for you, don't use vectors embedding. So in rag we convert documents in vectors and then do cosin similarly. But now there is something new which gives more accuracy and accurate answer which is pageIndex this is using tree for tracing the answers. Try this and if it's working then thanks me later with 95% accuracy",
          "score": 1,
          "created_utc": "2026-02-27 04:39:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o71nke4",
          "author": "Ok-Attention2882",
          "text": "Skill issue",
          "score": 0,
          "created_utc": "2026-02-23 23:53:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zct5j",
          "author": "Repulsive-Memory-298",
          "text": "It sounds like a troll but adding porn to your datasets calibrates the vector space",
          "score": -2,
          "created_utc": "2026-02-23 17:09:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhpmqw",
      "title": "Improved retrieval accuracy from 50% to 91% on finance bench",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rhpmqw/improved_retrieval_accuracy_from_50_to_91_on/",
      "author": "hrishikamath",
      "created_utc": "2026-03-01 06:06:00",
      "score": 44,
      "num_comments": 10,
      "upvote_ratio": 0.98,
      "text": "Built a open source financial research agent for querying SEC filings (10-Ks are 60k tokens each, so stuffing them into context is not practical at scale).  \nBasic open source embeddings, no OCR and no finetuning. Just good old RAG and good engineering around these constraints. Yet decent enough latency. \n\nStarted with naive RAG at 50%, ended at 91% on FinanceBench. The biggest wins in order:\n\n1. Separating text and table retrieval\n2. Cross-encoder reranking after aggressive retrieval (100 chunks down to 20)\n3. Hierarchical search over SEC sections instead of the full document\n4. Switching to agentic RAG with iterative retrieval and memory, each iteration builds on the previous answer\n\nThe constraint that shaped everything. To compensate I retrieved more chunks, use re ranker, and used a strong open source model.\n\nBenchmarked with LLM-as-judge against FinanceBench golden truths. The judge has real failure modes (rounding differences, verbosity penalties) so calibrating the prompt took more time than expected.\n\nFull writeup: [https://kamathhrishi.substack.com/p/building-agentic-rag-for-financial](https://kamathhrishi.substack.com/p/building-agentic-rag-for-financial)  \nGithub: [https://github.com/kamathhrishi/finance-agent](https://github.com/kamathhrishi/finance-agent)",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rhpmqw/improved_retrieval_accuracy_from_50_to_91_on/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o81eeu3",
          "author": "Ok_Bedroom_5088",
          "text": "Some questions:\n\nOpen source parsers are a red flag, why did you choose this path for the 10Ks?\n\nWhere do you get the transcripts from? From my pov, there are only 2, maybe 3 trustworthy vendors.\n\nWhy do you outsource your news layer?\n\nNot all LLM tasks are document-bound; \"What's the 2008-26 EPS of AAPL?\" shouldn't trigger a single document search. How do you handle xbrl and pure numeric tasks?\n\n\"Yet enough latency\" I wouldn't care too much about this. Given your stack, you are already normal-high latency, which isn't a bad thing because it's a research product (that I'd compare to smth like claude finance, right?)\n\nSeperating text and tables makes sense, as long as you don't lose the context (e.g. footnote(s) x table(s))\n\nThe hard part is to detect malformed tables, and tables that should be tables, but use divs etc.\n\nWhy do you use openai? It's not SOTA on what matters in  your stack\n\nOptimized pgvector becomes a bottleneck.\n\nDo you have a migration plan, or does it run smooth at 5M+ for you?",
          "score": 3,
          "created_utc": "2026-03-01 11:46:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82it8z",
              "author": "hrishikamath",
              "text": "Very good questions. First let me clarify this is an open source project, not a product. There will be compromises. People pay $$$$$$ for these kinds of AI tools just because those AI tools source quality data too. 1) open source parser? I canâ€™t spend $$$$ for all filings to make sure everything is a hundred percent accurate? I donâ€™t have the budget. I am a indie developer who did this on the side. Processing all companies sec filings and making it available is a full time job which I donâ€™t have the money and bandwidth for 2) I use the cheapest one api ninjas. They are decent. Never really saw any issues so far. Itâ€™s usually delay in publishing the transcript that makes it an issue. 3) maintaining your own news service is very very difficult 4) yes, itâ€™s actually much easier to do that. In fact itâ€™s much easier for those tasks, I had a parser that just pulled these up from tables before. That was mostly for stock screening. Also because it reduces latency since now it searches raw sec 10k filings and so on. 5) yes pg vector works smoothly 6) I dint use OpenAI, I used qwen235B. But, I might use Gemini/openai soon. You can always switch the models and use your own version. I did try running with Claude but itâ€™s very expensive for me to maintain and run the benchmarks or even test in general.",
              "score": 1,
              "created_utc": "2026-03-01 15:56:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o81hacv",
          "author": "Ok_Signature_6030",
          "text": "the cross-encoder reranking step is doing a lot of heavy lifting here. going from 100 to 20 chunks before feeding to the LLM probably saves a ton on hallucinated answers from noisy context.\n\n  \none thing i'm wondering about â€” how does the table retrieval pipeline handle cases where financial data spans multiple tables across different sections? like when a company reports segment revenue in one table but the margin breakdown is three sections later. does the hierarchical search catch those cross-references or do you need the agentic loop to piece it together?\n\n  \nalso the LLM-as-judge benchmarking approach is solid. way better than trying to do exact match on financial figures.",
          "score": 2,
          "created_utc": "2026-03-01 12:10:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82muv3",
              "author": "hrishikamath",
              "text": "Right now it reasons it reasons about the question and then searches them parallel. For example, you ask the question â€œcalculate roa for $tsla in 2025â€ it reasons that roa=(net income/total assets) so it invokes two separate questions: â€œwhat is net income ?â€ And â€œWhat is total assets?â€ This is after revolting the right document of the company. As in it searches in $tsla 2025 sec 10k filing",
              "score": 1,
              "created_utc": "2026-03-01 16:16:08",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o85tvdk",
              "author": "walewaller",
              "text": "What latency is added by the cross encoder model? My app is already high latency, so wondering if I should consider this step. Currently Iâ€™m extracting 150-200 chunks and performing a crude reranking based on retrieval frequency which is hit or miss.",
              "score": 1,
              "created_utc": "2026-03-02 02:23:01",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o80faih",
          "author": "D_E_V_25",
          "text": "Most motivating line \n\"50% -> 91% that too on Fintech \" \n\nAmazing brother ðŸ‘.. I definitely wanted to get to Fintech thing as I have already been working a good amount maths and other academic subjects close to 700k chunks of data with 350m tokens and 688k nodes of graph rag...  I have a very very high  accuracy by the way as well ðŸ˜Ž (I am constantly improving as well)  .. bcs of architecture I had mapped.. \n\nA skeleton of the work :: \nhttps://github.com/pheonix-delta/WiredBrain-Hierarchical-Rag\n\nBut, \nI can truly understand how much accuracy matters and more importantly when u r dealing with Fintech and core maths symbols..  \n\nGood work !! keep going ðŸ˜ŽðŸ‘",
          "score": 3,
          "created_utc": "2026-03-01 06:18:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80otgj",
          "author": "rigatoni-man",
          "text": "Well done, and thank you for sharing the writeup",
          "score": 1,
          "created_utc": "2026-03-01 07:43:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82mvuv",
              "author": "hrishikamath",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-03-01 16:16:16",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o80gke0",
          "author": "stepperbot6000",
          "text": "Take me as your pupil brother",
          "score": -1,
          "created_utc": "2026-03-01 06:29:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rej56g",
      "title": "Lessons from shipping a RAG chatbot to real users (not just a demo)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rej56g/lessons_from_shipping_a_rag_chatbot_to_real_users/",
      "author": "cryptoviksant",
      "created_utc": "2026-02-25 17:09:55",
      "score": 38,
      "num_comments": 23,
      "upvote_ratio": 0.78,
      "text": "I've been building a chatbot product (bestchatbot.io, works on Discord and websites) where users upload their docs and the bot answers questions from that content. Wanted to share some stuff I learned going from \"cool demo\" to \"people are actually paying for this\" because the gap between those two is way bigger than I expected.\n\n**Vanilla RAG gets you maybe 30% of the way there (no joke)**\n\nWhen I started I did the standard thing. Chunk docs, embed them, retrieve top-k, stuff into context, generate. It worked great on demos. Then real users uploaded real docs and it fell apart. The problem isn't retrieval in isolation, it's that real documents have structure, context, and relationships between sections that get destroyed when you just chunk and embed.\n\n**What actually mattered in production**\n\nWithout going too deep into our specific implementation, here's what moved the needle the most:\n\n* **Document quality > retrieval sophistication.** I spent weeks tweaking retrieval and got maybe 10% better. Then I added better doc preprocessing and got a bigger jump overnight. Garbage in garbage out is painfully real.\n* **Evaluation is everything.** You can't improve what you can't measure. I built a testing interface where I could ask questions and see exactly which sources the bot cited. That feedback loop was more valuable than any architecture change.\n* **Users don't care about your retrieval method.** They care about two things: did it answer correctly, and how fast. Our response time is 10-20 seconds which people complain about constantly. Nobody has ever asked me what embedding model we use.\n* **The knowledge base needs to be treated as a living thing.** We added a system where the bot learns from moderator corrections in Discord automatically. That continuous improvement loop has been surprsingly impactful compared to just static doc retrieval.\n\nMost of the accuracy gains came from boring stuff. Better chunking, better preprocessing, better prompting, testing obsessively. The architecture matters but it's maybe 30% of the outcome. The other 70% is everything around it.\n\nCurious what other people building production RAG systems have found. What moved the needle most for your accuracy?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rej56g/lessons_from_shipping_a_rag_chatbot_to_real_users/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7e9r1j",
          "author": "zsrt13",
          "text": "This is basic 101 stuff. There are no lessons here",
          "score": 10,
          "created_utc": "2026-02-25 20:59:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7dll7a",
          "author": "ChapterEquivalent188",
          "text": "jesus another wrapper and chatwithpdf.......move on ....all your text is shouting \"garbage in, garbage out\"  \n\nyou even called it by yourself, so how do you solve it ? you mention so many deadends.....how about semantic chunking ? how a about specialized parsers? what is your outcome without happypath tests ? If you are the world gratetes chef with the world best recipe, what do you get when your Ingrediens is mixed up mud ?",
          "score": 3,
          "created_utc": "2026-02-25 19:06:29",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7epzbm",
              "author": "welcome-overlords",
              "text": "Bro get my feet outta your mouth. Fuck im so bored of these ai posts selling their idea. Can i just talk with fellow fucking humans who are trying to solve these problems and maybe hear some ideas",
              "score": 4,
              "created_utc": "2026-02-25 22:15:10",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7eq3z7",
                  "author": "welcome-overlords",
                  "text": "If u r a fellow human solving thess issues and maybe getting paid for it so u dont want me to pay u, reply",
                  "score": 1,
                  "created_utc": "2026-02-25 22:15:48",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7h4u7c",
                  "author": "ChapterEquivalent188",
                  "text": "im here ;) always happy to meet peps focusing on the garbage in problem ",
                  "score": 1,
                  "created_utc": "2026-02-26 07:09:05",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7dy9vd",
              "author": "cryptoviksant",
              "text": "you wish this was just a wrapper ;) \n\n[bestchatbot.io](http://bestchatbot.io) \n\nHave a look. If you can built a similar thing with only \"chatwithdpf\" wrapper, I'll give you $1000. On god.",
              "score": -4,
              "created_utc": "2026-02-25 20:05:24",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7h4k2q",
                  "author": "ChapterEquivalent188",
                  "text": "LOL----https://github.com/2dogsandanerd/RAG_enterprise_core\n\nshow me your results on this and i believe everything you claim ;) \nhttps://github.com/2dogsandanerd/Liability-Trap---Semantic-Twins-Dataset-for-RAG-Testing",
                  "score": 2,
                  "created_utc": "2026-02-26 07:06:37",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7dgjwb",
          "author": "rigatoni-man",
          "text": "Iâ€™d love to know more about your evaluation.  How does the interface work?  How/what do you evaluate?",
          "score": 1,
          "created_utc": "2026-02-25 18:43:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7dygs0",
              "author": "cryptoviksant",
              "text": "The interface is tied to a backend running on FastAPI, which connects the front-end (Vite) with the backend (python). You can actually have a look at [https://bestchatbot.io](https://bestchatbot.io) , any feedback is welcomed!",
              "score": 0,
              "created_utc": "2026-02-25 20:06:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7eaaav",
                  "author": "rigatoni-man",
                  "text": "Ah I meant how do you / did you test to validate your strategies?",
                  "score": 2,
                  "created_utc": "2026-02-25 21:01:43",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7h4lvl",
                  "author": "ChapterEquivalent188",
                  "text": "LOL",
                  "score": 1,
                  "created_utc": "2026-02-26 07:07:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7gbhdp",
          "author": "hhussain-",
          "text": "Lessons learned, many missed usually. Production is not \"yeah... I can build this, nevermind the post\"\n\nAnyway, for performance you might think of using rust. Either as tour backend, or processors and lib used in python. Some oython lib are already rust or C under the hood. FastPAI is good choice, but rust performance is different level.",
          "score": 1,
          "created_utc": "2026-02-26 03:33:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ggqvn",
          "author": "nbass668",
          "text": "Sorry there is nothing learned. You just spit out something we all do.. and every comment you spamming your website.\n\nReporting this to the mods as Spam sugar coated with a fake knowledge",
          "score": 1,
          "created_utc": "2026-02-26 04:06:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hsid1",
              "author": "cryptoviksant",
              "text": "sure go ahead",
              "score": 0,
              "created_utc": "2026-02-26 10:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rebd0i",
      "title": "Built a four-layer RAG memory system for my AI agents (solving the context dilution problem)",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "author": "Independent-Cost-971",
      "created_utc": "2026-02-25 12:02:23",
      "score": 34,
      "num_comments": 9,
      "upvote_ratio": 0.93,
      "text": "We all know AI agents suffer from memory problems. Not the kind where they forget between sessions but something like context dilution. I kept running into this with my agents (it's very annoying tbh). Early in the conversation everything's sharp but after enough back and forth the model just stops paying attention to early context. It's buried so deep it might as well not exist.\n\nSo I started building a four-layer memory system that treats conversations as structured knowledge instead of just raw text. The idea is you extract what actually matters from a convo, store it in different layers depending on what it is, then retrieve selectively based on what the user is asking (when needed).\n\nDifferent questions need different layers. If someone asks for an exact quote you pull from verbatim. If they ask about preferences you grab facts and summaries. If they're asking about people or places you filter by entity metadata.\n\nI used workflows to handle the extraction automatically instead of writing a ton of custom parsing code. You just configure components for summarization, fact extraction, and entity recognition. It processes conversation chunks and spits out all four layers. Then I store them in separate ChromaDB collections.\n\nBuilt some tools so the agent can decide which layer to query based on the question. The whole point is retrieval becomes selective instead of just dumping the entire conversation history into every single prompt.\n\nTested it with a few conversations and it actually maintains continuity properly. Remembers stuff from early on, updates when you tell it something new that contradicts old info, doesn't make up facts you never mentioned.\n\nAnyway figured I'd share since context dilution seems like one of those problems everyone deals with but nobody really talks about.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rebd0i/built_a_fourlayer_rag_memory_system_for_my_ai/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7bggdt",
          "author": "Specific_Expert_2020",
          "text": "Awesome!\n\nI am building something very similar at work.. but i cannot disclose to much.\n\nSimilar approach with criteria and fields to help differentiate the data in the system.\n\nIs there a reason you kept it to 4?\n\nI am learning this whole RAG thing and I see the section mentions \"why 4\" but did you limit it? Or was 4 enough to keep the response accurate.\n\nJust curious :)",
          "score": 3,
          "created_utc": "2026-02-25 12:53:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bkror",
              "author": "Independent-Cost-971",
              "text": "I only used 4 in the blog because it made testing and demonstrating the results much cleaner and easier to follow. When youâ€™re explaining RAG concepts, smaller numbers help keep the examples readable and the behavior obvious.\n\nIn a real project, youâ€™d absolutely use more than 4.",
              "score": 1,
              "created_utc": "2026-02-25 13:19:22",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7bldbj",
                  "author": "Specific_Expert_2020",
                  "text": "Also I hope my comment did not come off as downplaying your post as it is way more technical than what I am working on so great share.\n\nI appreciate the follow up and sharing.\n\nI had concerns of over fielding as I work in a MSSP and thing vary by a variety of fields.\n\nSo as I develop the roll out.. I am seeing the fields scope creeping.\n\nThank you for the insight.",
                  "score": 2,
                  "created_utc": "2026-02-25 13:22:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7coofs",
          "author": "lez566",
          "text": "I did something similar. I built an AI agent that learns as you talk to it and builds a living profile of you and your needs. Then anytime a message is sent, the tool gives this living profile, a keyword search, a semantic search and the last n messages. Itâ€™s excellent and never forgets.",
          "score": 2,
          "created_utc": "2026-02-25 16:37:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b95tc",
          "author": "Independent-Cost-971",
          "text": "I wrote a whole blog about this that goes way deeper if anyone's interested: [https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/](https://kudra.ai/building-production-grade-agent-memory-from-scratch-llm-context-cost-fix/)",
          "score": 3,
          "created_utc": "2026-02-25 12:03:15",
          "is_submitter": true,
          "replies": [
            {
              "id": "o7bq031",
              "author": "arun4567",
              "text": "This is good. I was looking for some thing like this since my agent gets into loops and forgets that its been provided the details before. How do you handle updates of information that's already been provided,",
              "score": 3,
              "created_utc": "2026-02-25 13:48:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7qh8id",
          "author": "jd808nyc",
          "text": "Do you have a diagram of the flow? Curious how youâ€™re deciding what not to remember? Thatâ€™s been the harder problem for me than memory itself.\n\nA lot of these systems slowly turn into hoarding machines where everything feels important until retrieval quality quietly tanks.\n\nHave you tried any kind of decay or scoring over time, or is it all just accumulating right now?",
          "score": 1,
          "created_utc": "2026-02-27 17:32:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8409ij",
          "author": "loookashow",
          "text": "Oh, itâ€™s very interesting topic! I am working on multi layer agent memory right now in FoxNose. As the first step I am building a MCP that can be used by Claude/ChatGPT as a multi session memory with the same principles of memory as you described. actually, I am supposing the most challenging here is tests/benchmarking, especially for cases where count of â€œmemoriesâ€ grows",
          "score": 1,
          "created_utc": "2026-03-01 20:13:55",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rhx8ka",
      "title": "How can we build a full RAG system using only free tools and free LLM APIs?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rhx8ka/how_can_we_build_a_full_rag_system_using_only/",
      "author": "Me_On_Reddit_2025",
      "created_utc": "2026-03-01 13:25:44",
      "score": 34,
      "num_comments": 18,
      "upvote_ratio": 0.92,
      "text": "Hey everyone,\n\nIâ€™m trying to build a complete RAG pipeline end-to-end using only free or open resources no paid APIs, no enterprise credits.\n\nGoal:\n\nDocument ingestion (PDFs, web pages, etc.)\n\nChunking + embeddings\n\nVector storage\n\nRetrieval\n\nLLM generation\n\nBasic UI (optional but nice)\n\nConstraints:\n\nPrefer open-source stack\n\nIf APIs are used, they must have a real free tier (not temporary trial credits)\n\nDeployable locally or on free hosting\n\nToken limits are fine this is for learning + small-scale use\n\nQuestions:\n\nWhatâ€™s the most practical free embedding model right now?\n\nBest free vector DB for production-like experimentation (FAISS? Chroma? Weaviate local?)\n\nWhich LLMs can realistically be called for free via API?\n\nIs fully local (Ollama + open weights) more practical than chasing free hosted APIs?\n\nAny GitHub repos that show a clean, minimal, real-world RAG stack?\n\nIâ€™m looking for something thatâ€™s actually sustainable not just a weekend demo that dies when credits expire.\n\nWould appreciate architecture suggestions or real stacks youâ€™ve seen work.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rhx8ka/how_can_we_build_a_full_rag_system_using_only/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o81ucj9",
          "author": "rpg36",
          "text": "I'd suggest:\n- docling (document conversion and chunking)\n- chroma DB (local embedded vector database)\n- Ollama (easy setup run whatever model you want that fits on your system)\n\n\nI'd keep it simple as a first step for learning. Just simply take the user query, embed it, get the top K results. Make the a JSON array add them to an overall promo that's something like:\n\n\n\nYou are a helpful customer assistant agent answer the following question:\n$USER_QUESTION\n\nUae the following relevant snippets of information from our documentation to answer their question. If the answer isn't found in the information say you don't know.\n\n$JSON_DOCS\n\n\n\nThat should be enough to get you started running locally and you'll probably (hopefully) be surprised as to how well such a simple little setup will work. Especially for learning with small data sets.",
          "score": 13,
          "created_utc": "2026-03-01 13:44:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o823qqo",
          "author": "EnoughNinja",
          "text": "Your stack is solid for what you're describing, here's what I'd go with:\n\nfor embeddings: `bge-small-en-v1.5` is a very strong free default right now, good retrieval quality and runs local with no API. `all-MiniLM-L6-v2` if you just want fast and lightweight. If you have a GPU `bge-large` is a noticeable step up\n\nVector DB -Chroma to get started fast, Qdrant if you want something closer to production behavior. At small scale both are fine, FAISS works too but you end up writing more boilerplate around it\n\nLLM: Ollama + Mistral 7B or Llama 3 8B locally is way more sustainable than chasing free hosted tiers that rate limit you quickly once you start iterating. If your machine can't run it, Groq's free tier is a decent hosted fallback if you can live with the limits\n\nIngestion: [Unstructured.io](http://Unstructured.io) (open source) for PDFs and web pages, recursive splitting around 500 tokens (\\~1-2k chars) with some overlap as a starting point.\n\nThis stack works well for static documents but where it falls apart is conversational data like email threads, Slack exports etc. Standard chunking destroys who said what and when, so you get chunks that look right but your LLM starts confidently attributing decisions to the wrong person or resurfacing stuff that was walked back three messages later. I work on this exact problem at iGPT (igpt.ai), happy to share notes if that's relevant to what you're building.\n\nFor repos, LangChain's `rag-from-scratch` series and LlamaIndex's starter tutorials are both solid walkthroughs.",
          "score": 10,
          "created_utc": "2026-03-01 14:39:20",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82vcuu",
          "author": "Lazy-Kangaroo-573",
          "text": "I was in your exact shoes a few months ago. I built a fully production-grade Legal RAG system on a $0 budget, deployed on a 512MB RAM free tier server. No Ollama heavy RAM limits, real cloud deployment.\nI just published my entire architecture, failures (like ChromaDB deadlocks and OOM kills), and fixes in a field guide. [Field Guide](https://heyzine.com/flip-book/6b8aba4153.html)\nI used FastAPI, Qdrant Cloud (1GB free), Jina AI Embeddings, and model of your own choice (via OpenRouter.\nYou can flip through my entire architecture here: https://heyzine.com/flip-book/6b8aba4153.html  Hope this helps your build!",
          "score": 6,
          "created_utc": "2026-03-01 16:57:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81tdl8",
          "author": "Morphos91",
          "text": "I use postgresql as vector database, local ollama for embeddings and Gemini for answer generation. You could use a local llm but would not advise that.\n\nOn Gemini you have some free API calls, maybe that's enough for your use case",
          "score": 3,
          "created_utc": "2026-03-01 13:38:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o82370p",
              "author": "Skin_Life",
              "text": "Groq is also a great alternative when you're looking for decent open source models hosted online for free, Kimi K2 being the best one with 10k Tokens Per Minute / 300k TPDay limit",
              "score": 2,
              "created_utc": "2026-03-01 14:36:15",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o82l5bn",
              "author": "vidibuzz",
              "text": "I think you're going to find quite a few fans of Qwen 3.5, now a fully native vision model for the vLLM. Just came out 2 days ago but the testing so far has been amazing for 27B and 35B. Value depends on whether you want to cross over into indexing images and videos in addition to text. If you're looking for grounded truth in the results the additional visual information may become indispensable.",
              "score": 1,
              "created_utc": "2026-03-01 16:07:53",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o81vha2",
          "author": "z0han4eg",
          "text": "You can start completely free:  \nManticore local instance with huggingface embedings(something like sentence-transformers/all-MiniLM-L6-v2) - open source. Just believe me, at some point you will need a hybrid search, not just vecror - FROM/WHERE magic.  \nCloudflare AI workers free plan - limited daily free llm calls:  \nreranker - /cf/baai/bge-reranker-base  \nintent router - you can start with cf/meta/llama-3.1-8b-instruct-fast  \nresponse model - choose something from cf  \nlocal KV storage is preffered on free CF plan coz KV \"writes\" will hit you hard - just use any local db  \nAt the beginning you can use CF KV, but you can't scale it for free",
          "score": 2,
          "created_utc": "2026-03-01 13:51:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o821eez",
          "author": "gidea",
          "text": "well, hear me out:\n\n- run a vector db locally\n- use llama locally\n- create the chunks & embeddings with an app you build yourself\n\nI â€œneedâ€ a RAG only for a few hundred docs (marketing, sales collateral, proposals, onboarding plans) so rather than setting up the traditional RAG with a cloud hosted DB, re-embed docs constantly and deal with all the agent integration, I wanted to know if it was possible to build a more local-friendly version.\n\nYou would be surprised how well it performs, by using a hybrid approach to retrieve from the vector DB.\n\nI also added support for Claude & ChatGPT, so I can use my API keys and switch from llama to cloud models.\n\nMy docs are rather simple, my chunking uses meta-data to improve retrieval, and I think I might have figured out a nice way to run this on my M4 mac mini (which I had before the clawd chaos).\n\nThis is an app I build in Swift, with Claude Code & help from Xcode (which has great suggestions for swift apps). The app indexes local docs, uses textutil to extract from docx, pdfs etc and build markdown chunks I can further edit, and then uses the MLX Apple support to create embeddings with some embeddings model that scores high on huggingface (BGE something something). I picked sqlite vec instead of postgres with pgvector, but that was just because itâ€™s easier to configure and Iâ€™m thinking of publishing the app for free on the appstore (and itâ€™s a better choice if you want to distribute the app).\n\nif others are curious or have some feedback, check it out: https://github.com/gidea/chunkpad-swift",
          "score": 2,
          "created_utc": "2026-03-01 14:26:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8232oz",
          "author": "bella2859",
          "text": "Qdrant is also good u can try and pair with bge-m3",
          "score": 2,
          "created_utc": "2026-03-01 14:35:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8296wl",
          "author": "Milan_Robofy",
          "text": "Why built a rag in first place rather than using something like Google File Search? I have used Google File Search and its top quality for all kind of use cases.",
          "score": 1,
          "created_utc": "2026-03-01 15:08:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82mdo2",
          "author": "yoko_ac",
          "text": "You could also take a look at AnythingLLM: [https://github.com/Mintplex-Labs/anything-llm](https://github.com/Mintplex-Labs/anything-llm)",
          "score": 1,
          "created_utc": "2026-03-01 16:13:49",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84jyzd",
          "author": "Clipbeam",
          "text": "I built mine on llama.cpp, LanceDB and Qwen3. In all honesty I didn't consider trying to use free hosted models cause of privacy and data security concerns. What you can basically do is run a node server with https://node-llama-cpp.withcat.ai, and then interact with the model directly via js. You can then store embeddings using https://lancedb.com. I don't have a Github repo to share, but just following tutorials/documentafion for those two will already get you quite far?\n\nThe sustainable product I built this way is https://clipbeam.com.",
          "score": 1,
          "created_utc": "2026-03-01 21:55:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o852bkl",
          "author": "modpotatos",
          "text": "\\>  Whatâ€™s the most practical free embedding model right now? \n\ndepends on your hardware, if you can? qwen3 embeddings are insane, otherwise do miniLM or bge (i forget exact names)\n\n\\> Which LLMs can realistically be called for free via API?\n\nchatjimmy is lobotomized but at 17ktps and completely free, i dont see why not. otherwise [https://gist.github.com/mcowger/892fb83ca3bbaf4cdc7a9f2d7c45b081](https://gist.github.com/mcowger/892fb83ca3bbaf4cdc7a9f2d7c45b081)\n\n\\> Best free vector DB for production-like experimentation (FAISS? Chroma? Weaviate local?)\n\nneed throughput? [https://github.com/alibaba/zvec](https://github.com/alibaba/zvec) need scaling? cloudflare vectorize free tier, or a docker hosted something but its heavyweight.\n\n\\> Is fully local (Ollama + open weights) more practical than chasing free hosted APIs?\n\nif you care about quality, no. a lot of providers have really good free tiers (see gist above)",
          "score": 1,
          "created_utc": "2026-03-01 23:36:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85ix6a",
          "author": "Negative_Day_3058",
          "text": "Im working to prepare a rag using only local infra (intel i9 + nvidia 3090 24 gb vram) with Ubuntu+ ollama and a bunch of models.\nRight now we are in the testing stage, using different embeddings models, using differents chunk size and also different models,Â  saving the chunks in markdown and also in plain text, and we prepare a \"test benchmark\" against a 500 questions and answer \"the ideal answer\" so with that we test our knowdlege base (100+ pdf, database statisticall analysis+ a lot of xls files)Â \nOne of the best combination we have (embeddings BGE-M3 + chunk size medium 1024+ ollama llm model gpt os - 20b\n\n\nhttps://www.bentoml.com/blog/a-guide-to-open-source-embedding-models\n\n\nThings to considere, try to use 5 different models to create the chunks, with 3 differents chunk size, and test those against different llm models that work for us)\n\n\n\n",
          "score": 1,
          "created_utc": "2026-03-02 01:15:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o8201wt",
          "author": "Liangjun",
          "text": "Copy And paste into your post to Gemini, she will give a better /comprehensive answer than anyone else",
          "score": 1,
          "created_utc": "2026-03-01 14:18:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o81xhka",
          "author": "mprz",
          "text": "start slowly, by learning how to use google or search reddit",
          "score": 0,
          "created_utc": "2026-03-01 14:03:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o851vs8",
              "author": "gg223422",
              "text": "lmao",
              "score": 1,
              "created_utc": "2026-03-01 23:34:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o8293kq",
          "author": "CapitalShake3085",
          "text": "Fully local 0 api costs\n\nhttps://github.com/GiovanniPasq/agentic-rag-for-dummies",
          "score": 0,
          "created_utc": "2026-03-01 15:08:18",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfinck",
      "title": "The \"Silent Bottleneck\" in Production RAG: Why Cosine Similarity Fails at Scale",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rfinck/the_silent_bottleneck_in_production_rag_why/",
      "author": "Flat-Outside4620",
      "created_utc": "2026-02-26 18:46:59",
      "score": 28,
      "num_comments": 5,
      "upvote_ratio": 0.89,
      "text": "Most RAG tutorials work great on a 100-document corpus, but once you scale to production levels, a \"silent flaw\" usually emerges: **Document Redundancy.**\n\nIâ€™ve spent some time benchmarking retrieval performance and noticed that as the corpus grows, simple Cosine Similarity often returns the same document multiple times across different chunk sizes or overlapping slices. This effectively \"chokes\" the LLMâ€™s context window with redundant data, leaving no room for actual diverse information.\n\nIn my latest write-up, I break down the architecture to move past this:\n\n* **The Problem:** Why kNN/Cosine Similarity alone creates a retrieval bottleneck.\n* **The Fix:** Implementing Hybrid Search (**BM25 + kNN**) for better keyword/semantic balance.\n* **Diversity:** Using Maximal Marginal Relevance (**MMR**) to ensure the top-k results aren't just 5 versions of the same paragraph.\n* **Implementation:** How to leverage the native Vector functionality in **Elasticsearch** to handle this at scale.\n\nIâ€™ve included some benchmarks and sample code for those looking to optimize their retrieval layer.\n\n**Full technical breakdown here:**[https://medium.com/@dhairyapandya2006/going-beyond-cosine-similarity-hidden-bottleneck-for-production-grade-r-a-g-437ae0eaafa5](https://medium.com/@dhairyapandya2006/going-beyond-cosine-similarity-hidden-bottleneck-for-production-grade-r-a-g-437ae0eaafa5)\n\nIâ€™d love to hear how others are handling diversity in their retrieval- are you guys sticking to Re-rankers, or are you seeing better ROI by optimizing the initial search query?",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rfinck/the_silent_bottleneck_in_production_rag_why/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7oldvj",
          "author": "stingraycharles",
          "text": "This is absolutely beginner level RAG and I guarantee you that production systems that only use embedding distance donâ€™t exist or are intended to be simple.",
          "score": 5,
          "created_utc": "2026-02-27 11:16:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7lrj78",
          "author": "Cotega",
          "text": "You should take out the em dashes from your blog as it is pretty clear the content was AI generated. Perhaps you used your own experience, but this just makes it look like you got AI to create the blog for you.",
          "score": 7,
          "created_utc": "2026-02-26 23:13:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7pf0mf",
          "author": "singh_taranjeet",
          "text": "I think the key takeaway here is that pure cosine similarity almost *always* hits a wall once youâ€™re beyond toy corpora because it ends up returning the same document or very similar chunks over and over, which fills up your context window with redundant info instead of diverse evidence. \n\nThatâ€™s why hybrid search (lexical + vectors) or diversity-aware selection strategies like MMR/DF-RAG tend to outperform vanilla RAG at scale; you want relevance *and* non-redundancy. If youâ€™ve actually tested this in a production stack and found something better than MMR, itâ€™d be great to hear from Mem0 on what empirically worked",
          "score": 2,
          "created_utc": "2026-02-27 14:28:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7rcdfd",
              "author": "Single-Constant9518",
              "text": "Totally agree, redundancy can really mess with performance. Iâ€™ve seen some setups using a blend of query expansion and MMR that seem promising too. Have you tried any other selection strategies beyond MMR? Always curious about what works best in real-world scenarios.",
              "score": 1,
              "created_utc": "2026-02-27 20:02:13",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7tc6xs",
                  "author": "No-Consequence-1779",
                  "text": "Seems like a chunking strategy that does that may also not be good even for a simple system.Â ",
                  "score": 1,
                  "created_utc": "2026-02-28 02:42:55",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rf7xf6",
      "title": "What's your experience with hybrid retrieval (vector + BM25) vs pure vector search in RAG systems?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rf7xf6/whats_your_experience_with_hybrid_retrieval/",
      "author": "Beneficial-Grab4442",
      "created_utc": "2026-02-26 11:39:42",
      "score": 25,
      "num_comments": 17,
      "upvote_ratio": 0.94,
      "text": "I've been building RAG systems and recently switched from pure vector \n\nsearch (top-k cosine similarity) to hybrid retrieval combining vector \n\nsearch with BM25 keyword matching.\n\n\n\nThe difference was significant â€” accuracy went from roughly 60% to 85% \n\non my test set of 50 questions against internal documentation.\n\n\n\nMy theory on why: vector search is great at semantic similarity but \n\nmisses exact terminology. When a user asks, \"What's the PTO policy?\" \n\nthe vector search finds chunks about \"vacation time\" and \"time off \n\nbenefits\" but sometimes misses the exact chunk that uses the acronym \n\n\"PTO.\" BM25 catches that.\n\n\n\nFor those running RAG in production:\n\n\n\n1. Are you using pure vector, hybrid, or something else entirely?\n\n2. How much did re-ranking (cross-encoder) improve your results on top \n\n   of hybrid search?\n\n3. What's your chunk size? I settled on \\~500 chars with 100 overlap \n\n   after a lot of experimentation. Curious what others landed on.\n\n4. Anyone tried HyDE (hypothetical document embeddings) in production? \n\n   Interesting in theory but I'm unsure about the latency hit.\n\n\n\nWould love to hear real production numbers, not just tutorial benchmarks.",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rf7xf6/whats_your_experience_with_hybrid_retrieval/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7jw7t3",
          "author": "adukhet",
          "text": "Your questions donâ€™t have one single truth unfortunately, all these depends on use-case or depends on data. If your system intends to solve QA, technical/code based data retrieval most likely BM25 will provide better results, but if use case is enterprise/business questions then shifting towards semantic will make more sense. These stuff you can test on golden dataset during configuration and find out whatâ€™s the best parameters for customer given dataset. \n\nLast system i tested provided ndcg 0.74 on retrieving information and we gained very small to almost none improvement by applying rerankers thus plug out that component in order to reduce latency.. again data and/or user requirements dependent.\n\n\nChunk size.. again dependent on input data a lot. Are you working with long law-based documents or are you working with small-FAQ type of data. Each size will have its pros and cons depending on the problem you are trying to solve. Donâ€™t believe if people say 1024 is best or 512 is best. You have to experiment. \n\nLastly, hyde will not cause latency issues if you apply it right, hope above points help you",
          "score": 3,
          "created_utc": "2026-02-26 17:50:28",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l960k",
              "author": "Beneficial-Grab4442",
              "text": "This is super helpful, especially the point about BM25 being stronger for QA/technical retrieval vs semantic for enterprise questions. That actually lines up with what I'm seeing â€” my test set is mostly internal docs with a lot of acronyms and domain-specific terms, which is probably why BM25 made such a big difference for me.\n\nInteresting that rerankers barely moved the needle for you at 0.74 NDCG. Was that already with hybrid retrieval, or on top of pure vector? I'm wondering if there's a ceiling effect where if your initial retrieval is already good enough, reranking just adds latency for marginal gains.\n\nAnd good call on golden datasets â€” I've been building mine manually but it's tedious. Any tips on scaling that process?",
              "score": 1,
              "created_utc": "2026-02-26 21:40:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o84zo2z",
                  "author": "adukhet",
                  "text": "yeah also always ask for company glossaries they are hidden gems and will move your retrieval to next level eg in a business dept. people can keep calling NER (net expansion rate) in documents but if you dont use glossary than no way your search would do fine\n\non ndgm q, it was a alpha tuned weighted fusion but whether it depends or not you never know before testing. and lastly i wouldnt go crazy on creating your own eval sets, use whats available on the internet, and then go hunt for customer bc you ll nver be able to generate a better dataset than a business data itself",
                  "score": 1,
                  "created_utc": "2026-03-01 23:21:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hzouo",
          "author": "fabkosta",
          "text": "Most of the time hybrid is superior to vector alone or BM 25 alone. (There are exceptions, as always.)\n\nHyDE can be useful, but comes at an extra cost for information retrieval both financially, but more importantly also increasing retrieval times. That may be prohibitive, depending on the use case.\n\nChunk size is very dependent on your data and problem. It cannot be generalized easily. Xwitter data has very distinct characteristics than prose. Generally, a good start is to think of a paragraph or multiple paragraphs or a section in an article as a chunk.\n\nRe-ranking can also be useful, but, again, it depends on your problem and data. It's hard to generalize these things. Someone else's improvements may not be reproducible with you.\n\n80% of effort for a typical RAG project are optimizing for such stuff. You try, you fail, you try something else. That implies you need a systematic approach to measure your experiments and improvements they deliver. Never simply believe your ideas are \"good\", always measure.",
          "score": 1,
          "created_utc": "2026-02-26 11:52:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l99l1",
              "author": "Beneficial-Grab4442",
              "text": "This really resonates. I spent way more time tweaking retrieval than I did on the actual LLM prompting side. And you're right about measuring â€” I only caught the accuracy gap because I forced myself to build a proper eval set before changing anything.\n\nThe chunk size point is well taken too. I started with fixed character counts which felt arbitrary. Thinking about switching to more semantic boundaries like paragraphs or sections. Did you find that approach works better in practice, or is it more about matching the chunk granularity to the types of questions users actually ask?",
              "score": 1,
              "created_utc": "2026-02-26 21:41:14",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7lcist",
                  "author": "fabkosta",
                  "text": "Most time some sort of semantic chunking (paragraph, section) is preferable, in my opinion. Libraries like Langchain can help you with chunking, they have different chunking strategies built in.",
                  "score": 1,
                  "created_utc": "2026-02-26 21:56:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7i6ss3",
          "author": "Dapper-Turn-3021",
          "text": "yea hybrid works like a charm most of the time, for our product we are using the same strategy and we are achieving good results till now, although itâ€™s some time give slow or unrelated answers but that could be improve by further training",
          "score": 1,
          "created_utc": "2026-02-26 12:43:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l9b63",
              "author": "Beneficial-Grab4442",
              "text": "Good to hear it's working well for you too. When you say it sometimes gives slow or unrelated answers â€” is the slowness on the retrieval side or the generation side? I've been thinking about adding a relevance threshold so if the top retrieved chunks score below a certain confidence, the system just says \"I don't know\" instead of hallucinating an answer from weak context.\n\nCurious what further training you're considering â€” fine-tuning the embedding model on your domain data?",
              "score": 1,
              "created_utc": "2026-02-26 21:41:27",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7nc4kd",
                  "author": "Dapper-Turn-3021",
                  "text": "itâ€™s from retrieval side, yes I already have this logic in place so only relevance chunks is going into the LLM",
                  "score": 1,
                  "created_utc": "2026-02-27 04:45:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ig9f6",
          "author": "Dense_Gate_5193",
          "text": "https://github.com/orneryd/NornicDB. MIT licensed and handles the entire rag pipeline including embedding the original query with embedding and reranking models running in-process. drops full RRF search latency on a 1m embedding corpus to 7ms including http transport.",
          "score": 1,
          "created_utc": "2026-02-26 13:40:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l9f7m",
              "author": "Beneficial-Grab4442",
              "text": "Haven't come across NornicDB before â€” 7ms for full RRF search on 1M embeddings is impressive. The in-process embedding + reranking is a nice touch too, eliminates the network overhead of calling external services.\n\nHow's the documentation and community around it? MIT license is a big plus. Might spin it up this weekend and benchmark it against my current Postgres + pgvector setup.\n\nBetween i have STARED the repo will check it when i come out of the cave",
              "score": 2,
              "created_utc": "2026-02-26 21:42:00",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7l9xqi",
                  "author": "Dense_Gate_5193",
                  "text": "Oh i would LOVE to see benchmarks on other hardware and with varied datasets. iâ€™ve worked hard on the latency tuning with a lot of various optimizations so im excited to see it brutalized so i can see where it tips over. ðŸ«¶ any any all feedback is appreciated",
                  "score": 1,
                  "created_utc": "2026-02-26 21:44:28",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7jbxfv",
          "author": "Ascending_Valley",
          "text": "What embedding method and vector size?  We've had good results reducing native embedding vectors with various methods (PCA, PLS, UMAP, tSNE, proprietary methods) to the 25-100 range.  The goal of the reduction is to make distance more related to strong coupling and important facets, dropping low signal, noisy dimensions.\n\nWe've also use optimized weighted KNN to tune to dimensional weights (using an advanced hyper tuning method; this is still pending, but promising).",
          "score": 1,
          "created_utc": "2026-02-26 16:16:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7l9gtx",
              "author": "Beneficial-Grab4442",
              "text": "This is a really interesting angle I hadn't considered. So you're essentially compressing the embedding space to focus on the most meaningful dimensions before doing similarity search? That's clever â€” I imagine it also speeds up retrieval significantly with smaller vectors.\n\nA few questions: are you applying the dimensionality reduction per-corpus or using a general model? And how do you evaluate whether the reduction is actually improving relevance vs just making search faster? I'd worry about losing important signal in niche domains.\n\nThe weighted KNN approach sounds promising too â€” would love to hear how that turns out.",
              "score": 1,
              "created_utc": "2026-02-26 21:42:12",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7m51ku",
          "author": "geekheretic",
          "text": "A big piece I am discovering is the query decomposing, looking for keywords or other meta data to help on chunk ranking.",
          "score": 1,
          "created_utc": "2026-02-27 00:28:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7nppsi",
          "author": "Independent-Bag5088",
          "text": "\\#3. What is your document type? Is there a reason to settle on \\~500 chars? If the document has some structure to it, it would be beneficial to preserve the structure (even if it creates uneven chunks). For my RAG project with SEC filings, I used section-aware chunking with 15% overlap.",
          "score": 1,
          "created_utc": "2026-02-27 06:28:44",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rg1fjh",
      "title": "Atomic GraphRAG: using a single database query instead of application-layer pipeline steps",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rg1fjh/atomic_graphrag_using_a_single_database_query/",
      "author": "mbudista",
      "created_utc": "2026-02-27 08:46:01",
      "score": 22,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "Memgraph just published a post on a pattern weâ€™ve been calling Atomic GraphRAG:\n\n[https://memgraph.com/blog/atomic-graphrag-explained-single-query-pipeline](https://memgraph.com/blog/atomic-graphrag-explained-single-query-pipeline)\n\nThe core idea is simple: instead of stitching GraphRAG together across multiple application-layer steps, express retrieval, expansion, ranking, and final context assembly as a **single database query**.\n\nThe post breakdown:\n\n* what we mean by GraphRAG;\n* three common retrieval patterns (analytical, local, and global);\n* why GraphRAG systems often turn into pipeline sprawl in production;\n* and why pushing more of that logic into the database can simplify execution and make the final context easier to inspect.\n\nThe argument is that a single-query approach can reduce moving parts, return a more compact final payload to the LLM, and make it easier to trace how context was assembled.\n\nCurious how others here are structuring GraphRAG pipelines today - especially whether you keep orchestration mostly in app code or push more of it into the database.\n\n*Disclosure: Iâ€™m with Memgraph and the blog post author.*",
      "is_original_content": false,
      "link_flair_text": "Showcase",
      "permalink": "https://reddit.com/r/Rag/comments/1rg1fjh/atomic_graphrag_using_a_single_database_query/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7pf6ub",
          "author": "singh_taranjeet",
          "text": "This atomic graph-RAG idea makes a lot of sense because it avoids the classic multi-query feedback loop that bloats latency and forces you to stitch context after the fact. By treating the graph as the single source of truth and then deriving your prompt from one canonical query, you get consistency without repeated hits on the index.\n\nIf anyone has actually tried this pattern end-to-end at scale and found real gains, itâ€™d be cool to hear from Mem0 on what tradeoffs they ran into in practice",
          "score": 3,
          "created_utc": "2026-02-27 14:29:15",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ql6nq",
              "author": "mbudista",
              "text": "Exactly, it should be easier for everyone to implement GraphRAG pipelines.",
              "score": 1,
              "created_utc": "2026-02-27 17:51:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7silo2",
          "author": "Dense_Gate_5193",
          "text": "this is why i consolidated the whole pipeline into a single binary to reduce latency to 7ms RRf hybrid search on a 1m embedding corpus including embedding the user query in memory and reranking. \n\nhttps://github.com/orneryd/NornicDB",
          "score": 1,
          "created_utc": "2026-02-27 23:43:27",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7top41",
          "author": "New_Animator_7710",
          "text": "Comparing this to hybrid vector + graph approaches, many teams keep semantic retrieval in a vector database and only use graph traversal for structured expansion. Atomic GraphRAG suggests unifying those concerns. Iâ€™d be curious how this interacts with external embedding providers or frameworks like OpenAIâ€”does the atomicity stop at structured retrieval, or can embedding similarity also be expressed natively?",
          "score": 1,
          "created_utc": "2026-02-28 04:04:04",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ui97r",
              "author": "mbudista",
              "text": "A very good question/point. I think the whole approach works really well, even when multiple systems interact during a single GraphRAG pipeline execution. But there has to be an integration layer, so-called cross-database or foreign database capabilities. E.g., running a clause/function to call the embeddings in one system, search for matches under a second system, and expand the graph inside a third system.",
              "score": 1,
              "created_utc": "2026-02-28 08:04:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1reivma",
      "title": "Agentic RAG for Dummies v2.0",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1reivma/agentic_rag_for_dummies_v20/",
      "author": "CapitalShake3085",
      "created_utc": "2026-02-25 17:01:02",
      "score": 20,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hey everyone! I've been working on **Agentic RAG for Dummies**, an open-source project that shows how to build a modular Agentic RAG system with LangGraph â€” and today I'm releasing v2.0.\n\nThe goal of the project is to bridge the gap between basic RAG tutorials and real, extensible agent-driven systems. It supports any LLM provider (Ollama, OpenAI, Anthropic, Google) and includes a step-by-step notebook for learning + a modular Python project for building.\n\n## What's new in v2.0\n\nðŸ§  **Context Compression** â€” The agent now compresses its working memory when the context exceeds a configurable token threshold, keeping retrieval loops lean and preventing redundant tool calls. Both the threshold and the growth factor are fully tunable.\n\nðŸ›‘ **Agent Limits & Fallback Response** â€” Hard caps on tool invocations and reasoning iterations ensure the agent never loops indefinitely. When a limit is hit, instead of failing silently, the agent falls back to a dedicated response node and generates the best possible answer from everything retrieved so far.\n\n## Core features\n\n- Hierarchical indexing (parent/child chunks) with hybrid search via Qdrant\n- Conversation memory across questions\n- Human-in-the-loop query clarification\n- Multi-agent map-reduce for parallel sub-query execution\n- Self-correction when retrieval results are insufficient\n- Works fully local with Ollama\n\nThere's also a Google Colab notebook if you want to try it without setting anything up locally.\n\nGitHub: https://github.com/GiovanniPasq/agentic-rag-for-dummies",
      "is_original_content": false,
      "link_flair_text": "Tutorial",
      "permalink": "https://reddit.com/r/Rag/comments/1reivma/agentic_rag_for_dummies_v20/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7oqbo3",
          "author": "martinschaer",
          "text": "Take a look at [SurrealDB.com](http://SurrealDB.com) it allow you to store not just the vectors, but also any other data (users, chat history, etl metadata, ...). Plus, if you want to do graph RAG, you can add relationships between your data too. [https://surrealdb.com/blog/how-to-build-a-knowledge-graph-for-ai#practical-examples](https://surrealdb.com/blog/how-to-build-a-knowledge-graph-for-ai#practical-examples) (i'm the author of this blog post)",
          "score": 1,
          "created_utc": "2026-02-27 11:55:41",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rcecf4",
      "title": "How I Used AI + RAG to Automate Knowledge Management for a Consulting Firm",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "author": "Safe_Flounder_4690",
      "created_utc": "2026-02-23 10:52:36",
      "score": 19,
      "num_comments": 11,
      "upvote_ratio": 0.91,
      "text": "Recently, I built a workflow for a consulting firm that leverages AI combined with Retrieval-Augmented Generation (RAG) to fully automate knowledge management, transforming a fragmented document system into a centralized, actionable intelligence hub. The pipeline begins by ingesting structured and unstructured client reports, internal documents and market research into a vector database, then AI agents retrieve the most relevant information dynamically, reason over it and generate concise, actionable summaries or recommendations. By layering persistent memory, validation loops and workflow orchestration, the system doesnâ€™t just fetch data it contextualizes it for consultants, flags potential conflicts, and tracks follow-ups automatically. This approach drastically reduced time spent searching across multiple tools, eliminated duplication errors and improved decision-making speed. What made it successful is the combination of semantic search, structured reasoning and AI-driven content validation, ensuring that consultants always have the most accurate, up-to-date insights at their fingertips. The outcome: higher productivity, faster client delivery and a knowledge system that scales with the firmâ€™s growth.\nIf AI can summarize thousands of consulting documents in minutes, how much more value could your team create by focusing only on insights instead of searching for them?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rcecf4/how_i_used_ai_rag_to_automate_knowledge/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6xrg3b",
          "author": "prismaticforge",
          "text": "Have you had feedback from the users?  Are they happy with the results and using the tool.  I am curious also how you handle contradictory information from rag?",
          "score": 2,
          "created_utc": "2026-02-23 11:55:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6xnour",
          "author": "jannemansonh",
          "text": "nice setup... building rag pipelines with custom orchestration is solid but maintaining that glue code gets brutal over time. ended up using needle app for similar doc workflows since it handles the vector db + workflow orchestration in one place (just describe what you want vs wiring everything). kept custom stuff for edge cases though",
          "score": 1,
          "created_utc": "2026-02-23 11:23:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6xoaiw",
              "author": "Safe_Flounder_4690",
              "text": "Thatâ€™s a valid point maintaining custom orchestration can become complex as systems scale and edge cases increase. Managed platforms can reduce operational overhead and speed up deployment.\nÂ \nThat said, custom pipelines still offer deeper control over retrieval logic, validation and integration with internal processes. The right balance often comes from standardizing core infrastructure while keeping flexibility where business-specific reasoning and accuracy matter most.",
              "score": 1,
              "created_utc": "2026-02-23 11:28:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6y2094",
          "author": "Tired__Dev",
          "text": "Itâ€™s the actionable summaries among chunks that would have me worried tbh.",
          "score": 1,
          "created_utc": "2026-02-23 13:10:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6y2dma",
          "author": "Semoho",
          "text": "Did you have benchmark o test dataset that how this approach effects the system?",
          "score": 1,
          "created_utc": "2026-02-23 13:13:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6yc7ri",
          "author": "ChapterEquivalent188",
          "text": "how do you test ? do you trust your llm ?",
          "score": 1,
          "created_utc": "2026-02-23 14:10:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zeo3u",
          "author": "AmphibianNo9959",
          "text": "For personal use, I've been using Reseek to handle a lot of that ingestion and semantic search piece automatically. It pulls text from PDFs and images, tags everything, and makes my own notes and bookmarks searchable in a similar way. ",
          "score": 1,
          "created_utc": "2026-02-23 17:18:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zrox9",
              "author": "Material-River-2235",
              "text": "Ok, I will go take a look at it.",
              "score": 1,
              "created_utc": "2026-02-23 18:19:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zf2rh",
          "author": "nasnas2022",
          "text": "Can you add few more details on the pipeline",
          "score": 1,
          "created_utc": "2026-02-23 17:20:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6zj2uk",
          "author": "_os2_",
          "text": "We have built something similar with [Skimle](https://skimle.com), but our tool skips RAG completely and instead builds the categorization scheme in the beginning with LLM calls and then retrieves from a structured table rather than at runtime. Enables two-way transparency and stable responses.\n\nWould be great to compare results!",
          "score": 1,
          "created_utc": "2026-02-23 17:39:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7125c1",
          "author": "Infamous_Ad5702",
          "text": "Well done. Do you build a knowledge graph?\nAnd why Vector?\nI had the same project for a client and went with a custom tool called Leonata. It builds an index and works totally offline. No LLM. No GPU. And No hallucination. \n\nFor my client purpose, semantic retrieval, Vector found similar info but not the best fit.",
          "score": 1,
          "created_utc": "2026-02-23 21:59:04",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rdeibi",
      "title": "Fresh grad learning RAG, feeling lost, looking for guidance",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "author": "savinox23",
      "created_utc": "2026-02-24 12:07:10",
      "score": 18,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hello, I am a fresh grad trying to learn about RAG and develop my coding skills. I made this simple cooking assistant based on Moroccan recipes. Could you please tell me how I can improve my stack/architecture knowledge and my code?\n\nWhat I currently do is discuss best practices with ChatGPT, try to code it myself using documentation, then have it review my code. But I feel like I'm trying to learn blindly. It's been 6 days and I've only made this sloppy RAG, and I feel like there is a better way to do this.\n\nHereâ€™s the link to a throwaway repo with my code (original repo has my full name haha):\n\n  \n[https://github.com/Savinoy/Moroccan-cooking-assistant](https://github.com/Savinoy/Moroccan-cooking-assistant?utm_source=chatgpt.com)",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rdeibi/fresh_grad_learning_rag_feeling_lost_looking_for/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o74uxot",
          "author": "RobertLigthart",
          "text": "6 days and you already have a working RAG is not sloppy thats actually decent progress. most people spend weeks just trying to get embeddings to work\n\n  \nthe chatgpt code review loop is fine for learning syntax but for architecture you need to read how other people built theirs. check out the rag-from-scratch series by lance martin on youtube... its hands down the best resource for understanding why the pieces fit together not just how to copy paste them\n\n  \nbiggest thing I'd improve early on: add chunking strategy to your pipeline if you havent already. most beginners just dump full documents into the vector store and wonder why retrieval is bad. experiment with chunk sizes and overlap... makes a massive difference",
          "score": 4,
          "created_utc": "2026-02-24 13:44:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o74yjcp",
              "author": "savinox23",
              "text": "Thank you very much, i appreciate it !! Iâ€™ll check out the series. As for the chunking strategy, I experimented with a standard text splitter but it was giving me mixed/ incomplete recipe answers, so i made each separate recipe as a chunk and the results were better, but i will check out the resources you mentioned, Iâ€™m sure there are better methods i can try!",
              "score": 1,
              "created_utc": "2026-02-24 14:04:04",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7xdsov",
              "author": "Necessary-Dot-8101",
              "text": "contradiction compression",
              "score": 1,
              "created_utc": "2026-02-28 19:13:52",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o78hh8u",
          "author": "Asleep_Carpet_3403",
          "text": "Great to see someone trying to learn through manual coding and 6 days to a working Rag is absolutely impressive. One suggestion is to start using tools like cursor with their auto complete functionality. It'll 10x your coding speed while you still continue to learn and write the complete code yourself. \n\nA good skill to have now is to learn inference (deploying and using ML/DL models) for this you may consider including a data extraction model from scanned pages (images) upstream of your RAG pipeline",
          "score": 2,
          "created_utc": "2026-02-25 00:04:22",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78jocr",
              "author": "savinox23",
              "text": "Okay, thank you very much for the advice !!",
              "score": 1,
              "created_utc": "2026-02-25 00:16:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mcqpq",
          "author": "RecommendationFit374",
          "text": "I donâ€™t recommend using langchain iâ€™d use a memory layer for retrieval like papr.ai or mem0",
          "score": 2,
          "created_utc": "2026-02-27 01:11:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7oevw3",
              "author": "savinox23",
              "text": "Okay, iâ€™ll check it out, thank you !",
              "score": 2,
              "created_utc": "2026-02-27 10:18:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rclvtn",
      "title": "first RAG project, really not sure about my stack and settings",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "author": "Kas_aLi",
      "created_utc": "2026-02-23 16:21:16",
      "score": 17,
      "num_comments": 12,
      "upvote_ratio": 0.92,
      "text": "Hey guys, so ive been working on my first RAG project. its basically a system that takes medical PDFs (textbooks, clinical guidelines) and builds a knowledge graph from them to generate multi-choice exam questions for a medical exam. \n\nInput style: large textbooks, pdf, images, tables, etc\n\nI have been coding this like a monkey with claude opus 4.6 and codex 5.3 honestly, just prompting my way through it. it works but i have no idea if what im doing is the right approach.\n\nWould love some feedback, good sources or learning resources.  \n  \nHere is my current stack for context:\n\n    PDF â†’ Docling (no OCR, native text) â†’ markdown export with page breaks\n        â†’ heading-based chunker (~768 tok, tiktoken cl100k)\n          â†’ noise classifier (regex heuristics, filters TOC/references/headers)\n          â†’ batch extraction (3 chunks/batch, 4K token cap, 4 parallel workers)\n            â†’ Instructor (JSON mode) + Gemini 2.5 Flash via OpenRouter (it is cheap, but probably there are better now)\n            â†’ Pydantic schema: concepts (18 types) + claims (25 predicates) + evidence spans\n            â†’ fallback: batch fail â†’ individual chunk extraction\n          â†’ concept normalization + dedup\n          â†’ quality gate (error rate, claims/chunk, evidence/claim, noise ratio, page coverage)\n          â†’ embeddings: Qwen3-embedding-8b (1024d) â†’ pgvector\n    \n    storage: supabase (27 tables)\n    orchestration: langgraph (for downstream question generation, not ETL)\n    \n    all LLM calls through openrouter",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rclvtn/first_rag_project_really_not_sure_about_my_stack/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o6z75uc",
          "author": "Higgs_AI",
          "text": "Can I just ask you if this is for taking exams? If soâ€¦ DM me. Overall itâ€™s well put togetherâ€¦ the bones are solid. The fact that it has a quality gate at all puts it ahead of most implementations. ðŸ¤·ðŸ½â€â™‚ï¸\n\nHad to edit: this is really clean workâ€¦the schema enforced extraction with Instructor, the quality gate with multiple metrics, the batch fallback logic. Most people building extraction pipelines skip half of what youâ€™ve done hereâ€¦ bravo brotha. \n\nyour question generation is downstream and separate. What if the extraction and the pedagogy were part of the same adaptive loop where how the learner performs on generated questions feeds back into which concepts need deeper extraction, which claims need more evidence, which connections need to be surfaced?\n\nThereâ€™s other stuff Iâ€™d say but, I just thought Iâ€™d give you some substance without flooding your shit. Good work",
          "score": 3,
          "created_utc": "2026-02-23 16:43:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6z8hqn",
              "author": "Kas_aLi",
              "text": "Main usecase will be generating new and original exam questions but it should be good at taking the exam as well I guess... I think GPT 5.2 had more than 98% correct answers in this particular exam already so...",
              "score": 1,
              "created_utc": "2026-02-23 16:49:43",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zp35o",
          "author": "Semoho",
          "text": "Nice orchestration. \n\nYou can have a little academic approach to assess your system. In my opinion, you can create a test dataset from your exam or even from your knowledge of the docs (corpus). Then, run testing on GPT 5.2 and get the answers and also with your system. Now you have benchmark from gpt or other models and your system, so you can check if the results are good or not. We call it the test phase or making a test dataset. This gives you the power to assess your system. (Usually, you can see the baselines on the academic papers that they need to improve the baseline to compete with other approaches.)\n\nAlso, I would recommend using Knowledge Graph if you have relational data, such as something belongs to one paper/book, and there is some other evidence on the other resources.\n\n  \nSome tips and tricks to increase the retrieval phase:\n\n1. Use task-specific embedding models such as [https://huggingface.co/abhinand/MedEmbed-base-v0.1](https://huggingface.co/abhinand/MedEmbed-base-v0.1)\n\n2. Use other vector DBs like Milvus. Vector DB is so easy to set up, but it does not provide good accuracy.\n\n3. Check the knowledge graph.",
          "score": 3,
          "created_utc": "2026-02-23 18:07:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o77spi7",
              "author": "rigatoni-man",
              "text": "I've been building something to test models without a lot of overhead and legwork.  Basically upload your golden dataset and test it against every model out there.\n\nShoot me a message u/Kas_aLi  and I'd love to help you find the best model for free to test what i'm building ( [https://checkstack.ai](https://checkstack.ai) )",
              "score": 1,
              "created_utc": "2026-02-24 21:55:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o77toou",
                  "author": "Semoho",
                  "text": "It is a good platform. But as an IR guy to publish different papers in the information retrieval field, I think I do not need it :))  \nHowever, if I find time, I would check your platform",
                  "score": 1,
                  "created_utc": "2026-02-24 22:00:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6zcxj3",
          "author": "shlok-codes",
          "text": "I use DeepSeek nitro chat via openrouter look into DeepSeek and qwen models",
          "score": 2,
          "created_utc": "2026-02-23 17:10:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73ivc1",
          "author": "avebrahimi",
          "text": "Awesome stack.  \nWhat about UI?",
          "score": 1,
          "created_utc": "2026-02-24 07:14:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73tl37",
          "author": "LuckEcstatic9842",
          "text": "You might also want to take a look at LightRAG.",
          "score": 1,
          "created_utc": "2026-02-24 08:54:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7b1fde",
          "author": "aidenclarke_12",
          "text": "the heading-based chunker at 768 tokens works for linear textbook prose but clinical tables and cross-references tend to fragment badly across batch boundaries.. the 3 chunks per batch with 4k cap means a pharmacology table that spans two chunks might lose the column headers on one side and the values on the other. \n\nthe quality gate catching this after the fact is better than nothing but fixing it upstream in the chunking logic is probably worth exploring.. \n\non the model side you mentioned openrouter and questioned whether there are better options now.. runpod, together or deepinfra give direct access to the same gemini flash and qwen3 models at lower per-token cost which matters if you're running high extraction volumes on large textbooks",
          "score": 1,
          "created_utc": "2026-02-25 11:00:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7g66sn",
          "author": "New_Direction5479",
          "text": "Awesome stack but Embedding storing go with Qdrant ,add some open source reranker model's also and make 2 application 1. UI to Ingestion , 2. UI to Inference application.",
          "score": 1,
          "created_utc": "2026-02-26 03:02:03",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf7of7",
      "title": "RAG eval is broken if you're only testing offline - here's what changed for us",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rf7of7/rag_eval_is_broken_if_youre_only_testing_offline/",
      "author": "darkluna_94",
      "created_utc": "2026-02-26 11:25:49",
      "score": 17,
      "num_comments": 5,
      "upvote_ratio": 0.94,
      "text": "\nI've been building a RAG pipeline for internal document search for about 4 months now. Mostly legal and compliance docs so accuracy actually matters for my use case.\n\nMy offline eval was looking pretty solid. RAGAS scores were decent, faithfulness sitting around 0.87, context recall above 0.9. I shipped it feeling good about it.\n\nThen users started flagging answers. The pipeline was pulling the right chunks but still getting conclusions wrong sometimes. Not obvious hallucinations, more like the model was connecting retrieved context incorrectly for certain document structures. My benchmark never caught it because my test set didn't really reflect the docs users were actually uploading.\n\nThat's the thing nobody tells you. Your test set is a snapshot. Production keeps changing.\n\nHere's what I went through trying to fix it:\n\n**Manual test set curation** - I started reviewing failing queries and adding them to my golden dataset. Helped a bit but honestly didn't scale at all.\n\n**Langfuse** - added tracing so I could actually see which chunks were being retrieved per query. This alone was a big deal for debugging. Still needed manual review to spot patterns though.\n\n**Confident AI** - started running faithfulness and relevance metrics directly on live traces. The thing that actually saved me time was failing traces getting auto-flagged and curated into a dataset automatically so I wasn't doing it by hand.\n\n**Prompt tweaking** - turned out a lot of failures were fixable once I could actually see the pattern clearly.\n\nHonestly even just adding proper tracing was the biggest unlock for me. Going in blind was the real problem. Evaluation on top just made it less random.\n\nAnyone else dealing with this on domain specific or inconsistent document formats?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rf7of7/rag_eval_is_broken_if_youre_only_testing_offline/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7jurss",
          "author": "llamacoded",
          "text": "Same experience. Offline evals looked great, production was different. We sample 10% of live traffic for automatic evaluation now - catches retrieval drift before users report it. Way better than waiting for complaints. Docs: [https://www.getmaxim.ai/docs/offline-evals/overview](https://www.getmaxim.ai/docs/offline-evals/overview)",
          "score": 2,
          "created_utc": "2026-02-26 17:43:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ik8yp",
          "author": "Odd-Literature-5302",
          "text": "such a good reminder that offline eval only measures what you think users will ask, not what they actually do. In domains like legal and compliance, small structural quirks can completely change how context gets interpreted, so live tracing plus continuous dataset updates feels almost mandatory.",
          "score": 1,
          "created_utc": "2026-02-26 14:01:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j41ao",
          "author": "Elegant_Gas_740",
          "text": "Have you found that most of the issues came from retrieval gaps, or was it mainly the model misinterpreting the right chunks once they were pulled?",
          "score": 1,
          "created_utc": "2026-02-26 15:40:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jg24f",
          "author": "StrangerFluid1595",
          "text": "Offline scores can look solid but production always exposes the weird edge cases, especially with complex legal docs. Getting proper tracing in place is such a game changer.",
          "score": 1,
          "created_utc": "2026-02-26 16:35:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7k7hmh",
          "author": "Delicious-One-5129",
          "text": "Honestly the frozen test set problem is what got us too. Moved to Confident AI and prod failures just automatically become regression tests now. Night and day difference.",
          "score": 1,
          "created_utc": "2026-02-26 18:41:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ri0mnl",
      "title": "What metrics are you actually using to evaluate RAG quality? And how do you measure them at scale?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1ri0mnl/what_metrics_are_you_actually_using_to_evaluate/",
      "author": "Popular_Tour8172",
      "created_utc": "2026-03-01 15:50:45",
      "score": 17,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "I've read all the papers on RAG evaluation, RAGAS, ARES, etc, but I'm struggling to turn academic metrics into something I can actually run in a CI pipeline.\n \nSpecifically trying to measure:\n\n1. Retrieval quality: Are we pulling the right chunks?\n\n2. Faithfulness: Is the LLM sticking to what's in the context?\n\n3. Answer relevance: Is the final response actually addressing the question?\n \nThe challenge is doing this at scale. I have ~500 test queries and running GPT4 as a judge on every single one gets expensive fast. And I'm not sure GPT4 as judge is even reliable, it has its own biases.\n \nHow are people doing this in practice? Are there cheaper judge models that are accurate enough? Any tooling that makes this less painful?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1ri0mnl/what_metrics_are_you_actually_using_to_evaluate/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o82o2ih",
          "author": "hrishikamath",
          "text": "I think LLM as a judge is the way. The academic ways require you to curate evaluation set in terms of chunks which is harder than having the final expected answer. Use a good judge and you will be close to accurate.",
          "score": 3,
          "created_utc": "2026-03-01 16:22:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o82om6n",
          "author": "Pretty_Calendar_7871",
          "text": "I am currently dealing with this in my thesis, although on rather small scale. The metrics I gather are:\n\n* Percentage of the expected source documents / chunks retrieved\n* Percentage of literal keywords hits in the response (requires you to define a list of expected keywords per response, can potentially be made more resilient by using embedding similarity instead of literal string macthing)\n* Cosine similarity between the question and the answer to measure the response's relevancy to the question (obviously a rather flawed approach)\n* Cosine similarity between the answer and a hand-made reference answer (obviously a rather flawed approach)\n* LLM-as-a-judge without a reference answer\n* LLM-as-a-judge with a reference answer as context\n\nI have a small set of approx. 20 benchmark questions for which I gather all of these metrics. One interesting idea I had is to perform a correlation analysis of the gathered scores afterwards, to check which of these metrics actually correlate and might therefore be considered \"good measures\"  for a response's quality.  \nThis would ofc be even more effective if you had a human expert score the LLM's responses manually to have a reference. You could then fit a weighted combination of the scores above to get a scoring formula that approximates the human scoring behavior.\n\nOfc call of this is highly dependent on a lot of subjective factors...",
          "score": 3,
          "created_utc": "2026-03-01 16:24:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o84vnts",
          "author": "adukhet",
          "text": "in practice people split it. CI should be cheap + deterministic: a small golden set (~50 queries), recall@k vs known docs, check if the answer actually cites retrieved chunks, and a lightweight NLI model (DeBERTa-MNLI) for â€œdid the answer come from contextâ€. No GPT-4 here.\n\nfor large scale judging, folks use cheaper judges (Llama-3 8B, Mixtral, Haiku etc) with a strict 0/1 rubric and sometimes 2 passes to stabilize. Then run an expensive judge only nightly/weekly on the full 500 setâ€¦ CI just fails if recall or faithfulness regresses. Treat it like unit tests vs load tests.",
          "score": 2,
          "created_utc": "2026-03-01 22:58:26",
          "is_submitter": false,
          "replies": [
            {
              "id": "o84yruc",
              "author": "gg223422",
              "text": "yeah heâ€™s basically running into what almost everyone hits when rag leaves papers and meets prod. ci cant afford heavy llm judging every run, so teams keep ci cheap and simple and only do big evals sometimes. its more like unit tests vs full benchmark, not one pipeline for everything.",
              "score": 2,
              "created_utc": "2026-03-01 23:16:06",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o833taw",
          "author": "-penne-arrabiata-",
          "text": "How would you want to use it?\n\nIâ€™m building something for 2 and 3.  My bet has been on a solution that lets you do it early in the process as a spot check rather than an every build activity.  The upside is no integration needed, no API keys needed, 160 models available.  Though no integration is a downside too.",
          "score": 1,
          "created_utc": "2026-03-01 17:37:48",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o834e6r",
          "author": "licjon",
          "text": "I think if you test different types of files that are being run and test different types of queries, then scale should not be much of a factor. If anything it can help surface edge cases that you can cover in tests. I really think that a very well curated batch of tests with human-as-judge is the way to go. 500 seems excessive unless you have a huge variety of materials that you are running. I think making a part of the CI pipeline gives you a false sense of confidence and is mostly a waste. I'd do semantic similarity between chunks and answers that have been previously approved by a human and new LLM derived output to test for model drift and other factors. ",
          "score": 1,
          "created_utc": "2026-03-01 17:40:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o835its",
          "author": "NoSpeed6264",
          "text": "Confident AI implements all three of those metrics out of the box with their evaluation framework. The key thing they've done is optimize prompts for cheaper models, so you don't have to use GPT4 for every eval. They support using smaller/local models as judges which dramatically cuts cost. Their faithfulness metric is particularly well tuned.",
          "score": 1,
          "created_utc": "2026-03-01 17:46:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83832k",
          "author": "Fine-Perspective-438",
          "text": "For faithfulness, I found that comparing the LLM output against the retrieved chunks with a simple overlap check (not just cosine similarity, but checking if key claims in the response actually appear in the source) catches hallucinations better than using another LLM as judge.\n\nFor scale, instead of running GPT4 on every single query, I sample maybe 50 representative queries across different categories and do a manual spot check first. That helps me calibrate what \"good\" looks like before automating anything.\n\nHonestly, I don't think there's a perfect metric yet. I just try to catch the obvious failures first and iterate from there.",
          "score": 1,
          "created_utc": "2026-03-01 17:58:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83dvyl",
          "author": "laurentbourrelly",
          "text": "Generate a series of performance test prompts with an increasing level of difficulty.",
          "score": 1,
          "created_utc": "2026-03-01 18:24:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83hji1",
          "author": "Leather-Departure-38",
          "text": "â€œCorrectnessâ€ metrics in framework, llm as judge",
          "score": 1,
          "created_utc": "2026-03-01 18:41:22",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o847z62",
          "author": "welcome-overlords",
          "text": "Not an easy task. Currently i:\n1. dozens of test questions with expected answers\n2. built an automated pipeline that asks the agent these questions and saves the answers\n3. I send off the Q&A pairs with expected answers to LLM (Sonnet often) to grade the answers (often bad, ok, great)\n4. Try to get the results closer to all great and add new questions that we think should be answerable based on the source material (which is difficult to read)",
          "score": 1,
          "created_utc": "2026-03-01 20:53:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o85wt5p",
          "author": "Igergg",
          "text": "I am surprised noone mentioned deepeval/ragas. Aren't those tools the mainstream for that?",
          "score": 1,
          "created_utc": "2026-03-02 02:41:06",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfd4md",
      "title": "Building RAG pipelines using elasticsearch",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rfd4md/building_rag_pipelines_using_elasticsearch/",
      "author": "SQLsunset",
      "created_utc": "2026-02-26 15:27:55",
      "score": 16,
      "num_comments": 15,
      "upvote_ratio": 0.91,
      "text": "I chose Elasticsearch over Pinecone for RAG. Here's the honest breakdown.\n\nEveryone building a RAG app hits the same fork: dedicated vector DB (Pinecone, Weaviate) or just use Elasticsearch?\n\nMost tutorials default to Pinecone. I went a different direction and want to share why.\n\nThe core problem with dedicated vector DBs\n\nRAG isn't purely a vector search problem. In practice you need:\n\n\\- Semantic similarity (vectors)\n\n\\- Keyword relevance (BM25)\n\n\\- Metadata filtering\n\nPinecone gives you vectors + basic filters. The moment you need hybrid search and you will, because pure vector retrieval misses exact matches constantly, you're bolting on another system.\n\nElasticsearch does all three natively in one query using Reciprocal Rank Fusion. No extra infrastructure, no glue code.\n\nThe black box problem\n\nWhen Pinecone retrieval is bad, your options are: tweak embeddings, adjust top\\_k, and hope. You can't inspect query execution or see why documents scored the way they did.\n\nElasticsearch shows its work. You can see BM25 vs vector score contributions, profile queries, set up Kibana dashboards. When something breaks you can actually debug it.\n\nElastic Cloud removes the old objection\n\nThe classic knock on Elasticsearch was ops pain â€” shard management, rolling upgrades, cluster tuning. Elastic Cloud handles all of that. Autoscaling, automated snapshots, one-click upgrades. You get the power without babysitting a cluster.\n\nPinecone scales well too, but it only scales your vector index. Everything else still needs separate infrastructure.\n\nGCovers hybrid search setup, kNN index config, and a working RAG query in \\~15 minutes on Elastic Cloud.\n\nCurious if anyone else has gone this route or stuck with Pinecone what pushed your decision?",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1rfd4md/building_rag_pipelines_using_elasticsearch/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7j3wop",
          "author": "fabkosta",
          "text": "FYI: Azure AI Search is very similar to Elasticsearch and offers hybrid search out of the box. It's very powerful - but very expensive. If you don't want to host Elasticsearch yourself and are willing to pay for the premium price, then it's a good cloud PaaS alternative.",
          "score": 3,
          "created_utc": "2026-02-26 15:40:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7j9lxr",
              "author": "grabGPT",
              "text": "Exactly was about to write this. I am using Azure AI Search. What I like about it is how easy it is to wrap AI Search around MCP tools for orchestration and the entire architecture becomes much simpler. For deployment and governance.",
              "score": 1,
              "created_utc": "2026-02-26 16:06:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7knb2r",
                  "author": "Single-Constant9518",
                  "text": "Sounds like Azure AI Search has its perks, especially for those looking for a simpler deployment. Curious how it stacks up in terms of customization and debugging compared to Elasticsearch. Have you faced any limitations?",
                  "score": 1,
                  "created_utc": "2026-02-26 19:56:20",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ldvub",
          "author": "Unusual-Sector-2511",
          "text": "For my part, I choseÂ Amazon Bedrock Knowledge Bases. It offers\n\n\\- Advanced Retrieval:Â Including hybrid search and automated re-ranking  \n\\- Cost-Efficiency:Â Extremely low-cost RAG storage usingÂ S3 Vectors.  \n\\- Native Multimodal Support:Â Integrated embeddings for text, images, and video  \n\\- Enterprise-Grade:Â A secure, scalable, and easy-to-deploy managed service.",
          "score": 3,
          "created_utc": "2026-02-26 22:03:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lp9d7",
              "author": "543254447",
              "text": "does s3 do hybrid search?",
              "score": 1,
              "created_utc": "2026-02-26 23:01:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7lqjb3",
                  "author": "Unusual-Sector-2511",
                  "text": "Actually, S3 itself doesn't perform the hybrid search. It acts as the storage layer for your chunks and vectors. The 'magic' happens within Amazon Bedrock Knowledge Bases, which manages the retrieval logic. It pulls the data from S3 and handles the combination of semantic search and keyword search (hybrid) along with re-ranking to give you the most relevant context.",
                  "score": 3,
                  "created_utc": "2026-02-26 23:08:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7ttt47",
          "author": "SharpRule4025",
          "text": "Everyone's debating the retrieval backend but the thing that actually moved our accuracy numbers was what we were putting into the index in the first place. We spent weeks tuning Elasticsearch scoring and it turned out the real problem was ingesting raw markdown from our scraper. Navigation menus, footer links, language selectors, cookie banners, all of it was getting embedded alongside the actual content.\n\nOnce we switched to structured extraction where you get typed fields (title, body paragraphs, metadata) instead of a markdown blob, our retrieval precision jumped because the embeddings were actually representing the content, not the UI. The added bonus is you can index specific fields separately and weight them differently in your hybrid query, which is harder to do when everything is one big text chunk.",
          "score": 2,
          "created_utc": "2026-02-28 04:40:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j5kpu",
          "author": "jannemansonh",
          "text": "solid point on the hybrid search. ended up using needle app for doc workflows since the rag + hybrid search is built in... way easier than wiring pinecone + elastic together, especially when workflows need to actually understand content and not just move data around",
          "score": 1,
          "created_utc": "2026-02-26 15:47:54",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jae81",
          "author": "Next-Rush-9330",
          "text": "You can try Milvus",
          "score": 1,
          "created_utc": "2026-02-26 16:09:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7jvd5f",
          "author": "2BucChuck",
          "text": "Azure and AWS both also figured the same out and have rolled out tools like S3 vectors- I ,like you , built something custom on a Lucene engine- but also believe this is the right way to go with KBs.   I learned though the hard way donâ€™t try to emulate what Lucene does - itâ€™s just light years ahead of anything new you could build for non dense / vector approaches and it can store vectors all the same alongside.",
          "score": 1,
          "created_utc": "2026-02-26 17:46:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ldgy7",
          "author": "No-Leopard7644",
          "text": "You mentioned Weaviate, but did not choose. Was this after analysis for a fit for your use case/org skills? I have used Qdrant, but am planning to go to Weaviate",
          "score": 1,
          "created_utc": "2026-02-26 22:01:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7m07d0",
          "author": "Infamous_Ad5702",
          "text": "Vector finds similar items. I made a tool to find breadth and depth. \nChunking and embedding is a pain so it builds an index on auto. Then when I query a knowledge graph is built fresh each time.\n\nMy client is in defense. So needed offline. No GPU. No hallucination. No tokens. \n\nDeterministic. Accurate.",
          "score": 1,
          "created_utc": "2026-02-27 00:01:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ni2in",
          "author": "crewone",
          "text": "We went with opensearch. Still getting spammed by the elastic folks with email.",
          "score": 1,
          "created_utc": "2026-02-27 05:28:14",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7j4lro",
          "author": "z0han4eg",
          "text": "Well, there you go, you just leaked a piece of a corporate RAG. Letâ€™s just get back to topics about reinventing the wheel and YouTube promos for the latest \"breakthroughs\".\n\nAnd yeah, a proper hybrid search/waterfall setup with full-text, fuzzy, KNN, hybrid etc logic is done through Elastic, Manticore, or Meilisearch, which already have auto-embeddings and even rerankers. If weâ€™re talking about a toy RAG where the whole pipeline boils down to just retrieval from a vector database, then sure, use whatever-Pinecone, Weaviate, or even Alibaba Zvec -if itâ€™s anything more serious than a couple thousand PDFs about cat breeds.",
          "score": 0,
          "created_utc": "2026-02-26 15:43:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1regz4t",
      "title": "I think most RAG quality issues people post about here are actually extraction problems, not retrieval problems",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1regz4t/i_think_most_rag_quality_issues_people_post_about/",
      "author": "yfedoseev",
      "created_utc": "2026-02-25 15:53:50",
      "score": 15,
      "num_comments": 12,
      "upvote_ratio": 0.91,
      "text": "Every other post in this sub is \"my RAG pipeline hallucinates\" and the replies are always the same: try a different chunking strategy, use a better embedding model, add reranking, etc.\n\nNobody ever says \"go look at what your PDF parser actually output.\"\n\nI did. I took 3,830 real-world PDFs (veraPDF corpus, Mozilla pdf.js tests, DARPA SafeDocs) and ran them through the major Python parsers. Not cherry-picked -- government filings, academic papers, scanned forms, edge cases from the 90s, encrypted files, CJK text, the works.\n\n    Library      Mean     p99      Pass rate\n    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    pdf_oxide    0.8ms     9ms     100%\n    PyMuPDF      4.6ms    28ms     99.3%\n    pypdfium2    4.1ms    42ms     99.2%\n    pdfminer    16.8ms   134ms     98.8%\n    pdfplumber  23.2ms   189ms     98.8%\n    pypdf       12.1ms    97ms     98.4%\n\nHere's the thing nobody talks about: a 98.4% pass rate on 3,830 docs means \\~60 documents that silently fail. They crash, hang, or return empty strings. Those docs never enter your vector store. When a user asks about content from one of those documents, the retrieval step finds nothing relevant, so the LLM fills in the gap with a confident hallucination.\n\nYou debug the prompt. You debug the retrieval. You never think to check whether the document was even indexed.\n\nI built pdf\\_oxide (Rust, Python bindings) partly because I kept running into this. The thing that made the biggest difference for me wasn't the speed, it was the Markdown output with heading detection:\n\n        from pdf_oxide import PdfDocument\n    \n        doc = PdfDocument(\"paper.pdf\")\n        md = doc.to_markdown(0, detect_headings=True)\n\nYou get actual structure back. Headings, paragraphs, sections. Chunk on section boundaries instead of arbitrary token windows. Each chunk ends up being about one topic instead of the tail end of one section glued to the beginning of another. Retrieval precision went up noticeably for me once I switched to heading-based splits.\n\nBuilt-in OCR too (PaddleOCR via ONNX Runtime). It auto-detects scanned pages and falls back. No Tesseract, no subprocess shelling out, no extra config.\n\n        pip install pdf_oxide\n\nMIT licensed. No AGPL. Runs entirely locally.\n\nLimitations I won't hide: table extraction is basic compared to pdfplumber. There are \\~10 edge-case PDFs that still have minor extraction issues (tracked on GitHub). WASM support isn't done yet.\n\n[github.com/yfedoseev/pdf\\_oxide](http://github.com/yfedoseev/pdf_oxide)   \nDocs: [oxide.fyi](http://oxide.fyi)\n\nGenuine question for this sub: how many of you have actually diffed your parser's output against the source PDF? I'm starting to think a lot of the \"retrieval quality\" problems people debug for weeks are just garbage going in at step one.",
      "is_original_content": false,
      "link_flair_text": "Tools & Resources",
      "permalink": "https://reddit.com/r/Rag/comments/1regz4t/i_think_most_rag_quality_issues_people_post_about/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7ci7ij",
          "author": "ChapterEquivalent188",
          "text": "have you tried docling ? \njust in case, i thought iÄºl be on my own for ever ;) \nhttps://github.com/2dogsandanerd/RAG_enterprise_core",
          "score": 2,
          "created_utc": "2026-02-25 16:08:23",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7he74d",
          "author": "lucasbennett_1",
          "text": "this is the truth..most rag complaints are actually bad extraction in disguise.. inspect your parser output before anything else.. pdf\\_oxide looks like a solid fix for that.",
          "score": 2,
          "created_utc": "2026-02-26 08:35:45",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7f0wlv",
          "author": "SubstantialTea707",
          "text": "Io estraggo le immagini e le leggo con il modello glm ocr, tra estrazione e llm che gira su una 5090 ci perdo 5s a pagina e da ottimi risultati in estrazione",
          "score": 1,
          "created_utc": "2026-02-25 23:10:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7fau76",
              "author": "yfedoseev",
              "text": "It works as well if you have this 5 sec for processing",
              "score": 1,
              "created_utc": "2026-02-26 00:05:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7fhd5w",
          "author": "patbhakta",
          "text": "For tables, formulas, charts, etc. you need specialized parsers. OCR is fine for text, VL is fine for photos, you need different tools for different things.",
          "score": 1,
          "created_utc": "2026-02-26 00:41:03",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gdw1k",
          "author": "hhussain-",
          "text": "What difference rust gave you in here? Or what made you use rust if it is all python ?",
          "score": 1,
          "created_utc": "2026-02-26 03:48:18",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7gifx9",
              "author": "yfedoseev",
              "text": "Rust gave me a significant performance boost and allowed me to build bindings to most programming languages guaranteed high performance everywhere.",
              "score": 2,
              "created_utc": "2026-02-26 04:17:28",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7gjprz",
          "author": "New_Animator_7710",
          "text": "In our lab, weâ€™ve repeatedly observed that retrieval quality correlates more strongly with structural fidelity of extraction than with embedding choice. Clean section boundaries often outperform swapping to a â€œbetterâ€ model.",
          "score": 1,
          "created_utc": "2026-02-26 04:25:59",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hb3qw",
          "author": "stevevaius",
          "text": "How about laws? Many laws has sections, sub-sections, articles, quotes etc...? Did you run any test for legal texts?",
          "score": 1,
          "created_utc": "2026-02-26 08:06:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hbmd8",
              "author": "yfedoseev",
              "text": "Honestly, I did and the text conversation works well, but markdown,.something that requires more structure still requires improvement for some corner cases. Legal docs are on my radar. Thank you for your question.",
              "score": 2,
              "created_utc": "2026-02-26 08:10:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hck08",
                  "author": "stevevaius",
                  "text": "Looking fwd to hear about your project, specially on legal text developments. Best regs",
                  "score": 1,
                  "created_utc": "2026-02-26 08:19:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7l6e0b",
          "author": "tom_at_zedly",
          "text": "You're spot on. Everyone obsesses over the vector database or the LLM, but they ignore the mess coming out of the PDF parser.\n\nIf the text extraction is broken, the RAG is dead on arrival. We've found that handling tables and multi-column layouts is 80% of the battle. OCR quality matters way more than most people admit.",
          "score": 1,
          "created_utc": "2026-02-26 21:27:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rgwz5h",
      "title": "Reality check",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rgwz5h/reality_check/",
      "author": "GDAO54",
      "created_utc": "2026-02-28 08:05:59",
      "score": 15,
      "num_comments": 22,
      "upvote_ratio": 0.91,
      "text": "Iâ€™m looking for a \"no-BS\" reality check from anyone running RAG on top of large Document Management Systems (100k+ files).\n\nWe are looking at existing agents like M-Files Aino. Will test this in a few weeks. For another more custom eQMS system (with well-developed API endpoints), we are looking at a custom solution to manage a large repository of around 200k pdfs. My concern is whether the tech is actually there to support high-stakes QMS workflows \n\nIs the current tech stack (RAG/Agentic) actually precise enough for \"needle-in-a-haystack\" queries? If a user asks for a specific tolerance value in a 50-page spec, does it reliably find it, or does it give a \"semantic hallucination\"?\n\nAuthorization: How do you handle document permissions? If I have 100k files with complex authorizations, how do you sync those permissions to the AI's vector index in real-time so users don't \"see\" data they aren't cleared?\n\nAll in all, is the tech there for this or should we wait another year?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rgwz5h/reality_check/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7ujksw",
          "author": "fabkosta",
          "text": "Unfortunately, your question remains without a generic answer.\n\nFirst, you need to determine the target needs of your users. Then you have to translate the needs to measurable metrics If you have no measurable metric you cannot determine the quality at all.\n\nPerceived quality in information retrieval heavily depends on both the users' needs and the data you have. Long, windy, technical manuals are very different to process than e.g. Xwitter data or Reddit posts.\n\nParticularly for technical manuals you may consider not using RAG at all, but instead use vector (or, usually better: hybrid) search instead. So, you let the user decide what search result is the right one, but improve the search quality by going hybrid.\n\nAnother very important point that many engineers somehow miss: Design the UI first before thinking about quality of the information retrieval system. Why you should do that is: Almost always you can have users apply filters and those filters will remove a large part of the documents in your search space. Obviously, if you cut down 200k docs to only 5k ones, that's a massive gain in reduction of the search space, and it helps your information retrieval system massively in finding the right answer. The more filter people set, the easier it gets.\n\nRAG on gigantic datasets ***can*** work. That's what [Perplexity.ai](http://Perplexity.ai) shows us. But be prepared to have to put a lot of work into engineering. Engineering must be hypothesis-based, and hypotheses must be tested, measured, and then either rejected or accepted. That's an iterative, measured approach. Typically in RAG setups, 20% is the technology setup itself, and 80% is understanding and tweaking the data and indexes.",
          "score": 2,
          "created_utc": "2026-02-28 08:16:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7ul2h5",
          "author": "vanwal_j",
          "text": "Weâ€™re running a RAG on legal documents, +700k documents at the moment, and twice as much really soon.\n\nWe choose to go with small buckets (~5k docs per bucket at most) in Postgres for â€œprivateâ€ documents and a larger pool in qdrant for public/shared ones (canâ€™t really speak here about the product unfortunately) all embeded with Cohere embed 4 + Reranker 4\n\nRegarding the needle in a haystack problem, it really depends on the use case, if youâ€™re user expect a â€œsearch toolâ€ they could be disappointed, unless your budget is quite large and you can throw LLM refining in there ðŸ˜¬",
          "score": 2,
          "created_utc": "2026-02-28 08:30:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7up7xa",
              "author": "Natural_Squirrel_666",
              "text": "So you use both pgvector and qdrant?",
              "score": 1,
              "created_utc": "2026-02-28 09:08:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7v6l3m",
                  "author": "vanwal_j",
                  "text": "Yes ! We naively started with pg_vector but as our data pool kept growing our hnsw index memory usage too and since our cloud provider nodes are quite expensive for anything above 12Gb of memory we had to move to qdrant which allows horizontal scaling thanks to sharding",
                  "score": 1,
                  "created_utc": "2026-02-28 11:52:36",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7v4jwo",
          "author": "adukhet",
          "text": "short answer.. yes it can work at 100kâ€“200k PDFs, but only if you treat it like a serious search system, not a magic chatbot. if you combine keyword search + vectors, keep chunks tight, filter hard on metadata, and force answers to cite exact text spans, you can get very solid â€œneedle in a haystackâ€ performance.. if you donâ€™t, youâ€™ll get confident but slightly wrong answers.\n\npermissions are doable but theyâ€™re real engineering work. best pattern is enforcing access control at retrieval time using your existing acl model, synced through events from your dms or eqms api into the vector index.. per-user indexes usually donâ€™t scale.\n\noverall the tech is there today for high-stakes qms, but only with careful architecture, testing, and monitoring.. not as a plug-and-play agent feature.",
          "score": 2,
          "created_utc": "2026-02-28 11:35:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vo7yv",
          "author": "TechnicalGeologist99",
          "text": "There is no escaping human in the loop and a strong citation system for high stakes documents retrieval.",
          "score": 2,
          "created_utc": "2026-02-28 13:56:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7um2mx",
          "author": "AsparagusKlutzy1817",
          "text": "Regarding the needle part of your post - if there is an actual sentence that says it like this eg â€œthe tolerance value is 50â€ it should be retrieved but if in every document you have you find one or more such sentences this will be barely precise enough. The local context in the text like mentioning of machines or process steps can help to increase the likelihood of finding want you want but it really depends on how similar or dissimilar the documents are. We ran similar rag tasks in quality process management and users reported quite satisfactory results with a plain rag. At least for us it worked quite nicely",
          "score": 1,
          "created_utc": "2026-02-28 08:39:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7upmha",
              "author": "Necessary-Dot-8101",
              "text": "Compression-Aware Intelligence shows that contradiction is the visible trace of hidden compression loss, and that tracking it is necessary for reliable intelligence",
              "score": 1,
              "created_utc": "2026-02-28 09:12:37",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7updtw",
          "author": "Necessary-Dot-8101",
          "text": "u need compression aware intelligence",
          "score": 1,
          "created_utc": "2026-02-28 09:10:24",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uq1yf",
          "author": "AISmoothBrain",
          "text": "Itâ€™s certainly possible. We built an eQMS that weâ€™re selling to customers right now, that have similar size document datasets that span multiple products and sites. Check us out. https://sanai.ai/",
          "score": 1,
          "created_utc": "2026-02-28 09:16:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7utsf1",
          "author": "cointegration",
          "text": "Since others have addressed the ingestion and retrieval i will try to address the security side of it, my build is on postgresql, i collapsed as much as i can into it to reduce maintenance overheads, even the knowledge graph is implemented in postgresql, anything 4 hops and below is fast. During ingestion every document is given a security classification id, so it becomes quite trivial to exclude certain documents from a select based on ACL, this is done before the reranker so it produces the same top k minus redacted chunks.",
          "score": 1,
          "created_utc": "2026-02-28 09:53:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7uxifa",
          "author": "Otherwise_Wave9374",
          "text": "For high-stakes QMS, your instincts are right to be skeptical. The tech can work, but only with tight constraints: citation-required answers, chunking that preserves tables, and a retrieval stack you can measure (not just \"seems good\"). For needle-in-a-haystack specs, I usually see better results with hybrid search (BM25 + vectors) and forced quote extraction from the source paragraph.\n\nPermissions is the harder part, you basically need ACL-aware indexing (per-doc or per-chunk) and a query-time filter tied to the users identity, otherwise you will leak. If youre evaluating agentic RAG patterns, Ive got some notes on grounding, evals, and access control approaches here: https://www.agentixlabs.com/blog/",
          "score": 1,
          "created_utc": "2026-02-28 10:30:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7v3gv4",
          "author": "Morphos91",
          "text": "We have a dms with 1M documents and the rag still works great.",
          "score": 1,
          "created_utc": "2026-02-28 11:25:31",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7vh7og",
          "author": "voycey",
          "text": "The tradeoff is between how much you pre-process the dataset, you need to build in several different layers to get what you want, including corpus reduction, relationship and ontology mapping, combined with vector search, reranking and a few other tricks! \n\nWhat you want to do is absolutely possible with the correct pipeline!",
          "score": 1,
          "created_utc": "2026-02-28 13:12:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xtn62",
          "author": "ampancha",
          "text": "The authorization concern is the right one to prioritize. Most teams sync permissions at index time but don't handle the failure modes: what happens when permissions change mid-session, when sync lags, or when prompt injection bypasses retrieval filters entirely. For high-stakes QMS, you'll also need audit trails proving the AI layer respected authorization boundaries, not just that the vector DB had correct metadata. Sent you a DM",
          "score": 1,
          "created_utc": "2026-02-28 20:37:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7xtq0x",
          "author": "No_Indication_1238",
          "text": "By tying the vector to the ID of the auth group. If a vector from the wrong group is received, programatically remove it.",
          "score": 1,
          "created_utc": "2026-02-28 20:37:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o80rjbz",
          "author": "Ok-Regret3392",
          "text": "I guess a thing that people didnâ€™t mention: the ability to find/target small/detailed things is really in the metadata of each chunk. Get a summary done per chunk and use that as well as an additional way of finding the right chunks or use a dynamic graph approach to traverse the various nodes.",
          "score": 1,
          "created_utc": "2026-03-01 08:09:11",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o83wqkg",
          "author": "loookashow",
          "text": "Hey, these are real concerns and worth taking seriously for a QMS context.\n\non the needle-in-a-haystack problem - semantic RAG alone will miss exact tolerance values. the fix is parsing PDFs into typed structured records, then using deterministic filters instead of similarity search. â€œfind tolerance for component Xâ€ becomes an exact query, not a guess.\n\nAuthorization at 100k files is where most stacks fall apart. what actually works is hierarchical isolation at the data model level - access rules inherit down your document hierarchy, no separate permission sync to a vector index needed.\n\nweâ€™re building a platform called FoxNose thatâ€™s designed around exactly this kind of use case - no-BS structured records, hybrid search, hierarchical acces control. itâ€™s in open beta so not everything is polished yet, but your QMS case is genuinely interesting to us. happy to chat if you want to dig in.",
          "score": 1,
          "created_utc": "2026-03-01 19:55:56",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7unvk0",
          "author": "Infamous_Ad5702",
          "text": "â€œNeedle in haystackâ€ queries I wonâ€™t rely on vector search. I need breadth and depth so I built a tool. See if it works for your data collection? We are able to scale to large volumes.\n\nLeonata replaces RAG by eliminating embeddings, vector search, and probabilistic retrieval entirely â€” it lets you query your data directly through deterministic semantic structure instead of approximating meaning through similarity.\n\nRag guesses. Leonata knows.\n\nOur defence clients needed offline. No GPU. No Hallucination. So thatâ€™s what we built.",
          "score": -6,
          "created_utc": "2026-02-28 08:56:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7vdaoi",
              "author": "ajroyalmx",
              "text": "Isn't that how vector search work too? Embedding words based on semantic structure in the vector space so that similar words come near each other?",
              "score": 1,
              "created_utc": "2026-02-28 12:45:19",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7w4ohg",
                  "author": "Infamous_Ad5702",
                  "text": "Vector works by moving in straight lines almost. Finding similar items. This isnâ€™t vector it finds specificity and best fit. So you end up with unknown unknowns. I just submitted a poster to a conference in stuttgart. Can post it.",
                  "score": 1,
                  "created_utc": "2026-02-28 15:28:00",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rf54jj",
      "title": "So I made a GraphRAG product but i don't really know how to sell it.",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1rf54jj/so_i_made_a_graphrag_product_but_i_dont_really/",
      "author": "graphmesh-ai",
      "created_utc": "2026-02-26 08:49:35",
      "score": 15,
      "num_comments": 32,
      "upvote_ratio": 0.83,
      "text": "As title, I made an embeddable GraphRAG ingestion + retrieval as a service product. I know this is valuable but i have no idea how to get it in front of the people who might want it, nor really even who i should target marketing towards?\n\nAre small businesses starting to consider this stuff or is document intelligence still something that only large businesses are considering right now?\n\nFor reference its [graphmesh.ai](http://graphmesh.ai) ive put on a 20,000 free token promo but is selling by the token even the right way to go?",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1rf54jj/so_i_made_a_graphrag_product_but_i_dont_really/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7hwxnr",
          "author": "vornamemitd",
          "text": "When visiting your site:  \n\\- I don't whether you are talking to me B2C or B2B  \n\\- I don't know what sets it apart from already embedded offerings at Azure, AWS, GCP, OAI, Cohere et al.  \n\\- I don't know where my files go and what happens to them in the background  \n\\- I don't know what models you are using and who is running them  \n\\- I am confused what you mean by \"agent\" and \"chatspace\"  \n\\- I don't know how it compares security-, performance-, or reliability-wise  \n\\- I don't see any pricing/licensing details other than a hint at token charges hidden deeply in the terms  \n  \nSo unfortunately you are giving me no reason to stay on the site while planting a huge number of red flags.   \n",
          "score": 16,
          "created_utc": "2026-02-26 11:30:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hx1e0",
              "author": "graphmesh-ai",
              "text": "This is amazing feedback, thank you",
              "score": 9,
              "created_utc": "2026-02-26 11:31:18",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hzf74",
                  "author": "vornamemitd",
                  "text": "I think that there is still space for smart/bespoke retrieval solutions. In reality everyone keeps struggling to chat with three PDFs on their Sharepoint - even with Accenture on speed-dial (or because of that). But it's a super-crowded and over-slopped segment at the same time. Plus: I'd really try to ride the EU sovereignty wave - potentially partner up with an EU hosting/DC-provider and hit medium-sized enterprises like there was no tomorrow. Good luck! ",
                  "score": 4,
                  "created_utc": "2026-02-26 11:50:20",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7im930",
                  "author": "welcome-overlords",
                  "text": "Excellent that you take feedback so well",
                  "score": 2,
                  "created_utc": "2026-02-26 14:12:31",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hk75v",
          "author": "Canadianingermany",
          "text": "This is so very typical of AI- I built this shit before figuring out if anyone needed it.",
          "score": 6,
          "created_utc": "2026-02-26 09:34:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hkehv",
              "author": "graphmesh-ai",
              "text": "Well, i mean, this is also my day job, businesses are using this, i just dont know how to sell my own version :D",
              "score": -2,
              "created_utc": "2026-02-26 09:36:43",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7i8526",
                  "author": "kyngston",
                  "text": "doing it as my day job and selling my own version is a direct violation of my employment contract",
                  "score": 1,
                  "created_utc": "2026-02-26 12:51:41",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o7hpmlr",
                  "author": "Canadianingermany",
                  "text": "Which usually happens when you build stuff without even knowing what problem you are solving.",
                  "score": -3,
                  "created_utc": "2026-02-26 10:25:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o7hhzgm",
          "author": "jw520",
          "text": "\n[Fixed]",
          "score": 1,
          "created_utc": "2026-02-26 09:12:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hizse",
              "author": "graphmesh-ai",
              "text": "Well that could not have been more embarrassing, I have fixed :facepalm:",
              "score": 3,
              "created_utc": "2026-02-26 09:22:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hjs8z",
                  "author": "jw520",
                  "text": "I'm working on a product built on this idea (though not an dev/API play, so I'm more of a DIYer/customer.  Not a competitor.\n\nIt seems like there's a lot of possible approaches that can be taken with varying degrees of quality in extraction and mapping. Have you had the same thought? I'm just getting started with graph rag so this is sort of learning conversation for me",
                  "score": 1,
                  "created_utc": "2026-02-26 09:30:33",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7hi41k",
              "author": "graphmesh-ai",
              "text": "OOF",
              "score": 1,
              "created_utc": "2026-02-26 09:13:55",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7hl4ng",
          "author": "Striking-Bluejay6155",
          "text": "Weâ€™ve built something like this albeit more robust at FalkorDB, but we donâ€™t charge for the usage (as we are open source). Some considerations: security, why would someone trust uploading sensitive data to your environment? This is a major soc2 oversight. \nLarge companies donâ€™t need the GUI, they rely on our SDK and different APIs. The user friendly GUI is intended towards people who for some reason donâ€™t use conventional tools like Gemini to chat with their pdf. In essence, if youâ€™re ingesting large pdfs, are very good at entity extraction and deduplication, and can prove that your produce more accurate answers than others, then perhaps you could charge a monthly access fee with unlimited pdf uploads.",
          "score": 1,
          "created_utc": "2026-02-26 09:43:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7hlo16",
              "author": "graphmesh-ai",
              "text": "Fair considerations, SOC2 and GDPR are concerns to be sure. All extracted data stored in blob or database is encrypted and the databases themselves are encrypted at rest but the ToCs explicitly state that no PII or sensitive information should be uploaded.\n\nWhat i was aiming for was small entities that wanted a RAG knowledge base for internal use or customer product queries, hence the simple UI and embedded chat, should be easy for a small business solo developer to drop the product specs or manuals into a library and embed the chat on a page for their customers. Im guessing usage will be relatively low so the \"only pay for what you use\" seemed sensible.\n\nIt is pretty good at identifying facts in the documents and can also extract and query tables found in the documents. \n\nAs far as deduplication goes it can automatically version documents so if you upload a new version it will identify the existing document and roll the updates into that hence the blobs vs items distinction on the API.",
              "score": 1,
              "created_utc": "2026-02-26 09:48:57",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7hlxy0",
                  "author": "Striking-Bluejay6155",
                  "text": "Sounds cool, looking forward to updates",
                  "score": 1,
                  "created_utc": "2026-02-26 09:51:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7j98c5",
              "author": "AbsolutelyYouDo",
              "text": "Coming from Neo4j, I see lots of posts about FalcorDB being the newer, better option. I've already built quite a bit on Neo4j, and I doubt I'll be able to change over, but now I'm having to build my own ingest / ETL / knowledge graph RAG piece. Could you give more info about which specific part of FalcorDB you're referring to?",
              "score": 1,
              "created_utc": "2026-02-26 16:04:31",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7jdg54",
                  "author": "Striking-Bluejay6155",
                  "text": "Sure, FalkorDB graphrag-SDK. May I ask what would prevent you from making the switch? Iâ€™m not trying to sell, just trying to improve our migration docs. Weâ€™ve had a big blue chip observability platform switch over recently, so there may be tips I can share. At any rate, if itâ€™s user defined functions or concerns about production readiness, happy to chat",
                  "score": 2,
                  "created_utc": "2026-02-26 16:23:45",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o802xws",
          "author": "FarQuail277",
          "text": "We've built an agentic RAG-as-a-service platform as well and candidly shipping an infrastructure product is generally a lot harder than selling a vertical that is more targeted/ well defined. My advice for you is determining who your core ICP is first and REALLY stress testing that before figuring out effective GTM channels. Document intelligence is very important, especially in an age when non-technical founders are vibe coding vertical software in a weekend that work great in demos but fail spectacularly in production. Even within that domain though you can segment your market to figure out who exactly you want to sell to: non-technical folks? CTOs? Legal tech companies? Construction? etc. The answer to these questions is a combination of your product's strengths and your comforts as a salesperson. As a seemingly qualified MLE, you might prefer to sell to other engineers, or maybe you feel you'd fare better with other audiences - all that is your call. Happy to discuss some more and brainstorm if you'd like to DM me - we infra folk need to stick together! Wishing you the best.",
          "score": 1,
          "created_utc": "2026-03-01 04:42:16",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1reu6t0",
      "title": "For teams selling internal AI search/RAG: what does user behavior actually look like?",
      "subreddit": "Rag",
      "url": "https://www.reddit.com/r/Rag/comments/1reu6t0/for_teams_selling_internal_ai_searchrag_what_does/",
      "author": "adukhet",
      "created_utc": "2026-02-25 23:49:07",
      "score": 14,
      "num_comments": 4,
      "upvote_ratio": 0.9,
      "text": "Just like the title; question for people actually selling RAG/enterprise AI search products (not demos, not internal tools):\n\nHave you ever measured average user session length?\n\nIâ€™m especially curious about real production usage, not benchmarks.  \n\nIf youâ€™re willing to share, it would be super helpful to include:\n\n\\- vertical (legal, support, sales, engineering, etc.)\n\n\\- main use case (knowledge search, support copilot, internal documentation, analyst workflowsâ€¦)\n\n\\- average time spent in a session\n\n\\- roughly how many queries per session\n\nIâ€™m trying to understand actual behavioral patterns of users interacting with RAG systems. Papers and blog posts talk a lot about retrieval accuracy, but almost nothing about how people actually use these systems once deployed.\n\nHard to get this data without already operating one at scale, so even rough ranges or anonymized observations would be incredibly useful",
      "is_original_content": false,
      "link_flair_text": "Discussion",
      "permalink": "https://reddit.com/r/Rag/comments/1reu6t0/for_teams_selling_internal_ai_searchrag_what_does/",
      "domain": "self.Rag",
      "is_self": true,
      "comments": [
        {
          "id": "o7hpxe5",
          "author": "megAchiever",
          "text": "btw if anyone has such tool would like to incorporate for some of my clients. Reach to me if you have a solid RAG tool, specifically for long pdfs!",
          "score": 1,
          "created_utc": "2026-02-26 10:28:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7jxss0",
              "author": "adukhet",
              "text": "Yeah.. this actually matters a lot for how you configure a RAG system.\nMost implementations assume users will chat for a while, but in practice people ask 1â€“2 questions, open a source, and leave.\nBecause of that first-answer reliability and retrieval quality matter way more than conversation memory or huge context. Looks like many devs tune prompts/models and skip this behaviour part",
              "score": 1,
              "created_utc": "2026-02-26 17:57:40",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o7m7qyb",
              "author": "Mammoth-Camel1508",
              "text": "This is actually something we are focusing on with our solution - we build rag for businesses with lots of heavy technical documentations and manuals. Mainly for manufacturing and agriculture industry, where it's important to get it right, otherwise the end users won't use it.\n\nSome customers have 1M+ pages, with documents 1k+ pages long. I'd be happy to talk if you have some potential customers where it might help them.",
              "score": 1,
              "created_utc": "2026-02-27 00:42:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7mf3kg",
                  "author": "Wide_Brief3025",
                  "text": "Getting users to trust and actually use AI search in such technical environments comes down to surfacing the most relevant info at the right time and being quick to incorporate feedback. Tracking where users ask questions outside your tool can give a lot of insight into gaps. If you want to catch those unaddressed conversations across platforms, ParseStream helps surface them in real time so you can engage or learn what people actually need.",
                  "score": 1,
                  "created_utc": "2026-02-27 01:24:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    }
  ]
}