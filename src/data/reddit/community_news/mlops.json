{
  "metadata": {
    "last_updated": "2026-01-28 08:47:44",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 11,
    "total_comments": 38,
    "file_size_bytes": 52016
  },
  "items": [
    {
      "id": "1qnlepn",
      "title": "Static model selection did not work (enough) for us",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qnlepn/static_model_selection_did_not_work_enough_for_us/",
      "author": "tech2biz",
      "created_utc": "2026-01-26 16:49:11",
      "score": 60,
      "num_comments": 17,
      "upvote_ratio": 0.96,
      "text": "We spent a few months now on a solution for dynamic model routing because we tried several things and nothing really solved our problem.\n\nThe core issue / our background: we deployed nodes with SLM and RAG to regulated industry teams (the problem is relevant in any setup though). But users couldn't figure out when to use which model (despite ongoing effort to educate). We tried static routing but the classification of queries upfront didn't really work as it was very unpredictable what the users were doing. Also the \"guessing\" part did not feel right, we iterated really a lot on this. So next we thought hybrid with big models would be the solution but somewhat similar we always had to estimate complexity before we saw output. The estimates missed often enough that we either overspent (like, radically, breaking our unit economics) or quality was bad from routing too aggressively to small models.\n\nWe found a Google publication (happy to share) that approaches this very differently, not routing but cascading. Start generating with the small model, validate quality as you go, escalate only if needed.\n\nWe developed this and open-sourced our implementation: [github.com/lemony-ai/cascadeflow](http://github.com/lemony-ai/cascadeflow)\n\nIt plugs into your existing infrastructure, works with LiteLLM, OpenRouter, n8n, LangChain, or direct API calls. From there you can use whatever models you want: OpenAI, Anthropic, Groq, HuggingFace, local models via Ollama, self-hosted via vLLM.\n\nNot replacing your router or orchestration layer, just adding quality validation that decides when the cheap models output is actually good enough.\n\nSeeing 40-90% cost reduction in first production workloads and we are honestly quite excited. Would love feedback and happy to chat with others working on inference layers.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qnlepn/static_model_selection_did_not_work_enough_for_us/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1unm6l",
          "author": "monnamou",
          "text": "How does the quality valid‚Å§ation work? Is it a sep‚Å§arate model call? Worried about adding latency if it's another LLM judg‚Å§ing the outp‚Å§ut.",
          "score": 2,
          "created_utc": "2026-01-26 17:10:32",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uo88c",
              "author": "tech2biz",
              "text": "Totally fair concern. It's not another full LLM call. we use lightweight validation (confidence scores, completion checks, some heuristics depending on task type). Latency add is minimal, usually under 20ms. My co-founder Sascha can go deeper on the technical side if you want. But we are coming from the on-prem world, so latency was actually our initial focus (later figured it also reduces the costs alike).",
              "score": 4,
              "created_utc": "2026-01-26 17:13:13",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1v9nns",
                  "author": "ediblescholarship",
                  "text": "Confidence scores are tricky though.small models are often confidently wrong. How do you calibrate the threshold? Or is it task-specific?",
                  "score": 1,
                  "created_utc": "2026-01-26 18:44:51",
                  "is_submitter": false,
                  "replies": []
                },
                {
                  "id": "o1zi2jf",
                  "author": "monnamou",
                  "text": "Interesting, so what happens when the heuristics miss? Do you have a fallback or does it just accept the output?",
                  "score": 1,
                  "created_utc": "2026-01-27 09:06:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1utlal",
          "author": "CrossyAtom46",
          "text": "Can you share that Go‚Å§ogle publication?",
          "score": 2,
          "created_utc": "2026-01-26 17:36:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uwqf7",
              "author": "tech2biz",
              "text": "Sure! This is the blog: [https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/](https://research.google/blog/speculative-cascades-a-hybrid-approach-for-smarter-faster-llm-inference/)  \nAnd the research paper: [https://openreview.net/pdf?id=vo9t20wsmd](https://openreview.net/pdf?id=vo9t20wsmd)",
              "score": 3,
              "created_utc": "2026-01-26 17:50:38",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1xv5ro",
                  "author": "CrossyAtom46",
                  "text": "Ok I tried it on our summaries. 73% with Ha‚Å§iku, rest Sonn‚Å§et. quality does look good a first sight. this is wild",
                  "score": 2,
                  "created_utc": "2026-01-27 02:08:57",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1uz339",
          "author": "delulucoreandcrazyaf",
          "text": "Our Fin‚Å§ops team seems to have nothing else in mind than AI costs these days, nice you bilt this. Looks promising. What's the story on production stability?",
          "score": 2,
          "created_utc": "2026-01-26 18:00:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v4w0f",
              "author": "tech2biz",
              "text": "Streaming works, we use it ourselves. We've ran various 100k benchmarking queries incl concurrent without issues. But also honestly, we are still learning at scale. Would love to hear if you're planning to push volume.",
              "score": 1,
              "created_utc": "2026-01-26 18:24:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uzqh7",
          "author": "jasongreen7457",
          "text": "We're on Langch‚Å§ain with a mix of Open‚Å§AI and local Llama. You say this should work with all of that? Do we need to change our ch‚Å§ain structure or does it wrap around existing calls?",
          "score": 2,
          "created_utc": "2026-01-26 18:03:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v6tya",
              "author": "tech2biz",
              "text": "it wraps around your existing calls, you don't need to restructure your chains. Basically a few lines to initialize and then swap your calls. We have a LangChain example in the repo. If you hit any issues just open an issue or ping us, still early days so feedback on integration pain points is super useful.",
              "score": 1,
              "created_utc": "2026-01-26 18:33:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1v10l0",
          "author": "Outrageous-Ice426",
          "text": "This could be interesting for our agents, they make 10-15 tool calls per task and thinking about it most of them are simple. Will definitely look into this.\n\nTh‚Å§x for the n‚Å§8n integration.",
          "score": 2,
          "created_utc": "2026-01-26 18:08:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1v5aod",
              "author": "tech2biz",
              "text": "awesome! Every feedback much appreciated. And for agentic, we are also working on clawdbot integration.",
              "score": 5,
              "created_utc": "2026-01-26 18:26:40",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1uuhq4",
          "author": "needinghelp1234",
          "text": "This! We gave this a shot last week (cust‚Å§omer support bot). We were really sceptical but thought we'd try. We already have a class‚Å§ifier running but the cascading caught edge cases and we added some hug‚Å§gingface for better variation in the cascading (I wouldn't wanna take care of that but with this I dont have to so its nice). Not 90% sav‚Å§ings for us but solid 50 which still matters at our volume.",
          "score": 1,
          "created_utc": "2026-01-26 17:40:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1uxbls",
              "author": "tech2biz",
              "text": "Nice to hear!!! 50% is real money. And yes, the edge cases are exactly it, our routers always either led to bad quality or overpaying, it always felt there is no middle ground.",
              "score": 1,
              "created_utc": "2026-01-26 17:53:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qlgyco",
      "title": "DevOps ‚Üí MLOps Interview Lesson: They don't care about your infra skills until you show you understand their pain",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qlgyco/devops_mlops_interview_lesson_they_dont_care/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-24 07:46:18",
      "score": 59,
      "num_comments": 7,
      "upvote_ratio": 0.86,
      "text": "Had an interview recently that exposed a blind spot I didn't know I had.\n\nBackground: 11+ years in DevOps, extensive experience with Kubernetes, cloud infra, CI/CD. Transitioned into MLOps over the past few years.\n\nThe hiring manager asked: \"How would you help build a platform for our data science and research teams?\"\n\nMy brain immediately jumped to: Kubernetes, model serving, MLflow, autoscaling, GPU scheduling...\n\nBut that's not what they were asking. They wanted to know whether I understood the *problems* DS teams actually face day to day.\n\nI stumbled. Not because I don't know the tech, but because I framed everything around my expertise instead of their pain points.\n\nIt made me realise something (probably obvious to many of you, but it was a gap for me):\n\nIn DevOps, the customer is fairly clear‚Äîdevelopers want to ship faster, ops wants reliability. In MLOps, you're serving researchers and data scientists with very different workflows and frustrations.\n\nThe infra knowledge is table stakes. The harder part is understanding things like:\n\nWhy does a 3-hour training job failing on a dependency error feel so demoralising?\n\nWhy do they keep asking for \"just one more GPU\"?\n\nWhy does reproducibility matter *to them*, not just to the platform team?\n\nStill working on building this muscle. Curious if others who've made the DevOps ‚Üí MLOps shift have run into something similar?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qlgyco/devops_mlops_interview_lesson_they_dont_care/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1eqv2u",
          "author": "extreme4all",
          "text": "Not just devops, i'm more a security person and its the same instinctively most engineers think of the technical stuf and not about the problem. The otherday an architect even came to me with a tool & technology he thinks is great and we should use but than i asked him what problem and whose problem it would solve he couldn't answer. Yes the tech looked cool, but it didn't seem to address a (burning) problem my team or any other team in the org has.\n\nIt burns down to the same thing you explained, we need more people that can talk to the users of the services we provide, understand their problem ans engineer solutions for those instead of engineering solutions for problems that don't matter.",
          "score": 10,
          "created_utc": "2026-01-24 11:22:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1frdv4",
          "author": "NotSoGenius00",
          "text": "TBH what you said is 100% true and infra knowledge is okay but if MLOps is doing everything why is there a need for DevOps ? I think MLops is more nuanced that tools/methods etc. each ds/research workflow is different across all orgs and most orgs dont know what they want. \nIMHO devops is least thing I am worried about, the most important thing to be worried about during an interview is dev ex/velocity for their teams ! And that is gold if you understand",
          "score": 2,
          "created_utc": "2026-01-24 15:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kwdef",
          "author": "No_Refrigerator6755",
          "text": "is it good time for a 2026 grad to start learning mlops , already into devops",
          "score": 2,
          "created_utc": "2026-01-25 07:25:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o210mzh",
              "author": "Extension_Key_5970",
              "text": "Yes, you can start exploring mlops, but tbh, I would suggest to start with ML foundations or data distribution systems, as you are in early career, try to stick to any one of it, as currently, as per me, there are two kind of people are coming in MLOps, one coming from data/infra/devops or from core DS/ML eng.",
              "score": 1,
              "created_utc": "2026-01-27 15:12:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1j69vo",
          "author": "Hyperventilater",
          "text": "Look into the modern practices of \"Enterprise Architecture\". That field deals with this quite often in a more general way.\n\nThe crux of it is: if nobody is asking for it, then it doesn't provide value. Your experience doesn't mean anything if you can't relate it to the particular person's problems they're trying to solve by the position they're filling. Those problems are ALWAYS defined by the stakeholders.",
          "score": 1,
          "created_utc": "2026-01-25 00:55:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o20jzi5",
          "author": "Imaginary-Reading130",
          "text": "Hi op currently want to transform devops to mlops engineer with 10 years exp\nCare to share some resources",
          "score": 1,
          "created_utc": "2026-01-27 13:49:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2111ty",
              "author": "Extension_Key_5970",
              "text": "not specific courses I followed, but could suggest to start with ML foundations and Python coding must  \nreview a few of my past posts  \n[https://www.reddit.com/r/mlops/comments/1qiqcl6/coming\\_from\\_devopsinfra\\_to\\_mlops\\_heres\\_what\\_i/](https://www.reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/)  \n[https://www.reddit.com/r/mlops/comments/1q1vdh3/devops\\_ml\\_engineering\\_offering\\_11\\_calls\\_if\\_youre/](https://www.reddit.com/r/mlops/comments/1q1vdh3/devops_ml_engineering_offering_11_calls_if_youre/)",
              "score": 2,
              "created_utc": "2026-01-27 15:14:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qmj990",
      "title": "[Passed] NVIDIA Agentic AI Certification (NCP-AAI)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/",
      "author": "Ranger_1928",
      "created_utc": "2026-01-25 13:34:42",
      "score": 28,
      "num_comments": 24,
      "upvote_ratio": 0.95,
      "text": "Just wanted to share a data point for anyone eyeing the new NVIDIA Agentic AI certification. I sat for the exam this today and passed! üöÄ  \nI already had experience building agents with LangChain/OpenAI, but I quickly realized this exam requires a mindset shift. It‚Äôs less about generic Python loops and more about the \"NVIDIA Way\" (NIMs, Triton, NeMo).\n\n**The Results (The Good & The Ugly):**  \nI wanted to be transparent about the score breakdown because it tells a story:\n\n* **Platform Implementation:**¬†85% \n* **Deployment & Scaling:**¬†79%\n* **Safety, Ethics & Compliance:**¬†...35% üòÖ\n\n**My Takeaway:**  \nIf you are preparing,¬†**do not sleep on the infrastructure**. The reason I passed is that I focused nicely on understanding¬†**NIM microservices, Triton Inference Server, and Kubernetes scaling**. If I had relied only on my generic \"coding agents\" knowledge, I would have failed.\n\nAlso, Don't make my mistake‚Äîstudy the \"boring\" safety docs of safety, Ethics and Human in Loop Too!\n\n  \n**Rest assured, Ask me Anything about the exam and I will try my best to help** ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1n4fze",
          "author": "aksrao1998",
          "text": "Hi did you take any course to prepare?",
          "score": 3,
          "created_utc": "2026-01-25 16:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n5pws",
              "author": "Ranger_1928",
              "text": "Nope, online resources. Got a roadmap and list of online articles / videos with the help of ChatGPT.",
              "score": 1,
              "created_utc": "2026-01-25 16:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nm00r",
                  "author": "dank_coder",
                  "text": "Can you please share your roadmap?",
                  "score": 1,
                  "created_utc": "2026-01-25 17:43:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1n6amv",
              "author": "DenseIncome4394",
              "text": "\\+1",
              "score": 0,
              "created_utc": "2026-01-25 16:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1ma4an",
          "author": "proof_required",
          "text": "How long did you prepare it for? How experienced are you?",
          "score": 1,
          "created_utc": "2026-01-25 14:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mbbn2",
              "author": "Ranger_1928",
              "text": "Hey,\n\nI have 1.5 years of indsutry experience working with LLMs and agents. But most of it was using Open Source Libraries. The transition from open source to nvidia tools has its own learning curve and took around 3-4 days (\\~ 1hour/day) for me to understand it.\n\nOverall preparation was around 17-20 days (\\~ 1/1.5 hour/day) with initial sections taking over 4 days. The last 3 sections (around \\~15%) of whole syllabus was done by me in an hour, didn't gave much attention to it, and that became the lowest scoring section for me now. Should've given more time to it too.",
              "score": 3,
              "created_utc": "2026-01-25 14:08:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o303g",
                  "author": "Competitive-Fact-313",
                  "text": "Can you talk about your experience, what‚Äôs like using open source - on a daily basis how your day look like what are the tools you are most exposed with",
                  "score": 1,
                  "created_utc": "2026-01-25 18:53:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mldp3",
          "author": "Yarafsm",
          "text": "Do you think it makes sense for anyone with no ML experience but quite a bit of cloud/platform architect experience ?\nAlso was it more focussed on engineering or architecture?",
          "score": 1,
          "created_utc": "2026-01-25 15:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mm0lq",
              "author": "Ranger_1928",
              "text": "The exam is more **engineering-focused** than pure architecture. Your cloud background helps with deployment/scaling, but you‚Äôll need extra prep on NVIDIA‚Äôs AI/ML stack (NIM, Triton, NeMo, safety modules). It‚Äôs doable if you‚Äôre comfortable picking up new tools quickly.",
              "score": 3,
              "created_utc": "2026-01-25 15:04:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mwbsg",
                  "author": "dank_coder",
                  "text": "I am familiar with building AI agents. I have built and deployed agents using Langgraph. How time do you think I need to dedicate for this?",
                  "score": 1,
                  "created_utc": "2026-01-25 15:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1nu8xj",
          "author": "Agreeable-Court602",
          "text": "Congratulations üéâüéâ. Also recently passed the same.",
          "score": 1,
          "created_utc": "2026-01-25 18:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o1wki",
              "author": "Ranger_1928",
              "text": "Thanks, and Congratulations to you too üî•üî•",
              "score": 1,
              "created_utc": "2026-01-25 18:48:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1prxut",
              "author": "ab624",
              "text": "how did you prepare ? any good resources and suggestions",
              "score": 1,
              "created_utc": "2026-01-25 23:21:16",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1rjjht",
                  "author": "Ranger_1928",
                  "text": "Didn't took any course, only online freely available resources. The best I found for AI Agents was IBM videos, and for Nvidia architecture - Nvidia articles and youtube videos. Shared a roadmap for studying in this link:  \n[https://limewire.com/d/krbfj#WgVuKGW8kz](https://limewire.com/d/krbfj#WgVuKGW8kz)",
                  "score": 1,
                  "created_utc": "2026-01-26 04:58:21",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1q4jrg",
          "author": "burntoutdev8291",
          "text": "How useful is this cert? Do you have any plans on taking the NCA-AIIO?\n\nAnyway great work, I would think the safety and ethics stuff is very important because most of us know how to serve but don't know much about compliance.",
          "score": 1,
          "created_utc": "2026-01-26 00:22:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1rk6gd",
              "author": "Ranger_1928",
              "text": "Good point ‚Äî I think AIIO and AAI target slightly different audiences. AIIO is more about understanding the NVIDIA ecosystem and infra basics, while AAI dives into building and deploying agentic AI apps. I found AAI more hands‚Äëon since it‚Äôs closer to real-world workflows, but AIIO gives the foundation to appreciate how the infra side works. They complement each other depending on whether you‚Äôre infra‚Äëheavy or app‚Äëheavy. As for plans, I don't have any near future plans for NCA-AIIO.\n\nThanks, and yes, safety, ethics and compliance stuff is important, and it contains a very wide range of issues and solutions than we normally understand. So, its a must go.",
              "score": 1,
              "created_utc": "2026-01-26 05:02:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1si7oj",
          "author": "ConnentingDots",
          "text": "!RemindMe 2 weekd",
          "score": 1,
          "created_utc": "2026-01-26 09:40:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1si9w9",
              "author": "RemindMeBot",
              "text": "**Defaulted to one day.**\n\nI will be messaging you on [**2026-01-27 09:40:46 UTC**](http://www.wolframalpha.com/input/?i=2026-01-27%2009:40:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/o1si7oj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qmj990%2Fpassed_nvidia_agentic_ai_certification_ncpaai%2Fo1si7oj%2F%5D%0A%0ARemindMe%21%202026-01-27%2009%3A40%3A46%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qmj990)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-26 09:41:21",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qnazmo",
      "title": "MLOps Roadmap",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qnazmo/mlops_roadmap/",
      "author": "Deep_Priority_2443",
      "created_utc": "2026-01-26 08:58:33",
      "score": 28,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "Hi there, if this is of help to you, [roadmap.sh](http://roadmap.sh) has just launched a revised version of its [MLOps roadmap](https://roadmap.sh/mlops). I want to thank the people in this group who contributed to the review of the roadmap with their feedback. \n\nhttps://preview.redd.it/kolchhwvrnfg1.png?width=1088&format=png&auto=webp&s=151207b5db9b37c170fdbf58c3f39d131a826d90\n\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qnazmo/mlops_roadmap/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1u68ej",
          "author": "Competitive-Fact-313",
          "text": "Adding Prometheus and Grafana as observability as a wise choice.",
          "score": 2,
          "created_utc": "2026-01-26 15:56:11",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkrnnj",
      "title": "MLOps Free Course?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkrnnj/mlops_free_course/",
      "author": "AccountantUsual1948",
      "created_utc": "2026-01-23 14:04:22",
      "score": 11,
      "num_comments": 6,
      "upvote_ratio": 0.83,
      "text": "I‚Äôm getting into¬†**MLOps**¬†and looking for any¬†**free courses or solid resources**.",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qkrnnj/mlops_free_course/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o19savn",
          "author": "fmindme",
          "text": "Hello. I've created this course, focused on the coding part of MLOps: https://mlops-coding-course.fmind.dev/. It's totally free, and there is a side repository https://github.com/fmind/mlops-python-package with a concrete example.",
          "score": 6,
          "created_utc": "2026-01-23 17:18:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19rym3",
          "author": "navin_fakirpure",
          "text": "Search on youtube channel",
          "score": 2,
          "created_utc": "2026-01-23 17:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1asgd8",
          "author": "dvdsdr",
          "text": "[https://madewithml.com/courses/mlops/](https://madewithml.com/courses/mlops/)",
          "score": 2,
          "created_utc": "2026-01-23 20:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1r9f7q",
          "author": "NeatChipmunk9648",
          "text": "coursera and datacamp",
          "score": 1,
          "created_utc": "2026-01-26 03:54:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1zbx0n",
          "author": "Capital-Vehicle9906",
          "text": "vikash das on yt",
          "score": 1,
          "created_utc": "2026-01-27 08:09:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18y4cy",
          "author": "letsTalkDude",
          "text": "Why not searching this sub?",
          "score": 0,
          "created_utc": "2026-01-23 15:00:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qixc5n",
      "title": "Looking for consulting help: GPU inference server for real-time computer vision",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "author": "bix_mobile",
      "created_utc": "2026-01-21 13:04:09",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 0.92,
      "text": "We're building a centralized GPU server to handle inference requests from multiple networked instruments running YOLO-based object detection and classification models. Looking for someone with relevant experience to consult on our architecture.\n\n**What we're trying to optimize:**\n\n* End-to-end latency across the full pipeline: image acquisition, compression, serialization, request/response, deserialization, and inference\n* API design for handling concurrent requests from multiple clients\n* Load balancing between two RTX 4500 Blackwell GPUs\n* Network configuration for low-latency communication\n\n**Some context:**\n\n* Multiple client instruments sending inference requests over the local network\n* Mix of object detection and classifier models\n* Real-time performance matters‚Äîwe need fast response times\n\nIf you have experience with inference serving (Triton, TorchServe, custom solutions), multi-GPU setups, or optimizing YOLO deployments, I'd love to connect. Open to short-term consulting to review our approach and help us avoid common pitfalls.\n\n**If you're interested, please DM with your hourly rate.**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0zl93r",
          "author": "Friendly_Willow_8447",
          "text": "Are you building this on top of any cloud providers or you have your own infrastructure? Also do you plan or are you doing kubernetes?",
          "score": 1,
          "created_utc": "2026-01-22 04:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10v0ml",
              "author": "bix_mobile",
              "text": "Server is on prem. K8s is in the plans",
              "score": 3,
              "created_utc": "2026-01-22 10:28:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11ywks",
                  "author": "Friendly_Willow_8447",
                  "text": "Cool. DMed you",
                  "score": 1,
                  "created_utc": "2026-01-22 14:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o17o96w",
          "author": "Good-Coconut3907",
          "text": "Happy to help! Just DM'd you.\n\nI've built an open source platform to manage AI deployments across multiple GPUs. \n\n[https://github.com/kalavai-net/kalavai-client](https://github.com/kalavai-net/kalavai-client)",
          "score": 1,
          "created_utc": "2026-01-23 10:01:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkshsv",
      "title": "Who is training on TBs of data?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkshsv/who_is_training_on_tbs_of_data/",
      "author": "HahaHarmonica",
      "created_utc": "2026-01-23 14:38:04",
      "score": 9,
      "num_comments": 10,
      "upvote_ratio": 0.8,
      "text": "As the title says, who is training a single model on 10s-100sTB? What is your stack? What software are you using on the orchestration side of things to do this over multiple nodes? What are you using on the model training side?\n\nThey have about 18TB now, but are ramping up their data collection over the next 6 months and will be collecting significantly more data. This would be to train a single model. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qkshsv/who_is_training_on_tbs_of_data/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1c0yw4",
          "author": "burntoutdev8291",
          "text": "I am using, but what's the issue? Is it LLM?",
          "score": 1,
          "created_utc": "2026-01-23 23:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c1ajv",
              "author": "HahaHarmonica",
              "text": "what?",
              "score": 0,
              "created_utc": "2026-01-23 23:43:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1c34fb",
                  "author": "burntoutdev8291",
                  "text": "Software: If it's LLM based, we usually use training frameworks like NeMo. They support distributed training very well. Pytorch lightning, huggingface and mosaicml are good options as well if it's not LLMs. Also I didn't really get what you meant by stack, but for training we are using slurm. \n\nStorage: Data is always on a distributed storage, like lustre or weka. An NFS will work as well, but performance won't be as good. \nFor data storage, what is your current file format?",
                  "score": 2,
                  "created_utc": "2026-01-23 23:53:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d2ks4",
          "author": "Scared_Astronaut9377",
          "text": "You need to specify what you are doing. My experience training recommendation models on hundreds of billions of rows will not help you to fine-tune stable diffusion on a million high-resolution photos.",
          "score": 0,
          "created_utc": "2026-01-24 03:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nuv2s",
          "author": "H3zi",
          "text": "Aws SageMaker Training \n1-128 H100/H200\nS3 (fastfile mode)\nPlain PyTorch + HuggingFace Accelerate\nOur own data loader based on webdataset \n\nPre training / fine tuning diffusion models",
          "score": 0,
          "created_utc": "2026-01-25 18:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qntt3",
              "author": "tensorpool_tycho",
              "text": "u can get 128 h100 on demand w/ sagemaker? or is that a dedicated cluster?",
              "score": 1,
              "created_utc": "2026-01-26 01:59:18",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1regea",
                  "author": "H3zi",
                  "text": "Training plans",
                  "score": 1,
                  "created_utc": "2026-01-26 04:25:22",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1e31do",
          "author": "cosmic_timing",
          "text": "Idk but I'm not even training models anymore. Not for at least 6 months. 40$ fpga board does the trick \n\n\n\nWhat are you doing with all that compute? Surveillance? Lol",
          "score": -1,
          "created_utc": "2026-01-24 07:44:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkuup9",
      "title": "Azure ML v2 and MLflow hell",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkuup9/azure_ml_v2_and_mlflow_hell/",
      "author": "Ordinary_Platypus_81",
      "created_utc": "2026-01-23 16:07:15",
      "score": 7,
      "num_comments": 7,
      "upvote_ratio": 0.83,
      "text": "Hello,\n\n  \nI am just a recent grad (and from a ds degree too), so excuse my lack of expertise.\n\nWe are setting up ML orchestration in Azure ML and with MLflow. I have built the training pipelines and everything works nicely, I can register models and use them for scoring locally. However, I have had no luck deploying. I cannot seem to get the versions of packages to match up. The official Microsoft docs seem to be using varying versions and I just want a combination that works.  \n  \nWould y'all have any tips on finding one working combination and sticking to it? We are just in the building phase, so I can change everything still.\n\n(I am trying to deploy an xgboost model if that helps)\n\nThanks heaps!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qkuup9/azure_ml_v2_and_mlflow_hell/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1bvnwp",
          "author": "rishiarora",
          "text": "cfbr",
          "score": 1,
          "created_utc": "2026-01-23 23:13:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ddjdi",
          "author": "ZeroCool2u",
          "text": "Similar to my Azure experience. I'm at a large org that uses AWS and Azure. Sagemaker is a similar, perhaps marginally better experience. We ended up spending a lot on a vendor MLOps platform to make it so we didn't have to deal with this type of stuff. Works well now, but super annoying and we wasted months.",
          "score": 1,
          "created_utc": "2026-01-24 04:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dh7hf",
              "author": "prasanth_krishnan",
              "text": "Can you elaborate on what vendor you choose and why and what problem it solved. Thanks.",
              "score": 2,
              "created_utc": "2026-01-24 04:49:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1dm6gt",
                  "author": "ZeroCool2u",
                  "text": "I was trying to avoid incurring a mods wrath re vendor promo, but w/e it's called domino data lab. \n\n\nVery enterprise focused. In our case it replaced multiple research university class on-prem HPC clusters. It can support relatively arbitrary tooling and we have people that use python, r, julia, rust but also stata, matlab, and fortran. In a pinch I've gotten some java stuff working on it that would have been annoying otherwise.\n\n\nUsually it's compared to databricks, which we also have, but it goes far beyond what dbx can do and has the added bonus of no usage based billing, so higher upfront cost, but dramatically lower total cost. Internally, dbx is just used by data engineers to create tables and then is wired up to starburst. People either query starburst, dbx directly, or random on-prem sql databases and pull it all into domino to do the actual work with whatever tools they need. They deploy the models as batch jobs, flyte jobs, or as model API's and might wrap a dash/shiny/streamlit app or something around the deployed model api as an easy to use front end. It handles all the scaling and auth for us and has this governance policy thing that lets you gate deployments, so you don't have to figure out wtf paperwork you have to do beforehand. Just fill in the blank and legal or risk or whoever gets an email telling them to go read your answers and approve or reject. \n\n\nHas its own MLFlow server and you can spin up spark, ray, dask, and MPI clusters and all the other typical MLOps features you'd expect. \n\n\nIt's just a more cohesive vision and actually works instead of the half baked stuff the hyperscalers sell. Definitely not for everyone, but tends to just work. If you google it look for the user guide, the main site is pretty marketing heavy.",
                  "score": 2,
                  "created_utc": "2026-01-24 05:24:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g8a46",
          "author": "manwithaplandy",
          "text": "Iirc, you can create your own environment (basically just a custom container config) which lets you control what packages are installed and what versions, and deploy using that environment.",
          "score": 1,
          "created_utc": "2026-01-24 16:37:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1tnx6g",
              "author": "OtherPromisedLand",
              "text": "Yes, this. You can use tools like poetry/conda(maybe UV too) to build your .toml/environment.yml file with installed package versions from your local that works. I'm still learning, but I know in Azure you can configure your environment with a base image + your environment.yml and Azure builds these packages(from environment.yml) into your VM when it spins it up during deployment.",
              "score": 2,
              "created_utc": "2026-01-26 14:31:27",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o19ed90",
          "author": "mutlu_simsek",
          "text": "Try Perpetual ML.",
          "score": -2,
          "created_utc": "2026-01-23 16:15:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qm58ql",
      "title": "Deploy Your First ML Model on GCP Step-by-Step Guide with Cloud Run, GCS & Docker",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qm58ql/deploy_your_first_ml_model_on_gcp_stepbystep/",
      "author": "gringobrsa",
      "created_utc": "2026-01-25 01:25:48",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.78,
      "text": "Walks through deploying a machine learning model on Google Cloud from scratch.  \nIf you‚Äôve ever wondered how to take a trained model on your laptop and turn it into a real API with Cloud Run, Cloud Storage, and Docker, this is for you.\n\nHere‚Äôs the link if you‚Äôre interested:  \n[https://medium.com/@rasvihostings/deploy-your-first-ml-model-on-gcp-part-1-manual-deployment-933a44d6f658](https://medium.com/@rasvihostings/deploy-your-first-ml-model-on-gcp-part-1-manual-deployment-933a44d6f658)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qm58ql/deploy_your_first_ml_model_on_gcp_stepbystep/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qp2mq3",
      "title": "Machine learning Interview",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qp2mq3/machine_learning_interview/",
      "author": "jfhurtado89",
      "created_utc": "2026-01-28 05:29:56",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I have a ML interview coming up and these are the types of asking.\n\nTechnical / Role‚ÄëSpecific Questions (20 minutes):\n\nWe‚Äôll cover topics such as ML modeling, MLOps (deployment), system design, algorithms, GenAI, infrastructure & tooling, and commonly used frameworks.\n\nLive Coding Interview (30 minutes):\n\nA Google Collab notebook will be shared at the start of the interview. You‚Äôll be asked to share your screenwhile completing the exercises.\n\nCoding will focus on ML algorithms and implementations, transformer‚Äëbased GenAI concepts, debugging, and troubleshooting‚Äînot LeetCode‚Äëstyle problems.\n\nAdditional Note:\n\nYou will have full access to the internet and LLMs during the interview.\n\nWhat do you guys think, I should focus on the live coding part knowing that I‚Äôll have access to llms?\n\nI do have practical experience in deployment, works as a data scientist and finishing a masters in computer science in Georgia tech.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qp2mq3/machine_learning_interview/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qn2ho1",
      "title": "continuous debugging for long running training jobs?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qn2ho1/continuous_debugging_for_long_running_training/",
      "author": "tensorpool_tycho",
      "created_utc": "2026-01-26 01:47:45",
      "score": 3,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Are there any OSS agentic tools for debugging long running training jobs? Particularly Xid errors, OOMs, or other errors that pop up deep into training. \n\nor has anyone built tools out in house for this? curious what peoples' experiences have been.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qn2ho1/continuous_debugging_for_long_running_training/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1qp7a4",
          "author": "tensorpool_tycho",
          "text": "might just build this one myself but am curious if something exists alr. tbh if i cant debug an infra issue and i feed my whole context into claude, it usually gets it first or second try",
          "score": 1,
          "created_utc": "2026-01-26 02:05:59",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o1rxi0u",
          "author": "flyingPizza456",
          "text": "What do you mean by long running jobs? So you mean debugging during training? This is more a question of monitoring. Tensorboard, Mlflow etc. do help here.\n\nAnd why does ist need to be agentic? Feels like a buzzy question without more context.",
          "score": 1,
          "created_utc": "2026-01-26 06:40:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1ryf2h",
              "author": "tensorpool_tycho",
              "text": "Sorry ur right in retrospect that was kinda vague lol. I moreso mean if a run crashes from a Xid error, or an OOM issue, or something like that late into a training run. Feel like there have been a ton of times a job will crash and then my compute is just idle before I have to manually fix it in the morning",
              "score": 1,
              "created_utc": "2026-01-26 06:47:29",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1rygln",
              "author": "tensorpool_tycho",
              "text": "Gonna update my post",
              "score": 1,
              "created_utc": "2026-01-26 06:47:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o1zlmgl",
          "author": "Glad_Appearance_8190",
          "text": "havent seen a clean OSS silver bullet tbh. most teams i know end up stitching together logs, metrics, and checkpoints so you can rewind what state the job was actually in when it died. xid and oom stuff is usually more about visibility than fixing the error itself. if you cant trace what changed over time, debugging turns into guessing real fast.",
          "score": 1,
          "created_utc": "2026-01-27 09:40:16",
          "is_submitter": false,
          "replies": [
            {
              "id": "o256ssq",
              "author": "tensorpool_tycho",
              "text": "Hmm that‚Äôs interesting - so it‚Äôs really a problem about proper observability? I‚Äôve been thinking of how to holistically attack this so I can spend less time debugging training runs for customers. \n\nI‚Äôm thinking rn I‚Äôll properly track logs, metrics, etc, and just give CC access to my k8s cluster and have it go ham. Thoughts?",
              "score": 1,
              "created_utc": "2026-01-28 02:51:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o22wu8t",
          "author": "traceml-ai",
          "text": "I don‚Äôt know of a solid OSS ‚Äúagentic‚Äù solution for this yet.\n\nBut I have been working on a lightweight tool for continuous observability of long-running training jobs, mostly focused on surfacing ground-truth signals over time (step time drift, worst-rank vs median in DDP, memory evolution, dataloader stalls). \n\nIf this problem is something you are actively dealing with, happy to chat. I am still learning what signals actually matter in real systems.",
          "score": 1,
          "created_utc": "2026-01-27 20:09:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}