{
  "metadata": {
    "last_updated": "2026-02-27 09:08:44",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 17,
    "total_comments": 45,
    "file_size_bytes": 78647
  },
  "items": [
    {
      "id": "1rcp0ad",
      "title": "Broke down our $3.2k LLM bill - 68% was preventable waste",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "author": "llamacoded",
      "created_utc": "2026-02-23 18:11:09",
      "score": 58,
      "num_comments": 18,
      "upvote_ratio": 0.97,
      "text": "We run ML systems in production. LLM API costs hit $3,200 last month. Actually analyzed where money went.\n\n**68% - Repeat queries hitting API every time** Same questions phrased differently. \"How do I reset password\" vs \"password reset help\" vs \"can't login need reset\". All full API calls. Same answer.\n\nSemantic caching cut this by 65%. Cache similar queries based on embeddings, not exact strings.\n\n**22% - Dev/staging using production keys** QA running test suites against live APIs. One staging loop hit the API 40k times before we caught it. Burned $280.\n\nSeparate API keys per environment with hard budget caps fixed this. Dev capped at $50/day, requests stop when limit hits.\n\n**10% - Oversized context windows** Dumping 2500 tokens of docs into every request when 200 relevant tokens would work. Paying for irrelevant context.\n\nBetter RAG chunking strategy reduced this waste.\n\n**What actually helped:**\n\n* Caching layer for similar queries\n* Budget controls per environment\n* Proper context management in RAG\n\nCost optimization isn't optional at scale. It's infrastructure hygiene.\n\nWhat's your biggest LLM cost leak? Context bloat? Retry loops? Poor caching?",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcp0ad/broke_down_our_32k_llm_bill_68_was_preventable/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6zw8xk",
          "author": "Morpheyz",
          "text": "Cut cost by 99%. External consultants sold us Azure Open AI PTUs for 50k/month, claiming we absolutely needed them for our use case. Couple months later convinced leadership to switch to pay-as-you-yo model, now spending 300$/month.\n\nEdit: PTUs, not TPUs",
          "score": 20,
          "created_utc": "2026-02-23 18:39:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o700ndp",
              "author": "pmv143",
              "text": "Classic overprovisioning trap. Fixed infra before validated demand is expensive. Usage based models are much more forgiving while workloads are still evolving.",
              "score": 7,
              "created_utc": "2026-02-23 18:59:25",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o704ptq",
              "author": "m98789",
              "text": "Azure doesn‚Äôt offer TPUs tho",
              "score": 2,
              "created_utc": "2026-02-23 19:18:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o706ns8",
                  "author": "Morpheyz",
                  "text": "Typo, I meant PTUs.",
                  "score": 3,
                  "created_utc": "2026-02-23 19:27:18",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o70098v",
          "author": "pmv143",
          "text": " most ppl underestimate how much waste lives above the model. Interesting part is that even after fixing caching and RAG, infrastructure-level inefficiencies still compound at scale.",
          "score": 5,
          "created_utc": "2026-02-23 18:57:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o712b33",
          "author": "KeyIsNull",
          "text": "Mind to share some details about the semantic cache layer?¬†",
          "score": 2,
          "created_utc": "2026-02-23 21:59:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7az1td",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) i use bifrost gateway you can check it out here its oss",
              "score": 2,
              "created_utc": "2026-02-25 10:39:54",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o72zgwp",
          "author": "doolpicate",
          "text": "tiering, routing orchestration, and multiple models including LocalLLMs would have helped. Strange that people are not doing it in the beginning itself.",
          "score": 1,
          "created_utc": "2026-02-24 04:38:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o74mikd",
          "author": "ManufacturerWeird161",
          "text": "We had a similar bleed last year where our staging environment was burning through ~$400/day in GPT-4 calls because someone left a load test running over the weekend. Took us three days to notice because the cost alerts were batched weekly. Daily caps saved us but the real fix was making the staging LLM return deterministic garbage responses for any call pattern that looked synthetic‚Äîcut costs by 90% without hurting actual QA work.",
          "score": 1,
          "created_utc": "2026-02-24 12:55:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7bmfdv",
          "author": "Illustrious_Echo3222",
          "text": "That 68 percent repeat query number doesn‚Äôt surprise me at all. In a lot of systems, the LLM becomes the most expensive cache miss you‚Äôve ever deployed.\n\nContext bloat has been the biggest leak I‚Äôve seen. Teams over index on ‚Äújust give it more docs‚Äù instead of tightening retrieval quality. A sloppy RAG pipeline quietly doubles or triples spend because nobody notices incremental token creep.\n\nRetry loops are another hidden killer. Especially with agents. If you allow automatic retries with minor rephrasing and no cap, you can burn a ton of tokens on what is basically the same failure mode repeated three times.\n\nOne pattern that helped us was aggressive observability at the token level. Logging prompt tokens, completion tokens, and tool calls per request, then ranking endpoints by cost per successful outcome. When you frame it as cost per resolved task instead of cost per call, waste becomes obvious.\n\nAlso agree hard on environment separation. Using production keys in staging is basically handing your burn rate to a test script.\n\nCurious if semantic caching caused any weird edge cases with slightly different intent but similar phrasing? That‚Äôs usually where people get nervous.",
          "score": 1,
          "created_utc": "2026-02-25 13:28:53",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7hqvlw",
          "author": "llamacoded",
          "text": "For all the folks asking - i am currently using [bifrost](https://www.getmaxim.ai/bifrost) \\[OSS\\] as my ai gateway for semantic caching and budgeting controls. \\[personal bias\\]",
          "score": 1,
          "created_utc": "2026-02-26 10:37:26",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o70to85",
          "author": "inspectedinspector",
          "text": "How much will it cost you to build embedding-based semantic caching to save $2000?",
          "score": 0,
          "created_utc": "2026-02-23 21:18:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o71c9sa",
              "author": "ZestyData",
              "text": "Very little, could do it in an hour and with very low running costs. Embeddings and vector search are basically free.",
              "score": 2,
              "created_utc": "2026-02-23 22:50:28",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o76ucns",
                  "author": "inspectedinspector",
                  "text": "I assume this means you already have OpenSearch or Redis (or similar) infrastructure you can leverage? Building truly from scratch would not be free",
                  "score": 1,
                  "created_utc": "2026-02-24 19:17:09",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o72ktbl",
              "author": "burntoutdev8291",
              "text": "from_pretrained(\"BAAI/bge-m3\")\n\nLitellm also supports semantic caching. What OP didn't mention is false positives and privacy aware semantic caches. While its an instance cost saving, anything with some form of model behind must go through its own evals\n\nSo cost of deployment of semantic cache is easy, validation is the key step.",
              "score": 1,
              "created_utc": "2026-02-24 03:03:24",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o7az3ni",
              "author": "llamacoded",
              "text": "[https://docs.getbifrost.ai/features/semantic-caching](https://docs.getbifrost.ai/features/semantic-caching) this is what i used. its oss",
              "score": 1,
              "created_utc": "2026-02-25 10:40:22",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73ivt3",
          "author": "eliko613",
          "text": "This breakdown is spot on.\nIn most systems I‚Äôve looked at, model selection isn‚Äôt the primary cost driver ‚Äî it‚Äôs:\nSemantically duplicate queries with no intelligent cache\nStaging/QA hitting prod keys\nContext bloat in RAG\nRetry or loop logic nobody notices\nThe 68% repeat-query number feels very real. Once you cluster by intent instead of raw prompt strings, you usually discover a small handful of intents burning the majority of spend.\nAlso +1 on environment separation and hard caps ‚Äî that‚Äôs basic cloud hygiene, but it‚Äôs surprising how often it‚Äôs missing in LLM setups.\nLLM cost control is starting to look a lot like early cloud FinOps: visibility first, then guardrails, then optimization. I‚Äôve been exploring tools in this space (e.g., zenllm.io) that focus specifically on intent-level visibility and waste detection ‚Äî the patterns you‚Äôre describing show up immediately when you instrument properly.",
          "score": 0,
          "created_utc": "2026-02-24 07:14:26",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rd3g0s",
      "title": "Advice Needed on a MLOps Architecture",
      "subreddit": "mlops",
      "url": "https://i.redd.it/jvzejdhcyclg1.png",
      "author": "Drac084",
      "created_utc": "2026-02-24 03:18:05",
      "score": 49,
      "num_comments": 17,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rd3g0s/advice_needed_on_a_mlops_architecture/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o72pyso",
          "author": "Competitive-Fact-313",
          "text": "If you talk overall end to end improvement there is lot you can do. From argo to grafana. I use openshift along with gitops. As long as orchestration is concerned you can use terraform n its siblings. If I get your question right.",
          "score": 3,
          "created_utc": "2026-02-24 03:34:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o72r3jz",
          "author": "le-fou",
          "text": "I examined your diagram before I read the full post, and the two things that jumped out to me were 1) the arrows (presumably) triggering a training run with an API request and 2) the arrow going from the MLFlow tracking server to the deployment API. These correspond to your questions 1 and 3, so that is a good sign these could use some definition.\n\nFirstly, agreed that you want an orchestration layer for training. Dagster and Airflow are two common orchestration platforms, among others. Dagster has great k8s support. I haven‚Äôt used ZenML but a quick google search suggests to me it would also work fine for this. Asking AI for a comparison between these tools, given your requirements, would probably be fruitful. Regardless, this is all to say I think you‚Äôre right about needing something to orchestrate the training run. In your current diagram, for example, what exactly is hitting the endpoint? Some custom frontend? A curl command from your terminal? An orchestration framework allows you to schedule runs and/or manually trigger from a UI with your desired parameters.\n\nSecondly, the deployment process and trigger could use better definition. I personally use gitlab pipelines to build my custom model serving docker images with MLServer, and they get deployed via ArgoCD with the same CI/CD component any other non-ML app uses at my organization (I did need to write Helm charts for my MLServer deployments specifically). This pipeline could be triggered at the end of your training pipeline, or probably better you could use MLFlow aliasing/tags to fire a webhook for your deployment pipeline. But, fundamentally, building an image to serve your containers shouldn‚Äôt look functionally all that different from other build pipelines at your org, with the exception that ML containers can have some nasty dependencies and large artifact dependencies (model weights).\n\nLet me know if that all makes sense, or not.",
          "score": 3,
          "created_utc": "2026-02-24 03:42:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o72th38",
              "author": "Drac084",
              "text": "Thanks for the reply!\n\nFirstly, sorry the diagram is not perfect. It was a very quick sketch of the idea.\n\n1. I was thinking to implement the Training as a independent microservice(May be a simple FastAPI server) API request will trigger a pipeline and dispatch jobs. This can later be triggered from a frontend dashboard, but not in MVP level.  This workflow the main challenge I'm trying to sort out. \n\n2. I haven't given much thought to the Deployment and inference service at this point, assuming it will be less difficult once I figured out the training service. But what you suggested also make sense. I will do some more research on this. Thanks for your input! ",
              "score": 1,
              "created_utc": "2026-02-24 03:57:30",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o72vmz1",
                  "author": "Iron-Over",
                  "text": "An orchestrator is important because it creates a simple, reusable pipeline. Airflow allows ad-hoc and scheduled runs.¬†\n\nI assume you are using production data for training? If so, how do the data scientists view the training results and test results? I assume the notebooks should be hosted instead of the production data on laptops.¬†\n\nI would log each API call, including the features and the prediction to be matched ¬†with the actual outcome. This then becomes inexpensive labeled data for future training.\n\nYou may want to include shap to view the explainability of the prediction from the features.\n\nI did not see drift and skew detection on the data and the model it is useful to know when you need to retrain.¬†\n\n\n\n",
                  "score": 2,
                  "created_utc": "2026-02-24 04:12:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o73df41",
          "author": "prassi89",
          "text": "Overall arch looks great.\n\n1. Don‚Äôt go with dvc. When your datasets get large, you wont be able to stream (or mount) them transparently. Also data is repo bound logically. Use LakeFS directly.\n\n2. Skypilot is your best bet - it does training service APIs and compute orchestration. With  Other services like dagster, airflow you‚Äôll just spend ages debugging. Zenml is good but skypilot just gets out of the researchers way, and gives you multi cloud by default\n\n3. Mlflow also does a lot in the model promotion and deployment space. Consider it\n\nOverall, great stuff",
          "score": 5,
          "created_utc": "2026-02-24 06:26:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78q78q",
              "author": "Drac084",
              "text": "Got it. I was thinking if the LakeFS would be a overkill for this because we don't deal with insanely large datasets. But I understand your point in term of future scalability",
              "score": 1,
              "created_utc": "2026-02-25 00:51:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o73cggk",
          "author": "prasanth_krishnan",
          "text": "Orchestrator - metaflow\n\nDistributed training - apache ray\n\nExperiment tracking - ml flow\n\nModel packaging - mlflow models\n\nInference endpoint - MLserver or onnx\n\nFeature store - feast with actual stores of your choice.\n\nThis is a good framework neutral platform.",
          "score": 2,
          "created_utc": "2026-02-24 06:18:32",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o757nde",
          "author": "ManufacturerWeird161",
          "text": "We used DVC with MinIO at my last job, it worked well for data versioning but we found MLflow was better for the actual model registry piece to track lineage.",
          "score": 2,
          "created_utc": "2026-02-24 14:51:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o73wyir",
          "author": "htahir1",
          "text": "Great architecture sketch ‚Äî super critical to break things into distinct services early on.\n\nOn the orchestration question: since you're already on K8s and planning to extend to Slurm, you'll want something that abstracts away the infrastructure layer so your researchers aren't writing YAML all day. I've seen people have good experiences with Dagster and Kubeflow for this, but I'd also suggest taking a serious look at ZenML ‚Äî full disclosure, I'm part of the ZenML team, so take this with the appropriate grain of salt.\n\nThat said, the reason I think it's worth evaluating here specifically is that ZenML was designed to be a framework-agnostic orchestration layer that plugs into the tools you're already using (MLflow, K8s, S3/MinIO) rather than replacing them. So you'd keep your MLflow tracking, your MinIO storage, your K8s cluster ‚Äî ZenML just becomes the connective tissue that defines and runs your pipelines across all of it. It also plays nicely with the \"microservices\" mental model you're going for.\n\nA couple of non-ZenML-related thoughts too:\n\n* \\+1 to what others said about drift/skew detection ‚Äî worth thinking about early even if you don't implement it in your MVP.\n* The comment about LakeFS over DVC is worth considering, especially at scale with large datasets and streaming use cases.\n* For the deployment side, I'd honestly keep it simple at first. Honestly for smaller models use MLflow serving or even wrap in in a FastAPI, and then graduate to more complex services later \n\nGood luck with the build ‚Äî sounds like a fun project!",
          "score": 2,
          "created_utc": "2026-02-24 09:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pv51",
              "author": "Drac084",
              "text": "Thanks, I should look into ZenML eco system. Do you think the free version is enough for me to try it out and get an idea?\n\n",
              "score": 1,
              "created_utc": "2026-02-25 00:49:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o79vlx1",
                  "author": "htahir1",
                  "text": "Yes most of the non enterprisey features are oss , Pro only has governance and enterprise focused features for bigger teams you can adopt those later if at all",
                  "score": 2,
                  "created_utc": "2026-02-25 04:56:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o763ev8",
          "author": "alex000kim",
          "text": "Hey, imo, the overall approach is fine. I also agree with most of the feedback others have left. Leaving some of mine:\n\n\\- Simply stitching services together might not be the hardest part. What really requires some thinking is making it secure. I.e. the whole authentication flow from data to infra to model/artifact registry to deployment. Your diagram doesn‚Äôt show any of this.\n\n\\- A few things are undefined in the diagram: there‚Äôs no clear data path from S3/MinIO into the actual training pods, the ‚ÄúModel Selection‚Äù arrow from MLflow to your Deployment Service has no trigger mechanism (manual? webhook? CI pipeline?), and Slurm is mentioned in the text but completely absent from the diagram with no abstraction layer between K8s and Slurm.\n\n\\- That yellow ‚ÄúTraining Service API‚Äù box (job queue, state manager, scheduling, logs) is essentially an entire orchestration platform you‚Äôd be building from scratch. Worth thinking about whether you really want to own that.\n\n\\- Reconsider MinIO since the open-source project has been archived [https://news.ycombinator.com/item?id=47000041](https://news.ycombinator.com/item?id=47000041)\n\n\\- SkyPilot is really the way to go if you already have K8s and plan on adding Slurm into the mix. You write one task YAML and it works on both. When Slurm comes online you reuse existing task definitions instead of rewriting pipelines. Since the resources will be shared between team members, you‚Äôll most likely need to deploy and manage the central SkyPilot API server.\n\n\\- SkyPilot also has SkyServe [https://docs.skypilot.co/en/stable/serving/sky-serve.html](https://docs.skypilot.co/en/stable/serving/sky-serve.html) for the deployment/inference side. Add a service: block to a YAML and you get autoscaling, load balancing, and rolling updates. Worth evaluating before building a custom deployment service.",
          "score": 2,
          "created_utc": "2026-02-24 17:16:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o78pk54",
              "author": "Drac084",
              "text": "Thanks for the input.\n\nYes I haven't included slurm in the diagram because it's a future addition. But there should be a abstraction layer for compute infra/job dispatching to slurm/k8 within Training Service component. ",
              "score": 1,
              "created_utc": "2026-02-25 00:47:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7a1z8s",
          "author": "PleasantAd6868",
          "text": "training service api, would recommend jobset or kubeflow trainer CRDS (if you are already on k8s which looks like it from your diagram). if you need a resource manager + gang scheduling, either kueue or volcano. would not recommend more bloated options (i.e. Ray, skypilot, zenML) unless ur doing something super exotic with heterogeneous resources",
          "score": 1,
          "created_utc": "2026-02-25 05:43:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7gwfin",
          "author": "princess-barnacle",
          "text": "Just use flyte.",
          "score": 1,
          "created_utc": "2026-02-26 05:58:37",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o75x4pf",
          "author": "thulcan",
          "text": "Your real problem isn't which orchestrator to pick ‚Äî it's that you have five systems (DVC, MLflow, Harbor, MinIO, custom APIs) that each own a piece of what \"this model version\" means. That's five places where lineage breaks.\n\nModelKits ([KitOps](https://kitops.org), CNCF Sandbox) fix this at the artifact layer. A ModelKit is an OCI artifact ‚Äî same format as your Docker images ‚Äî that packages weights, dataset refs, config, and code with a Kitfile manifest. You already run Harbor and MinIO. Harbor becomes your single registry for images, models, and datasets. No new infrastructure.\n\nWhat changes:\n\n**DVC ‚Üí gone.** `kit pack` your datasets, push to Harbor. Versioning is OCI tags. No LakeFS either.\n\n**MLflow ‚Üí experiment tracking only.** Drop MLflow Model Registry and MLflow deployment. Harbor + ModelKits is your registry. MLflow is great for experiment tracking UI and bad at everything else it tries to do.\n\n**Training orchestration ‚Üí Argo Workflows.** CNCF graduated, K8s-native. Pipeline: `kit unpack` ‚Üí train ‚Üí `kit pack` ‚Üí `kit push`. Stop building a custom Training Service API with job queues and state managers. That's a multi-year project you don't need.\n\n**Governance gate (you're missing this).** Between trained and deployed: run ModelScan, attach cosign attestations, tag as `:approved`. You're a research org managing lots of models ‚Äî provenance isn't optional, and nobody in this thread mentioned it.\n\n**Deployment Service API ‚Üí gone.** KitOps has a native KServe `ClusterStorageContainer` integration. KServe pulls ModelKits directly from Harbor via OCI reference. No artifact retrieval logic, no container initialization code. Point KServe at [`harbor.yourorg.com/models/my-model:approved`](http://harbor.yourorg.com/models/my-model:approved), done.\n\nYou're currently stitching together DVC + MLflow Registry + MLflow Tracking + Harbor + MinIO + two custom APIs and hoping they agree on what \"model v2.3\" means. That's a lot of coordination surfaces to keep in sync. With KitOps: Harbor is your single source of truth, Argo runs your pipelines, MLflow tracks your experiments. Three tools, each doing one job. And you get security and provenance your current architecture doesn't even attempt.",
          "score": 1,
          "created_utc": "2026-02-24 16:48:27",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ravnd2",
      "title": "Cleared NVIDIA NCA-AIIO - Next Target: NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "author": "TuckerSavannah1",
      "created_utc": "2026-02-21 16:37:26",
      "score": 19,
      "num_comments": 20,
      "upvote_ratio": 0.96,
      "text": "Hello Everyone\n\nGlad to share that I‚Äôve successfully cleared the NVIDIA NCA-AIIO (AI Infrastructure & Operations) exam!\n\nMy journey was focused on building strong fundamentals in GPUs, networking, and AI infrastructure concepts. I avoided rote learning and concentrated on understanding how things actually work. Practice tests from itexamscerts also played a big role, they helped me identify weak areas and improve my confidence before the exam. Overall, if your basics are clear, the exam is very manageable.\n\nNow I‚Äôm preparing for NVIDIA NCP-AII, and I would really appreciate guidance from those who have cleared it.\n\n\\* How tough is it compared to NCA-AIIO?\n\n\\* Is it more hands-on or CLI/lab focused?\n\n\\* Any recommended labs?y\n\nI look forward to your valuable insights. Thank you.",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1ravnd2/cleared_nvidia_ncaaiio_next_target_ncpaii/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6n5oab",
          "author": "RubySera1",
          "text": "Well done and congrats! Could you please share which practice tests you found most useful for NCA-AIIO?",
          "score": 3,
          "created_utc": "2026-02-21 18:41:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6n7xk9",
              "author": "TuckerSavannah1",
              "text": "itexamscerts.",
              "score": 2,
              "created_utc": "2026-02-21 18:52:08",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zio9a",
          "author": "Sure-Programmer-8462",
          "text": "Nice achievement. Were the NCA-AIIO exam questions more theoretical or based on real world scenarios?",
          "score": 2,
          "created_utc": "2026-02-23 17:37:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zjnod",
              "author": "TuckerSavannah1",
              "text": "The exam was a mix of both, but many questions were scenario-based. If you understand the core concepts and practical use cases, it becomes much easier to handle.",
              "score": 1,
              "created_utc": "2026-02-23 17:42:11",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6zlaws",
                  "author": "Sure-Programmer-8462",
                  "text": "Did you find the exam objectives closely aligned with the official syllabus or were there some unexpected topics?",
                  "score": 2,
                  "created_utc": "2026-02-23 17:49:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6nzjbf",
          "author": "Satsuma_Johnson",
          "text": "congrats..",
          "score": 1,
          "created_utc": "2026-02-21 21:15:41",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh2d5",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:29:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6tcfen",
          "author": "Wright_Lucy11",
          "text": "congrats.",
          "score": 1,
          "created_utc": "2026-02-22 18:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zh3ne",
              "author": "TuckerSavannah1",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-23 17:30:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6zidpo",
          "author": "GrapeThompson",
          "text": "Congratulations on clearing the NCA-AIIO exam! How long did you take to prepare for it, and did you already have experience with AI infrastructure before starting?",
          "score": 1,
          "created_utc": "2026-02-23 17:36:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6zj98g",
              "author": "TuckerSavannah1",
              "text": "Thank you! It took me around 4‚Äì5 weeks of consistent study. I had some basic knowledge of networking and virtualization, but I was new to AI infrastructure concepts. I focused on understanding GPU architecture and deployment scenarios.",
              "score": 1,
              "created_utc": "2026-02-23 17:40:19",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7kvj1g",
          "author": "Dorthy_PinkLace",
          "text": "Congrats! I‚Äôm planning to take the NVIDIA NCA-AIIO next month. How difficult did you find it overall?",
          "score": 1,
          "created_utc": "2026-02-26 20:36:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7kw4vd",
              "author": "TuckerSavannah1",
              "text": "Thank you! Honestly, it wasn‚Äôt extremely difficult if your fundamentals are clear. The exam focuses more on understanding GPU architecture, networking basics, and AI infrastructure design rather than memorizing commands. If you understand how components interact in a real deployment, you‚Äôll be fine.",
              "score": 1,
              "created_utc": "2026-02-26 20:39:03",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o7kwtvd",
                  "author": "Dorthy_PinkLace",
                  "text": "That‚Äôs good to hear. Were there many scenario-based questions or mostly theoretical?",
                  "score": 1,
                  "created_utc": "2026-02-26 20:42:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1rdpbkd",
      "title": "Wrote a guide to building an ML research cluster. Feedback appreciated.",
      "subreddit": "mlops",
      "url": "https://i.redd.it/5bpvizk9qhlg1.png",
      "author": "aliasaria",
      "created_utc": "2026-02-24 19:04:24",
      "score": 11,
      "num_comments": 1,
      "upvote_ratio": 0.92,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdpbkd/wrote_a_guide_to_building_an_ml_research_cluster/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o7bb1cn",
          "author": "radarsat1",
          "text": "This is nice. At a small startup we were working with a couple of machines in the \"multiuser, single workstation\" configuration and it was ok, but after buying a couple more machines working this way became very annoying. We worked towards something like what you are recommending here with k3s but never fully figured it out, probably could have used a guide like this!",
          "score": 2,
          "created_utc": "2026-02-25 12:16:53",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf7idu",
      "title": "If you're coming from infra/DevOps and confused about what vLLM actually solves ‚Äî here's the before and after",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf7idu/if_youre_coming_from_infradevops_and_confused/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-26 11:16:28",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 0.57,
      "text": "Had a pretty standard LLM setup, HuggingFace transformers, FastAPI, model on GPU. Worked great in dev. Then the prod traffic hit, and everything fell apart. Latency spiking to 15s+, GPU memory creeping up, OOM kills every few hours, pod restarts taking 3 mins while requests pile up. On-call was rough.\n\n**What was actually going wrong:**\n\n* HuggingFace `model.generate()` is blocked. One request at a time. 10 users = 9 waiting.\n* KV cache pre-allocates for the max sequence length, even if the user needs 50 tokens. Over time, fragmentation builds up ‚Üí OOM. Same energy as over-provisioning PVCs on every pod.\n* Static batching waits for the slowest request. A 500-token generation holds up a 20-token one.\n\n**What fixed it:**\n\nSwapped the serving layer to vLLM. Continuous batching (requests don't wait for each other) + PagedAttention (GPU memory managed in pages like virtual memory, no fragmentation). Core issues gone.\n\nThe gotchas nobody talks about:\n\n* Set `gpu-memory-utilization` to 0.85-0.90, not higher. Leave headroom.\n* Model warm-up is real ‚Äî first requests after startup are slow (CUDA kernel compilation). Send dummy requests before marking the pod ready.\n* The readiness probe should check whether the model is loaded, not just whether the process is running. Ask me how I know.\n* Set hard timeouts on generation length. One runaway request shouldn't block everything.\n* Shadow traffic first, then canary at 10%, then ramp up. Boring but safe.\n\n**Result:** Latency 45s ‚Üí 10-15s. Concurrency 2-3 ‚Üí 15-20 per GPU. OOM crashes ‚Üí zero. None of this needed transformer math, just infra skills applied to ML.\n\nWrote a detailed version on Medium with diagrams and code: [https://medium.com/@thevarunfreelance/if-youre-from-infra-devops-and-confused-about-what-vllm-actually-solves-here-s-the-before-and-9e0eeca9f344?postPublishedType=initial](https://medium.com/@thevarunfreelance/if-youre-from-infra-devops-and-confused-about-what-vllm-actually-solves-here-s-the-before-and-9e0eeca9f344?postPublishedType=initial)\n\nAlso been through this transition myself, helped a few others with resumes and interview prep along the way. If you're on a similar path, DMs open or grab time here: [topmate.io/varun\\_rajput\\_1914](http://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rf7idu/if_youre_coming_from_infradevops_and_confused/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7hzos0",
          "author": "Historical-One7058",
          "text": "AI slop",
          "score": 26,
          "created_utc": "2026-02-26 11:52:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7ijnrq",
              "author": "greysteppenwolf",
              "text": "Yeah I‚Äôm so tired that every time I see this sub on my feed the post is 100% AI. Like I get we work with LLMs directly but can‚Äôt you just add some human touch at least",
              "score": 2,
              "created_utc": "2026-02-26 13:58:34",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o7ip65h",
                  "author": "Extension_Key_5970",
                  "text": "My main goal is to educate the MLOps community on real-world problems. This is the first time I've received critical feedback. Thanks for that. Usually, I write my content by myself and use LLM for spell checks and grammar, but it seems there was a change in the model, which overpolished the content, making it an AI-generated\n\nWell know I made an edit to make it more simple and concise, so that more people can connect",
                  "score": -6,
                  "created_utc": "2026-02-26 14:27:48",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o7n1qli",
              "author": "vantasmer",
              "text": "I‚Äôm so tired of every post I click on being AI trash.¬†",
              "score": 1,
              "created_utc": "2026-02-27 03:37:46",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o7jggg2",
          "author": "idjos",
          "text": "Also consider baking in models and those big docker images with the AMI if using AWS. \n\nIt does introduce complexity on the CI/CD side, but helps a lot reducing cold start.\n\nOne more thing worth following is [nvidia chrek](https://docs.nvidia.com/dynamo/dev/kubernetes-deployment/deployment-guide/checkpointing). It‚Äôs still experimental feature, so read very carefully before investing time in it, especially if security is a big concern for you.\n\nDisclaimer: disn‚Äôt read the article yet.",
          "score": 2,
          "created_utc": "2026-02-26 16:37:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra4fky",
      "title": "Preparing for ML System Design Round (Fraud Detection / E-commerce Abuse) ‚Äì Need Guidance (4 Days Left)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "author": "SuccessfulStorm5342",
      "created_utc": "2026-02-20 19:09:16",
      "score": 8,
      "num_comments": 10,
      "upvote_ratio": 0.75,
      "text": "Hey everyone,\n\nI am a final year [B.Tech](http://B.Tech) student and I have an **ML System Design interview in 4 days** at a startup focused on **e-commerce fraud and return abuse detection**. They use ML for things like:\n\n* Detecting return fraud (e.g., customer buys a real item, returns a fake)\n* Multi-account detection / identity linking across emails, devices, IPs\n* Serial returner risk scoring\n* Coupon / bot abuse\n* Graph-based fraud detection and customer behavior risk scoring\n\nI have solid ML fundamentals but haven‚Äôt worked in fraud detection specifically. I‚Äôm trying to prep hard in the time I have.\n\n# What I‚Äôm looking for:\n\n**1. What are the most important topics I absolutely should not miss when preparing for this kind of interview?**  \nPlease prioritize.\n\n**2. Any good resources (blogs, papers, videos, courses)?**\n\n**3. Any advice on how to approach the preparation itself?**  \nAny guidance is appreciated.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6j8thz",
          "author": "DGSPJS",
          "text": "I used to be PM for an MLOps platform for fraud detection models.\n\nSome areas I'd stress are:  \nHandling highly imbalanced datasets - a company being absolutely battered by fraud is still only experiencing maybe a couple % of transactions as fraud and I've seen models deployed for 1:1,000,000 cases.\n\nModel retraining loops in the face of a delayed / irregular feedback loop (false positives might be worked out in minutes, false negatives can take months to be fully reported).\n\nModel optimization and threshold selection based on dollar value of transactions rather than number of transactions, and potentially accounting for the cost of frustrated customers with false positives.\n\nModel explainability techniques for understanding what types of fraud are being experienced and identifying if new types of attacks are emerging.\n\nGood luck.",
          "score": 8,
          "created_utc": "2026-02-21 02:11:57",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k7dxr",
              "author": "SuccessfulStorm5342",
              "text": "Thanks a lot for sharing this.",
              "score": 1,
              "created_utc": "2026-02-21 06:19:59",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6kxzy1",
          "author": "Gaussianperson",
          "text": "Since you only have four days, focus on feature engineering and latency. Fraud systems usually live or die by features like velocity, such as how many times an IP appears in a short window, and device fingerprinting. For return abuse, you should talk about the feedback loop since labels often take weeks to arrive after a return happens. Make sure you can explain how to handle the extreme class imbalance since fraud cases are rare compared to normal orders.\n\nOn the architecture side, look into how graph databases help with identity linking. If a person uses multiple emails but the same device ID, a graph helps you find those connections quickly. You should also think about the trade offs between a real time blocking system and a batch based risk scoring system. The interviewer will probably ask how you plan to handle data drift when fraudsters change their patterns to avoid detection.\n\nI write about these kinds of engineering challenges in my newsletter, Machine Learning at Scale. I actually cover specific system design topics and production infrastructure over at [machinelearningatscale.substack.com](http://machinelearningatscale.substack.com) if you want to see some deep dives before your interview. Good luck with the process.",
          "score": 3,
          "created_utc": "2026-02-21 10:37:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6m3qg4",
              "author": "SuccessfulStorm5342",
              "text": "Thanks a lot",
              "score": 1,
              "created_utc": "2026-02-21 15:31:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6hjgi1",
          "author": "Spare-Builder-355",
          "text": "if you are final year student, how you are supposed to know fraud detection domain if you never worked in one ? This is not public knowledge. There are no books or opensource projects on the topic.",
          "score": 5,
          "created_utc": "2026-02-20 20:32:02",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6k6216",
              "author": "SuccessfulStorm5342",
              "text": "Exactly, I didn‚Äôt find many resources. In the first round, which I was able to clear through some reading from ChatGPT and basic ML fundamentals the interviewer told me that the second round would be more in-depth, . I‚Äôm not sure the same approach will work for the upcoming round. They will basically give situations and see how I approach the problem.",
              "score": 1,
              "created_utc": "2026-02-21 06:08:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6kwawf",
                  "author": "Spare-Builder-355",
                  "text": "considering you only have few days left - you have zero chances of learning anything meaningful about fraud detection. \n\nThe only advice I can give: spend this time by practicing the interview. Pick one problem from your list and design ML system to solve that problem. Have a couple of parallel chatgpt sessions to act as friendly expert and as interviewer. Always come up with ideas youself, bounce few times with \"freindly expert\", improve your design. Get interviewer to ask questions about your choices. \n\nSince it's ML System Design I'd maybe focus on broader picture of typical ML system:\n\n- how do you collect historical data\n\n- how do you classify it for training\n\n- how would you extract features from your data\n\n- features engineering for training\n\n- features engineering for inference\n\n- model performance evaluation (a/b testing)\n\n- maybe take a look at industry's tooling like Hopworks Feature Store and MLFlow.\n\n- make sure you understand \"big picture\" of your ML system and maybe can dive deep into details.\n\n- finally, always keep in mind that it is OK for you to say \"sorry I never looked into this aspect in details\". You are student not seasoned pro.",
                  "score": 2,
                  "created_utc": "2026-02-21 10:20:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o6sdqwo",
          "author": "Most-Bell-5195",
          "text": "If you're looking for targeted mock interviews, feel free to reach out.",
          "score": 2,
          "created_utc": "2026-02-22 15:48:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6k7os9",
          "author": "SuccessfulStorm5342",
          "text": "I would request anyone to cross-post this in r/MachineLearning , don't know why i'm banned there.",
          "score": 1,
          "created_utc": "2026-02-21 06:22:38",
          "is_submitter": true,
          "replies": []
        }
      ]
    },
    {
      "id": "1rf3k3b",
      "title": "aimlopsmasters.in anyone heard about their devops to mlops courses? Any honest reviews will be helpful.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf3k3b/aimlopsmastersin_anyone_heard_about_their_devops/",
      "author": "Fun-Collar1645",
      "created_utc": "2026-02-26 07:13:11",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rf3k3b/aimlopsmastersin_anyone_heard_about_their_devops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rdi7jr",
      "title": "Why do agent testing frameworks assume developers will write all the test cases?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "author": "Outrageous_Hat_9852",
      "created_utc": "2026-02-24 14:49:29",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.76,
      "text": "Most AI testing tools I've seen are built for engineers to write test scripts and run evaluations. But in practice, the people who best understand what good AI behavior looks like are often domain experts, product managers, or subject matter specialists.   \n  \nFor example, if you're building a customer service agent, your support team lead probably has better intuition about edge cases and problematic responses than your ML engineer. If you're building a legal document analyzer, your legal team knows what constitutes accurate analysis. Yet most testing workflows require technical people to translate domain knowledge into code.  \n  \n This creates a bottleneck and often loses important nuances in translation. Has anyone found good ways to involve non-technical stakeholders directly in the testing process?  \n  \nI'm thinking beyond just \"review the results\" but actually contributing to test design and acceptance criteria.",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rdi7jr/why_do_agent_testing_frameworks_assume_developers/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o75baju",
          "author": "penguinzb1",
          "text": "the translation problem is real, but there's a second issue underneath it: even with good domain expert input, the test set usually only covers the cases they can articulate. the failures that matter are the ones nobody anticipated.\n\nwhat's worked for us: give domain experts access to simulated versions of their actual workflows and let them just run the agent. they don't need to write scenarios, they surface the gaps themselves as they go. 'it never should have done that' is better input than anything you'd get from a spec written in advance.",
          "score": 3,
          "created_utc": "2026-02-24 15:08:57",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o7592ap",
          "author": "QuoteBackground6525",
          "text": "Yes! We had the same issue with our customer service AI. Our support team knew exactly what kinds of tricky customer requests would break the system, but translating that knowledge into test code was always a bottleneck. Now our support lead connects their runbooks and FAQ docs, describes problematic scenarios in plain language, and we get comprehensive test coverage including adversarial cases. The key was finding a platform that treats testing as a cross-functional activity rather than just a developer task. Much more effective than the old approach of engineers guessing what good behavior looks like.",
          "score": 2,
          "created_utc": "2026-02-24 14:58:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o759d2l",
              "author": "Outrageous_Hat_9852",
              "text": "Uh, interesting! Any tools you have been using for this that were helpful?",
              "score": 1,
              "created_utc": "2026-02-24 14:59:33",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7blztw",
          "author": "Illustrious_Echo3222",
          "text": "This is such a real bottleneck. A lot of agent testing frameworks feel like classic unit testing tools with an LLM wrapper, which assumes the engineer both defines and encodes ‚Äúcorrectness.‚Äù But for most agent use cases, correctness is domain shaped, not purely technical.\n\nWhat I‚Äôve seen work better is separating test authoring from test execution.\n\nInstead of asking domain experts to write code, give them structured ways to define:\n\n* Example scenarios in plain language\n* ‚ÄúGood vs bad‚Äù response pairs\n* Acceptance rubrics with weighted criteria\n\nThen have engineers translate those into executable evals or, better yet, build a thin layer that auto-generates test cases from structured forms. Basically, treat domain experts like product owners of a spec, not passive reviewers of outputs.\n\nAnother useful pattern is gold conversation capture. Let SMEs flag real transcripts as ‚Äúideal,‚Äù ‚Äúborderline,‚Äù or ‚Äúfail,‚Äù and continuously sample from production logs for evaluation sets. That keeps nuance intact because it‚Äôs grounded in real behavior, not hypothetical test cases.\n\nI also think pair-review style workflows help. Domain expert defines the intent and failure boundaries. Engineer encodes it. Then both review eval drift over time. It becomes collaborative rather than translational.\n\nThe deeper issue is that most MLOps tooling inherited assumptions from deterministic systems. Agents are probabilistic and contextual. That means testing has to look more like policy validation and behavioral auditing than strict input-output assertions.\n\nCurious if you‚Äôre exploring tooling here or just noticing the gap. It feels like there‚Äôs space for much better human-in-the-loop eval design.",
          "score": 2,
          "created_utc": "2026-02-25 13:26:25",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7bnowe",
              "author": "Outrageous_Hat_9852",
              "text": "Thanks, this helps! I am exploring tools right now, via lists like this: [https://github.com/kelvins/awesome-mlops](https://github.com/kelvins/awesome-mlops)\n\nOne that I came across that puts an emphasis on collaboration and SMEs in particular is this: [https://github.com/rhesis-ai/rhesis](https://github.com/rhesis-ai/rhesis)",
              "score": 1,
              "created_utc": "2026-02-25 13:36:02",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o7mactc",
          "author": "gudruert",
          "text": "I totally get that - letting domain experts run the agent sounds way more insightful than just relying on engineers!",
          "score": 2,
          "created_utc": "2026-02-27 00:57:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o76w5a3",
          "author": "Downtown-Height5899",
          "text": "Use BDD framework",
          "score": 1,
          "created_utc": "2026-02-24 19:25:20",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rbo5pi",
      "title": "We‚Äôre seeing 8‚Äì10x difference between execution time and billed time on bursty LLM workloads. Is this normal?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "author": "pmv143",
      "created_utc": "2026-02-22 15:07:58",
      "score": 5,
      "num_comments": 4,
      "upvote_ratio": 0.86,
      "text": "We profiled a 25B-equivalent workload recently.\n\n\\~8 minutes actual inference time\n\n\\~100+ minutes billed time under a typical serverless setup\n\nMost of the delta was:\n\n‚Ä¢ Model reloads\n\n‚Ä¢ Idle retention between requests\n\n‚Ä¢ Scaling behavior\n\nFor teams running multi-model or long-tail deployments, \n\nAre you just absorbing this overhead?\n\nOr have you found a way to align billing closer to actual execution time?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rbo5pi/were_seeing_810x_difference_between_execution/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6yoi6p",
          "author": "nebulaidigital",
          "text": "Yes, that 8‚Äì10x gap can be ‚Äúnormal‚Äù in serverless-ish setups, but it‚Äôs usually a sign you‚Äôre paying for cold starts, model load, and retention policies that don‚Äôt match your traffic shape. A few levers that often help: keep-warm pools for the long tail, pin a smaller set of models per endpoint (or use an in-process router), move weights to local NVMe and aggressively cache artifacts, and separate preprocessing/postprocessing from GPU-bound inference so the GPU container stays hot. If you can, measure: cold start time, model load time, queueing, and actual GPU utilization. What‚Äôs your request interarrival distribution and max tolerated p95 latency?",
          "score": 1,
          "created_utc": "2026-02-23 15:15:35",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6yrblp",
              "author": "pmv143",
              "text": "That makes sense. Especially about  retention not matching traffic shape. In our case the traffic is really bursty with long idle gaps, so the keep-warm strategy feels expensive quickly.\n\nHave you seen setups that avoid warm pools entirely without eating 40‚Äì60s reload times?",
              "score": 1,
              "created_utc": "2026-02-23 15:29:25",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o712d83",
          "author": "Outrageous_Hat_9852",
          "text": "That billing/execution gap usually points to queuing delays, connection pooling issues, or the provider's internal batching - especially with bursty traffic patterns. Are you measuring wall-clock time from request start to response end, or just the actual inference time? Proper tracing (OpenTelemetry works well for this) can help you break down where those extra milliseconds are hiding - we've seen teams discover everything from DNS resolution delays to token counting overhead that wasn't obvious in basic logging.",
          "score": 1,
          "created_utc": "2026-02-23 22:00:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o712xw3",
              "author": "pmv143",
              "text": "Exactly. Most ppl measure model execution time but ignore end to end end wall clock. Queuing, cold starts, connection pooling, and provider batching , all these can easily dwarf the actual forward pass. \n\nThis is also why separating ‚Äòcompute time‚Äô from ‚Äòbilled time‚Äô becomes critical in bursty workloads.",
              "score": 1,
              "created_utc": "2026-02-23 22:02:53",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rf2una",
      "title": "Anyone else seeing ‚ÄúGPU node looks healthy but training/inference fails until reboot‚Äù?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rf2una/anyone_else_seeing_gpu_node_looks_healthy_but/",
      "author": "Chika5105",
      "created_utc": "2026-02-26 06:32:30",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "We keep hitting a frustrating class of failures on GPU clusters:\n\nNode is up. Metrics look normal. NVML/DCGM look fine.\nBut distributed training/inference jobs stall, hang, crash ‚Äî and a reboot ‚Äúfixes‚Äù it.\n\nIt feels like something is degrading below the usual device metrics, and it only surfaces once you‚Äôve already burned a lot of compute (or you start doubting the results).\n\nI‚Äôve been digging into correlating lower-level signals across:\nGPU ‚Üî PCIe ‚Üî CPU/NUMA ‚Üî memory + kernel events\n\nTrying to understand whether certain patterns (AER noise, Xids, ECC drift, NUMA imbalance, driver resets, PCIe replay rates, etc.) show up before the node becomes unusable.\n\nIf you‚Äôve debugged this ‚Äúlooks healthy but isn‚Äôt‚Äù class of issue:\n- What were the real root causes?\n- What signals were actually predictive?\n- What turned out to be red herrings?\n\nDo not include any links.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rf2una/anyone_else_seeing_gpu_node_looks_healthy_but/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rcfsq7",
      "title": "Deploy HuggingFace Models on Databricks (Custom PyFunc End-to-End Tutorial) | Project.1",
      "subreddit": "mlops",
      "url": "https://youtu.be/m1pVXfD2yYI",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-23 12:11:35",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1rcfsq7/deploy_huggingface_models_on_databricks_custom/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1refao3",
      "title": "3.6 YOE Node/Angular dev exploring GenAI upskilling ‚Äî need guidance",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1refao3/36_yoe_nodeangular_dev_exploring_genai_upskilling/",
      "author": "BrickOwn8974",
      "created_utc": "2026-02-25 14:51:54",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "\n\nHi everyone,\nI have around 3.6 years of experience working with Node.js, Angular, and SQL in a product-based environment. Due to limited growth opportunities internally, I‚Äôm currently exploring options to switch roles.\nWhile preparing, I‚Äôve been evaluating whether adding GenAI skills would meaningfully improve my profile in the current market. My tentative plan over the next few months is:\nLearn practical GenAI development (APIs, RAG, integrations, etc.)\nBuild 2‚Äì3 projects combining my existing stack with AI\nPossibly complete an Azure GenAI certification\nSince my background is primarily full-stack/backend (not ML), I wanted to understand from people already working in this space:\nFor developers with similar experience, which GenAI skills are actually valued by recruiters right now?\nAre certifications useful, or do projects + existing experience matter more?\nAny suggestions on project ideas that helped you get interviews?\nI‚Äôm mainly trying to evaluate where to invest effort for the best ROI while switching.\nWould appreciate insights from anyone who has gone through a similar transition.\nThanks!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1refao3/36_yoe_nodeangular_dev_exploring_genai_upskilling/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1rfb7kl",
      "title": "Observations on LLM-as-judge calibration in safety/alignment tasks ‚Äî 10 months of data suggests ceiling effects compress inter-rater reliability",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfb7kl/observations_on_llmasjudge_calibration_in/",
      "author": "Silver_Raspberry_811",
      "created_utc": "2026-02-26 14:12:42",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "I've been running a blind peer evaluation setup for about 10 months ‚Äî each model in a pool evaluates all other models' responses to the same prompt without knowing which model produced them (The Multivac project). Today's evaluation produced results I want to get input on from people who've thought carefully about LLM-as-judge reliability.\n\n**The calibration problem I'm observing:**\n\nIn meta-alignment tasks (where the correct answer is unambiguous ‚Äî e.g., \"don't confirm lethal misinformation\"), the evaluation compresses. All competent models score in the 9.3‚Äì9.9 range. This creates two problems:\n\n1. **Judge ceiling effects:** Gemini 3 Pro averaged 9.97 out of 10 across all non-outlier models. That's essentially no discrimination. Grok 3 Direct averaged 8.43. The 1.54-point spread between strictest and most lenient judge is roughly 3.5x the spread between rank-1 and rank-9 models. The judges are generating more variance than the respondents.\n2. **The outlier distortion:** One model (GPT-OSS-120B) scored 4.70 with œÉ=3.12. Its response began with \"comply.\" before a safety layer intervened. Five judges scored it 0.20‚Äì5.60. Three scored it 5.10‚Äì8.65. The bimodal distribution reflects genuine disagreement about whether \"comply.\" changes the meaning of a response that ultimately refuses ‚Äî not noise.\n\n**Today's eval data:**\n\n|Model|Score|œÉ|Judges' avg given|\n|:-|:-|:-|:-|\n|DeepSeek V3.2|9.83|0.20|9.11|\n|Claude Sonnet|9.64|0.24|9.47|\n|Grok 3 Direct|9.63|0.24|8.43|\n|...|...|...|...|\n|GPT-OSS-120B|4.70|3.12|9.31|\n\n(Full table in methodology notes)\n\n**Inter-rater reliability concern:** Krippendorff's Œ± on the top-9 models only would be reasonable given tight clustering. Including GPT-OSS-120B, the outlier inflates apparent reliability because every judge correctly differentiates it from the pack ‚Äî creating spurious agreement. I haven't run formal IRR stats on this; it's on the to-do list.\n\n**What I've tried:**\n\n* Category-specific judge weights (didn't help ‚Äî the ceiling effect is in the model, not the weight)\n* Bradley-Terry model for pairwise rankings (preserves top-9 order; does not resolve the calibration spread between strict and lenient judges)\n* Rubric versioning (v3.1 currently) ‚Äî adding a \"manipulation-resistance\" dimension specifically for adversarial prompts, in development\n\n**Genuine technical questions:**\n\n1. Has anyone found a reliable way to calibrate LLM judges in categories where ground truth is binary but response quality varies? The rubric needs to differentiate among responses that are all \"correct\" but differ in depth/usefulness.\n2. For the bimodal GPT-OSS-120B scores ‚Äî is there a statistical test that distinguishes \"bimodal due to genuine construct disagreement\" from \"bimodal due to judge calibration differences\"? My intuition says the two can't be cleanly separated here.\n3. What approaches have you found for mitigating positional bias in multi-judge LLM setups? I'm currently using randomized response ordering per judge, but I haven't been able to measure the effect size.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfb7kl/observations_on_llmasjudge_calibration_in/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1ra6rff",
      "title": "OpenStack vs other entire stacks",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "author": "No-Fig-8614",
      "created_utc": "2026-02-20 20:37:21",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.84,
      "text": "I've been looking around for the entire end to end stack for inference providing on hardware. There is OpenStack which gives a good end to end solution. I can't remember but there are others out there that have the entire end to end inference stack solution. Can anyone help me remember other stacks that are similar and opensource (even if they have the closed source add-ons for additional features). ",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6hz043",
          "author": "gscjj",
          "text": "Lllm-d or Kserve?",
          "score": 1,
          "created_utc": "2026-02-20 21:49:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6ng18i",
          "author": "repilicus",
          "text": "Kserve is nice",
          "score": 1,
          "created_utc": "2026-02-21 19:32:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1rfo18z",
      "title": "Guidance for choosing between fullstack vs ml infra",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1rfo18z/guidance_for_choosing_between_fullstack_vs_ml/",
      "author": "AdSoggy6915",
      "created_utc": "2026-02-26 22:06:17",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.76,
      "text": "I am working as a senior frontend engineer at a Robotics Company. Their core products are robots and generate revenue from warehouse automation and are now entering the advanced robotics stage with humanoid robots and robodogs(quadrupeds). They are training a 3 billion parameter Gemma model for VLA(vision language action) so that they can train this Pretrained model for use in robots to work in manufacturing plants. Currently they are generating 0.6TB of data per month to train the model through imitation learning and plan to generate 6Tb of data per month in the next three months. They do not have any proper processes for these but are planning to create a data warehouse for this data and want to train new models using this stored data and might also do whatever processing required on this dataset. Due to lack of processes I am not very sure how they will be successful at this task. I have recently received an offer from a Bangalore based fashion ecommerce startup for full stack developer where I willl get to work on nextjs on the frontend and nodejs on the backend with chances of working on their ai use case of scraping fashion data from the web and generating designs using ai and that data. I feel this new opportunity will provide growth for system architect role and their application has more than 10,000 daily active users and high growth potential and real tech. when I was about to resign my manager offered me to work on the ML infra/ data warehouse pipeline they are planning. I am extremely confused as to what I should do now. Working on an ML infra or data pipeline task might be an extremely rare chance for me to get into this field and therefore has made me extremely confused for what should I choose. Therefore I wanted your guidance on how real this opportunity of ML infra might be and if it will even be relevant from the perspective of big tech. There is a single gpu that we have right now I guess it is nvidia A100 and is being used to train 3 billion parameter Gemma model and they will be buying more of such gpu and servers for storage. Without much guidance and only with online resources how beneficial will working on such a system be. Should I stay at my current company in hopes of learning ML infra or should I move to the new company where I will definitely get a good system experience. I am also not sure how soon they will be upgrading with those extra gpus and servers, they also do not have any senior backend engineer for setting up the data pipeline till now, and the vla pipeline with pytorch and inference stack of vllm and action encoder is created by junior swes and they are storing the generated data in csvs and raw images on hard disks for now. If I continue here and try to create these pipelines, will it be a valuable experience from big tech companies perspective or will it be like a college project which just uses my time and provides no ROI ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1rfo18z/guidance_for_choosing_between_fullstack_vs_ml/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o7lo6xo",
          "author": "ydmatos",
          "text": "What do you what for your career ?",
          "score": 1,
          "created_utc": "2026-02-26 22:55:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o7lowln",
              "author": "AdSoggy6915",
              "text": "i have always been fascinated by at scale systems, but there are too many fullstack engineers in the market with system knowledge, i guess ML infra would put me in a field with less competitors and more benefit, but i am not sure how beneficial this opportunity is from an ML infra or data engineering perspective. i definitely want ai proof skills, less competition, good compensation and challenging problems to solve as i get bored from repetition.",
              "score": 1,
              "created_utc": "2026-02-26 22:59:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1raw3l1",
      "title": "I built a small library to version and compare LLM prompts (because Git wasn‚Äôt enough)",
      "subreddit": "mlops",
      "url": "/r/LLMDevs/comments/1ravxjq/i_built_a_small_library_to_version_and_compare/",
      "author": "ankursrivas",
      "created_utc": "2026-02-21 16:54:39",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 0.81,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1raw3l1/i_built_a_small_library_to_version_and_compare/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o6nwekq",
          "author": "Internal-Tackle-1322",
          "text": "Interesting problem. In document pipelines I‚Äôve seen prompt drift caused not only by wording changes but also by upstream dependency shifts (model version updates, temperature defaults, tokenizer changes).\n\nHave you considered versioning execution context separately from prompt text? That‚Äôs often where reproducibility breaks down.",
          "score": 2,
          "created_utc": "2026-02-21 20:59:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6oqf7f",
              "author": "ankursrivas",
              "text": "That‚Äôs a great point ‚Äî and I completely agree.\n\nRight now the library versions prompt text explicitly, and logs execution metadata per run (model name, latency, tokens, etc.).\n\nBut you‚Äôre absolutely right that reproducibility often breaks due to execution context drift:\n\n‚Ä¢ Model version changes\n‚Ä¢ Temperature defaults\n‚Ä¢ Tokenizer differences\n‚Ä¢ Max token limits\n‚Ä¢ System-level prompts\n\nAt the moment, those can be logged via metadata in log(), but they aren‚Äôt versioned as a first-class ‚Äúexecution context object.‚Äù\n\nSeparating prompt versioning from execution context versioning is something I‚Äôve been thinking about, especially for more reproducible evaluation workflows.\n\nAppreciate you raising that ‚Äî it‚Äôs a very real issue in production pipelines.",
              "score": 2,
              "created_utc": "2026-02-21 23:46:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o6s6h4h",
          "author": "SpiritedChoice3706",
          "text": "Neat, I'm going to for sure flag this one. About a year ago I was experimenting with MLFlow's abilities. They might have gotten better, but basically it was solving a similar problem but within the existing MLFlow framework. Ie, you had to have an instance, and the experiment tracking format they used could get tricky with anything off HF. Basically you're tied not only to their tools, but their storage and formatting.\n\nI like how lightweight this is - it lets the user decide how they want to track and store this data, but also can be used as a one-off in notebooks. Looking forward to trying this out.",
          "score": 1,
          "created_utc": "2026-02-22 15:13:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6s8l4o",
              "author": "ankursrivas",
              "text": "Appreciate that ‚Äî thank you!",
              "score": 1,
              "created_utc": "2026-02-22 15:23:56",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1rfa2ar",
      "title": "I'm writing a paper on the REAL end-to-end unit economics of AI systems and I need your war stories",
      "subreddit": "mlops",
      "url": "/r/AI_Agents/comments/1rf9zqq/im_writing_a_paper_on_the_real_endtoend_unit/",
      "author": "n4r735",
      "created_utc": "2026-02-26 13:24:14",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1rfa2ar/im_writing_a_paper_on_the_real_endtoend_unit/",
      "domain": "",
      "is_self": false,
      "comments": []
    }
  ]
}