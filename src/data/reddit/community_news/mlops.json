{
  "metadata": {
    "last_updated": "2026-01-26 02:37:15",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 12,
    "total_comments": 38,
    "file_size_bytes": 54655
  },
  "items": [
    {
      "id": "1qiqcl6",
      "title": "Coming from DevOps/Infra to MLOps? Here's what I learned after several interviews at product companies",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-21 06:22:21",
      "score": 80,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "I've been interviewing for MLOps and ML Platform Engineer roles over the past few months, and I wanted to share some observations that might help others make a similar transition.\n\n**The Interview Gap**\n\nMost interviewers I've faced come from research or pure ML engineering backgrounds. They think in terms of model architectures, feature engineering, and training pipelines. If you're coming from a pure infrastructure or DevOps background like me, there's often a disconnect.\n\nYou talk about Kubernetes orchestration, GPU cluster management, and cost optimisation. They ask about data drift, model retraining strategies, or how you'd debug a model's performance degradation. The conversation doesn't flow naturally because you're speaking different languages.\n\n**What Actually Helped**\n\nI realised I needed to invest time in **ML fundamentals** ‚Äì not to become a data scientist, but to bridge the communication gap. Understanding basic statistics, how different model types work, and what \"overfitting\" or \"data leakage\" actually mean made a huge difference.\n\nWhen I could frame infrastructure decisions in ML terms (\"this architecture reduces model serving latency by X%\" vs \"this setup has better resource utilisation\"), interviews went much more smoothly.\n\n**Be Strategic About Target Companies**\n\nNot all MLOps roles are the same. If you're targeting companies heavily invested in **real-time inferencing** (think fraud detection, recommendation engines, autonomous systems), the focus shifts to:\n\n* Data distribution and streaming pipelines\n* Low-latency prediction infrastructure\n* Real-time monitoring and anomaly detection\n* Data engineering skills\n\nIf they're doing **batch processing and research-heavy ML**, it's more about:\n\n* Experiment tracking and reproducibility\n* Training infrastructure and GPU optimization\n* Model versioning and registry management\n\nMatch your preparation to what they actually care about. Don't spray-and-pray applications.\n\n**MLOps Roles Vary Wildly**\n\nHere's something that actually helped my perspective: **MLOps means different things at different companies**.\n\nI've had interviews where the focus was 90% infrastructure (Kubernetes, CI/CD, monitoring). Others were 70% ML-focused (understanding model drift, feature stores, retraining strategies). Some wanted a hybrid who could do both.\n\nThis isn't because teams don't know what they want. It's because MLOps is genuinely different depending on:\n\n* Company maturity (startup vs established)\n* ML use cases (batch vs real-time)\n* Team structure (centralised platform vs embedded engineers)\n\nIf an interview feels misaligned, it's often a mismatch in role expectations, not a reflection of your skills. The \"MLOps Engineer\" title can mean vastly different things across companies.\n\n**Practical Tips**\n\n* Learn the basics: bias-variance tradeoff, cross-validation, common model types\n* Understand the ML lifecycle beyond just deployment\n* Be able to discuss model monitoring (not just infra monitoring)\n* Know the tools: MLflow, Kubeflow, Ray, etc. ‚Äì but more importantly, know *why* they exist\n* Read ML papers occasionally ‚Äì even if you don't implement them, you'll understand what your ML colleagues are dealing with\n\n**Final Thought**\n\nThe transition from DevOps to MLOps isn't just about learning new tools. It's about understanding a new domain and the people working in it. Meet them halfway, and you'll find the conversations get a lot easier.\n\nKeep learning, keep iterating.\n\nIf anyone's going through a similar transition and wants to chat, feel free to DM or connect here: [https://topmate.io/varun\\_rajput\\_1914/](https://topmate.io/varun_rajput_1914/)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0tlygd",
          "author": "____Kitsune",
          "text": "How often was kubernetes a hard req vs EKS or other managed services?",
          "score": 4,
          "created_utc": "2026-01-21 08:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tt6cn",
              "author": "Extension_Key_5970",
              "text": "For MLOPs, I have not faced a deep dive wrt core on-prem K8, nowadays it's all EKS managed, of course, one should be good enough with the K8 ecosystem, as ultimately models and apps are deployed on K8, so one needs debugging and troubleshooting skills with respect to it.",
              "score": 3,
              "created_utc": "2026-01-21 09:34:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tok4t",
          "author": "mwon",
          "text": "Very nice summary. What you think about the role AI Scientists that in many cases requires knowledge in all those skills?",
          "score": 3,
          "created_utc": "2026-01-21 08:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tswyg",
              "author": "Extension_Key_5970",
              "text": "Scientists focus on research; the skills I mentioned are engineering skills. I've seen companies expect research expertise from engineers and vice versa. Some overlap is fine, but fully merging these roles isn't ideal in the long term.\n\nThe engineering skills are accessible to anyone from a software background moving into ML ‚Äì even ML scientists can pick them up if transitioning from research to engineering.\n\nBe intentional about your path rather than being pushed into a hybrid role that doesn't align with your strengths.",
              "score": 2,
              "created_utc": "2026-01-21 09:32:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0uqa",
          "author": "eemamedo",
          "text": "This is probably one of the best posts I have seen here for a while. I am going exactly the same. Some companies hire me for ML Infra but want me to live code LLM based application. It‚Äôs very rare that companies actually know what MLOps or ML Infra is supposed to do/be.¬†",
          "score": 2,
          "created_utc": "2026-01-21 14:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tc3eo",
          "author": "EviliestBuckle",
          "text": "Which ml course did you take?",
          "score": 1,
          "created_utc": "2026-01-21 06:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tlj5t",
              "author": "Extension_Key_5970",
              "text": "For specific ML knowledge, actually i havent follow any one course, instead I went for Top to bottom approach, I bought an practise exam for AWS ML Speciality, as it covers all ML foundations topics I suppose, went through exam scenarios one by one, learn from the answers and wrong choices, view YT videos - statsquest is awesome, if you want to dig in any of ml topics, explained very well by statsquest, these will create a strong base for ML",
              "score": 5,
              "created_utc": "2026-01-21 08:20:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tmuef",
                  "author": "EviliestBuckle",
                  "text": "So in your experience how similar is llmops to mlops?",
                  "score": 1,
                  "created_utc": "2026-01-21 08:33:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tv57p",
          "author": "Competitive-Fact-313",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-21 09:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zmfj1",
          "author": "Friendly_Willow_8447",
          "text": "That‚Äôs really good details\nThanks for sharing",
          "score": 1,
          "created_utc": "2026-01-22 04:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10f2ve",
          "author": "tortuga_me",
          "text": "Thats kind of true. Different companies view mlops tasks as something no other person can do. Part of the job is to advise management what is missing in their current setup and by default this means interviewers wont have that much insight into what they are looking for.",
          "score": 1,
          "created_utc": "2026-01-22 07:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qlgyco",
      "title": "DevOps ‚Üí MLOps Interview Lesson: They don't care about your infra skills until you show you understand their pain",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qlgyco/devops_mlops_interview_lesson_they_dont_care/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-24 07:46:18",
      "score": 52,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "text": "Had an interview recently that exposed a blind spot I didn't know I had.\n\nBackground: 11+ years in DevOps, extensive experience with Kubernetes, cloud infra, CI/CD. Transitioned into MLOps over the past few years.\n\nThe hiring manager asked: \"How would you help build a platform for our data science and research teams?\"\n\nMy brain immediately jumped to: Kubernetes, model serving, MLflow, autoscaling, GPU scheduling...\n\nBut that's not what they were asking. They wanted to know whether I understood the *problems* DS teams actually face day to day.\n\nI stumbled. Not because I don't know the tech, but because I framed everything around my expertise instead of their pain points.\n\nIt made me realise something (probably obvious to many of you, but it was a gap for me):\n\nIn DevOps, the customer is fairly clear‚Äîdevelopers want to ship faster, ops wants reliability. In MLOps, you're serving researchers and data scientists with very different workflows and frustrations.\n\nThe infra knowledge is table stakes. The harder part is understanding things like:\n\nWhy does a 3-hour training job failing on a dependency error feel so demoralising?\n\nWhy do they keep asking for \"just one more GPU\"?\n\nWhy does reproducibility matter *to them*, not just to the platform team?\n\nStill working on building this muscle. Curious if others who've made the DevOps ‚Üí MLOps shift have run into something similar?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qlgyco/devops_mlops_interview_lesson_they_dont_care/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1eqv2u",
          "author": "extreme4all",
          "text": "Not just devops, i'm more a security person and its the same instinctively most engineers think of the technical stuf and not about the problem. The otherday an architect even came to me with a tool & technology he thinks is great and we should use but than i asked him what problem and whose problem it would solve he couldn't answer. Yes the tech looked cool, but it didn't seem to address a (burning) problem my team or any other team in the org has.\n\nIt burns down to the same thing you explained, we need more people that can talk to the users of the services we provide, understand their problem ans engineer solutions for those instead of engineering solutions for problems that don't matter.",
          "score": 8,
          "created_utc": "2026-01-24 11:22:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1frdv4",
          "author": "NotSoGenius00",
          "text": "TBH what you said is 100% true and infra knowledge is okay but if MLOps is doing everything why is there a need for DevOps ? I think MLops is more nuanced that tools/methods etc. each ds/research workflow is different across all orgs and most orgs dont know what they want. \nIMHO devops is least thing I am worried about, the most important thing to be worried about during an interview is dev ex/velocity for their teams ! And that is gold if you understand",
          "score": 2,
          "created_utc": "2026-01-24 15:19:00",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1kwdef",
          "author": "No_Refrigerator6755",
          "text": "is it good time for a 2026 grad to start learning mlops , already into devops",
          "score": 2,
          "created_utc": "2026-01-25 07:25:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1j69vo",
          "author": "Hyperventilater",
          "text": "Look into the modern practices of \"Enterprise Architecture\". That field deals with this quite often in a more general way.\n\nThe crux of it is: if nobody is asking for it, then it doesn't provide value. Your experience doesn't mean anything if you can't relate it to the particular person's problems they're trying to solve by the position they're filling. Those problems are ALWAYS defined by the stakeholders.",
          "score": 1,
          "created_utc": "2026-01-25 00:55:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qmj990",
      "title": "[Passed] NVIDIA Agentic AI Certification (NCP-AAI)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/",
      "author": "Ranger_1928",
      "created_utc": "2026-01-25 13:34:42",
      "score": 17,
      "num_comments": 20,
      "upvote_ratio": 0.95,
      "text": "Just wanted to share a data point for anyone eyeing the new NVIDIA Agentic AI certification. I sat for the exam this today and passed! üöÄ  \nI already had experience building agents with LangChain/OpenAI, but I quickly realized this exam requires a mindset shift. It‚Äôs less about generic Python loops and more about the \"NVIDIA Way\" (NIMs, Triton, NeMo).\n\n**The Results (The Good & The Ugly):**  \nI wanted to be transparent about the score breakdown because it tells a story:\n\n* **Platform Implementation:**¬†85% \n* **Deployment & Scaling:**¬†79%\n* **Safety, Ethics & Compliance:**¬†...35% üòÖ\n\n**My Takeaway:**  \nIf you are preparing,¬†**do not sleep on the infrastructure**. The reason I passed is that I focused nicely on understanding¬†**NIM microservices, Triton Inference Server, and Kubernetes scaling**. If I had relied only on my generic \"coding agents\" knowledge, I would have failed.\n\nAlso, Don't make my mistake‚Äîstudy the \"boring\" safety docs of safety, Ethics and Human in Loop Too!\n\n  \n**Rest assured, Ask me Anything about the exam and I will try my best to help** ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qmj990/passed_nvidia_agentic_ai_certification_ncpaai/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1ma4an",
          "author": "proof_required",
          "text": "How long did you prepare it for? How experienced are you?",
          "score": 1,
          "created_utc": "2026-01-25 14:02:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mbbn2",
              "author": "Ranger_1928",
              "text": "Hey,\n\nI have 1.5 years of indsutry experience working with LLMs and agents. But most of it was using Open Source Libraries. The transition from open source to nvidia tools has its own learning curve and took around 3-4 days (\\~ 1hour/day) for me to understand it.\n\nOverall preparation was around 17-20 days (\\~ 1/1.5 hour/day) with initial sections taking over 4 days. The last 3 sections (around \\~15%) of whole syllabus was done by me in an hour, didn't gave much attention to it, and that became the lowest scoring section for me now. Should've given more time to it too.",
              "score": 3,
              "created_utc": "2026-01-25 14:08:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1o303g",
                  "author": "Competitive-Fact-313",
                  "text": "Can you talk about your experience, what‚Äôs like using open source - on a daily basis how your day look like what are the tools you are most exposed with",
                  "score": 1,
                  "created_utc": "2026-01-25 18:53:02",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1mldp3",
          "author": "Yarafsm",
          "text": "Do you think it makes sense for anyone with no ML experience but quite a bit of cloud/platform architect experience ?\nAlso was it more focussed on engineering or architecture?",
          "score": 1,
          "created_utc": "2026-01-25 15:01:37",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1mm0lq",
              "author": "Ranger_1928",
              "text": "The exam is more **engineering-focused** than pure architecture. Your cloud background helps with deployment/scaling, but you‚Äôll need extra prep on NVIDIA‚Äôs AI/ML stack (NIM, Triton, NeMo, safety modules). It‚Äôs doable if you‚Äôre comfortable picking up new tools quickly.",
              "score": 3,
              "created_utc": "2026-01-25 15:04:47",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1mwbsg",
                  "author": "dank_coder",
                  "text": "I am familiar with building AI agents. I have built and deployed agents using Langgraph. How time do you think I need to dedicate for this?",
                  "score": 1,
                  "created_utc": "2026-01-25 15:52:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1n4fze",
          "author": "aksrao1998",
          "text": "Hi did you take any course to prepare?",
          "score": 1,
          "created_utc": "2026-01-25 16:27:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1n5pws",
              "author": "Ranger_1928",
              "text": "Nope, online resources. Got a roadmap and list of online articles / videos with the help of ChatGPT.",
              "score": 1,
              "created_utc": "2026-01-25 16:33:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1nm00r",
                  "author": "dank_coder",
                  "text": "Can you please share your roadmap?",
                  "score": 1,
                  "created_utc": "2026-01-25 17:43:16",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o1n6amv",
              "author": "DenseIncome4394",
              "text": "\\+1",
              "score": 0,
              "created_utc": "2026-01-25 16:35:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1nu8xj",
          "author": "Agreeable-Court602",
          "text": "Congratulations üéâüéâ. Also recently passed the same.",
          "score": 1,
          "created_utc": "2026-01-25 18:17:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1o1wki",
              "author": "Ranger_1928",
              "text": "Thanks, and Congratulations to you too üî•üî•",
              "score": 1,
              "created_utc": "2026-01-25 18:48:28",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o1prxut",
              "author": "ab624",
              "text": "how did you prepare ? any good resources and suggestions",
              "score": 1,
              "created_utc": "2026-01-25 23:21:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1q4jrg",
          "author": "burntoutdev8291",
          "text": "How useful is this cert? Do you have any plans on taking the NCA-AIIO?\n\nAnyway great work, I would think the safety and ethics stuff is very important because most of us know how to serve but don't know much about compliance.",
          "score": 1,
          "created_utc": "2026-01-26 00:22:10",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbtc9",
      "title": "MLOps vs MLE System Design Prep Dilemma for EM -> Which to Focus?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "author": "Low-Breakfast2018",
      "created_utc": "2026-01-19 18:11:06",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "Hi ML Leaders,  \n  \nI'm prepping for MLOps EM roles at FAANG/big tech + backups at legacy cos. But interviews seem split:  \n  \n1) SOP-hiring: Google & Meta, even \"MLOps\" JDs hit you with MLE-style system designs (classification/recommendation etc)  \n2) Team-oriented-hiring companies: Amazon/Uber/MSFT/Big Tech, more pure MLOps system design (feature stores, serving, monitoring, CI/CD).  \n3) Legacy (smaller/enterprise): Mostly general ML lead/director roles leaning MLE-heavy, few pure MLOps spots.  \n  \nDon't want to spread prep thin on two \"different\" system designs. How should I do to make sure to focus since the competition is high. Or any strategy or recommendation on double down on MLOps? How'd you balance? Seeking for experienced folks input.  \n  \nYOE: 13+ (non-FAANG)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ir6fp",
          "author": "Comprehensive_Gap_88",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-01-19 18:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhfaj",
          "author": "dank_coder",
          "text": "!remind me in 1 day",
          "score": 1,
          "created_utc": "2026-01-19 20:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jhlgg",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-20 20:26:55 UTC**](http://www.wolframalpha.com/input/?i=2026-01-20%2020:26:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/o0jhfaj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qhbtc9%2Fmlops_vs_mle_system_design_prep_dilemma_for_em%2Fo0jhfaj%2F%5D%0A%0ARemindMe%21%202026-01-20%2020%3A26%3A55%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qhbtc9)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-19 20:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kughz",
          "author": "Key_Base8254",
          "text": "following",
          "score": 1,
          "created_utc": "2026-01-20 00:35:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkrnnj",
      "title": "MLOps Free Course?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkrnnj/mlops_free_course/",
      "author": "AccountantUsual1948",
      "created_utc": "2026-01-23 14:04:22",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.87,
      "text": "I‚Äôm getting into¬†**MLOps**¬†and looking for any¬†**free courses or solid resources**.",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qkrnnj/mlops_free_course/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o19savn",
          "author": "fmindme",
          "text": "Hello. I've created this course, focused on the coding part of MLOps: https://mlops-coding-course.fmind.dev/. It's totally free, and there is a side repository https://github.com/fmind/mlops-python-package with a concrete example.",
          "score": 4,
          "created_utc": "2026-01-23 17:18:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19rym3",
          "author": "navin_fakirpure",
          "text": "Search on youtube channel",
          "score": 2,
          "created_utc": "2026-01-23 17:17:12",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1asgd8",
          "author": "dvdsdr",
          "text": "[https://madewithml.com/courses/mlops/](https://madewithml.com/courses/mlops/)",
          "score": 1,
          "created_utc": "2026-01-23 20:04:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o18y4cy",
          "author": "letsTalkDude",
          "text": "Why not searching this sub?",
          "score": 0,
          "created_utc": "2026-01-23 15:00:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qkuup9",
      "title": "Azure ML v2 and MLflow hell",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkuup9/azure_ml_v2_and_mlflow_hell/",
      "author": "Ordinary_Platypus_81",
      "created_utc": "2026-01-23 16:07:15",
      "score": 10,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hello,\n\n  \nI am just a recent grad (and from a ds degree too), so excuse my lack of expertise.\n\nWe are setting up ML orchestration in Azure ML and with MLflow. I have built the training pipelines and everything works nicely, I can register models and use them for scoring locally. However, I have had no luck deploying. I cannot seem to get the versions of packages to match up. The official Microsoft docs seem to be using varying versions and I just want a combination that works.  \n  \nWould y'all have any tips on finding one working combination and sticking to it? We are just in the building phase, so I can change everything still.\n\n(I am trying to deploy an xgboost model if that helps)\n\nThanks heaps!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qkuup9/azure_ml_v2_and_mlflow_hell/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1bvnwp",
          "author": "rishiarora",
          "text": "cfbr",
          "score": 1,
          "created_utc": "2026-01-23 23:13:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1ddjdi",
          "author": "ZeroCool2u",
          "text": "Similar to my Azure experience. I'm at a large org that uses AWS and Azure. Sagemaker is a similar, perhaps marginally better experience. We ended up spending a lot on a vendor MLOps platform to make it so we didn't have to deal with this type of stuff. Works well now, but super annoying and we wasted months.",
          "score": 1,
          "created_utc": "2026-01-24 04:25:12",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1dh7hf",
              "author": "prasanth_krishnan",
              "text": "Can you elaborate on what vendor you choose and why and what problem it solved. Thanks.",
              "score": 2,
              "created_utc": "2026-01-24 04:49:56",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o1dm6gt",
                  "author": "ZeroCool2u",
                  "text": "I was trying to avoid incurring a mods wrath re vendor promo, but w/e it's called domino data lab. \n\n\nVery enterprise focused. In our case it replaced multiple research university class on-prem HPC clusters. It can support relatively arbitrary tooling and we have people that use python, r, julia, rust but also stata, matlab, and fortran. In a pinch I've gotten some java stuff working on it that would have been annoying otherwise.\n\n\nUsually it's compared to databricks, which we also have, but it goes far beyond what dbx can do and has the added bonus of no usage based billing, so higher upfront cost, but dramatically lower total cost. Internally, dbx is just used by data engineers to create tables and then is wired up to starburst. People either query starburst, dbx directly, or random on-prem sql databases and pull it all into domino to do the actual work with whatever tools they need. They deploy the models as batch jobs, flyte jobs, or as model API's and might wrap a dash/shiny/streamlit app or something around the deployed model api as an easy to use front end. It handles all the scaling and auth for us and has this governance policy thing that lets you gate deployments, so you don't have to figure out wtf paperwork you have to do beforehand. Just fill in the blank and legal or risk or whoever gets an email telling them to go read your answers and approve or reject. \n\n\nHas its own MLFlow server and you can spin up spark, ray, dask, and MPI clusters and all the other typical MLOps features you'd expect. \n\n\nIt's just a more cohesive vision and actually works instead of the half baked stuff the hyperscalers sell. Definitely not for everyone, but tends to just work. If you google it look for the user guide, the main site is pretty marketing heavy.",
                  "score": 2,
                  "created_utc": "2026-01-24 05:24:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1g8a46",
          "author": "manwithaplandy",
          "text": "Iirc, you can create your own environment (basically just a custom container config) which lets you control what packages are installed and what versions, and deploy using that environment.",
          "score": 1,
          "created_utc": "2026-01-24 16:37:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o19ed90",
          "author": "mutlu_simsek",
          "text": "Try Perpetual ML.",
          "score": -3,
          "created_utc": "2026-01-23 16:15:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qixc5n",
      "title": "Looking for consulting help: GPU inference server for real-time computer vision",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "author": "bix_mobile",
      "created_utc": "2026-01-21 13:04:09",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "We're building a centralized GPU server to handle inference requests from multiple networked instruments running YOLO-based object detection and classification models. Looking for someone with relevant experience to consult on our architecture.\n\n**What we're trying to optimize:**\n\n* End-to-end latency across the full pipeline: image acquisition, compression, serialization, request/response, deserialization, and inference\n* API design for handling concurrent requests from multiple clients\n* Load balancing between two RTX 4500 Blackwell GPUs\n* Network configuration for low-latency communication\n\n**Some context:**\n\n* Multiple client instruments sending inference requests over the local network\n* Mix of object detection and classifier models\n* Real-time performance matters‚Äîwe need fast response times\n\nIf you have experience with inference serving (Triton, TorchServe, custom solutions), multi-GPU setups, or optimizing YOLO deployments, I'd love to connect. Open to short-term consulting to review our approach and help us avoid common pitfalls.\n\n**If you're interested, please DM with your hourly rate.**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0zl93r",
          "author": "Friendly_Willow_8447",
          "text": "Are you building this on top of any cloud providers or you have your own infrastructure? Also do you plan or are you doing kubernetes?",
          "score": 1,
          "created_utc": "2026-01-22 04:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10v0ml",
              "author": "bix_mobile",
              "text": "Server is on prem. K8s is in the plans",
              "score": 3,
              "created_utc": "2026-01-22 10:28:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11ywks",
                  "author": "Friendly_Willow_8447",
                  "text": "Cool. DMed you",
                  "score": 1,
                  "created_utc": "2026-01-22 14:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o17o96w",
          "author": "Good-Coconut3907",
          "text": "Happy to help! Just DM'd you.\n\nI've built an open source platform to manage AI deployments across multiple GPUs. \n\n[https://github.com/kalavai-net/kalavai-client](https://github.com/kalavai-net/kalavai-client)",
          "score": 1,
          "created_utc": "2026-01-23 10:01:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhesvw",
      "title": "Setup a data lake",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "author": "Subatomail",
      "created_utc": "2026-01-19 19:56:01",
      "score": 9,
      "num_comments": 7,
      "upvote_ratio": 0.92,
      "text": "Hi everyone,\n\nI‚Äôm a junior ML engineer, I have 2 years experience so I‚Äôm not THAT experienced and especially not in this.\n\nI‚Äôve been asked in my current job to design some sort of data lake to make the data independent from our main system and to be able to use this data for future projects in ML.\n\nTo give a little context, we already have a whole IT department working with the ‚Äúmain‚Äù company architecture. We have a very centralized system with one guy supervising every in and out. It‚Äôs a mix of AWS and on-prem.\n\nEverytime we need to access data, we either have to export them manually via the software (like a client would do) or if we are lucky and there is already an API that is setup we get to use it too.\n\nSo my manager gave me the task to try to create a data lake (or whatever the correct term might be for this) to make a copy of the data that already exists in prod and also start to pump data from the sources used by the other software. And by doing so, we‚Äôll have the same data but we‚Äôll have it independently whenever we want.\n\nThe thing is I know that this is not a simple task and other than the courses I took on DBs at school, I never designed or even thought about anything like this. I don‚Äôt know what would be the best strategy, the technologies to use, how to do effective logs‚Ä¶.\n\nThe data is basically a fleet management, there are equipment data with gps positions and equipment details,  there are also events like if equipment are grouped together then they form a ‚Äújob‚Äù with ids, start date, location‚Ä¶ so it‚Äôs a very structured data so I believe a simple sql db would suffice but I‚Äôm not sure if it‚Äôs scalable.\n\nI would appreciate it if I could get some kind of books to read or leads that I should follow to at least build something that might not break after two days and that will be a good foundation long term for ML.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0os2hr",
          "author": "ClearML",
          "text": "You‚Äôre not wrong, as this is a big ask, especially for a junior role. From an ML standpoint, don‚Äôt overthink ‚Äúdata lake‚Äù yet.\n\nFor structured fleet/event data, a simple SQL store is fine to start. What matters more for ML is: having reproducible snapshots of data, knowing which model trained on which version, avoiding manual exports long-term\n\nif you want something outside SQL that still works well for ML, a common choice is an object-store ‚Äúlake‚Äù:\n\n* Land raw data as files in S3 (or MinIO on-prem), partitioned by date/entity (e.g., events/date=.../, gps/date=.../).\n* Use a table format like Delta Lake / Apache Iceberg / Apache Hudi on top so you get versioning + schema evolution + time travel (super helpful for reproducible training datasets).\n* Query it later with Trino/Athena/Spark when needed, without locking yourself into one database.\n\nThe hard parts aren‚Äôt scale, they‚Äôre ingestion, schema changes, and data ownership. Start with append-only ingestion from prod (even on a schedule), keep it boring, and design for traceability first.\n\nIf you build something reliable and reproducible, you‚Äôll have a solid ML foundation, you can always optimize later.",
          "score": 5,
          "created_utc": "2026-01-20 16:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pizjw",
              "author": "Subatomail",
              "text": "Thank you for the tips. I‚Äôll look more into the steps you proposed and I‚Äôll give extra attention to reliability and reproducibility",
              "score": 1,
              "created_utc": "2026-01-20 18:16:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s478f",
          "author": "HC-Klown",
          "text": "As a former ML Engineer I agree with u/ClearML about the fact that the most important thing you need is a way to track your ML Experiments and to add on that, a way to monitor deployments (your an ml engineer primarily and not a data scientist). \n\nBut as a data engineer who took part in designing and implementing my company's data platform, my advice is to NOT try to build your own version of a data platform.\nIf i understand correctly, there is a centralized team in charge of gathering the data and hopefully doing efforts to establish a source of truth for data about important entities and processes in the company.\n\nOther than ingesting already existing data from their platform, you are suggesting to also ingest data from other sources which they have already ingested, figured out potentially complex source data models, quality tested and likely implemented business logic which you do not know about. \nSo, your statement that \"we will have the same data but we'll have it independently\" is a highly unlikely scenario. Data ia not extracted and 'voila\" ready to use, there is likely mane steps inbetween.\nYou are risking:\n1. Redoing work that has already been done by another team\n2. Training your models on data that does not represent an already established and potentially evolving truth. Effectively building a shadow data platform will in the long run not be beneficial for you or the company\n\nSo my advice would be to:  \n* Focus your efforts on building a bridge between your team and the centralized data team, and trying to get the data you need from the centralized platform. I know this might take time and managers want quick results. But doing this is better in the long run. Moreover, you should be able to get support from your manager and higher stakeholders on this approach. As an ML engineer you cannot be starved of the data you need. Try doing this in parallel to starting your \"shadow data lake\" if you really need quick results.\n* From this data build a feature store, and advice like using Open Table Formats like delta or iceberg that support time travel is a nice to have and not a MUST at the beginning.",
          "score": 1,
          "created_utc": "2026-01-21 02:00:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksynn",
          "author": "eemamedo",
          "text": "Data Lake as a technology is fairly simple. Think, S3 Buckets but many of them. \n\n\"Simple DB\" would be Data Warehouse. \n\n\nNeither of them are suitable as is for ML workloads.",
          "score": 1,
          "created_utc": "2026-01-20 00:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kxl59",
              "author": "Subatomail",
              "text": "What would be suitable for ML then ? Or would the data lake be a first step and then there should be an intermediate between the data lake and the ml pipeline ? Then what technology would be used for this intermediate step ?",
              "score": 1,
              "created_utc": "2026-01-20 00:51:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lik4o",
                  "author": "eemamedo",
                  "text": "Yup. Data lake is the first step. Usually, data lake is used for raw, unprocessed data that you then clean using ETL or ELT pipeline and load into data warehouse. After that, you do ML modeling. I skipped couple of steps but those steps depend on the company. For example, in my previous work, we used OLTP DB to process data from DWH and then ML consumes that data. Some companies use Feature Stores.",
                  "score": 1,
                  "created_utc": "2026-01-20 02:46:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qkshsv",
      "title": "Who is training on TBs of data?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkshsv/who_is_training_on_tbs_of_data/",
      "author": "HahaHarmonica",
      "created_utc": "2026-01-23 14:38:04",
      "score": 7,
      "num_comments": 9,
      "upvote_ratio": 0.74,
      "text": "As the title says, who is training a single model on 10s-100sTB? What is your stack? What software are you using on the orchestration side of things to do this over multiple nodes? What are you using on the model training side?\n\nThey have about 18TB now, but are ramping up their data collection over the next 6 months and will be collecting significantly more data. This would be to train a single model. ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qkshsv/who_is_training_on_tbs_of_data/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o1c0yw4",
          "author": "burntoutdev8291",
          "text": "I am using, but what's the issue? Is it LLM?",
          "score": 1,
          "created_utc": "2026-01-23 23:41:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1c1ajv",
              "author": "HahaHarmonica",
              "text": "what?",
              "score": 0,
              "created_utc": "2026-01-23 23:43:35",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o1c34fb",
                  "author": "burntoutdev8291",
                  "text": "Software: If it's LLM based, we usually use training frameworks like NeMo. They support distributed training very well. Pytorch lightning, huggingface and mosaicml are good options as well if it's not LLMs. Also I didn't really get what you meant by stack, but for training we are using slurm. \n\nStorage: Data is always on a distributed storage, like lustre or weka. An NFS will work as well, but performance won't be as good. \nFor data storage, what is your current file format?",
                  "score": 2,
                  "created_utc": "2026-01-23 23:53:25",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o1d2ks4",
          "author": "Scared_Astronaut9377",
          "text": "You need to specify what you are doing. My experience training recommendation models on hundreds of billions of rows will not help you to fine-tune stable diffusion on a million high-resolution photos.",
          "score": 0,
          "created_utc": "2026-01-24 03:15:46",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o1nuv2s",
          "author": "H3zi",
          "text": "Aws SageMaker Training \n1-128 H100/H200\nS3 (fastfile mode)\nPlain PyTorch + HuggingFace Accelerate\nOur own data loader based on webdataset \n\nPre training / fine tuning diffusion models",
          "score": 0,
          "created_utc": "2026-01-25 18:19:49",
          "is_submitter": false,
          "replies": [
            {
              "id": "o1qntt3",
              "author": "tensorpool_tycho",
              "text": "u can get 128 h100 on demand w/ sagemaker? or is that a dedicated cluster?",
              "score": 1,
              "created_utc": "2026-01-26 01:59:18",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o1e31do",
          "author": "cosmic_timing",
          "text": "Idk but I'm not even training models anymore. Not for at least 6 months. 40$ fpga board does the trick \n\n\n\nWhat are you doing with all that compute? Surveillance? Lol",
          "score": -1,
          "created_utc": "2026-01-24 07:44:54",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhtov0",
      "title": "Releasing KAOS - The K8s Agent Orchestration System",
      "subreddit": "mlops",
      "url": "https://i.redd.it/ypcs28cn7geg1.gif",
      "author": "axsauze",
      "created_utc": "2026-01-20 06:34:03",
      "score": 6,
      "num_comments": 1,
      "upvote_ratio": 0.88,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhtov0/releasing_kaos_the_k8s_agent_orchestration_system/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mlq55",
          "author": "RetiredApostle",
          "text": "How does it differ from kagent by Solo.io?",
          "score": 1,
          "created_utc": "2026-01-20 07:08:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhvmu9",
      "title": "Tracking access created by AI tools in MLOps pipelines tips",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhvmu9/tracking_access_created_by_ai_tools_in_mlops/",
      "author": "Abelmageto",
      "created_utc": "2026-01-20 08:29:55",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "Lately I‚Äôm noticing that a lot of access in MLOps setups isn‚Äôt coming from humans anymore. LLM assistants, training pipelines, feature stores, CI jobs, notebooks, plugins, browser tools. They all end up with tokens, OAuth scopes, or service accounts tied into SaaS systems.  \n  \nWhat feels tricky is that this access doesn‚Äôt behave like classic infra identities. Things get added fast, ownership changes, scopes drift, and months later nobody is really sure which model or tool still needs what.  \n  \nDo you treat AI tools as first-class identities, or is this still mostly handled ad-hoc?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qhvmu9/tracking_access_created_by_ai_tools_in_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0mysbo",
          "author": "RasheedaDeals",
          "text": "I ran into this after tracing a data exposure that didn‚Äôt come from infra at all. The access path was Airflow triggering a training job, MLflow logging artifacts, and a feature pipeline pulling from Snowflake using an OAuth app nobody remembered creating. The model was already retired but the token was still valid and had broad read access.\n\nIAM and cloud audit logs didn‚Äôt help much because the identity wasn‚Äôt a human or a workload identity tied to Kubernetes. It was a SaaS-level integration created by an ML tool months earlier. We only spotted it once we started mapping non-human identities across SaaS apps, not infra.\n\nWhat made this manageable was correlating service accounts, OAuth apps, and API tokens back to actual usage. Stuff like Datadog and cloud logs helped with activity, but not ownership or blast radius. Reco was useful there since it focuses on SaaS access paths and shows where AI tools and automations still have permissions long after pipelines change.\n\nFeels like most MLOps stacks still treat this as a blind spot unless something breaks.",
          "score": 2,
          "created_utc": "2026-01-20 09:07:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ngg74",
              "author": "Adventurous-Date9971",
              "text": "The blind spot isn‚Äôt just ‚Äúwho has access‚Äù but ‚Äúwhich pipeline behavior justifies that access right now.‚Äù If you don‚Äôt tie identities to concrete jobs and data flows, SaaS tokens just linger forever.\n\nWhat‚Äôs worked for me:\n\n\\- Treat every non-human identity (OAuth app, API token, service account) as code: defined in Git, named after a specific pipeline or model, with an owner and expiry date.\n\n\\- Add usage checks: if an identity hasn‚Äôt hit a critical API or table in X days, auto-flag it for review and scheduled revocation.\n\n\\- Log mapping: every ML job emits a jobid + identityid + dataset\\_id, and you keep that in a small metadata store to query ‚Äúwhat breaks if I kill this token?‚Äù\n\nOn the SaaS side, I‚Äôve used Reco and DoControl for visibility, and more recently Pulse alongside internal tooling to surface ‚Äúzombie‚Äù ML integrations people forgot about. Start by forcing every token to have an owner, scope, and TTL, then make unused access noisy until someone deletes it.",
              "score": 1,
              "created_utc": "2026-01-20 11:45:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qm58ql",
      "title": "Deploy Your First ML Model on GCP Step-by-Step Guide with Cloud Run, GCS & Docker",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qm58ql/deploy_your_first_ml_model_on_gcp_stepbystep/",
      "author": "gringobrsa",
      "created_utc": "2026-01-25 01:25:48",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.71,
      "text": "Walks through deploying a machine learning model on Google Cloud from scratch.  \nIf you‚Äôve ever wondered how to take a trained model on your laptop and turn it into a real API with Cloud Run, Cloud Storage, and Docker, this is for you.\n\nHere‚Äôs the link if you‚Äôre interested:  \n[https://medium.com/@rasvihostings/deploy-your-first-ml-model-on-gcp-part-1-manual-deployment-933a44d6f658](https://medium.com/@rasvihostings/deploy-your-first-ml-model-on-gcp-part-1-manual-deployment-933a44d6f658)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qm58ql/deploy_your_first_ml_model_on_gcp_stepbystep/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    }
  ]
}