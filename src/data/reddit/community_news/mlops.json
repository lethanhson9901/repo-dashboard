{
  "metadata": {
    "last_updated": "2026-01-23 16:53:50",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 9,
    "total_comments": 25,
    "file_size_bytes": 36171
  },
  "items": [
    {
      "id": "1qiqcl6",
      "title": "Coming from DevOps/Infra to MLOps? Here's what I learned after several interviews at product companies",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-01-21 06:22:21",
      "score": 71,
      "num_comments": 16,
      "upvote_ratio": 0.99,
      "text": "I've been interviewing for MLOps and ML Platform Engineer roles over the past few months, and I wanted to share some observations that might help others make a similar transition.\n\n**The Interview Gap**\n\nMost interviewers I've faced come from research or pure ML engineering backgrounds. They think in terms of model architectures, feature engineering, and training pipelines. If you're coming from a pure infrastructure or DevOps background like me, there's often a disconnect.\n\nYou talk about Kubernetes orchestration, GPU cluster management, and cost optimisation. They ask about data drift, model retraining strategies, or how you'd debug a model's performance degradation. The conversation doesn't flow naturally because you're speaking different languages.\n\n**What Actually Helped**\n\nI realised I needed to invest time in **ML fundamentals** ‚Äì not to become a data scientist, but to bridge the communication gap. Understanding basic statistics, how different model types work, and what \"overfitting\" or \"data leakage\" actually mean made a huge difference.\n\nWhen I could frame infrastructure decisions in ML terms (\"this architecture reduces model serving latency by X%\" vs \"this setup has better resource utilisation\"), interviews went much more smoothly.\n\n**Be Strategic About Target Companies**\n\nNot all MLOps roles are the same. If you're targeting companies heavily invested in **real-time inferencing** (think fraud detection, recommendation engines, autonomous systems), the focus shifts to:\n\n* Data distribution and streaming pipelines\n* Low-latency prediction infrastructure\n* Real-time monitoring and anomaly detection\n* Data engineering skills\n\nIf they're doing **batch processing and research-heavy ML**, it's more about:\n\n* Experiment tracking and reproducibility\n* Training infrastructure and GPU optimization\n* Model versioning and registry management\n\nMatch your preparation to what they actually care about. Don't spray-and-pray applications.\n\n**MLOps Roles Vary Wildly**\n\nHere's something that actually helped my perspective: **MLOps means different things at different companies**.\n\nI've had interviews where the focus was 90% infrastructure (Kubernetes, CI/CD, monitoring). Others were 70% ML-focused (understanding model drift, feature stores, retraining strategies). Some wanted a hybrid who could do both.\n\nThis isn't because teams don't know what they want. It's because MLOps is genuinely different depending on:\n\n* Company maturity (startup vs established)\n* ML use cases (batch vs real-time)\n* Team structure (centralised platform vs embedded engineers)\n\nIf an interview feels misaligned, it's often a mismatch in role expectations, not a reflection of your skills. The \"MLOps Engineer\" title can mean vastly different things across companies.\n\n**Practical Tips**\n\n* Learn the basics: bias-variance tradeoff, cross-validation, common model types\n* Understand the ML lifecycle beyond just deployment\n* Be able to discuss model monitoring (not just infra monitoring)\n* Know the tools: MLflow, Kubeflow, Ray, etc. ‚Äì but more importantly, know *why* they exist\n* Read ML papers occasionally ‚Äì even if you don't implement them, you'll understand what your ML colleagues are dealing with\n\n**Final Thought**\n\nThe transition from DevOps to MLOps isn't just about learning new tools. It's about understanding a new domain and the people working in it. Meet them halfway, and you'll find the conversations get a lot easier.\n\nKeep learning, keep iterating.\n\nIf anyone's going through a similar transition and wants to chat, feel free to DM or connect here: [https://topmate.io/varun\\_rajput\\_1914/](https://topmate.io/varun_rajput_1914/)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qiqcl6/coming_from_devopsinfra_to_mlops_heres_what_i/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0tlygd",
          "author": "____Kitsune",
          "text": "How often was kubernetes a hard req vs EKS or other managed services?",
          "score": 4,
          "created_utc": "2026-01-21 08:24:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tt6cn",
              "author": "Extension_Key_5970",
              "text": "For MLOPs, I have not faced a deep dive wrt core on-prem K8, nowadays it's all EKS managed, of course, one should be good enough with the K8 ecosystem, as ultimately models and apps are deployed on K8, so one needs debugging and troubleshooting skills with respect to it.",
              "score": 3,
              "created_utc": "2026-01-21 09:34:48",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0tok4t",
          "author": "mwon",
          "text": "Very nice summary. What you think about the role AI Scientists that in many cases requires knowledge in all those skills?",
          "score": 3,
          "created_utc": "2026-01-21 08:49:58",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tswyg",
              "author": "Extension_Key_5970",
              "text": "Scientists focus on research; the skills I mentioned are engineering skills. I've seen companies expect research expertise from engineers and vice versa. Some overlap is fine, but fully merging these roles isn't ideal in the long term.\n\nThe engineering skills are accessible to anyone from a software background moving into ML ‚Äì even ML scientists can pick them up if transitioning from research to engineering.\n\nBe intentional about your path rather than being pushed into a hybrid role that doesn't align with your strengths.",
              "score": 2,
              "created_utc": "2026-01-21 09:32:13",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0v0uqa",
          "author": "eemamedo",
          "text": "This is probably one of the best posts I have seen here for a while. I am going exactly the same. Some companies hire me for ML Infra but want me to live code LLM based application. It‚Äôs very rare that companies actually know what MLOps or ML Infra is supposed to do/be.¬†",
          "score": 2,
          "created_utc": "2026-01-21 14:39:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0tc3eo",
          "author": "EviliestBuckle",
          "text": "Which ml course did you take?",
          "score": 1,
          "created_utc": "2026-01-21 06:54:47",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0tlj5t",
              "author": "Extension_Key_5970",
              "text": "For specific ML knowledge, actually i havent follow any one course, instead I went for Top to bottom approach, I bought an practise exam for AWS ML Speciality, as it covers all ML foundations topics I suppose, went through exam scenarios one by one, learn from the answers and wrong choices, view YT videos - statsquest is awesome, if you want to dig in any of ml topics, explained very well by statsquest, these will create a strong base for ML",
              "score": 5,
              "created_utc": "2026-01-21 08:20:59",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0tmuef",
                  "author": "EviliestBuckle",
                  "text": "So in your experience how similar is llmops to mlops?",
                  "score": 1,
                  "created_utc": "2026-01-21 08:33:34",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o0tv57p",
          "author": "Competitive-Fact-313",
          "text": "Thanks for sharing",
          "score": 1,
          "created_utc": "2026-01-21 09:53:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0zmfj1",
          "author": "Friendly_Willow_8447",
          "text": "That‚Äôs really good details\nThanks for sharing",
          "score": 1,
          "created_utc": "2026-01-22 04:16:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o10f2ve",
          "author": "tortuga_me",
          "text": "Thats kind of true. Different companies view mlops tasks as something no other person can do. Part of the job is to advise management what is missing in their current setup and by default this means interviewers wont have that much insight into what they are looking for.",
          "score": 1,
          "created_utc": "2026-01-22 07:59:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhbtc9",
      "title": "MLOps vs MLE System Design Prep Dilemma for EM -> Which to Focus?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "author": "Low-Breakfast2018",
      "created_utc": "2026-01-19 18:11:06",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 0.95,
      "text": "Hi ML Leaders,  \n  \nI'm prepping for MLOps EM roles at FAANG/big tech + backups at legacy cos. But interviews seem split:  \n  \n1) SOP-hiring: Google & Meta, even \"MLOps\" JDs hit you with MLE-style system designs (classification/recommendation etc)  \n2) Team-oriented-hiring companies: Amazon/Uber/MSFT/Big Tech, more pure MLOps system design (feature stores, serving, monitoring, CI/CD).  \n3) Legacy (smaller/enterprise): Mostly general ML lead/director roles leaning MLE-heavy, few pure MLOps spots.  \n  \nDon't want to spread prep thin on two \"different\" system designs. How should I do to make sure to focus since the competition is high. Or any strategy or recommendation on double down on MLOps? How'd you balance? Seeking for experienced folks input.  \n  \nYOE: 13+ (non-FAANG)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0ir6fp",
          "author": "Comprehensive_Gap_88",
          "text": "Following",
          "score": 1,
          "created_utc": "2026-01-19 18:27:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0jhfaj",
          "author": "dank_coder",
          "text": "!remind me in 1 day",
          "score": 1,
          "created_utc": "2026-01-19 20:26:55",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0jhlgg",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 day on [**2026-01-20 20:26:55 UTC**](http://www.wolframalpha.com/input/?i=2026-01-20%2020:26:55%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qhbtc9/mlops_vs_mle_system_design_prep_dilemma_for_em/o0jhfaj/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qhbtc9%2Fmlops_vs_mle_system_design_prep_dilemma_for_em%2Fo0jhfaj%2F%5D%0A%0ARemindMe%21%202026-01-20%2020%3A26%3A55%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qhbtc9)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-01-19 20:27:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o0kughz",
          "author": "Key_Base8254",
          "text": "following",
          "score": 1,
          "created_utc": "2026-01-20 00:35:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qixc5n",
      "title": "Looking for consulting help: GPU inference server for real-time computer vision",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "author": "bix_mobile",
      "created_utc": "2026-01-21 13:04:09",
      "score": 10,
      "num_comments": 5,
      "upvote_ratio": 1.0,
      "text": "We're building a centralized GPU server to handle inference requests from multiple networked instruments running YOLO-based object detection and classification models. Looking for someone with relevant experience to consult on our architecture.\n\n**What we're trying to optimize:**\n\n* End-to-end latency across the full pipeline: image acquisition, compression, serialization, request/response, deserialization, and inference\n* API design for handling concurrent requests from multiple clients\n* Load balancing between two RTX 4500 Blackwell GPUs\n* Network configuration for low-latency communication\n\n**Some context:**\n\n* Multiple client instruments sending inference requests over the local network\n* Mix of object detection and classifier models\n* Real-time performance matters‚Äîwe need fast response times\n\nIf you have experience with inference serving (Triton, TorchServe, custom solutions), multi-GPU setups, or optimizing YOLO deployments, I'd love to connect. Open to short-term consulting to review our approach and help us avoid common pitfalls.\n\n**If you're interested, please DM with your hourly rate.**",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qixc5n/looking_for_consulting_help_gpu_inference_server/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0zl93r",
          "author": "Friendly_Willow_8447",
          "text": "Are you building this on top of any cloud providers or you have your own infrastructure? Also do you plan or are you doing kubernetes?",
          "score": 1,
          "created_utc": "2026-01-22 04:09:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o10v0ml",
              "author": "bix_mobile",
              "text": "Server is on prem. K8s is in the plans",
              "score": 3,
              "created_utc": "2026-01-22 10:28:45",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o11ywks",
                  "author": "Friendly_Willow_8447",
                  "text": "Cool. DMed you",
                  "score": 1,
                  "created_utc": "2026-01-22 14:46:51",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o17o96w",
          "author": "Good-Coconut3907",
          "text": "Happy to help! Just DM'd you.\n\nI've built an open source platform to manage AI deployments across multiple GPUs. \n\n[https://github.com/kalavai-net/kalavai-client](https://github.com/kalavai-net/kalavai-client)",
          "score": 1,
          "created_utc": "2026-01-23 10:01:25",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qgdznn",
      "title": "Thin agent / heavy tools + validation loops + observability: what would you add for prod?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/gallery/1qgdznn",
      "author": "OnlyProggingForFun",
      "created_utc": "2026-01-18 17:05:03",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.91,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qgdznn/thin_agent_heavy_tools_validation_loops/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": [
        {
          "id": "o0bi737",
          "author": "OnlyProggingForFun",
          "text": "If anyone wants the PDF, I can share it too :)",
          "score": 1,
          "created_utc": "2026-01-18 17:05:21",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o0cqote",
          "author": "Revolutionary-Bet-58",
          "text": "I would say check for infinite loops/recursion, does it meet regulatory requirements and no token bombing patterns",
          "score": 1,
          "created_utc": "2026-01-18 20:35:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ekdhj",
          "author": "sapiensush",
          "text": "What kind of eval you follow to be specific?",
          "score": 1,
          "created_utc": "2026-01-19 02:18:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0wh4ui",
          "author": "Competitive-Fact-313",
          "text": "Amazing",
          "score": 1,
          "created_utc": "2026-01-21 18:36:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qfy95h",
      "title": "I built a tool that forces 5 AIs to debate and cross-check facts before answering you",
      "subreddit": "mlops",
      "url": "https://i.redd.it/h7duxwz871eg1.jpeg",
      "author": "S_Anv",
      "created_utc": "2026-01-18 03:59:55",
      "score": 6,
      "num_comments": 5,
      "upvote_ratio": 0.8,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qfy95h/i_built_a_tool_that_forces_5_ais_to_debate_and/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0alini",
          "author": "functionalfunctional",
          "text": "So your solution to llms being bad at facts is to spend 5x as much $ and power to get a consensus on bad facts?  Maybe should have asked 6 if that was a good product idea.",
          "score": 3,
          "created_utc": "2026-01-18 14:23:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0nt6ax",
              "author": "Hyperventilater",
              "text": "I don't think that's 100% a fair criticism.\n\nThe idea behind this tool is a good one, power demands not taken into account. How do you determine consensus among a group of experts? You have them do honest debate and get to the bottom of it. \n\nThe tool might be more of \"have 5 lay-people debate until consensus\", but it's definitely a good idea in theory.",
              "score": 1,
              "created_utc": "2026-01-20 13:13:47",
              "is_submitter": false,
              "replies": []
            },
            {
              "id": "o0apsu4",
              "author": "S_Anv",
              "text": "You can enable/disable any model. the minimum is 2 models.¬†",
              "score": 0,
              "created_utc": "2026-01-18 14:46:41",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0xhi0b",
          "author": "SnooOwls966",
          "text": "How do you mitigate context pollution?",
          "score": 1,
          "created_utc": "2026-01-21 21:20:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o11jbm0",
          "author": "Artistic-Border7880",
          "text": "Why not ask a single service to give you multiple options and then challenge it on which one is better?\n\nIs there any actual advantage to using multiple services? As it definitely has the disadvantage of multiple accounts, billing, and APIs.",
          "score": 1,
          "created_utc": "2026-01-22 13:25:00",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhesvw",
      "title": "Setup a data lake",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "author": "Subatomail",
      "created_utc": "2026-01-19 19:56:01",
      "score": 6,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\n\nI‚Äôm a junior ML engineer, I have 2 years experience so I‚Äôm not THAT experienced and especially not in this.\n\nI‚Äôve been asked in my current job to design some sort of data lake to make the data independent from our main system and to be able to use this data for future projects in ML.\n\nTo give a little context, we already have a whole IT department working with the ‚Äúmain‚Äù company architecture. We have a very centralized system with one guy supervising every in and out. It‚Äôs a mix of AWS and on-prem.\n\nEverytime we need to access data, we either have to export them manually via the software (like a client would do) or if we are lucky and there is already an API that is setup we get to use it too.\n\nSo my manager gave me the task to try to create a data lake (or whatever the correct term might be for this) to make a copy of the data that already exists in prod and also start to pump data from the sources used by the other software. And by doing so, we‚Äôll have the same data but we‚Äôll have it independently whenever we want.\n\nThe thing is I know that this is not a simple task and other than the courses I took on DBs at school, I never designed or even thought about anything like this. I don‚Äôt know what would be the best strategy, the technologies to use, how to do effective logs‚Ä¶.\n\nThe data is basically a fleet management, there are equipment data with gps positions and equipment details,  there are also events like if equipment are grouped together then they form a ‚Äújob‚Äù with ids, start date, location‚Ä¶ so it‚Äôs a very structured data so I believe a simple sql db would suffice but I‚Äôm not sure if it‚Äôs scalable.\n\nI would appreciate it if I could get some kind of books to read or leads that I should follow to at least build something that might not break after two days and that will be a good foundation long term for ML.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qhesvw/setup_a_data_lake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0os2hr",
          "author": "ClearML",
          "text": "You‚Äôre not wrong, as this is a big ask, especially for a junior role. From an ML standpoint, don‚Äôt overthink ‚Äúdata lake‚Äù yet.\n\nFor structured fleet/event data, a simple SQL store is fine to start. What matters more for ML is: having reproducible snapshots of data, knowing which model trained on which version, avoiding manual exports long-term\n\nif you want something outside SQL that still works well for ML, a common choice is an object-store ‚Äúlake‚Äù:\n\n* Land raw data as files in S3 (or MinIO on-prem), partitioned by date/entity (e.g., events/date=.../, gps/date=.../).\n* Use a table format like Delta Lake / Apache Iceberg / Apache Hudi on top so you get versioning + schema evolution + time travel (super helpful for reproducible training datasets).\n* Query it later with Trino/Athena/Spark when needed, without locking yourself into one database.\n\nThe hard parts aren‚Äôt scale, they‚Äôre ingestion, schema changes, and data ownership. Start with append-only ingestion from prod (even on a schedule), keep it boring, and design for traceability first.\n\nIf you build something reliable and reproducible, you‚Äôll have a solid ML foundation, you can always optimize later.",
          "score": 5,
          "created_utc": "2026-01-20 16:12:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0pizjw",
              "author": "Subatomail",
              "text": "Thank you for the tips. I‚Äôll look more into the steps you proposed and I‚Äôll give extra attention to reliability and reproducibility",
              "score": 1,
              "created_utc": "2026-01-20 18:16:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o0s478f",
          "author": "HC-Klown",
          "text": "As a former ML Engineer I agree with u/ClearML about the fact that the most important thing you need is a way to track your ML Experiments and to add on that, a way to monitor deployments (your an ml engineer primarily and not a data scientist). \n\nBut as a data engineer who took part in designing and implementing my company's data platform, my advice is to NOT try to build your own version of a data platform.\nIf i understand correctly, there is a centralized team in charge of gathering the data and hopefully doing efforts to establish a source of truth for data about important entities and processes in the company.\n\nOther than ingesting already existing data from their platform, you are suggesting to also ingest data from other sources which they have already ingested, figured out potentially complex source data models, quality tested and likely implemented business logic which you do not know about. \nSo, your statement that \"we will have the same data but we'll have it independently\" is a highly unlikely scenario. Data ia not extracted and 'voila\" ready to use, there is likely mane steps inbetween.\nYou are risking:\n1. Redoing work that has already been done by another team\n2. Training your models on data that does not represent an already established and potentially evolving truth. Effectively building a shadow data platform will in the long run not be beneficial for you or the company\n\nSo my advice would be to:  \n* Focus your efforts on building a bridge between your team and the centralized data team, and trying to get the data you need from the centralized platform. I know this might take time and managers want quick results. But doing this is better in the long run. Moreover, you should be able to get support from your manager and higher stakeholders on this approach. As an ML engineer you cannot be starved of the data you need. Try doing this in parallel to starting your \"shadow data lake\" if you really need quick results.\n* From this data build a feature store, and advice like using Open Table Formats like delta or iceberg that support time travel is a nice to have and not a MUST at the beginning.",
          "score": 1,
          "created_utc": "2026-01-21 02:00:26",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o0ksynn",
          "author": "eemamedo",
          "text": "Data Lake as a technology is fairly simple. Think, S3 Buckets but many of them. \n\n\"Simple DB\" would be Data Warehouse. \n\n\nNeither of them are suitable as is for ML workloads.",
          "score": 1,
          "created_utc": "2026-01-20 00:27:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0kxl59",
              "author": "Subatomail",
              "text": "What would be suitable for ML then ? Or would the data lake be a first step and then there should be an intermediate between the data lake and the ml pipeline ? Then what technology would be used for this intermediate step ?",
              "score": 1,
              "created_utc": "2026-01-20 00:51:36",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o0lik4o",
                  "author": "eemamedo",
                  "text": "Yup. Data lake is the first step. Usually, data lake is used for raw, unprocessed data that you then clean using ETL or ELT pipeline and load into data warehouse. After that, you do ML modeling. I skipped couple of steps but those steps depend on the company. For example, in my previous work, we used OLTP DB to process data from DWH and then ML consumes that data. Some companies use Feature Stores.",
                  "score": 1,
                  "created_utc": "2026-01-20 02:46:44",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qhtov0",
      "title": "Releasing KAOS - The K8s Agent Orchestration System",
      "subreddit": "mlops",
      "url": "https://i.redd.it/ypcs28cn7geg1.gif",
      "author": "axsauze",
      "created_utc": "2026-01-20 06:34:03",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qhtov0/releasing_kaos_the_k8s_agent_orchestration_system/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o0mlq55",
          "author": "RetiredApostle",
          "text": "How does it differ from kagent by Solo.io?",
          "score": 1,
          "created_utc": "2026-01-20 07:08:19",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qhvmu9",
      "title": "Tracking access created by AI tools in MLOps pipelines tips",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qhvmu9/tracking_access_created_by_ai_tools_in_mlops/",
      "author": "Abelmageto",
      "created_utc": "2026-01-20 08:29:55",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.81,
      "text": "Lately I‚Äôm noticing that a lot of access in MLOps setups isn‚Äôt coming from humans anymore. LLM assistants, training pipelines, feature stores, CI jobs, notebooks, plugins, browser tools. They all end up with tokens, OAuth scopes, or service accounts tied into SaaS systems.  \n  \nWhat feels tricky is that this access doesn‚Äôt behave like classic infra identities. Things get added fast, ownership changes, scopes drift, and months later nobody is really sure which model or tool still needs what.  \n  \nDo you treat AI tools as first-class identities, or is this still mostly handled ad-hoc?",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1qhvmu9/tracking_access_created_by_ai_tools_in_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o0mysbo",
          "author": "RasheedaDeals",
          "text": "I ran into this after tracing a data exposure that didn‚Äôt come from infra at all. The access path was Airflow triggering a training job, MLflow logging artifacts, and a feature pipeline pulling from Snowflake using an OAuth app nobody remembered creating. The model was already retired but the token was still valid and had broad read access.\n\nIAM and cloud audit logs didn‚Äôt help much because the identity wasn‚Äôt a human or a workload identity tied to Kubernetes. It was a SaaS-level integration created by an ML tool months earlier. We only spotted it once we started mapping non-human identities across SaaS apps, not infra.\n\nWhat made this manageable was correlating service accounts, OAuth apps, and API tokens back to actual usage. Stuff like Datadog and cloud logs helped with activity, but not ownership or blast radius. Reco was useful there since it focuses on SaaS access paths and shows where AI tools and automations still have permissions long after pipelines change.\n\nFeels like most MLOps stacks still treat this as a blind spot unless something breaks.",
          "score": 2,
          "created_utc": "2026-01-20 09:07:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o0ngg74",
              "author": "Adventurous-Date9971",
              "text": "The blind spot isn‚Äôt just ‚Äúwho has access‚Äù but ‚Äúwhich pipeline behavior justifies that access right now.‚Äù If you don‚Äôt tie identities to concrete jobs and data flows, SaaS tokens just linger forever.\n\nWhat‚Äôs worked for me:\n\n\\- Treat every non-human identity (OAuth app, API token, service account) as code: defined in Git, named after a specific pipeline or model, with an owner and expiry date.\n\n\\- Add usage checks: if an identity hasn‚Äôt hit a critical API or table in X days, auto-flag it for review and scheduled revocation.\n\n\\- Log mapping: every ML job emits a jobid + identityid + dataset\\_id, and you keep that in a small metadata store to query ‚Äúwhat breaks if I kill this token?‚Äù\n\nOn the SaaS side, I‚Äôve used Reco and DoControl for visibility, and more recently Pulse alongside internal tooling to surface ‚Äúzombie‚Äù ML integrations people forgot about. Start by forcing every token to have an owner, scope, and TTL, then make unused access noisy until someone deletes it.",
              "score": 1,
              "created_utc": "2026-01-20 11:45:58",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qkuup9",
      "title": "Azure ML v2 and MLflow hell",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qkuup9/azure_ml_v2_and_mlflow_hell/",
      "author": "Ordinary_Platypus_81",
      "created_utc": "2026-01-23 16:07:15",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hello,\n\n  \nI am just a recent grad (and from a ds degree too), so excuse my lack of expertise.\n\nWe are setting up ML orchestration in Azure ML and with MLflow. I have built the training pipelines and everything works nicely, I can register models and use them for scoring locally. However, I have had no luck deploying. I cannot seem to get the versions of packages to match up. The official Microsoft docs seem to be using varying versions and I just want a combination that works.  \n  \nWould y'all have any tips on finding one working combination and sticking to it? We are just in the building phase, so I can change everything still.\n\n(I am trying to deploy an xgboost model if that helps)\n\nThanks heaps!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qkuup9/azure_ml_v2_and_mlflow_hell/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o19ed90",
          "author": "mutlu_simsek",
          "text": "Try Perpetual ML.",
          "score": 0,
          "created_utc": "2026-01-23 16:15:42",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}