{
  "metadata": {
    "last_updated": "2026-02-07 02:58:05",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 17,
    "total_comments": 38,
    "file_size_bytes": 82219
  },
  "items": [
    {
      "id": "1qvhmjc",
      "title": "The weird mismatch in MLOps hiring that nobody talks about",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvhmjc/the_weird_mismatch_in_mlops_hiring_that_nobody/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-04 06:58:18",
      "score": 62,
      "num_comments": 18,
      "upvote_ratio": 0.93,
      "text": "Something I've noticed after being in this space for a while, and mentioned in past weeks' posts as well.\n\nMLOps roles need strong infrastructure skills. Everyone agrees on that. The job descriptions are full of Kubernetes, CI/CD, cloud, distributed systems, monitoring, etc.\n\nBut the people interviewing you? Mostly data scientists, ML engineers, and PhD researchers.\n\nSo you end up in a strange situation where the job requires you to be good at production engineering, but the interview asks you to speak ML. And these are two very different conversations.\n\nI've seen really solid DevOps engineers, people running massive clusters, handling serious scale, get passed over because they couldn't explain what model drift is or why you'd choose one evaluation metric over another. Not because they couldn't learn it, but because they didn't realise that's what the interview would test.\n\nAnd on the flip side, I've seen ML folks get hired into MLOps roles and MAY struggle because they've never dealt with real production systems at scale.\n\nThe root cause I think is that most companies are still early in their ML maturity. They haven't separated MLOps as its own discipline yet. The ML team owns hiring for it, so naturally, they filter for what they understand: ML knowledge, not infra expertise.\n\nThis isn't a complaint, just an observation. And practically speaking, if you're coming from the infra/DevOps side, it means you kinda have to meet them where they are. Learn enough ML to hold the conversation. You don't need to derive backpropagation on a whiteboard, but you should be able to talk about the model lifecycle, failure modes, why monitoring ML systems is different from monitoring regular services, etc.\n\nThe good news is the bar isn't that high. A few weeks of genuine study go a long way. And once you bridge that language gap, your infrastructure background becomes a massive advantage, because most ML teams are honestly struggling with production engineering.\n\nCurious if others have experienced this same thing? Either as candidates or on the hiring side?\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:¬†[topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvhmjc/the_weird_mismatch_in_mlops_hiring_that_nobody/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3hxttg",
          "author": "BeatTheMarket30",
          "text": "I'm not surprised as often people doing the hiring - hiring managers and recruiters are also clueless. They are hiring people with skills they themselves don't have, so have no clue what questions to ask. Data scientists will of course ask questions on ML that nobody besides themselves needs to know much about.\n\nMost open positions are also junior ones, while this field could benefit from solid DevOps experience. They don't seem to grasp that transition from DevOps to MLOps is perhaps just a few weekends of study as these people don't need low level details/maths. They will not be implementing KServe, Dynamo, vLLM, Tensor-RT, they will be deploying these solutions and monitoring them.\n\nFor a data scientist, MLOps will not be interesting and they will likely leave.",
          "score": 18,
          "created_utc": "2026-02-04 08:33:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3ka9em",
              "author": "teucros_telamonid",
              "text": "If you really want to understand the root problem, you need to think in terms of the business life cycle.\n\nStartups are just happy to secure some ML engineer, data scientist or any other similar title. The focus is to build something at least working, MLOps is not a priority yet. Even if they hire you, it will get ugly soon.\n\nScale-ups are at least starting to get in place there scale and quality becomes important, but they probably still too focused on their competitive advantage. Some AI feature that is, not something smooth and well running from day to day.\n\nI think the best places are way more mature companies which have experience with AI and want to turn it from a successful experiment to really stable service. But these are already quite successful, so not a lot of them in this field...",
              "score": 2,
              "created_utc": "2026-02-04 17:20:49",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3ie23t",
          "author": "proof_required",
          "text": "ML + anything is pretty much a big umbrella of things. Same goes for dedicated ML engineers. They might be asked DevOps question and won't be hired since there might be someone out there in this market who knows it. It's just that the companies can be super picky! So yeah messing up 1-2 questions can reduce your chances of hiring in this market.",
          "score": 8,
          "created_utc": "2026-02-04 11:03:59",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3k8ffi",
              "author": "eled_",
              "text": "In my experience it's more to do with the fact that a lot of companies, even in the \"AI\" space, have yet to understand what MLeng is really about. Probably even more so than MLOps which can have a more \"direct\" impact on operations when they're not around.\n\nOften MLE are just mapped to \"DS with a few SE chops\", when it's not \"DS without an explicit aversion for SE\". And it's plain wrong, and it has an opportunity cost, but it's what you tend to get when data-scientists select and manage them.",
              "score": 2,
              "created_utc": "2026-02-04 17:12:23",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3i7oja",
          "author": "karthikjusme",
          "text": "This is my biggest worry when applying new jobs. I am good taking models to production, setting pipelines for training and evaluation and dataset creation but I lack many concepts inside LLM's. Hard to keep track of them as well as there is a new thing every week.",
          "score": 6,
          "created_utc": "2026-02-04 10:06:33",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3j8p2f",
          "author": "TheRealStepBot",
          "text": "Maybe you don‚Äôt understand what mlops is? It‚Äôs not just running kubernetes for the ml team that‚Äôs for certain. \n\nIt‚Äôs responsible for managing the overall model lifecycle. The physical infrastructure to do that is just a necessary evil you have to deal with. The actual skills are related to understanding all the assumptions being made in the model and the data to make sure that that the performance at inference time matches performance at train time. \n\nThis involves a ton of work on how data is collected for the training process and then debugging at inference time to determine if the model is performing as intended and dig through all the layers of the stack to figure out why it may not be performing correctly. \n\nYou can have a perfectly good model trained on great data that is undeployable because the inference process has slightly different consistency guarantees from the offline training data. MLops is the final stop on the make it all work train. It‚Äôs not possible to do the job in any non trivial setting without a deep understanding of the math probability and statistics of what is going on.",
          "score": 3,
          "created_utc": "2026-02-04 14:22:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3xzg1j",
              "author": "SpiritedChoice3706",
              "text": "Scrolled down way too far before I got to this. I agree, lack of engineering skills is a big problem in hiring MLOps folks. And not just because you need a knowledge of DS (though you definitely do). The fact is, ML applications are different than plain Software applications, and there are some specific software skillsets that need to be applied to effectively serve ML.\n\nOn my last project, I was the bridge between a DS team and a Platform team. We had extreme troubles, because the Platform team didn't understand how serving an ML model is different than serving a normal API. For example, the ML artifacts we needed (or the size of them). They were really upset that we wanted to put a large model file in our deployment, but also told us we shouldn't need a volume mount - LOL. How else you gonna serve a model? I have dozens of examples like this. But also, in order to serve my DS' models, I need some understanding of how it works. I don't need the nitty gritty, but I need a solid working knowledge. How I pipeline and retrain a supervised vs unsupervised model is different. How I evaluate the model for drift depends on the algorithm.\n\nMy boss also once was short on MLOps engineers so he had to borrow some DevOps engineers. He said that two of them couldn't do what one engineer from our team can. Not because they weren't skilled - but because the lack of context for what they were engineering was a huge part.",
              "score": 2,
              "created_utc": "2026-02-06 18:18:53",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3y7hd7",
                  "author": "TheRealStepBot",
                  "text": "Its really all about managing reproducibility. Perfect reproducibility is expensive. So knowing what you can relax and still hit operational targets is the name of the game.",
                  "score": 1,
                  "created_utc": "2026-02-06 18:56:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3izf35",
          "author": "Beneficial_Aioli_797",
          "text": "The idea i have about MLOps is that no one wants to do it. Everyone wants to do data science or machine learning engineering because thats whats most attractive. As result, these jobs are much more competitive and MLOps or infra roles adjacent to ML are left In the shadows.\n\n\nThey are still incredibly well paid, highly specialized skills and imo are currently at a discount in the market. And the market favours you to be ML engineer first and DevOps second (for some reason).\n\n\nIn my opinion, MLOps and DE focused on features stores/online learning etc are even better than ML engineering.",
          "score": 4,
          "created_utc": "2026-02-04 13:32:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3new0k",
              "author": "weeyummy1",
              "text": "\\> They are still incredibly well paid, highly specialized skills and imo are currently at a discount in the market\n\nWhat do you mean by this? Are you saying it should be higher paid than it is?",
              "score": 1,
              "created_utc": "2026-02-05 02:49:51",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3p0k1x",
                  "author": "Beneficial_Aioli_797",
                  "text": "No. When i mean discount its Im the economical sense.\n\n\nML engineering and MLOps are similar in terms of pay, but competition for ML engineering is much more fierce. So you end up \"paying\" more to get a ML engineering job than MLOps and they both have the same face value.",
                  "score": 1,
                  "created_utc": "2026-02-05 10:29:32",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3hz8nt",
          "author": "No_Mongoose6172",
          "text": "In my experience, deployment is one of the things less taken into account in ML, yet I find it to be one of the most interesting and demanding",
          "score": 2,
          "created_utc": "2026-02-04 08:46:30",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3si54q",
          "author": "d1ddydoit",
          "text": "What different teams are hiring for will depend on where that team is in terms of the maturity of its ML system as you rightly say OP. If they are at low levels of maturity, it is as you say often teams of scientists who are hiring and they will often just look for an engineer who can scale up and automate a lot of what they have been crafting - that means at interview they will want an engineer that talks their language and shows that they understand them. If the hiring org is mature, they have employed a system / platform that clearly dictates the engineering skills that need to be brought in.\n\nEvery business and even within a large business, the level of maturity varies dramatically. My advice to applicants looking for roles is to find the role that matches your skills and experience given that the responsibilities are so varied from company to company for the same job title. If you are looking to learn, a firm that is mature is the easiest way to understand scale but if you want a challenge, want to shape something that could be exciting, take a chance on a firm that doesn‚Äôt know what it wants if you think you can really help, you like their culture and the remuneration is agreeable. It could still be the happiest place you‚Äôve ever worked (and some of the most fun data scientists I have ever worked with were the most clueless at what was needed outside of their IDE to make it all work).\n\nThat said‚Ä¶. If you aren‚Äôt able to explain concepts like model drift then you are unable to explain a key metric that helps determine the end of a model‚Äôs lifecycle (by monitoring its declining quality and kicking off a new build of the model). It is still never going to be the same as a DevOps or platform engineer and there will have to be language learnt that is employed by the data scientists if the engineers have not been/worked closely with data scientists previously.",
          "score": 2,
          "created_utc": "2026-02-05 21:37:21",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3izx7o",
          "author": "NotSoGenius00",
          "text": "Hot take devops is least mandatory thing for MLOps ! Do you need to know infra ? Yes. Should you setup all the infra yourselves? Fuck no ! There is a devops team for that. Yes data scientists will ask you traditional ML questions but you should know traditional ML because you will be deploying those models so you should be worried about things like latency, memory requirements etc. an MLOps engineer is more like ML + software rather than devops.  \n\nSo devops is the least important thing in my opinion. You need to know modelling well enough to deploy those models ! Tooling is not all you need",
          "score": 2,
          "created_utc": "2026-02-04 13:35:41",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3hp7bv",
          "author": "mystery_biscotti",
          "text": "Literally my plan. Decided to look at an AI professional certification because I need to speak the language. I miss my prod Ops jobs, monitoring systems, proactively fixing stuff, rolling my eyes at the adorable things vendors do during maintenance windows...",
          "score": 2,
          "created_utc": "2026-02-04 07:14:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k8289",
          "author": "klipseracer",
          "text": "How is this weird? They are looking for some who understands BOTH: deep learning models, how to package and distribute them, your experience with tools like DVC, the challenges around versioning models/weights within a data scientists typical workflow without disrupting it or making iterating painful, yada yada.\n\nPlus, they want to know if you have the infra skills they don't have. These two things allows you to meet them in the middle and understand their needs and help them understand what they don't know and would be absolutely expected. The fact you don't realize this is one reason you're not getting the job.\n\nReference: \nI used to work at a deep learning computer vision startup and wrote the cicd that deployed our models to thousands of physical locations across the USA.",
          "score": 1,
          "created_utc": "2026-02-04 17:10:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o5axq",
          "author": "simpleharmonicmotion",
          "text": "+1 This is my life :/ I've flat out said that we're lying to candidates by requiring ML skills for what turns out to be an infra job.",
          "score": 1,
          "created_utc": "2026-02-05 05:43:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3k19e8",
          "author": "fiddysix_k",
          "text": "Mlops is just devops except everyone around you pretends you're not just doing standard devops work until you actually just gaslight yourself into believing it.",
          "score": 1,
          "created_utc": "2026-02-04 16:39:36",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu1p9q",
      "title": "Transformer Lab is an Open-source Control Plane for Modern AI Workflows",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qu1p9q/transformer_lab_is_an_opensource_control_plane/",
      "author": "aliasaria",
      "created_utc": "2026-02-02 17:36:23",
      "score": 28,
      "num_comments": 0,
      "upvote_ratio": 0.98,
      "text": "Just released our latest open source project: **Transformer Lab for Teams** after the past year talking with research labs about friction in their daily workflows. It works with Slurm and SkyPilot to build a unified experience for ML researchers.\n\nTrends we observed:\n\n* The frontier labs invest a ton to build and maintain their own proprietary tooling.\n* Most other AI/ML research teams work with a fragmented landscape of legacy scripts, manual workflows which gets more complicated as you grow your team and run more experiments\n* Researchers spend as much as half their time dealing with environment and experiment logistics. For example, results get lost or rerun because jobs fail before finishing and artifacts aren‚Äôt tracked consistently.\n\nHow Transformer Lab for Teams is helpful:\n\n* **Unified Interface:** A single dashboard to manage data ingestion, model fine-tuning, and evaluation.\n* **Seamless Scaling:** The platform is architected to run locally on personal hardware (Apple Silicon, NVIDIA/AMD GPUs) and seamlessly scale to high-performance computing clusters using orchestrators like Slurm and SkyPilot.\n* **Extensibility:** A flexible plugin system allows researchers to add custom training loops, evaluation metrics, and model architectures without leaving the platform.\n* **Privacy-First:** The platform processes data within the user's infrastructure, whether on-premise or in a private cloud, ensuring sensitive research data never leaves the lab's control.\n* **Simplifying workflows:** Capabilities that used to require complex engineering are now built-in.\n   * Capturing checkpoints (with auto-restart)\n   * One-line to add hyperparameter sweeps\n   * Storing artifacts in a global object store accessible even after ephemeral nodes terminate.\n\nThe project is **open source and free to use** with a goal to advance the tools used by any research team big and small.\n\nWould something like this be useful? Welcome feedback for us to make it better. I‚Äôm one of the maintainers and can answer any questions.\n\nTry it here: [https://lab.cloud/](https://lab.cloud/)\n\nAsk any questions below -- really excited to keep working on this with the community!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qu1p9q/transformer_lab_is_an_opensource_control_plane/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qt9i0j",
      "title": "Can someone explain MLOps steps and infrastructure setup? Feeling lost",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qt9i0j/can_someone_explain_mlops_steps_and/",
      "author": "FreshIntroduction120",
      "created_utc": "2026-02-01 20:18:16",
      "score": 15,
      "num_comments": 13,
      "upvote_ratio": 0.94,
      "text": "Hey folks,\n\nI'm trying to wrap my head around MLOps and honestly feeling a bit overwhelmed with all the different info out there.\n\nWould love to hear from people who actually work with this stuff - what are the main steps you go through in an MLOps pipeline? Like from when you start building a model to getting it running in production and keeping it alive?\n\nAlso, how do you even set up the infrastructure for this? What tools do you use and how does it all connect together?\n\nI've been reading articles but they all seem kinda high-level or vendor-specific. Just want to understand how this works in the real world.\n\nAny advice or pointers would be awesome, thanks!",
      "is_original_content": false,
      "link_flair_text": "Great Answers :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qt9i0j/can_someone_explain_mlops_steps_and/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o32vesj",
          "author": "Competitive-Fact-313",
          "text": "MLOps in the real world is basically about taking a model from a notebook to something that can run reliably like normal software. At a high level, you clean and prepare data in a repeatable way, train models using scripts while tracking experiments and artifacts with tools like MLflow, package the final model into a service (usually FastAPI) and a Docker image, and then deploy that image on infrastructure like Kubernetes (for example EKS) so it can scale and stay up. CI/CD tools such as GitHub Actions are mainly used to test code, build and push images (to ECR), and trigger deployments, not to train large models directly heavy training usually runs on dedicated compute (EKS jobs, Batch, SageMaker, etc.) and just reports results back to MLflow. The key idea is separating concerns: training is compute-heavy and controlled, deployment is automated, and monitoring closes the loop so you know when to retrain or update the model.",
          "score": 7,
          "created_utc": "2026-02-02 01:41:44",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3bnb22",
              "author": "Glad_Appearance_8190",
              "text": "this is a solid summary tbh. the part people miss is that once it‚Äôs in prod, it behaves like any other system. data drifts, inputs go missing, edge cases pile up. ops work is mostly about knowing what version ran, on what data, and why it made a call. infra matters less than having repeatability, logs, and some way to explain failures when something goes weird at 2am.",
              "score": 3,
              "created_utc": "2026-02-03 11:12:20",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3c5fnu",
                  "author": "Competitive-Fact-313",
                  "text": "Thanks üôè i am happy to help.",
                  "score": 1,
                  "created_utc": "2026-02-03 13:21:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o31clz9",
          "author": "HahaHarmonica",
          "text": "Think of MLOps as just DevOps+Data. \n\nPeople write code to train models on some data. MLOps is the automation of that training to show data lineage of what data was used to train the models. Making the model accessible (e.g. hosting the inferencing), and then monitoring said model. Adjust code, rinse repeat.",
          "score": 6,
          "created_utc": "2026-02-01 20:51:46",
          "is_submitter": false,
          "replies": [
            {
              "id": "o31d7gq",
              "author": "EviliestBuckle",
              "text": "Can you please point to some structured course plz",
              "score": 2,
              "created_utc": "2026-02-01 20:54:44",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o33xqrx",
                  "author": "apexvice88",
                  "text": "No",
                  "score": 3,
                  "created_utc": "2026-02-02 05:40:52",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o323cym",
          "author": "MyBossIsOnReddit",
          "text": "Won't really straight up answer your question but MLOps is broad and needs pervasive support in tooling, storytelling and permissions. For that you need buy-in with management and other teams. So it's a lot of pitching and powerpointing until you get there.\n\nThen comes a bit of architecture and system design. Any given design won't meet every requirement but you don't need to roll out everything at once. So work iteratively. Try to close the model lifecycle loop (the etl-train-eval-deploy-monitor-retrain) first. Then add the other features depending on need. The problem is the quality of the platform largely depends on the sum of the parts. It will be painful before it gets better.\n\nYou also need to figure out what you need and where it lives. Models, api keys, networking, dashboards, storage, vector databases, infra as code, ci/cd, git? Infra as code, templating, containers, what metrics do you track? p99, latency?\n\nYou can tell the biggest pain point for me has always been getting everyone on the same page. None of the folks I work with get it and think they can either do it themselves over the weekend or that it's complex and we need another 4 consultants.\n\nPersonally I've always used some Infra as code (CDK, terraform), some form of managed services for serving (no way you're going to handle everything without), mlflow, tensorboard. Start with terraform and build infra and templates first. I've had good experiences with AWS sagemaker and Bedrock, and GCP's Vertex.",
          "score": 3,
          "created_utc": "2026-02-01 23:05:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o341k6o",
          "author": "Extension_Key_5970",
          "text": "Not being sure about your background, whether you are a fresh graduate or have some experience with software engineering, here are a few general pointers that are a must  \n  \n\\- MLOps is more of solving Data scientists, ML researchers' problems, rather than pure Infra. Companies usually have a DevOps team to handle Infra problems, what they don't have is someone who can make the model for the production infrastructure, that's where MLOps comes in\n\n\\- So think like an ML engineer, having an Infra experience is a must, that's what I think, skillset needed to become an MLOps, and that's what companies are trying to analyse in an interview\n\n\\- So, start with understanding ML Foundations, good with Python, hands-on, must  \n\\- Try to look for scenarios where DS/ML engineers want to push their model into production, or convert their prototypes from notebooks to a pipeline\n\n\\- From Infra--> Think of exposing models to end users in a scalable, reliable fashion, what metrics are needed to evaluate model performace",
          "score": 2,
          "created_utc": "2026-02-02 06:11:43",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o33y1dw",
          "author": "apexvice88",
          "text": "OP I suggest you stop asking reddit and start from the bottom. MLOps is not for beginners and clearly you're not ready. I doubt anyone has time to hand hold you the entire way. Either go out there and learn more and then come back and ask good questions or don't get into this field at all. Sorry if this sounds harsh, but it will be much harsher if you cannot be self sufficient and except things to be handed to you on a silver platter.",
          "score": 2,
          "created_utc": "2026-02-02 05:43:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3aq4qw",
              "author": "BackgroundLow3793",
              "text": "üò≥üò≥",
              "score": 1,
              "created_utc": "2026-02-03 06:05:32",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o37agpg",
          "author": "thelonious_stonk",
          "text": "MLOps has a tough learning curve but is basically: data -> model -> train -> deploy -> monitor. Learn by doing. For tools to help you get started, check out Kubeflow, MLflow and Transformer Lab for model work.",
          "score": 1,
          "created_utc": "2026-02-02 18:45:29",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3i8cb6",
          "author": "llamacoded",
          "text": "Honestly, it's a mess out there with all the info. In the real world, it boils down to a few core things:  \n  \n1. Dev & Tracking:Build your model, track experiments with something like MLflow (code, data, model artifacts). Version everything; code with Git, data on S3.  \n2. CI/CD: Automate training (e.g., Kubeflow Pipelines on EKS for us) and model deployment. Test the hell out of your model \\*before\\* it goes live.  \n3. Serving: Deploy models via FastAPI on K8s (EKS) or AWS SageMaker endpoints if you want to skip some infra setup.  \n4. Monitoring: Crucial. Track model accuracy, latency, data drift, and infra cost. Set up alerts. We use custom Python scripts hooked into Prometheus/Grafana for this.  \n  \nIt's not about fancy tools; it's about reliable pipelines and knowing your numbers. Last time, a model's accuracy tanked because we didn't monitor data quality properly post-deployment. Keep it simple at first.",
          "score": 1,
          "created_utc": "2026-02-04 10:12:37",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qshhhd",
      "title": "Deployed an ML Model on GCP with Full CI/CD Automation (Cloud Run + GitHub Actions)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "author": "gringobrsa",
      "created_utc": "2026-01-31 23:14:50",
      "score": 7,
      "num_comments": 2,
      "upvote_ratio": 0.82,
      "text": "# Hey folks\n\nI just published Part 2 of a tutorial showing how to deploy an ML model on GCP using Cloud Run and then evolve it from manual deployment to full CI/CD automation with GitHub Actions.\n\nOnce set up, deployment is as simple as:\n\n    git tag v1.1.0\n    git push origin v1.1.0\n\nFull post:  \n[https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582](https://medium.com/@rasvihostings/deploy-your-ml-model-on-gc-part-2-evolving-from-manual-deployments-to-ci-cd-399b0843c582)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qshhhd/deployed_an_ml_model_on_gcp_with_full_cicd/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o369o2r",
          "author": "Informal_Tangerine51",
          "text": "Nice setup. We're doing something similar for our document processing agent but hit a wall CI/CD doesn't solve: behavioral regression.\n\nYour pipeline deploys the model, runs tests, everything green. But when we update the underlying LLM (say GPT-4 to 4.5) or tweak retrieval logic, tests pass but production behavior changes on 15-20% of edge cases we never wrote tests for.\n\nCI catches code regressions because unit tests are deterministic. Agent behavior isn't. We have 80 synthetic test cases that pass every time, but production processes 5,000 document types with edge cases we didn't imagine. Model update ships, extraction accuracy drifts, customers notice before we do.\n\nThe gap: CI needs fixtures from production reality, not synthetic imagination. When agent breaks in prod, that failure should automatically become a regression test. Currently we fix the bug, merge, deploy, and six months later it happens again because we never captured the actual failure as a permanent fixture.\n\nYour tag-based deployment is clean, but how do you prevent model updates from silently changing behavior on real production patterns? Are you running evals against production traces, or just synthetic test suites?",
          "score": 1,
          "created_utc": "2026-02-02 15:57:14",
          "is_submitter": false,
          "replies": [
            {
              "id": "o38j1zk",
              "author": "gringobrsa",
              "text": "You‚Äôre absolutely right: CI/CD guarantees **deployment** correctness, but it cannot guarantee **semantic** correctness. When you swap GPT-4 for 4.5, or tweak a retrieval embedding, the prompt's 'meaning' might stay the same, but the output distribution shifts.\n\nHere is how you can bridge that gap:\n\n* **CI is the Plumbing, Evals are the Gate:** GitHub Actions shouldn't run your 5,000-case regression suite it‚Äôs too heavy and non-deterministic. Instead, CI triggers an external **Evaluation Pipeline**.\n* **Production-to-Fixture Pipeline:**  treat production failures as the highest-value data. When a user flags a hallucination or an extraction error, that trace is automatically sanitized and added to a 'Golden Dataset.' Your test suite is no longer 'synthetic imagination'; it‚Äôs a reflection of reality.\n* **Semantic Versioning for Models:**  treat a model swap like a major database schema migration. We run the new model in 'Shadow Mode' against those production fixtures. We don't look for 1:1 string matches (which always break), but use **LLM-as-a-Judge** to ensure the extraction logic hasn't drifted.\n* **Drift Analytics:** The pipeline looks for 'Regression Sensitivity.' If the new model improves 5% of cases but breaks 2% of previously 'solved' edge cases, the CI/CD pipeline blocks the merge.\n\nHaven't tried though you can try like this or have you implemenetd differently ?",
              "score": 1,
              "created_utc": "2026-02-02 22:15:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qx9ieg",
      "title": "Do you still need MLOps if you're just orchestrating APIS and RAG?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/",
      "author": "polyber42",
      "created_utc": "2026-02-06 05:52:28",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 0.73,
      "text": "I‚Äôm starting to dive into MLOps, but I‚Äôve hit a bit of a skeptical patch.\n\nIt feels like the \"heavy\" MLOps stack‚Äîexperiment tracking, distributed training, GPU cluster management, and model versioning‚Äîis really only meant for FAANG-scale companies or those fine-tuning their own proprietary models.\n\n  \nIf a compnay uses APIs(openai/anthropic), the model is a black box behind an endpoint. \n\nIn this case:  \n1. is there a real need for a dedicated MLOps role?\n\n2. does this fall under standard software engineering + data pipelines?\n\n3. If you're in this situation, what does your \"Ops\" actually look like? Are you mostly just doing prompt versioning and vector DB maintenance?\n\n  \nI'm curious if I should still spend time learning the heavy infra stuff",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3v5g2t",
          "author": "UnreasonableEconomy",
          "text": "Proompting isn't machine learning...\n\nEven RAG isn't machine learning. What are you learning? \n\nIf you're at least finetuning, then the need becomes obvious. But the ML field is significantly bigger than than just language models...",
          "score": 13,
          "created_utc": "2026-02-06 07:35:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v7jnr",
          "author": "Scared_Astronaut9377",
          "text": "You are very confused.",
          "score": 7,
          "created_utc": "2026-02-06 07:55:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3v81df",
              "author": "polyber42",
              "text": "That's why I'm here - to clear up that confusion.  \nMaybe you can help me understand.  \nI'm genuinely trying to learn where the line is drawn.",
              "score": 3,
              "created_utc": "2026-02-06 07:59:32",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3y66ro",
                  "author": "Scared_Astronaut9377",
                  "text": "Read about how companies use ML. I cannot explain where the line between things you don't understand is.",
                  "score": 1,
                  "created_utc": "2026-02-06 18:50:42",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o3vme2o",
          "author": "Glad_Appearance_8190",
          "text": "i‚Äôve seen this land closer to ‚Äúops for behavior‚Äù than classic mlops. even with api models you still have prompt drift, data freshness, weird edge cases, and no idea why something changed last week. logs, traces, versioned prompts, and clear rollback end up mattering more than GPUs. heavy infra maybe not, but zero ops usually hurts later....",
          "score": 2,
          "created_utc": "2026-02-06 10:16:28",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3v4ucb",
          "author": "raiffuvar",
          "text": "Its probably need at least 1 mlops per 4 DS. \nAnd api is not everything. Even with API compani4s need to set up experiment tracking etc. \nThe issue is that companies do not understand the need of mlops.",
          "score": 4,
          "created_utc": "2026-02-06 07:30:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3vfmp3",
          "author": "Classic_Swimming_844",
          "text": "RemindMe! -30 day",
          "score": 1,
          "created_utc": "2026-02-06 09:12:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3vfqel",
              "author": "RemindMeBot",
              "text": "I will be messaging you in 1 month on [**2026-03-08 09:12:08 UTC**](http://www.wolframalpha.com/input/?i=2026-03-08%2009:12:08%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/mlops/comments/1qx9ieg/do_you_still_need_mlops_if_youre_just/o3vfmp3/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fmlops%2Fcomments%2F1qx9ieg%2Fdo_you_still_need_mlops_if_youre_just%2Fo3vfmp3%2F%5D%0A%0ARemindMe%21%202026-03-08%2009%3A12%3A08%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201qx9ieg)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|",
              "score": 1,
              "created_utc": "2026-02-06 09:13:09",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o3wbwm5",
          "author": "dan994",
          "text": "I think your confusion stems from thinking those things are just reserved for those training LLMs. There are many many companies training their own models that aren't LLMs, and all of those will need MLOps.\n\nIf you're not training and deploying models then your MLOps will likely just become DevOps",
          "score": 1,
          "created_utc": "2026-02-06 13:25:50",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3xxly7",
          "author": "SpiritedChoice3706",
          "text": "1. It seems like your definition of \"ML\" is quite narrow and only includes GenAi. There are many, many kinds of ML algorithms in the industry - recommendation and ranking (search, suggestions), forecasting, etc. It's not getting the same kind of hype, but those models are definitely not black box APIs and occasionally actually require a lot of work to connect with the data properly, feature engineering, etc.\n\n2. Even in GenAI, Ops of some sort is often required for anything more than a pretty simple setup. It might end up being devops, but you need things like guardrails, rate limiting, and secure handling of API keys, not to mention appropriate data pipelining, in order to actually have these things return business value. And a lot of ML still involved in things like handling tokens efficiently, etc. So maybe this role isn't 'traditional' MLOps, but the use-case is certainly there. Maybe it's called \"AI engineering\". but it's certainly not that far off from trad MLOps, if a little different.\n\nIn my opinion, learning heavy infra  is one of the more valuable skills these days. Anyone can build now. Getting something scalable and secure into production? That is definitely a more secure skillset, even if the title shifts a bit.",
          "score": 1,
          "created_utc": "2026-02-06 18:10:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3ytuk0",
          "author": "Anti-Entropy-Life",
          "text": "If you are not training or serving your own models, you can ignore a lot of ‚Äúclassic MLOps‚Äù (distributed training, GPU fleet, checkpoint lineage). But you still need ops, because you are still shipping a probabilistic system whose behavior changes when any of these move: model endpoint, prompt/tooling, retrieval data, embeddings, index parameters, and guardrails.\n\nHow I usually frame it:\n\n1. Dedicated MLOps role?\n\n* Early stage: usually no. It is a backend or platform engineer plus a data engineer wearing an ‚ÄúLLM platform‚Äù hat.\n* You want a dedicated role when you have multiple teams shipping LLM features, regulated data, strict uptime, or you are doing frequent prompt and retrieval changes that need disciplined releases.\n\n1. Is it ‚Äújust software engineering + data pipelines‚Äù?\n\n* Mostly yes, but with extra failure modes: non-determinism, silent quality regressions, prompt injection, data leakage, vendor model updates, and evaluation that is not a simple unit test.\n* So, the missing piece is not GPU infra, it is evaluation, observability, and safety controls designed for LLM behavior.\n\n1. What does ‚ÄúOps‚Äù look like in API + RAG land?\n\n* Data and retrieval ops: ingestion, parsing, chunking, embedding generation, reindexing, backfills, access control, and index versioning/rollbacks.\n* Release management: prompt and config versioning, model version pinning, canary releases, fallbacks (smaller model, ‚Äúno answer‚Äù mode), and feature flags.\n* Evals: a regression suite with golden queries, retrieval quality checks (did we fetch the right docs), answer quality checks, and red team cases. Run it in CI before merges and continuously in production.\n* Observability: tracing across app ‚Üí retriever ‚Üí model call, token and latency budgets, cost tracking, citation coverage, refusal rates, and user feedback loops.\n* Security and compliance: prompt injection defenses, tool permissioning, PII filtering, and audit logs.\n\nSo yes, you still ‚Äúneed MLOps,‚Äù but it is closer to SRE + data engineering + QA for an LLM system. If you are choosing what to learn, prioritize: evaluation harnesses, observability, data/retrieval pipelines, and safe rollout patterns. Learn the heavy GPU stuff when you have a clear reason to own training or serving.\n\nI hope this helps! :)",
          "score": 1,
          "created_utc": "2026-02-06 20:47:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3zcdel",
              "author": "Cat_Carrot",
              "text": "THIS.\n\nCall it LMOps if you want, OP, but these skills are a differentiator for standout engineers and definitely worth learning.\n\nIt‚Äôs not just ‚Äúlearning infra‚Äù; it‚Äôs learning to think about and work with tools and processes the same way you learned to work with simple functions.",
              "score": 2,
              "created_utc": "2026-02-06 22:19:46",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3zf8s5",
                  "author": "Anti-Entropy-Life",
                  "text": "Thanks. I would argue that is what makes it so much fun! :D",
                  "score": 1,
                  "created_utc": "2026-02-06 22:34:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o40452t",
          "author": "Simple_Ad_9944",
          "text": "Yes you still need ‚Äúops,‚Äù but it looks different. If you‚Äôre calling black-box APIs, you‚Äôre not versioning weights, you‚Äôre versioning **inputs, policies, and failure handling**: prompt/config change control, eval suites, rollback, audit logs, escalation paths, and monitoring for drift in behavior even when the provider model changes under you. The hard part becomes governance + reliability, not training infra.\n\n  \nCurious how others are doing rollback / auditability for prompt+tool changes today.",
          "score": 1,
          "created_utc": "2026-02-07 00:56:30",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qux21q",
      "title": "[For Hire] Senior Data & MLOps Engineer | 9+ Years Experience | Azure, Spark, Palantir Foundry | Available IST ‚Äì 9 PM IST",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qux21q/for_hire_senior_data_mlops_engineer_9_years/",
      "author": "mcheetirala2510",
      "created_utc": "2026-02-03 16:46:17",
      "score": 5,
      "num_comments": 1,
      "upvote_ratio": 0.69,
      "text": "\nHi everyone! I am a Senior Data Engineer and MLOps Specialist with over 9 years of experience building scalable data architectures and productionizing machine learning models for global leaders like Microsoft, EPAM, and HCL.  \nI specialize in migrating legacy systems to modern cloud stacks and implementing \"Data Contracts\" to ensure long-term business continuity and data integrity.  \nWhy Hire Me?\nProven Cost Savings: Saved clients $250K USD by migrating bespoke datasets to Palantir Foundry and optimizing refresh rates.  \nArchitectural Leadership: Successfully influenced key architectural pivots that protected 300+ datasets from downstream failures.  \nEnd-to-End MLOps: Experienced in deploying models using Docker, AWS SageMaker, Azure Kubernetes (AKS), and MLflow for both real-time and batch inferencing.  \nInfrastructure & DevOps: Proficient in CI/CD (GitHub Actions, Azure DevOps) and Infrastructure as Code (Terraform).  \nHighly Certified: 6x Azure Certified, 2x Databricks Certified, and 1x AWS Certified.  \nTechnical Toolkit\nLanguages & Frameworks: SQL, Python, PySpark, Scala, Spark.  \nData Engineering: Azure Data Factory (ADF), Palantir Foundry, Databricks, Azure Data Lake.  \nMLOps & AI: Scikit-Learn, XGBoost, MLflow, Azure ML, AWS SageMaker.  \nDatabases: MongoDB, MS SQL Server.  \nVisualization: Power BI, Seaborn, Bokeh.  \nAvailability & Location\nTarget Region: EMEA (Open to remote roles).\nHours: Available from IST until 9 PM IST, providing excellent overlap with UK and European business hours.\nRole Type: Full-time.\nExperience Highlights\nEPAM (Senior Software Engineer): Currently migrating a 30-year legacy PL/SQL Data Warehouse to Spark and Palantir Foundry.  \nMicrosoft (Data Engineer): Built scalable ETL pipelines and handled real-time event processing with Azure Event Hubs.  \nYash Technologies (Data Scientist): Led a team of 6 to build MLOps solutions and successfully onboarded insurance clients through technical presales.  \nLooking for a seasoned engineer to bridge the gap between Data Engineering and Machine Learning?\nPlease DM me or reach out at mcheetirala@gmail.com to discuss how I can help your team!\n",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qux21q/for_hire_senior_data_mlops_engineer_9_years/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3u1wps",
          "author": "apexvice88",
          "text": "If someone as skilled as you has to look for jobs on Reddit, oh man, economy is doing pretty bad lol. Sucks for the graduates (american term) or freshers (international term)",
          "score": 1,
          "created_utc": "2026-02-06 02:47:05",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1quzaun",
      "title": "Orchestrating Two-Tower retrieval: Managing the training-to-serving loop",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1quzaun/orchestrating_twotower_retrieval_managing_the/",
      "author": "skeltzyboiii",
      "created_utc": "2026-02-03 18:05:58",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "The deployment of Two-Tower models for retrieval usually involves significant infrastructure overhead. Beyond just training the user and item encoders, the production pipeline typically requires:\n\n1. Index Orchestration:¬†Triggering embedding updates whenever item metadata changes to prevent drift.\n2. Vector DB Synchronization:¬†Managing the handoff between the feature store and the ANN index (e.g Pinecone, Milvus, or Weaviate).\n3. Hybrid Querying:¬†Implementing a way to combine vector similarity with hard business logic (e.g filtering out \"out of stock\" items) without incurring significant latency penalties.\n\nThe code required to keep these systems in sync often becomes more complex than the model architecture itself.\n\nWe‚Äôve been working on a more declarative approach that treats the training, indexing, and retrieval as a single layer. By using a SQL-based interface, you can query the model directly, the system handles the embedding updates and indexing in the background, allowing for standard¬†WHERE¬†clauses to be applied to the similarity results.\n\nWe put together a technical breakdown of this architecture using a fashion marketplace as the case study. It covers:\n\n* Connecting Postgres/data warehouses directly to the training pipeline.\n* Configuring Two-Tower schemas via YAML.\n* Sub-50ms retrieval benchmarks when combining neural search with SQL filters.\n\nIf you‚Äôre interested in the implementation details or the pipeline design:  \n[https://www.shaped.ai/blog/how-to-deploy-a-production-two-tower-model-in-less-than-a-day](https://www.shaped.ai/blog/how-to-deploy-a-production-two-tower-model-in-less-than-a-day)\n\n*Full disclosure: I‚Äôm with the team at Shaped and authored this technical guide.*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1quzaun/orchestrating_twotower_retrieval_managing_the/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qsn0d7",
      "title": "Non sucking, easy tool to convert websites to LLM ready data, Mojo",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "author": "malvads",
      "created_utc": "2026-02-01 03:16:12",
      "score": 5,
      "num_comments": 5,
      "upvote_ratio": 0.78,
      "text": "Hey all! After running into *only paid tools or overly complicated setups* for turning web pages into structured data for LLMs, I built **Mojo,** a **simple, free, open-source tool** that does exactly that. It‚Äôs designed to be easy to use and integrate into real workflows.\n\nIf you‚Äôve ever needed to prepare site content for an AI workflow without shelling out for paid services or wrestling with complex scrapers, this might help. Would love feedback, issues, contributions, use cases, etc. <3\n\n[https://github.com/malvads/mojo](https://github.com/malvads/mojo) (and it's MIT licensed)\n\n*Cheers!*",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qsn0d7/non_sucking_easy_tool_to_convert_websites_to_llm/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3g4zo8",
          "author": "burntoutdev8291",
          "text": "Nice work! But isn't the common problem with scrapers more of the rate limit? Would it be better to combine a crawler with your tool for parsing? Like HTTrack.",
          "score": 1,
          "created_utc": "2026-02-04 01:05:11",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3g62el",
              "author": "malvads",
              "text": "Hey, thanks for your comment, mojo does support proxy rotation (socks4, socks5, http), so technically, it can bypass rate-limits (429 HTTP) but just dont be evil (hehe)...",
              "score": 1,
              "created_utc": "2026-02-04 01:11:15",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o2xzawh",
          "author": "Anti-Entropy-Life",
          "text": "This looks genuinely useful, especially the focus on being simple, free, and MIT licensed without trying to be a full ‚ÄúAI platform\" = super cool :D\n\nWhat I particularly like is that it tackles a very real and annoying part of the stack: getting web content into a reasonably clean, LLM-ready form without paying per page or maintaining a complex scraping pipeline. That‚Äôs a big win for early experiments, internal tools, and small teams.\n\nA few questions that would help me understand how far this can go in real workflows:\n\n‚Ä¢ Is the extraction deterministic, meaning the same page always produces the same output?  \n  \n‚Ä¢ How do you think about drift and updates, for example re-ingesting pages that change over time?  \n  \n‚Ä¢ When things go wrong, like odd markup, partial loads, or missing content, where is the best place to debug? Raw HTML, Mojo‚Äôs intermediate output, or the final chunks?\n\nOne thing that could make this even stronger is being explicit about failure modes and contracts in the README. In other words, what Mojo guarantees versus what it intentionally does not. Even a short section in the README about what Mojo will not handle would build a lot of trust.\n\nThanks for building and open-sourcing this, though! Tools that remove friction without over-claiming are rare and genuinely appreciated! :D",
          "score": 0,
          "created_utc": "2026-02-01 09:21:06",
          "is_submitter": false,
          "replies": [
            {
              "id": "o2ydh60",
              "author": "malvads",
              "text": "Hi, thanks for your question. My idea with Mojo is to provide fast conversion between pages and LLM data.\n\nAnswering your questions:\n\n**‚Ä¢ Is the extraction deterministic, meaning the same page always produces the same output?**  \n‚Üí The same HTML provides the same output. Right now there are two options to get data: with the `--render` flag and without it.\n\nThe first one uses pure HTTP requests (ideal for static web pages). With the `--render` flag, it connects to Chrome using CDP (so no extra dependencies are downloaded, just your existing setup). -- This is still not released, planned for 0.1.0, but you can build Mojo and test this feature\n\nSo it depends on the setup you are using and how the web page is programmed.\n\n**‚Ä¢ How do you think about drift and updates, for example re-ingesting pages that change over time?**  \n‚Üí In my opinion, the best way to handle this is via CI pipelines (Jenkins/GitHub Actions), but you can always set up cron jobs as well (macOS/Linux).\n\n**‚Ä¢ When things go wrong, like odd markup, partial loads, or missing content, where is the best place to debug? Raw HTML, Mojo‚Äôs intermediate output, or the final chunks?**  \n‚Üí In my opinion, the best way to debug is to fetch the page via `curl` and use a converter. For example:\n\n    curl -X GET your-web -o file.html\n\nThen later you can use Mojo to fetch the static source using:\n\n    ./mojo -d 0 file://your_file -o ./debug\n\nand see the output it generates (always with depth 0).\n\n**‚ÄúOne thing that could make this even stronger is being explicit about failure modes and contracts in the README. In other words, what Mojo guarantees versus what it intentionally does not. Even a short section in the README about what Mojo will not handle would build a lot of trust.‚Äù**\n\nThanks for your suggestion, I will include it after I finish the render crawler.\n\nThanks :)",
              "score": 1,
              "created_utc": "2026-02-01 11:30:25",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o2yv58y",
                  "author": "Anti-Entropy-Life",
                  "text": "Thanks so much for taking the time to write such a detailed and awesome response! I think this is really a spectacular project my friend! :)\n\nThe deterministic model of ‚Äúsame HTML gives the same output‚Äù makes sense, and the split between pure HTTP fetching and the `--render` path is clear. Using CDP for rendering without pulling in extra dependencies seems like the right call for JS-heavy pages.\n\nThe CI / cron approach for handling drift also feels pragmatic. Treating ingestion as an explicit job instead of something implicit or magical is exactly how I‚Äôd expect this to be used in real pipelines! This is fantastic!\n\nSo, just to be sure I am understanding you properly, the optimal debugging workflow you described is:\n\n0. Fetch HTML with  `curl`\n\n1. Validate or convert it independently\n2. Run Mojo at depth 0 against a static source\n\n(Edit: sorry, I don't know what's wrong with this list, the Reddit formatting refuses to not render it with this weird spacing/gap for some reason I can't quite determine at the moment)  \n  \nIf that‚Äôs right, that‚Äôs a great, clean debugging protocol. It‚Äôs one of the most realistic and sane ways I‚Äôve seen to isolate issues and understand where things are going wrong.\n\nI feel honored you found my simple README suggestion helpful at all. Being explicit about guarantees and non-goals is something I really appreciate in tools like this.\n\nThank you absolutely supremely for being a clear-minded thinker and building such beautiful tools that increase the coherence of the world :)\n\nLooking forward to seeing and testing out the render crawler once it's out. Thanks again for the thoughtful explanation and for building such a well-reasoned project. :)",
                  "score": 0,
                  "created_utc": "2026-02-01 13:40:39",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qv0dzk",
      "title": "Setting up production monitoring for LLMs without evaluating every single request",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qv0dzk/setting_up_production_monitoring_for_llms_without/",
      "author": "llamacoded",
      "created_utc": "2026-02-03 18:44:20",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "We needed observability for our LLM app but evaluating every production request would cost more than the actual inference. Here's what we implemented.\n\nDistributed tracing: Every request gets traced through its full execution path - retrieval, tool calls, LLM generation. When something breaks, we can see exactly which step failed and what data it received.\n\nSampled quality evaluation: Instead of running evaluators on 100% of traffic, we sample a percentage and run automated checks for hallucinations, instruction adherence, and factual accuracy. The sampling rate is configurable based on your cost tolerance.\n\nAlert thresholds: Set up Slack alerts for latency spikes, cost anomalies, and quality degradation. We track multiple severity levels - critical for safety violations, high for SLA breaches, medium for cost issues.\n\nDrift detection: Production inputs shift over time. We monitor for data drift, model drift from provider updates, and changes in external tool behavior.\n\nThe setup took about an hour using Maxim's SDK. We instrument traces, attach metadata for filtering, and let the platform handle aggregation.\n\nDocs: [https://www.getmaxim.ai/docs/tracing/overview](https://www.getmaxim.ai/docs/tracing/overview)\n\nHow are others handling production monitoring without breaking the bank on evals?",
      "is_original_content": false,
      "link_flair_text": "Tools: paid üí∏",
      "permalink": "https://reddit.com/r/mlops/comments/1qv0dzk/setting_up_production_monitoring_for_llms_without/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qwdbtk",
      "title": "My fraud model didn‚Äôt crash  it quietly dropped from F1 0.79 ‚Üí 0.18. Here‚Äôs how I caught it.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "author": "AffectionateSir8341",
      "created_utc": "2026-02-05 06:00:22",
      "score": 4,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "I had a fraud detection demo where nothing ‚Äúbroke‚Äù in production.\n\nNo errors, no crashes, no deploys.\n\n\n\nBut the model‚Äôs F1 score quietly dropped from 0.79 to 0.18 ‚Äî purely due to data drift.\n\n\n\nThat‚Äôs what scares me most about production ML: models don‚Äôt fail loudly,\n\nthey slowly start lying.\n\n\n\nTo explore this properly, I built ModelGuard ‚Äî a small reference implementation\n\nfocused on what happens \\*after\\* deployment:\n\n\n\n\\- detects data & prediction drift using multiple statistical tests\n\n\\- scores severity (not just yes/no drift)\n\n\\- recommends actions (ignore / monitor / retrain / rollback)\n\n\\- gates retraining with human approval\n\n\\- exposes everything via a CLI + lightweight REST API\n\n\n\nThis is intentionally not a framework or SaaS ‚Äî just a learning artifact for\n\napplied ML / MLOps.\n\n\n\nThe demo uses the Kaggle credit card fraud dataset with a simulated fraud-ring\n\nattack. The original dataset is extremely imbalanced (0.17% fraud), so I use\n\nbalanced resampling for faster iteration. Drift is synthetically injected, but\n\nall drift statistics and severity scores are real calculations.\n\n\n\nIn the demo, ModelGuard:\n\n\\- detects drift in 19 / 30 features (63%)\n\n\\- assigns HIGH severity (0.62)\n\n\\- flags a 77% drop in fraud-class F1\n\n\\- recommends retraining and creates a pending alert for human review\n\n\n\nRepo: [https://github.com/Aagam-Bothara/ModelGuard](https://github.com/Aagam-Bothara/ModelGuard)\n\n\n\nI‚Äôd really appreciate feedback from folks running models in prod:\n\n‚Äì does this feel realistic or over-engineered?\n\n‚Äì what would you simplify or remove?\n\n‚Äì what‚Äôs the first thing you‚Äôd add?\n\n",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwdbtk/my_fraud_model_didnt_crash_it_quietly_dropped/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1qvx8lj",
      "title": "The AI Analyst Hype Cycle",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-ai-analyst-hype-cycle",
      "author": "growth_man",
      "created_utc": "2026-02-04 18:52:19",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qvx8lj/the_ai_analyst_hype_cycle/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": [
        {
          "id": "o3z8txn",
          "author": "Anti-Entropy-Life",
          "text": "This is so real, and makes me so happy about the work I am doing in my lab, I can't wait to unveil our product that solves all of these issues :D",
          "score": 1,
          "created_utc": "2026-02-06 22:01:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qu4ehl",
      "title": "\"What data trained this model?\" shouldn't require archeology ‚Äî EU AI Act Article 10 compliance with versioned training data",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qu4ehl/what_data_trained_this_model_shouldnt_require/",
      "author": "DoltHub_Official",
      "created_utc": "2026-02-02 19:09:17",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.67,
      "text": "We build Dolt (database with Git-style version control), and we've been writing about how it applies to EU AI Act compliance. Article 10 requires audit trails for training data and reproducible datasets.\n\nHere's a pattern from Flock Safety (computer vision for law enforcement ‚Äî definitely high-risk):\n\n# How It Works\n\nEvery training data change is a commit. Model training = tag that commit. `model-2026-01-28` maps to an immutable snapshot.\n\nWhen a biased record shows up later:\n\nhttps://preview.redd.it/6injhhn4r4hg1.png?width=2182&format=png&auto=webp&s=1ea975d0f08a21025c98cd84644ac43420d582a0\n\nThat's the difference between \"we believe it was clean\" and \"here's the proof.\"\n\n\n\nMore detail: [https://www.dolthub.com/blog/2026-02-02-eu-ai-act/](https://www.dolthub.com/blog/2026-02-02-eu-ai-act/)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qu4ehl/what_data_trained_this_model_shouldnt_require/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3q1djl",
          "author": "NewClaim7739",
          "text": "This feels even more important once synthetic data enters the mix ",
          "score": 1,
          "created_utc": "2026-02-05 14:38:52",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qwz1y7",
      "title": "Open sourced an AI for debugging production incidents - works for ML infra too",
      "subreddit": "mlops",
      "url": "https://v.redd.it/jmn587p30rhg1",
      "author": "Useful-Process9033",
      "created_utc": "2026-02-05 21:59:17",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwz1y7/open_sourced_an_ai_for_debugging_production/",
      "domain": "v.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o3yvxt7",
          "author": "Anti-Entropy-Life",
          "text": "This looks pretty great! A few things that stand out as especially well done:\n\nThe framing is correct. Root cause analysis in prod is mostly about context stitching across logs, metrics, and deploys, not about clever ML. The fact that the agent explicitly gathers and correlates across the stack is the real value here.\n\nI like that it reads the system on setup. Most tools fail because they stay generic and never internalize how a specific system is wired. Treating that as a first-class step is the right call.\n\nPosting findings back into Slack is underrated but critical. Debugging lives where humans already are. Anything that requires a separate UI to be useful usually dies.\n\nOne thing I would watch carefully as this evolves is trust calibration. Engineers need to know when the AI is confident versus when it is exploring. Clear signals around evidence strength, uncertainty, and ‚Äúthis is my best hypothesis‚Äù vs ‚Äúthis is confirmed‚Äù will matter a lot for adoption.\n\nAnother thought is historical learning. If logs expire, the real long-term value may be in the incident summaries it generates and how those feed future investigations. That is where this could compound.\n\nOverall, this looks like real infrastructure, not a demo. If you keep it grounded in observability first and avoid overclaiming intelligence, this could be something teams actually depend on in the future!",
          "score": 1,
          "created_utc": "2026-02-06 20:57:39",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qvw8p3",
      "title": "Traditional OCR vs AI OCR vs GenAI OCR. When does this become a systems problem?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "author": "Tricky_Reveal_5951",
      "created_utc": "2026-02-04 18:17:32",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Early OCR conversations often focus on models and accuracy benchmarks.\n\nIn production, the harder problems show up elsewhere.\n\nTraditional OCR fails quietly when layouts drift.\n\n AI based OCR improves coverage but needs stronger guardrails.\n\n GenAI works on complex documents, but requires careful controls to avoid unreliable outputs.\n\nAt scale, OCR becomes less about choosing a model and more about designing a system that knows when to trust automation and when to stop.\n\nMost production pipelines rely on layered approaches, confidence thresholds, fallback strategies, and human review for edge cases.\n\nFor teams running document extraction in production, when did choosing an OCR approach turn into an MLOps and systems decision for you?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qvw8p3/traditional_ocr_vs_ai_ocr_vs_genai_ocr_when_does/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o3m1p2f",
          "author": "Informal_Tangerine51",
          "text": "OCR accuracy matters less than extraction accountability. When GenAI extracts wrong field from invoice, can you prove which text it saw and why it chose that interpretation?\n\nWe layer OCR methods too - traditional for structured forms, AI for complex layouts, GenAI for unstructured docs. Works until Legal asks \"what text informed this customer data extraction\" and we have confidence scores but not the actual OCR output the model operated on.\n\nThe systems problem isn't fallback strategies, it's decision evidence. Confidence threshold says \"trust this\" but doesn't capture what text was extracted, whether it was from correct page region, or why boundary detection chose those coordinates.\n\nYour layered approach handles accuracy. The gap: when extraction breaks, can you replay the exact OCR output and layout interpretation the model saw? Or just thresholds and final results?",
          "score": 4,
          "created_utc": "2026-02-04 22:17:35",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o3o4nph",
          "author": "Zoekielshane",
          "text": "From what I have seen, production OCR success depends more on system design and trade offs than picking the most accurate models choice. I have recently tested Traditional OCR (Tesseract), Deep Learning OCR (PaddleOCR), and GenAI OCR (VLM-based) on 10K+ financial documents. We shared real production examples and lessons learned here, in this technical writeup if it is useful context:\n https://visionparser.com/blog/traditional-ocr-vs-ai-ocr-vs-genai-ocr",
          "score": 1,
          "created_utc": "2026-02-05 05:38:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3oziai",
              "author": "letsTalkDude",
              "text": "Thanks",
              "score": 1,
              "created_utc": "2026-02-05 10:19:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o3ozl2y",
                  "author": "Zoekielshane",
                  "text": "welcome",
                  "score": 1,
                  "created_utc": "2026-02-05 10:20:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qws06r",
      "title": "CI quality gatekeeper for AI agents",
      "subreddit": "mlops",
      "url": "https://github.com/marketplace/actions/maos-agentgate-ci-quality-gatekeeper-for-ai-agents",
      "author": "TranslatorSalt1668",
      "created_utc": "2026-02-05 17:44:21",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qws06r/ci_quality_gatekeeper_for_ai_agents/",
      "domain": "github.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1qwrdn9",
      "title": "What happens when you outgrow the wrappers?",
      "subreddit": "mlops",
      "url": "/r/LocalLLaMA/comments/1qwrd6z/what_happens_when_you_outgrow_the_wrappers/",
      "author": "Left-Reflection-8508",
      "created_utc": "2026-02-05 17:21:51",
      "score": 3,
      "num_comments": 3,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1qwrdn9/what_happens_when_you_outgrow_the_wrappers/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o3z0mom",
          "author": "Anti-Entropy-Life",
          "text": "Neocloud vs AWS:\n\nNeoclouds can be great for raw GPU cost, but you usually trade money for operational burden:\n\n* Spiky capacity\n* Weaker networking or storage primitives\n* Slower support and incident response\n* More SRE work on your side\n\nA common compromise:\n\n* Run the control plane and observability where reliability is high (often AWS or a stable cloud)\n* Run GPU execution where it is cheapest, with fallback\n\n\n\nThe Middle Ground Most People Never See:\n\nYou do not need AWS or a wrapper if you separate control from execution.\n\nRun a small control plane yourself on cheap, boring infra (Hetzner, OVH, or a single VM):\n\n* routing\n* versioning\n* observability\n* rollouts\n* cost visibility\n\nThen attach execution underneath it:\n\n* cheaper GPU providers\n* on prem GPUs\n* spot instances\n* even wrappers as one backend\n\nAt that point, AWS and wrappers stop being architectural commitments and become interchangeable execution targets.\n\nSimple Rule Of Thumb:\n\n* If the wrapper is cheaper than the engineering time to replace it, keep it.\n* If it blocks reliability, observability, or unit economics, build a thin control plane first.\n* Move execution only when cost or reliability forces you.\n\nThis path gives leverage without overbuilding.",
          "score": 1,
          "created_utc": "2026-02-06 21:21:00",
          "is_submitter": false,
          "replies": [
            {
              "id": "o3z507a",
              "author": "Left-Reflection-8508",
              "text": "Appreciate the thoughtful reply, thank you.\n\nIt sounds like you've done it, is that so?",
              "score": 1,
              "created_utc": "2026-02-06 21:42:40",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o3z77e6",
                  "author": "Anti-Entropy-Life",
                  "text": "Yeah, I have done a smaller version of it. We ran FastAPI as a thin control layer in front of GPUs on Lambda so the interface, routing, and logging were ours, and compute was just an execution backend.\n\nThe separation worked well conceptually, but at the scale we were at it ended up costing more than I expected once you factor in always-on capacity and lower utilization. That pushed me to stay inside wrapper constraints a bit longer while being more deliberate about versioning and observability.\n\nThe main thing I took away is that owning even a minimal control plane is the key inflection point. You do not need to go all the way to full self hosting immediately, but once you own the interface, switching execution backends becomes a business decision instead of a rewrite.",
                  "score": 1,
                  "created_utc": "2026-02-06 21:53:35",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qxy8lw",
      "title": "Jupyter Notebook Validator Operator for automated validation in MLOps pipelines",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qxy8lw/jupyter_notebook_validator_operator_for_automated/",
      "author": "millionmade03",
      "created_utc": "2026-02-06 23:58:01",
      "score": 3,
      "num_comments": 0,
      "upvote_ratio": 0.81,
      "text": "\\- üìä Built-in observability: Expose Prometheus metrics and structured logs so you can wire dashboards and alerts quickly.\n\n\n\nHow you can contribute\n\n\n\n\\- Smart error messages (Issue #9)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/9)): Make notebook failures understandable and actionable for data scientists.\n\n\n\n\\- Community observability dashboards (Issue #8)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/8)): Build Grafana dashboards or integrations with tools like Datadog and Splunk.\n\n\n\n\\- OpenShift-native dashboards (Issue #7)(https://github.com/tosin2013/jupyter-notebook-validator-operator/issues/7)): Help build a native dashboard experience for OpenShift users.\n\n\n\n\\- Documentation: Improve guides, add more examples, and create tutorials for common MLOps workflows.\n\n\n\n\n\nGitHub: [https://github.com/tosin2013/jupyter-notebook-validator-operator](https://github.com/tosin2013/jupyter-notebook-validator-operator)\n\n\n\n\n\nDev guide (local env in under 2 minutes): [https://github.com/tosin2013/jupyter-notebook-validator-operator/blob/main/docs/DEVELOPMENT.md](https://github.com/tosin2013/jupyter-notebook-validator-operator/blob/main/docs/DEVELOPMENT.md)\n\n\n\n\n\nWe're at an early stage and looking for contributors of all skill levels. Whether you're a Go developer, a Kubernetes enthusiast, an MLOps practitioner, or a technical writer, there are plenty of ways to get involved. Feedback, issues, and PRs are very welcome.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qxy8lw/jupyter_notebook_validator_operator_for_automated/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    }
  ]
}