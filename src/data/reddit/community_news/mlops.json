{
  "metadata": {
    "last_updated": "2026-02-21 02:42:34",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 15,
    "total_comments": 24,
    "file_size_bytes": 55451
  },
  "items": [
    {
      "id": "1r8v6v5",
      "title": "Friendly advice for infra engineers moving to MLOps: your Python scripting may not enough, here's the gap to close",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r8v6v5/friendly_advice_for_infra_engineers_moving_to/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-19 09:54:09",
      "score": 51,
      "num_comments": 9,
      "upvote_ratio": 0.94,
      "text": "In my last post, I covered ML foundations. This one's about Python, specifically, the gap between \"I know Python\" and the Python you actually need for MLOps.\n\nIf you're from infra/DevOps, your Python probably looks like mine did: boto3 scripts, automation glue, maybe some Ansible helpers. That's scripting. MLOps needs programming, and the difference matters.\n\n**What you're probably missing:**\n\n* **Decorators & closures** ‚Äî ML frameworks live on these. Airflow's \\`@tasks\\`, FastAPI's \\`@app.get()\\`. If you can't write a custom decorator, you'll struggle to read any ML codebase.\n* **Generators** ‚Äî You can't load 10M records into memory. Generators let you stream data lazily. Every ML pipeline uses this.\n* **Context managers** ‚Äî GPU contexts, model loading/unloading, DB connections. The `with` Pattern is everywhere.\n\n**Why memory management suddenly matters:**\n\nIn infra, your script runs for 5 seconds and exits. In ML, you're loading multi-GB models into servers that run for weeks. You need to understand Python's garbage collector, the difference between a Python list and a NumPy array, and the GPU memory lifecycle.\n\n**Async isn't optional:**\n\nFastAPI is async-first. Inference backends require you to understand when to use asyncio, multiprocessing, or threading, and why it matters for ML workloads.\n\n**Best way to learn all this?** Don't read a textbook. Build an inference backend from scratch, load a Hugging Face model, wrap it in FastAPI, add batching, profile memory under load, and make it handle 10K requests. Each step targets the exact Python skills you're missing.\n\nThe uncomfortable truth: you can orchestrate everything with K8s and Helm, but the moment something breaks *inside* the inference service, you're staring at Python you can't debug. That's the gap. Close it.\n\nIf anyone interested in detailed version, with an atual scenarios covering WHYs and code snippets please refer: [https://medium.com/@thevarunfreelance/friendly-advice-for-infra-engineers-moving-to-mlops-your-python-scripting-isnt-enough-here-s-f2f82439c519](https://medium.com/@thevarunfreelance/friendly-advice-for-infra-engineers-moving-to-mlops-your-python-scripting-isnt-enough-here-s-f2f82439c519)\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:¬†[topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r8v6v5/friendly_advice_for_infra_engineers_moving_to/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o67u125",
          "author": "pmv143",
          "text": "Totally. The difference shows up fast when you‚Äôre running real inference workloads. A five second boto3 script mindset doesn‚Äôt translate to managing GPU memory, batching, async request handling, and long-lived model state.",
          "score": 13,
          "created_utc": "2026-02-19 10:02:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o68u8a6",
          "author": "Ancient_Canary1148",
          "text": "as in DevOps,im not coding applications or api but helping Dev teams to build,deploy,run and observe. why do i need to learn deep python ml programming to be an MlOps? as infra engineer,im helping ml teams to run models,prepare infra for them (kafka,ml flow,flink) and etc.",
          "score": 6,
          "created_utc": "2026-02-19 14:21:24",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6d8bdf",
              "author": "Extension_Key_5970",
              "text": "That's a fair point, and honestly, you're not wrong. If you're in a pure infra role, the toolset is completely different, and that work is genuinely valuable. ML teams need someone to set up Kafka, MLflow, Flink, and the K8S layer.\n\nBut here's where MLOps gets tricky, the line is blurred. In traditional DevOps, you don't touch the app code. Clear boundary. In MLOps, that boundary keeps breaking. One day, you're debugging why an inference service is leaking memory, or why a pipeline DAG is failing, and the answer isn't in the infrastructure; it's in the Python running on top of it.\n\nYou don't need to become a developer. But knowing enough Python to read, debug, and make sense of what's running on your infra, that's the difference. Both paths are valid; it just depends on where you want to grow.",
              "score": 3,
              "created_utc": "2026-02-20 04:22:26",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o6dz26b",
                  "author": "burntoutdev8291",
                  "text": "Depends on the team, I am the other way, developer / AI engineer to MLOps. Sometimes the lines are abit blurry, but my senior mentioned we need to know when to draw the line because deployment friendly code is on the developer. Otherwise very soon MLOps needs to help deploy jupyter notebooks.\n\nOur job is to reduce toil on the developer and solve infrastructure related problems. Because from what you are saying, I also need to know how to debug rust, go, python, ts, fortran depending on what the team uses. Python is easy enough that most MLOps can learn it but not the rest.",
                  "score": 1,
                  "created_utc": "2026-02-20 08:07:29",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            },
            {
              "id": "o6hdt76",
              "author": "Useful-Process9033",
              "text": "You are right that you do not need to become a Python expert to do MLOps from the infra side. But understanding how model serving works, how GPU memory behaves, and how to observe inference latency will make you 10x more effective at supporting ML teams than just provisioning Kafka clusters.",
              "score": 2,
              "created_utc": "2026-02-20 20:04:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o68klzr",
          "author": "TranslatorSalt1668",
          "text": "Great. Exactly what I was looking for. Thanks",
          "score": 1,
          "created_utc": "2026-02-19 13:27:18",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6h3lqz",
          "author": "bedel99",
          "text": "It sounds easy !",
          "score": 1,
          "created_utc": "2026-02-20 19:15:09",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r52ypf",
      "title": "Practical SageMaker + MLflow Stage/Prod Workflow for Small MLOps + DS Team?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "author": "ZeroSilver87",
      "created_utc": "2026-02-15 02:25:32",
      "score": 21,
      "num_comments": 6,
      "upvote_ratio": 1.0,
      "text": "Hey all ‚Äî As the title says, looking for practical input from teams operating at a similar scale...\n\nWe have a small MLOps team supporting a small Data Science team... \\~4-6 per team. We‚Äôre enabling SageMaker + MLflow this year and trying to move toward more sustainable, repeatable ML workflows.\n\nHistorically, our ML efforts have been fairly ad hoc and home-grown. We‚Äôre now trying to formalize things and improve R&D velocity without overburdening either the DS team or our platform engineers.\n\nOne major constraint is that our DevOps/infra process is heavily gated. New AWS resources require approvals outside our teams and move slowly. So we‚Äôre trying to design something clean and safe that doesn‚Äôt require frequent new infrastructure or heavyweight process for each new model.\n\nI‚Äôm aware of the AWS-recommended workflows, but they seem optimized for larger teams or environments with more autonomy than we have.\n\nSome Additional Context:\n\n* Data lake on S3 (queried via Athena)\n* Models are often entity-specific (i.e., many model instances derived from a shared training pipeline)\n\nCurrent thinking:\n\n* Non-Prod:\n   * EDA + pipeline development + model experimentation\n   * read-only access to prod archive data to remove need to set up complicated replication from prod to non-prod\n* Prod:\n   * Inference endpoints\n   * Single managed MLflow workspace\n      * DS can log runs + register models (from non-prod or local)\n      * Only a prod automation role can promote models to ‚ÄúProduction‚Äù\n      * Production Inference services only load models marked \"Production\"\n   * Automated retraining pipelines\n\nThoughts or suggestions on this setup?\n\nThe goal is to embed sustainable workflows and guardrails without turning this into a setup that requires large teams to support it.\n\nWould love to hear what‚Äôs worked (or failed) for teams in similar size ranges or if you have any good experience with AWS Sagemaker to suggest good workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5hxwfs",
          "author": "Bezza100",
          "text": "It seems good, you will need to invest significant time on the CI/CD for promoting to make sure it's robust. Also consider standard examples and templates for the DS team so there isn't too much refactoring to use your CI/CD.",
          "score": 3,
          "created_utc": "2026-02-15 12:34:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lpux4",
              "author": "ZeroSilver87",
              "text": "Good points. Yes I‚Äôm not sure exactly what the CI/CD will look like for this but some templates to make it faster is a good idea",
              "score": 1,
              "created_utc": "2026-02-16 00:41:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qki7y",
          "author": "Gaussianperson",
          "text": "You should definitely use the MLflow Model Registry as the central hub for model handoffs. When a data scientist is happy with a model, they register it in MLflow, and that acts as the signal for your staging or production pipelines to pick it up and deploy it to a SageMaker endpoint. This keeps the roles clear and avoids a lot of back and forth between the teams.  \n  \nOne thing to watch out for is over engineering your internal tools too soon.\n\nStick to the native MLflow and SageMaker APIs before building complex wrappers.\n\nIf you are looking for more examples of how different companies handle these kinds of architectural choices, check out machinelearningatscale.substack.com. (author here)\n\nIt has some solid case studies on how bigger shops manage their production infrastructure which can give you some good ideas for your own roadmap.",
          "score": 3,
          "created_utc": "2026-02-16 19:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qnu67",
              "author": "ZeroSilver87",
              "text": "Excellent, thanks for the advice. Agree on not over engineering too soon. We may have some hurdles to creating SageMaker endpoints on the fly via CI/CD but I think there is a reasonable work around with MME since they feed off an S3 bucket‚Ä¶ assuming we can maintain a consistent set of requirements.",
              "score": 1,
              "created_utc": "2026-02-16 19:51:24",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o6hekx5",
              "author": "Useful-Process9033",
              "text": "The model registry as handoff signal is a clean pattern. One thing to add: make sure you have monitoring on the deployed endpoints too, not just the training pipeline. Most teams nail the training workflow and then fly blind once the model is serving traffic.",
              "score": 1,
              "created_utc": "2026-02-20 20:08:03",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o62qkl5",
          "author": "gdnmaia",
          "text": "You should be able to access prd data from nonprd via access not replication - the architecture you are using is dated, pre data science. How can you test AI models that are probabilistic in nature (I am not talking about probabilistic models) with partial data? Read this for inspiration (https://www.databricks.com/sites/default/files/2024-06/2023-10-EB-Big-Book-of-MLOps-2nd-Edition.pdf?itm_source=www&itm_category=resources&itm_page=thank-you&itm_location=body&itm_component=hero&itm_offer=2023-10-eb-big-book-of-mlops-2nd-edition.pdf) and don‚Äôt worry about the platform used in this documentation, focus on the paradigm across environments (model promotion versus replication, code promotion, data access versus copy, and data lineage).",
          "score": 1,
          "created_utc": "2026-02-18 16:15:40",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r7s2ni",
      "title": "[D] We tested the same INT8 model on 5 Snapdragon chipsets. Accuracy ranged from 93% to 71%. Same weights, same ONNX file.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r7s2ni/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "author": "NoAdministration6906",
      "created_utc": "2026-02-18 03:30:42",
      "score": 17,
      "num_comments": 3,
      "upvote_ratio": 0.96,
      "text": "We've been doing on-device accuracy testing across multiple Snapdragon SoCs and the results have been eye-opening.\n\nSame model. Same quantization. Same ONNX export. Deployed to 5 different chipsets:\n\n|Device|Accuracy|\n|:-|:-|\n|Snapdragon 8 Gen 3|91.8%|\n|Snapdragon 8 Gen 2|89.1%|\n|Snapdragon 7s Gen 2|84.3%|\n|Snapdragon 6 Gen 1|79.6%|\n|Snapdragon 4 Gen 2|71.2%|\n\nCloud benchmark reported 94.2%.\n\nThe spread comes down to three things we've observed:\n\n1. **NPU precision handling**¬†‚Äî INT8 rounding behavior differs across Hexagon generations. Not all INT8 is created equal.\n2. **Operator fusion differences**¬†‚Äî the QNN runtime optimizes the graph differently per SoC, sometimes trading accuracy for throughput.\n3. **Memory-constrained fallback**¬†‚Äî on lower-tier chips, certain ops fall back from NPU to CPU, changing the execution path entirely.\n\nNone of this shows up in cloud-based benchmarks. You only see it when you run on real hardware.\n\nCurious if others are seeing similar drift across chipsets ‚Äî or if anyone has a good strategy for catching this before shipping. Most CI pipelines we've seen only test on cloud GPUs and call it a day.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r7s2ni/d_we_tested_the_same_int8_model_on_5_snapdragon/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o60owaq",
          "author": "KeyIsNull",
          "text": "Wow that's a huge performance drop, it is always a good idea to measure on device but I'd never expect a 15/20% drop. Have you also tested the model in full precision?",
          "score": 1,
          "created_utc": "2026-02-18 08:13:38",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o60wvu3",
          "author": "datashri",
          "text": "Did you publish anywhere?",
          "score": 1,
          "created_utc": "2026-02-18 09:28:47",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o629f1b",
          "author": "Commercial-Fly-6296",
          "text": "Maybe the chip wear and tear also matters ?",
          "score": 1,
          "created_utc": "2026-02-18 14:56:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r64jfl",
      "title": "We built hardware-in-the-loop regression gates for AI models on Snapdragon ‚Äî here's what we learned",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r64jfl/we_built_hardwareintheloop_regression_gates_for/",
      "author": "NoAdministration6906",
      "created_utc": "2026-02-16 08:49:34",
      "score": 11,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "We deploy AI models to Snapdragon devices and got tired of cloud tests passing while real hardware failed. Built a CI tool that runs your model on physical Snapdragon devices and blocks the PR if gates fail.\n\nBiggest surprise: same INT8 model showed 23% accuracy variance across 5 Snapdragon chipsets. Cloud benchmarks predicted none of this.\n\nFull disclosure: I built this (EdgeGate). Happy to answer questions about the architecture or edge AI testing in general.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r64jfl/we_built_hardwareintheloop_regression_gates_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5ovn0i",
          "author": "Comfortable_Holiday3",
          "text": "Looks good. I built something similar: an RPC with telemetry that also supports non-OS hardware abstracting the physical layer (USB/OTA streaming) and the model runtime/engine/interpreter. Interested in the architecture. Curiously, what do you think is the cause of the large accuracy variance? Is it something related to optimized kernel implementation in the Snapdragon?",
          "score": 1,
          "created_utc": "2026-02-16 14:49:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p2vg6",
              "author": "NoAdministration6906",
              "text": "Thanks! Your setup sounds really cool ‚Äî abstracting the physical layer for non-OS hardware is no joke.\n\nOur approach is more cloud-first ‚Äî we go through Qualcomm AI Hub's API for compilation and on-device execution, then wrap results in signed evidence bundles so CI gates can't be spoofed. Think \"unit tests but for model quality on real hardware.\"\n\nOn the accuracy variance ‚Äî I think you're on the right track with the kernel implementations. Different Hexagon NPU generations (v69, v73, v75) likely use different fixed-point arithmetic paths. But it's probably a mix of that plus compiler-level graph optimizations varying per target, and possibly quantization calibration differences during compilation. We're also not pinning the runtime library version across devices yet, so that's another uncontrolled variable.\n\nHonestly, isolating *which* of these contributes *how much* is half the reason we built this. Would love to compare notes on what you've seen on the bare-metal side.",
              "score": 2,
              "created_utc": "2026-02-16 15:25:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p5gg6",
          "author": "penguinzb1",
          "text": "the 23% variance is wild. i've been thinking about hardware-in-the-loop testing for agents that need to run on different devices, and this kind of variance is exactly what makes it hard to trust cloud benchmarks for anything that ships to physical hardware.\n\ncurious how you're handling the gate thresholds across different chipsets. do you set per-device accuracy targets, or do you have a single gate that accounts for the worst-case variance? seems like the latter would be overly conservative but the former creates a maintenance nightmare.\n\nalso wondering if you're seeing variance patterns that correlate with specific model architectures or quantization approaches. like, does the variance show up more in certain layer types or is it pretty uniform across the whole model?",
          "score": 1,
          "created_utc": "2026-02-16 15:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ryosw",
              "author": "NoAdministration6906",
              "text": "Yeah, we ran into the same tradeoff. We don‚Äôt do ‚Äúworst-case single gate‚Äù ‚Äî it‚Äôs too conservative.\n\nWe keep per-chipset baselines (golden outputs/metrics) and gate on regression vs that device + an absolute floor. Fleet-wise we require key ‚Äúrelease‚Äù devices to pass, and use a percentile-ish rule for the rest so one flaky/outlier chipset doesn‚Äôt block everything.\n\nOn patterns: it‚Äôs not uniform ‚Äî INT8 variance usually clusters around specific ops/kernels (backend differences) and calibration-sensitive layers, not the whole network.",
              "score": 1,
              "created_utc": "2026-02-16 23:50:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4dcq9",
      "title": "Transitioning into MLOps from API Gateway background ‚Äî looking for realistic paths & pitfalls",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "author": "EntropyTamer-007",
      "created_utc": "2026-02-14 06:26:58",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "Hi everyone,\n\nI‚Äôm looking for advice from people actually working in MLOps / ML platform roles, especially those who transitioned from non-ML backgrounds.\n\nMy current background (honest assessment):\n\n\\~4 years of experience working with Axway API Gateway\n\nMost of my work has been configuration-focused (policies in Policy Studio)\n\nI understand concepts like OAuth2, JWT, rate limiting, traffic mediation, etc., but mainly at a conceptual / tool-usage level\n\nI haven‚Äôt owned end-to-end systems, production ML pipelines, CI/CD, Kubernetes, or cloud infrastructure yet\n\nBeginner-level Python\n\nNo hands-on AWS/Azure/GCP or IaC experience so far\n\nSo while I‚Äôm not new to tech, I‚Äôm aware that my system ownership depth is limited.\n\nWhat I‚Äôm doing currently:\n\nI‚Äôm enrolled in a Data Science with Generative AI course\n\nI‚Äôm trying to avoid rushing into ‚ÄúML titles‚Äù without the necessary platform depth\n\nMy goal (longer-term):\n\nTransition into MLOps / ML Platform Engineering\n\nWork closer to model deployment, reliability, governance, and infrastructure, not pure research\n\nPrefer roles that are remote-friendly and have long-term growth\n\nFrom my background, \n\nwhat are the most realistic entry points into MLOps?\n\nIs it better to first transition into a Cloud / Platform / DevOps role and then move into MLOps, or are there viable direct bridges?\n\nWhich skills tend to be non-negotiable for MLOps roles that people often underestimate?\n\nWhat are common mistakes people make when trying to move into MLOps without prior ML ownership?\n\nIf you had to do this transition again, what would you focus on first vs ignore initially?\n\nI‚Äôm deliberately trying to avoid hype-driven decisions and would really value advice grounded in real hiring and on-the-job experience.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5bgie5",
          "author": "flyingPizza456",
          "text": "Common \"mistake\" (and mistake is not really the right word here) is to think that MLOps is something you do as a profession. It is like devops, it is about how to do things, not what things have to be done.\n\nIt is an approach, which still can be backed with specific tools, processes, skills, roles etc.\n\nYou do not transition into it in my opinion. It is more like you are an ML engineer or data scientist or infrastructure engineer or sulution architect or whatever. Even having a role named MLOps or Devops engineer is totally fine but your main tasks with regards to MLOps thinking is to support the reliability of services that are realized through Ai technologies (mostly ML as the name suggests).\n\nThen this comes down to you are doing machine learning or infrastructure work and you want to professionalize this even more.\n\nOne cannot say this often enough: MLOps is NOT FOR BEGINNERS\n\nDo all the other work that relates to it and after some years of experience you will realize what the important bits of the MLOps approach are. Also: MLOps is, like often times with other approaches, highly individual and has to be adapted for the setting / service / organization (factors could be size of service, number of people using the pipelines and many more)\n\nAnd: I really don't want to dampen your ambitions, but I think you'll find it easier if you don't view it as a dedicated profession, but rather as a task that must be fulfilled by everyone involved. That's why it's so difficult to introduce it into organisations and apply it successfully.",
          "score": 9,
          "created_utc": "2026-02-14 10:06:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jhd6s",
              "author": "EntropyTamer-007",
              "text": "Yes! Agree with you as I am learing about MLOps the common thing I found was same the MLOps is not for beginner and its not a profession. \nThe particular thing I was trying to figure out is having invested API Gateway (particularly Axway API gateway) what could be my way forwards and MLOps was one of the long term path that was in my options. \n\n\nThanks for you insights!!",
              "score": 1,
              "created_utc": "2026-02-15 17:38:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r8nnui",
      "title": "A 16-mode failure map for LLM / RAG pipelines (open source checklist)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r8nnui/a_16mode_failure_map_for_llm_rag_pipelines_open/",
      "author": "Over-Ad-6085",
      "created_utc": "2026-02-19 02:55:11",
      "score": 8,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "If you are running LLM / RAG / agent systems in production, this might be relevant. If you mostly work on classic ML training pipelines (tabular, CV etc.), this map probably does not match your day-to-day pain points.\n\nIn the last year I kept getting pulled into the same kind of fire drills: RAG pipelines that pass benchmarks, but behave strangely in real traffic. Agents that look fine in a notebook, then go off the rails in prod. Incidents where everyone says ‚Äúthe model hallucinated‚Äù, but nobody can agree what exactly failed.\n\nAfter enough of these, I tried to write down a **failure map** instead of one more checklist. The result is a **16-problem map for AI pipelines** that is now open source and used as my default language when I debug LLM systems.\n\nVery roughly, it is split by layers:\n\n* **Input & Retrieval \\[IN\\]** hallucination & chunk drift, semantic ‚â† embedding, debugging is a black box\n* **Reasoning & Planning \\[RE\\]** interpretation collapse, long-chain drift, logic collapse & recovery, creative freeze, symbolic collapse, philosophical recursion\n* **State & Context \\[ST\\]** memory breaks across sessions, entropy collapse, multi-agent chaos\n* **Infra & Deployment \\[OP\\]** bootstrap ordering, deployment deadlock, pre-deploy collapse\n* **Observability / Eval {OBS}** tags that mark ‚Äúthis breaks in ways you cannot see from a single request‚Äù\n* **Security / Language / OCR {SEC / LOC}** mainly cross-cutting concerns that show up as weird failure patterns\n\nThe 16 concrete problems look like this, in plain English:\n\n1. **hallucination & chunk drift** ‚Äì retrieval returns the wrong or irrelevant content\n2. **interpretation collapse** ‚Äì the chunk is right, but the logic built on top is wrong\n3. **long reasoning chains** ‚Äì the model drifts across multi-step tasks\n4. **bluffing / overconfidence** ‚Äì confident tone, unfounded answers\n5. **semantic ‚â† embedding** ‚Äì cosine match is high, true meaning is wrong\n6. **logic collapse & recovery** ‚Äì reasoning hits a dead end and needs a controlled reset\n7. **memory breaks across sessions** ‚Äì lost threads, no continuity between runs\n8. **debugging is a black box** ‚Äì you cannot see the failure path through the pipeline\n9. **entropy collapse** ‚Äì attention melts into one narrow path, no exploration\n10. **creative freeze** ‚Äì outputs become flat, literal, repetitive\n11. **symbolic collapse** ‚Äì abstract / logical / math style prompts break\n12. **philosophical recursion** ‚Äì self-reference loops and paradox traps\n13. **multi-agent chaos** ‚Äì agents overwrite or misalign each other‚Äôs roles and memories\n14. **bootstrap ordering** ‚Äì services fire before their dependencies are ready\n15. **deployment deadlock** ‚Äì circular waits inside infra or glue code\n16. **pre-deploy collapse** ‚Äì version skew or missing secret on the very first call\n\nEach item has its own page with:\n\n* how it typically shows up in logs and user reports\n* what people usually *think* is happening\n* what is actually happening under the hood\n* concrete mitigation ideas and test cases\n\nEverything lives in one public repo, under a single page:\n\n* **Full map + docs:** [https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md](https://github.com/onestardao/WFGY/blob/main/ProblemMap/README.md)\n\nThere is also a small helper I use when people send me long incident descriptions:\n\n* **‚ÄúDr. WFGY‚Äù triage link (ChatGPT share):** [https://chatgpt.com/share/68b9b7ad-51e4-8000-90ee-a25522da01d7](https://chatgpt.com/share/68b9b7ad-51e4-8000-90ee-a25522da01d7)\n\nYou paste your incident or pipeline description, and it tries to:\n\n1. guess which of the 16 modes are most likely involved\n2. point you to the relevant docs in the map\n\nIt is just a text-only helper built on top of the same open docs. No signup, no tracking, MIT license.\n\nOver time this map grew from my own notes into a public resource. The repo is sitting around \\~1.5k stars now, and several **awesome-AI / robustness / RAG** lists have added it as a reference for failure-mode taxonomies. That is nice, but my main goal here is to stress-test the taxonomy with people who actually own production systems.\n\nSo I am curious:\n\n* Which of these 16 do you see the most in your own incidents?\n* Is there a failure mode you hit often that is completely missing here?\n* If you already use some internal taxonomy or external framework for LLM failure modes, how does this compare?\n\nIf you end up trying the map or the triage link in a real postmortem or runbook, I would love to hear where it feels helpful, and where it feels wrong. The whole point is to make the language around ‚Äúwhat broke‚Äù a bit less vague for LLM / RAG pipelines.",
      "is_original_content": false,
      "link_flair_text": "Freemium :snoo_tableflip:",
      "permalink": "https://reddit.com/r/mlops/comments/1r8nnui/a_16mode_failure_map_for_llm_rag_pipelines_open/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r87p2u",
      "title": "The Human Elements of the AI Foundations",
      "subreddit": "mlops",
      "url": "https://metadataweekly.substack.com/p/the-human-elements-of-the-ai-foundations",
      "author": "growth_man",
      "created_utc": "2026-02-18 16:30:28",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 0.87,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r87p2u/the_human_elements_of_the_ai_foundations/",
      "domain": "metadataweekly.substack.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r6yai3",
      "title": "How deeply should an SRE understand PyTorch for ML production environments?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r6yai3/how_deeply_should_an_sre_understand_pytorch_for/",
      "author": "Simple-Toe20",
      "created_utc": "2026-02-17 06:10:03",
      "score": 6,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r6yai3/how_deeply_should_an_sre_understand_pytorch_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r923zw",
      "title": "Deploy ML Models Securely on K8s: KitOps + KServe Integration Guide",
      "subreddit": "mlops",
      "url": "https://youtu.be/0gXe_q458K4",
      "author": "iamjessew",
      "created_utc": "2026-02-19 15:26:42",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 0.86,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r923zw/deploy_ml_models_securely_on_k8s_kitops_kserve/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r4f6uw",
      "title": "Passed NVIDIA NCA-AIIO and now need Guidance for NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "author": "Sufficient_Berry_311",
      "created_utc": "2026-02-14 08:14:04",
      "score": 5,
      "num_comments": 6,
      "upvote_ratio": 0.86,
      "text": "Hello Everyone \n\nI had passed the NCA-AIIO on 12th Feb 2026. The questions are simple and you can pass the exam using your logic. You can ask me questions about the exam. I have used notebooklm for study, if you want I can give it also. \n\n  \nI need help to clear the NCP-AII. Is there any person here who cleared it. I wanted to know how hard is this and how many questions we need to solve in cli (provided in exam) or is there any lab related work?  \n  \nAny help from where I will get the lab access?\n\nThank you üôè  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5cw4cw",
          "author": "Fantastic-Chicken748",
          "text": "Hey!!! I‚Äôm also studying to ncp-aai. If you have any tips that you want to share, I would be very much appreciated",
          "score": 2,
          "created_utc": "2026-02-14 16:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dqe4c",
              "author": "Sufficient_Berry_311",
              "text": "I am from infrastructure side not from application side, cannot help on that. You can follow the nvidia study guide to notebooklm and you can ask question.\nNCA-AIIO is very easy, if you have knowledge how gpu works thats it.",
              "score": 2,
              "created_utc": "2026-02-14 18:37:23",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o66oc3k",
                  "author": "Significant-Fall6341",
                  "text": "I'm also an Infrastructure engineer. How difficult is NCP-AII given limited exposure to NVIDIA Infrastructure?\n\n",
                  "score": 1,
                  "created_utc": "2026-02-19 04:08:47",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o66o0ra",
          "author": "Significant-Fall6341",
          "text": "Please share the notebooklm. I'm planning to take the exam this weekend",
          "score": 1,
          "created_utc": "2026-02-19 04:06:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6dkvxq",
              "author": "Sufficient_Berry_311",
              "text": "[https://notebooklm.google.com/notebook/efdd849f-3d5b-462a-991a-b6dc51a4e39d](https://notebooklm.google.com/notebook/efdd849f-3d5b-462a-991a-b6dc51a4e39d)\n\n",
              "score": 1,
              "created_utc": "2026-02-20 05:59:31",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1ra4fky",
      "title": "Preparing for ML System Design Round (Fraud Detection / E-commerce Abuse) ‚Äì Need Guidance (4 Days Left)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "author": "SuccessfulStorm5342",
      "created_utc": "2026-02-20 19:09:16",
      "score": 5,
      "num_comments": 2,
      "upvote_ratio": 0.73,
      "text": "Hey everyone,\n\nI am a final year [B.Tech](http://B.Tech) student and I have an **ML System Design interview in 4 days** at a startup focused on **e-commerce fraud and return abuse detection**. They use ML for things like:\n\n* Detecting return fraud (e.g., customer buys a real item, returns a fake)\n* Multi-account detection / identity linking across emails, devices, IPs\n* Serial returner risk scoring\n* Coupon / bot abuse\n* Graph-based fraud detection and customer behavior risk scoring\n\nI have solid ML fundamentals but haven‚Äôt worked in fraud detection specifically. I‚Äôm trying to prep hard in the time I have.\n\n# What I‚Äôm looking for:\n\n**1. What are the most important topics I absolutely should not miss when preparing for this kind of interview?**  \nPlease prioritize.\n\n**2. Any good resources (blogs, papers, videos, courses)?**\n\n**3. Any advice on how to approach the preparation itself?**  \nAny guidance is appreciated.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1ra4fky/preparing_for_ml_system_design_round_fraud/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6hjgi1",
          "author": "Spare-Builder-355",
          "text": "if you are final year student, how you are supposed to know fraud detection domain if you never worked in one ? This is not public knowledge. There are no books or opensource projects on the topic.",
          "score": 2,
          "created_utc": "2026-02-20 20:32:02",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o6j8thz",
          "author": "DGSPJS",
          "text": "I used to be PM for an MLOps platform for fraud detection models.\n\nSome areas I'd stress are:  \nHandling highly imbalanced datasets - a company being absolutely battered by fraud is still only experiencing maybe a couple % of transactions as fraud and I've seen models deployed for 1:1,000,000 cases.\n\nModel retraining loops in the face of a delayed / irregular feedback loop (false positives might be worked out in minutes, false negatives can take months to be fully reported).\n\nModel optimization and threshold selection based on dollar value of transactions rather than number of transactions, and potentially accounting for the cost of frustrated customers with false positives.\n\nModel explainability techniques for understanding what types of fraud are being experienced and identifying if new types of attacks are emerging.\n\nGood luck.",
          "score": 1,
          "created_utc": "2026-02-21 02:11:57",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r88f78",
      "title": "From 40-minute builds to seconds: Why we stopped baking model weights into Docker images",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r88f78/from_40minute_builds_to_seconds_why_we_stopped/",
      "author": "No-Pay5841",
      "created_utc": "2026-02-18 16:55:48",
      "score": 4,
      "num_comments": 10,
      "upvote_ratio": 0.63,
      "text": "We‚Äôve all been there. You spend weeks tweaking hyperparameters, the validation loss finally drops, and you feel like a wizard. You wrap the model in a Docker container, push to the registry, and suddenly you‚Äôre just a plumber dealing with a clogged pipe.\n\nWe recently realized that treating ML models like standard microservices was killing our velocity. Specifically, the anti-pattern of baking gigabyte-sized weights directly into the Docker image (`COPY ./model_weights.pt /app/`).\n\nHere is why this destroys your pipeline and how we fixed it:\n\n**The Cache Trap:** Docker builds rely on layer caching. If you bundle code (KB) with weights (GB), you couple two artifacts with vastly different lifecycles.\n\n* Change one line of Python logging?\n* Docker invalidates the cache.\n* The CI runner re-copies, re-compresses, and re-uploads the entire 10GB blob.\n* **Result:** 40+ minute build times and autoscaling that lags so bad users leave before the pod boots.\n\n# Model-as-Artifact with Render\n\nWe decided to stop fighting the infrastructure and moved our stack to Render to implement the \"Model-as-Artifact\" pattern properly. Here‚Äôs how we decoupled the state (weights) from the logic (code):\n\n* **External Storage via Render Disks:** Instead of baking weights into the image, we store them on Render Persistent Disks. These are high-performance SSDs that stay attached to our instances even when the code changes.\n* **Decoupled Logic:** Our container now only holds the API code. When a build triggers on Render, it only has to package the lightweight Python environment, not the 10GB model.\n* **Smart Rollouts:** We used Render Blueprints to declaratively manage our GPU quotas and disk mounts. This ensures that every time we push to Git, the new code mounts the existing weight-filled disk instantly.\n* **Proper Probing:** We configured Render‚Äôs health checks to distinguish between the container starting and the model actually being loaded into VRAM, preventing \"zombie pods\" from hitting production.\n\n**The Results**\n\n* Build time: Dropped from \\~45 mins to <2 minutes.\n* Cold starts: Reduced to seconds using local NVMe caching on GPU nodes.\n* Cost: Stopped paying for idle GPUs while waiting for massive image pulls.\n\nI wrote a deeper dive on the architecture, specifically regarding Kubernetes probes and Docker BuildKit optimizations here: [https://engineersguide.substack.com/p/from-git-push-to-gpu-api-stop-baking](https://engineersguide.substack.com/p/from-git-push-to-gpu-api-stop-baking)",
      "is_original_content": false,
      "link_flair_text": "Tales From the Trenches :snoo_shrug:",
      "permalink": "https://reddit.com/r/mlops/comments/1r88f78/from_40minute_builds_to_seconds_why_we_stopped/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o63et27",
          "author": "NotSoGenius00",
          "text": "Idk why people copy model weights onto docker files ? üòÇ \n\nLike just use PVC on k8s or use s3 to stream your weights. \n\nPeople want instant results imagine running deepseek MOE and expecting your api to respond in second. \n\nPeople need to go back to school who expect such things to happen, it takes time to load the model ! There is IO (most of the people are either in leaderships or PM ) \n\nNo offence but the truth is truth",
          "score": 9,
          "created_utc": "2026-02-18 18:04:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63lw9y",
              "author": "No-Pay5841",
              "text": "Exactly. I think the 'baking weights' habit comes from people wanting that 'one-click' deployment convenience without realizing they‚Äôre killing their autoscaling.\n\nYou're right about the I/O bottleneck: waiting for a 50GB image pull is just a slow way to burn money. Moving to PVCs or S3-streaming isn't just an 'optimization' anymore; for LLMs/MoE, it‚Äôs the only way to actually stay operational.",
              "score": 1,
              "created_utc": "2026-02-18 18:35:22",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o64ky8d",
              "author": "KeyIsNull",
              "text": "I actually froze when I read op‚Äôs post: I lost count of the times I scolded my juniors during a code review for a baked model\n\nMaybe it‚Äôs me, I get upset even for cuda dependencies¬†\n",
              "score": 1,
              "created_utc": "2026-02-18 21:16:47",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o68je5p",
                  "author": "No-Pay5841",
                  "text": "I love that you‚Äôve avoided this! To be honest, our team started exactly there**.** In the early days, when models were only 100MB, `COPY ./weights` felt like the clean way to ensure reproducibility.\n\nIt wasn't until our models grew into the multi-gigabyte range and our CI/CD builds started becoming the bottleneck that we realized we‚Äôd accidentally built a monster. We had to break our own habits to get back our deployment speed. My post is basically a 'letter to my past self' to save others from that same headache.",
                  "score": 1,
                  "created_utc": "2026-02-19 13:20:05",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            },
            {
              "id": "o65lxy0",
              "author": "burntoutdev8291",
              "text": "AI generated post, probably AI generated problem then AI generated solution.",
              "score": 1,
              "created_utc": "2026-02-19 00:22:16",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o649gth",
          "author": "conditiosinequano",
          "text": "I agree, these things should not be in the image. \nThe example of changing a loc and getting a cache bust is a bit forced though:\n\nYou could just put the copy in an early layer.",
          "score": 2,
          "created_utc": "2026-02-18 20:23:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o64lg1k",
              "author": "KeyIsNull",
              "text": "Or simply mount as volume the HF, Ollama, whatever cache folder. Works even in ECS and it‚Äôs one of the lowest hanging fruits when doing cost and time optimisation¬†",
              "score": 1,
              "created_utc": "2026-02-18 21:19:02",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o685h0l",
          "author": "symphonicdev",
          "text": "My first reaction when reading this post is: Why? Why would anyone bake the model weights into their Docker image? I personally haven't seen anyone doing so.\n\nBut thank you for sharing this insight!",
          "score": 2,
          "created_utc": "2026-02-19 11:43:52",
          "is_submitter": false,
          "replies": [
            {
              "id": "o68jb9r",
              "author": "No-Pay5841",
              "text": "I love that you‚Äôve avoided this! To be honest, our team started exactly there**.** In the early days, when models were only 100MB, `COPY ./weights` felt like the clean way to ensure reproducibility.\n\nIt wasn't until our models grew into the multi-gigabyte range and our CI/CD builds started becoming the bottleneck that we realized we‚Äôd accidentally built a monster. We had to break our own habits to get back our deployment speed. My post is basically a 'letter to my past self' to save others from that same headache.",
              "score": 2,
              "created_utc": "2026-02-19 13:19:35",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o639rzx",
          "author": "[deleted]",
          "text": "[deleted]",
          "score": 1,
          "created_utc": "2026-02-18 17:42:39",
          "is_submitter": false,
          "replies": [
            {
              "id": "o63lisn",
              "author": "No-Pay5841",
              "text": "Multi-stage builds are great for stripping out build dependencies (like compilers or dev headers) to keep images slim, but they don't solve the '10GB weights' problem.\n\nEven with multiple stages, if your final `COPY` command includes a massive model file, you're still stuck with huge registry upload times and slow cold starts on your nodes. The goal here isn't just a clean Dockerfile, it‚Äôs getting that 10GB blob out of the deployment artifact entirely so your CI/CD can actually move at the speed of code.",
              "score": 1,
              "created_utc": "2026-02-18 18:33:44",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r68h1l",
      "title": "MLflow on Databricks End-to-End Tutorial | Experiments, Registry, Serving, Nested Runs",
      "subreddit": "mlops",
      "url": "https://youtu.be/9AenofD8GZ8",
      "author": "Remarkable_Nothing65",
      "created_utc": "2026-02-16 12:35:18",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 0.72,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r68h1l/mlflow_on_databricks_endtoend_tutorial/",
      "domain": "youtu.be",
      "is_self": false,
      "comments": [
        {
          "id": "o5pd2m7",
          "author": "penguinzb1",
          "text": "mlflow experiment tracking gets tricky when you're trying to validate model behavior under different data conditions. logging metrics is the easy part, but testing whether your registered model actually handles edge cases correctly is where most pipelines fall apart.\n\ncurious how you're handling regression testing for models in the registry. do you have automated checks that run when a new version gets registered, or is it more manual validation before moving to serving?\n\nthe nested runs setup is interesting for hyperparameter sweeps. we've been testing agents that optimize ml workflows and the hard part is catching when a sweep finds technically better metrics but the model actually performs worse on production-like scenarios.",
          "score": 2,
          "created_utc": "2026-02-16 16:13:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o6he0wg",
              "author": "Useful-Process9033",
              "text": "Edge case validation is where most ML pipelines silently fail. Logging metrics looks good in a demo but catching data drift or unexpected input distributions in production requires actual monitoring beyond what the registry gives you out of the box.",
              "score": 1,
              "created_utc": "2026-02-20 20:05:19",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r9fcau",
      "title": "Need Data for MLFlow Agent",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r9fcau/need_data_for_mlflow_agent/",
      "author": "lauptimus",
      "created_utc": "2026-02-19 23:39:59",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,   \nI'm working on a project involving making an agent that can interact with MLFlow logs and provide analysis and insights into experiment runs. So far, I've been using a bit of dummy data, but it would be great if anyone would help me understand where to get some real data from.  \nI don't have compute to run a lot of DL experiments. If anyone has any logs lying around, or knows where I can find some, I'd be grateful if they can share.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r9fcau/need_data_for_mlflow_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6c495h",
          "author": "data-intel-dev",
          "text": "For this case, maybe, you should use a Pycaret to create experiments easier. What do you think ?",
          "score": 1,
          "created_utc": "2026-02-20 00:12:28",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1ra6rff",
      "title": "OpenStack vs other entire stacks",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "author": "No-Fig-8614",
      "created_utc": "2026-02-20 20:37:21",
      "score": 3,
      "num_comments": 1,
      "upvote_ratio": 0.81,
      "text": "I've been looking around for the entire end to end stack for inference providing on hardware. There is OpenStack which gives a good end to end solution. I can't remember but there are others out there that have the entire end to end inference stack solution. Can anyone help me remember other stacks that are similar and opensource (even if they have the closed source add-ons for additional features). ",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1ra6rff/openstack_vs_other_entire_stacks/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o6hz043",
          "author": "gscjj",
          "text": "Lllm-d or Kserve?",
          "score": 1,
          "created_utc": "2026-02-20 21:49:07",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}