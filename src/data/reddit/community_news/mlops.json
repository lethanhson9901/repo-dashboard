{
  "metadata": {
    "last_updated": "2026-02-17 09:06:22",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 14,
    "total_comments": 40,
    "file_size_bytes": 68929
  },
  "items": [
    {
      "id": "1r3kxcd",
      "title": "What YouTube content actually helped you in your MLOps journey? And what's still confusing?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-13 09:37:26",
      "score": 35,
      "num_comments": 11,
      "upvote_ratio": 1.0,
      "text": "I've been in the ML/DevOps space for 11+ years and recently started doing 1:1 calls helping people transition into MLOps. One thing I keep noticing, almost everyone I talk to is overwhelmed.\n\nNot because they're not smart. But because MLOps is so vast, with batch vs. real-time ML pipelines, inference, infrastructure, and monitoring, every course teaches it differently. One guy will say start with Kubeflow, another says MLflow, another says forget tools, learn fundamentals first.\n\nI genuinely want to understand from this community:\n\n1. When you search MLOps on YouTube, what kind of videos do you actually watch fully? Tool-specific tutorials? Career roadmaps? Architecture walkthroughs?\n2. What's your biggest struggle right now ‚Äî is it picking the right tools? Understanding how pieces connect end to end? Or knowing what the market actually wants vs what courses teach?\n3. Is there a video or channel that genuinely helped you \"get it\"? Not just theory, but actually made something click?\n4. What's missing? What video do you wish existed but doesn't?\n\nAsking because I see so much content out there, but people on my calls are still confused. \n\nSomething is clearly not working. Curious what you all think.\n\nI've been thinking of creating some content on this myself, but before that, I just want to understand the current situation and where people are really stuck. No point in adding more noise if the real gap is elsewhere.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o557kiv",
          "author": "ConsciousML",
          "text": "Youtube was not very helpful on my side for this as there‚Äôs very little quality content in my opinion.\n\nWhat worked best for me is reading quality articles:\n- [ml-ops.org](https://ml-ops.org/) is great for the basics\n- [Neptune.ai](https://neptune.ai/blog) is great but they surf a lot on the GenAI wave so I don‚Äôt read much anymore\n- [ZenML](https://docs.zenml.io/stacks) has amazing doc for the MLOps components (experiment tracker, feature store, orchestrator, etc.)\n- [Hopsworks](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines) explains the three pipeline paradigm better than anyone else\n\n\nOnce you know the theory well and you had some good hands-on experience, I‚Äôve found that the big tech engineering blogs are the best source of trusted information.\n\nI‚Äôve compiled a list of [interesting blogs](https://foremost-tea-3e3.notion.site/Resources-1973205f20be80d6923bd4a18ee62cd6?source=copy_link).\n\nI also try my best to share useful content on my [personal blog](https://www.axelmendoza.com/) if that‚Äôs helpful!\n\nI‚Äôm really interested in your process to try helping people getting into MLOps.\n\nDM me if you want to share some thoughts!",
          "score": 26,
          "created_utc": "2026-02-13 11:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56d3gt",
              "author": "Neither_Film_8641",
              "text": "I like your Blog!",
              "score": 3,
              "created_utc": "2026-02-13 15:17:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56mm3h",
                  "author": "ConsciousML",
                  "text": "Thanks man! I enjoy to write. Even better if it‚Äôs helpful ;)",
                  "score": 1,
                  "created_utc": "2026-02-13 16:02:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55w9ul",
          "author": "TruDanceCat",
          "text": "DataCamp is my go-to. Tons of great courses on ML and MLOPS. Theory videos, coding exercises, labs, and practice quizzes all based on career tracks like ML, MLOps, Data Scientist, Data Analytics, Data Engineering and more.\n\nWe have a learning budget at work, so they reimburse us for the cost, but I would pay for the subscription myself if they didn‚Äôt- it‚Äôs totally worth it.",
          "score": 5,
          "created_utc": "2026-02-13 13:50:05",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dmkaw",
              "author": "JayRathod3497",
              "text": "What's a subscription piece ?",
              "score": 1,
              "created_utc": "2026-02-14 18:18:39",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o55nfka",
          "author": "DifficultDifficulty",
          "text": "I found AWS/GCP tech blogs and OSS repos useful, particularly those laying out architecture blueprints",
          "score": 2,
          "created_utc": "2026-02-13 12:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56sp6s",
          "author": "Inevitable_Resort902",
          "text": "I am a SWE looking to transition into MLOps and would love to get some advice. Can I get in touch with you?",
          "score": 1,
          "created_utc": "2026-02-13 16:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5739xf",
              "author": "Extension_Key_5970",
              "text": "sure, you can DM me",
              "score": 1,
              "created_utc": "2026-02-13 17:22:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59do9g",
          "author": "Ambitious-Estate4356",
          "text": "I would say claude learning + claude code. They try to make you learn each and every concepts from ground up with some example and code.",
          "score": 1,
          "created_utc": "2026-02-14 00:25:01",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5g5zzf",
              "author": "CupFine8373",
              "text": "what is claude learning ? ",
              "score": 1,
              "created_utc": "2026-02-15 03:12:24",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k2vpf",
          "author": "Gaussianperson",
          "text": "The biggest gap I see is that most content teaches MLOps tool-by-tool (\"here's MLflow\", \"here's Kubeflow\") but almost nobody explains how real production ML systems are actually designed end-to-end: the trade-offs, the architecture decisions, why companies pick certain patterns over others.\n\nI also feel like youtube videos are too long...\n\nThat's what I try to cover in my newsletter Machine Learning at Scale production ML patterns, system design deep dives (rec systems, ads, serving infra), and how things actually work inside big tech.\n\nWritten from the perspective of someone building these systems daily, not teaching them in a course: [https://machinelearningatscale.substack.com/](https://machinelearningatscale.substack.com/)",
          "score": 1,
          "created_utc": "2026-02-15 19:23:24",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1hcgr",
      "title": "What's your Production ML infrastructure in 2026?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "author": "Repulsive_Ad_9950",
      "created_utc": "2026-02-10 23:46:35",
      "score": 28,
      "num_comments": 14,
      "upvote_ratio": 0.97,
      "text": "I'm currently studying the tools generally associated with MLOps. Some stuff seem to be non-negotiable: Cloud provides like AWS, GCP and Azure, Kubernetes, Docker, CI/CD and monitoring/observability. I'd like to hear about the tooling your company use to handle ML workflows, so I can have some direction in my studies. Here are my questions. \n\n**CI/CD**  \nGithub Actions, GitLab or other? Do you use different CI/CD tools depending for training and deployment?\n\n**Orchestration for training models**  \nWhat actually runs your training jobs? Airflow, Prefect, Kubeflow Pipelines, Argo, or something else?  \nHow does the flow work? For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow?  \n\n\n**Serving**  \nAre your inference endpoints deployed with FastAPI, KServe or other (like Lambda)? I heard that KServe has the advantage of batching requests, which is more compute-efficient for fetching data from database, feature engineering, and making predictions, and as well as some automated A/B and canary deployments, which seems a big advantage to me. \n\n**Monitoring and Observability**  \nCloud-nativa services like CloudWatch or Prometheus+Grafana?\n\n**Integrated or Scattered?**  \nAll-in on one platform (Kubeflow end-to-end, or everything in SageMaker), or scattered (Airflow/Prefect, Kubernetes, etc)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4qvk1q",
          "author": "pmv143",
          "text": "For serving LLMs at scale, the stack starts to diverge from traditional ML serving pretty quickly.\n\nFastAPI or KServe work fine for classic models, but large LLM inference introduces different constraints:\n\n‚Äì GPU residency and memory fragmentation\n‚Äì cold start latency\n‚Äì multi model scheduling on shared GPUs\n‚Äì batching vs interactive latency tradeoffs\n\nIn our case we ended up building a snapshot based runtime specifically for inference. Instead of treating endpoints as long lived pods, we snapshot the fully initialized CUDA graph and restore directly into GPU memory. That lets us scale to zero without warm pools and still keep cold start under ~2 seconds for 70B class models.\n\nMonitoring wise we still rely on Prometheus style metrics and GPU level telemetry, but the core efficiency gains come from how the runtime manages GPU state rather than from orchestration layers.\n\nThe training stack can stay fairly conventional. Inference is where the architecture really changes.",
          "score": 10,
          "created_utc": "2026-02-11 04:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rrcin",
              "author": "elsatan666",
              "text": "That snapshot approach sounds very cool, was that something you built yourselves or is it off the shelf?",
              "score": 1,
              "created_utc": "2026-02-11 09:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4s8275",
                  "author": "pmv143",
                  "text": "We built it ourselves.\n\nThere isn‚Äôt really an off-the-shelf solution that snapshots a fully initialized CUDA graph and restores it directly into GPU memory for large LLMs.\n\nContainer images snapshot disk state.\nCheckpointing saves weights.\nBut neither preserves the full runtime state required for fast GPU restore.\n\nThis took several years of work around GPU memory management, CUDA initialization, and state restoration. It‚Äôs very inference-specific. We didn‚Äôt touch the training stack much. The real complexity was managing GPU residency and multi-model state transitions safely.\n\nStill early, but it‚Äôs been working well for 30B‚Äì300B class models so far.",
                  "score": 2,
                  "created_utc": "2026-02-11 11:44:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ppj8q",
          "author": "ixrequalv",
          "text": "GitHub, harness for cicd\nSagemaker for everything it can, also bedrock and lambdas / other AWS services \nPrometheus cloud watch and grafana",
          "score": 3,
          "created_utc": "2026-02-11 00:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qu00i",
          "author": "Scared_Astronaut9377",
          "text": "\nLet me better describe what makes sense and what makes less sense. \n\n> Some stuff seem to be non-negotiable: Cloud provides\n\nI mean, there are many jobs that are MLOps in nature that require Spark, Flink, etc. instead of clouds. But they typically hire Data Engineers to do that lol.\n\n> Github Actions, GitLab or other? \n\nBoth work + many other building tools including those in major clouds.\n\n> For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow? \n\nBoth work, action -> airflow -> training job is a very solid pattern for many use cases.\n\n> Are your inference endpoints deployed with FastAPI, KServe\n\nDepends on the requirements and the framework. Sometimes just throwing things in FastAPI is ok, sometimes you want, say, tf serving. I wouldn't touch lambda, always make your own containers. KServe is indeed solid for many use cases.\n\n> Cloud-nativa services like CloudWatch or Prometheus+Grafana?\n\nMostly what the company is already using lol. If I am designing from scratch, yeah, prometheus+grafana. \n\n> Kubeflow end-to-end\n\nKubeflow pipelines are good. Kubeflow as a platform is garbage. \n\n> everything in SageMaker\n\nWhen I was new to MLOps in cloud. Never again. First-class sitizen cloud services + k8s.",
          "score": 2,
          "created_utc": "2026-02-11 04:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xy6fx",
          "author": "NoobZik",
          "text": "Kedro + MLFlow + Airflow + NannyML\nScattered but cloud agnostic\n\nFor inference I‚Äôm using FastAPI",
          "score": 1,
          "created_utc": "2026-02-12 07:04:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r52ypf",
      "title": "Practical SageMaker + MLflow Stage/Prod Workflow for Small MLOps + DS Team?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "author": "ZeroSilver87",
      "created_utc": "2026-02-15 02:25:32",
      "score": 20,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Hey all ‚Äî As the title says, looking for practical input from teams operating at a similar scale...\n\nWe have a small MLOps team supporting a small Data Science team... \\~4-6 per team. We‚Äôre enabling SageMaker + MLflow this year and trying to move toward more sustainable, repeatable ML workflows.\n\nHistorically, our ML efforts have been fairly ad hoc and home-grown. We‚Äôre now trying to formalize things and improve R&D velocity without overburdening either the DS team or our platform engineers.\n\nOne major constraint is that our DevOps/infra process is heavily gated. New AWS resources require approvals outside our teams and move slowly. So we‚Äôre trying to design something clean and safe that doesn‚Äôt require frequent new infrastructure or heavyweight process for each new model.\n\nI‚Äôm aware of the AWS-recommended workflows, but they seem optimized for larger teams or environments with more autonomy than we have.\n\nSome Additional Context:\n\n* Data lake on S3 (queried via Athena)\n* Models are often entity-specific (i.e., many model instances derived from a shared training pipeline)\n\nCurrent thinking:\n\n* Non-Prod:\n   * EDA + pipeline development + model experimentation\n   * read-only access to prod archive data to remove need to set up complicated replication from prod to non-prod\n* Prod:\n   * Inference endpoints\n   * Single managed MLflow workspace\n      * DS can log runs + register models (from non-prod or local)\n      * Only a prod automation role can promote models to ‚ÄúProduction‚Äù\n      * Production Inference services only load models marked \"Production\"\n   * Automated retraining pipelines\n\nThoughts or suggestions on this setup?\n\nThe goal is to embed sustainable workflows and guardrails without turning this into a setup that requires large teams to support it.\n\nWould love to hear what‚Äôs worked (or failed) for teams in similar size ranges or if you have any good experience with AWS Sagemaker to suggest good workflows.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r52ypf/practical_sagemaker_mlflow_stageprod_workflow_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5hxwfs",
          "author": "Bezza100",
          "text": "It seems good, you will need to invest significant time on the CI/CD for promoting to make sure it's robust. Also consider standard examples and templates for the DS team so there isn't too much refactoring to use your CI/CD.",
          "score": 3,
          "created_utc": "2026-02-15 12:34:10",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5lpux4",
              "author": "ZeroSilver87",
              "text": "Good points. Yes I‚Äôm not sure exactly what the CI/CD will look like for this but some templates to make it faster is a good idea",
              "score": 1,
              "created_utc": "2026-02-16 00:41:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5qki7y",
          "author": "Gaussianperson",
          "text": "You should definitely use the MLflow Model Registry as the central hub for model handoffs. When a data scientist is happy with a model, they register it in MLflow, and that acts as the signal for your staging or production pipelines to pick it up and deploy it to a SageMaker endpoint. This keeps the roles clear and avoids a lot of back and forth between the teams.  \n  \nOne thing to watch out for is over engineering your internal tools too soon.\n\nStick to the native MLflow and SageMaker APIs before building complex wrappers.\n\nIf you are looking for more examples of how different companies handle these kinds of architectural choices, check out machinelearningatscale.substack.com. (author here)\n\nIt has some solid case studies on how bigger shops manage their production infrastructure which can give you some good ideas for your own roadmap.",
          "score": 2,
          "created_utc": "2026-02-16 19:35:09",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5qnu67",
              "author": "ZeroSilver87",
              "text": "Excellent, thanks for the advice. Agree on not over engineering too soon. We may have some hurdles to creating SageMaker endpoints on the fly via CI/CD but I think there is a reasonable work around with MME since they feed off an S3 bucket‚Ä¶ assuming we can maintain a consistent set of requirements.",
              "score": 1,
              "created_utc": "2026-02-16 19:51:24",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r312b5",
      "title": "The agent security landscape is kind of a mess and I'm not sure what to do about it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "author": "Exact-Literature-395",
      "created_utc": "2026-02-12 18:21:21",
      "score": 15,
      "num_comments": 4,
      "upvote_ratio": 0.93,
      "text": "So my team has been pushing me to evaluate autonomous agents for some of our workflow automation. Specifically looking at OpenClaw since it has massive traction (something like 160k+ GitHub stars) and can connect LLMs to local files, browsers, Slack, Discord, etc. Our ops lead is really excited about using it to auto-triage the \\~200 support tickets we get daily, basically having it read incoming tickets, check our internal docs, and route them to the right team with a priority score. Also been talking about automating the data validation checks we run every Monday where someone manually compares CSV exports against our postgres tables. Tedious stuff that would be perfect for an agent.\n\nBut honestly? The more I dig into this, the more I want to pump the brakes.\n\nI stumbled across some security research that genuinely unsettled me. Apparently there are tens of thousands of OpenClaw instances just... exposed directly to the internet. But the number that really stopped me was this: something like 15% of community built skills contain malicious instructions. Prompts designed to download malware or steal data. And when these get flagged and removed, they apparently just reappear under new identities pretty quickly.\n\nThe project's own FAQ literally describes this as a \"Faustian bargain\" with no \"perfectly safe\" setup. I appreciate the honesty but also... what am I supposed to do with that? How do I bring this to my team without sounding like I'm just being obstructionist?\n\nWhat's frustrating from an MLOps perspective is that this completely changes how I think about threat modeling. We've spent so much time worrying about model poisoning, adversarial inputs, data drift. With agents though, the attack surface just explodes. Prompt injection could come through any email or webpage the agent processes. If someone compromises the agent itself they basically inherit every permission we've granted it. And the plugin ecosystem? Nobody has time to audit all that, but you're essentially running untrusted code with access to your systems.\n\nThere's also this concept I keep seeing called \"judgment hallucination\" where the agent appears trustworthy but lacks genuine reasoning, so users just... hand over more and more authority. That one hits different because I can already see how it would play out with some of the less technical folks on our team who already treat ChatGPT like its omniscient.\n\nI looked at some alternatives like AutoGPT and BabyAGI but they seem to have similar issues, maybe even less mature from a security standpoint. A coworker mentioned something called Agent Trust Hub that supposedly scans skills for hidden logic and data exfiltration patterns before you install them, still need to actually try it though. The usual advice seems to be run everything in containers, dont expose default ports, start with read only permissions and expand from there. Basically treat it like you would any untrusted code I guess.\n\nBut I'm genuinely torn. The capability is exciting and I get why leadership wants this. The current state of the ecosystem though... it feels like we'd be taking on a lot of risk that we're not equiped to manage yet. Maybe I'm being too conservative here.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o53p7au",
          "author": "KarmaIssues",
          "text": "I suppose the only real change would have to be sandboxing it. That would atleast help with some of the more damaging actions.",
          "score": 1,
          "created_utc": "2026-02-13 03:27:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5aecc4",
          "author": "South-Opening-9720",
          "text": "You're not being conservative ‚Äî you're threat-modeling correctly. I'd start with an 'assistive' agent first (summarize + classify + suggest KB links), keep it read-only, and lock it in a sandbox with tight egress + allowlisted tools; actions/handoffs only after evals + red-team prompt injection tests. I use chat data for the safer 'draft + route' step and it helps without giving it keys to the kingdom. What systems would it touch on day 1?",
          "score": 1,
          "created_utc": "2026-02-14 04:23:58",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5fboic",
          "author": "CompelledComa35",
          "text": "You're right to pump the brakes. That 15% malicious skill rate is insane and judgment hallucination is a real problem. Sandboxing helps but doesn't solve prompt injection through emails/web content. Check out alice's caterpillar for agentic scanning, it surfaces these attack vectors before production. Run it against your planned workflows and show leadership the actual risk exposure with evidence. ",
          "score": 1,
          "created_utc": "2026-02-14 23:53:44",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5pfpy1",
          "author": "penguinzb1",
          "text": "you're not being too conservative, you're being realistic. the 15% malicious plugin rate is wild and the \"faustian bargain\" framing from the project itself should tell you everything you need to know about production readiness.\n\nthe thing that jumps out to me is that you're thinking about this correctly but the solution isn't just sandboxing or read-only permissions. those help, but they don't address the core problem which is that you have no way to validate agent behavior before it hits production. if someone compromises a plugin or the agent hallucinates a bad judgment call, you won't know until it's already happened.\n\nwhat you actually need is a way to test failure scenarios before deployment. run simulations where the agent processes malicious emails, gets fed prompt injection attempts through support tickets, or encounters plugins that try to exfiltrate data. see if your sandboxing actually catches it, or if the agent routes something incorrectly because it got confused by adversarial input.\n\nthe \"assistive agent\" approach someone mentioned is smart, keep it read-only and limit blast radius. but even that needs validation. you can't just hope the agent summarizes tickets correctly or routes them to the right team. you need to inject edge cases and see where it breaks. what happens when a ticket contains sql injection attempts disguised as support questions? what if someone writes a ticket specifically designed to trick the agent into misclassifying priority?\n\nfor the monday data validation checks, same thing. test scenarios where the csv and postgres deliberately disagree, or where the data has been subtly corrupted. does the agent catch it, or does it just report everything looks fine because it's operating on vibes?\n\nthe judgment hallucination thing is real and it's exactly what makes this hard. people will trust the agent more than they should because it sounds confident. the only way to combat that is to have evidence that the agent actually behaves correctly under adversarial conditions, not just happy path demos.\n\ncurious what your current testing strategy looks like for the rest of your mlops stack. if you're already doing adversarial testing on models, this is basically the same mindset but applied to agent workflows instead of model outputs.",
          "score": 1,
          "created_utc": "2026-02-16 16:26:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1tlnt",
      "title": "Need some suggestions on using Open-source MLops Tool",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "author": "NetFew2299",
      "created_utc": "2026-02-11 10:13:22",
      "score": 10,
      "num_comments": 14,
      "upvote_ratio": 0.92,
      "text": "I am a Data scientist by Profession. For a project, I need to setup a ML Infrastructure in a local VM. I  am working on  A daily prediction /timeseries analysis. In the case of Open-Source, I have heard good things about ClearML (there are others, such as ZenML/MLrun), to my [knowledge.It](http://knowledge.It) is simply because it offers a complete MLops solution\n\nApart from this, I know I can use a combination of Mlflow, Prefect, Evidently AI, Feast, Grafana, as well. I want suggestions in case of ClearML, if any, on ease of use. Most of the Softwares claim, but I need your feedback.\n\nI am open to using paid solutions as well. My major concerns:\n\n1. Infrastructure cannot run on the cloud\n2. Data versioning\n3. Reproducible Experiment\n4. Tracking of the experiment\n5. Visualisation of experiment\n6. Shadow deployment\n7. Data drift",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4up367",
          "author": "niek29",
          "text": "Hey! This is pretty much the exact problem we‚Äôre building LUML to solve.\n\nhttps://github.com/luml-ai/luml\n\nWe already have experiment tracking and a deployment module you can self-host wherever you want, so the no-cloud constraint isn‚Äôt a problem. We‚Äôre also building a new MLflow-like module with an easy transition to the full platform - centralized registry, deployments, and monitoring, all out of the box. Data drift monitoring is actively in development too.\n\nWe‚Äôre onboarding early users right now and your use case is exactly what we‚Äôre designing for. Happy to jump on a quick call to walk you through it, DM me if you‚Äôre interested!",
          "score": 2,
          "created_utc": "2026-02-11 19:26:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s4xi3",
          "author": "kayhai",
          "text": "It sounds like you are keen on ClearML. I‚Äôve tried ML Flow (model registry and experiment tracking) + a scheduling tool of your choice (prefect, airflow or dagster etc). I‚Äôm not familiar with ClearML, may I ask which features of ClearML stands out to you?",
          "score": 1,
          "created_utc": "2026-02-11 11:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sb0a9",
              "author": "NetFew2299",
              "text": "It is simply because it offers a complete MLops solution.\n\n",
              "score": 2,
              "created_utc": "2026-02-11 12:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sbcu1",
          "author": "Fritos121",
          "text": "This is almost exactly what I came here looking for. A lot of focus on Cloud, but it‚Äôs been a bit harder for me to find resources on how best to deploy locally. Thanks for asking the question!",
          "score": 1,
          "created_utc": "2026-02-11 12:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sqchp",
          "author": "Garbatronix",
          "text": "I have had positive experiences using LakeFS in conjunction with MinIO. It enables you to version data in a similar way to Git. With an MLFlow server, I can log all the relevant parameters, such as branch and ref. MLFlow enables models to be versioned and stored. An MLFlow Docker image can then be generated and easily deployed on a Docker host or Kubernetes. \n\nDrift detection and data visualisation can be implemented in Python scripts prior to training and stored as artefacts in MLFlow. I have created a custom Python model in MLFlow by generating my own Prometheus metrics. These can then be collected via Prometheus and visualised in Grafana.",
          "score": 1,
          "created_utc": "2026-02-11 13:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sru98",
          "author": "DifficultDifficulty",
          "text": "\"I need to setup a ML infrastructure in a local VM\" -> is this infra mostly for your own VM-local experiments, and is there no need to distribute workloads in the cloud where the infra would be shared by multiple team members?",
          "score": 1,
          "created_utc": "2026-02-11 13:52:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4qym",
              "author": "NetFew2299",
              "text": "No,, I don't need it for multiple teams....I just need to setup an API, currently being done with flask later being changed to fastapi.",
              "score": 1,
              "created_utc": "2026-02-12 13:15:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4z8nw7",
                  "author": "DifficultDifficulty",
                  "text": "I see. I've spoken to a few people who described a similar need to yours, and they spoke well about Kedro + MLFlow for this kind of VM-local experience. Please see [https://docs.kedro.org/en/stable/integrations-and-plugins/mlflow/](https://docs.kedro.org/en/stable/integrations-and-plugins/mlflow/) ",
                  "score": 1,
                  "created_utc": "2026-02-12 13:38:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yubra",
          "author": "SassFrog",
          "text": "My suggestion is that in order to use ClearML, or another tool reliably you likely need kubernetes set up beneath it (or another scheduler, which I'd recommend against). Once you have kubernetes you're 90% of the way there, you can easily delploy ClearML through a helmchart, another component besides ClearML or you could replace ClearML.",
          "score": 1,
          "created_utc": "2026-02-12 12:04:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tvtv",
          "author": "Iron-Over",
          "text": "You are missing explainability, understand which features determined the decision.¬†\n\nA prediction store where you can store every prediction to map to an actual result down the road, which helps you create lots of labeled data for future training.¬†\n\nYou may want a feature store.¬†\n\nModel registry is highly recommended.¬†\n\nNot sure if you need bias checks.¬†\n\nAre you serving via batch or api?",
          "score": 1,
          "created_utc": "2026-02-12 18:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54ig61",
              "author": "NetFew2299",
              "text": "Serving via batch¬†",
              "score": 1,
              "created_utc": "2026-02-13 07:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54h4zu",
          "author": "NoobZik",
          "text": "My stack cloud agnostic \nKedro, MLFlow, Airflow. \n\nMinio is dead actually so I shifted to rustfs",
          "score": 1,
          "created_utc": "2026-02-13 06:56:43",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5oqp4f",
              "author": "NetFew2299",
              "text": "How difficult was it to set up? Why do u need Kedro? Can you please guide me?",
              "score": 1,
              "created_utc": "2026-02-16 14:22:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4dcq9",
      "title": "Transitioning into MLOps from API Gateway background ‚Äî looking for realistic paths & pitfalls",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "author": "EntropyTamer-007",
      "created_utc": "2026-02-14 06:26:58",
      "score": 8,
      "num_comments": 2,
      "upvote_ratio": 0.83,
      "text": "Hi everyone,\n\nI‚Äôm looking for advice from people actually working in MLOps / ML platform roles, especially those who transitioned from non-ML backgrounds.\n\nMy current background (honest assessment):\n\n\\~4 years of experience working with Axway API Gateway\n\nMost of my work has been configuration-focused (policies in Policy Studio)\n\nI understand concepts like OAuth2, JWT, rate limiting, traffic mediation, etc., but mainly at a conceptual / tool-usage level\n\nI haven‚Äôt owned end-to-end systems, production ML pipelines, CI/CD, Kubernetes, or cloud infrastructure yet\n\nBeginner-level Python\n\nNo hands-on AWS/Azure/GCP or IaC experience so far\n\nSo while I‚Äôm not new to tech, I‚Äôm aware that my system ownership depth is limited.\n\nWhat I‚Äôm doing currently:\n\nI‚Äôm enrolled in a Data Science with Generative AI course\n\nI‚Äôm trying to avoid rushing into ‚ÄúML titles‚Äù without the necessary platform depth\n\nMy goal (longer-term):\n\nTransition into MLOps / ML Platform Engineering\n\nWork closer to model deployment, reliability, governance, and infrastructure, not pure research\n\nPrefer roles that are remote-friendly and have long-term growth\n\nFrom my background, \n\nwhat are the most realistic entry points into MLOps?\n\nIs it better to first transition into a Cloud / Platform / DevOps role and then move into MLOps, or are there viable direct bridges?\n\nWhich skills tend to be non-negotiable for MLOps roles that people often underestimate?\n\nWhat are common mistakes people make when trying to move into MLOps without prior ML ownership?\n\nIf you had to do this transition again, what would you focus on first vs ignore initially?\n\nI‚Äôm deliberately trying to avoid hype-driven decisions and would really value advice grounded in real hiring and on-the-job experience.\n\nThanks in advance.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4dcq9/transitioning_into_mlops_from_api_gateway/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5bgie5",
          "author": "flyingPizza456",
          "text": "Common \"mistake\" (and mistake is not really the right word here) is to think that MLOps is something you do as a profession. It is like devops, it is about how to do things, not what things have to be done.\n\nIt is an approach, which still can be backed with specific tools, processes, skills, roles etc.\n\nYou do not transition into it in my opinion. It is more like you are an ML engineer or data scientist or infrastructure engineer or sulution architect or whatever. Even having a role named MLOps or Devops engineer is totally fine but your main tasks with regards to MLOps thinking is to support the reliability of services that are realized through Ai technologies (mostly ML as the name suggests).\n\nThen this comes down to you are doing machine learning or infrastructure work and you want to professionalize this even more.\n\nOne cannot say this often enough: MLOps is NOT FOR BEGINNERS\n\nDo all the other work that relates to it and after some years of experience you will realize what the important bits of the MLOps approach are. Also: MLOps is, like often times with other approaches, highly individual and has to be adapted for the setting / service / organization (factors could be size of service, number of people using the pipelines and many more)\n\nAnd: I really don't want to dampen your ambitions, but I think you'll find it easier if you don't view it as a dedicated profession, but rather as a task that must be fulfilled by everyone involved. That's why it's so difficult to introduce it into organisations and apply it successfully.",
          "score": 8,
          "created_utc": "2026-02-14 10:06:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5jhd6s",
              "author": "EntropyTamer-007",
              "text": "Yes! Agree with you as I am learing about MLOps the common thing I found was same the MLOps is not for beginner and its not a profession. \nThe particular thing I was trying to figure out is having invested API Gateway (particularly Axway API gateway) what could be my way forwards and MLOps was one of the long term path that was in my options. \n\n\nThanks for you insights!!",
              "score": 1,
              "created_utc": "2026-02-15 17:38:51",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r64jfl",
      "title": "We built hardware-in-the-loop regression gates for AI models on Snapdragon ‚Äî here's what we learned",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r64jfl/we_built_hardwareintheloop_regression_gates_for/",
      "author": "NoAdministration6906",
      "created_utc": "2026-02-16 08:49:34",
      "score": 8,
      "num_comments": 4,
      "upvote_ratio": 0.85,
      "text": "We deploy AI models to Snapdragon devices and got tired of cloud tests passing while real hardware failed. Built a CI tool that runs your model on physical Snapdragon devices and blocks the PR if gates fail.\n\nBiggest surprise: same INT8 model showed 23% accuracy variance across 5 Snapdragon chipsets. Cloud benchmarks predicted none of this.\n\nFull disclosure: I built this (EdgeGate). Happy to answer questions about the architecture or edge AI testing in general.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r64jfl/we_built_hardwareintheloop_regression_gates_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5ovn0i",
          "author": "Comfortable_Holiday3",
          "text": "Looks good. I built something similar: an RPC with telemetry that also supports non-OS hardware abstracting the physical layer (USB/OTA streaming) and the model runtime/engine/interpreter. Interested in the architecture. Curiously, what do you think is the cause of the large accuracy variance? Is it something related to optimized kernel implementation in the Snapdragon?",
          "score": 1,
          "created_utc": "2026-02-16 14:49:03",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5p2vg6",
              "author": "NoAdministration6906",
              "text": "Thanks! Your setup sounds really cool ‚Äî abstracting the physical layer for non-OS hardware is no joke.\n\nOur approach is more cloud-first ‚Äî we go through Qualcomm AI Hub's API for compilation and on-device execution, then wrap results in signed evidence bundles so CI gates can't be spoofed. Think \"unit tests but for model quality on real hardware.\"\n\nOn the accuracy variance ‚Äî I think you're on the right track with the kernel implementations. Different Hexagon NPU generations (v69, v73, v75) likely use different fixed-point arithmetic paths. But it's probably a mix of that plus compiler-level graph optimizations varying per target, and possibly quantization calibration differences during compilation. We're also not pinning the runtime library version across devices yet, so that's another uncontrolled variable.\n\nHonestly, isolating *which* of these contributes *how much* is half the reason we built this. Would love to compare notes on what you've seen on the bare-metal side.",
              "score": 2,
              "created_utc": "2026-02-16 15:25:46",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5p5gg6",
          "author": "penguinzb1",
          "text": "the 23% variance is wild. i've been thinking about hardware-in-the-loop testing for agents that need to run on different devices, and this kind of variance is exactly what makes it hard to trust cloud benchmarks for anything that ships to physical hardware.\n\ncurious how you're handling the gate thresholds across different chipsets. do you set per-device accuracy targets, or do you have a single gate that accounts for the worst-case variance? seems like the latter would be overly conservative but the former creates a maintenance nightmare.\n\nalso wondering if you're seeing variance patterns that correlate with specific model architectures or quantization approaches. like, does the variance show up more in certain layer types or is it pretty uniform across the whole model?",
          "score": 1,
          "created_utc": "2026-02-16 15:38:17",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5ryosw",
              "author": "NoAdministration6906",
              "text": "Yeah, we ran into the same tradeoff. We don‚Äôt do ‚Äúworst-case single gate‚Äù ‚Äî it‚Äôs too conservative.\n\nWe keep per-chipset baselines (golden outputs/metrics) and gate on regression vs that device + an absolute floor. Fleet-wise we require key ‚Äúrelease‚Äù devices to pass, and use a percentile-ish rule for the rest so one flaky/outlier chipset doesn‚Äôt block everything.\n\nOn patterns: it‚Äôs not uniform ‚Äî INT8 variance usually clusters around specific ops/kernels (backend differences) and calibration-sensitive layers, not the whole network.",
              "score": 1,
              "created_utc": "2026-02-16 23:50:10",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1frz2",
      "title": "Learning AI deployment & MLOps (AWS/GCP/Azure). How would you approach jobs & interviews in this space?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "author": "c0bitz",
      "created_utc": "2026-02-10 22:44:19",
      "score": 7,
      "num_comments": 15,
      "upvote_ratio": 0.89,
      "text": "I‚Äôm currently learning how to deploy AI systems into production. This includes deploying LLM-based services to AWS, GCP, Azure and Vercel, working with MLOps, RAG, agents, Bedrock, SageMaker, as well as topics like observability, security and scalability.\n\nMy longer-term goal is to build my own AI SaaS. In the nearer term, I‚Äôm also considering getting a job to gain hands-on experience with real production systems.\n\nI‚Äôd appreciate some advice from people who already work in this space:\n\nWhat roles would make the most sense to look at with this kind of skill set (AI engineer, backend-focused roles, MLOps, or something else)?\n\nDuring interviews, what tends to matter more in practice: system design, cloud and infrastructure knowledge, or coding tasks?\n\nWhat types of projects are usually the most useful to show during interviews (a small SaaS, demos, or more infrastructure-focused repositories)?\n\nAre there any common things early-career candidates often overlook when interviewing for AI, backend, or MLOps-oriented roles?\n\nI‚Äôm not trying to rush the process, just aiming to take a reasonable direction and learn from people with more experience.\n\nThanks üôå",
      "is_original_content": false,
      "link_flair_text": "beginner helpüòì",
      "permalink": "https://reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4pc5cb",
          "author": "bad_detectiv3",
          "text": "How are you learning to do this OP",
          "score": 3,
          "created_utc": "2026-02-10 23:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfgj3",
              "author": "c0bitz",
              "text": "Mostly self-study + building small experiments. I try to avoid just watching courses and instead replicate simple pipelines end-to-end from training to deployment even if it‚Äôs basic. Right now I‚Äôm more focused on understanding inference architecture and cost tradeoffs rather than just model building.",
              "score": 1,
              "created_utc": "2026-02-11 07:23:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pc20t",
          "author": "Otherwise_Wave9374",
          "text": "On the MLOps side, the \"agent\" specific stuff I see teams miss early is observability: log every tool call (inputs, outputs, latency), version prompts, and have a tiny golden eval set you can run on PRs.\n\nAlso, make the agent fail closed. If a tool is down or confidence is low, it should ask a clarifying question or hand off, not hallucinate.\n\nIf you want a few practical patterns for agent tracing/evals, I have been collecting notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-10 23:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfpih",
              "author": "c0bitz",
              "text": "Fail closed is such an underrated point. I‚Äôve seen too many demos where agents just hallucinate confidently instead of degrading gracefully.The golden eval set on PRs is smart too, are you automating those checks in CI or running them manually?",
              "score": 1,
              "created_utc": "2026-02-11 07:25:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pk9v5",
          "author": "overemployed74737",
          "text": "In my J2 im working as MLOps engineer  and during my interview i just explained the entire lifecycle for any ml models and talk about some differents needs between some models. Explained a little about observability and performance drift too",
          "score": 2,
          "created_utc": "2026-02-10 23:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfisd",
              "author": "c0bitz",
              "text": "That‚Äôs a good point. I‚Äôve noticed lifecycle/system thinking comes up way more than specific tools. When you explained drift and observability, did they go deep into monitoring stack questions or keep it high level?",
              "score": 1,
              "created_utc": "2026-02-11 07:23:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pkgcl",
          "author": "Competitive-Fact-313",
          "text": "I this your spectrum atm is too broad, try to narrow down a learn specific things first and then widen the scope. Making AI saas is one things and working in Mlops is another. If you define well I can help better. To start small just play with a simple linear regression model on sagemaker  and use how many instances endpoints you want‚Äî->> take a lambda function‚Äî‚Äî>api gateway ‚Äî-> test the api gateway endpoint using postman once done. Use your choice of frontend to show it as saas. This is the lowest level you can start with.",
          "score": 2,
          "created_utc": "2026-02-10 23:50:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfkqn",
              "author": "c0bitz",
              "text": "That‚Äôs actually helpful. Breaking it down that way makes it less overwhelming. I was thinking too much in terms of ‚Äúfull AI SaaS‚Äù instead of just understanding one clean deployment path first. Did you find AWS interviews expect hands-on experience with those services or mostly conceptual understanding?",
              "score": 2,
              "created_utc": "2026-02-11 07:24:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4spi8o",
                  "author": "Competitive-Fact-313",
                  "text": "In aws interview it depends for seniors roles they may ask you  hands on or sometimes just ask you something from the the pipeline so that mean you must have had those done before that‚Äôs the only things makes you explain stuff",
                  "score": 2,
                  "created_utc": "2026-02-11 13:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pt1ae",
          "author": "burntoutdev8291",
          "text": "In my experience, hiring has shifted more to understanding requirements and system design. I have never used bedrock or sagemaker, places I work at usually run self hosted vLLM. It also depends on the job description, backend roles have a bit more coding and system design, MLOps asks you on more MLOps stuff like model lineage, tracing, observability, and maybe some sysadmin stuff related to GPU. Never worked as an agentic or prompt engineer kind of AI engineer so I can't comment on that.",
          "score": 1,
          "created_utc": "2026-02-11 00:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4repik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-11 07:16:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s13h1",
              "author": "c0bitz",
              "text": "Totally agree, practical demos always carry more weight. I‚Äôve been focusing on getting code + infra clean for simple model endpoints before scaling.",
              "score": 1,
              "created_utc": "2026-02-11 10:45:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o5k2cwo",
          "author": "Gaussianperson",
          "text": "Great questions ‚Äî I work as an ML Engineer at a large tech company building production ML systems (rec systems, ads ranking, abuse detection) and have been writing about exactly this kind of stuff.\n\nA few things from my experience:\n\n\\*\\*Roles\\*\\*: \"AI Engineer\" is becoming the catch-all, but what matters more is whether the role is model-focused or infra-focused. If you enjoy the deployment/scaling side (MLOps, serving, observability), look for titles like ML Platform Engineer or MLOps Engineer. If you want to be closer to the product, AI Engineer or Applied ML Engineer roles are a better fit. Backend roles with ML exposure can also be a great entry point.\n\n\\*\\*Interviews\\*\\*: System design is increasingly what separates candidates. Everyone can code LeetCode mediums ‚Äî fewer people can design an end-to-end ML serving pipeline, explain trade-offs between batch vs real-time inference, or talk about how they'd handle model monitoring in production. Cloud/infra knowledge is a bonus but rarely the bottleneck.\n\n\\*\\*Projects\\*\\*: A small but complete SaaS that shows you can go from model to deployed product is worth more than 10 Jupyter notebooks. Bonus points if it has monitoring, CI/CD, and handles real traffic. Infrastructure repos (Terraform configs, deployment pipelines) are solid for MLOps roles specifically.\n\n\\*\\*Common blind spots\\*\\*: People underestimate data pipeline design, cost optimization, and failure modes. Production ML is 90% engineering and 10% modeling. Show that you understand that.\n\nI write a newsletter called Machine Learning at Scale where I cover exactly these topics: production ML patterns, system design deep dives, and how things actually work at scale in big tech.\n\nMight be useful if you're going down this path: [https://ludovicoloreti.substack.com](https://ludovicoloreti.substack.com)",
          "score": 1,
          "created_utc": "2026-02-15 19:20:48",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2039h",
      "title": "Migrating from Slurm to Kubernetes",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/kubernetes/comments/1r202tx/migrating_from_slurm_to_kubernetes/",
      "author": "alex000kim",
      "created_utc": "2026-02-11 15:17:48",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r2039h/migrating_from_slurm_to_kubernetes/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r1xg77",
      "title": "Hello every one! üëã",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1xg77/hello_every_one/",
      "author": "3MR_MLops",
      "created_utc": "2026-02-11 13:31:16",
      "score": 6,
      "num_comments": 9,
      "upvote_ratio": 0.75,
      "text": "Hi everyone! I‚Äôm Amr, a 17-year-old aspiring MLOps Engineer from Egypt. I‚Äôve already covered Python, SQL, Linux, Git/GitHub, and some FastAPI. I recently finished the first two courses of Andrew Ng‚Äôs Machine Learning Specialization in just 7 days! To make sure I truly understood the concepts, I applied what I learned in two projects which you can find here: https://github.com/3MR-MLops/my_project_of_ML\n\nHere is my upcoming plan for the next few weeks: 1. Finish Andrew Ng‚Äôs 3rd ML course. 2. Deep Learning Specialization. 3. Advanced FastAPI. 4. Docker & Containerization. 5. CI/CD Pipelines. 6. MLflow (Experiment Tracking). 7. Cloud (AWS). 8. Kubernetes (k8s).\n\nMy goal is to be \"Production-ready\" for international internships. Does this order make sense? Is there anything I should add or change to stand out more to recruiters?\n\nThanks for your guidance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1xg77/hello_every_one/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4srdbn",
          "author": "dukesb89",
          "text": "I think your plan is good. Try and add more projects to your GitHub portfolio to demonstrate your skills. This is the key way to differentiate yourself when you have no work experience.",
          "score": 1,
          "created_utc": "2026-02-11 13:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4srxwt",
              "author": "3MR_MLops",
              "text": "ok thanks bro \nI'm trying to add at least one project to everything I'm learning now, and eventually I'll work on 3 or more projects like a chatbot, a product suggestion project, videos, and so on.",
              "score": 1,
              "created_utc": "2026-02-11 13:53:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sskmo",
                  "author": "dukesb89",
                  "text": "Instead of doing lots of small projects, do a few bigger ones. The one you have now is a good start from a learning perspective but too simple to get a hiring manager to take notice. You will want to get to a point where you are engineering a full system. It will take time",
                  "score": 1,
                  "created_utc": "2026-02-11 13:56:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r21sp7",
      "title": "A question for seniors",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r21sp7/a_question_for_seniors/",
      "author": "3MR_MLops",
      "created_utc": "2026-02-11 16:21:58",
      "score": 4,
      "num_comments": 14,
      "upvote_ratio": 0.84,
      "text": "If you are now HR\nWhat is the one thing that you rarely see in entry-level employee files that would make you want to hire someone immediately? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r21sp7/a_question_for_seniors/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4tm5jn",
          "author": "dayeye2006",
          "text": "My hiring manager told me they know this person and we must have this person at our company",
          "score": 5,
          "created_utc": "2026-02-11 16:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tmi5g",
              "author": "3MR_MLops",
              "text": "Who is he?",
              "score": 1,
              "created_utc": "2026-02-11 16:26:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4toe6l",
                  "author": "dayeye2006",
                  "text": "A hypothetical person",
                  "score": 1,
                  "created_utc": "2026-02-11 16:35:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o502t0x",
          "author": "Hyperventilater",
          "text": "Accurate representation of contributions with numerical impact, personal projects that show legitimate interest rather than just ambition, any personalization that highlights a strong desire to learn. Those really signify a junior that will bring innovation and talent to the role to me.",
          "score": 4,
          "created_utc": "2026-02-12 16:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o509v2m",
              "author": "3MR_MLops",
              "text": "This is gold! I never realized how much 'numerical impact' outweighs just listing skills for a junior. It makes total sense to show how a project actually solved a problem or improved a metric. I‚Äôm definitely going to apply this mindset to my current MLOps learning path. Much appreciated!",
              "score": 3,
              "created_utc": "2026-02-12 16:43:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r4f6uw",
      "title": "Passed NVIDIA NCA-AIIO and now need Guidance for NCP-AII",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "author": "Sufficient_Berry_311",
      "created_utc": "2026-02-14 08:14:04",
      "score": 4,
      "num_comments": 2,
      "upvote_ratio": 0.75,
      "text": "Hello Everyone \n\nI had passed the NCA-AIIO on 12th Feb 2026. The questions are simple and you can pass the exam using your logic. You can ask me questions about the exam. I have used notebooklm for study, if you want I can give it also. \n\n  \nI need help to clear the NCP-AII. Is there any person here who cleared it. I wanted to know how hard is this and how many questions we need to solve in cli (provided in exam) or is there any lab related work?  \n  \nAny help from where I will get the lab access?\n\nThank you üôè  ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r4f6uw/passed_nvidia_ncaaiio_and_now_need_guidance_for/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o5cw4cw",
          "author": "Fantastic-Chicken748",
          "text": "Hey!!! I‚Äôm also studying to ncp-aai. If you have any tips that you want to share, I would be very much appreciated",
          "score": 2,
          "created_utc": "2026-02-14 16:05:56",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5dqe4c",
              "author": "Sufficient_Berry_311",
              "text": "I am from infrastructure side not from application side, cannot help on that. You can follow the nvidia study guide to notebooklm and you can ask question.\nNCA-AIIO is very easy, if you have knowledge how gpu works thats it.",
              "score": 2,
              "created_utc": "2026-02-14 18:37:23",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r2t5sn",
      "title": "Seeking deep 1:1 mentoring (Databricks / Snowflake / Azure ML)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "author": "Admirable-Crab-9908",
      "created_utc": "2026-02-12 13:17:02",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "Looking for structured 1:1 mentoring to go from implementation-level expertise to platform-level mastery.\n\nFocus areas:\n\n\t‚Ä¢\tDatabricks MLOps (Unity Catalog, MLflow, CI/CD, governance)\n\n\t‚Ä¢\tSnowflake ML (Snowpark ML, feature pipelines, deployment patterns)\n\n\t‚Ä¢\tAzure ML (enterprise pipelines, model serving, security)\n\nKindly DM. Will be happy to pay hourly. ",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4zpz76",
          "author": "kchandank",
          "text": "I will DM you",
          "score": 2,
          "created_utc": "2026-02-12 15:10:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3s22c",
      "title": "How did you learn Ray Serve? Any good resources?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3s22c/how_did_you_learn_ray_serve_any_good_resources/",
      "author": "FreshIntroduction120",
      "created_utc": "2026-02-13 15:25:21",
      "score": 3,
      "num_comments": 2,
      "upvote_ratio": 1.0,
      "text": "Hi everyone,\nI‚Äôm trying to learn Ray Serve for model deployment and scaling, but I‚Äôm a bit lost on where to start.\nIf you‚Äôve learned it before, how did you do it? Are there any good tutorials, courses, blog posts, or documentation that really helped you?\nAlso, if you know any good YouTube channels or resources to learn more about MLOps in general (model serving, deployment, infrastructure, etc.), I‚Äôd really appreciate it.\nI‚Äôm looking for practical, real-world examples if possible.\nThanks a lot üôè",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3s22c/how_did_you_learn_ray_serve_any_good_resources/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o56zfwj",
          "author": "Delicious-One-5129",
          "text": "I learned it by working straight through the Ray Serve docs and deploying a small project end to end.\n\nI started with a single local model, then added replicas and autoscaling, and finally containerized it and ran it on a small cluster. For broader MLOps context, the main Ray docs and hands on demos combining it with FastAPI, Docker, and Kubernetes helped a lot.",
          "score": 2,
          "created_utc": "2026-02-13 17:03:36",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5c8y6w",
              "author": "ConsciousML",
              "text": "Second this! Always work from the official docs and then use additional content when necessary.",
              "score": 1,
              "created_utc": "2026-02-14 13:57:47",
              "is_submitter": false,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}