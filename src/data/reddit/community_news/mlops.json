{
  "metadata": {
    "last_updated": "2026-02-14 08:45:32",
    "time_filter": "week",
    "subreddit": "mlops",
    "total_items": 19,
    "total_comments": 57,
    "file_size_bytes": 80043
  },
  "items": [
    {
      "id": "1qzaeeu",
      "title": "Best resource to learn modular code for MLOPs",
      "subreddit": "mlops",
      "url": "https://i.redd.it/bsf6sm1c7aig1.jpeg",
      "author": "Deep-Blue-Sea-645",
      "created_utc": "2026-02-08 14:32:30",
      "score": 33,
      "num_comments": 10,
      "upvote_ratio": 0.98,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qzaeeu/best_resource_to_learn_modular_code_for_mlops/",
      "domain": "i.redd.it",
      "is_self": false,
      "comments": [
        {
          "id": "o49arex",
          "author": "MattA2930",
          "text": "Check out ArjanCodes on YouTube. Great channel on code design in Python, and should help you re-write your notebook functionality with Python best practices.\n\nThere is no single right way though. I usually advise to do whatever you think makes it easiest for someone else to come in and make changes to your codebase.",
          "score": 9,
          "created_utc": "2026-02-08 14:39:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4ebvxk",
              "author": "JayRathod3497",
              "text": "Yes I have followed him for FastAPI modulation",
              "score": 1,
              "created_utc": "2026-02-09 07:31:44",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o49au2g",
          "author": "MindlessYesterday459",
          "text": "Cookiecutter data science could be relevant here.\n\nhttps://cookiecutter-data-science.drivendata.org/",
          "score": 3,
          "created_utc": "2026-02-08 14:40:13",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49ke8m",
          "author": "alex_0528",
          "text": "Marvelous MLOps combines both modular code and notebooks in Databricks so you've got the utility of both: https://www.marvelousmlops.io/\n\nThey also cover ditching the notebooks altogether for paramterised scripts. \n\nYes they use Databricks as the platform to deliver this but the principal is pretty universal and could be applied elsewhere, especially once you've started using the scripts to run your modular, testable code.",
          "score": 3,
          "created_utc": "2026-02-08 15:31:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4f66bv",
              "author": "Moist-Matter5777",
              "text": "Databricks is great for that! If you're looking for more variety, check out the MLOps Specialization on Coursera. It dives into modular code practices across different platforms and tools. Also, the book \"Building Machine Learning Powered Applications\" has some solid insights on structuring your code.",
              "score": 3,
              "created_utc": "2026-02-09 12:15:13",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4hgq4l",
          "author": "Joker_420_69",
          "text": "Vikas Das MLOps. (If hindi)",
          "score": 2,
          "created_utc": "2026-02-09 19:25:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49ao42",
          "author": "_caramel_popcorn",
          "text": "Artifacts should be stored remotely right?",
          "score": 1,
          "created_utc": "2026-02-08 14:39:17",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49lvzf",
          "author": "Standard-Distance-92",
          "text": "How about Asset bundles MLOps stacks?",
          "score": 1,
          "created_utc": "2026-02-08 15:39:01",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o49nv22",
          "author": "Krekken24",
          "text": "Check my comment which I did on some other post - [link](https://www.reddit.com/r/learnmachinelearning/s/RLENZH0ZuD)",
          "score": 1,
          "created_utc": "2026-02-08 15:48:55",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4comai",
          "author": "Just_Deal6122",
          "text": "The feature/inference/training design pattern described in the LLM Engineer Handbook is a useful reference. The authors apply this pattern to LLM engineering, but it was originally used for MLOps folder structure.",
          "score": 1,
          "created_utc": "2026-02-09 01:02:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r3kxcd",
      "title": "What YouTube content actually helped you in your MLOps journey? And what's still confusing?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-13 09:37:26",
      "score": 27,
      "num_comments": 8,
      "upvote_ratio": 1.0,
      "text": "I've been in the ML/DevOps space for 11+ years and recently started doing 1:1 calls helping people transition into MLOps. One thing I keep noticing, almost everyone I talk to is overwhelmed.\n\nNot because they're not smart. But because MLOps is so vast, with batch vs. real-time ML pipelines, inference, infrastructure, and monitoring, every course teaches it differently. One guy will say start with Kubeflow, another says MLflow, another says forget tools, learn fundamentals first.\n\nI genuinely want to understand from this community:\n\n1. When you search MLOps on YouTube, what kind of videos do you actually watch fully? Tool-specific tutorials? Career roadmaps? Architecture walkthroughs?\n2. What's your biggest struggle right now â€” is it picking the right tools? Understanding how pieces connect end to end? Or knowing what the market actually wants vs what courses teach?\n3. Is there a video or channel that genuinely helped you \"get it\"? Not just theory, but actually made something click?\n4. What's missing? What video do you wish existed but doesn't?\n\nAsking because I see so much content out there, but people on my calls are still confused. \n\nSomething is clearly not working. Curious what you all think.\n\nI've been thinking of creating some content on this myself, but before that, I just want to understand the current situation and where people are really stuck. No point in adding more noise if the real gap is elsewhere.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r3kxcd/what_youtube_content_actually_helped_you_in_your/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o557kiv",
          "author": "ConsciousML",
          "text": "Youtube was not very helpful on my side for this as thereâ€™s very little quality content in my opinion.\n\nWhat worked best for me is reading quality articles:\n- [ml-ops.org](https://ml-ops.org/) is great for the basics\n- [Neptune.ai](https://neptune.ai/blog) is great but they surf a lot on the GenAI wave so I donâ€™t read much anymore\n- [ZenML](https://docs.zenml.io/stacks) has amazing doc for the MLOps components (experiment tracker, feature store, orchestrator, etc.)\n- [Hopsworks](https://www.hopsworks.ai/post/mlops-to-ml-systems-with-fti-pipelines) explains the three pipeline paradigm better than anyone else\n\n\nOnce you know the theory well and you had some good hands-on experience, Iâ€™ve found that the big tech engineering blogs are the best source of trusted information.\n\nIâ€™ve compiled a list of [interesting blogs](https://foremost-tea-3e3.notion.site/Resources-1973205f20be80d6923bd4a18ee62cd6?source=copy_link).\n\nI also try my best to share useful content on my [personal blog](https://www.axelmendoza.com/) if thatâ€™s helpful!\n\nIâ€™m really interested in your process to try helping people getting into MLOps.\n\nDM me if you want to share some thoughts!",
          "score": 18,
          "created_utc": "2026-02-13 11:01:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o56d3gt",
              "author": "Neither_Film_8641",
              "text": "I like your Blog!",
              "score": 3,
              "created_utc": "2026-02-13 15:17:02",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o56mm3h",
                  "author": "ConsciousML",
                  "text": "Thanks man! I enjoy to write. Even better if itâ€™s helpful ;)",
                  "score": 1,
                  "created_utc": "2026-02-13 16:02:48",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o55w9ul",
          "author": "TruDanceCat",
          "text": "DataCamp is my go-to. Tons of great courses on ML and MLOPS. Theory videos, coding exercises, labs, and practice quizzes all based on career tracks like ML, MLOps, Data Scientist, Data Analytics, Data Engineering and more.\n\nWe have a learning budget at work, so they reimburse us for the cost, but I would pay for the subscription myself if they didnâ€™t- itâ€™s totally worth it.",
          "score": 3,
          "created_utc": "2026-02-13 13:50:05",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o55nfka",
          "author": "DifficultDifficulty",
          "text": "I found AWS/GCP tech blogs and OSS repos useful, particularly those laying out architecture blueprints",
          "score": 2,
          "created_utc": "2026-02-13 12:59:16",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o56sp6s",
          "author": "Inevitable_Resort902",
          "text": "I am a SWE looking to transition into MLOps and would love to get some advice. Can I get in touch with you?",
          "score": 1,
          "created_utc": "2026-02-13 16:31:31",
          "is_submitter": false,
          "replies": [
            {
              "id": "o5739xf",
              "author": "Extension_Key_5970",
              "text": "sure, you can DM me",
              "score": 1,
              "created_utc": "2026-02-13 17:22:20",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o59do9g",
          "author": "Ambitious-Estate4356",
          "text": "I would say claude learning + claude code. They try to make you learn each and every concepts from ground up with some example and code.",
          "score": 1,
          "created_utc": "2026-02-14 00:25:01",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r1hcgr",
      "title": "What's your Production ML infrastructure in 2026?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "author": "Repulsive_Ad_9950",
      "created_utc": "2026-02-10 23:46:35",
      "score": 26,
      "num_comments": 13,
      "upvote_ratio": 0.96,
      "text": "I'm currently studying the tools generally associated with MLOps. Some stuff seem to be non-negotiable: Cloud provides like AWS, GCP and Azure, Kubernetes, Docker, CI/CD and monitoring/observability. I'd like to hear about the tooling your company use to handle ML workflows, so I can have some direction in my studies. Here are my questions. \n\n**CI/CD**  \nGithub Actions, GitLab or other? Do you use different CI/CD tools depending for training and deployment?\n\n**Orchestration for training models**  \nWhat actually runs your training jobs? Airflow, Prefect, Kubeflow Pipelines, Argo, or something else?  \nHow does the flow work? For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow?  \n\n\n**Serving**  \nAre your inference endpoints deployed with FastAPI, KServe or other (like Lambda)? I heard that KServe has the advantage of batching requests, which is more compute-efficient for fetching data from database, feature engineering, and making predictions, and as well as some automated A/B and canary deployments, which seems a big advantage to me. \n\n**Monitoring and Observability**  \nCloud-nativa services like CloudWatch or Prometheus+Grafana?\n\n**Integrated or Scattered?**  \nAll-in on one platform (Kubeflow end-to-end, or everything in SageMaker), or scattered (Airflow/Prefect, Kubernetes, etc)",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r1hcgr/whats_your_production_ml_infrastructure_in_2026/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4qvk1q",
          "author": "pmv143",
          "text": "For serving LLMs at scale, the stack starts to diverge from traditional ML serving pretty quickly.\n\nFastAPI or KServe work fine for classic models, but large LLM inference introduces different constraints:\n\nâ€“ GPU residency and memory fragmentation\nâ€“ cold start latency\nâ€“ multi model scheduling on shared GPUs\nâ€“ batching vs interactive latency tradeoffs\n\nIn our case we ended up building a snapshot based runtime specifically for inference. Instead of treating endpoints as long lived pods, we snapshot the fully initialized CUDA graph and restore directly into GPU memory. That lets us scale to zero without warm pools and still keep cold start under ~2 seconds for 70B class models.\n\nMonitoring wise we still rely on Prometheus style metrics and GPU level telemetry, but the core efficiency gains come from how the runtime manages GPU state rather than from orchestration layers.\n\nThe training stack can stay fairly conventional. Inference is where the architecture really changes.",
          "score": 11,
          "created_utc": "2026-02-11 04:39:50",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rrcin",
              "author": "elsatan666",
              "text": "That snapshot approach sounds very cool, was that something you built yourselves or is it off the shelf?",
              "score": 1,
              "created_utc": "2026-02-11 09:16:04",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o4s8275",
                  "author": "pmv143",
                  "text": "We built it ourselves.\n\nThere isnâ€™t really an off-the-shelf solution that snapshots a fully initialized CUDA graph and restores it directly into GPU memory for large LLMs.\n\nContainer images snapshot disk state.\nCheckpointing saves weights.\nBut neither preserves the full runtime state required for fast GPU restore.\n\nThis took several years of work around GPU memory management, CUDA initialization, and state restoration. Itâ€™s very inference-specific. We didnâ€™t touch the training stack much. The real complexity was managing GPU residency and multi-model state transitions safely.\n\nStill early, but itâ€™s been working well for 30Bâ€“300B class models so far.",
                  "score": 2,
                  "created_utc": "2026-02-11 11:44:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4ppj8q",
          "author": "ixrequalv",
          "text": "GitHub, harness for cicd\nSagemaker for everything it can, also bedrock and lambdas / other AWS services \nPrometheus cloud watch and grafana",
          "score": 2,
          "created_utc": "2026-02-11 00:19:04",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4qu00i",
          "author": "Scared_Astronaut9377",
          "text": "\nLet me better describe what makes sense and what makes less sense. \n\n> Some stuff seem to be non-negotiable: Cloud provides\n\nI mean, there are many jobs that are MLOps in nature that require Spark, Flink, etc. instead of clouds. But they typically hire Data Engineers to do that lol.\n\n> Github Actions, GitLab or other? \n\nBoth work + many other building tools including those in major clouds.\n\n> For example, GitHub Actions -> Airflow DAG -> SageMaker job, or the pipeline occur integrated within Kubeflow? \n\nBoth work, action -> airflow -> training job is a very solid pattern for many use cases.\n\n> Are your inference endpoints deployed with FastAPI, KServe\n\nDepends on the requirements and the framework. Sometimes just throwing things in FastAPI is ok, sometimes you want, say, tf serving. I wouldn't touch lambda, always make your own containers. KServe is indeed solid for many use cases.\n\n> Cloud-nativa services like CloudWatch or Prometheus+Grafana?\n\nMostly what the company is already using lol. If I am designing from scratch, yeah, prometheus+grafana. \n\n> Kubeflow end-to-end\n\nKubeflow pipelines are good. Kubeflow as a platform is garbage. \n\n> everything in SageMaker\n\nWhen I was new to MLOps in cloud. Never again. First-class sitizen cloud services + k8s.",
          "score": 2,
          "created_utc": "2026-02-11 04:28:42",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4xy6fx",
          "author": "NoobZik",
          "text": "Kedro + MLFlow + Airflow + NannyML\nScattered but cloud agnostic\n\nFor inference Iâ€™m using FastAPI",
          "score": 1,
          "created_utc": "2026-02-12 07:04:32",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0tdmj",
      "title": "If you're struggling with ML foundations for MLOps, there's another path, the inference & serving side",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "author": "Extension_Key_5970",
      "created_utc": "2026-02-10 06:28:20",
      "score": 22,
      "num_comments": 2,
      "upvote_ratio": 0.89,
      "text": "In my last post, I discussed the importance of ML foundations and Python as key aspects of MLOps. But I realised I left out the other side of the coin, one that's equally valid and may be a better fit for many of you.\n\nIf math and stats aren't your thing and you dread memorising gradient descent variants or probability distributions, hear me out: there's a whole side of MLOps where that's not the focus.\n\nThis side focuses on **model serving, inference optimisation, and production scaling**. \n\nCompanies need people who can:\n\n* Expose models via FastAPI\n* Optimise inference latency and throughput using vLLM, TensorRT, or Triton\n* Manage serving infrastructure with KServe, Seldon, or Ray Serve\n* Handle autoscaling, batching strategies, A/B deployments, and canary rollouts\n* Build observability, monitoring drift, tracking latency p99s, and managing GPU utilisation\n\nNone of this requires you to derive backpropagation from scratch. What it *does* require is strong production engineering instincts, the kind you already have if you've been in DevOps, SRE, or platform engineering.\n\nSo if you're coming from an infrastructure background and feel overwhelmed trying to learn ML theory just to break into MLOps, know that there's a legit path that maps directly to your existing skills. Inference at scale is genuinely hard engineering, and most ML teams desperately need people who can do it well.\n\nThe ML foundations will come naturally over time through exposure. You don't need to master them before you start contributing meaningfully.\n\nI've also helped a few folks navigate this transition, review their resumes, prepare for interviews, and figure out what to focus on. If you're going through something similar and want to chat, my DMs are open, or you can book some time here:Â [topmate.io/varun\\_rajput\\_1914](https://topmate.io/varun_rajput_1914)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tdmj/if_youre_struggling_with_ml_foundations_for_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4kodi4",
          "author": "Extension_Key_5970",
          "text": "For those who are thinking, on how to start and where to explore, read my detailed blog post: [https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319](https://medium.com/@thevarunfreelance/you-dont-need-to-master-ml-theory-to-break-into-mlops-here-s-the-other-path-no-one-talks-about-56bc6fb45319)",
          "score": 2,
          "created_utc": "2026-02-10 06:42:30",
          "is_submitter": true,
          "replies": []
        },
        {
          "id": "o4kvk18",
          "author": "--Thunder",
          "text": "Thank you for sharing, I was also looking to dive into mlOps stuff & come from a strong infra background.\n\nCan you recommend any book or some videos which might have helped you as well. I have read the doc on medium, itâ€™s precise & awesome.\n\nThank you ðŸ™ðŸ¼",
          "score": 1,
          "created_utc": "2026-02-10 07:48:02",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r312b5",
      "title": "The agent security landscape is kind of a mess and I'm not sure what to do about it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "author": "Exact-Literature-395",
      "created_utc": "2026-02-12 18:21:21",
      "score": 14,
      "num_comments": 2,
      "upvote_ratio": 0.93,
      "text": "So my team has been pushing me to evaluate autonomous agents for some of our workflow automation. Specifically looking at OpenClaw since it has massive traction (something like 160k+ GitHub stars) and can connect LLMs to local files, browsers, Slack, Discord, etc. Our ops lead is really excited about using it to auto-triage the \\~200 support tickets we get daily, basically having it read incoming tickets, check our internal docs, and route them to the right team with a priority score. Also been talking about automating the data validation checks we run every Monday where someone manually compares CSV exports against our postgres tables. Tedious stuff that would be perfect for an agent.\n\nBut honestly? The more I dig into this, the more I want to pump the brakes.\n\nI stumbled across some security research that genuinely unsettled me. Apparently there are tens of thousands of OpenClaw instances just... exposed directly to the internet. But the number that really stopped me was this: something like 15% of community built skills contain malicious instructions. Prompts designed to download malware or steal data. And when these get flagged and removed, they apparently just reappear under new identities pretty quickly.\n\nThe project's own FAQ literally describes this as a \"Faustian bargain\" with no \"perfectly safe\" setup. I appreciate the honesty but also... what am I supposed to do with that? How do I bring this to my team without sounding like I'm just being obstructionist?\n\nWhat's frustrating from an MLOps perspective is that this completely changes how I think about threat modeling. We've spent so much time worrying about model poisoning, adversarial inputs, data drift. With agents though, the attack surface just explodes. Prompt injection could come through any email or webpage the agent processes. If someone compromises the agent itself they basically inherit every permission we've granted it. And the plugin ecosystem? Nobody has time to audit all that, but you're essentially running untrusted code with access to your systems.\n\nThere's also this concept I keep seeing called \"judgment hallucination\" where the agent appears trustworthy but lacks genuine reasoning, so users just... hand over more and more authority. That one hits different because I can already see how it would play out with some of the less technical folks on our team who already treat ChatGPT like its omniscient.\n\nI looked at some alternatives like AutoGPT and BabyAGI but they seem to have similar issues, maybe even less mature from a security standpoint. A coworker mentioned something called Agent Trust Hub that supposedly scans skills for hidden logic and data exfiltration patterns before you install them, still need to actually try it though. The usual advice seems to be run everything in containers, dont expose default ports, start with read only permissions and expand from there. Basically treat it like you would any untrusted code I guess.\n\nBut I'm genuinely torn. The capability is exciting and I get why leadership wants this. The current state of the ecosystem though... it feels like we'd be taking on a lot of risk that we're not equiped to manage yet. Maybe I'm being too conservative here.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r312b5/the_agent_security_landscape_is_kind_of_a_mess/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o53p7au",
          "author": "KarmaIssues",
          "text": "I suppose the only real change would have to be sandboxing it. That would atleast help with some of the more damaging actions.",
          "score": 1,
          "created_utc": "2026-02-13 03:27:09",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o5aecc4",
          "author": "South-Opening-9720",
          "text": "You're not being conservative â€” you're threat-modeling correctly. I'd start with an 'assistive' agent first (summarize + classify + suggest KB links), keep it read-only, and lock it in a sandbox with tight egress + allowlisted tools; actions/handoffs only after evals + red-team prompt injection tests. I use chat data for the safer 'draft + route' step and it helps without giving it keys to the kingdom. What systems would it touch on day 1?",
          "score": 1,
          "created_utc": "2026-02-14 04:23:58",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0269t",
      "title": "What LLM workloads are people actually running asynchronously?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "author": "NewClaim7739",
      "created_utc": "2026-02-09 11:50:20",
      "score": 10,
      "num_comments": 7,
      "upvote_ratio": 1.0,
      "text": "Feels like most AI infra is still obsessed with latency when it isn't always the thing that moves the needle. The highest-volume workloads we're seeing are offline:\n\nâ€¢ eval pipelines  \nâ€¢ dataset labeling  \nâ€¢ synthetic data  \nâ€¢ document processing  \nâ€¢ research agents\n\nOnce you stop caring about milliseconds, the economics change completely.\n\nCurious what people here are running in batch vs realtime - and where the break-even tends to be?",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0269t/what_llm_workloads_are_people_actually_running/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4f54ac",
          "author": "Otherwise_Wave9374",
          "text": "Totally seeing the same trend, batch wins. Research agents, doc pipelines, and evals are way more forgiving on latency, and you can do smarter scheduling (spot instances, queues, retries). The trick is getting idempotency and good observability so your agent runs are actually debuggable. I have some notes on async agent workloads and eval loops here: https://www.agentixlabs.com/blog/ What are people using for tracing in batch, OpenTelemetry or vendor tools?",
          "score": 3,
          "created_utc": "2026-02-09 12:07:07",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lhakg",
              "author": "NewClaim7739",
              "text": "For the model serving / inference API, we actually ended up building an internal batch tool for this because realtime pricing just didnâ€™t make sense - happy to share details if useful",
              "score": 1,
              "created_utc": "2026-02-10 11:14:26",
              "is_submitter": true,
              "replies": []
            },
            {
              "id": "o4s1s1n",
              "author": "burntoutdev8291",
              "text": "Bot vs bot",
              "score": 1,
              "created_utc": "2026-02-11 10:51:48",
              "is_submitter": false,
              "replies": []
            }
          ]
        },
        {
          "id": "o4h1r9b",
          "author": "penguinzb1",
          "text": "eval pipelines are the big one for us. we've been working on simulating agent runs to catch issues before they hit production, saves a lot of headaches when you can batch test 100s of scenarios overnight instead of waiting for users to hit edge cases",
          "score": 3,
          "created_utc": "2026-02-09 18:15:27",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfpmi",
              "author": "NewClaim7739",
              "text": "Super interesting - evals seem to be one of the main use cases. What are you using as the inference API?",
              "score": 1,
              "created_utc": "2026-02-10 11:00:37",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4ghdmg",
          "author": "AIML_Tom",
          "text": "The common theme is that theyâ€™re not waiting on a human in real time â€” theyâ€™re queued, retried, and scaled out with workers. Synchronous chat is the flashy demo, but async workloads are where throughput and reliability matter most.",
          "score": 2,
          "created_utc": "2026-02-09 16:38:51",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4lfmfi",
              "author": "NewClaim7739",
              "text": "Massively agree - 'set and forget' your agent and come back to the results you asked for when you've done other tasks",
              "score": 2,
              "created_utc": "2026-02-10 10:59:49",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qyk81r",
      "title": "What course to take?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyk81r/what_course_to_take/",
      "author": "Berlibur",
      "created_utc": "2026-02-07 17:45:37",
      "score": 9,
      "num_comments": 12,
      "upvote_ratio": 0.85,
      "text": "I'm a data scientist in a not too data scientisty company. I want to learn MLOps in a prod-ready way, and there might be budget for me to take a course.\n\nAny recommendations?\n\na colleague did a data bricks course on AI with a lecturer (online) and it was basically reading slides and meaningless notebooks. so trying to avoid that",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qyk81r/what_course_to_take/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o45uxfd",
          "author": "Competitive-Fact-313",
          "text": "take a simple iris dataset and then train the model and deploy the model using fastapi+UI(react or streamlit)- create the docker file and piush them to registry+also add mlflow for tracking+ once image is publish then create a CI/CD pipeline. Now take the image and publish using ecs + farget or eks. (you can also chose minikube or kind). Once done edit the dataset and trigger the pipeline. with every edit (data edit or model edit) your workflow should trigger and you will find how model perform. this is a typical mlops (traditional project). You will learn a lot using this. \n\nTo help you get started [https://github.com/amit-chaubey/mlops-docker-k8s-fastapi](https://github.com/amit-chaubey/mlops-docker-k8s-fastapi)  \nyou can check out this repo. edit as per your need. try not to do everthing by yourself (you are not a data scientist) so focus more on deployment and production part. ",
          "score": 5,
          "created_utc": "2026-02-07 23:14:06",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o445wpr",
          "author": "Commercial-Fly-6296",
          "text": "Datatalks club - not sure if it is prod ready though",
          "score": 3,
          "created_utc": "2026-02-07 17:51:15",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4629k3",
          "author": "NoobZik",
          "text": "Kedro + MLFlow is the minimum for production\nIf you want to dig deeper, Airflow + DVC + NannyML",
          "score": 1,
          "created_utc": "2026-02-08 00:00:52",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o44i4t8",
          "author": "apexvice88",
          "text": "But why?",
          "score": 0,
          "created_utc": "2026-02-07 18:51:19",
          "is_submitter": false,
          "replies": [
            {
              "id": "o44iimu",
              "author": "Berlibur",
              "text": "To really make the value from models come to life. \nOne offs or some analysis etc is one thing, having a properly deployed model + a process how to monitor / upgrade / etc is a whole other thing",
              "score": 1,
              "created_utc": "2026-02-07 18:53:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o44sqrq",
                  "author": "apexvice88",
                  "text": "Very good reason",
                  "score": 1,
                  "created_utc": "2026-02-07 19:45:30",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o45o1pz",
          "author": "Anti-Entropy-Life",
          "text": "I strongly recommend you don't take any courses. There is no need.\n\nIf you want to learn about LLMs, you can literally just ask the LLM.\n\nIf you need a proper gated workflow, I have a Dual Window Learning system you can use to teach yourself anything.\n\nI have found this to work better than any of the courses I ever tried out.\n\nYour mileage may vary, of course, but this seems to also be fairly common amongst people truly building with LLMs, so you may not get a lot of good course recommendations from this particular sub.\n\nSorry I can't be more helpful in this specific case, but would be happy to send you the doc on the Dual Window Learning system I use if you decide to go that route :)",
          "score": -1,
          "created_utc": "2026-02-07 22:34:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o45ri6e",
              "author": "Berlibur",
              "text": "What do you mean by that system",
              "score": 2,
              "created_utc": "2026-02-07 22:54:12",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o463vqm",
                  "author": "Anti-Entropy-Life",
                  "text": "I'm sorry, I don't know if this a question, or what it's referring to, would it be possible to be more specific?",
                  "score": 0,
                  "created_utc": "2026-02-08 00:10:38",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1r1tlnt",
      "title": "Need some suggestions on using Open-source MLops Tool",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "author": "NetFew2299",
      "created_utc": "2026-02-11 10:13:22",
      "score": 9,
      "num_comments": 12,
      "upvote_ratio": 1.0,
      "text": "I am a Data scientist by Profession. For a project, I need to setup a ML Infrastructure in a local VM. I  am working on  A daily prediction /timeseries analysis. In the case of Open-Source, I have heard good things about ClearML (there are others, such as ZenML/MLrun), to my [knowledge.It](http://knowledge.It) is simply because it offers a complete MLops solution\n\nApart from this, I know I can use a combination of Mlflow, Prefect, Evidently AI, Feast, Grafana, as well. I want suggestions in case of ClearML, if any, on ease of use. Most of the Softwares claim, but I need your feedback.\n\nI am open to using paid solutions as well. My major concerns:\n\n1. Infrastructure cannot run on the cloud\n2. Data versioning\n3. Reproducible Experiment\n4. Tracking of the experiment\n5. Visualisation of experiment\n6. Shadow deployment\n7. Data drift",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1tlnt/need_some_suggestions_on_using_opensource_mlops/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4up367",
          "author": "niek29",
          "text": "Hey! This is pretty much the exact problem weâ€™re building LUML to solve.\n\nhttps://github.com/luml-ai/luml\n\nWe already have experiment tracking and a deployment module you can self-host wherever you want, so the no-cloud constraint isnâ€™t a problem. Weâ€™re also building a new MLflow-like module with an easy transition to the full platform - centralized registry, deployments, and monitoring, all out of the box. Data drift monitoring is actively in development too.\n\nWeâ€™re onboarding early users right now and your use case is exactly what weâ€™re designing for. Happy to jump on a quick call to walk you through it, DM me if youâ€™re interested!",
          "score": 2,
          "created_utc": "2026-02-11 19:26:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4s4xi3",
          "author": "kayhai",
          "text": "It sounds like you are keen on ClearML. Iâ€™ve tried ML Flow (model registry and experiment tracking) + a scheduling tool of your choice (prefect, airflow or dagster etc). Iâ€™m not familiar with ClearML, may I ask which features of ClearML stands out to you?",
          "score": 1,
          "created_utc": "2026-02-11 11:19:08",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4sb0a9",
              "author": "NetFew2299",
              "text": "It is simply because it offers a complete MLops solution.\n\n",
              "score": 2,
              "created_utc": "2026-02-11 12:07:04",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4sbcu1",
          "author": "Fritos121",
          "text": "This is almost exactly what I came here looking for. A lot of focus on Cloud, but itâ€™s been a bit harder for me to find resources on how best to deploy locally. Thanks for asking the question!",
          "score": 1,
          "created_utc": "2026-02-11 12:09:40",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sqchp",
          "author": "Garbatronix",
          "text": "I have had positive experiences using LakeFS in conjunction with MinIO. It enables you to version data in a similar way to Git. With an MLFlow server, I can log all the relevant parameters, such as branch and ref. MLFlow enables models to be versioned and stored. An MLFlow Docker image can then be generated and easily deployed on a Docker host or Kubernetes. \n\nDrift detection and data visualisation can be implemented in Python scripts prior to training and stored as artefacts in MLFlow. I have created a custom Python model in MLFlow by generating my own Prometheus metrics. These can then be collected via Prometheus and visualised in Grafana.",
          "score": 1,
          "created_utc": "2026-02-11 13:44:34",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4sru98",
          "author": "DifficultDifficulty",
          "text": "\"I need to setup a ML infrastructure in a local VM\" -> is this infra mostly for your own VM-local experiments, and is there no need to distribute workloads in the cloud where the infra would be shared by multiple team members?",
          "score": 1,
          "created_utc": "2026-02-11 13:52:48",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4z4qym",
              "author": "NetFew2299",
              "text": "No,, I don't need it for multiple teams....I just need to setup an API, currently being done with flask later being changed to fastapi.",
              "score": 1,
              "created_utc": "2026-02-12 13:15:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4z8nw7",
                  "author": "DifficultDifficulty",
                  "text": "I see. I've spoken to a few people who described a similar need to yours, and they spoke well about Kedro + MLFlow for this kind of VM-local experience. Please see [https://docs.kedro.org/en/stable/integrations-and-plugins/mlflow/](https://docs.kedro.org/en/stable/integrations-and-plugins/mlflow/) ",
                  "score": 1,
                  "created_utc": "2026-02-12 13:38:26",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4yubra",
          "author": "SassFrog",
          "text": "My suggestion is that in order to use ClearML, or another tool reliably you likely need kubernetes set up beneath it (or another scheduler, which I'd recommend against). Once you have kubernetes you're 90% of the way there, you can easily delploy ClearML through a helmchart, another component besides ClearML or you could replace ClearML.",
          "score": 1,
          "created_utc": "2026-02-12 12:04:19",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o50tvtv",
          "author": "Iron-Over",
          "text": "You are missing explainability, understand which features determined the decision.Â \n\nA prediction store where you can store every prediction to map to an actual result down the road, which helps you create lots of labeled data for future training.Â \n\nYou may want a feature store.Â \n\nModel registry is highly recommended.Â \n\nNot sure if you need bias checks.Â \n\nAre you serving via batch or api?",
          "score": 1,
          "created_utc": "2026-02-12 18:17:42",
          "is_submitter": false,
          "replies": [
            {
              "id": "o54ig61",
              "author": "NetFew2299",
              "text": "Serving via batchÂ ",
              "score": 1,
              "created_utc": "2026-02-13 07:08:01",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o54h4zu",
          "author": "NoobZik",
          "text": "My stack cloud agnostic \nKedro, MLFlow, Airflow. \n\nMinio is dead actually so I shifted to rustfs",
          "score": 1,
          "created_utc": "2026-02-13 06:56:43",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r0tb2u",
      "title": "Lessons from Analyzing 18,000 Exposed Agent Instances",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "author": "RevealNoo",
      "created_utc": "2026-02-10 06:24:20",
      "score": 7,
      "num_comments": 1,
      "upvote_ratio": 0.9,
      "text": "I work on security research at Gen Threat Labs, and we recently wrapped up an analysis of autonomous AI agents in production that I wanted to share. Specifically focused on OpenClaw given its popularity (165k GitHub stars and growing fast).\n\nQuick caveat upfront: our methodology has limitations. We scanned for exposed instances and analyzed publicly available community skills, but we don't have visibility into properly secured deployments or private enterprise setups. We also couldn't verify intent behind everything we flagged, so some of what we classified as malicious might just be poorly written code with bad patterns. Take the numbers with that context.\n\nThat said, what we found was worse than I expected going in.\n\nWe identified over 18,000 OpenClaw instances exposed directly to the internet. Not behind VPNs, not containerized, just sitting on default port 18789 accepting connections. One instance we found had full access to the user's email, calendar, and file system. Just... open. That one stuck with me because it's exactly the kind of setup that makes agents useful, and exactly what makes them dangerous.\n\nBut the finding that actually surprised me was in the community skill ecosystem. We analyzed hundreds of skills that users build and share, and nearly 15% contained what I'd classify as malicious instructions. Some were designed to download external payloads, others to exfiltrate data. A few had hidden logic that only triggered after repeated uses, which made them harder to catch in initial review.\n\nWe spent a while trying to use static analysis to catch these automatically, but the false positive rate was brutal. Ended up needing a mix of pattern matching and actually running skills in sandboxed environments to see what they do. Still not perfect.\n\nWe also noticed something frustrating: malicious skills that got flagged and removed would reappear under different names within days. Same payload, new identity. Whack a mole.\n\nThe attack pattern we kept seeing is what I've started calling \"Delegated Compromise.\" Instead of targeting the user directly, adversaries target the agent. Once they get in through prompt injection or a poisoned skill, they inherit every permission that user granted. It's honestly elegant from an attacker's perspective.\n\nTo OpenClaw's credit, their docs are transparent about this. They literally describe it as a \"Faustian bargain\" and acknowledge no perfectly safe setup exists. I respect that honesty, but I don't think most users deploying these agents fully internalize what that means.\n\nThe risk vectors we kept categorizing:\n\nâ€¢ Expanded attack surface from agents with read/write/execute across multiple applications â€¢ Prompt injection through messages and web content with hidden instructions â€¢ Supply chain risk from community skills built without security review â€¢ System level impact when broadly permissioned agents get compromised â€¢ What I've been calling \"judgment hallucination\" where agents appear trustworthy but lack genuine reasoning, so users over delegate\n\nIf you're running agents in production, the practical stuff that actually matters:\n\nâ€¢ Isolated environments (VMs or containers), not your primary machine â€¢ Don't expose default ports to public internet (seems obvious but 18,000 instances say otherwise) â€¢ Start read only, expand permissions incrementally â€¢ Secondary accounts during testing â€¢ Actually review activity logs, not just set and forget â€¢ Treat third party skills like installing unknown software, because that's basically what it is\n\nThe detection stuff we built for catching the hidden logic patterns, we've been calling it Agent Trust Hub internally. Happy to compare notes if anyone's working on similar approaches or has found better ways to handle the false positive problem.\n\nCurious how other teams are approaching this. Is agent security getting dedicated attention in your org, or is it still lumped in with general appsec? Trying to get a sense of whether this is becoming a recognized problem or if we're early to the panic.",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r0tb2u/lessons_from_analyzing_18000_exposed_agent/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4n5h9l",
          "author": "Informal_Tangerine51",
          "text": "18,000 exposed instances is concerning but the deeper problem is debuggability. When agent gets compromised via poisoned skill, can you reconstruct what it accessed and when?\n\nYou found malicious skills that trigger after repeated uses. That's nightmare fuel without audit trails. User runs skill 5 times safely, 6th execution exfiltrates data. Post-incident question: what did it access across all 6 runs? Most deployments can't answer without forensics.\n\n\"Judgment hallucination\" + over-delegation compounds when you can't verify agent decisions. User trusts agent with email access, agent gets compromised, attacker inherits permissions. Without signed traces of every action (what was accessed, what policy should have blocked it, when it happened), incident response is archaeology. Security gaps plus evidence gaps means compromise detection happens weeks late.",
          "score": 1,
          "created_utc": "2026-02-10 16:54:22",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyi6ca",
      "title": "Why I chose Pulumi, SkyPilot, and Tailscale for a multi-tenant / multi-region ML platform and open-sourced it",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyi6ca/why_i_chose_pulumi_skypilot_and_tailscale_for_a/",
      "author": "DifficultDifficulty",
      "created_utc": "2026-02-07 16:27:06",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 0.9,
      "text": "As an MLOps Dev, I've stood up enough ML platforms to know the drill: VPC, EKS with GPU node pools, a dozen addons, an abstraction layer like Airflow, multi-tenancy, and maybe repeat it all in another region. The stack was usually Terraform, AWS Client VPN, Kubeflow or Airflow, and an external IdP like Okta.\n\nEvery time I'd finish, the same thought would creep up: \"If I started from scratch with fewer constraints, what would I actually pick?\"\n\nI finally worked through that question and open-sourced the result: \n\n**link**: [https://github.com/Roulbac/pulumi-eks-ml](https://github.com/Roulbac/pulumi-eks-ml)\n\n**The repo**\n\nIt's a Python library (named `pulumi-eks-ml`) of composable Pulumi components: VPC, EKS cluster, GPU node pools with Karpenter, networking topologies, etc. You import what you need and wire up your own topology rather than forking a monolithic template. The repo includes three reference architectures that go from simple to complex:\n\n- **Starter** : single VPC, single EKS cluster, recommended addons. Basically a \"hello world\" for ML on EKS.\n\n- **Multi-Region** : full-mesh VPC peering across regions, each with its own cluster. Useful if you need compute close to data in different geographies.\n\n- **SkyPilot Multi-Tenant** : the main one. Hub-and-spoke network, multi-region EKS clusters, a SkyPilot API server in the hub, isolated data planes (namespaces + IRSA) per team, Cognito auth, and Tailscale for VPN access.\n\n**Why SkyPilot?**\n\nI looked at a few options for the \"ML platform layer\" on top of Kubernetes and kept coming back to SkyPilot. It's fully open-source (no vendor lock beyond your cloud provider), it has a clean API server mode that supports workspaces with RBAC out of the box, and it handles the annoying parts of submitting jobs/services to Kubernetes, GPU scheduling, spot instance preemption, etc. It was a natural fit for a multi-tenant setup where you want different teams to have isolated environments but still share the underlying compute. It's not the only option, but for a reference architecture like this, its flexibility made it nice to build around.\n\n**Why Pulumi over Terraform?**\n\nHonestly, this mostly comes down to the fact that writing actual Python is nicer than HCL when your infrastructure has real logic in it. When you're looping over regions, conditionally peering VPCs, creating dynamic numbers of namespaces per cluster based on config, that stuff gets painful in Terraform. Pulumi lets you use normal language constructs, real classes, type hints, tests with pytest. The component model also maps well to building a library that others import, which is harder to do cleanly with Terraform modules. It's not that Terraform can't do this, it's just that the ergonomics of \"infrastructure as an actual library\" fit Pulumi better.\n\n**Why Tailscale?**\n\nThe whole network is designed around private subnets, no public endpoint for the SkyPilot API. You need some way to reach things, and Tailscale makes that trivially easy. You deploy a subnet router pod in the hub cluster, and suddenly your laptop can reach any private IP across all the peered VPCs through your Tailnet. No bastion hosts, no SSH tunnels, no client VPN endpoint billing surprises. It just works and it's basically a lot less config compared to the alternatives.\n\n**What this is and is not:**\n\n- This is not production-hardened. It's a reference/starting point, not a turnkey platform.\n- This is not multi-cloud. It's AWS-only (EKS specifically).\n- This is opinionated by design: the addon choices, networking topology, and SkyPilot integration reflect a specific yet limited set of use cases. Your needs might call for different designs.\n\nIf you're setting up ML infrastructure on AWS and want a place to start, or if you're curious about how these pieces fit together, take a look. Happy to answer questions or take feedback.",
      "is_original_content": false,
      "link_flair_text": "Tools: OSS:doge:",
      "permalink": "https://reddit.com/r/mlops/comments/1qyi6ca/why_i_chose_pulumi_skypilot_and_tailscale_for_a/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": []
    },
    {
      "id": "1r2039h",
      "title": "Migrating from Slurm to Kubernetes",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/kubernetes/comments/1r202tx/migrating_from_slurm_to_kubernetes/",
      "author": "alex000kim",
      "created_utc": "2026-02-11 15:17:48",
      "score": 7,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r2039h/migrating_from_slurm_to_kubernetes/",
      "domain": "reddit.com",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r1frz2",
      "title": "Learning AI deployment & MLOps (AWS/GCP/Azure). How would you approach jobs & interviews in this space?",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "author": "c0bitz",
      "created_utc": "2026-02-10 22:44:19",
      "score": 6,
      "num_comments": 14,
      "upvote_ratio": 0.81,
      "text": "Iâ€™m currently learning how to deploy AI systems into production. This includes deploying LLM-based services to AWS, GCP, Azure and Vercel, working with MLOps, RAG, agents, Bedrock, SageMaker, as well as topics like observability, security and scalability.\n\nMy longer-term goal is to build my own AI SaaS. In the nearer term, Iâ€™m also considering getting a job to gain hands-on experience with real production systems.\n\nIâ€™d appreciate some advice from people who already work in this space:\n\nWhat roles would make the most sense to look at with this kind of skill set (AI engineer, backend-focused roles, MLOps, or something else)?\n\nDuring interviews, what tends to matter more in practice: system design, cloud and infrastructure knowledge, or coding tasks?\n\nWhat types of projects are usually the most useful to show during interviews (a small SaaS, demos, or more infrastructure-focused repositories)?\n\nAre there any common things early-career candidates often overlook when interviewing for AI, backend, or MLOps-oriented roles?\n\nIâ€™m not trying to rush the process, just aiming to take a reasonable direction and learn from people with more experience.\n\nThanks ðŸ™Œ",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1r1frz2/learning_ai_deployment_mlops_awsgcpazure_how/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4pc5cb",
          "author": "bad_detectiv3",
          "text": "How are you learning to do this OP",
          "score": 3,
          "created_utc": "2026-02-10 23:03:53",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfgj3",
              "author": "c0bitz",
              "text": "Mostly self-study + building small experiments. I try to avoid just watching courses and instead replicate simple pipelines end-to-end from training to deployment even if itâ€™s basic. Right now Iâ€™m more focused on understanding inference architecture and cost tradeoffs rather than just model building.",
              "score": 1,
              "created_utc": "2026-02-11 07:23:09",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pc20t",
          "author": "Otherwise_Wave9374",
          "text": "On the MLOps side, the \"agent\" specific stuff I see teams miss early is observability: log every tool call (inputs, outputs, latency), version prompts, and have a tiny golden eval set you can run on PRs.\n\nAlso, make the agent fail closed. If a tool is down or confidence is low, it should ask a clarifying question or hand off, not hallucinate.\n\nIf you want a few practical patterns for agent tracing/evals, I have been collecting notes here: https://www.agentixlabs.com/blog/",
          "score": 2,
          "created_utc": "2026-02-10 23:03:23",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfpih",
              "author": "c0bitz",
              "text": "Fail closed is such an underrated point. Iâ€™ve seen too many demos where agents just hallucinate confidently instead of degrading gracefully.The golden eval set on PRs is smart too, are you automating those checks in CI or running them manually?",
              "score": 1,
              "created_utc": "2026-02-11 07:25:29",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pk9v5",
          "author": "overemployed74737",
          "text": "In my J2 im working as MLOps engineer  and during my interview i just explained the entire lifecycle for any ml models and talk about some differents needs between some models. Explained a little about observability and performance drift too",
          "score": 2,
          "created_utc": "2026-02-10 23:49:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfisd",
              "author": "c0bitz",
              "text": "Thatâ€™s a good point. Iâ€™ve noticed lifecycle/system thinking comes up way more than specific tools. When you explained drift and observability, did they go deep into monitoring stack questions or keep it high level?",
              "score": 1,
              "created_utc": "2026-02-11 07:23:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        },
        {
          "id": "o4pkgcl",
          "author": "Competitive-Fact-313",
          "text": "I this your spectrum atm is too broad, try to narrow down a learn specific things first and then widen the scope. Making AI saas is one things and working in Mlops is another. If you define well I can help better. To start small just play with a simple linear regression model on sagemaker  and use how many instances endpoints you wantâ€”->> take a lambda functionâ€”â€”>api gateway â€”-> test the api gateway endpoint using postman once done. Use your choice of frontend to show it as saas. This is the lowest level you can start with.",
          "score": 2,
          "created_utc": "2026-02-10 23:50:40",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4rfkqn",
              "author": "c0bitz",
              "text": "Thatâ€™s actually helpful. Breaking it down that way makes it less overwhelming. I was thinking too much in terms of â€œfull AI SaaSâ€ instead of just understanding one clean deployment path first. Did you find AWS interviews expect hands-on experience with those services or mostly conceptual understanding?",
              "score": 2,
              "created_utc": "2026-02-11 07:24:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4spi8o",
                  "author": "Competitive-Fact-313",
                  "text": "In aws interview it depends for seniors roles they may ask you  hands on or sometimes just ask you something from the the pipeline so that mean you must have had those done before thatâ€™s the only things makes you explain stuff",
                  "score": 2,
                  "created_utc": "2026-02-11 13:39:50",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4pt1ae",
          "author": "burntoutdev8291",
          "text": "In my experience, hiring has shifted more to understanding requirements and system design. I have never used bedrock or sagemaker, places I work at usually run self hosted vLLM. It also depends on the job description, backend roles have a bit more coding and system design, MLOps asks you on more MLOps stuff like model lineage, tracing, observability, and maybe some sysadmin stuff related to GPU. Never worked as an agentic or prompt engineer kind of AI engineer so I can't comment on that.",
          "score": 1,
          "created_utc": "2026-02-11 00:39:07",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o4repik",
          "author": "[deleted]",
          "text": "[removed]",
          "score": 1,
          "created_utc": "2026-02-11 07:16:20",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4s13h1",
              "author": "c0bitz",
              "text": "Totally agree, practical demos always carry more weight. Iâ€™ve been focusing on getting code + infra clean for simple model endpoints before scaling.",
              "score": 1,
              "created_utc": "2026-02-11 10:45:45",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1qz7jpc",
      "title": "Every team wants \"MLOps\", until they face the brutal truth of DevOps under the hood",
      "subreddit": "mlops",
      "url": "/r/devops/comments/1qz7e1r/every_team_wants_mlops_until_they_face_the_brutal/",
      "author": "pm19191",
      "created_utc": "2026-02-08 12:21:10",
      "score": 5,
      "num_comments": 0,
      "upvote_ratio": 1.0,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1qz7jpc/every_team_wants_mlops_until_they_face_the_brutal/",
      "domain": "",
      "is_self": false,
      "comments": []
    },
    {
      "id": "1r21sp7",
      "title": "A question for seniors",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r21sp7/a_question_for_seniors/",
      "author": "3MR_MLops",
      "created_utc": "2026-02-11 16:21:58",
      "score": 5,
      "num_comments": 14,
      "upvote_ratio": 0.86,
      "text": "If you are now HR\nWhat is the one thing that you rarely see in entry-level employee files that would make you want to hire someone immediately? ",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r21sp7/a_question_for_seniors/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4tm5jn",
          "author": "dayeye2006",
          "text": "My hiring manager told me they know this person and we must have this person at our company",
          "score": 3,
          "created_utc": "2026-02-11 16:24:38",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4tmi5g",
              "author": "3MR_MLops",
              "text": "Who is he?",
              "score": 1,
              "created_utc": "2026-02-11 16:26:16",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4toe6l",
                  "author": "dayeye2006",
                  "text": "A hypothetical person",
                  "score": 1,
                  "created_utc": "2026-02-11 16:35:04",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o502t0x",
          "author": "Hyperventilater",
          "text": "Accurate representation of contributions with numerical impact, personal projects that show legitimate interest rather than just ambition, any personalization that highlights a strong desire to learn. Those really signify a junior that will bring innovation and talent to the role to me.",
          "score": 3,
          "created_utc": "2026-02-12 16:11:21",
          "is_submitter": false,
          "replies": [
            {
              "id": "o509v2m",
              "author": "3MR_MLops",
              "text": "This is gold! I never realized how much 'numerical impact' outweighs just listing skills for a junior. It makes total sense to show how a project actually solved a problem or improved a metric. Iâ€™m definitely going to apply this mindset to my current MLOps learning path. Much appreciated!",
              "score": 2,
              "created_utc": "2026-02-12 16:43:57",
              "is_submitter": true,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "1r1xg77",
      "title": "Hello every one! ðŸ‘‹",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r1xg77/hello_every_one/",
      "author": "3MR_MLops",
      "created_utc": "2026-02-11 13:31:16",
      "score": 5,
      "num_comments": 9,
      "upvote_ratio": 0.73,
      "text": "Hi everyone! Iâ€™m Amr, a 17-year-old aspiring MLOps Engineer from Egypt. Iâ€™ve already covered Python, SQL, Linux, Git/GitHub, and some FastAPI. I recently finished the first two courses of Andrew Ngâ€™s Machine Learning Specialization in just 7 days! To make sure I truly understood the concepts, I applied what I learned in two projects which you can find here: https://github.com/3MR-MLops/my_project_of_ML\n\nHere is my upcoming plan for the next few weeks: 1. Finish Andrew Ngâ€™s 3rd ML course. 2. Deep Learning Specialization. 3. Advanced FastAPI. 4. Docker & Containerization. 5. CI/CD Pipelines. 6. MLflow (Experiment Tracking). 7. Cloud (AWS). 8. Kubernetes (k8s).\n\nMy goal is to be \"Production-ready\" for international internships. Does this order make sense? Is there anything I should add or change to stand out more to recruiters?\n\nThanks for your guidance!",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1r1xg77/hello_every_one/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4srdbn",
          "author": "dukesb89",
          "text": "I think your plan is good. Try and add more projects to your GitHub portfolio to demonstrate your skills. This is the key way to differentiate yourself when you have no work experience.",
          "score": 1,
          "created_utc": "2026-02-11 13:50:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o4srxwt",
              "author": "3MR_MLops",
              "text": "ok thanks bro \nI'm trying to add at least one project to everything I'm learning now, and eventually I'll work on 3 or more projects like a chatbot, a product suggestion project, videos, and so on.",
              "score": 1,
              "created_utc": "2026-02-11 13:53:21",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o4sskmo",
                  "author": "dukesb89",
                  "text": "Instead of doing lots of small projects, do a few bigger ones. The one you have now is a good start from a learning perspective but too simple to get a hiring manager to take notice. You will want to get to a point where you are engineering a full system. It will take time",
                  "score": 1,
                  "created_utc": "2026-02-11 13:56:49",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "1qyg78x",
      "title": "Best books/resources for production ML & MLOps?",
      "subreddit": "mlops",
      "url": "/r/learnmachinelearning/comments/1qyby8i/best_booksresources_for_production_ml_mlops/",
      "author": "Giux99",
      "created_utc": "2026-02-07 15:10:04",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.84,
      "text": "[External Link]",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qyg78x/best_booksresources_for_production_ml_mlops/",
      "domain": "",
      "is_self": false,
      "comments": [
        {
          "id": "o4lxzaf",
          "author": "Big-Cockroach4492",
          "text": "[https://github.com/harvard-edge/cs249r\\_book?tab=readme-ov-file](https://github.com/harvard-edge/cs249r_book?tab=readme-ov-file)\n\n\n\n\n\n[https://github.com/harvard-edge/cs249r\\_book/blob/dev/book/README.md](https://github.com/harvard-edge/cs249r_book/blob/dev/book/README.md)\n\n  \n",
          "score": 1,
          "created_utc": "2026-02-10 13:14:46",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1r2t5sn",
      "title": "Seeking deep 1:1 mentoring (Databricks / Snowflake / Azure ML)",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "author": "Admirable-Crab-9908",
      "created_utc": "2026-02-12 13:17:02",
      "score": 4,
      "num_comments": 1,
      "upvote_ratio": 0.75,
      "text": "Looking for structured 1:1 mentoring to go from implementation-level expertise to platform-level mastery.\n\nFocus areas:\n\n\tâ€¢\tDatabricks MLOps (Unity Catalog, MLflow, CI/CD, governance)\n\n\tâ€¢\tSnowflake ML (Snowpark ML, feature pipelines, deployment patterns)\n\n\tâ€¢\tAzure ML (enterprise pipelines, model serving, security)\n\nKindly DM. Will be happy to pay hourly. ",
      "is_original_content": false,
      "link_flair_text": "MLOps Education :snoo_hearteyes:",
      "permalink": "https://reddit.com/r/mlops/comments/1r2t5sn/seeking_deep_11_mentoring_databricks_snowflake/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o4zpz76",
          "author": "kchandank",
          "text": "I will DM you",
          "score": 2,
          "created_utc": "2026-02-12 15:10:17",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qyp1t2",
      "title": "Tech job search : how to get an entry level positions in tech.",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qyp1t2/tech_job_search_how_to_get_an_entry_level/",
      "author": "Traditional-War-9554",
      "created_utc": "2026-02-07 20:51:19",
      "score": 3,
      "num_comments": 10,
      "upvote_ratio": 0.8,
      "text": "recent graduate and no prior work experience[](https://www.reddit.com/submit/?source_id=t3_1qyp0i3)",
      "is_original_content": false,
      "link_flair_text": null,
      "permalink": "https://reddit.com/r/mlops/comments/1qyp1t2/tech_job_search_how_to_get_an_entry_level/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o45s73h",
          "author": "denim_duck",
          "text": "mlops is not an entry level role",
          "score": 8,
          "created_utc": "2026-02-07 22:58:13",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48ja8f",
              "author": "Traditional-War-9554",
              "text": "ok, can you suggest other roles to start with ?",
              "score": 0,
              "created_utc": "2026-02-08 11:26:41",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48x7xs",
                  "author": "denim_duck",
                  "text": "What was your plan for after graduation when you started school?",
                  "score": 1,
                  "created_utc": "2026-02-08 13:17:24",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o455ihd",
          "author": "randoomkiller",
          "text": "what are your qualifications?",
          "score": 3,
          "created_utc": "2026-02-07 20:54:10",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45zl3u",
          "author": "SpiritedChoice3706",
          "text": "The first step is probably posting in a relevant subreddit.",
          "score": 2,
          "created_utc": "2026-02-07 23:44:36",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o46edob",
          "author": "ApprehensiveFroyo94",
          "text": "I really donâ€™t want to be the bearer of bad news, but this is not an entry level field. \n\nI know sometimes this sub might feel like itâ€™s gatekeeping the mlops role, but we say this for a reason - thereâ€™s just so many tools you need to understand, build, and monitor, which is just not feasible for someone with no work experience.\n\nTry going for analytics / entry ds role if you have a sufficient math background and move from there.",
          "score": 2,
          "created_utc": "2026-02-08 01:13:45",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48jc06",
              "author": "Traditional-War-9554",
              "text": "i dont like analytics",
              "score": 0,
              "created_utc": "2026-02-08 11:27:08",
              "is_submitter": true,
              "replies": [
                {
                  "id": "o48wp5z",
                  "author": "denim_duck",
                  "text": "You donâ€™t have to like it. You have to do it.",
                  "score": 3,
                  "created_utc": "2026-02-08 13:13:56",
                  "is_submitter": false,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o47fxkz",
          "author": "Boognish28",
          "text": "Mlops is a mixture of a ton of specializations. Traditional engineering, delivery, infra, sre, etc. \n\nLearn one. Then learn another. Give it five or ten years, then you might be good.",
          "score": 2,
          "created_utc": "2026-02-08 05:25:39",
          "is_submitter": false,
          "replies": []
        },
        {
          "id": "o45ocdu",
          "author": "Anti-Entropy-Life",
          "text": "Build something end-to-end, make it public on GitHub, then find jobs looking for people to build similar things at scale.",
          "score": 1,
          "created_utc": "2026-02-07 22:36:12",
          "is_submitter": false,
          "replies": []
        }
      ]
    },
    {
      "id": "1qz7aoh",
      "title": "Logging Model Description",
      "subreddit": "mlops",
      "url": "https://www.reddit.com/r/mlops/comments/1qz7aoh/logging_model_description/",
      "author": "kayhai",
      "created_utc": "2026-02-08 12:07:51",
      "score": 3,
      "num_comments": 4,
      "upvote_ratio": 1.0,
      "text": "Iâ€™m using self-hosted ML Flow. How do I log the model description using mlflow.sklearn.log\\_model? In other words, how can I programmatically add or update the model description, instead of manually typing it into the ML Flow UI? \n\nAm unable to find the answer in documentationâ€¦.\n\nThanks!",
      "is_original_content": false,
      "link_flair_text": "beginner helpðŸ˜“",
      "permalink": "https://reddit.com/r/mlops/comments/1qz7aoh/logging_model_description/",
      "domain": "self.mlops",
      "is_self": true,
      "comments": [
        {
          "id": "o48ts7i",
          "author": "MyBossIsOnReddit",
          "text": "[https://mlflow.org/docs/latest/api\\_reference/python\\_api/mlflow.client.html?highlight=update#mlflow.client.MlflowClient.update\\_registered\\_model](https://mlflow.org/docs/latest/api_reference/python_api/mlflow.client.html?highlight=update#mlflow.client.MlflowClient.update_registered_model)",
          "score": 2,
          "created_utc": "2026-02-08 12:53:33",
          "is_submitter": false,
          "replies": [
            {
              "id": "o48u0p7",
              "author": "MyBossIsOnReddit",
              "text": "Oh and you might also want to look at update\\_model\\_version! ",
              "score": 2,
              "created_utc": "2026-02-08 12:55:15",
              "is_submitter": false,
              "replies": [
                {
                  "id": "o49cmpk",
                  "author": "kayhai",
                  "text": "Thanks! Strange that it canâ€™t be done from log_model!",
                  "score": 1,
                  "created_utc": "2026-02-08 14:50:11",
                  "is_submitter": true,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "o4g3qjl",
          "author": "Internal-Tackle-1322",
          "text": "Thatâ€™s actually by design.                                                                                                                                   log\\_model is intentionally artifact-focused, while descriptions live in the Model Registry metadata.             Keeping them separate avoids coupling training runs with registry semantics.",
          "score": 2,
          "created_utc": "2026-02-09 15:33:33",
          "is_submitter": false,
          "replies": []
        }
      ]
    }
  ]
}